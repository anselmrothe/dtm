{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking into the titles of individual articles with certain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from gensim.models import ldaseqmodel\n",
    "from gensim.corpora import Dictionary, bleicorpus, textcorpus\n",
    "import numpy as np\n",
    "from gensim.matutils import hellinger\n",
    "import time\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from IPython.core.debugger import set_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['doc_length', 'docnames', 'terms', 'term_frequency', 'docs_per_year', 'term_topic', 'doc_topic'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata_new = pickle.load(open('output/dtm_processed_output.p', 'rb'))\n",
    "alldata_new.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[159, 188, 145, 201, 244, 397, 377, 269, 173, 514, 443, 553, 431, 676, 497, 597, 464, 592]\n"
     ]
    }
   ],
   "source": [
    "doc_year=alldata_new['docs_per_year']\n",
    "print(doc_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "doc_year=alldata_new['docs_per_year']\n",
    "doc_ids =[0]+list(np.cumsum(doc_year))\n",
    "\n",
    "term_topic = alldata_new['term_topic']# term_topic is n_years*n_topics*n_terms\n",
    "terms = alldata_new['terms']\n",
    "\n",
    "doc_topicyrs = alldata_new['doc_topic']\n",
    "\n",
    "doc_topic = []\n",
    "for year in range(len(term_topic)):    \n",
    "    doc_topic.append(alldata_new['doc_topic'][doc_ids[year]:doc_ids[year+1]])# doc_topic is nyear*n_docs given year*n_topics\n",
    "    \n",
    "# rename topics by their top freq word\n",
    "topics = range(term_topic.shape[1])\n",
    "\n",
    "def topic_label(topic, term_topic, terms):\n",
    "    term_freqs = np.sum(term_topic[:,topic,:], axis = 0)\n",
    "    max_term = np.argsort(-term_freqs)[0]\n",
    "    return(terms[max_term])\n",
    "\n",
    "\n",
    "topic_labels = [topic_label(topic, term_topic, terms) for topic in topics]\n",
    "print(topic_labels)\n",
    "\"\"\"with open('output/all_visdtm.p','br') as f:\n",
    "    allvisdtm=pickle.load(f)\n",
    "for visdtm in allvisdtm:\n",
    "\n",
    "    visdtm[0]['topiclabel']=topic_labels\"\"\"\n",
    "#topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 159,\n",
       " 347,\n",
       " 492,\n",
       " 693,\n",
       " 937,\n",
       " 1334,\n",
       " 1711,\n",
       " 1980,\n",
       " 2153,\n",
       " 2667,\n",
       " 3110,\n",
       " 3663,\n",
       " 4094,\n",
       " 4770,\n",
       " 5267,\n",
       " 5864,\n",
       " 6328,\n",
       " 6920]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alltitles=alldata_new['docnames']\n",
    "doctitle = []\n",
    "for year in range(len(doc_year)):\n",
    "    doctitle.append(alltitles[doc_ids[year]:doc_ids[year+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature1: most typical article of each topics\n",
    "with open('highest_freq.txt','w') as f:\n",
    "    for kt in range(len(topic_labels)):\n",
    "        f.write('\\ntopic '+str(kt)+': '+topic_labels[kt]+'\\n')        \n",
    "        for year in range(len(doc_topic)):  \n",
    "            topicfreq=np.array(doc_topic[year]).T[kt]\n",
    "            idx=np.argmax(topicfreq)\n",
    "            title=doctitle[year][idx]\n",
    "            \n",
    "            f.write(str(year+2000)+': '+title+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we read the titles and give names for each topic. then store it.\n",
    "topicnames=['Sequential learning','Face and emotion perception','Reasoning','Text processing and creativity','Mathematical Psychology','Decision making','Language: syntax','Causal reasoning','Knowledge structure','Developmental psychology','Spatial cognition and embodied cognition','Memory','Categorization','Language: semantics','Educational psychology','Artificial network and Neuroscience','non-verbal communication','Probabilistic modeling','Consciousness and identity','Visual attention']\n",
    "with open('topicnames.p','wb') as f:\n",
    "    pickle.dump(topicnames,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gettitle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-512d74654331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmaxE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallentrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mminE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallentrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgettitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'min:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgettitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gettitle' is not defined"
     ]
    }
   ],
   "source": [
    "# feature2: \"purest\" / \"most chaotic paper\" \n",
    "maxen=0\n",
    "with open('maxmin_entropy.txt','w') as f:\n",
    "    for year in range(len(doc_topic)):  \n",
    "        f.writelines('\\n'+str(year+2000)+':\\n')\n",
    "        alldocs = doc_topic[year]\n",
    "        allentrop=[]\n",
    "        for d in alldocs:\n",
    "            allentrop.append(entropy(d))\n",
    "        # rank them\n",
    "        maxE = np.argmax(allentrop)\n",
    "        minE = np.argmin(allentrop)\n",
    "        f.writelines('max:'+doctitle[year][maxE]+'\\n')\n",
    "        f.writelines('min:'+doctitle[year][minE]+'\\n')\n",
    "        \n",
    "        # max entropy across years?\n",
    "        if maxE>maxen:\n",
    "            maxen=maxE\n",
    "            maxtitle=gettitle(year,maxE)\n",
    "print(maxtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# does entropy correlate with length of title? -- no, not really\n",
    "alltitlen=[]\n",
    "allentrop=[]\n",
    "for year in range(len(doc_topic)):  \n",
    "    alldocs = doc_topic[year]\n",
    "    for d in alldocs:\n",
    "        allentrop.append(entropy(d))\n",
    "    for idx in range(len(alldocs)):\n",
    "        alltitlen.append(len(gettitle(year,idx)))\n",
    "plt.plot(alltitlen,allentrop,'.')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for kt in range(len(topic_labels)):\n",
    "    print(len(doc))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
