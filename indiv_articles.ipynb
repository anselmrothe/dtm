{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking into the titles of individual articles with certain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from gensim.models import ldaseqmodel\n",
    "from gensim.corpora import Dictionary, bleicorpus, textcorpus\n",
    "import numpy as np\n",
    "from gensim.matutils import hellinger\n",
    "import time\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from IPython.core.debugger import set_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['doc_length', 'docs_per_year', 'term_frequency', 'term_topic', 'docnames', 'doc_topic', 'terms'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata_new = pickle.load(open('output/dtm_processed_output.p', 'rb'))\n",
    "alldata_new.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sequence', 'face', 'reasoning', 'text', 'color', 'decision', 'sentence', 'causal', 'concept', 'child', 'spatial', 'error', 'category', 'english', 'student', 'network', 'speaker', 'probability', 'agent', 'visual']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "doc_year=alldata_new['docs_per_year']\n",
    "doc_ids =[0]+list(np.cumsum(doc_year))\n",
    "\n",
    "term_topic = alldata_new['term_topic']# term_topic is n_years*n_topics*n_terms\n",
    "terms = alldata_new['terms']\n",
    "\n",
    "doc_topicyrs = alldata_new['doc_topic']\n",
    "\n",
    "doc_topic = []\n",
    "for year in range(len(term_topic)):    \n",
    "    doc_topic.append(alldata_new['doc_topic'][doc_ids[year]:doc_ids[year+1]])# doc_topic is nyear*n_docs given year*n_topics\n",
    "    \n",
    "# rename topics by their top freq word\n",
    "topics = range(term_topic.shape[1])\n",
    "\n",
    "def topic_label(topic, term_topic, terms):\n",
    "    term_freqs = np.sum(term_topic[:,topic,:], axis = 0)\n",
    "    max_term = np.argsort(-term_freqs)[0]\n",
    "    return(terms[max_term])\n",
    "\n",
    "\n",
    "topic_labels = [topic_label(topic, term_topic, terms) for topic in topics]\n",
    "print(topic_labels)\n",
    "\"\"\"with open('output/all_visdtm.p','br') as f:\n",
    "    allvisdtm=pickle.load(f)\n",
    "for visdtm in allvisdtm:\n",
    "\n",
    "    visdtm[0]['topiclabel']=topic_labels\"\"\"\n",
    "#topic_labels\n",
    "\n",
    "alltitles=alldata_new['docnames']\n",
    "doctitle = []\n",
    "for year in range(len(doc_year)):\n",
    "    doctitle.append(alltitles[doc_ids[year]:doc_ids[year+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('topicnames.p','rb') as f:\n",
    "    topicnames=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature1: most typical article of each topics\n",
    "\"\"\"with open('highest_freq.txt','w') as f:\n",
    "    for kt in range(len(topic_labels)):\n",
    "        f.write('\\ntopic '+str(kt)+': '+topic_labels[kt]+'\\n')        \n",
    "        for year in range(len(doc_topic)):  \n",
    "            topicfreq=np.array(doc_topic[year]).T[kt]\n",
    "            idx=np.argmax(topicfreq)\n",
    "            title=doctitle[year][idx]\n",
    "            \n",
    "            f.write(str(year+2000)+': '+title+'\\n')\n",
    "\n",
    "# we read the titles and give names for each topic. then store it.\n",
    "topicnames=['Sequential learning','Face and emotion perception','Reasoning','Text processing and creativity','Mathematical Psychology','Decision making','Language: syntax','Causal reasoning','Knowledge structure','Developmental psychology','Spatial cognition and embodied cognition','Memory','Categorization','Language: semantics','Educational psychology','Artificial network and Neuroscience','non-verbal communication','Probabilistic modeling','Consciousness and identity','Visual attention']\n",
    "with open('topicnames.p','wb') as f:\n",
    "    pickle.dump(topicnames,f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature 2: papers connecting 2 fields\n",
    "maxtp = 17 # the leading topic\n",
    "# now search for each secondary topic\n",
    "for kt in range(len(topic_labels)):\n",
    "    if kt == maxtp:\n",
    "        continue\n",
    "    f.write('2ndary topic '+str(kt)+': '+topicnames[kt]+'\\n')        \n",
    "    for year in range(len(doc_topic)):  \n",
    "        topicfreq=np.array(doc_topic[year]).T[kt]        \n",
    "        toptidx=np.argmax(topicfreq) # topic\n",
    "        kfreq = topicfreq[kt]\n",
    "        \n",
    "        title=doctitle[year][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_topic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0028c3ed4b73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmaxen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxmin_entropy.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m':\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0malldocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_topic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_topic' is not defined"
     ]
    }
   ],
   "source": [
    "# feature3: \"purest\" / \"most chaotic paper\" \n",
    "maxen=0\n",
    "with open('maxmin_entropy.txt','w') as f:\n",
    "    for year in range(len(doc_topic)):  \n",
    "        f.writelines('\\n'+str(year+2000)+':\\n')\n",
    "        alldocs = doc_topic[year]\n",
    "        allentrop=[]\n",
    "        for d in alldocs:\n",
    "            allentrop.append(entropy(d))\n",
    "        # rank them\n",
    "        maxE = np.argmax(allentrop)\n",
    "        minE = np.argmin(allentrop)\n",
    "        f.writelines('max:'+doctitle[year][maxE]+'\\n')\n",
    "        f.writelines('min:'+doctitle[year][minE]+'\\n')\n",
    "        \n",
    "        # max entropy across years?\n",
    "        if maxE>maxen:\n",
    "            maxen=maxE\n",
    "            maxtitle=doctitle[year][maxE]\n",
    "print(maxtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# does entropy correlate with length of title? -- no, not really\n",
    "alltitlen=[]\n",
    "allentrop=[]\n",
    "for year in range(len(doc_topic)):  \n",
    "    alldocs = doc_topic[year]\n",
    "    for d in alldocs:\n",
    "        allentrop.append(entropy(d))\n",
    "    for idx in range(len(alldocs)):\n",
    "        alltitlen.append(len(gettitle(year,idx)))\n",
    "plt.plot(alltitlen,allentrop,'.')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for kt in range(len(topic_labels)):\n",
    "    print(len(doc))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
