UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Psychological Implausibility of Naturalized Content

Permalink
https://escholarship.org/uc/item/4db5x3qr

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)

Author
Scott, Sam

Publication Date
2002-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

The Psychological Implausibility of Naturalized Content
Sam Scott (sscott@ccs.carleton.ca)
Department of Cognitive Science
Carleton University, Ottawa, ON K1S 5B6
Abstract
Conceptual Atomism (CA) is the view that most concepts are
represented psychologically as atoms, with no internal
structure and CA Atomism on its own is a
psychological/semantic theory, but from its inception, it has
been mixed up with the separate, meta-semantic project of
naturalizing content. I will show that this combined project is
forced to end in the self-defeating position of positing nonatomic structures for a large number of concepts. I suggest
that a better way out would be to separate the two projects,
and allow each to develop on its own.

Introduction
For the last two decades, a number of psychologically
minded philosophers have been pursuing a project aimed at
naturalizing mental content (Dretske, 1981; 1986; Fodor,
1987; 1990; Millikan, 1984; 1989; 1990). This is a metasemantic project that seeks an explanation of how
meaningful states can arise from non-meaningful ordinary
matter. The leading players in this project are also
proponents of Conceptual Atomism, the view that concepts
are atoms, with no internal structure or necessary relations
to other concepts. Conceptual Atomism is a
psychological/semantic project, for which the project of
naturalizing mental content is supposed to provide a metasemantics. This combined project – call it Naturalized
Conceptual Atomism (NCA) – is still very much a going
concern today (Fodor, 1998; Laurence and Margolis, 1999;
Margolis 1998; Millikan, 1998; Usher 2001).
The meta-semantic project has a big problem with what I
will call ‘unacquainted content’ (defined below). Proposed
solutions to this problem either do not work, or lead to a
psychological/semantic position that proponents of NCA
have explicitly rejected in the past – namely, that a large
number of lexical primitives correspond to complex (nonatomic) concepts. I will look at the three main attempts to
naturalize mental content and show how they all either fail
or lead to a non-atomic structure for large numbers of
concepts. The remedy for this situation, as I see it, is to
separate
the
meta-semantic
project
from
the
psychological/semantic project, and let each develop, for the
time being, independently of the other.

A Few Definitions
Concept
Following the standard psychological usage, I am using the
term ‘concept’ to mean a sub-propositional mental

representation. (This is in contrast to the standard
philosophical usage in which a concept is more like an
abstract object.) For the present purposes, I will stick to
examples of concepts (mental representations) that are about
objects or natural kinds.

Unacquainted Content
Unacquainted content is the Achilles heel of NCA. It is the
kind of content that a concept has if its bearer has had no
direct experience with the represented object or kind. For
example, anyone who has experience with dogs (i.e. almost
everyone reading this) will have a normal DOG concept. But
most North Americans who have heard of, but never
directly experienced, wombats have a WOMBAT concept
with unacquainted content.1
The term ‘unacquainted content’ also covers many kinds
of hypothesized, future or fictional content. For instance the
concept, UNICORN, has unacquainted content because the
concept bearers could not possibly have directly
experienced the nonexistent objects to which it refers.

Nonexistent Object
Nonexistent objects are what concepts with unacquainted
content seem to refer to. Maybe nonexistent objects are
objects in possible worlds, maybe they have some kind of
Meinongian nonexistent being,2 or maybe they don’t exist at
all and references to them are vacuous. I don’t intend to take
a position on this ontological issue, because the main
question of the paper is not whether there are unicorns, but
whether there are UNICORNs (atomic representations for
unacquainted content).3

The Problem of Unacquainted Content
The main proponents of NCA are Dretske, Millikan and
Fodor. All three are engaged in a philosophical project that
seeks (a) a naturalized account of (b) external content, and
all three tend to assume that (c) concepts are atoms with no
internal structure. Their three different brands of NCA
differentiate around (d) the special problems posed by
misrepresentation. I will briefly discuss these four points of
agreement and then I will discuss the differences between

1

A word in small caps (e.g. WOMBAT) refers to a concept, while a
word in single quotes (e.g. ‘wombat’) refers to a lexical item.
2
Alexius Meinong was the German philosopher and psychologist
credited with proposing this solution (Meinong, 1904).
3
My hunch is that everyday common sense is pseudo-Meinongian,
and therefore my description of a unicorn as a nonexistent object
will be perfectly intelligible to all but the most dogmatic readers.

the three proposals, focusing on the special problem posed
by unacquainted content.4
(a) A Naturalized Account. To naturalize content would
be to find a coherent story to tell about how the intentional
nature of concepts arises from the non-intentional nature of
ordinary matter. In practice this has typically meant
grounding the meaning of a symbol in some kind of causal
or information-bearing relationship between the symbol and
the object it represents.
(b) External Content. Proponents of NCA follow Putnam
(1975) in insisting that there has to be an external or broad
component to representational content. Meaning is not
(only) in the head.
(c) Conceptual Atomism. Dretske, Millikan, and Fodor all
make the assumption that concepts and other meaningful
mental states must be both syntactically and semantically
atomic. A concept simply refers to an object in the world.
Semantically speaking, no part of a concept’s meaning
derives from any relationship it may have with other
concepts. Syntactically speaking, if the concept had an
internal structure of some kind, it would raise the question
of what the individual parts of the structure refer to, and it’s
doubtful whether that is even a meaningful question to ask
in this context. If, for example, DOG is satisfied by all and
only dogs because of a causal relationship between DOGs
and dogs, then there is just no internal structure in the
equation that needs to be explained.
(d) Misrepresentation. If the meaning of DOG is just dog,
and if DOG gets its meaning in virtue being caused by dogs,
what do we do with the fact that sometimes DOG tokens
might be caused by things other than dogs? For example, a
cat on a dark night might cause a DOG token. If so, this
seems to imply that DOG means the same as ‘dog or cat on a
dark night’, which is intuitively wrong. In fact, this
“disjunction problem” is much bigger than that. Pictures of
dogs can also cause DOG tokens. So can the word ‘dog’,
thoughts about pets, and so on. So the meaning of DOG, on
this account, would actually be an infinite disjunction
including things like dogs, cats on dark nights, ‘dog’ tokens,
PET tokens, LEASH tokens, and so on. It is in attempting to
solve this problem that the three accounts proposed by
Dretske, Millikan and Fodor diverge.

Dretske on Misrepresentation
Dretske was the first to formulate a version of NCA built on
information theory (Dretske, 1981). According to this
approach, a concept C represents some X in the world only
if C carries information about X. More specifically, if X and
only X causes C then C represents X. The formulation is
meant to be counterfactual supporting. So if X and only X
4

Sometimes the term ‘misrepresentation’ is used to include
representations of nonexistent objects and states of affairs as well
as representations tokened in error. But nonexistent objects are a
kind of unacquainted content. So for my purposes, a
misrepresentation is a representation that was supposed to correctly
represent an existing object or state of affairs, but, for some reason,
failed to do so.

would cause C, then C represents X. Left like this, Dretske’s
theory suffers from the disjunction problem as badly as any
causal theory possibly could – the condition that only X
would ever cause C is far too strong to apply to real
cognitive agents in noisy environments.
Dretske’s proposed solution (Dretske, 1986)5 begins by
making a distinction between simple and complex
organisms. Simple organisms have only one route to a
representational state. As an example, he points to marine
bacteria that contain magnetic sensors called magnetosomes.
These sensors detect the surrounding magnetic field and
allow the bacterium to align itself with magnetic north.
Since in the northern hemisphere, the lines of the magnetic
field are inclined downwards, the bacterium can use the
signal from its magnetic sensors to swim upwards or
downwards in the water. The bacteria die in the oxygen-rich
water close to the surface, so bacteria living in the north are
naturally selected to use their sensors to swim downwards
(towards magnetic north). If they are transplanted to the
southern hemisphere where the field lines incline upwards,
they will kill themselves by swimming into oxygen-rich
water.
Dretske thinks that simple organisms like the
magnetosome bacteria cannot accidentally misrepresent,
because the information contained in whatever
representations they form is ambiguous. In its natural
environment, the bacterium’s magnetosome representations
reliably causally covary with the direction of oxygen-free
water. Hence it is tempting to say that when the bacterium is
moved to the southern hemisphere, it begins to misrepresent
that direction. But on the other hand, the magnetosome
representations also reliably causally covary with the
direction of magnetic north, and this does not change no
matter where on earth the bacterium is moved to. So on this
latter view, it is not a case of misrepresentation that causes
the northern bacteria to kill themselves when moved to the
south. The magnetosome mechanism still reliably indicates
magnetic north, but something else is going wrong inside
the organism that causes it to swim in that direction and kill
itself. Dretske concludes from this that where there is only
one causal route to a representation, misrepresentation
cannot occur because the informational content of the
representation (i.e. what the representation is supposed to
mean) is indeterminate. Unless it is possible to
unambiguously determine a representation’s informational
content, it is not possible to determine whether it has been
tokened in error.
In more complex organisms, there can be more than one
route to a representation. For instance, a human being can
detect a hamburger by seeing it, smelling it, tasting it,
feeling it, and so on. There are multiple sensory routes that
end in the same representation, H. If, on the contrary, one
could only detect a hamburger by smelling it, H would
reliably causally covary with both the hamburger and the
5

In later work, Dretske (1988) pursued a different solution that
shares more in common with Millikan’s approac h, discussed
below.

odor. So the content of H, on Dretske’s story, would be
indeterminate. But since there are at least four sensory
routes (in a human) to H, the content can be fixed. A token
of H caused by seeing a hamburger does not causally covary
with the odor of the burger, so the odor can be ruled out as
part of H’s content. Now we can see how misrepresentation
is possible. Any one of the senses can be tricked into
causing a token of H when there is no hamburger present,
but since the content of H is fixed by the intersection of
multiple causal routes, the resulting token H can sensibly be
considered to accidentally misrepresent.

Dretske and Unacquainted Content
Information-based NCA of this kind suffers from a big
problem with unacquainted content. In Dretske’s version,
the problem is, in many cases, one of indeterminacy. Take
Jay Leno, the host of the tonight show. Like most people
with a LENO concept, I have watched him for hours on TV. I
know both what he looks like and what he sounds like, so I
have two causal routes to my LENO concept. If I ever saw
Jay Leno in person, it’s reasonable to suppose my LENO
concept would be tokened through one or more of these
causal routes. So the condition that Leno would cause LENO
tokens is satisfied. But the condition that only Leno would
cause LENO tokens is violated – recordings of Leno also
cause LENO tokens. Unfortunately, the multiple causal
routes story is no help here because I have only two causal
routes to LENO tokens and they would both be engaged
whether I saw him live or on TV. It’s possible that this
problem can be set aside by noting that there is a causal
relationship of some sort between the real Leno and the TV
Leno, but going down this road will likely produce more
problems than it solves. There is a causal relationship
between a certain type of bacteria and pimples, but it should
not follow, at least in any Conceptual Atomist story, that
any part of the content of my PIMPLE concept is a type of
bacteria.
The problem gets worse when there is no possibility at all
of a direct sensory causal route to a token, as is the case for
nonexistent objects like the fictional detective, Sherlock
Holmes, or the Second Shooter hypothesized in certain
theories about the assassination of John F. Kennedy.6 I do
know a lot of facts about what these two nonexistent objects
are, having heard the conspiracy theory about the Kennedy
assassination and read the stories about Sherlock Holmes.
But it does not follow that either of these individuals
(should they turn out to exist after all) would cause
appropriate tokenings in me if I ever saw them, because I
have no history of a direct sensory link with them, and
therefore no tokens with the appropriate informational
content.

6

I have no opinion about these theories. Let’s just say for the sake
of argument that there was no Second Shooter.

Millikan on Misrepresentation
Millikan approaches the problem of misrepresentation from
another direction. One way of looking at misrepresentation
is to say that it arises when a given representation fails to
perform its proper function. For example, if DOG is tokened
in response to a cat, we can intuitively say that the
mechanism that outputs DOG tokens has failed to do its job
properly. The DOG token is only supposed to represent dogs,
but it is being tokened (in this case, accidentally) in
response to a cat. So all the approaches to explaining
misrepresentation within a theory of NCA have in common
that they want to find some naturalistic way to describe the
proper function of a given representation. Millikan meets
this challenge head on by trying to find a teleological
solution rooted in the theory of natural selection (Millikan
1984; 1989; 1990; also see Dretske, 1988).
Consider the human heart. Intuitively, we would like to
say that its proper function is to circulate blood, but where
do we get the authority to say such a thing? Millikan
answers that we can say the heart has the function of
circulating blood if we can show that’s what hearts were
naturally selected for. Applying this idea to mental
representations, Millikan suggests that we can say that, for
instance, DOG refers to dogs if we can show that’s what DOG
tokens were naturally selected for.
To determine what a representation was selected for,
Millikan urges us to focus on the systems within the
organism that make use of the representation (Millikan,
1989). For example, the representations produced by the
navigation mechanism within a magnetosome bacterium are
consumed by some other part of the organism that uses the
information to pick the current swimming direction. If we
assume that these various mechanisms were selected for
their ability to propel the bacterium away from oxygen-rich
water, then the proper function of the magnetosome
representations must be to represent the direction of such
water. So when we transplant the bacterium, it can truly said
to be Accidentally Misrepresenting that direction. Millikan’s
solution has the advantage of allowing us to say what we
intuitively want to say about the bacteria – that in normal
conditions they represent, and in abnormal conditions they
misrepresent.
A tempting way of looking at this solution is that it is the
same as Dretske’s information -based solution, but with the
causal covariation occurring on an evolutionary time scale
rather than over the lifetime of a single organism. In fact,
Dretske (1981: 234) does toy with the idea of innate
representational content produced in just such a way –
representations that are selected for the informational
content they carry. But reflection on the case of the
magnetosome bacteria shows the real difference in the two
theories. Recall that Dretske (1986) was forced to conclude
that the content of the magnetosome mechanism’s
representations were indeterminate – there were just too
many things the representations causally covaried with to
judge which was the ‘proper’ informational content. Exactly
the same argument would apply on an evolutionary scale.

But by focusing on the naturally selected proper function of
the representations, Millikan avoids this indeterminacy.

Millikan and Unacquainted Content
As appealing as Millikan’s solution to misrepresentation
may seem, it has problems with unacquainted content that
are at least as bad as those associated with Dretske’s
approach. As Dretske himself has pointed out (Dretske,
1986), the theory cannot explain representational content for
anything that a species either has not encountered during its
evolutionary history, or has encountered but had no need or
use for. If no member of the species, or any ancestor
species, ever encountered a particular type of object, then no
part of the organisms that comprise that species could
possibly have been selected for the purpose of representing
that content. This denies representational content to almost
any representation of a nonexistent object, and many
representations of real things such as works of art, new
pieces of technology, or anything that is recent enough to
have played no role in the evolutionary history of the
species. Millikan has a problem with unacquainted content
on an evolutionary scale.7

Fodor on Misrepresentation
Arguably, the most promising version of NCA comes from
Fodor (1987; 1990; 1994; 1998). For the last 15 years or so,
he has been pushing a theory of Asymmetric Causal
Dependence (ACD) theory to explain how an informationbased semantics could deal with, among other things,
misrepresentation.8 In his essay, “A theory of content II”, he
combines Dretskian informational semantics (a concept C
means X if it’s a law that X’s cause C’s) with an a symmetric
dependence condition (Y’s that cause C’s only do so
because X’s cause C’s and not vice versa). This takes care of
misrepresentations such as cats on dark nights causing DOG
tokens (this state of affairs is dependent on dogs causing
DOG tokens but not the other way around), and it is also
extendible to explain various kinds of “robust” tokenings
(non-X-caused C tokenings that are nevertheless not error
cases – for instance, DOG tokens that are caused by pictures
of dogs or thoughts about leashes.)9
7

It could be objected that the representation of hypothetical or
nonexistent things in general is very useful, and thus could have
been selected for. But Millikan’s theory is supposed to provide an
explanation for the specific content of specific representations, and
this is what it fails to do for unacquainted content.
8
Lately, Fodor (1998) prefers to talk about concept acquisition as a
process of “locking on” to a relevant property. The new
formulation addresses some concerns about nativism and ontology,
but Fodor is clear that however locking on works, the meaning of
the resulting concept is still grounded in an informational
relationship, and ACD remains his most mature attempt to
characterize that relationship.
9
Note that I am actually describing what Fodor (1990) called the
“pure” version of ACD. He also suggests the possibility of a
“mixed” version in which he adds the condition that C must have
actually been caused by X at least once. This mixed version will
obviously fail for unacquainted content, so I will only deal with

Fodor and Unacquainted Content
The problem of unacquainted content for pure ACD is
immediately apparent, particularly for nonexistent objects.
For example, how can non-unicorn-caused tokenings of
UNICORN be asymmetrically dependent on unicorn-caused
tokenings when there are no existing unicorns? Fodor thinks
that this objection can be answered, by reminding us that,
like Dretske, he is telling a nomic story:
It can be true that the property of being a unicorn is
nomologically linked with the property of being a cause of
UNICORNs even if there aren' t any unicorns… There wouldn’t
be non-unicorn-caused UNICORN tokens but that unicorns
would cause UNICORN tokens if there were any unicorns.
(Fodor, 1990, p101, italics removed and single quotes
changed to small caps for consistency).

Fodor has been attacked on the unicorn front before. For
instance Baker (1991) constructed a detailed argument
based on unicorns and “shunicorns” (a creature of her own
design) that requires us to speculate about which of various
possible worlds containing unicorns and/or shunicorns is
“closer” to our own. If your mind boggles at this kind of
talk, I will now offer what I hope is a slightly simpler
explanation below for why unicorns are a big thorn in the
side of the pure version of ACD.
In this unicorn-free world, all valid UNICORN tokenings
must be robust tokenings – they are caused by things other
than unicorns. The acquisition of the concept UNICORN in
the absence of unicorns comes from exposure to
representations (visual or verbal) of unicorns. Having
learned about unicorns from books and stories, if a unicorn
suddenly popped into existence in front of you, it would
likely cause a UNICORN token. So we have two valid causal
routes to UNICORN tokens: one from representations of
unicorns, and one from possible real unicorns that you
might encounter in the future (if unicorns began to exist). To
apply ACD, we have to know what would happen if we
broke either of these two causal links. Would breaking the
causal link between future unicorns and UNICORN tokens
break the link between representations of unicorns and
UNICORN tokens? My intuition is that this scenario doesn’t
even make sense, but suppose for the sake of argument that
breaking the unicorn/UNICORN link would break the
representation/UNICORN link. Then UNICORN tokens are
causally dependent on (future) unicorns.
But what would happen if we broke the causal link
between representations of unicorns and UNICORN tokens?
According to ACD, if UNICORN is to mean unicorn, then this
should not affect the causal link between future unicorns
and UNICORN tokens. But it obviously does. In a world
without unicorns, if you don’t learn about them from
representations of them then you don’t learn about them at
all. This means that if a unicorn suddenly popped into
existence in front of you, you wouldn’t know what it was.
Maybe it would cause tokens of HORSE, HORN or whatever,
pure ACD here. And to be fair, Fodor (1994: Appendix B) has
made it pretty clear that he doesn’t think much of the mixed theory
anyway.

but it wouldn’t cause a UNICORN token because you
wouldn’t have one for it to cause. So in the best case, causal
dependence runs both ways and ACD doesn’t apply. In the
worst case (where you don’t buy the story about breaking
the link between future unicorns and UNICORN tokens) ACD
runs in the wrong direction and UNICORN ends up having
representations of unicorns as its content. But this must be
false – UNICORN has unicorns as it’s content. 10 Notice that
you can run exactly the same argument for any type of
unacquainted content, such as my LENO concept. Tokenings
of LENO in the presence of Leno are causally dependent on
tokenings of LENO in response to representations of Leno.
There is a way out of this trap for an extreme radical
nativist. Fodor (e.g. 1998) entertains, though he does not
endorse, the possibility that we are born with a stock of
atomic concepts waiting to be triggered by the right sort of
content-fixing experiences. Applying this idea to
unacquainted content, if we all have built-in UNICORN token
types that just need to be “triggered” somehow, then maybe
our first encounter with a unicorn would cause a UNICORN
token after all. Of course we wouldn’t have a word for this
token, but that is irrelevant. So ACD would be satisfied by
assuming that we are born with a lifetime supply of tokens
that already have their nomic triggering conditions fixed.
But radical nativism is not a popular option in cognitive
science. Though Fodor correctly points out that whether (or
to what extent) nativism is true is an empirical question, it
seems very unlikely to most researchers that the empirical
facts will bear the theory out. Furthermore, if the project is
to naturalize content, then all radical nativism does is open
up new questions. We are now owed a naturalistic account
of how it can be the case that an individual is born with a
large stock of mental states that already have the appropriate
nomic connections. Given the problems with both Dretske
and Millikan’s evolutionary accounts, it seems unlikely that
such a story is forthcoming. Without the story, all we have
reduces to the statement that UNICORN means unicorn
because it has a set of properties that causes it to mean
unicorn.

The Non-atomic Way Out
All three attempts to construct a theory of NCA seem to fail
for unacquainted content. However there is still a way out
that is consistent with a slightly weakened version of
Conceptual Atomism. This solution, proposed by Fodor
(1990: 124) and Dretske (1981: 222, 230) is to allow some
concepts to be non-atomic, structured entities built out of
atomic components.11 So UNICORN, LENO, and so on actually
10

There is a persistent notion that UNICORN must refer to an idea or
to a representation. But a unicorn is not an idea or a representation;
it’s an animal that looks like a horse with a horn on its head. Ideas
and representations are not animals and they have neither heads
nor horns. So ideas and representations are the wrong sorts of
things to serve as the content for UNICORN.
11
Fodor proposes this (somewhat apologetically) only for cases of
nonexistent objects, but it is easily extendible to any unacquainted
content.

unpack into phrasal entities in the language of thought,
assembled out of primitive atoms. That is, they are
definitions.12 Fodor fails to provide any serious defense of
the position, except to state that he thinks the situation in
which a complex concept would be required is “very, very
rare” (1990:124, his italics). Dretske proposes the same
solution, but like Fodor, balks at defending it: “I hope [the
compositional solution] is sufficiently plausible not to need
argument” (1981:222, also his italics).
But contrary to Fodor, concepts with unacquainted
content don’t seem to be particularly rare at all. And
contrary to Dretske, the definitional solution does need an
argument, having been judged implausible, at least as a
general account of conceptual structure, by a wide
consensus of Cognitive Scientists.13 Almost any standard
account of the recent history of empirical research into
conceptual structure begins with a recounting of the demise
of so-called definitional theories (e.g. Komatsu, 1992;
Laurence and Margolis, 1999; Smith and Medin, 1981). The
most commonly cited reasons for abandoning of a
definitional account of conceptual structure are that: a) there
is a widespread consensus that most concept words of any
interest are not rigorously definable (see Laurence and
Margolis, 1999); b) no attempt to find psychological data
that might reveal a definitional structure for simple lexical
items has succeeded (e.g. Kintsch, 1974); and c) the wellestablished psychological phenomenon of typicality ratings,
or “goodness of example” effects (e.g. R osch, 1973) is
extremely difficult to account for within a definitional
theory (see Smith and Medin, 1981).

Conclusion: A Better Way Out?
Dretske, Millikan, and Fodor have no solution to the
problem of unacquainted content, unless we take one of two
rather unpalatable options: a) accept a radical concept
nativism in which tokens like UNICORN are an innate part of
our psychological make-up; or b) accept that many
concepts, including UNICORN, WOMBAT, LENO, and so on
must have a definitional structure. Nobody seems wants to
take option (a) seriously, and it begs the question anyhow,
so we’re left with option (b), which not only has no
empirical support, but also contradicts the whole spirit of
the Conceptual Atomist enterprise. What do we do now?
Recall that there are at least two projects here: the metasemantic project of naturalizing content, and the
psychological/semantic project of Conceptual Atomism.
The first project is stalled by the problem of unacquainted
content, and in attempting to save itself, has wreaked havoc
on the second project. My suggestion is that we do not
accept this conclusion, and that we separate the projects
from now on. Let those interested in the meta-semantic
12

“… the idea that many terms express concepts that have internal
structure is tantamount to the idea that many terms have
definitions.” (Fodor, 1981: 289)
13
Ironically, this consensus includes Fodor himself (e.g. Fodor,
1998; Fodor, Fodor and Garrett, 1975; Fodor, Garrett and Walker,
1980).

problem try to solve it on its own terms, and leave
Conceptual Atomism to develop on its own. That way
Conceptual Atomism can be consistent with itself in
claiming that UNICORN and WOMBAT are atomic, just like
DOG and COW. This is essentially the Language of Thought
hypothesis (Fodor, 1975) with a referential semantics, but
without the causal-historical meta-semantics. UNICORN
refers to unicorns, but how, exactly, it comes to do that is an
issue to be resolved (or not) by the separate project of metasemantics.
I suspect that there will be some skepticism as to whether
Conceptual Atomism can survive without its accompanying
meta-semantic theory. Therefore, I will end with two
reasons why I think that it can.
1. No competing theory is tied to a similar meta-semantic
project. For example, neither the prototype theory nor the
theory-theory of concepts attempts to say anything about
how meaning arises from non-meaningful stuff. Neither do
most modern versions of the definitional theory. And, after
all, why should they? At this early stage, a
psychological/semantic theory should be judged on its own
merits, not by standards set at some other level of analysis.
2. Conceptual Atomism is still a decent theory even
without the meta-semantic project. There is no
psychological evidence for definitional structure, and the
evidence that drives the prototype and theory theories can be
accounted for within Conceptual Atomism – the former by
supposing that typicality effects arise from a separate
categorization mechanism, and the latter by supposing that
people do have theories that guide their behavior, but that
these theories are about the concepts they involve, rather
than being constitutive of them. And above all, Conceptual
Atomism is arguably one of the most natural fits to the
computational theories of mind that are still so popular.

Acknowledgements
Many thanks to Rob Stainton for repeated discussion of this
material. Thanks also to Andy Brook, Craig Leth-Steensen,
and two anonymous reviewers for very helpful comments.

References
Baker, Lynne Rudder. (1991). Has content been
naturalized? In Barry Loewer and Georges Rey (Ed.)
Meaning in Mind: Fodor and his Critics. Oxford:
Blackwell.
Dretske, Fred. (1981). Knowledge and the Flow of
Information. Cambridge, MA: MIT Press.
Dretske, Fred. (1986). Misrepresentation. In R. Bogden (ed.)
Belief, Form, Content and Function. Oxford: Oxford
University Press.
Dretske, Fred. (1988). Explaining Behavior. Cambridge,
MA: MIT Press.
Fodor, Janet D., Jerry A. Fodor, and Merrill F. Garrett.
1975. The psychological unreality of semantic
representations. Linguistic Inquiry, 4, 515-531.
Fodor, Jerry A. (1975) The Language of Thought. New
York: Crowell.

Fodor, Jerry A. (1981). The present status of the innateness
controversy. In Representations: Philosophical Essays on
the Foundations of Cognitive Science. Cambridge, MA:
MIT Press.
Fodor, Jerry A. (1987). Psychosemantics. Cambridge, MA:
MIT Press.
Fodor, Jerry A. (1990). A theory of content II. In A Theory
of Content and Other Essays. Cambridge, MA: The MIT
Press.
Fodor, Jerry A. (1994). The Elm and the Expert. Cambridge,
MA: MIT Press.
Fodor, Jerry A. (1998). Concepts: Where Cognitive Science
Went Wrong. Oxford: Clarendon Press.
Fodor, Jerry A., Merrill F. Garrett, E. Walker and C. Parkes.
1980. Against definitions. Cognition, 8, 263-367
Kintsch, Walter. (1974). Lexical decomposition:
Compression and memory. In The Representation of
Meaning in Memory. Hillsdale, NJ: Lawrence Erlbaum
Associates.
Komatsu, Lloyd K. (1992). Recent views of conceptual
structure. Psychological Bulletin. 112(3): 500-526.
Laurence, Stephen and Eric Margolis. (1999). Concepts and
cognitive science. In Eric Margolis and Stephen Laurence
(Ed.) Concepts: Core Readings. Cambridge, MA: MIT
Press.
Margolis, Eric. (1998). How to acquire a concept. Mind and
Language, 13, 347-369.
Meinong, Alexius. 1904. The theory of objects. In Alexius
Meinong (ed.) Untersuchungen zur Gegenstandstheorie
und Psychologie. Reprinted in Roderick M. Chisholm
(ed.) 1960. Realism and the Background of
Phenomenology. New York: The Free Press. 76-117.
Millikan, Ruth Garrett. (1984). Language, Thought and
other Biological Categories. Cambridge, MA: MIT Press.
Millikan, Ruth Garrett. (1989). Biosemantics. Journal of
Philosophy, 86(6), 281-297.
Millikan, Ruth Garrett. (1990). Compare and contrast
Dretske, Fodor, and Millikan on teleosemantics.
Philosophical Topics, 18(2), 151-161.
Millikan, Ruth Garrett. (1998). A common structure for
concepts of individuals, stuffs, and real kinds; more
mamma, more milk and more mouse. Behavioral and
Brain Sciences, 9, 55-100.
Putnam, Hilary. (1975). The meaning of meaning. In Mind,
Language, and Reality. Cambridge, UK: Cambridge
University Press.
Rosch, Eleanor H. (1973). On the internal structure of
perceptual and semantic categories. In Timothy E. Moore
(Ed.) Cognitive Development and the Acquisition of
Language. New York: Academic Press.
Smith, Edward E. and Douglas L. Medin. (1981).
Categories and Concepts. Cambridge, MA: Harvard
University Press.
Usher, Marius. (2001). A statistical referential theory of
content: Using information theory to account for
misrepresentation. Mind and Language, 16(3), 311-334.

