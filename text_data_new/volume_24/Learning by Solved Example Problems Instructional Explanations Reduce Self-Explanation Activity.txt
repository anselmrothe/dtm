UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning by Solved Example Problems: Instructional Explanations Reduce Self-Explanation
Activity

Permalink
https://escholarship.org/uc/item/00q0t4h1

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)

Authors
Schworm, Silke
Renkl, Alexander

Publication Date
2002-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Learning by Solved Example Problems: Instructional Explanations Reduce
Self-Explanation Activity
Silke Schworm (schworm@psychologie.uni-freiburg.de)
Department of Psychology, Educational Psychology, Engelbergerstr.41
79085 Freiburg, Germany

Alexander Renkl (renkl@psychologie.uni-freiburg.de)
Department of Psychology, Educational Psychology, Engelbergerstr.41
79085 Freiburg, Germany
Abstract
Learning from worked-out examples is of major
importance for initial skill acquisition in well-structured
domains. In addition, research has provided knowledge
in regards to structuring worked-out examples and how
to effectively combine self-explanation activity and
instructional explanations. The goal of the present
project was to develop a computer-based learning
environment in which teachers can learn how to use
worked-out examples. Examples of favorably and
unfavorably designed worked-out examples were the
primary source of information for the teachers. The
examples (of worked-out examples) were not in
themselves worked-out examples if one views them from
a design perspective as the (design) solution steps were
not given. We have labeled this type of examples "solved
example problems." We investigated to what extent
learning from such solved example problems could be
fostered by self-explanation prompts and by providing
instructional explanations. The results of our 2x2 design
(80 student teachers) showed that prompting selfexplanations in particular had favorable effects. Hence,
self-explanations fostered learning not only from
worked-out examples but also from solved example
problems. Supplementary instructional explanations only
partially enhanced learning and at times they were even
detrimental.

Introduction
This study applies the results of cognitive science
research (i.e., worked-out example and self-explanation
research) to the design of a computer-based learning
environment. An empirical study about this learning
environment, in turn, contributes to the research on
example-based learning and self-explanations.
Learning from worked-out examples is of major
importance for the acquisition of cognitive skills in
well-structured domains such as mathematics or physics
(for an overview see Atkinson, Derry, Renkl, &
Wortham, 2000). However, worked-out examples do
not guarantee effective learning. One moderating factor
is the learner's self-explanation activity. Only when a
learner actively self-explains the rationale of the
worked-out solutions to her/himself will s/he gain an
understanding of the solution procedures. Another

factor is the provision of instructional explanations. In
the study presented below, teacher students learned in
an example-based computer learning environment how
to effectively structure and combine worked-out
examples. It was intended to foster their learning by the
employment of self-explanation prompts and by
supplementary instructional explanations.

Learning by Worked-Out Examples
Worked-out examples consist of a problem, solution
steps and the complete solution to the problem. Usually
they can be found in mathematics and physics
schoolbooks. In most cases, a principle or law is
introduced in the beginning followed by a worked-out
example. The worked-out example shows how the
principle can be applied to problem solving. Then,
problems to be solved by the students are given.
Learning by worked-out examples is not meant to
refer to the short learning phase between the
introduction of a principle and problem-solving. It
means, instead, that the example phase is prolonged.
Several studies have shown that such example-based
learning is more effective for skill acquisition than the
standard procedure of studying just one example and
then solving problems (for an overview see Sweller,
van Merriënboer, & Paas, 1998).
Of course, the use of worked-out examples do not
guarantee effective learning. Learning outcomes are
influenced mainly by (1) the learner's self-explanation
activity and the provided instructional explanations and
(2) how the learning materials (examples and problems)
are structured (cf. Atkinson et al., 2000). These two
aspects are discussed in the following sections.

Self-Explanations and Instructional
Explanations
The extent to which learners benefit from the study of
worked-out examples depends on how well they explain
the rationale of the presented solutions to themselves
("self-explanation effect", Chi et al., 1989; Renkl, 1997;
Renkl, Stark, Gruber, & Mandl, 1998). It is especially
useful to make the "meaning" of specific operations
explicit by reifying the relationship between (sub-)

1

goals and operators or with the principles underlying a
specific operation.
Whereas self-explanations are of major importance,
research has shown that the effects of instructional
explanations are often disappointing (e.g., Brown &
Kane, 1988; Chi, 1996). It seems to be more effective to
prompt self-explanations than to offer instructional
explanations. On the other hand, it has to be taken into
account that relying solely on self-explanations also has
several disadvantages. For example, at times the learner
is not able to self-explain a specific solution step or the
given self-explanations are incorrect.
Renkl (in press) developed a set of instructional
principles to support the spontaneous self-explanation
activity by providing instructional explanations. Two
central principles are (1) the priority of selfexplanations (instructional explanations should just be
used as type of support) and (2) the furnishing of
instructional explanations on learner demand. The study
of Renkl (in press) showed that such instructional
explanations heightened the average learning outcomes.
However, instructional explanations may not only
have positive effects. Due to their feedback functions, a
specific problem can occur (e.g., Kulhavy, 1977). If
feedback containing the correct answer (here: the
explanation) is easily available, learners typically
reduce their effort in attempting to find the answer on
their own. They tend to look up the right answer instead
of coming up with the answer themselves – which
reduces learning outcomes. Thus, the provision of
instructional explanations may reduce self-explanation
activities. Aleven and Koedinger (2000) found, in the
context of computer-based learning, that in more than
80% of the cases their learners did not use available
help which additionally required self-explanation
activity. The learners asked directly for the help which
contained the final solution.
In summary, self-explanations are of major
importance when learning from worked-out examples.
Instructional explanations can foster learning but often
they do not. What is left open is the question as to what
extent the findings on (self-) explanation can be
generalized to non-mathematized content areas.

Design of Worked-out Examples
Researchers (e.g., Catrambone, 1996; Mwangi &
Sweller, 1998) have suggested that the design of
worked-out examples plays a critical role in their
effectiveness (intra-example features). For example,
featuring sub-goals prominently fosters learning
outcomes (Catrambone, 1996). In the present study, we
focused on the so-called integrated format: Examples
are constructed which integrate all sources of
information into one source (e.g., diagrams and text).
Splitting learners’ attention across multiple, nonintegrated informational sources causes irrelevant
cognitive load and impairs learning (Mwangi &
Sweller, 1998; Ward & Sweller 1990).

Beyond the structure of single worked-out examples
the combination of multiple examples is of significance
for learning outcomes (inter-example features). In
general, multiple examples that contain different
surface features (e.g. figures, objects) should be used.
This aids the learner's ability to recognize the common
underlying structure when asked to compare examples.
Often there are two or more different structures to
be learned. It is important to emphasize the structural
aspects by using very similar surface features for
different problem types. Learners frequently do not
recognize the difference of the underlying structure
because they concentrate on the similarity of the surface
features. Therefore, they often choose the wrong
solution. Thus, when dealing with different but
interrelated problem types, multiple examples should be
combined in such a way that the relevant structural
features are apparent to the learner (structureemphasized example set). This can be achieved, as
stated above, by using different surface features within
one problem type and similar surface features between
the different problem types (Quilici & Mayer, 1996).
Of course, there are many other example features
which influence the effectiveness of learning from
worked-out examples, but in the first module of our
computer-based learning environment, we implemented
only two features: the integrated format and the
structure-emphasized example set.

Research questions
Taking into account the current research on learning
from worked-out examples, two important questions
can be formulated:
(1) How can we teach teachers the knowledge about
the effective use of worked-out examples in their
classroom? To reach this goal means to bridge research
findings into practice and to improve the quality of
instruction.
(2) Do self-explanations and additional instructional
explanations foster learning when learning from solved
example problems, that is, examples that do not provide
solution steps but only the problem and the final
solution? Such a solved example problem would be, for
example, a well-written essay (literature) or the picture
of an intriguing mask (arts). So far it remains an open
question as to what extent the results of the worked-out
example research, which were mainly obtained within
the context of learning mathematics and physics, can be
transferred to learning by solved example problems in
other topic areas.
For addressing the first question we have developed
an initial module for a computer-based learning
environment. It is the first part of a future web-based
learning environment in which teachers can learn how
to use worked-out examples in their classrooms. Due to
its intended net-based use, not only "objective" learning
outcomes were of interest but also the program's
acceptance and the perceived learning results. Those

2

aspects are of major importance when offering a
facultative learning opportunity to practitioners.
Acceptance is the basis of the usage of the program
itself whereas the perceived results are a predictor for
its implementation in classrooms.
In this module, future teachers learn about the
design of worked-out examples by studying cases of
well and poorly designed worked-out examples. It is
important to note that the design of worked-out
examples is not an algorithmic process with specific
solution steps. Therefore, the examples of the program
are, as viewed from the teacher's perspective, not
worked-out examples (they do not contain any solution
steps) but instead solved example problems (there is
simply the problem and its solution) (see Fig. 1).
Solved example problem for the learning
content "design of worked-out examples"
For teachers:
Design perspective

participants were randomly assigned to the four
experimental conditions of a 2x2-factorial design (n =
20 in each group). In the experiment, the participants
learned in a computer-based learning environment how
to effectively design worked-out examples by studying
solved example problems (factor 1: prompting selfexplanations [with and without], factor 2: instructional
explanations [with and without]). In the analysis of the
written self-explanations (reactions to the prompts), 6
participants are missing due to technical problems (3
per cell). Technical problems also led to 4 missing
values with respect to time-on-task which was recorded
as a control variable.

Learning Environment
The program contained a short introduction about
learning with worked-out examples. Afterwards
examples of worked-out examples or sets of examples
were displayed. They were taken from the domains of
geometry and physics.

Problem givens (e.g. velocity problem) + solution steps + solution
For students:
math / physics perspective
Worked-out example for the learning content "velocity"

Figure 1: Contents of the learning program from
different perspectives
The second research question of this study addresses
the effectiveness of self-explanations and instructional
explanations when learning by solved example
problems: Do self-explanation prompts and additional
instructional explanations upon learner demand foster
learning?
The following specific research questions were
addressed:
1. Is there – as expected – a positive effect of
prompting self-explanations on learning outcomes?
2. Is there a positive effect of providing instructional
explanations?
3. Do the two instructional means combine additively
or non-additively?
4. Do the different instructional treatments influence
acceptance and perceived leaning outcomes?

Methods
Sample and Design
80 student teachers from two different colleges
volunteered to take part in this study (mean age: 22.3
years; 52 female and 28 male participants). One part of
the participants were future teachers in German lowtrack and medium-track schools (n = 47), the other part
were future teachers in high-track schools (n = 33). The

Figure 2: Screenshot of the learning environment
Figure 2 shows a screenshot of the learning
environment with a solved example problem with an
integrated format. Two worked-out examples were
presented to the participants—one required the mapping
of different sources of information (graphic,
calculation, and text – left side) whereas the other was
designed in an integrated format (right side). The selfexplanation prompts asked the learners to comment on
why one of these two worked-out examples was more
favorable. Self-explanations had to be provided to all 13
prompts but their extensiveness (number of
elaborations) was self-regulated by the learner. The
provided instructional explanations were—in a sense—
answers to the self-explanation prompts. They were
presented verbally when the learners clicked on a
button. The button contained a picture of an expert
teacher introduced earlier in the program (see Fig. 2). A
"text"-button, which appeared after the acoustic
presentation, enabled the learner to view a written
explanation. The instructional explanations were

3

demanded 4.13 times on average (SD = 3.31). The
frequency of demands did not correlate with learning
outcomes. Therefore, this variable was not further
considered.

Procedure
The participants worked in individual sessions of
approximately 3 hours. In order to provide basic
knowledge to allow the participants to be able to
understand the solved example problems, an
instructional text containing the basic principles of the
worked-out example design was given to the
participants. Afterwards they studied several solved
example problems dealing with an integrated format
and structure-emphasized example set in the domain of
geometry and physics. The different domains were
chosen to foster the transfer of the acquired knowledge.
All participants were instructed to think aloud during
this period in order to assess (oral) self-explanations
(these data have not yet been analyzed). The group with
prompted self-explanations had to write down their
explanations in note-boxes. During the study of
examples, the time spent on different pages of the
learning program was registered. After studying the
solved example problems, the participants worked on
the post-test (learning outcomes). Lastly, the
participants filled out a questionnaire regarding the
perceived usefulness of the program.

Instruments
Post-test: Assessment of the Learning Outcomes.
The first part of our post-test consisted of selection
tasks. The participants had to choose one of several
given worked-out examples (integrated format) or they
had to combine four examples to a structureemphasized example set. The domains used were
geometry, physics (near transfer), and arithmetic (far
transfer). For near and far transfer there were three
tasks with increasing complexity. Depending on the
complexity of the task 1, 4 or 6 points could be
achieved (selection part: maximum of 22 points). The
second part was a generation task: The participant had
to create a structure-emphasized example set in an
integrated format. The quality of this task solution was
rated by three raters according to specified criteria (e.g.,
using integrated format in all examples). An entirely
correct solution was awarded with 12 points.
Questionnaire. Included in the questionnaire were
demographic questions as well as questions concerning
the acceptance of the learning environment and the
perceived learning results. The items were to be
answered on a rating scale from 1 to 6. For the
acceptance scale (19 items; e.g., "The content of the
program was easy to understand."), we obtained a
of .86. There were four items that
Cronbach’s
assessed the perceived learning results (e.g., "How

would you judge your current knowledge about
worked-out examples?"; Cronbach’s : .72).
Written Self-Explanations. In the learning program,
the learners in the self-explanation groups were
prompted 13 times. The written self-explanations were
analyzed using a specifically developed coding system.
The main categories were as follows:
(1) Connection between the design principles of
worked-out examples and the solved example problem
presented in the program (e.g., "The variables are
written next to the lines, therefore there will be less
mapping problems.").
(2) Linkage to the learning-goals (integrated format
or structure-emphasized example set) (e.g., "A different
surface does not automatically require a different
solution method.").
(3) Mathematical content of the solved example
problems (e.g., "In both examples I have to determine
the speed.").
(4) "Side-aspects" These remarks were correct but
did not refer to the learning goals of the program
(knowledge about integrated format or structureemphasized example set) (e.g., "… provided that the
second [example in integrated format; comment by the
authors] could be drawn clearer.").
(5) Metacognition (e.g., "I would have solved the
problem in the same way.").
The written reactions to the self-explanation
prompts were segmented with the coding categories in
mind. Often more than just one elaboration (category)
was coded in a reaction to a prompt. The coding
categories were distinct and there were no inclusions of
segments. A few utterances did not fall into any of the
categories (e.g., statements about specifics of the
learning program), so they were not taken into account.
We aggregated the codings in two respects. First, all
categories were summed up together to an overall score
of elaboration activity. The single categories occurred
relatively infrequently so that corresponding scores
would have not been reliable. Second, the elaborations
in reaction to the 13 prompts were aggregated.

Results
Pre-Analyses
In the post-test, a maximum of 34 points could be
achieved (M = 23.20; SD = 5.67). The two subtests
(selection and generation) were summarized, due to
their similar result patterns and their positive correlation
(r = .31; p < .05).
The post-test correlated significantly with the
perceived learning results (r = .41; p < .05). It did not
significantly co-vary with the acceptance scale (r = .06;
p > .10). There was a significant correlation between
the acceptance and perceived learning results (r = .51; p
< .05).

4

The amount of elaborations in the written selfexplanations was found to be an important predictor of
the learning outcomes (r = .55; p < .05). There was no
significant influence of time-on-task on learning
outcomes (r = .19; p > .10); even in the single
subgroups there were no significant relationships
between time-on-task and learning outcomes. This
pattern in the results indicates that it was not primarily
the quantitative aspect of learning (learning time), but
the qualitative aspect (elaborations) which influenced
learning outcomes. Table 1 summarizes the descriptive
results of time-on-task, written self-explanations,
acceptance, perceived usefulness, and learning
outcomes (posttest).

Effects of Self-Explanation Prompts and
Instructional Explanations
In order to determine the effects of our experimental
variations, we performed an ANCOVA controlling for
the type of student teachers. This variable significantly
influenced learning outcomes (low- and medium-track
teachers: M =22.05, SD = 5.76; high-track teacher M =
24.83, SD = 5.18; t(78) = 2.21; p < .05). As the
regression slopes of the experimental groups did not
differ significantly, the type of student teachers could
be used as a covariate.
The analysis of the posttest showed a significant
main effect for the prompting self-explanations, that
was of medium to high practical significance (F(1,75) =
8.68; p < .05; 2 =.11). There was no significant main
effect for the instructional explanations (F(1,75) = 0.37;
p > 0.1). The interaction effect reached the level of
significance and was of medium practical significance
(F(1,75) = 4.91; p < .05; 2 = .06; see Fig. 3).
Table 1: Means and standard deviations of the
experimental groups.
no self-expl.
& no
instructional
expl.

self-expl.
& no
instructional
expl

no self-expl.
&
instructional
expl.

self-expl.
&
instructional
expl

23.75 (8.82)

46.91 (10.09)

29.64 (9.14)

50.31 (16.76)

Elaborations

--

19.82 (4.51)

--

16.88 (4.51)

Acceptance

4.51 (0.39)

4.39 (0.58)

4.67 (0.65)

4.44 (0.61)

Perceived
learning
results

3.58 (0.64)

3.98 (0.89)

4.16 (0.77)

3.76 (1.17)

learning
outcomes

20.36 (5.46)

25.80 (4.44)

22.75 (5.30)

23.86 (6.29)

.
Time-on-task

As expected, the group without self-explanation
prompts and without any instructional explanations
performed
the
worst.
Offering
instructional
explanations fostered learning when self-explanations
were not prompted. However, when self-explanations
were
prompted,
supplementary
instructional
explanations impaired learning. Hence, the most

successful group received prompting of
explanations, but no instructional explanations.

self-

Figure 3: Interaction between prompting selfexplanations and instructional explanations with respect
to learning outcomes.
As stated in the introduction, the option to request
instructional explanations could reduce self-explanation
activities. Indeed, the group with self-explanation
prompts and additional instructional explanation did
elaborate less than the self-explanations only group
(about 17 versus 20 elaborations). The difference was
significant (t(32) = 1.72; p < .05; one-sided).
Did the groups differ in their time-on-task? Table 1
displays remarkable differences between groups (24
minutes versus 50 minutes). There was a main effect of
prompting self-explanations (F(1,72) = 64.91; p < .05),
with prompting increasing time-on-task. The main
effect of instructional explanations reached only a
significance level of 10% (F(1,72) = 2.93; p < .10).
There was no interaction effect (F< 1). Even though
there were differences in time-on-task between the
groups, the experimental effects could not be
interpreted as mere time-on-task effects. Firstly, typing
the self-explanations per se requires time; secondly, as
mentioned above, our analyses showed that the quality
and not the quantity of learning activities determined
the learning outcomes. However, the results indicated
that fostering learning by prompting self-explanations
requires additional learning time.
An analysis of the treatment effects on perceived
learning results (using the type of student teachers as a
covariate; there were no significant differences in the
regression slopes of the experimental groups) yielded
no main effects in the prompting of self-explanations (F
< 1) and instructional explanations (F(1,75) = 1.54; p >
.10), but a significant interaction (F(1,75) = 6.36; p <
.05). The two groups with self-explanation prompts
showed similar perceived learning results of medium
size. The group without such prompts and without
instructional explanations evaluated their learning
results as low. When only instructional explanations
were provided, the participants perceived the highest
learning results.
The various conditions did not differ in their
acceptance by the learners. There is neither a main
effect in the prompting of self-explanations (F (1,76) =

5

1.91; p > .10) nor a main effect for instructional
explanations (F < 1) nor an interaction (F < 1).

Discussion
An informal look at the posttest results shows that all
student teachers learned substantially about the design
of worked-out examples. Hence, we made a significant
step towards answering the question as to how to teach
teachers knowledge about example-based learning. The
extent of learning, however, varied significantly with
the experimental conditions. The most effective method
was to prompt self-explanations. Instructional
explanations were detrimental, at least if they were
combined with prompting self-explanations, because
they reduced self-explanation activity and thereby the
learning outcomes.
Nevertheless, it can be stated that instructional
explanations without self-explanation prompts leads to
better learning outcomes than leaving the students
completely to their own devices. This result is
consistent with the findings of Renkl (in press) on the
effectiveness of instructional explanations. In Renkl's
experiment, no self-explanation prompts were
employed.
In contrast to the objective learning outcomes, the
highest perceived learning outcomes were found in the
instructional explanations-only group. There is an
obvious contrast between the real learning outcome and
the perceived one. While fostering the learners' own
activities objectively leads to the best results, the
learners seem to prefer having the explanations
presented to them. Apparently, learners do not value
instructional measures which require their own activity.
An important consequence of the results presented
here is the evident relevance of self-explanation activity
for learning outcomes, not only when learning with
worked-out examples but also when learning with
solved example problems. For this reason the various
results concerning the self-explanation-effect are
probably transferable to content areas where no
worked-out examples can be sensibly constructed. At
the same time instructional explanations seem to be less
important than self-explanations – equivalent to the
results of worked-out example research. However,
further research has to be performed using other types
of solved example problems.
Looking at the learning processes it is important to
note that the mere amount of elaborations substantially
predicts learning outcomes. In the near future, the
thinking aloud protocols will be analyzed which will
give us further insight into the underlying learning
processes in the different experimental groups.

Acknowledgments

References
Aleven, V., & Koedinger, K. R. (2000). Limitations of
student control: Do students know when they need
help? In G. Gauthier, C. Frasson, & K. VanLehn
(Eds.), Proceedings of the 5th International
Conference on Intelligent Tutoring Systems (pp. 292303). Berlin: Springer.
Atkinson, R. K., Derry, S. J., Renkl, A., & Wortham, D.
W. (2000). Learning from examples: Instructional
principles from the worked examples research.
Review of Educational Research, 70, 181-214.
Brown, A. L., & Kane, M. J. (1988). Preschool children
can learn to transfer: Learning to learn and learning
from examples. Cognitive Psychology, 20, 493-523.
Catrambone, R. (1996). Generalizing solution
procedures learned from examples. Journal of
Experimental Psychology: Learning, Memory, and
Cognition, 22, 1020-1031.
Chi, M. T. H. (1996). Constructing self-explanations
and scaffolded explanations in tutoring. Applied
Cognitive Psychology, 10, S33-S49.
Chi, M. T. H., Bassok, M., Lewis, M. W., Reimann, P.,
& Glaser, R. (1989). Self-explanations: How students
study and use examples in learning to solve problems.
Cognitive Science, 13, 145-182.
Kulhavy, R.W. (1977). Feedback in written instruction.
Review of Educational Research, 47, 211-232.
Mwangi, W., & Sweller, J. (1998). Learning to solve
compare word problems: The effect of example
format and generating self-explanations. Cognition
and Instruction, 16, 173-199.
Quilici, J. L., & Mayer, R. E. (1996). Role of examples
in how students learn to categorize statistics word
problems. Journal of Educational Psychology, 88,
144-161.
Renkl, A. (1997). Learning from worked-out examples:
A study on individual differences. Cognitive Science,
21, 1-29.
Renkl, A. (in press). Learning from worked-out
examples: Instructional explanations supplement selfexplanations. Learning & Instruction.
Renkl, A., Stark, R., Gruber, H., & Mandl, H. (1998).
Learning from worked-out examples: The effects of
example variability and elicited self-explanations.
Contemporary Educational Psychology, 23, 90-108.
Sweller, J., van Merriënboer, J. J. G., & Paas, F. G. W.
C. (1998). Cognitive architecture and instructional
design. Educational Psychology Review, 10, 251-296.
Tarmizi, R. A., & Sweller, J. (1988). Guidance during
mathematical problem solving. Journal of
Educational Psychology, 80, 424-436.
Ward, M., & Sweller, J. (1990). Structuring effective
worked examples. Cognition and Instruction, 7, 1-39.

This study was funded by the German Research
Foundation (DFG; RE 1040/5-1).

6

