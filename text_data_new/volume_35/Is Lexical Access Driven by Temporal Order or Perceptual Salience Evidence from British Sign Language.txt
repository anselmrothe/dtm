UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Is Lexical Access Driven by Temporal Order or Perceptual Salience? Evidence from British
Sign Language

Permalink
https://escholarship.org/uc/item/45k7h1vf

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Thompson, Robin L.
Vinson, David P.
Fox, Neil
et al.

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Is Lexical Access Driven by Temporal Order or Perceptual Salience? Evidence from
British Sign Language
Robin L. Thompson (robin.thompson@ucl.ac.uk)
David P. Vinson (d.vinson@ucl.ac.uk)
Neil Fox (neil.fox@ucl.ac.uk)
Gabriella Vigliocco (g.vigliocco@ucl.ac.uk)
Deafness, Cognition and Language Research Centre, Department of Cognitive, Perceptual and Brain Sciences
University College London, 26 Bedford Way, London, WC1H 0AP, UK
Abstract
While processing spoken language, people look towards
relevant objects, and the time course of their gaze(s) can
inform us about online language processing (Tanenhaus et al,
1995). Here, we investigate lexical recognition in British Sign
Language (BSL) using a visual world paradigm, the first such
study using a signed language. Comprehension of spoken
words and signs could be driven by temporal constraints
regardless of modality (“first in, first processed”), or by
perceptual salience which differs for speech (auditorialy
perceived) and sign (visually perceived). Deaf BSL signers
looked more often to semantically related distracter pictures
than to unrelated pictures, replicating studies using
acoustically-presented speech. For phonologically related
pictures, gaze increased only for those sharing visually salient
phonological features (i.e., location and movement features).
Results are discussed in the context of language processing in
different modalities. Overall, we conclude that lexical
processing for both speech and sign is likely driven by
perceptual salience and that potential differences in processing
emerge from differences between visual and auditory systems.
Keywords: lexical access; sign
phonology, visual world; modality

language;

semantics,

Introduction
General theories of language processing have developed on
the basis of extensive data from spoken, but not signed
languages, making it impossible to tease apart those aspects
of language processing that are truly general from those
dependent on the oral-aural language modality. While
spoken language processing happens through aural
perception of sounds, sign language processing occurs
through visual perception which allows for more
simultaneous input of information; spoken languages make
use of mouth and vocal tract, while signed languages use
slower manual articulators (hands, as well as eyes, mouth
and body). An understanding of the processing differences
that arise from these differing language modalities is critical
for understanding the interaction of language processing
with other cognitive systems such as perception and action.
Here we take advantage of these physical differences in
language processing for signed languages compared to
spoken languages to investigate the nature of lexical
processing and lexical access.

For spoken languages, it is generally uncontroversial that
information is processed almost immediately as it comes in
(e.g., Rayner & Clifton, 2009). Such incremental momentby-moment language processing is likely necessary to keep
up with the incredibly fast rate of speech input (estimated to
be between 150-190 words per minute, Marslen-Wilson,
1973). However, during incremental processing listeners,
processing even a single word, are faced with many possible
alternatives that match the current acoustic-phonetic input.
Empirical evidence suggests that instead of waiting until
temporary ambiguities are resolved, partial activation of
possible words (i.e., lexical competitors) that match current
phonological information proceeds, with potential words
being eliminated across time as more information becomes
available (e.g., McClelland and Elman, 1986; Gaskell &
Marslen-Wilson, 1997).
Evidence for incremental activation of lexical competitors
during spoken language processing comes from the “visual
world” paradigm (language presented simultaneously with
related pictures; Allopena, Magnuson, & Tanenhaus, 1998;
Altman & Kamide, 2004; Huettig & Altmann, 2005; Yee &
Sedivy, 2006). For example, in Allopena et al. (1998),
subjects heard an utterance like “Pick up the beaker” while
viewing a display with four pictures including: 1) an object
matching the noun (the target; e.g. “beaker”), 2) an object
with a name beginning with the same phoneme (e.g.
“beetle”), 3) an object with a name sharing the same rhyme
(e.g., “speaker”) and, 4) an unrelated object (e.g., carriage).
The probability of fixating the target and onset competitor
were identical immediately after word onset (when the two
could not be distinguished from each other), and fixations to
these picture types were higher than fixations to the rhyme
or unrelated competitors. Immediately after reaching a
phoneme differentiating the target and onset competitor, the
probability of fixating the target rose sharply while the
probability of fixating the related competitor fell. A weaker,
but significant effect was also observed for rhyme
competitors compared to unrelated competitors, indicating
that activation is not restricted to words sharing onsets but is
continuous (see for example McClelland and Elman, 1986).
A question of interest, then, is why words that share
onsets make the strongest lexical competitors. One
possibility is that strong activation of onset competitors
compared to word rhymes is due to temporal considerations:
i.e., word onsets occur earlier in time. This view about the

1450

activation of onset competitors can be called a ‘first in, first
processed’ account. However, onsets also tend to be salient,
particularly in languages such as English (used in the
majority of visual world studies) in which stress has the
effect of lengthening the first syllable as well as adding both
intensity and pitch change: all of which serve to make the
first part of a word more salient. Evidence that stress is
important to lexical access comes from Reinisch, Jesse, and
McQueen (2010). In a visual world study they found that
participants use lexical stress information to direct eye gaze
such that upon hearing a word with initial stress (e.g.,
octopus) fixations on printed target words with first-syllable
stress (e.g., octopus) were more frequent than fixations on
differently stressed competitors (e.g., October, with stress
on the second syllable). Thus, an alternate account of the
strong activation of onset competitors observed in visual
world studies is that word onsets are the most auditorily
salient part of a word and that auditory salience drives
lexical access for processing efficiency. However, because
spoken word onsets tend to be both temporally early and
auditorily salient, it is difficult to tease apart these alternate
accounts based on previous studies.
Interestingly, unlike spoken words, for visually processed
signs there is evidence that the phonological features that
form the onset of a sign (i.e., the first features to be formed
as a sign moves through time) may not coincide with the
most visually salient features (i.e., the features that can be
seen most easily, for example, under visually noisy
conditions). Just as in spoken languages, signed languages
have sub-lexical units (phonological features) that combine
in rule-governed ways to form words/signs. Signs are made
up of phonological features from three major parameters
(handshape, movement, and location [place of articulation];
see Sandler & Lillo-Martin, 2006 for discussion, and Figure
1 for examples of signs sharing these features). In terms of
sign onsets, results from early gating studies (single frame
presentation of a sign, with subsequent presentations
increasing in length; Grosjean, 1981, Emmorey & Corina,
1990) suggest that handshape and location features are
recognized first across time. In Emmorey & Corina (1990)
subjects’ initial responses tended to share the handshape and
location of the target sign but differed in movement
features. Once the movement of the sign was identified, the
target sign also tended to be identified. The authors suggest
that lexical recognition in a signed language is a two-stage
process such that handshape and location are identified
almost from the start of the sign (i.e., form the onset of the
sign) followed by movement which coincides with sign
recognition.
In terms of sign salience, Corina & Hildebrandt (2002)
used a sign similarity judgement task and found that
subjects preferred to pair non-signs with other non-signs
sharing location and movement features more frequently
than pairing non-signs with matching handshape and
location features, or handshape and movement features,
suggesting that they are paying attention to these feature
pairings. Further support for the salience of movement and
location features is found in Corina & Knapp (2006) who

used a picture sign interference task (subjects named a
picture in ASL while trying to ignore a superimposed image
of a related distracter) and found that distracter signs sharing
both movement and location with the target sign resulted in
significant facilitation effects at all stimulus onset
asynchronies (-130, 0, 130 ms), while signs sharing
handshape and location, or handshape and movement
features did not affect picture naming.

Figure 1: Examples of phonological minimal pairs in
BSL. Top: car and robot share location and movement (up
and down) parameters, but differ in handshape. Middle:
saxophone and computer share handshape and movement
(finger wiggle) features, but differ in location. Bottom:
mouse and nose share handshape and location features, but
differ in movement (mouse, with a twisting movement and
nose with a tapping movement).
While location features are available early in sign
perception, movement features only emerge later and are
therefore crucial in teasing apart whether lexical access (at
least for signs) is driven by temporal constraints or by
perceptual salience. If temporal constraints drive lexical
access in sign, signers should pay attention to handshape
and location features which are available at the start of a
sign and ignore movement features which emerge later.
Movement features have been argued to be the most
sonorous or salient part of a sign (Perlmutter, 1992). Thus,

1451

if perceptual salience is instead key to lexical access, then
signers may pay attention to movement features.
Here we investigate lexical recognition in BSL using a
visual world paradigm and asking whether or not the nature
of access in a dynamic visual language is also incremental
and graded with alternate possible words considered
simultaneously over the time-course of processingFurther,
we consider the nature of activation of lexical competitors
(if any) and whether sign access supports a first in, first
processed pattern, or a pattern driven by visual salience.
We include two critical conditions. First, a semantic
condition will determine if a visual world paradigm can be a
successful methodology using sign language which must be
presented visually. Previously subjects have been found to
look towards semantically related competitor pictures
during spoken language visual world studies (Huettig, &
Altmann, 2005). Here we explore whether eye movements
are drawn to a semantically related object in a signed visual
world in the absence of a phonological or visual
relationship. The Visual World paradigm has never been
used with sign language stimuli and a semantic condition
(see methods) serves as our test case, under the assumption
that semantic relationships should hold regardless of
language. If the visual world methodology is successful
with BSL, we should expect subjects to look more
frequently to distracter pictures that are semantically related
to a given target sign. Secondly, we examine the nature of
sign recognition in real time using pictures that have
phonologically related signs. If temporal information is
most important, and signers process information primarily
through sequential, incremental, first-in first-processed
order, then signs sharing handshape and location features
should be particularly salient for them. Alternatively, if
perceptual salience is more relevant then signers may
instead look more frequently to distracters that share
movement and location features.

Method
Subjects
24 Deaf signers (13 women, 11 men, mean age 34.8) were
recruited from deaf communities in England and took part in
the experiment. Of these, eleven were native signers (born
to deaf signing parents), four began signing by the age of
five (early signers) and 9 learned BSL after age five. All
subjects use BSL as their preferred and primary language.

Materials
For each trial, four pictures of objects were presented
simultaneously with a centrally located video clip (see
Figure 2). In each video clip, a native BSL signer produced
the carrier phrase, “I see…”, followed by the target sign.
Subjects were asked to indicate (with button press, “yes” or
“no”) as quickly and accurately as possible whether the
target BSL sign matched one of the pictures. “Target
Present” trials (n= 79) in which a picture of the target sign
was present constituted our fillers. On critical “Target

Absent” trials (n=28), three unrelated distractor pictures
with no semantic, phonological or visual relationship to the
target sign were presented along with a related distracter
picture. Related distracter pictures had signs that were either
semantically (e.g., target: banana, distracter: strawberry,
target: zipper, distracter: button) or phonologically related to
the target. Phonologically related pictures were minimal
pairs that shared two out of three parameters (see Figure 1
for examples). Semantically related distracter pictures were
not phonologically related to the target, and phonologically
related pictures were not semantically related.

Figure 2: Example of a single trial. Areas of interest for
gaze analyses were set directly around the (250x250 pixels)
pictures and the (320x240 pixels) video.

Procedure
After giving consent to participate, subjects were presented
with video-recorded instructions in BSL (signed by N.F., a
native BSL signer) and invited to ask clarification questions.
Subjects were then fitted with a head-mounted eye-tracker
(SR Research, EyeLink II) and initial calibration was
performed (9 fixation points). Subjects were seated 50 cm
from the monitor with the tracker positioned in front of the
right eye. There were four practice trials before the
experiment began. Another calibration check was performed
after these practice items and then again after every 36 trials
(the final set had only 35 trials), at which time subjects took
a self-paced break (total 107 trials, 3 sets). Additionally,
drift correction on a single centrally located fixation point
was performed at the start of each trial. Responses were
recorded using a hand-held joypad with buttons that can be
located tactilely without the need to look at keys. The entire
experiment (with instructions and calibration) took
approximately 20 minutes to complete. In order to ensure
that all pictures were familiar to the subjects as well as to
obtain naming data, subjects named all of the pictures used
in the visual world experiment before we began.
The location of the pictures was balanced so that each
picture type (related distracter, unrelated distracter [filler])
occurred a roughly equal number of times in each location
within a given condition. Additionally, we created two sets
of stimuli such that half the subjects saw any one picture in
one location and half of the subjects saw it in a different
location. The order of trial presentation was randomized

1452

throughout. Pictures were presented simultaneously with the
sign video.

Results
First we analyzed signs produced during picture naming
to ensure that signs for target and related pictures in the
phonological conditions were indeed phonologically related
in subjects’ lexicons. Individual trials were excluded when
subjects produced a sign (for either target or related
pictures) that did not have the intended phonological
relationship (6%). Error trials, in which participants
mistakenly indicated that the target sign matched a picture
on the screen (12.4%) were also excluded from analyses of
response latencies and eye gaze. The number of trials by
condition along with average correct response latencies and
percent of correct answers across different conditions are
reported in Table 1. A one-way repeated measures ANOVA
by subjects revealed no significant differences for accuracy
between conditions: F(3,69)=1.686, p=.178. However, a
significant difference was found between conditions for
response latencies (F(3,66)1=3.202, p=.029). Post-hoc tests
revealed that responses were slower for handshapemovement trials than other conditions.
Table 1. Average correct response time (standard deviation
by subjects in brackets) and percent correct as a function of
relatedness type. Sem: related picture sharing a semantic
relationship to the target; HS-MV: related picture sharing
the handshape and movement of the target sign; LOC-MV:
sharing the location and movement of the target sign; HSLOC: sharing both the handshape and location of the target
sign.

SEM
LOC-MV
HS-MV
HS-LOC

Items
n=11
n=5
n=6
n=6

RT(SD)
2792 (462)
2730 (421)
2887 (421)
2662 (446)

%Correct
88.3
91.1
87.9
83.2

Five areas of interest within each time period were
identified: the location of the signer in the middle of the
screen (displayed as video), and one corresponding to each
of the pictures displayed (coded as target, related, unrelated
and matching in size to the actual pictures). The dependent
measure of interest was dwell time (summed gaze duration
in a given area, measured in milliseconds). Not surprisingly,
across all trial types, gaze was primarily directed to the
signer in the video (M=86.9%). This led to fewer looks
towards pictures than would be expected in a study with
auditory stimuli, so we started with a broad analysis.
Specifically, for each trial, we identified two time windows.
The early period, began at the start of the trial and ended
when the carrier phrase "I see…" finished. Because the
1

Reduced df is due to empty cells
participant/condition combinations in this analysis.

for

some

target sign was not yet produced during the early period,
gaze could not yet be informed by the target. The late period
was defined as the period from the start of the target sign
until the button was pressed. Gaze during the late period
should provide information about processing of the target
sign.
In the first set of gaze analyses across the different
pictures, we tested whether subjects looked longer at related
pictures than unrelated pictures in the late time period, once
the meaning of the target sign could be processed. We
conducted hierarchical linear regressions, treating subjects
and target signs as random effects, including picture
relatedness (considering only related vs. unrelated pictures)
and time period (early vs. late) as predictors, and dwell time
(in milliseconds) as the dependent measure. Separate
models were fit for each relatedness condition (semantic,
location-movement, handshape-movement, handshapelocation)2. Across all conditions there was a main effect of
time period indicating longer gaze overall in the late period:
semantic (95% CI [183.7, 221.1], pMCMC <.001); locationmovement (95% CI [134.0, 199.3], pMCMC <.001);
handshape-movement (95% CI [135.8, 190.0], pMCMC
<.001); and handshape-location (95% CI [121.3, 172.4],
pMCMC <.001). The main effect of picture relatedness was
not significant in any of the four conditions (all pMCMC >.67).
The crucial effect is the interaction between picture
relatedness (related vs. unrelated) and time period (early vs.
late) on dwell times, as increased looks to related pictures
should only start to occur once the target sign is being
produced. For semantic trials, the picture by time period
interaction was significant (95% CI of relative increase for
related pictures in the late period [63.5, 107.9], pMCMC
<.001) reflecting longer gaze to related compared to
unrelated pictures in the later time period (that is, after the
carrier phrase was complete and the target sign was being
produced). A significant interaction of picture by time
period was also observed in location-movement trials (95%
CI of relative increase for related pictures in the late period
[20.9, 96.6], pMCMC =.001), again reflecting longer gaze to
related than unrelated pictures in the later time period when
information about the target sign becomes available.
However, for the other two phonological conditions
(handshape-movement
and
handshape-location)
the
interaction of picture and time period did not reach
significance (both pMCMC >.3).
We next conducted a Wilcoxon signed-rank test
comparing looks to related and unrelated distracter pictures
to explore possible differences in gaze across time,
beginning at target sign onset. Cumulative fixations,
analyzed as arcsine transformations, were grouped into
2

We fit separate models for the different phonological
relatedness conditions because a combined model revealed a
significant interaction between relatedness, time period and type of
phonological relation. Using location-movement as a reference
condition, the 95% CI of the change in relatedness × time period
interaction coefficient was (11.8, 103.8) for handshape-movement
(pMCMC =.040), and (-1.7, 94.3) for handshape-location (pMCMC
=.064).

1453

100ms bins starting from 400 ms after the target onset and
continuing until 1000 ms (see Figure 3 for time course
plots). 100ms bins were used to ensure the presence of
sufficient fixations to each area of interest during each time
period for statistical analyses. Additionally, analyses began
at 400ms after the start of the target period because during
the first 300 milliseconds of the target period across all
trials, subjects fixated the sign video almost exclusively. For
semantic trials, cumulative gaze toward related pictures
differed significantly from the unrelated pictures across all
bins from 400-1000ms (range of Z from -2.20 to -3.59, all
p<.03). This same pattern of results was observed for
location-movement trials (range of Z from -2.31 to -3.63, all
p<.02). There was no difference between related and
unrelated picture gaze for handshape-location trials across
all bins (all p> .24). However, there were significantly more
looks to related pictures compared to unrelated pictures for
the handshape-movement condition, but this difference was
found only from 800ms-1000ms (p<.05, between 800100ms; all p>.2 up to 800 ms).

Figure 3: Time course of eye gaze from onset of the target
sign for 1000ms for target-absent trials across the four
conditions (from left to right: semantic, location-movement,
handshape-location, handshape-movement).

Discussion
Overall, we found both semantic and phonological effects
during online processing of BSL using a visual world
paradigm. Once information about the target sign became
available, subjects looked at related pictures longer than
unrelated pictures during the semantic condition. During the
production of the target sign, related pictures also attracted
more looks than unrelated pictures for one phonological
condition (location-movement) but not for the others
(handshape-location
and
handshape-movement).
Importantly, in the early period of each trial (i.e. before the
target sign was produced), there was no difference in gaze
patterns to the different picture types (related and unrelated)
confirming that the results are not driven by visual
characteristics of the related pictures.

In the semantic condition, subjects looked at semantically
related distracter pictures more frequently than unrelated
pictures, the first time such findings have been demonstrated
for a signed language. This result is predicted under the
view that activation of semantically related lexical
competitors should not be affected by the modality in which
a language occurs. The results from the semantic condition
reveal that despite the need for split visual attention to both
visual linguistic stimuli and pictures related to that stimuli,
it is possible to investigate sign language processing using
visual world and related paradigms.
The results from the three phonological conditions pairing
different phonological parameters produced differing
results. Phonological competitors that shared information
occurring at the onset of the sign (handshape and location
features) did not draw more looks either at the onset of the
period in which the target sign was produced (as evidenced
by our analysis of the time period from 400-1000ms after
the target sign onset) or during the entire time period from
the target sign onset until a button press decision was made
(the late time period). This finding suggests that onsets may
not be as relevant to sign language processing as has been
suggested for spoken language processing (e.g., Gaskell &
Marslen-Wilson, 1997).
Crucially, in the location-movement condition, subjects
looked significantly more toward the phonologically related
picture than unrelated pictures in the late time period. This
finding parallels the Corina and Knapp (2006) study that
found effects only for signs sharing location and movement
features. Further, for location-movement trials, looks to the
related and unrelated pictures differed significantly from
400ms after the onset of the target sign, a time comparable
with that found in spoken language studies (e.g., Allopena et
al, 1998). Finally, competitor pictures that shared handshape
and movement features with the target did not draw more
looks than unrelated pictures during the late time period.
However, there was a significant, but short-lived difference
such that looks to related and unrelated pictures differed
between 800-1000ms after the start of a target sign. Because
a difference between related and unrelated pictures was not
observed in the overall late period analyses we conclude
that, while subjects may be aware of the phonological
similarity of signs sharing handshape and movement
features, they are likely not making use of these feature
pairs during online processing. Instead, looks to related
pictures occurring between 800 and 1000 ms (relatively late
after the onset of the target sign) appears to be a post-lexical
effect in which subjects consider alternate competitor
pictures before determining that the target picture is not
shown. Crucially, the pattern of gaze in the handshapemovement condition differs from the location-movement
condition for which gaze to the related competitor picture is
early and consistent. Thus, we suggest that gaze toward
related competitors in location-movement trials is indicative
of active online lexical processing as has been found in
spoken language studies.
In the introduction, we offered two explainations for why
onsets play a special role in auditory lexical access (i.e.,

1454

either temporal constraints or salience). In terms of sign
language processing, if temporal constraints are driving
lexical access, then signers in our study should have paid
attention to handshape and location features because these
features are available at the start of a sign. Instead, the sign
language results suggest that sign onsets are not similarly
privileged to spoken word onsets, which in turn suggests
that lexical processing is not temporally driven.
Alternatively, the data support a view under which
perceptual salience drives sign access. Specifically, our data
show that signers pay attention to movement features which
are visually salient, but which occur relatively late in sign
production: only trials with related distracters that share
movement features show differences between looks to
unrelated filler pictures and related competitor pictures.
Further, the pairing of location-movement features appears
to be of greatest importance during online processing.
The nature of acoustically perceived speech in languages
such as English makes it impossible to determine why word
onsets seem to have privileged status in lexical access:
either due to temporal characteristics (perceived first) or
perceptual salience. Investigating signed languages such as
BSL allows us to clearly tease these apart, because the most
salient perceptual properties (e.g. movement) are not
available at the onset but only become available later. Thus,
increased looks towards related distracter pictures in the
location-movement condition may provide insight, not only
into the nature of online sign processing but online speech
processing as well. It is important to note that there is no a
priory reason to assume that (visual) signs and (auditory)
words will be processed similarly and therefore different
strategies might be used.
Overall, the results here reveal important characteristics of
lexical access concerning the role of lexical variables
(semantic condition) and relative time course of access to
different phonological parameters (phonological condition)
for sign language processing. More broadly, our current
understanding of language processing is intimately tied to
oral-aural modality of spoken languages. The current work
clearly shows that language processing interacts with
modality, and that the key to lexical access for both signed
and spoken languages may be perceptual saliency, instead of
temporal recency.

Acknowledgments
This work was supported by the Economic and Social
Research Council of Great Britain (Grant RES-620-286001), Deafness, Cognition and Language Research Centre
(DCAL).

References
Allopena, P. D., Magnuson, J. S., & Tanenhaus, M. K.
(1998). Tracking the time course of spoken word
recognition using eye movements: Evidence for
continuous mapping models. Journal of Memory and
Language, 38, 419- 439.
Altmann, G. T. M., & Kamide, Y. (2004). Now you see it,

now you don’t: Mediating the mapping between language
and the visual world. In J. Henderson & F. Ferreira (Eds.),
The interface of language, vision, and action. New York:
Psychology Press.
Corina, D. P., & Knapp, H. (2006). Lexical retrieval in
American Sign Language production. In L. M. Goldstein,
D. H. Whalen, & C. T. Best (eds.). Papers in laboratory
phonology. Berlin: Mouton de Gruyter, 213–240.
Emmorey, K., & Corina, D. (1990). Lexical recognition in
sign language:
Effects of phonetic structure and
morphology. Perceptual and Motor Skills, 71: 1227-1252.
Emmorey, K. (2002). Language, cognition, and the brain:
Insights from sign language research. Lawrence Erlbaum
and Associates, Mahwah, NJ.
Gaskell, M.G. & Marslen-Wilson, W.D. (1997). Integrating
form and meaning: A distributed model of speech perception. Language and Cognitive Processes, 12: 613-656.
Grosjean, F. (1981). Sign and word recognition: A first
comparison. Sign Language Studies, 32: 195-219.
Corina, D. P., & Hildebrandt, U. (2002). Psycholinguistic
investigations of phonological structure in American Sign
Language. In R. P. Meier, K. Cormier & D. Quinto-Pozos
(Eds.), Modality and Structure in Signed and Spoken
Languages. Cambridge: Cambridge University Press.
Huettig, F., Altmann, G. (2005). Word meaning and the
control of eye fixation: semantic competitor effects and
the visual world paradigm. Cognition, 96(1): B23-B32.
Marslen-Wilson,W.D. (1973). Linguistic structure and
speech shadowing at very short latencies. Nature,
244:522–523.
McClelland, J.L. & Elman, J. (1986). The TRACE model of
speech perception. Cognitive Psychology, 18:1–86.
Perlmutter, David M. (1992): Sonority and syllable structure
in American Sign Language. Linguistic Inquiry, 23, 3,
407-442.
Rayner, K. & Clifton C. Jr. (2009). Language processing in
reading and speech perception is fast and incremental:
Implications for event related potential research.
Biological Psychology, 80: 4-9.
Reinisch, E., Jesse, A., & McQueen, J. M. (2010). Early use
of phonetic information in spoken word recognition:
Lexical stress drives eye-movements immediately. Quarterly Journal of Experimental Psychology, 63(4), 772-783.
Sandler, W. & Lillo-Martin, D. (2006). Sign Language and
Linguistic Universals. Cambridge: Cambridge Univ.Press.
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K.
M., & Sedivy, J. C. (1995). Integration of visual and
linguistic information in spoken language comprehension.
Science, 268(5217), 1632-1634.
Yee, E. & Sedivy, J.C. (2006). Eye movements to pictures
reveal transient semantic activation during spoken word
recognition. Journal of Experimental Psychology:
Learning, Memory & Cognition, 32: 1-14.
Vinson, D.P., Cormier, K., Denmark, T., Schembri, A. &
Vigliocco, G. (2008). The British Sign Language (BSL)
norms for age of acquisition, familiarity and iconicity.
Behavior Research Methods, 40: 1079-87.

1455

