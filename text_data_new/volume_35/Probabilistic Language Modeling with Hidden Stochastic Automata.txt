UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Probabilistic Language Modeling with Hidden Stochastic Automata

Permalink
https://escholarship.org/uc/item/8mg8t7jm

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Author
Andrews, Mark

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Probabilistic Language Modeling with Hidden Stochastic Automata
Mark Andrews
Division of Psychology
Nottingham-Trent University
and
Division of Psychology and Language Sciences
University College London
Abstract
In this paper, we introduce a novel dynamical Bayesian
network model for probabilistic language modeling. We
refer to this as the Hidden Stochastic Automaton. This
model, while based on a generalization of the Hidden Markov model, has qualitatively greater generative
power than either the Hidden Markov model itself or
any of its existing variants and generalizations. This allows the Hidden Stochastic Automaton to be used as a
probabilistic model of natural languages in a way that is
not possible with existing dynamical Bayesian networks.
Its relevance to Cognitive Science is primarily as a computational — in the Marr (1982) sense of the term —
model of cognition, but potentially also as a model of
resource bounded cognitive processing, and as a model
of the implementation of computation in physical dynamical systems.

A probabilistic language model is a hypothetical generative model of a language, where a language is defined
most generally as a set of strings concatenated out of
a finite set of symbols. By far the most widely used
formalisms for specifying probabilistic language models are stochastic grammars, which are symbol rewriting rules with accompanying probabilities. The use of
grammars is motivated by the fact that human languages
are structurally complex, with properties that place
them between the so-called context-free and contextsensitive formal languages (see, e.g., Chomsky, 1956,
1963; Shieber, 1985), and formal grammars are computationally universal in the sense that they can generate
any recursively enumerable set (see, e.g., Hopcroft, Motwani, & Ullman, 2001).
By contrast to the case of language modeling, in
probabilistic modeling more generally, the most widely
used formalism for specifying probabilistic models is the
graphical model (see, e.g., Koller & Friedman, 2009; Jordan, 2004). Graphical models are directed or undirected
graphs whose vertices are identified with random variables and whose edges indicate conditional dependencies. The appeal of graphical models is their flexibility
to represent complex relationships between large numbers of variables, and their graph-theoretic properties
that afford general and computationally efficient algorithms for probabilistic inference, whether exactly or
approximately by, for example, Monte Carlo methods.
As a result, graphical models have effectively become
a graph-based modeling language for developing and extending probabilistic models. They have had widespread
application in fields such as bioinformatics (e.g., Fried-

man, 2004), computer vision (e.g., Oliver, Rosario, &
Pentland, 2000), machine learning (e.g., Bishop, 2006,
2013), expert systems (e.g., Lauritzen & Spiegelhalter,
1988; Pearl, 1988), information retrieval (e.g., Salakhutdinov & Hinton, 2009), and in cognitive science (see,
e.g., Chater & Oaksford, 2008; Griffiths, Chater, Kemp,
Perfors, & Tenenbaum, 2010, for overviews).
Despite their breadth of appeal, graphical models have
had a rather limited role as language models, if by language models we specifically mean generative models of
language. There are at least two important reasons for
this. On the one hand, stochastic grammars can not, in
general, be represented as graphical models. (In some
cases, notably stochastic regular grammars, the terminal and nonterminal variables of the grammar can be
identified with vertices of a directed Markovian graph.
For the super-regular grammars, however, this is not
the case and the variables of the grammar can not be
identified with the vertices of any fixed graph). On the
other hand, the most widely used graphical models for
sequential probabilisitic modeling, including the Hidden
Markov model and its extensions, are limited in their
generative power to the regular languages (i.e. the Type3 languages in the Chomsky hierarchy). In other words,
graphical models have had a relatively limited role as
language models because the most widely used probabilistic models that have sufficient generative power to
model human languages can not be represented as graphical models, and the most widely used graphical models
for sequential structures do not have sufficient generative
power to model natural languages.
There is, however, no inherent limitation to the generative power of graphical models. In this paper, we
introduce a graphical model, specifically a dynamical
Bayesian network, whose generative power is equivalent
to that of an arbitrary stochastic grammar. This model,
that we will refer to as the Hidden Stochastic Automaton, is based on a novel generalization of the widely used
Hidden Markov model. As such, it retains many of the
appealing characteristics of the Hidden Markov model
while extending its generative power.

Hidden Stochastic Automata
To introduce the Hidden Stochastic Automaton (HSA),
it is necessary to first briefly describe the Hidden Markov
model (HMM). Given a set of J independent discrete

1750

valued sequences w1 , w2 . . . wj . . . wJ , where the jth
sequence is wj = wj1 , wj2 . . . wji . . . wjnj , the generative model assumed by the HMM treats each wji as
drawn from one of K discrete probability distributions
φ1 , φ2 . . . φk . . . φK over a finite vocabulary of length V .
Which distribution is chosen for wji is determined by
the value of the unobserved variable xji ∈ {1 . . . K} that
corresponds to wji . For all j, each xj1 , xj2 . . . xji . . . xjnj
is a first-order Markov chain, with initial distribution π
and a K × K transition matrix θ. More formally, the
HMM assumes that for all j,
wji |xji , φ ∼ Categorical(wji |φxji )

1 ≤ i ≤ nj ,

xji |π ∼ Categorical(xji |π)
xji |xji−1 , θ ∼ Categorical(xji |θxji−1 )

i = 1,
1 < i ≤ nj .

For reasons that will be made clear, we will collectively
refer to generalizations of the HMM using a state-vector
as Hidden Stochastic Automata (HSA). For the purposes
of this paper, however, we will mostly concentrate on one
specific form of the HSA. For simplicity, we will also refer
to this particular case of the model as the HSA, with the
understanding that it is but one of many variants based
on the same principles.

The graphical model for the HMM is shown below.
φ

wj1

wj2

···

wji

···

wjn
j

xj1

xj2

···

xji

···

xjn
j

j ∈ {1. . .J}

π

be done by replacing the single valued xji in the HMM
by a variable sized array or vector. In other words, while
in the HMM, each state variable is xji ∈ {1 . . . K}, this
may be generalized to xji ∈ {1 . . . K}∗ . Here ∗ indicates
Kleene star, or the union of all concatenations of the elements from {1 . . . K} and {∅}. This change clearly increases the cardinality of the state space to a countably
infinite set. Importantly, however, as we will elaborate,
if the set of operations that can increase or decrease the
state-vector are limited to a finite set, and if the the conditional dependencies on this state-vector are limited to
a finite range of elements, then inference in this generalized model is almost identical in kind to inference in the
standard HMM.

θ

Figure 1: The graphical model or dynamical
Bayesian network for the Hidden Markov model.
The shaded nodes indicate the observed variables.
For simplicity, we have omitted the priors on φ, π
and θ.
Precisely because graphical models naturally afford
generalizations and extensions, the HMM has lead to
many variants. Most notably, these include the mixed
memory HMM (Saul & Jordan, 1999), the coupled HMM
(Brand, Oliver, & Pentland, 1997), the factorial HMM
(Ghahramani & Jordan, 1997), and the hierarchical
HMM (Fine, Singer, & Tishby, 1998). These extensions
are often based on introducing additional chains of latent variables with varying degrees of conditional independence between them. Despite the evident value of
these models, they do not qualitatively alter the formal
generative complexity of the underlying model. In all of
these extensions, the sequences generated are equivalent
to regular or Type-3 formal languages

From HMM’s to Hidden Stochastic
Automata
It is possible, however, to generalize the HMM in such a
way that its generative complexity is increased. This can

Just as with the HMM, the HSA is a generative
model of discrete valued sequences. It assumes that
each variable wji in the sequence of observations wj =
wj1 , wj2 . . . wji . . . wjnj is drawn from one of (H + 1) × K
discrete probability distributions φ01 , φ02 . . . φhk . . . φHK
over a length V vocabulary. Which of these (H + 1) × K
distributions is chosen is determined by the values of two
latent or unobserved state variables that correspond to
wji . On the one hand, there is a standard finite state
variable xji ∈ {1 . . . K}. On the other hand, there is
an additional state-vector variable zji ∈ {1 . . . H}∗ , with
wji being conditional on only the first element of zji , if
1 ,x ] ,
zji 6= ∅. In other words, wji is sampled from φ[zji
ji
1
where zji
indicates the value of the first element of the
state-vector zji , or else 0 when zji = ∅.
For all j, the sequence (xj1 , zji ), (xj2 , zji ) . . .
(xji , zji ) . . . (xjnj , zji ) is a first-order Markov chain
of coupled state variables. The distribution over xj1
is given by the K valued distribution π, and the
value of zj1 is deterministically set to zj1 = ∅. For
1 < i ≤ nj , both xji and zji are conditional on xji−1
and, if zji 6= ∅, the first element of zji . The value of
xji is determined by sampling from the K dimensional
1
probability distribution specified by θ[zji
, where
−1 ,xji−1 ]
θ is a (H + 1) × K × K stochastic transition matrix,
1
and zji−1
is as above. The value of zji is determined
by applying one of H + 1 different operations to
zji−1 , specifically prepending zji−1 by one symbol from
{1 . . . H} or removing the first element from zji−1 . For
example, if σ1 σ2 σ3 (with each σl ∈ {1 . . . H}) is the
value of the state-vector zji−1 , a possible sequence of

1751

Generative Power of Hidden Stochastic
Automata

operations and their effect on the state-vector could be
remove

zji−1 = σ1 σ2 σ3 −−−−→ zji = σ2 σ3 ,
prepend 3

zji = σ2 σ3 −−−−−−→ zji+1 = 3σ2 σ3 ,
prepend 2

zji+1 = 3σ2 σ3 −−−−−−→ zji+2 = 23σ2 σ3 .
Which of these H + 1 operations is applied is determined by sampling from the H + 1 dimensional prob1
ability distribution specified by Ω[zji−1
, xji−1 ], where Ω
is a (H + 1) × K × (H + 1) stochastic transition matrix.
More formally, the probabilistic generative model defined by this HSA is, for i ≤ i ≤ nj ,
1 ,x ] ),
wji |xji , zji , φ ∼ Categorical(wji |φ[zji
ji

and for i = 1,
xji |π ∼ Categorical(xji |π),

zji = ∅,

and for 1 < i ≤ nj ,
1
xji |xji−1 , zji−1 , θ ∼ Categorical(xji |θ[zji
),
−1 ,xji−1 ]

zji |uji−1 , zji−1 = O[uji−1 ] (zji−1 ),
1
uji−1 |xji−1 , zji−1 , Ω ∼ Categorical(uji−1 |Ω[zji
).
−1 ,xji−1 ]

Here, we use the auxilary variable uji to refer to the
operation applied to zji , and O is the set of (H + 1)
functions that map zji to zji+1 when these operations are
applied. In other words, this makes clear that the value
of zji+1 is a deterministic function of zji when the value of
uji is known, but this value is stochastically conditional
on xji and zji . In terms of the original variables, the
graphical model for the HSA is as follows:
φ

wj1

wj2

···

wji

···

wjn
j

xj1

xj2

···

xji

···

xjn
j

zj1

zj2

···

zji

···

zjnj

The generative power of the HSA model (as shown in
Figure 2) relative to that of the standard HMM (as
shown in Figure 1) arises from the fact that the statespace of the state-vector zji , namely {1 . . . H}∗ , is a
countably infinite set yet the conditional relationships to
and from zji are finitely specifiable. The consequences
of this can be better appreciated by reference to discrete
automata of the kind that form the foundations of theoretical computer science (see, e.g., Hopcroft et al., 2001).
As we have described it, the state-vector zji is identical to a pushdown stack with a symbol set {1 . . . H}.
Prepending an element to the state-vector is equivalent
to a push operation, while removing the first element is
a pop operation. Assuming known values for Ω, which
operation is applied to zji is dependent only on the value
of the finite state variable xji−1 and the first element or
head of zji−1 . Likewise, assuming known values for θ,
the value taken by xji is also dependent only on xji−1
and the head of zji−1 . In other words, the HSA model
described above is equivalent to a stochastic generative
version of a pushdown stack automaton.
If we allow a greater variety of operations on the statevector than just prepending or removing symbols from
the left, the computational power of the HSA can be
beyond that of a generative pushdown stack automaton.
For example, if
σ1 σ2 σ̇3 σ4 σ5 σ6
is the value of the state-vector, we may treat an arbitrary element — in this cases σ3 — as its head. If we
allow for the appending of new elements to the left or
the right of the head, or for the deleting of the element
at the head, followed by the moving of the head pointer
to the left or right, then this state-vector is equivalent
to a two-way memory tape. As before, assuming known
values for Ω, which of the operations is applied to the
state-vector zji is again dependent only on the value of
the finite state variable xji−1 and the head of zji−1 . Likewise, as before, assuming known values for θ, the value
taken by xji is also dependent only on xji−1 and the head
element of zji−1 . As such, with these changes the HSA is
now equivalent to a stochastic generative version of the
Turing machine.

Inference
j ∈ {1. . .J}

π

Ω

θ

Figure 2: The graphical model or dynamical Bayesian
network for the Hidden Stochastic Automaton. As
with Figure 1, shaded nodes indicate observed variables and we have omitted the priors on φ, π, θ and
Ω.

As is clear from Figure 2, only the variables w = {wj1 . . .
wji . . . wjnj }Jj=1 are observed. In general, therefore, the
problem of inference in the HSA is the problem of inferring the joint posterior
P(θ, φ, Ω, π, x, z|w, α, β, γ, ν),
where x and z are the set of finite state and state-vectors
variables, and α, β, γ, ν are the Dirichlet priors for θ, φ,
Ω, π, respectively.

1752

The procedure for inference that we will follow is to
use a collapsed Gibbs sampler to draw samples from the
posterior
P(x, z|w, α, β, γ, ν),
that integrates over the values of θ, φ, Ω, π. This Gibbs
sampler is identical in nature to the collapsed sampler
used in Andrews and Vigliocco (2010) for the case of a
hierarchical mixture of Hidden Markov models.
For all j ∈ {1 . . . J} and i ∈ {1 . . . nj }, the Gibbs sampler iteratively draws samples from the posterior over
xji and over zji , conditioned on sampled values for all
remaining variables.
The posterior distribution over xji , conditioned on
known values for all the other variables is1

takes the value of 1 is k− = k = k+ and takes the value
of zero otherwise. Likewise, δk− ,k takes the value of 1
when k− = k, and takes the value of 0 otherwise. The
terms a, b and c are the sums of α, β, γ, respectively.
For the case of the posterior distribution of the statevector, it is sufficient to infer the distribution over operations applied to it. As mentioned, the value of the
state-vector zji is deterministic function of zji−1 when
the operation uji−1 is known. The posterior distribution
over uji is given by
P(uji |wji , zji , x¬ji , w¬ji , z¬ji , α, β, γ) ∝
"Z
×

#

P(xji |wji , zji , x¬ji , w¬ji , z¬ji , α, β, γ) ∝
Z
P(wji |xji , zji , φ)P(φ|w¬ji , x¬ji , z¬ji , β)dφ ×
Z
P(zji+1 |xji , zji , Ω)P(Ω|x¬ji , z¬ji , γ)dΩ ×
Z
P(xji+1 |xji , zji , θ)P(xji |xji−1 , zji−1 , θ)P(θ|x¬ji , z¬ji , α)dθ.

P(φ|w¬ji
~ , x¬ji
~ , z¬ji
~ , β)dφ
"Z
×

P(θ|x¬ji
~ , z¬ji
~ , β)dθ
× P(zji+1 . . . zjnj |uji , zji )
Z
× P(uji |xji , zji , Ω)P(Ω|x¬ji
~ , z¬ji
~ , γ)dΩ,

P(xji = k|wji , zji , x¬ji , w¬ji , z¬ji , α, β, γ)

×

¬ji
Shkv
+ βv
¬ji
Shk·
+b

×

Q¬ji
hkq + γq
Q¬ji
hk· + c

¬ji
(Rhkk
+ δk− ,k,k+ + αk+ )(Rh¬ji
+ αk )
+
− k− k
¬ji
Rhk·
+ δk− ,k + a

.

Here, we are assuming that the value of the observed
variable at ji is v, the value of head of the state-vector
at ji is h, its value at ji−1 is h− , the value of the finite
state variable at ji−1 is k− and its value at ji+1 is k+ .
The S, Q and R are rank-3 arrays of frequencies, with
the superscript of ¬ji indicating that they are based on
¬ji
excluding variables at ji. As such, Shkv
is the number
of times the observed variable has a value of v when
the finite state variables has the value k and the head
(e.g., the first) element of state-vector takes the value of
¬ji
h ∈ {0 . . . H}, Qhkq
is the number of times that statevector operation q occurs whenever the head element
of the state-vector takes the value of k and the finite
¬ji
state variable takes the value of k, and Rhkk
gives the
+
number of times the finite state variable takes the value
k+ whenever its value at the previous index is k and
the value of the head of the state-vector at the previous
index is h. The dot in place of the third index, e.g.,
¬ji
Shk·
, indicates a sum over the index. The term δk− ,k,k+
1

P(xji+1 . . . xjnj |xji . . . xjnj−1 , zji . . . zjnj−1 , θ)
#

This leads to the following closed form:

∝

P(wji+1 . . . wjnj |xji+1 . . . xjnj , zji+1 . . . zjnj , φ)

We will provide the conditional distributions for values
of xji and zji where 1 < i < nj . The distributions for the
cases where i = 1 and i = nj require minor modifications,
which we will omit here in the interests in space.

where we see that because a change to the operation uji
deterministically changes the values of zji+1 . . . zjnj , the
likelihood terms for the uji variable include the variables
wji+1 . . . wji+1 and xji+1 . . . xji+1 2 . In the above, the nota~ e.g., in x ~ , indicates the exclusion of variables
tion ¬ji,
¬ji
ji . . . jnj . This distribution leads to the closed form
P(uji = q|wji , zji , x¬ji , w¬ji , z¬ji , α, β, γ) ∝
q
Q
QShkv
~
−1 ¬ji
q
Shkv + βv + s
s=0
{hkv : Shkv
>0}
×
q
Q
QShk·
~
−1 ¬ji
q
Shk· + b + s
s=0
>0}
{hk : Shk·
Q
QQqhkq −1 ¬ji
~
q
Qhkq + γq + s
s=0
{hkq : Qhkq >0}
×
Q
QQqhk· −1 ¬ji
~
q
Q
+
c
+
s
s=0
{hk : Qhk· >0}
hk·
q
Q
QRhkl
~
−1 ¬ji
q
Rhkl + αl + s
s=0
{hkl : Rhkl
>0}
.
q
Q
QRhk·
~
−1 ¬ji
q
R
+
a
+
s
s=0
{hk : R
>0}
hk·
hk·

~

~

~

¬ji
ji
¬ji
Here, Shkv
, Q¬
hkq and Rhkl have the same meaning as
~

~

~

¬ji
ji
¬ji
Shkv
, Q¬
hkq and Rhkl with the difference being that the
frequencies are calculated excluding variables at the inq
dices ij . . . jnj . By contrast, the arrays Shkv
, Qqhkq and
q
Rhkl
are the frequencies of the co-occurrences the values
2
In graphical model terms, the variables wji+1 . . . wji+1 ,
xji+1 . . . xji+1 and zji+1 . . . zjnj are all children of uji .

1753

Figure 3: Strings generated by the probabilistic context-free grammar S → 0S1 (p = 0.66), S → 01 (p = 0.34) were
used as observed data in a HSA. Shown above are samples of the binary strings generated by the HSA model on
the basis of estimates of the parameters φ, θ, Ω and π after 3, 5, 10, 20, 50 and 100 iterations of the Gibbs sampler.
The dark shade codes the value of 1. It is evident that by over 50 iterations, the HSA has inferred the correct
generative model of the probabilistic language. By 100 iterations, it is only generating strings from the language
L = {0n 1n : n ≥ 0}.
of the variables after operation q is applied to the statevector zji+1 and the changes to the subsequent statevectors are deterministically applied.

Demonstration
We demonstrate inference of a language from data by
using the textbook example of a simple context-free language, namely L = {0n 1n : n ≥ 0}. We can generate
strings from a probabilistic version of this language using the probabilistic context-free grammar
S → 0S1,

p = 0.66,

→ 01,

p = 0.34.

We sample J = 25 strings from this language and use
them as the data w = w1 , w2 . . . wj . . . wJ for a HSA
model of the kind described.
Using the collapsed Gibbs sampler, we can sample
from the posterior over the finite state and state-vector
trajectories conditional on w. From these, we may then
draw sample estimates of φ, θ, Ω and π. Shown in Figure 3 are strings generated by the HSA model with parameters φ, θ, Ω and π as estimated after, from left to
right, 3, 5, 10, 20, 50 and 100 iterations of the Gibbs
sampler.

Relevance for Cognitive Science
Our initial motivation for the HSA model was put in
terms of the computational advantages of graphical models as formalisms for probabilistic modeling. Graphical

models, we have argued, have effectively become a graphbased modeling language for developing and extending
probabilistic models. They have had a remarkable influence on the progress of probabilistic modeling in a
wide variety of fields, including cognitive science. It is
notable, therefore, that graphical models have had a relatively limited role in the probabilistic modeling of natural language. The obvious reason for this is due to the
structurally complex nature of natural languages. While
this structure is modeled well by probabilistic grammars,
grammars can not, in general, be represented by graphical models. By contrast, the graphical models most
widely used for modeling sequential data do not have
the structural complexity necessary for modeling natural language.
We have introduced the HSA as a dynamical Bayesian
network model that is capable of modeling structurally
complex sequences. Its principal relevance to cognitive
science is therefore as a computational model of cognition, where by computational model we specifically mean
the Marr (1982) sense of the term: a model of the abstract nature of problem being faced and of its rational solution. However, the HSA model is potentially
as relevant as a model of the resource limited practice,
or possibly even the physical implementation, of cognition. For example, the HSA is an incremental statespace model, where inference is naturally modeled by
the kind of sequential Monte Carlo methods, particularly particle filters, that have been advocated by, for

1754

example, Griffiths, Vul, and Sanborn (2012); Sanborn,
Griffiths, and Navarro (2010); Levy, Reali, and Griffiths
(2009) as models of memory and time constrained approximations to rational computational models. On the
other hand, from the point of view of physical implementation, the state-vector of the HSA can be represented
naturally by a real-valued variable. If the state-vector is
σ1 σ2 . . . σi . . P
. σn , this can be represented exactly by the
n
real number i=1 σi (H+1)−i and the operations applied
to the state vector correspond to real-valued functions.
For example, if the state-vector is binary, prepending a
σ ∈ {0, 1} to σ1 σ2 . . . σi . . . σn is identical to multiplying
P
n
−i
by 12 and adding σ2 . By treating the finite
i=1 σi 2
state variable as another real number, this allows us to
represent the HSA exactly as a stochastic nonlinear dynamical system that is directly comparable to a recurrent neural network (see, e.g., Tabor, 2000, for related
discussion).

References
Andrews, M., & Vigliocco, G. (2010). The Hidden
Markov Topics Model: A Probabilistic Model of
Semantic Representation. Topics in Cognitive Science, 2 , 101-113.
Bishop, C. M. (2006). Pattern Recognition and Machine
Learning. New York, NY: Springer.
Bishop, C. M. (2013, Feb 13). Model-Based Machine
Learning. Philosophical Transactions of the Royal
Society A - Mathematical Physical and Engineering Sciences, 371 (1984).
Brand, M., Oliver, N., & Pentland, A. (1997). Coupled Hidden Markov Models for Complex Action
Recognition. In 1997 IEEE Computer Society Conference On Computer Vision And Pattern Recognition, Proceedings (p. 994-999).
Chater, N., & Oaksford, M. (2008). The Probabilistic
Mind. Oxford, UK: Oxford University Press.
Chomsky, N. (1956). Three models for the description
of language. Institute of Radio Engineers Transactions on Information Theory, 2 , 113-124.
Chomsky, N. (1963). Formal properties of grammars. In
R. D. Luce, R. R. Bush, & E. Galanter (Eds.),
Handbook of mathematical psychology (Vol. 2,
p. 323-418). New York and London: John Wiley
and Sons, Inc.
Fine, S., Singer, Y., & Tishby, N. (1998). The Hierarchical Hidden Markov Model: Analysis and Applications. Machine Learning, 32 (1), 41–62.
Friedman, N. (2004, Feb 6). Inferring Cellular Networks using Probabilistic Graphical Models. Science, 303 (5659), 799-805.
Ghahramani, Z., & Jordan, M. (1997). Factorial Hidden
Markov Models. Machine Learning, 29 , 245-273.
Griffiths, T. L., Chater, N., Kemp, C., Perfors, A., &
Tenenbaum, J. B. (2010, Aug). Probabilistic Mod-

els of Cognition: Exploring Representations and
Inductive Biases. Trends in Cognitive Sciences,
14 (8), 357-364.
Griffiths, T. L., Vul, E., & Sanborn, A. N. (2012).
Bridging Levels of Analysis for Probabilistic Models of Cognition. Current Directions in Psychological Science, 21 (4), 26-268.
Hopcroft, J., Motwani, R., & Ullman, J. (2001). Introduction to automata theory, languages and computation (2nd ed.). Addison Wesley.
Jordan, M. I. (2004). Graphical models. Statistical
Science, 19 (1), 140-155.
Koller, D., & Friedman, N. (2009). Probabilistic graphical models: Principles and techniques. Cambridge,
MA: MIT Press.
Lauritzen, S., & Spiegelhalter, D. (1988). Local Computations With Probabilities On Graphical Structures And Their Application To Expert Systems.
Journal Of The Royal Statistical Society Series BMethodological , 50 (2), 157-224.
Levy, R., Reali, F., & Griffiths, T. L. (2009). Modeling
the Effects of Memory on Human Online Sentence
Processing with Particle Filters. In Advances in
Neural Information Processing Systems (Vol. 21,
p. 937-944).
Marr, D. (1982). Vision. New York, NY: W. H. Freeman
& Company.
Oliver, N., Rosario, B., & Pentland, A. (2000, Aug).
A Bayesian Computer Vision System for Modeling Human Interactions. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 22 (8),
831-843.
Pearl, J. (1988). Probabilistic reasoning in intelligent
systems: Networks of plausible inference. Morgan
Kaufmann.
Salakhutdinov, R., & Hinton, G. (2009, Jul). Semantic Hashing. International Journal of Approximate
Reasoning, 50 (7), 969-978.
Sanborn, A. N., Griffiths, T. L., & Navarro, D. J. (2010).
Rational approximations to rational models: Alternative algorithms for category learning. Psychological Review , 117 , 1144-1167.
Saul, L., & Jordan, M. (1999, Oct). Mixed Memory
Markov Models: Decomposing Complex Stochastic
Processes as Mixtures of Simpler Ones. Machine
Learning, 37 (1), 75-87.
Shieber, S. M. (1985). Evidence Against the ContextFreeness of Natural-Language. Linguistics And
Philosophy, 8 (3), 333-343.
Tabor, W. (2000). Fractal encoding of context-free grammars in connectionist networks. Expert Systems,
17 (1), 41-56.

1755

