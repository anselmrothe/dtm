UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Social Affordances and Using Them for Planning

Permalink
https://escholarship.org/uc/item/9cj412wg

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Uyanik, Kadir F.
Calskan, Yigit
Bozcuoglu, Asil Kaan
et al.

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Learning Social Affordances and Using Them for Planning
Kadir Firat Uyanik, Yigit Caliskan, Asil Kaan Bozcuoglu, Onur Yuruten, Sinan Kalkan, Erol Sahin
{kadir, yigit, asil, oyuruten, skalkan, erol}@ceng.metu.edu.tr
KOVAN Research Lab, Dept. of Computer Eng., METU, Ankara, Turkey
Abstract
This study extends the learning and use of affordances on
robots on two fronts. First, we use the very same affordance
learning framework that was used for learning the affordances
of inanimate things to learn social affordances, that is affordances whose existence requires the presence of humans. Second, we use the learned affordances for making multi-step
plans. Specifically, an iCub humanoid platform is equipped
with a perceptual system to sense objects placed on a table, as
well as the presence and state of humans in the environment,
and a behavioral repertoire that consisted of simple object manipulations as well as voice behaviors that are uttered simple
verbs. After interacting with objects and humans, the robot
learns a set of affordances with which it can make multi-step
plans towards achieving a demonstrated goal.

Introduction
Motor competences of robots operating in our environments,
is likely to remain inferior to ours on most fronts in the near
future. In order to complete tasks that require competences
beyond their abilities, the robots will need need to interact
with humans in the environment towards compensating these
deficiencies. The inspiration for our study comes from babies and small children who can compensate the lack of their
physical competences through the use of adults via social interaction . For instance, for a child, the reachability of a candy
on a high shelf becomes possible only in the presence of an
adult, as long as he can “manipulate” him properly using his
social behaviors.
In this paper, we extend an affordance framework proposed
for robots towards learning interactions with inanimate objects, to learning interactions with humans. The notion of affordances, proposed by Gibson (Gibson, 1986), emphasized
the interaction between the agent and the environment, as opposed to the agent or the environment only, and provided a
unifying frameworks for the study.

Contribution
This study extends the learning and use of affordances on
robots on two fronts. First, we use the very same affordance
learning framework that was used for learning the affordances
of inanimate things to learn social affordances1 (viz. affordances whose existence requires the presence of humans).
Second, we use learned affordances to make multi-step plans.
In our earlier studies, we had proposed a framework that
allowed the robot to learn affordances such as traversability of an environment (Ugur & Şahin, 2010) or graspability
1 We would like to note that the term, social affordances has been
used in different contexts, e.g., for the possibilities emerging from
social networks (Wellman et al., 2003), or the affordances of an environment and properties of people that facilitate social interaction
in a group of people (Kreijns & Kirschner, 2001).

(Ugur, Şahin, & Oztop, 2009), liftability of objects (Dag, Atil,
Kalkan, & Sahin, 2010) and showed how one can make multistep plans using the learned affordances.
In this paper, we argue that robots can use the very same
framework to learn what a human may afford. Moreover, we
enhance our prior study on multi-step planning (Ugur et al.,
2009) via a new form of prototypes for effect representation.
Specifically, we equipped the humanoid robot iCub with
a perceptual system to sense tabletop objects, as well as the
presence and state of humans in the environment, and a behavioral repertoire that consisted of simple object manipulations and voice behaviors that uttered simple verbs. After
interacting with objects and humans, we show that the robot
is able to learn a set of affordances with which it can make
multi-step plans towards achieving a demonstrated goal.

Related Work
The notion of affordances provides a perspective that puts the
focus on the interaction (rather than the agent or the environment) and was formalized as a relation a between an entity
or environment e, a behavior b and the effect f of behavior b
on e (Şahin, Çakmak, Doğar, Uğur, & Üçoluk, 2007; Montesano, Lopes, Bernardino, & Santos-Victor, 2008):
a = (e, b, f ),

(1)

For example, a behavior bli f t that produces an effect fli f ted on
an object ecup forms an affordance relation (ecup , bli f t , fli f ted ).
Note that an agent would require more of such relations on
different objects and behaviors to learn more general affordance relations.
The studies on learning and use of affordances have
mostly been confined to inanimate things, such as objects
(Fitzpatrick, Metta, Natale, Rao, & Sandini, 2003; Detry,
Kraft, Buch, Kruger, & Piater, 2010; Atil, Dag, Kalkan,
& Şahin, 2010; Dag et al., 2010) and tools (Sinapov &
Stoytchev, 2008; Stoytchev, 2008) that the robot can interact
with. In these studies, the robot interacts with the environment through a set of actions, and learns to perceptually detect and actualize them. Moreover, with the exception of few
studies (Ugur et al., 2009; Williams & Breazeal, 2012), the
robots were only able to perceive the immediate affordances
which can be actualized with a single-step action plan.
Formalizations, such as 1, are proved to be practical with
successful applications in navigation (Ugur & Şahin, 2010),
and manipulation (Fitzpatrick et al., 2003; Detry et al., 2010;
Ugur et al., 2009; Ugur, Oztop, & Şahin, 2011), conceptualization and language (Atil et al., 2010; Dag et al., 2010;
Yürüten et al., 2012), and vision (Dag et al., 2010). However,

3604

in these studies, the environment is limited to objects only, excluding the possible diversities or use-cases that might arise
due to the existence of humans in addition to the objects.
Human-assistance has been incorporated in (Montesano et
al., 2008; Dag et al., 2010; Ugur, Oztop, & Şahin, 2011) using the same affordance formalization (1) to learn object affordances by imitation and emulation. However, the role of
the human is limited to teaching affordances as part of the
training phase, and he is out of the loop during execution of
actions for possible assistance in creating a certain effect in
the environment to extend robot’s motor capacities.
Robot’s motor capacities are extended by learning the affordances of tools in (Sinapov & Stoytchev, 2008; Stoytchev,
2008). However, these studies are focused on learning affordances of tools while the objects are kept fixed, hence the
affordances of objects themselves couldn’t be captured.
In most of the HRI or social robotics studies, the robots are
intended to collaborate with their human partners and they
are “active learners” that learn from their partners the correct way to execute and sequence actions for achieving a goal
(see, e.g., (Fong, Thorpe, & Baur, 2003; Breazeal, 2004; Weber, 2008; Cakmak, DePalma, Arriaga, & Thomaz, 2010) for
a review). This way, one can teach a robot to learn complicated sequences of actions (e.g., dancing with a human partner (Kosuge & Hirata, 2004)) using several mechanisms like
scaffolding (Ugur, Celikkanat, Sahin, Nagai, & Oztop, 2011;
Saunders, Nehaniv, Dautenhahn, & Alissandrakis, 2007) or
demonstration (Pastor, Hoffmann, Asfour, & Schaal, 2009;
Argall, Chernova, Veloso, & Browning, 2009; Akgun, Cakmak, Jiang, & Thomaz, 2012). Similarly, affordances are also
utilized in planning (Ugur, Oztop, & Şahin, 2011) over action
possibilities, but human is not a part of the plan. However, in
(Williams & Breazeal, 2012), humans are important part of
the plan, yet their participation is limited with the experiment
scenario, and participants are priorly acknowledged about the
type of assistance they are going to provide to the robot.
For a similar goal, affordances (called “interpersonal affordances”) that emerge from coordinated joint actions of two
robots are investigated (Richardson, Marsh, & Baron, 2007;
Marsh, Richardson, & Schmidt, 2009); e.g., two robots learning the interpersonal affordance of lifting a table which, otherwise, is liftable by neither of them. Our approach differs
from these studies in the sense that the human is seen as part
of the environment (with no special status) and uses the very
same framework to learn social affordances as the physical
affordances of objects.

Research Platform
Perception and Environment Representation
iCub perceives its environment through two Kinect cameras
(K1 and K2). K1 is used to extract table and tabletop objects.
K2 –accompanied with a motion capture system (Visualeyez
II VZ4000)- is used to detect human’s body posture and gaze
direction. For gaze direction detection, participants are provided with a hat with active LEDs on top. Overall interaction

(a)

(b)

Figure 1: Visualization of the interaction environment. (a)
Robot is on the left, and the human is on the right. (b) Object
related features. From top left to bottom right: raw RGBD
point cloud of an object on a table, table and extracted tabletop object with its oriented bounding box and id, surface normals, min curvatures, max curvatures, shape indices. [Figure
best viewed in color]

environment is represented as a feature vector containing the
following features:
Surface features are surface normals (azimuth and zenith),
principal curvatures (min and max), and shape indices. They
are represented as a 20-bin histogram in addition to the min,
max, mean, standard deviation and variance information.
Spatial features are bounding box pose (x, y, z, theta),
bounding box dimensions (x, y, z), and object presence.
Social features are human torso pose (x, y, z, roll, pitch,
yaw), human gaze direction (roll, pitch, yaw), and human
presence, all with respect to robot’s own coordinate system
shown as coordinate axis in Fig. 1a.

Figure 2: The interaction objects. From left to right; balls,
boxes, cylinders, mugs, and irregular objects.

Behaviors and Effect Representation
The robot is equipped with six behaviors (push-left, pushright, push-forward, pull, top-grasp, side-grasp) and some
voice behaviors (“pass me”, “hello”, “come”, “sit down”,
“stand up”, “bye”, “push right”, “push left”, “take”).
Effects –in their raw form- are computed as the difference
between the final and the initial state of the environment (viz.
difference between the feature vectors representing the environment before and after the behavior performance).
Effects are supervisedly matched to an effect category chosen from a set of effects such as grasped, knocked, no-

3605

change2 , disappeared, moved right, moved left, moved forward, pulled, sat down, stood up, got attention, got closer.
Effect categories are compactly represented as a vector
of “0”, “+”, “−”, and “∗” to represent changes in certain
feature value as unchanged (mean close to zero, small variance); consistently increased (positive mean, small variance),
consistently decreased (negative mean, small variance); and
inconsistently changed (large variance), respectively. This
prototype-based effect representation is claimed to correspond to verb concepts in our earlier studies (Atil et al., 2010).
For extracting the prototypes for each effect cluster, we analyze the mean and variance values for each element of the
features in the cluster. Specifically, we apply unsupervised
clustering (RGNG, (Qin & Suganthan, 2004)) on the meanvariance space. RGNG finds four clusters naturally formed.
From the obtained effect consistencies, we determine the prototype of each effect cluster.

Methodology
Data Collection
We used 35 objects (Fig. 2) that are chosen to be in different colors, and shape complexities (from primitive cubes,
spheres, cylinders to mugs, wine glasses, coke cans etc.), easily identified as “cylinder”, “ball”, “cup”, “box”, while some
of them had irregular shapes to show generalization ability of
the system.
We had iCub interact with objects and with humans by using all of the behaviors precoded in its behavior repertoire.
In order to collect social interaction data, we have worked
with 10 participants of different genders (4 female, 6 male),
ages (20-40) and professions (4 undergrad, 2 grad students,
4 researchers with non-CS background). They were asked
to respond naturally to a random sequence of voice behaviors
enacted by iCub. Some of the voice behaviors were accompanied by simple movements (nodding head, waving arm, etc.).

Affordance Learning
We collected 413 triplets of (e, b, f ) (Eqn. 1) for object interactions and 150 triplets for human interactions, and used
them to train a Support Vector Machine (SVM) classifier for
each behavior to predict the effect label given an entity. During training, we normalized the feature vectors with respect
to the range of values each feature can take.

Planning with Forward Chaining
Since we trained SVMs for predicting the effect of each behavior on an object, iCub can do forward chaining to find the
set of behaviors leading to a goal state. Since the effect labels
are represented by effect prototypes, the similarity between
the goal state (which is an effect instance) and the predicted
2 The no-change label denotes that the applied behavior did not
produce any notable change on the object. For example, when iCub
asks a participant to stand up who was already standing, the participant would not change his position. This yields negligibly small
changes in the feature vector.

effect prototype is needed and we use the Mahalanobis distance, which is calculated by taking the mean µEi of first effect cluster Ei (if the first Ei is an effect instance, we take the
effect instance as µEi ) and using the second effect cluster’s E j
mean µE j and variance σE j :

d(Ei , E j ) =

r

+,−,0

µEi − f proto,E j

T



+,−,0
S−1
µ
−
f
E
i
j
proto,E j

(2)

where S j is the covariance matrix of the second effect cluster
E j . In computing the Mahalanobis distance, the features
marked inconsistent in the prototype are disregarded (denoted
+,−,0
by f proto,E
for the effect prototype f proto,Ei of an effect cluster
i
Ei ), as those correspond to an unpredictable/inconsistent
change in the feature elements.
Finding the effects Planning toward achieving the goal is
found using a breadth-first tree search. Starting with the initial state, we construct a tree such that it contains all the possible effect sequences with length n (empirically chosen as 3).
The plan is made as the goal is matched with the predicted
states after applying a sequence of behaviors.
In the first step of future state
calculation (Fig. 3), the current
state of the object is fed to the
trained SVM for each behavior.
Then, the predicted effect’s prototype is determined. The mean
value of this effect is added to the
initial features, with the exception of the inconsistent features,
and the predicted future state is
found.
After this application,
the predicted future state can be
compared with other states; but the
inconsistent features of the applied
effect (denoted as black columns
in predicted after-state) is excluded
from the comparison calculations.

Initial State

Predict-bi
SVM for bi

+

Predicted
after-state

Figure 3: Future state
prediction.

Application of effects Given the object, we can obtain from
the trained SVMs the behavior that can achieve a desired effect with the highest probability. Thus, we obtain the behaviors required for each step in the planned effect sequence,
forming a sequence of behaviors. If the obtained effect at any
step in the behavior sequence does not match with the expectation, then the planning restarts. Fig. 3 and 4 respectively
exemplify how a sequence of effect prototypes for reaching a
desired effect is sought and how a behavior that produces an
effect on an object is found. The system executes the planned
behavior sequence. If, at any step, the predicted effect is not
achieved (including overshoots or undershoots), the planning
restarts from the current object state.

3606

the participants would leave the scene. However, participants
mostly kept their positions and responded by waving back.

Initial
(t = 0)
Predict-b0

Predict-b1

Predict-b2

Object
t=1
b = {b1}

Object
t=1
b = {b2}

1
0.5

E1

Predicted
States
After
Step 1

0
−0.5
−1
1

Feature Index

0.5

E2

Object
t=1
b = {b0}

0
−0.5
−1
1

Predict-b1

Predict-b2

Object
t=2
b = {b1,b0}

Object
t=2
b = {b1,b1}

Object
t=2
b = {b1,b2}

Feature Index

0.5

E3

Predict-b0

0
−0.5
−1

Predicted
States
After
Step 2

1

Feature Index

E4

0.5
0
−0.5
−1
1

Feature Index

E5

0.5
0
−0.5

F3

F1

F2

−1

Figure 4: A simple depiction of how the planning is performed using the combinations of effects in the repertoire. At
each step, the prediction block described in Fig. 3 is applied
for each behavior. Once a future state close enough to goal
state is obtained, the search is terminated.

Figure 5: Some of the results obtained from unsupervised
clustering of feature mean and variances. Vertical axis represents some of the effects (E1: moved-right, E2: moved-left,
E3: moved-forward, E4: pulled and E5: disappeared) and
the horizontal axis represents some of the features detected
as consistently increasing (red upwards arrow) or decreasing
(blue downwards arrow) in the given effects (F1: position-x,
F2: position-y, F3: human presence))

Results
Social Affordance Learning
Fig. 5 shows some of the effect prototypes that lead us to
interesting observations. In the first place, some effects can
apparently be produced both by social and non-social behaviors. An obvious example is “say push to your left” and pushright (causing the moved right effect most of the time). Furthermore, we observe that in some cases, social behaviors can
be a better option for goal emulation. For instance, when the
object is far enough from the robot, the predicted effect for
pull behavior is no change; whose effect prototype has only
*’s and 0’s (features with inconsistent change and negligible
change), whereas the predicted effect for “pass me” behavior is pulled, the effect whose prototype denotes consistent
decrease in object’s distance to the robot (x-position). In emulating a goal to pull this object towards itself, Eq. 2 yields
that pulled effect brings the object closer to the goal, hence
iCub chooses to ask a human to pass the object.
The effects got attention and got closer turned out to be
ambiguous effect labels - their corresponding clusters did not
have any consistently increasing or decreasing features. This
might also be related with our feature set. Similar results were
observed for the effects clustered as sat down and stood up,
although they were unambiguously interpreted by the participants. The amount of standing and sitting of our experiment
participants has had a high variance. The participants had
two major interpretations for the “pass me” behavior: they
either (i) pushed the object towards the robot (causing pulled
effect) or (ii) tried to pass it to robot’s hand (Fig. 6). Similar
response was also observed when the voice behavior “take
this” was applied: while most of the participants took the object and removed it from the scene (causing the disappeared
effect), some of the participants just dragged the object towards themselves (causing moved forward effect). We were
expecting that when iCub enacted the voice behavior “bye”,

Figure 6: Some different reactions by experiment participants
when the robot uses the “pass me” voice behavior.
Both social and non-social behaviors contribute to these
results. For example, pulled can be produced both from pull
and “pass me” behaviors. Note that some of the features,
which were inconsistently changed (marked with star) or negligibly changed (marked with circle), grouped into one column for brevity.

Social Affordances and Multi-step Planning
We demonstrate social affordances in three scenarios:
1- Multi-step planning without human presence In this
scenario, the object is placed in front of iCub as the initial
position, and the target position is shown with red circles
(Fig. 7a). After initial and final positions are shown to iCub,
it plans without a human present in the environment; i.e., it
cannot make use of “social affordances”. According to the
plan, the effect sequence is determined as moved forward,
moved left, moved forward. After a successful push-forward
behavior, the object is moved-forward (Fig. 7b), then with a
push-left behavior, the object reaches close to the target position (Fig. 7c). Appropriate behaviors to end up with the last
moved-forward effect may have been push-forward behavior
or “pass me” voice behavior. Since there is no human across

3607

the table and because the object is too far to be moved forward
to its target position, iCub figures out that it is impossible for
the object to reach its final position (Fig. 7d) and stops at this
stage.

(a)

(b)

(c)

a new effect sequence of moved left, moved forward. This
re-planned effect sequence is achieved by using push-left behavior (Fig. 9f) and “take” voice behavior (Fig. 9g) and
object reaches its target position (Fig. 9h).

(d)

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Figure 7: Scenario #1: The robot cannot reach the target position and cannot fulfill the goal due to absence of a human.
2- Multi-step planning with a human - using “pass me”
voice behavior This scenario demonstrates a case for successful planning. As the initial position, the object is placed
closer to the human and the target position is shown with a red
circle (Fig. 8a). After planning, the effect sequence pulled,
pulled, moved left is determined to reach the target position.
For the first pulled effect, since the object is placed far from
iCub and with the contribution of human presence, “pass me”
voice behavior has the highest probability and is executed
(Fig. 8b). For the remaining pulled and moved-left effects,
pull (Fig. 8c) and push-left (Fig. 8d) behaviors are executed
respectively. As a result, each planned effect is achieved and
the object reaches its target position (Fig. 8e).

Figure 9: Scenario #3: The robot can use human’s affordances when stuck, by using the with “take” voice behavior.
From these 3 different scenarios, we conclude as follows:
(i) After iCub executes a behavior, if it observes an unexpected effect, it re-plans. (ii) iCub executes its behaviors
by planning (and re-planning if necessary) until the object
reaches the target position or iCub decides that it is impossible for the object to reach the target position. (iii) If there is a
human, iCub may benefit from the affordances offered by the
human to get a desired effect. (iv) If there is no human and
the desired effect requires a human, iCub can realize that it is
impossible for the object to reach the target.

Conclusion
(a)

(b)

(c)

(d)

(e)

Figure 8: Scenario #2: The robot can use human’s affordances when stuck, by using the “pass me” voice behavior.
3- Multi-step planning with a human - using “take” voice
behavior This scenario shows a demonstration in which
iCub finds a valid plan but because of a behavior which results with an unexpected effect, iCub re-plans. For this scenario, the object is placed close to iCub and the target position
is shown with a red circle (Fig. 9a). The planner finds out
the required effect sequence as moved forward, moved right,
moved forward. The first two effects are achieved using the
push-forward (Fig. 9b) and then push-right behavior (Fig.
9c). For the last effect, push-forward behavior is executed.
However, instead of a moved-forward effect, moved-right effect occurs (Fig. 9d). Because of this unexpected effect, iCub
needs a re-planning (Fig. 9e). This re-planning results with

In this paper, we used the very same affordance learning
framework developed for discovering the affordances of inanimate things to learn social affordances, that is affordances
whose existence require the presence of humans. We demonstrated that our humanoid robot can interact with objects and
with humans (using simple verbal communication) and from
these interactions, it can learn what the objects as well as the
humans afford . Moreover, we showed that the robot can ask
for human assistance whenever it is required while executing
multi-step plans to satisfy demonstrated/given goals.
Our approach towards learning the social affordances is
in line with the findings that affordances at different levels (intra-level and inter-level) share the same intrinsic constraints and organizations (e.g., (Richardson et al., 2007)).

Acknowledgement
This work is partially funded by the EU project ROSSI (FP7ICT-216125) and by TÜBİTAK through project 109E033.
The authors Kadir F. Uyanik, Onur Yuruten and Asil K.

3608

Bozcuoglu acknowledges the support of Turkish Science and
Technology Council through the 2210 scholarship.
Figures in the result section are adapted from (Mutlu, 2009)
with author’s permission.

References
Akgun, B., Cakmak, M., Jiang, K., & Thomaz, A. L. (2012).
Keyframe-based learning from demonstration. International Journal of Social Robotics, 4(4), 343–355.
Argall, B., Chernova, S., Veloso, M., & Browning, B. (2009).
A survey of robot learning from demonstration. Robotics
and Autonomous Systems, 57(5), 469–483.
Atil, I., Dag, N., Kalkan, S., & Şahin, E. (2010). Affordances
and emergence of concepts. In Epigenetic robotics.
Breazeal, C. (2004). Social interactions in hri: the robot view.
Systems, Man, and Cybernetics, Part C: Applications and
Reviews, IEEE Transactions on, 34(2), 181–186.
Cakmak, M., DePalma, N., Arriaga, R., & Thomaz, A.
(2010). Exploiting social partners in robot learning. Autonomous Robots, 29(3), 309–329.
Dag, N., Atil, I., Kalkan, S., & Sahin, E. (2010). Learning
affordances for categorizing objects and their properties. In
In 20th int. conference on pattern recognition (pp. 3089–
3092).
Detry, R., Kraft, D., Buch, A., Kruger, N., & Piater, J. (2010,
may). Refining grasp affordance models by experience.
In Robotics and automation (icra), 2010 ieee international
conference on (p. 2287 -2293).
Fitzpatrick, P., Metta, G., Natale, L., Rao, S., & Sandini, G.
(2003). Learning about objects through action - initial steps
towards artificial cognition. In Ieee international conference on robotics and automation (icra’03) (p. 3140-3145).
Fong, T., Thorpe, C., & Baur, C. (2003). Collaboration, dialogue, human-robot interaction. Robotics Research, 255–
266.
Gibson, J. (1986). The ecological approach to visual perception. Lawrence Erlbaum.
Kosuge, K., & Hirata, Y. (2004). Human-robot interaction.
In Robotics and biomimetics, 2004. robio 2004. ieee international conference on (pp. 8–11).
Kreijns, K., & Kirschner, P. (2001). The social affordances of
computer-supported collaborative learning environments.
In Frontiers in education conference (Vol. 1).
Marsh, K., Richardson, M., & Schmidt, R. (2009). Social
connection through joint action and interpersonal coordination. Topics in Cognitive Science, 1(2), 320–339.
Montesano, L., Lopes, M., Bernardino, A., & Santos-Victor,
J. (2008). Learning object affordances: From sensory–
motor coordination to imitation. Robotics, IEEE Transactions on, 24(1), 15–26.
Mutlu, B. (2009). Designing gaze behavior for humanlike
robots. Unpublished doctoral dissertation, Pittsburgh, PA,
USA. (AAI3367045)
Pastor, P., Hoffmann, H., Asfour, T., & Schaal, S. (2009).
Learning and generalization of motor skills by learning

from demonstration. In Robotics and automation, 2009.
icra’09. ieee international conference on (pp. 763–768).
Qin, A. K., & Suganthan, P. N. (2004). Robust growing
neural gas algorithm with application in cluster analysis.
Neural Networks, 17(8-9).
Richardson, M., Marsh, K., & Baron, R. (2007). Judging and actualizing intrapersonal and interpersonal affordances. Journal of experimental psychology: Human Perception and Performance, 33(4), 845.
Şahin, E., Çakmak, M., Doğar, M., Uğur, E., & Üçoluk, G.
(2007). To afford or not to afford: A new formalization of
affordances toward affordance-based robot control. Adaptive Behavior, 15(4), 447–472.
Saunders, J., Nehaniv, C., Dautenhahn, K., & Alissandrakis,
A. (2007). Self-imitation and environmental scaffolding for
robot teaching. International Journal of Advanced Robotic
Systems, 4(1), 109–124.
Sinapov, J., & Stoytchev, A. (2008, aug.). Detecting the
functional similarities between tools using a hierarchical
representation of outcomes. In Development and learning,
2008. icdl 2008. 7th ieee international conference on (p. 91
-96).
Stoytchev, A. (2008). Learning the affordances of tools using
a behavior-grounded approach. Towards Affordance-Based
Robot Control, 140–158.
Ugur, E., Celikkanat, H., Sahin, E., Nagai, Y., & Oztop, E.
(2011). Learning to grasp with parental scaffolding. In
Humanoid robots (humanoids), 2011 11th ieee-ras international conference on (pp. 480–486).
Ugur, E., & Şahin, E. (2010). Traversability: A case study
for learning and perceiving affordances in robots. Adaptive
Behavior, 18(3-4), 258-284.
Ugur, E., Şahin, E., & Oztop, E. (2009). Affordance learning from range data for multi-step planning. Int. Conf. on
Epigenetic Robotics.
Ugur, E., Oztop, E., & Şahin, E. (2011). Goal emulation
and planning in perceptual space using learned affordances.
Robotics and Autonomous Systems, 59(7-8).
Weber, J. (2008). Human-robot interaction. Handbook of Research on Computer Mediated Communication. Hershey,
PA: Information Science Reference.
Wellman, B., Quan-Haase, A., Boase, J., Chen, W., Hampton,
K., Dı́az, I., et al. (2003). The social affordances of the internet for networked individualism. Journal of ComputerMediated Communication.
Williams, K., & Breazeal, C. (2012). A reasoning architecture for human-robot joint tasks using physics-, social-,
and capability-based logic. In Intelligent robots and systems (iros), 2012 ieee/rsj international conference on (pp.
664–671).
Yürüten, O., Uyanık, K. F., Çalışkan, Y., Bozcuoğlu, A. K.,
Şahin, E., & Kalkan, S. (2012). Learning adjectives and
nouns from affordances on the icub humanoid robot. In
From animals to animats 12 (pp. 330–340). Springer.

3609

