UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Gestures for Thinking

Permalink
https://escholarship.org/uc/item/0zk7z5h9

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Jamalian, Azadeh
Giardino, Valeria
Tversky, Barbara

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Gestures for Thinking
Azadeh Jamalian (aj2334@columbia.edu)

Teachers College, Columbia University, 525 W. 120th Street
New York, NY 10027 USA

Valeria Giardino (Valeria.Giardino@ens.fr)
Institut Jean Nicod, Ecole Normale Supérieure, 29, rue d'Ulm,
F-75005 Paris

Barbara Tversky (btversky@stanford.edu)

Teachers College, Columbia University, 525 W. 120th Street
New York, NY 10027 USA
Abstract

comprehended and learned felicitously, yielding an
integrated external model of the information that can be
inspected and mentally manipulated (e. g., Tversky, 2011).
Gestures are crude, and as such almost necessarily abstract.
They can also create integrated external models. In
explaining complex environments or scientific systems,
people produced a coordinated and integrated series of
gestures that modeled the spaces of environment (Emmorey,
Tversky, and Taylor, 2000), family trees (Enfield, 2003),
and scientific processes (Kang, Tversky, and Black, 2013)
to be learned.
People gesture to explain spatial environments to others,
creating external models with their hands. Will they do so
for themselves, as aids to comprehension and memory?
Here, we investigate whether people, alone in a room
studying descriptions of complex environments will gesture
for themselves. If so, what is the nature of their gestures?
And does gesturing help them learn and remember the
environments?
Gesturing could help learning and memory indirectly by
off-loading memory to another modality. Gestures have
been shown to be effective in off-loading memory during
explanations (Goldin-Meadow, et al, 2001). But gestures
could also help learning and memory in direct ways, by
constructing an external model of the environment to be
learned. Half the environments participants studied had 4
landmarks and half had 8 landmarks; the latter should put
greater stress on working memory (e. g., Jonides, Lewis,
Nee, Lustig, Berman, and Moore, 2008). If the primary role
of gestures is to offload working memory, participants
should gesture more when studying descriptions with more
landmarks. If the primary role of gestures is to construct a
model of the environment, much like a diagram, then there
is little reason to expect more gesturing for the
environments with more landmarks. Gestures can reflect
mental representations (e. g., Alibali, Bassok, Olseth, Syc,
and Goldin-Meadow, 1999). Description perspective was
manipulated because route and survey descriptions yield
different mental representations early (but not late) in
learning (Lee and Tversky, 2005).

Can our gestures help us think, and, if so, how? Previous
work suggests that they can. Here, students, alone in a room,
studied descriptions of environments for later tests of
knowledge. The majority of participants spontaneously
gestured while reading the descriptions, and most of those
also gestured while answering true-false questions. They did
not gesture proportionately more time for environments with
many landmarks than for environments with few. Their
gestures laid out the environments, primarily using points to
places and lines for paths. Descriptions and questions
accompanied by gestures were remembered more accurately.
Participants rarely looked at their hands. Gestures seem to
promote learning by establishing embodied representations of
the environments.
Keywords: Gesture; embodiment; spatial representation;
spatial memory; route/survey perspectives; navigation.

Introduction
Gestures serve many ends and have many forms. People
gesture in communications to others, but also for
themselves, that is, they gesture to think (Goldin-Meadow,
2003; McNeill, 1992). Gestures for thinking help thinking in
different ways. They help people find words (Krauss &
Hadar, 2001). They offload memory (Cook, Yip, & GoldinMeadow, 2012; Goldin-Meadow, Nusbaum, Kelly, &
Wagner, 2001). They help people perform mental rotation
(Chu & Kita, 2008; Wexler, Kosslyn, & Berthoz, 1998;
Wohlschlager & Wohlschlager 1998). They help people
count (Carlson, Avraamides, Cary, & Strasberg, 2007).
Gestures are actions in space, and as such, can readily
represent spatial structures and spatial actions. In fact,
gestures help people solve spatial problems (Kessell &
Tversky, 2006; Schwartz & Black, 1996). Interestingly, in
solving spatial problems, gestures can serve much like
diagrams. When given paper and pencil during problem
solving, one group diagrammed the same spatial problems
that another group gestured to solve (Kessell & Tversky,
2006). Diagrams also offload memory, but they serve
cognition in many other ways. Creating a good diagram
entails extracting the crucial information and structuring it
to represent a problem to be solved or information to be

645

Method

Design. Each participant read four descriptions, one with 4
landmarks and one with 8 landmarks from each perspective.
The specific environment for each condition was chosen
from the set of three outdoor environments and three indoor
environments. All variables, size, perspective, environment,
order were counter-balanced and appeared equally often
across participants.
True-false Questions. Verbatim and inference statements
were designed for each description, 10 for the 8 landmark
environments and 6 for the 4 landmark environments. For
the 8 landmark environments, there were 2 statements taken
verbatim from the text with the same perspective, 2
statements taken verbatim from the text with the other
perspective, and 6 inference statements, 3 route, and 3
survey. For the 4 landmark environments, there were a total
of 6 statements: 1 verbatim from the route perspective, 1
verbatim from the survey perspective, 2 inference from a
route perspective, and 2 inference from a survey
perspective. Inference statements could be verified from
information provided in the descriptions. Half of the
statements were true and the other half was false. The
statements were presented in a random order for each
participant. Table 2 shows examples of true/false statements
for Etna.

Participants. 48 (28 female, 20 male), primarily graduate
students from Columbia University, were paid to participate
in the study. Participants were native English speakers or
have graduated from an English speaking high school.
Descriptions. The environments had 4 or 8 landmarks.
There were three outdoor environments, Etna City,
Chinatown, and the Financial district, and three indoor
environments, a spa, an electronics show, and a grocery
store. There were 8 landmarks and 4 landmarks versions of
each of these.
There were also versions of each environment from route
(R) or survey (S) perspectives. A route perspective takes an
imaginary traveler, you, through an environment describing
the turns and landmarks with respect to “you” in terms of
your left, right, front, and back. A survey perspective takes
an overview of an environment and describes landmarks
with respect to each other in terms of north-south-east-west.
The route descriptions always began with cardinal directions
so that participants could answer questions from a survey
perspective. The descriptions and the environments were
based on earlier work (Taylor & Tversky, 1992).
The average length of the 8R descriptions was 141 words,
of the 8S descriptions, 127 words, of the 4R descriptions, 69
words, and of the 4S descriptions, 72 words. Table 1 shows
an example of a description of an outdoor environment with
4 landmarks from a survey perspective, and of an indoor
environment with 8 landmarks from a route perspective.

Table 2: Examples of true/false statements

Route

Table 1: Examples of descriptions
Example 1: 4S outdoor environment
Etna is a charming town nestled in an attractive valley,
entered on River Highway. River Highway runs eastwest at the southern edge of the town of Etna. Toward
the eastern border, River Highway intersects with
Mountain Rd, which runs north of it. At the northwest
corner of the intersection is a gas station. North of the
gas station, Mountain Road will intersect with Maple
Ave, which runs west.

Survey

Verbatim
Going east on River
Highway, at the
intersection with
Mountain Rd, you will
find a gas station on
your left.
North of the gas
station, Mountain
Road will intersect
with Maple Ave,
which runs east.

Inference
From Mountain Rd,
turn right on River
Highway and you
will have the Gas
Station on your
right.
South of Maple Ave
to the west of
Mountain Rd is the
Gas Station.

Procedure. Participants first signed a consent form,
assenting to participating in the experiment and to being
videotaped. They were additionally asked for permission to
show their videos in presentations of the research. They then
completed a paper version of the Mental Rotation Task
(Vandenberg & Kuse, 1978), a common test of spatial
ability.
Participants were seated in front of a Mac OS X 10.7, as
shown in Figure 1. Video records of the computer screen
and front views of participants were captured with
Silverback© software, and participants’ side views with a
videocam. The experimenter explained the procedure to
each participant: “In this study you will be asked to read 4
text descriptions of environments. After reading each
description, your memory for the information in the text will
be tested. You will start with a practice text description.
Throughout the study, you will not have access to a

Example 2: 8R indoor environment
Rock Creek Center is a showcase for new electronic
devices. Enter Rock Creek Center from the east side of
the building near the southeast corner. As you enter, you
see, on the left wall, a Bulletin Board. Past the Bulletin
Board, on your right is the Video Camera room and on
your left is the Office stretching to the corner of the
building. Past the office you are forced to turn right and
you will find the Cafeteria on your left stretching to the
corner of the building. After the Cafeteria, you are forced
to turn right and you will find a large room with Mobile
Phones on your left. On your right you will see the
Televisions room. At the end of the hallway, turn right
and you will find the Laptop Center on your left. Past the
Laptop Center, you will return to the entrance on your
left.

646

keyboard and will send commands to the computer with
your voice.” The participants responded verbally, saying
“next”, “yes”, or “no” when appropriate, to advance from
screen to screen. Their responses were analyzed by the Mac
speech recognition program and used to advance screens
and record responses. This left participants’ hands free to
gesture, on or off the table.
Participants first had a practice trial. The first screen
explained the task: “You will be asked to read the
description of an environment as practice. Once you are
done reading the description say aloud “Next”. After the
description you will be asked to judge the truth of some
statements about the environment. You may take as much as
time you need.” Then participants read a description of an
amusement park. The complete description was on the
screen. Participants were free to read the practice and
experimental descriptions as long as they liked. Immediately
after reading the description, participants were presented
with 4 true/false questions, one on each screen. They said
“yes” for true and “no” for false. After the practice trial, the
experimenter answered any questions the participant had,
and then left the room.
Participants then proceeded through the experiment,
reading each of the four descriptions and answering the
corresponding true/false questions after each.

a third coder. One coder coded the remaining videos,
discussing uncertain cases with the second coder.
Qualitative coding of the gestures is ongoing, but it is clear
that gestures indicating places, primarily points, and
indicating connections between places, drawing lines or
placing the edge of a hand, predominate. Most gestures were
performed on the table, but some were in the air (see
Figures 1 and 2).
Gesture at study. Seventy-three percent of participants (35
out of 48) gestured at least once for at least one description
during study. Twelve participants (25%) gestured for all
four descriptions, 7 gestured for three, 10 gestured for two,
and 6 for only one. Notably, number of landmarks in the
environments (4 vs. 8) did not influence whether
participants gestured at study, 𝝌2(1, N= 48)= 1.132, p=
0.289. Similarly, neither perspective (route vs. survey),
𝝌2(1, N= 48)= .023, p= 0.879, order (1st, 2nd, 3rd, or 4th),
𝝌2(3, N= 48)= 1.687, p= 0.171, or MRT score , F (1, 185)=
0.089, p= 0.765, influenced gesturing at study.
For each participant, the percentage of time gesturing
while studying was computed. Neither spatial ability F(1,
45)= 0.357, p= 0.553) nor gender (F(1, 45)= 0.505, p=
0.481) affected the percent of time gesturing
Gesture at test. Sixty-five percent of participants (31 out of
48) gestured at least once when verifying the true/false
statements. Table 3 shows number of statements for which
participants gestured both when studying and answering,
only when studying, only when answering, or not at all, out
of the total of 1526 statements (excluding 10 cases in which
participants’ answers were missing).

Figure 1: Experimental Setup. Participant gesturing while
studying description.

Results
Coding. Two trained coders, coded 10 of 48 videos for
gesturing while studying, Kappa = 0.76 (p < 0.001), for
length of time spent gesturing while studying, t(39)= 0.244,
p= 0.809, for looking at their hands while gesturing while
studying, Kappa = 0.56 (p < 0.001), for studying time,
t(39)= 1.402, p= 0.169, for gesturing while verifying
statements, Kappa = 0.90 (p < 0.001), for looking at their
hands while gesturing in verifying statements, Kappa = 0.44
(p < 0.001), and for length of time to verify statements,
t(359)= 0.120, p= 0.90. Any movement of hands or fingers,
excluding beat gestures, was coded as gesturing. Any glance
at hands while gesturing was coded as looking. The coded
duration of the gesture included active movements and
periods when individuals left their hands still on the table or
in mid-air in a certain position and form. Times were coded
from the Silverback© videos of the screen and by using
ELAN software. In cases of disagreement coders consulted

Figure 2. Participant gesturing while answering question.
Table 3: Number of questions and gesture behavior
Gesturing
Both at study and when
verifying
Only at study
Only when verifying
None

Frequency
547

Percentage
35.8%

220
21
738

14.4%
1.4%
48.4%

As evident from Table 3, participants were far more likely
to gesture to verify statements for the descriptions they
gestured at study. Only 1.4% of the questions received

647

gestures at verification that had not received gestures at
study.
Moreover, for 85% of the descriptions accompanied by
gesture, at least one question was also accompanied by
gesture. Participants, who did not gesture at all while
studying the descriptions did not gesture when answering
questions. Specifically, 27% of participants (13 out of 48)
did not gesture either at study or at verification.
Overall, neither the environment’s perspective (survey vs.
route), 𝝌2(1, N= 48)= .743, p= 0.389, nor question

The effects of gesturing at verification were analyzed
separately. Participants were more likely to be accurate
verifying statements when they gestured (M= 0.814, SD=
0.23), than when they did not (M= 0.757, SD= 0.29), F(1,
1515)= 5.325, p= 0.038 < 0.05. As before, accuracy
increased with spatial ability, F(1, 1515)= 10.191, p=0.001
< 0.01, and was affected by statement category in the same
ways as the previous analysis, F(1, 1515)= 17.084, p <
0.001.

perspective, 𝝌2(1, N= 48)= .264, p= 0.608, nor number of

0.85

landmarks (4 vs. 8), 𝝌 (1, N= 48)= .028, p= 0.868, nor type

Average Accuracy

2

Average Accuracy

of statement (verbatim vs. inference), 𝝌2(1, N= 48) = .439,
p= 0.508, nor MRT scores, F(1, 1520) = 0.899, p= 0.343
influenced whether participants gestured at verification.
In short, most participants gestured while studying and
verifying and most who gestured at verification had also
gestured at study. Neither spatial ability nor length nor
perspective of the descriptions or questions affected whether
participants gestured.
Accuracy. As evident in Figure 3, when participants had
gestured at study, they were more likely to be accurate at
testing (M= 0.821, SD = 0.29) than when they had not
gestured at study (M= 0.743, SD = 0.30), F(1, 1517) =
8.249, p=0.004 < 0.01. Not surprisingly, accuracy was
higher for the 4 landmark environments (M= 0.810, SD=
0.24) than for the 8 landmark environments (M= 0.760, SD=
0.28), F(1, 1517)= 6.561, p= 0.011 < 0.05. Accuracy
improved with spatial ability, F(1, 1517)= 10.210, p= 0.001
< 0.01 but the correlation between accuracy and spatial
ability was low and not significant. Accuracy varied with
kind of statement, F(1, 1517)= 7.182, p < 0.001. Replicating
Taylor and Tversky (1992), post-hoc analyses showed that
verbatim statements (M= 0.838, SD= 0.21) were more
accurate than inference statements (M= 0.720, SD= 0.31),
t(1513)= 3.809, p < 0.01, and that for inference statements,
there was no advantage for statements in the perspective of
reading (same perspective (M = 0.727, SD = 0.30); other
perspective (M = 0.718, SD = 0.31), t(1513) =0.311, p=
0.756), indicating that memory representations were
perspective-free.

0.75
0.7
Gesture When
Verifying

No Gesture When
Verifying

Figure 4: Accuracy by gesturing at verification.
To examine the effects of gesture at study and gesture at
response, participants were divided into 4 groups: gesture at
both, gesture only at study, gesture only at response, no
gesture. Gesture behavior had an effect on accuracy, F(3,
1494)= 3.593, p= 0.013 < 0.05. Post-hoc analyses showed
that participants were more accurate at testing when they
had gestured both at study and verification (M= 0.780, SD=
0.27), than when they did not gesture at all (M= 0.705, SD=
0.32), t(1494)= 2.491, p= 0.013 < 0.05. Similarly, they were
more accurate when they only gestured at study (M=0.816,
SD=0.23), than when they did not gesture at all, t(1494)=
2.655, p= 0.008 < 0.01. However, there was not a significant
improvement for gesture only at response (M= 0.811,
SD=0.25) than for no gesture, t(1494)= 0.333, p= 0.739; this
could be due to the severely limited number of cases in
which participants only gestured at response (See Table 3).
To make sure that the advantage of gesturing was not
because the better learners gestured, comparisons were done
within participants who gestured when studying two or three
descriptions, but not all descriptions. For those who
gestured sometimes, accuracy was higher when they
gestured at study (M= 0.762, SD = 0.29) than when they did
not (M= 0.677, SD = 0.35), F(1, 513) = 3.938, p= 0.048 <
0.05. Similarly, they were more accurate verifying
statements when they gestured (M= .764, SD= 0.29) than
when they did not gesture (M= 0.628, SD= 0.35), F(1,
513)= 3.910, p= 0.049 < 0.05. So, gesturing itself helps - it
is not just that those who tend to gesture also remember
better.
Studying Times. As expected, participants took longer to
study the longer descriptions with 8 landmarks (M=
112.14sec, SD= 28.43) than the shorter ones with 4
landmarks (M= 56.57sec, SD=28.43), F(1, 187)= 94.104, p

0.85
0.8
0.75
0.7
0.65
Gesture at Study

0.8

No Gesture at
Study

Figure 3. Accuracy by gesturing at study. Error bars
represent standard error

648

Verification Times (sec)

< 0.001. Gesturing did not influence study time, F(1, 187)=
1.212, p= 0.272. Similarly, neither spatial ability, F(1, 187)=
2.198, p= 0.140, nor text perspective, F(1, 187)= 0.101, p=
0.752, affected study times.
Verification Times. Figure 5 shows that gesture behavior
influenced verification time, F(3, 1441)= 3.431, p= 0.016 <
0.05. Post-hoc Bonferroni comparisons showed that
participants were faster to verify statements when they had
only gestured at study (M= 8.95 sec, SD= 2.61) than when
they had not gestured at all (M= 10.35 sec, SD= 4.16), p <
0.001. By contrast, answering took longer when participants
only gestured at verification (M= 15.65 sec, SD= 6.19) than
when they only gestured at study, p=0.004 < 0.01. There
was no difference on verification time when participants
gestured both at study and at verification (M= 11.47 sec,
SD= 3.88), compared to when they did not gesture at all.
Spatial ability, perspective, and size of environment did not
effect verification times. Thus gesture at study decreased
verification time while gesture at responding increased
verification time, and in cases when they gestured both at
study and at verification, the two effects cancelled each
other.

18
16
14
12
10
8
6
4
2
0

No Gesture

Gesture at
Study Only

descriptions accompanied by gestures were remembered
better than those that were not, and the questions that were
accompanied by gestures were answered more accurately
than those that were not. The advantage of gesturing on
memory cannot be explained as the better participants both
gestured and remembered better. Even within those
participants who frequently gestured, gesturing at study and
at responding improved memory. Gestures modeled the
structures of the environments, pointing to places and
outlining paths between places. Except on rare occasions,
participants did not look at their hands as they gestured,
suggesting that it is the actions per se that serve
comprehension and learning, rather than the visual
accompaniments. Overall, spontaneous gesturing at learning
and spontaneous gesturing at memory retrieval promoted
learning. Gestures appeared to improve learning by
establishing embodied representations of the structures of
the environments and appear to improve memory by
redintegrating the queried parts of the environments.
In addition to providing embodied representations of the
environments, gestures might also have served to offload
memory, as in previous research (e. g., Cook, et al., 2012;
Goldin-Meadow, et al., 2001), just as diagrams offload
memory. However, the proportion of study time gesturing
did not increase as memory load increased from light to
heavy. Thus, the role of gesture in lightening memory load
appears to be less important for comprehending and learning
complex environments than other features of gestures,
notably, creating embodied representations.
Gestures are actions, and thereby provide an additional
code beyond the verbal code participants read. Multiple
codes in multiple modalities are known to promote memory
(e. g., Paivio, 1986). Motor codes in particular augment
memory (e. g. Engelkamp & Zimmer, 1994; Hommel,
Musseler, Aschersleben, & Prinz, 2001) but the cases that
have been studied have primarily been cases where the
memory was for the action per se. In the present case, the
actions served memory not for the actions but rather for
what the actions represented.
Actions, like diagrams and words, can represent, that is,
they can stand for something other than themselves.
Certainly for the case of words but also for the case of
diagrams, representation seems to be their primary function.
Not so for actions. Actions can represent, but they are
primarily used for the ordinary (and extraordinary) tasks of
life, manipulating objects and navigating environments.
Gestures are a special class of actions that serve to represent
rather than to act on or in the world. Similar to diagrams,
gestures can represent more directly than purely symbolic
words; they bear some resemblance to what they represent
(e. g., Tversky, 2011).
Like diagrams, gestures can use space to represent ideas
that are spatial or metaphorically spatial (e. g., Enfield,
2003; Emmorey, et al., 2000; Tversky, 2011; Tversky,
Heiser, Lee, & Daniel, 2009). Like diagrams, gestures are
spatial and visual. However, it seems that the spatial and
action components of representational gestures serve

Gesture at Gesture at Both
Verification
Study and
Only
Verification

Figure 5: Verification time by gesture behavior
Did participants look at their hands while gesturing? For
the most part, participants did not look at their hands while
gesturing; they looked at their hands for 35.8% of the texts
during reading but they were typically brief glances. Out of
the 35 participants who gestured at least once when reading
texts, 15 never looked at their hands. At verification,
participants looked at their hands for less than 10% of the
statements they gestured while verifying. Out of the 31
participants who gestured for at least one of the statements,
16 never looked at their hands.

Discussion
Participants, alone in a room, read descriptions of a variety
of complex environments that they were to learn for later
questions. While they were studying, most of them gestured
at least once, and the majority gestured for most of the
descriptions, in the absence of any communication. The

649

comprehension and memory rather than the visual.
Participants rarely looked at their hands. Researchers in art,
sketching, and design refer to drawing as gesture.
Blindfolded architects gesture copiously as they design, and
they cannot see either their gestures or their designs.
Nevertheless, their designs equal those they create without
blindfolds (Bilda and Gero, 2006). Together, these findings
suggest that some of the benefits of gesturing to those who
gesture may be the embodiment of thought into action.

Jonides, J., Lewis, R. L., Nee, D. E., Lustig, C. A., Berman,
M. G., and Moore, K. S. (2008). The mind and brain of
short-term memory. Annual Review of Psychology, 19,
193-224.
Kang, S., Tversky, B. & Black, J. B. (2013) Gesture and
speech in explanations to experts and novices. Submitted.
Kessell, A. M. & Tversky, B. (2006). Using gestures and
diagrams to think and talk about insight problems. In R.
Sun and N. Miyake (Eds) Proceedings of the 28th Annual
Conference of the Cognitive Science Society. P. 2528.
Krauss R. M. & Hadar U. (2001). The role of speech-related
arm/hand gestures in word retrieval. In R. Campbell & L.
Messing, (Eds.) Gesture, speech, and sign. Pp. 93-116.
Oxford: Oxford University Press.
Lee, P. U. and Tversky, B. (2005). Interplay between visual
and spatial: the effect of landmark descriptions on
comprehension of route/survey spatial descriptions.
Spatial cognition and computation, 5, 163-185.
McNeill, D. (1992). Hand and mind: What gestures reveal
about thought. Chicago: University of Chicago Press.
Paivio, A (1986). Mental representations: a dual coding
approach. Oxford: Oxford University Press.
Schwartz, D. L., & Black, J. B. (1996). Shuttling between
depictive models and abstract rules: Induction and
fallback. Cognitive Science, 20, 457– 497.
Taylor, H. A., & Tversky, B. (1992). Spatial mental models
derived from survey and route descriptions. Journal of
Memory and Language, 31, 261-282.
Tversky, B. (2011). Visualizing thought. Topics in
Cognitive Science. 3, 499-535.
Tversky, B., Heiser, J., Lee, P. and Daniel, M.P. (2009).
Explanations in gesture, diagram, and word. In K. R.
Coventry, T. Tenbrink, & J. A. Bateman (Editors), Spatial
language and dialogue. Pp. 119-131. Oxford: Oxford
University Press.
Vandenberg, S. G. and Kuse, A. R. (1978). Mental
rotations: A group test of three-dimensional spatial
visualization. Perceptual Motor Skills, 47, 599-604.
Wexler, M., Kosslyn, S. M., & Berthoz, A. (1998). Motor
processes in mental rotation. Cognition, 68, 77–94.
Wohlschläger, A., & Wohlschläger, A. (1998). Mental and
manual rotatation. Journal of Experimental Psychology:
Human Perception and Performance, 24, 397–412.

Acknowledgments
The authors are indebted to NSF IIS-0725223, NSF IIS0855995 and NSF IIS-0905417 for supporting parts of the
research and manuscript preparation. Valeria Giardino is
grateful to The Italian Academy for Advanced Studies in
America at Columbia University for supporting her visit.

References
Alibali, M. W., Bassok, M., Olseth, K. L., Syc, S. E., &
Goldin-Meadow, S. (1999). Illuminating mental
representations through speech and gesture. Psychological
Science, 10, 327-333.
Bilda, Z and Gero, J.S. (2006) Reasoning with internal and
external representations: A case study with expert
architects. In R Sun (ed), CogSci/ICCS 2006, Lawrence
Erlbaum, pp. 1020- 1026.
Carlson, R. A., Avraamides, M. N., Cary, M., & Strasberg,
S. (2007). What do the hands externalize in simple
arithmetic? Journal of Experimental Psychology:
Learning, Memory, and Cognition, 33(4), 747-756.
Chu, M., & Kita, S. (2008). Spontaneous gestures during
mental rotation tasks: Insights into the microdevelopment
of the motor strategy. Journal of Experimental
Psychology: General, 137, 706–723.
Cook, S. W., Yip, T. & Goldin-Meadow, S. (2012).
Gestures, but not meaningless movements, lighten
working memory load when explaining math. Language
and Cognitive Processes, 27, 594-610.
Emmorey, K., Tversky, B., & Taylor, H. (2000). Using
space to describe space: Perspective in speech, sign, and
gesture. Spatial Cognition and Computation, 2, 157-180.
Enfield, N. (2003). Producing and editing diagrams using
co-speech gesture: Spatializing non-spatial relations in
explanations of kinship in Laos. Journal of Linguistic
Anthropology, 13, 17-50.
Engelkamp, J. and Zimmer, H. D. (1994). Human memory:
A multimodal approach. Seattle: Hogref and Huber.
Goldin-Meadow, S. (2003). Hearing gesture: How our
hands help us think, Cambridge, Mass.: The Belknap
Press.
Goldin-Meadow, S., Nusbaum, H., Kelly, S. D., & Wagner,
S. (2001). Explaining math: Gesturing lightens the load.
Psychological Science, 12, 516-522.
Hommel, B., Musseler, J., Aschersleben, G., & Prinz, W.
(2001). The theory of event coding (TEC): A framework
for perception and action planning. Behavioral & Brain
Sciences, 24, 849-937.

650

