UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Oscillator Model of Categorical Rhythm Perception

Permalink
https://escholarship.org/uc/item/21x1k5n6

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Bååth, Rasmus
Lagerstedt, Erik
Gärdenfors, Peter

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

An Oscillator Model of Categorical Rhythm Perception
Rasmus Bååth (rasmus.baath@lucs.lu.se)
Erik Lagerstedt (drattans@gmail.com)
Peter Gärdenfors (peter.gardenfors@lucs.lu.se)
Lund University Cognitive Science
Lund University, Kungshuset, Lundagård, 222 22 Lund, Sweden
Abstract
Categorical perception is a well studied phenomenon in, for
example, colour perception, phonetics and music. In this article we implement a dynamical systems model of categorical
rhythm perception based on the resonance theory of rhythm
perception developed by Large (2010). This model is used to
simulate the categorical choices of participants in two experiments of Desain and Honing (2003). The model is able to accurately replicate the experimental data. Our results supports
that resonance theory is a viable model of rhythm perception
and they show that by viewing rhythm perception as a dynamical system it is possible to model properties of categorical perception.
Keywords: Categorical perception; rhythm perception; dynamical systems; resonance theory

Introduction
Categorical perception occurs when categorization is amplified by the perceptual systems so that distances within a category are perceived as being smaller and distances between
categories are perceived as larger than they are according to
the values of some physical measurement. It is a common
phenomenon that is well studied in, for example, colour perception, phonetics and music (Harnad, 1990). A central question for understanding categorical perception is: What is the
underlying mechanism? There have been attempts to model
categorical perception in terms of neural networks (Damper
& Harnad, 2000). In this article, we will focus on modelling categorical rhythm perception and present a model that
is based on oscillators.
In the field of music perception rhythm refers to a temporal pattern of sound onsets. A rhythm in this sense does not
have to be periodic or recurrent. This is in contrast with how
that word is used in other fields (cf. circadian rhythm or delta
rhythm). A related concept that does involve periodicity is
beat. When listening to a piece of music a common response
is to move one’s body with a perceived periodic pulse (Snyder
& Krumhansl, 2001), that pulse is the beat of the corresponding piece of music. It is not common that all beats in a piece
of music are perceived as being equally accented and a periodically recurring pattern of strong and weak beats is called
a meter. For example, a duple meter would imply that every
second beat is perceived as having a stronger accent while every third beat is perceived as having a stronger accent in the
case of a triple meter. Rhythm perception and the ability to
entrain to a musical beat was long thought to be uniquely human and, while it has recently been shown that some vocal
mimicking species are, to some degree, able to move in synchrony with a beat (Schachner, Brady, Pepperberg, & Hauser,

2009), humans are still unique in their aptitude for rhythmic processing. Already infants have been shown to have a
sense of rhythm (Honing, Ladinig, Háden, & Winkler, 2009)
and there exists only one documented case of ”beat deafness”
(Phillips-Silver et al., 2011), that is, the inability to reliably
synchronize to a musical beat.
Desain and Honing (2003) showed in two experiments that
listeners reliably experienced rhythms as belonging to rhythmic categories and that categorizations were strongly influenced when the listeners were primed with a metric beat before hearing a rhythm. Furthermore, participants agreed to
a large degree on which rhythms belonged to what category
and, similar to categorization of other kinds of stimuli (c.f.
Jäger (2010) on colour categories), the categories were found
to be roughly convex with respect to a temporal performance
space (Gärdenfors, 2000). Honing (2013) concludes that: ”It
is puzzling, however, that although meter was shown to exert
a strong influence on the recognition of rhythm [...] existing
computational models of meter can explain this phenomenon
only to a small extent”. In this article we show that an oscillation based, resonance theory model of rhythm perception (Large, 1996, 2010) can replicate many of the findings
of Desain and Honing (2003). Our results support the notion
that resonance theory is a viable model of rhythm perception
and show that by viewing rhythm perception as a dynamical
system it is indeed possible to model the properties of categorical rhythm perception.

Resonance Theory and Categorical Rhythm
Perception
Modelling of human timing and rhythm perception has a long
history. One influential model is the one described by Wing
and Kristofferson (1973), which is based on an information
theoretic perspective. Like many such models (cf. Repp,
2005), it models a participant’s behaviour when attempting
to elicit isochronous timing responses. An alternative to this
information theoretic perspective is to take a dynamical systems perspective and model time and rhythm perception as
an emergent, dynamic phenomenon. A number of models
of this kind have been proposed (e.g., Large, 1996; Noorden
and Moelants, 1999). Here, the term resonance theory (cf.
Large, 2010) will be used to refer to this type of models. Resonance theory does not dictate a specific model but rather incorporates a number of related models. All can be considered as dynamical system models and they consist of one or
more resonating oscillatory units. Resonance theory provides
a compelling framework since it is biologically plausible, has

1803

(e.g., Brochard, Abecasis, Potter, Ragot, and Drake, 2003).
One persuasive study that clearly showed that rhythm perception involves neural oscillatory activity is that of Nozaradan,
Peretz, Missal, and Mouraux (2011). They found that playing a rhythmic beat to a participant elicited a sustained periodic neural response, as measured by EEG, that matched
the frequencies of the beat. Resonance theory models differ in their biological plausibility, the number of oscillatory
units employed and the type of oscillators used. Eck (2002)
constructed a model with a clear biological connection as it
used the Fitzhugh-Nagumo model of neural action potential
(Nagumo, Arimoto, & Yoshizawa, 1962) as the oscillatory
unit. Other models claim no biological plausibility, for example, the model by Scheirer (1998) that employs a comb filter
as the oscillatory unit. Toiviainen and Snyder (2003) modelled participants behaviour when tapping along to excerpts
of music composed by Bach using a single oscillatory unit
while Large and Kelso (2002) used a bank of 96 oscillators to
model participants’ tapping to ragtime music.
Figure 1: (a) and (b) show examples of two possible rhythms
and their placement in the triangular performance space (c)
defined by Desain and Honing (2003). All one second long,
four sound rhythms can be represented as a point in this space.
(From Honing, 2013 with permission).
a solid base in dynamical systems theory and is able to model
many aspects of meter and rhythm perception (Large, 2000).
The general idea of resonance theory is that an external auditory rhythm can be represented by the rhythm of internal
oscillatory units. These oscillatory units are coupled to the
external rhythm and are by definition periodic while the external rhythm does not have to be periodic. Given a rhythm sequence as input the basic output of a resonance theory model,
or resonance model for short, is the amplitude response of
the oscillators over time. This high dimensional representation might be difficult to work with directly, however, and a
more convenient representation is given by creating an activation pattern, A, by summing the amplitude responses of
each oscillator over time, as in
te

Ai =

∑ ai,t

(1)

t=ts

where ai,t is the amplitude for oscillator i at time t while ts and
te are the start and end time steps for the summation. Before
the resonance model is given any input it is in a resting state
and it takes a number of time steps before the system is activated by the stimuli. Therefore it is not necessarily desirable
to sum over the whole extent of the duration of the rhythm
sequence and an activation pattern created by summing over
the latter time steps may represent the rhythm sequence better
than an activation pattern created by summing over all time
steps.
While not all resonance theory models claim biological
plausibility, a number of neuroimaging studies have shown
connections between neural resonance and rhythm perception

To our knowledge, resonance theory models have not previously been used to model categorical rhythm perception.
One reason for this might be that while the amplitude response of the oscillators in the resonance model reflects, perhaps even represents, the rhythm sequence given as input to
the system it does not give rise to a categorization per se. That
is, while the state the resonance model arrives at depends on
the given rhythm sequence, there is no finite number of discrete states that can be said to constitute categories. Still, the
state of the resonance model can be used as the basis of a
categorical decision based on learnt prototype states or a discrete partitioning of the system state space. By considering
the activation pattern of a resonance model as a point in an
n-dimensional space, n being the number of oscillatory units,
this space can be partitioned into regions corresponding to
rhythm categories and used to produce categorical decisions
(following the general model of concepts from Gärdenfors
(2000)). The relation between the activation pattern of a resonance model and such a rhythm categorization is analogous
to the relation between the hue, saturation and lightness of a
colour percept and a colour categorization. That is, a colour
percept can similarly be viewed as a point in a three dimensional space with dimensions hue, saturation and lightness
and this space can be partitioned into regions, each representing a colour category, and used to produce categorical colour
decisions.
If the state of the resonance model is viewed as the basis
for a categorical decision then two predictions regarding categorical rhythm perception can be made:
(1) More distinct states will facilitate categorization. Here
a distinct state refers to a subset of oscillators in a resonance
model having a strong amplitude response while most oscillators show a weak amplitude response. This is in contrast
to a non-distinct state where most oscillators have a similar
amplitude response, that is, there are many competing signals and there is no clear single winning candidate among

1804

Figure 2: Maps over categorization consistency. (a) shows
the entropy of the categorical choices for the participant given
the same rhythm sequences multiple times from Desain and
Honing (2003, used with permission). (b) shows the signalto-noise measure calculated from the activation patterns generated by the resonance model.
the categories. In an experimental categorical task it would
then be predicted that a participant would categorize a rhythm
sequence more consistently, and with more confidence, if
the sequence resulted in a more distinct state in a resonance
model than if the sequence resulted in a less distinct state.
(2) Rhythm sequences resulting in similar states are categorized similarly. That is, different rhythm sequences resulting in similar states when used as the input to a resonance
model should be categorized similarly by participants in an
experimental task.
In order to test these predictions, data from a rhythm categorization task is needed. A study by Desain and Honing
(2003) provides suitable experimental data to do so.

The Rhythm Categorization Study of Desain
and Honing (2003)
Desain and Honing (2003) employed a novel paradigm where
participants were asked to categorize 66 different rhythm sequences by transcribing them into common music notation.
The sequences all lasted for one second and consisted of four
tone onsets and were therefore uniquely determined by the
three interonset intervals (IOI) between the tones. Two such

possible sequences are shown in figure 1a and 1b where a possible categorization of 1b could be ♩ ♩ (or 1-1-2 when written as an integer ratio). Any possible one second, four tone
rhythm sequence can be thought of as a point in a three dimensional triangular performance space that determines the
lengths of the three IOIs as shown in figure 1. The 66 rhythm
sequences were constructed so that they evenly covered the
area in the performance space with the constraint that no IOI
would be shorter than 153 ms. The location of these sequences in the performance space can be seen in figure 2b
where each circle marks the position of one of the 66 sequences.
In a first experiment, 29 highly trained musicians categorized the rhythm sequences and the result was that even
though the rhythms were performed on a more or less continuous scale, the participants tended to stick to a limited number of categories with 1-1-1 being the single most common.
Twelve categories, all categories considered, stood out as being the most common and the location in performance space
of these categories are shown in figure 3a. One participant
was presented with the 66 rhythm sequences multiple times
and as a measure of consistency the entropy was calculated
of her responses for each rhythm. These entropy values were
mapped on to the performance space and the resulting entropy
map is shown in figure 2a.
In a second experiment two meter conditions were added.
Duple meter versions of the rhythms were constructed by
prepending the rhythms with a repeated, one second long,
two sound beat, thus inducing a 2/4 meter feeling. Triple
meter versions of the rhythms were similarly constructed by
prepending a three sound beat instead. This resulted in three
different meter conditions: The original no meter condition,
a duple meter condition and a triple meter condition. Maps
over what categories the participants ascribed to the different
rhythms, similar to the map shown in figure 3a, were constructed (shown on p. 358 in Desain and Honing, 2003). A
main finding was that the participants’ categorization in the
no meter condition was significantly more similar to the participants’ categorization in the duple meter condition than in
the triple meter condition.
For the purpose of the current study, data from Desain and
Honing was downloaded from a web resource containing supplementary material1 . The data downloaded was the information regarding which of the twelve most common categories
was most often ascribed to each of the 66 rhythm sequences
for the no meter condition in experiment one and the duple
and triple meter conditions in experiment two. A data point
for a rhythm sequence was excluded if none of the twelve
most common categories was the most common for that specific rhythm. Information regarding the categorization entropy for the participant presented with the rhythms multiple
times was unfortunately not available from the web resource.
This information was retrieved manually from figure 2a.
1 http://www.mcg.uva.nl/categorization

1805

as:
SNR =

Figure 3: Categorization maps for (a) the experimental data
from Desain and Honing (2003, used with permission) and
(b) the resonance model. The transparent areas in (a) indicate areas where there was a large amount of disagreement
between the participants.

Resonance Theory and the Data of Desain and
Honing
It is possible to test the two predictions from resonance theory
concerning how rhythms are categorized by implementing a
resonance model that consists of an array of oscillators (as in
Large, 2000). We used the rhythm stimuli from Desain and
Honing (2003) as input to such a model and compared the
results with the experimental data using the methods outlined
below.
Prediction (1) implies that rhythm sequences resulting in
distinct states in a resonance model should be the sequences
that are categorized more consistently. In Desain and Honing’s data, a measure of consistency is the categorization entropy for the participant presented with the rhythm sequences
multiple times. The prediction is that this measure of consistency is correlated with a measure of distinctness of the
state of a resonance model. Signal-to-noise ratio is a common
measure of distinctness of a signal and a modified version of
this measure can be used to quantify the distinctness of the
state of a resonance model. For a resonance model that has
been given a rhythm sequence as input, the activation pattern
is first calculated according to equation (1). In this activation pattern, the signal As is defined as being the Ai with the
highest amplitude. The signal-to-noise ratio is then defined

As
∑ni=1 Ai

i 6= s

(2)

where the sum in the denominator is over the rest of the Ai
oscillator amplitudes. Notice that this measure of consistency
should be negatively correlated with the entropy measure of
Desain and Honing: As the signal gets weaker relative to
the noise, the entropy of the participants choices of category
should go up.
Prediction (2) implies that rhythm sequences resulting in
similar states when given as input to a resonance model
should be categorized similarly in an experimental task. A
resonance model does not directly produce a categorization
but this is not required for testing this prediction. It is possible to compare the resulting states of two rhythm sequences
by calculating the respective activation patterns and comparing these. A suitable similarity measure is given by considering the activation patterns as points in an n-dimensional
space, where n is the number of oscillators in the resonance
model, and then taking the Euclidean distance between these
two points, where a shorter distance corresponds to more similar states. Considering the twelve most common rhythm categories chosen by the participants in Desain and Honing’s
study as prototype categories, it is possible to use the rhythm
sequences corresponding to these categories to generate prototype activation patterns. For example, to generate the prototype activation pattern for the category 1-2-1 (as shown in
figure 4) the rhythm sequence with IOIs 0.25 s, 0.5 s and
0.25 s would be used as input to the resonance model. A
rhythm sequence’s activation pattern can then be compared
with these prototypes’ activation patterns and the prototype
category with the most similar activation pattern can be assigned to that rhythm sequence. In this way, all rhythm sequences can be assigned a category and these categories can
be compared with the categories selected by the participants
in Desain and Honing’s study. Specific hypotheses are then
that a resonance model categorization of the stimulus used
by Desain and Honing should be similar to the categorization
made by the participants in the no meter, duple meter and
triple meter conditions. Furthermore, since the participants’
categorizations of the rhythm sequences in the duple meter
condition was more similar than the triple meter condition to
the categorization in the no meter condition the same relation
should be present in the categories generated by the resonance
model.

The Setup of the Resonance Model
The resonance model was implemented in MATLAB2 using
the Nonlinear Time-Frequency Transformation Workbench
(Large and Velasco, in preparation). The model consisted of
145 Hopf oscillators, a type of oscillator that entrains to periodic input and where the amplitude of an oscillator depends
on that oscillator’s intrinsic frequency and the periodicities
2 http://www.mathworks.se/products/matlab/

1806

Figure 4: An example of an activation pattern generated by
feeding the resonance model a rhythm with IOIs 0.25 s, 0.5 s
and 0.25 s.
of the input. The differential equation of the Hopf oscillator
used is:


dz
βε|z|4
x
1
√ ·
√
= z α + iω +
+
(3)
2
dt
1 − ε|z|
1 − εx 1 − εz
α = −0.1,

β = −0.1,

ε = 0.5

where α is a damping term, β is an amplitude compression
factor and ε is a scale factor. The last term in equation (3)
is the resonant term, which is dependent on the stimulus x.
These parameter values and this specific formulation of the
Hopf oscillator were not chosen on the basis of any specific
theoretical considerations; many other configurations are possible and a more general form of the Hopf oscillator is derived
in Large, Almonte, and Velasco (2010). The oscillators’ intrisic frequencies were centred around 1 Hz with frequencies
logarithmically distributed from 0.25 Hz to 4 Hz. The method
used for creating activation patterns was that in equation (1)
with ts set to the time step corresponding to half the stimulus
length and te set to the last time step3 .
The 66 rhythm sequences from the no meter condition were
encoded and given as input to the model yielding 66 activation patterns. This was repeated for the rhythm sequences
from the duple and triple meter conditions. Additionally, the
rhythm sequences of the prototype categories were encoded
in the same way as the no meter condition sequences yielding
twelve prototype activation patterns.

Results
The signal-to-noise measure was calculated for all activation
patterns in the no meter condition and, as predicted, a negative correlation between Desain and Honing’s (2003) entropy measure of consistency and the signal-to-noise ratio
(Pearson product-moment correlation, r = −0.33, p = 0.006)
was found. The two measures of consistency are expected
3 The MATLAB code for the model and both input data and the
resulting output are available on request from the first author. The
code for the Nonlinear Time-Frequency Transformation Workbench
(Large and Velasco, in preparation) has not yet been publicly released and has to be requested separately.

to have a reverse relationship, that is, low entropy in the experimental data should indicate high consistency, while a low
signal-to-noise ratio in the simulated data should indicate low
consistency. A comparison between these two measure of
consistency is shown in figure 2. To facilitate comparison,
the colour scales have been matched so that red indicates
low consistency while blue indicates high consistency. The
measures of consistency are clearly comparable, showing the
same broad patterns in both the simulated (figure 2b) and experimental data (figure 2a).
The activation patterns for all the three meter conditions
were compared with the prototype activation patterns using
Euclidean distance as the similarity measure and each rhythm
sequence was assigned the category of the most similar prototype. A comparison with the categories assigned in the experimental task for the no meter condition is shown in figure 3.
It is clear that the categorizations to a large extent agree. The
1-1-1 category is the most common in both the experimental and the simulated categorizations and both categorizations
exhibit roughly convex category regions. A randomized permutation test4 also showed that the categorization generated
by the resonance model and the categorization from Desain
and Honing’s data was more similar than would be expected
by chance alone for all the three meter conditions. In the no
meter condition (shown in figure 3) the agreement was 71%
(p < 0.001) and in the duple and triple meter conditions 67%
(p < 0.001) and 61% (p < 0.001) respectively.
In the experimental data, the categorization of the duple
meter condition was more similar than the triple meter condition to the no meter condition and this was also the case for
the simulated categorizations. The agreement between the no
meter condition and the duple and triple meter conditions for
the simulated categorizations was calculated as being 77%
and 71% respectively with duple meter agreeing with the no
meter categorization in 6 percentage points more of the cases
(p = 0.045, one-tailed randomized permutation test).

Conclusions
Many models of categorical perception have been based on
neural networks and there exist several models of rhythm perception based on neural networks (Mozer, 1993; Miller, Scarborough, & Jones, 1992). We believe that using a dynamical
4 Randomized permutation tests (Ernst, 2004) were used to compare the categorization of the rhythm sequences from the behavioural data with the categorization from the resonance model.
Given two different categorizations of the 66 rhythms a similarity
score, is calculated as the number of rhythms that are given the same
category by both categorizations. In the cases when the most common categorization of a specific rhythm sequence in the behavioural
data is not one of the twelve prototype categories this rhythm sequence is excluded from further analysis. Next, all category labels
are randomly assigned to different rhythm sequences and a new similarity score is calculated. This is repeated 10,000 times, yielding a
randomized permutation distribution of similarity scores. A p-value
is then calculated as the probability of achieving the actual similarity
score, or a similarity score being more extreme, given the randomized distribution of similarity scores. The permutation tests were
two-tailed (calculated according to the method in Ernst, 2004) in all
cases except when noted.

1807

system of resonating oscillators provides a more principled
way of modelling such phenomena. By modelling rhythm
perception in such a system, we have shown that it is possible
to explain empirical findings of listeners’ categorical perception of rhythm. Our oscillator model has been able to accurately replicate the experimental data from Desain and Honing (2003).
An advantage of oscillator models is that they can be generalized to other kinds of categorical perception. Examples
from the domain of music are pitch perception and tonality
perception (Large, 2010). Oscillatory models are not confined to temporal processes and can be used for other modalities. The main importance of our model is perhaps that the
example of how oscillator models can be constructed for categorical rhythm perception can serve as inspiration for similar
models of other cognitive phenomena.

Acknowledgements
The authors gratefully acknowledges support from the Linnaeus environment Thinking in Time: Cognition, Communication and Learning (CCL), Swedish Research Council grant
number 349- 2007-8695.

References
Brochard, R., Abecasis, D., Potter, D., Ragot, R., & Drake, C.
(2003). The ”ticktock” of our internal clock : direct brain
evidence of subjective accents in isochronous sequences.
Psychological Science, 14(4), 362–366.
Damper, R., & Harnad, S. (2000). Neural network models
of categorical perception. Attention, Perception, & Psychophysics.
Desain, P., & Honing, H. (2003). The formation of rhythmic categories and metric priming. Perception, 32(3), 341–
365.
Eck, D. (2002). Finding downbeats with a relaxation oscillator. Psychological research, 66(1), 18–25.
Ernst, M. D. (2004). Permutation methods: a basis for exact
inference. Statistical Science, 19(4), 676–685.
Gärdenfors, P. (2000). Conceptual Spaces: The Geometry of
Thought. MIT press.
Harnad, S. (1990). Categorical Perception: The Groundwork
of Cognition. Cambridge University Press.
Honing, H. (2013). Structure and interpretation of rhythm in
music. In D. Deutsch (Ed.), The psychology of music (pp.
369–390). Elsevier.
Honing, H., Ladinig, O., Háden, G. P., & Winkler, I. (2009).
Is beat induction innate or learned? Probing emergent meter perception in adults and newborns using event-related
brain potentials. Annals of the New York Academy of Sciences, 1169, 93–6.
Jäger, G. (2010). Natural color categories are convex sets.
In Logic, language and meaning: 17th amsterdam colloquium, amsterdam. Springer.
Large, E. W. (1996). Modeling beat perception with a nonlinear oscillator. In Proceedings of the Eighteenth Annual Conference of the Cognitive Science Society: July

12-15, 1996, University of California, San Diego (p. 420).
Lawrence Erlbaum.
Large, E. W. (2000). On synchronizing movements to music.
Human Movement Science, 19(4), 527–566.
Large, E. W. (2010). Neurodynamics of music. In M. Riess
Jones, R. R. Fay, & A. N. Popper (Eds.), Music Perception
(pp. 201–231). Springer.
Large, E. W., Almonte, F. V., & Velasco, M. J. (2010). A
canonical model for gradient frequency neural networks.
Physica D: Nonlinear Phenomena, 239(12), 905–911.
Large, E. W., & Kelso, J. A. S. (2002). Tracking simple and
complex sequences. Psychological Research, 3–17.
Large, E. W., & Velasco, M. (n.d.). Nonlinear TimeFrequency Transformation Workbench: MATLAB software
for signal processing using nonlinear oscillators.
Miller, B., Scarborough, D., & Jones, J. (1992). On the perception of meter. Understanding Music with AI.
Mozer, M. (1993). Neural net architectures for temporal
sequence processing. In A. Weigend & N. Gershenfeld
(Eds.), Predicting the future and understanding the past.
(pp. 243–264). Addison-Wesley.
Nagumo, J., Arimoto, S., & Yoshizawa, S. (1962). An active
pulse transmission line simulating nerve axon. Proceedings
of the IRE, 50(10), 2061–2070.
Noorden, L. van, & Moelants, D. (1999). Resonance in the
perception of musical pulse. Journal of New Music Research, 28(1), 43–66.
Nozaradan, S., Peretz, I., Missal, M., & Mouraux, A. (2011).
Tagging the neuronal entrainment to beat and meter. The
Journal of Neuroscience : The Official Journal of the Society for Neuroscience, 31(28), 10234–40.
Phillips-Silver, J., Toiviainen, P., Gosselin, N., Piché, O.,
Nozaradan, S., Palmer, C., et al. (2011). Born to dance but
beat deaf: A new form of congenital amusia. Neuropsychologia, 49(5), 961–969.
Repp, B. H. (2005). Sensorimotor synchronization: A review
of the tapping literature. Psychonomic Bulletin & Review,
12(6), 969.
Schachner, A., Brady, T., Pepperberg, I., & Hauser, M.
(2009). Spontaneous motor entrainment to music in multiple vocal mimicking species. Current Biology, 19(10),
831–836.
Scheirer, E. D. (1998). Tempo and beat analysis of acoustic
musical signals. The Journal of the Acoustical Society of
America, 103(1), 588–601.
Snyder, J., & Krumhansl, C. L. (2001). Tapping to Ragtime:
Cues to Pulse Finding. Music Perception, 18(4), 455–489.
Toiviainen, P., & Snyder, J. (2003). Tapping to Bach:
Resonance-based modeling of pulse. Music Perception,
21(1), 43–80.
Wing, A., & Kristofferson, A. (1973). Response delays and
the timing of discrete motor responses. Attention, Perception, & Psychophysics, 14(1), 5–12.

1808

