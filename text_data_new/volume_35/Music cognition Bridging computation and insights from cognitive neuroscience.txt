UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Music cognition: Bridging computation and insights from cognitive neuroscience

Permalink
https://escholarship.org/uc/item/43p6n082

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Pearce, Marcus
Rohrmeier, Martin
Toiviainen, Petri
et al.

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Music cognition: Bridging computation and insights from cognitive neuroscience
Marcus Pearce (marcus.pearce@eecs.qmul.ac.uk)

Martin Rohrmeier (mr1@mit.edu)

Centre for Digital Music and Research Centre in Psychology,
Queen Mary, University of London, E1 4NS, UK.

MIT Intelligence Initiative, Department of Linguistics and
Philosophy, Massachusetts Institute of Technology,
Cambridge, MA, USA

Psyche Loui (ploui@bidmc.harvard.edu)

Edward Large (large@ccs.fau.edu)
Ji Chul Kim (kim@ccs.fau.edu)

Beth Israel Deaconess Medical Center and
Harvard Medical School, Boston, MA, USA.

Center for Complex Systems & Brain Sciences
Florida Atlantic University

Petri Toiviainen (petri.toiviainen@jyu.fi)

Elvira Brattico (brattico@mappi.helsinki.fi)

University of Jyväskylä, Finland

Aalto University, Finland

Keywords: Music cognition; cognitive neuroscience;
computational modelling; processing; prediction; grammar

Petri Toiviainen and Elvira Brattico
Decoding the musical brain during naturalistic
listening

Goals and Scope
In recent years, computational models have become an
increasingly important part both of cognitive science and
cognitive neuroscience. In tandem with these developments
neuroscientific and cognitive investigations of musical
experience and behaviour have been gathering pace. In this
context, music cognition constitutes a rich and challenging
area of cognitive science in which the processing of
complex, multi-dimensional temporal sequences can be
studied without interference of meaning or semantics (see
Pearce & Rohrmeier, 2012, for a review). Because of its
complexity and well-defined problem-space, computational
modelling of music witnessed a rapid growth of successful
higher-order modelling approaches. This workshop
investigates computational modelling as a bridge between
cognition and the brain, with a focus on understanding the
psychological mechanisms involved in perceiving and
producing music.
Many approaches have been taken to modelling the large
variety of different cognitive processes involved in music
perception and creation involving various modules of basic
structural processing, statistical learning, memory, as well as
motor, emotional and social cognitive processes. Recent
computational models range from hierarchical, rule-based
systems for representing harmonic movement inspired by
probabilistic grammars for language, through oscillator
based network models for modelling metrical and tonal
perception, to probabilistic methods derived from machine
learning for modelling dynamic learning and predictive
processing of style-specific musical structure. Turning to
cognitive neuroscience, recent years have seen increasing
interest in advanced computational modelling of EEG and
fMRI data used to distinguish brain regions responsible for
the processing of different aspects of music (e.g., rhythm,
pitch, timbre, harmony) and the functional connectivity
between them. The purpose of this symposium is to bring
together and display current research trends towards a
synthesis of these two research areas linking the parameters
and subcomponents of cognitive models of musical
processing to functional and anatomical properties of the
brain.

Encoding, or prediction of neural activation from
stimulus, is a common modeling approach in neuroscience.
In our recent neuroimaging study, we applied encoding to
predict brain activity during listening to different pieces of
music from an extensive set of musical features
computationally extracted from the pieces, and found
widespread brain activation, including auditory, limbic, and
motor areas (Alluri et al., Neuroimage, under review). With
such complex and distributed neural activation, evaluation
of different encoding models is not straightforward, because
the goodness of prediction is difficult to assess. Decoding,
or prediction of physical or perceived stimulus features from
the observed neural activation, has the potential benefit of a
more straightforward model evaluation because of easier
performance characterization in terms of, for instance,
correct classification rate.
In a series of experiments, our participants were
measured with functional magnetic resonance imaging
(fMRI) while they were listening to three different musical
pieces. Subsequently, musical features were computationally
extracted from the pieces, and continuous emotion ratings
were collected from the participants. For decoding, the
fMRI data were subjected to dimensionality reduction via
voxel selection and spatial subspace projection, and the
obtained projections were subsequently regressed against
the musical features or the emotion ratings. To avoid
overfitting, cross-validation was utilized. Different voxel
selection criteria and subspace projection dimensionalities
were used to find optimal prediction accuracy. The decoding
results and the challenges of the approach will be discussed
at the symposium.

Psyche Loui
Behavioral and DTI Studies on Normal and
Impaired Learning of Musical Structure
One of the central questions of cognitive science concerns
how humans acquire knowledge from exposure to stimuli in
the environment. In the context of music, knowledge

99

includes the structure of harmony and melody that govern
how musical pitches are combined. While people of all
cultures and ages show some knowledge of the structure of
their music, people with tone-deafness (also known as
congenital amusia) show a lack of behavioral sensitivity to
harmony and melody. Here we combine behavioral evidence
from subjective ratings, neuroimaging evidence from
Diffusion Tensor Imaging, and neuropsychological evidence
from tone-deaf individuals, to support the thesis that much
of what we know and love about music is acquired via
statistical sensitivity to the frequency and probability of
occurrence of events in the auditory environment. This
statistical learning mechanism relies on intact white matter
connectivity between temporal and frontal lobe regions, and
may subserve multiple auditory-motor functions including
language as well as music.

structural parsing and implicit learning to deal with these
syntactic features. This talk presents the picture emerging
from converging evidence of theoretical approaches, recent
experimental work and computational modeling.
Probabilistic models of musical syntactic processing based
on Hidden Markov Models and probabilistic context-free
grammars underpin that the inference of complex nonlocal
dependencies from mere exposure is plausible. They further
predict experimental data of musical tension and expectancy
showing that a variety of features of musical experience can
be modeled by such approaches.

Marcus Pearce
Expectation and Emotion in Music Perception:
Computational Modeling of Dynamic
Cognitive and Neural Processes

Edward Large and Ji Chul Kim
A Universal 'Grammar' for Music

The idea that aesthetic experience of music is dependent on
the confirmation and violation of expectation dates back at
least to Hanslick. Meyer (1957) further proposed that such
expectations depend on probabilistic models of musical
structure, acquired through exposure. However, until
recently such theories remained largely untested. Here we
present evidence corroborating these proposals and filling in
some of the details in terms of cognitive and neural
processing. First, we show that musical expectations elicited
in a range of musical styles result reflect probabilities
acquired through a process of statistical learning. Subjective
expectedness and uncertainty can be modeled dynamically
through time using the information-theoretic concepts of
information content and Shannon entropy respectively.
Second, we identify time-locked electrophysiological brain
responses to events differing in information content. Third,
we show that variations in information content lead to
distinct psychological and physiological emotional
responses elicited in a live concert of music for solo flute.
The results also indicate that expectations and emotion are
influenced by factors other than the musical structure such
as visual aspects of the performance. In summary this
research suggests that musical expectations rely on dynamic
probabilistic cognitive processing of musical structure,
supported by corresponding neural processes, and generates
characteristic physiological and psychological emotional
responses.

Since antiquity, science has asked whether mathematical
relationships among acoustic frequencies govern the
perception of musical relationships. Modern psychophysicists rejected this approach, citing evidence that the
auditory system performs a linear analysis of sound.
Cognitive psychologists have since relied primarily on
statistical learning to explain music cognition, despite
continued demonstrations of the importance of frequency
relationships. Today evidence is rapidly mounting that the
auditory system is highly nonlinear, inviting reevaluation of
the role of frequency in constraining in the perception of
music. Here, we present a dynamical systems analysis of
auditory nonlinearities that predicts substantive universals
in music perception and cognition. This approach explains
perceptual ratings of Hindustani raga not only by encultured
listeners, but also by listeners who were completely
unfamiliar with the music of North India. This evidence
suggests that universal properties of neural oscillation
explain cross-cultural invariants in the perception of tonal
music, implying neurodynamic constraints on the
acquisition of musical languages.

Martin Rohrmeier
Computational Models of Musical Syntax
In order to create the variety of our rich musical
experience, Western music employs a complex combination
of interwoven features of pitch, rhythm, metrical structure,
harmony and melody. Various computational models of
music processing are based on local (event-to-event)
processing of musical features (cf. Rohrmeier & Koelsch,
2012). On the other hand, a number of theoretical
approaches suggest that music involves recursive,
hierarchical structures organized in ways similar to
linguistic syntax. Further recent neurocognitive research
provides evidence indicating that musicians as well as
nonmusicians are sensitive to subtle long-distance violations
resulting from the underlying syntactic structure. These
insights suggest that musical processing is more complex
than previously assumed and involves rich mechanisms of

Moderators:
Marcus Pearce and Martin Rohrmeier
References
Pearce, M. T. & Rohrmeier, M. (2012). Music cognition and
the cognitive sciences. TopiCS in Cognitive Science, 4,
468-484.
Meyer, L. B. (1957). Meaning in music and information
theory. Journal of Aesthetics and Art Criticism, 412-424.
Rohrmeier, M., Koelsch, S. (2012). Predictive information
processing in music cognition. A critical review.
International Journal of Psychophysiology, 83 (2),
164-175.

100

