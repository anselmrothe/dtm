UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Movement Correlation as a Nonverbal Cue in the Judgment of Affiliation during Social
Interaction

Permalink
https://escholarship.org/uc/item/2jg627m8

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Latif, Nida
Barbosa, Adraino
Vatikiotis-Batesom, Eric
et al.

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Movement Correlation as a Nonverbal Cue in the Judgment of Affiliation during
Social Interaction
Nida Latif (n.latif@queensu.ca)1
Adriano V. Barbosa (adriano.vilela@gmail.com)3
Eric Vatikiotis-Bateson (evb@mail.ubc.ca)3
Monica S. Castelhano (monica.castelhano@queensu.ca)1
Kevin G. Munhall (kevin.munhall@queensu.ca)1,2
1

Department of Psychology, Queen’s University
Kingston, ON K7L 3N6 Canada
2
Department of Otolaryngology, Queen’s University
Kingston, ON K7L 3N6 Canada
3
Department of Linguistics, University of British Columbia
Vancouver, BC V6T 1Z4 Canada
Abstract
It has been demonstrated that brief exposure to behavioral
information is sufficient for making accurate social
judgments. Movement coordination during social interaction,
is one potential cue. Although coordination between
individuals has been identified, our ability to perceive it when
making judgments regarding affiliation (friends vs. strangers)
is unknown. In the present studies, we investigated how
correlated movement contributes to observers’ accuracy when
judging affiliation. Using correlation map analysis to quantify
coordination, we showed that individuals familiar with each
other correlated their movements more frequently. Observers
were able to use coordination as a cue, but only when the
information presented was restricted to movement related to
speech (i.e. while only viewing faces). These results suggest
that observed movement coordination is influenced by
speech-related movements. We suggest that social perception
is multi-faceted and cues may be prioritized differentially
based on availability.
Keywords: social perception; social
conversation; movement correlation

interaction

&

Introduction
Humans are constantly immersed in social interaction and
conversation. It is not surprising that we have mechanisms
to facilitate these interactions, both while engaging in and
observing them. One mechanism is our ability to create a
rich representation of social cues from very brief exposures
as short as a few seconds. These short exposures or “thin
slices” of behavioral and linguistic information are
sufficient for making remarkably accurate judgments
regarding social situations. Research has demonstrated great
accuracy in judgments in a variety of domains including
personality, social status and mental states (Ambady &
Rosenthal, 1993). However, little known about how specific
cues contribute to our accuracy in social perception.
Understanding how we use available social cues is
important to human behavior because monitoring others’
intentions and actions is a prerequisite for modulating and
guiding our own behavior, interactions, and relationship
formation (Foulsham et al., 2010). In addition, using social
cues is often impaired in many social and psychological
disorders such as autism spectrum disorder and

schizophrenia leading to difficulty in successful interaction
(Klin et al., 2002).
Previous research has looked at specific motion patterns
that occur during interpersonal communication. Studies
have demonstrated that individuals unintentionally
synchronize and coordinate their movements and converge
in linguistic properties during conversation (Richardson,
Dale & Shockley, 2008; Richardson & Dale, 2005;
Chartrand & van Baaren, 2009; Pardo, 2006). This has been
shown with different attributes of conversation such as
facial expression, postures and accents (Capella & Planalp,
1981; McHugo, Lanzetta, Sullivan, Masters & Englis,
1985). Individuals even unintentionally coordinate their
movements without visual information from their partner
(Shockley et al., 2003). Coordination without visual
information suggests that convergence in behavior can be
directly influenced by vocal information exchanged during
conversation. Further, studies examining social-cognitive
variables in convergence have shown through subjective
observation that individuals with good rapport coordinate
their movements (Grahe & Bernieri, 1999). Also, friends
converge more in linguistic properties than strangers (Dunne
& Ng, 2002). Coordination may occur because of inherent
biological and behavioral rhythms as well as a coupling of
conversation-engaged individuals’ mental representation of
their perceptions of each other (Richardson & Dale, 2005;
Meltzoff & Prinz, 2002).
Although the presence of convergence in nonverbal and
linguistic properties has been examined, our ability to
perceive this convergence has not been investigated. In
particular, the contribution of correlated movements has not
been examined objectively in the perception of affiliation
(i.e. whether individuals engaged in conversation are friends
or strangers). The current studies use movement and
coordination
quantification
methods
to
examine
convergence between interacting individuals. These
methods allowed us to investigate whether the amount of
coordination differs as a function of affiliation and whether
it contributes to the accuracy of affiliation judgments made
by an external observer.

2832

Experiment 1: Movement Analysis
This experiment investigated if there was an observable
variation in coordination between individuals as a result of
known affiliation differences.

Methods
Participants Sixty-two undergraduates (Mean Age = 21.2,
36 females) from Queen’s University were recruited in pairs
to engage in video-recorded conversation. Thirty-one dyads
were either recruited as friend pairs or were experimentally
paired. Conversations from 15 same gender friend pairs (10
female), 12 same gender stranger pairs (6 female) and two
mixed friend and stranger pairs were video recorded.
Stimulus Collection Stimuli were collected by video
recording, unstructured conversation between two
participants, using a single camera aimed to capture both
individuals1. Individuals sat on fixed chairs and were left to
converse without an experimenter present for approximately
10 minutes.
Stimulus Analysis An algorithm computing spatiotemporal
coordination was used on the video clips. In these
experiments, only the visual information was examined
using this algorithm. The algorithm developed by Barbosa
et al. (2012) first computes optical flow using a standard
image processing technique where velocities of brightness
patterns in an image are calculated within a region of pixels
and summed to give a global value for a particular cluster of
pixels (Horn & Shunck, 1981). Then, a correlation analysis
is used to compute instantaneous correlation between
movement signals within a specified region of interest.
Using the optical flow analysis, the Barbosa et al.
(2012) algorithm computes total motion in an identified
region of interest by summing the optical flow in that
region. Regions of interest were drawn around each
individual engaged in conversation, for a gross estimate of
their total body motion. Correlation Map Analysis (CMA)
was then used to quantify the coordination between the two
speakers' movements. A key characteristic of CMA is that it
computes the correlation between a pair of signals as a
function of both time and the lag between the signals. This
not only allows us to characterize the correlation throughout
the duration of the signals, but also to capture correlations
between events that are not perfectly aligned in time.
Therefore, CMA is able to capture coordination between
signals, where events in the signals are related to each other
but do not necessarily happen at exactly the same time;
rather, they fluctuate around some specific lag between the
signals as the signals evolve through time. Capturing
coordination at a time lag allows for alternating behavior,
such as that seen in social interaction, to be captured. This
kind of mechanism is ideal for biological rhythms that are
rarely synchronous and allows for convergence in social
1

Although audio was recorded, it was not analyzed in these
experiments. Only visual information was examined.

interaction to be quantified (Winfree, 1980, Barbosa et al,
2012).
Correlation Data Analysis Average distributions of
correlation were created for each of the friends and strangers
groups. For statistical comparison, a resampling nonparametric technique was used to create null distributions.
The correlations for friends and strangers were compared to
a null distribution. The distributions were simplified to look
only at the positive lags (0 - +0.5s). Because of the
rhythmical structure of conversation, the positive and
negative lags tend to be redundant and we included only the
positive lags in all analyses. This resulted in distributions
looking at 16 lags in total, including 0 lag (i.e. completely
synchronous correlation; Frame rate = 30fps; 0.5s = 15
frames plus one 0 lag frame).
Motion Magnitude Analysis To control for the magnitude
of motion when looking at the correlation, distributions of
all motion magnitudes from the friends and strangers pairs
were generated. The two distributions were compared to
determine differences in total motion.

Results
Correlation Analysis The probability distributions of each
of the friend and stranger correlations at each lag were
compared with a null distribution representing the
correlations computed between the motions of all subjects
who were not actually in a conversation together. The
means for the correlations at lags closest to synchronous
were significantly different (p<0.05) from the null
distribution for both friends and strangers. These results are
displayed in Figure 1 where the first half of both the friend
and stranger distributions (first 8 lags/frames) displays
higher mean correlations than observed for the random
pairings. Correlations significantly different at time-points
closest to synchronous indicates that individuals engaged in
conversation are highly sensitive to their partner’s
movements and coordination occurs within moments of the
movement first being initiated. Thus, the data indicate that
engaging in a face-to-face conversation produces
correlations greatly exceeding what would be produced by
chance pairings of motion signals.
To determine how correlation differed based on
affiliation, the friends and strangers distributions were
subtracted from each other to create a difference
distribution. This was compared with two null difference
distributions: One looked at correlation differences between
randomly paired individual’s movements and randomly
assigned affiliation categories and the second distribution
looked at correlation differences between real pairs that
actually conversed but who were arbitrarily categorized as a
particular affiliation for this analysis. Both the overall
difference as well as the mean difference per lag was
significantly different when compared to both null
distributions where friends had more correlated events than

2833

strangers. (p<0.05) (See Figure 3). Although comparisons to
both these null distributions is interesting in that they
suggest that friends’ and strangers’ conversation contain
content unique to their affiliation categorization, the
comparison to the null containing real pairs is more
informative. The real-pair null contains motion that can be
attributed to conversational motion in general as opposed to
random motion. These results support our hypothesis that
affiliation results in correlational variation, and that friends
correlate more frequently than strangers.

Figure 3. (Top to Bottom) Three-dimensional average correlation
difference distributions for friends-strangers, random-pair
subtractions and real-pair random subtractions. Lighter colors
indicate higher correlation differences along the x-axis with height
indicating frequency of events. Lag counts (in frames) indicate 16
temporal points between 0 and 0.5s where average correlation was
computed. Greater positive peaks indicate more correlated events
for friends in comparison to strangers.

than strangers. This supported our hypothesis and previous
studies that indicated that familiarity and good rapport
resulted in linguistic and behavioral coordination. The
correlation data from this study were used to identify stimuli
for a perceptual judgment task in Experiment 2.

Figure 2. (Top to Bottom) Distributions of average correlation for
the null, friend and strangers. Lighter colors indicate larger
proportion of events occurring at correlation values plotted along
the x-axis. Lag counts (in frames) indicate 16 temporal points
between 0 and 0.5s where average correlation was computed.
Here, both friends and strangers had correlation value greater than
the null but only significant in the first few frames.

Experiment 2: Perceptual Judgment

Motion Magnitude Analysis A distribution of magnitudes
of motion for the friends and strangers was created to
determine any differences in motion present within the
groups. Results indicated that friend pairs contained more
motion (Mean=0.88 pixels/frame, SE=5.79e-04) than
stranger pairs (Mean=0.84 pixels/frame, SE=5.38e-04). This
analysis was carried in preparation for Experiment 2. We
wanted to be able to control the amount of motion presented
to observers making perceptual judgments so that judgments
about accuracy were not being affected by motion
differences.
Results from Experiment 1 demonstrated in a quantified
manner that correlation was an inherent part of
conversational movement and that friends coordinated more

The previous experiment demonstrated that affiliation led to
differences in movement correlation. This experiment
investigated whether observers were attuned to the
correlational differences and if perception of correlation was
influencing accuracy of affiliation judgments. In this study,
we varied amount of correlation while controlling for the
amount of motion in the perception of thin-slices of
conversation

Methods
Stimuli The analysis of motion magnitudes from
Experiment 1 was used to control the amount of motion so
that differences in perception of clips could be attributed to
correlation differences rather than confounded by motion

2834

differences. Clips were selected from a window that was
centered at half a standard deviation around the mean of the
sum of the motion distributions for friends and strangers.
Clips and their corresponding correlation values were
extracted if they were contained within a 5s continuous2 clip
that contained average motion from within our defined
thresholds. Correlations were re-computed using the same
procedure as Experiment 1 for only those clips that were
controlled for motion magnitude to ensure that the
friend/stranger correlation results were true for our
perception stimuli.
All possible clips were sorted from lowest to highest
average correlation. Six of the lowest and six of the highest
correlated clips were selected for each of the friends and
strangers groups (n=24 clips) such that each conversing pair
was only presented once. The final clips contained ten same
gender friend pairs, ten same gender stranger pairs and two
mixed-gender pairs for each group.
Procedure Twenty undergraduates (Mean age = 20.8, 16
females) from Queen’s University participated for monetary
compensation. A within-subjects design was used where all
participants viewed both high and low correlated friend and
stranger clips. The 24 five-second clips were presented and
participants were asked to perform a social judgment rating
using a Likert scale. On a scale of 1-7, participants indicated
whether the two individuals engaged in conversation had
just met (1) or were friends (7). Following the experiment,
each participant was asked to record the kind of information
they used to make their judgments.

Results
Correlation Analysis Average correlations and correlation
difference patterns seen in Experiment 1 were also observed
in the 24 stimulus clips, confirming that the restricted
magnitude of motion was not influencing correlations.
Social Perception Accuracy The accuracy of affiliation
perception was determined by computing the average score
for all videos presented as a function of their correlation and
affiliation. Figure 3 displays the perceptual rating results. A
factorial ANOVA was performed and results indicated that
although participants could accurately discriminate between
friends and strangers (F(1,20)=4.28, p=0.05), correlation did
not seem to affect perceptual judgments. In addition, scores
were analyzed using an extreme groups analysis (Preacher,
Rucker, MacCallum & Nicewander, 2005) where neutral
responses were eliminated and a factorial ANOVA was

2
Continuous was defined as a clip with no sections longer than
0.25s where range of motion magnitude did not fall within motion
criteria. This 0.25s buffer was used to accommodate naturally
oscillating motion magnitudes.

*

Error Bars=SE

Figure 3. Average score for affiliation rating. A greater score
represents a preference towards a judgment of ‘friends’ and a
lower score represents preference towards a ‘strangers’ rating.

performed. Results demonstrated the same effect as the
simple analysis.
The perceptual judgment results in this experiment
demonstrated that observers could clearly make affiliation
judgments. However, our results showed no evidence that
degree of correlation between pairs influenced perceptual
judgments. Subjective responses of reported cues used by
participants indicated that subtle movements such as those
of the hands and the mouth were given precedence. A high
correlation between speech and face/head motion related to
speech has been demonstrated in previous studies (Barbosa
et al, 2008); perhaps these smaller gestures of speech were
not contributing to our correlation measures as much as
larger body motions. Experiment 3 was conducted to
investigate whether the correlational structure of smaller
speech related movements, such as face/head motion, might
better account for the perceptual data.

Experiment 3: Selected Perceptual Judgment
In this experiment, we looked at whether an observer’s
ability to make perceptual judgments of affiliation altered
when the social information presented was restricted to
more subtle, speech related correlations within the face/head
region.
Methods
Stimuli All video recordings from Experiment 1 were
cropped at participants’ shoulders to include only head and
facial movement. Correlation and motion analysis was
performed in the same manner as Experiment 1. Motion
thresholds and clip selection criteria were created using the
same procedure as Experiment 2. Twenty-four clips were
selected to include 12 same gender friends and 12 same
gender strangers.3 These included six of the highest and six
of the lowest correlated clips in each category.
3

Mixed pairs were eliminated due to factors related to past
relationship studies showing that they are not perceived in the
same manner as same-gender dyads.

2835

Procedure Twenty undergraduates from Queen’s
University (Mean Age=20.1, 16 females) participated in this
experiment. The perceptual judgment task used the same
procedure as that used in Experiment 2.

Results
Correlation Analysis Overall, average correlations and
correlation differences reflected the same pattern as
Experiments 1 and 2 confirming that movement correlation
was influenced by affiliation, independent of amount of
motion.
Social Perception Accuracy Analysis of affiliation
perception accuracy was performed as in Experiment 2.
Average score for all videos as a function of correlation and
affiliation was computed and a factorial ANOVA was
performed. Results showed that there was a significant
effect of correlation indicating observers provided a higher
proportion of ‘friends’ responses for highly correlated clips
(F(1,20)=7.78, p=0.01). There was no significant effect for
affiliation indicating that scores were not dependent on the
true affiliation between conversing individuals. Results are
presented in Figure 4. Observers were more likely to
misperceive high-correlation pairs as friends and lowcorrelation individuals as strangers. These results
demonstrated that movement correlation was a significantly
influencing cue when perceiving subtle, speech-related
movement.

*

*

Error Bars=SE

Figure 5. Average score for affiliation rating when viewing only
faces. A greater score represents a preference towards a judgment
of ‘friends’ and a lower score represents preference towards a
‘strangers’ rating.

General Discussion
In these studies, we were interested in examining how
affiliation between two individuals resulted in variation in
movement and coordination. Additionally, we were
interested in determining whether this variation influenced
the accuracy of observers making rapid judgments regarding
that affiliation. Based on previous studies on convergence in
linguistic and behavioral properties, we predicted that
familiarity would result in greater coordination which would
influence judgments of affiliation by external observers.

The results of these studies demonstrated that there was
indeed variation in motion and coordination resulting from
affiliation. In general, movement correlation was present
during social interaction, regardless of affiliation, although
higher correlations were present for friend pairs. This was
supported by studies that suggest that we interactively align
our representations of conversation content (Garrod &
Pickering, 2004). But, is this correlation used as information
when making social judgments?
The observation that participants were not sensitive to the
correlation differences presented in Experiment 2 suggests
that other cues in the full body stimuli such as static postural
cues as well as motion cues might have influenced the way
participants attributed affiliation. The correlational structure
of the larger body movements used to select stimuli clearly
was not the major determinant of participant responses.
Observers reported that they prioritized more subtle
movements related to speech when producing their
judgments. Experiment 3 tested this by restricting visible
motion to the head and face area to minimize the
contribution of other possible cues and showed that
coordination was a determinant of affiliation judgments.
Observing a clear decrease in accuracy by eliciting use of
coordination cues indicates that integration of many cues,
including movement correlation, contributes to our
remarkable ability to make accurate social judgments.
These studies provided us with two important
conclusions regarding movement coordination in social
perception: 1) Perception of unintentional coordination
observed during social interaction in previous studies is
directly influenced by speech-related movement and 2)
Multiple factors contribute to social perception however,
observers can use coordination as their basis of their
affiliation judgments.
Previous studies have shown individuals engaged in
conversation become mutually entrained in their movements
and this coordination persists even when individuals are
interacting verbally without visual input from the other
individual. Even when facing an individual with whom
participants were not conversing, coordinated movements
persisted with the direct conversation partner (Shockley et
al., 2003). These findings can be explained by the fact that
speech-related movement of the head and face is directly
correlated with the auditory signal of speech (Barbosa et al.,
2008). Our studies have demonstrated that coordination
occurred between individuals actively involved in the
conversation but also that third-party observers were
sensitive to the speech-related correlation between talkers.
The complete explanation for participants’ performance in
Experiment 2 and 3 warrants further research. We know that
full body information provides additional information
contributing to greater accuracy but the exact nature of this
information has yet to be identified.
Human communication provides a rich information set
for making judgments and many cues can contribute to
perceptual decisions. The perceptual strength of cues will
vary with the context and the observer’s history with a

2836

judgment and the manner in which multiple social cues are
integrated is still unknown. As in the study of the general
visual world (Gibson, 1968), we need to identify potential
sources of information in the social world.
Future work examining where observers visually fixate
when making affiliation judgments is necessary since the
distribution of attention will be a window into the
perceptual cues observers use. Information about the
allocation of attention may explain individual differences in
performance as well as accuracy differences in different
social contexts.
These studies aimed to investigate social perception in a
less arbitrary and more objective manner. We used motoric
correlation to address how judgments of affiliation could be
affected by nonverbal factors. We demonstrated that
affiliation influenced coordination of movement during
social interaction. Further, we showed that observers used
this correlation information as a cue, at the expense of
accuracy, when making judgments but only when the rich
social information set present in human communication was
restricted. This broad area of research will continue
informing us about our sensitivity to information used for
the successful social interactions we encounter everyday.

Acknowledgments
This research was supported by funding from the Natural
Sciences and Engineering Research Council of Canada, the
Brian R. Shelton Fellowship and Queen’s University.

References
Ambady, N. & Rosenthal, R. (1993). Half a minute:
Predicting teacher evaluations from thin slices of
nonverbal behavior and physical attractiveness. Journal
of Personality and Social Psychology, 64, 431-441.
Barbosa, A. V., Dechaine, R.-M., Vatikiotis-Bateson, E., &
Yehia, H. C. (2012). Quantifying time-varying
coordination of multimodal speech signals using
correlation map analysis. The Journal of the Acoustical
Society of America, 131(3), 2162-2172.
Barbosa, A., Yehia , H. & Vatikiotis-Bateson, E. (2008).
Algorithm for computing spatiotemporal coordination.
In Proceedings of the International Conference
on Auditory-Visual Speech Processing: Moreton Island,
Australia
Capella, J. & Planalp, S. (1981). Talk and silence sequences
in informal conversations III: Interspeaker influence.
Human Communication Research, 7(2), 117-132.
Chartrand, T.L. & van Baaren, R. (2009). Human mimicry.
Advances in Experimental Psychology, 42, 219-274.
Dunne, M. & Ng, S (1994). Simultaneous speech in small
group conversation: All-together-now and One-at-atime? Journal of Language and Social Psychology,
13(45), 45-69.
Foulsham, T. & Underwood, G. (2008). What can saliency
models predict about eye movements? Spatial and

sequential aspects of fixations during encoding and
recognition. Journal of Vision, 8(2), 1-17.
Gibson, J.J. (1968). What gives rise to the perception of
motion? Psychological Review, 75(5), 335-346.
Grahe, J.E. & Bernieri, J. (1999). The importance of
nonverbal cues in judging rapport. Journal of Nonverbal
Behavior, 23(4), 253-269.
Horn, B.K.P. & Schunck, B.G. (1981). Determining optical
flow. Artifical Intelligence, 17, 185-203.
Klin, A., Jones, W., Schultz, R., Volkmar, F., & Cohen, D.
(2002). Visual fixation patterns during viewing of
naturalistic social situations as predictors of social
competence in individuals with autism. Archives of
General Psychiatry, 59, 809-816.
McHugo, G.J, Lanzetta, J.T., Sullivan, D.G., Masters, R.D.,
& Englis, B.G. (1985). Emotional reactions to a political
leader’s expressive displays. Journal of Personality and
Social Psychology, 49(6), 1513-1529.
Meltzoff A.N. & Prinz, W. (2002) The Imitative Mind:
Development, Evolution and Brain Bases, Cambridge,
United Kingdom: Oxford University Press.
Pardo, J.S. (2006). On phonetic convergence during
conversational interaction. Journal of the Acoustical
Society of America, 119, 2382-2393.
Richardson, M. Dale, R. & Shockley, K. (2008). Synchrony
and swing in conversation: coordination, temporal
dynamics and communication. In I. Wachsmuth, M.
Lenzen, G. Knoblich (eds), Embodied Communication
in Humans and Machines. (pp. 75-93). Oxford, United
Kingdom: Oxford University Press.
Richardson, D.C & Dale, R. (2005). Looking to understand:
The coupling between speakers’ and listeners’ eye
movements and its relationship to discourse
comprehension. Cognitive Science, 29, 1045-1060.
Shockley, K. Santana, M.V. & Fowler, C.V. (2003). Mutual
interpersonal postural constraints are involved in
cooperative conversation. Journal of Experimental
Psychology: Human Perception and Performance, 29,
326-332.
Winfree, A.T. (1980). The geometry of biological time.
Springer: New York.

2837

