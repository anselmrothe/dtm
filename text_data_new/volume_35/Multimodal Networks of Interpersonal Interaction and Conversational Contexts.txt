UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Multimodal Networks of Interpersonal Interaction and Conversational Contexts

Permalink
https://escholarship.org/uc/item/88v077x9

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Paxton, Alexandra
Dale, Rick

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Multimodal Networks of Interpersonal Interaction and Conversational Contexts
Alexandra Paxton (paxton.alexandra@gmail.com)
Rick Dale (rdale@ucmerced.edu)
Cognitive and Information Sciences, University of California, Merced
Merced, CA 95341 USA
Abstract
In interpersonal interaction, the terms synchrony or alignment
refer to the way in which communication channels like speech
or body movement become intertwined over time, both across
interlocutors and within a single individual. A recent trend in
alignment research has targeted multimodal alignment,
exploring how various communication channels affect one
another over time (e.g., Louwerse et al., 2012). While existing
research has made significant progress in mapping
multimodal alignment during task-based or positively
valenced interactions, little is known about the dynamics of
multimodal alignment during conflict. We visualize
multimodal alignment during naturalistic affiliative and
argumentative interactions as networks based on analyses of
body movement and speech. Broadly, we find that
conversational contexts strongly impact the ways in which
interlocutors’ movement and speech systems self-organize
interpersonally and intrapersonally.
Keywords: alignment; conflict; conversation; interaction;
movement; network; speech; synchrony

Introduction
Interpersonal communication is a multimodal activity.
Conversation
incorporates
multiple
channels
of
communication that enrich the interaction, like hand
gestures, facial expressions, posture, and speech. To
effectively communicate with one another, interlocutors cue
in to each of these channels simultaneously, often without
realizing the importance placed on each of them.
Considerable work has surveyed multimodal qualities of
interaction (e.g., Norris, 2004). However, in general,
experimental study of interpersonal communication in
cognitive science tends to target single behavioral channels.
This has led to significant advances in our understanding of
these specific channels, but there is still much work to be
done in investigating the connections among them.
Continued multimodal research will likely yield
interesting—and possibly unexpected—relationships among
channels that have been extensively explored unimodally.
For this reason, the current research explores multimodal
alignment situated within different conversational contexts.
Specifically, the present research examines interpersonal
communication through alignment dynamics. Research on
interpersonal alignment focuses on how affect, behavior,
and cognition of interacting individuals affect one another
over time. Over the past several decades, researchers have
explored interpersonal alignment over a range of channels,
from movement (e.g., Richardson et al., 2007) to speech
(e.g., Garrod & Pickering, 2004) to cognition (e.g., Brennan,
Galati, & Kuhlen, 2010). As aforementioned, the majority

of this work centers on one or two behavioral channels, but
a growing body of literature has begun to investigate how
multiple channels align during communication (e.g.,
Louwerse, Dale, Bard, & Jeuniaux, 2012).
A distinction can be drawn between alignment in a
general sense and synchrony.1 We use the term alignment to
refer broadly to the concept that individuals, over time,
change their affect, behavior, and cognition as a direct result
of their interaction with another individual. This umbrella
term encompasses everything from mimicry, in which
individuals are performing highly similar behaviors to their
interaction partner (e.g., Chartrand & Bargh, 1999), to more
complementary behavior patterns like synergy (e.g., Riley,
Richardson, Shockley, & Ramenzoni, 2011). Synchrony, on
the other hand, can be considered a specific pattern of
alignment and refers exclusively to the in-phase entrainment
of behavior or communication channels.
In the spirit of intrapersonal alignment research spanning
the last several decades (e.g., Haken, Kelso, & Bunz, 1985),
the current research explores the dynamics of interpersonal
alignment as a self-organizing property of human
interaction. The present research focuses specifically on two
channels of communication—speech and body movement—
and the ways in which these channels are affected by each
other, by conversation partners, and by the conversational
context. If it is true that human interaction self-organizes
around, for example, conversational goals, we should expect
to see multimodal alignment patterns changing across these
different contexts.

Body Movement and Speech Alignment
Previous work on speech and body movement demonstrates
rich interactivity between and within individuals. Some of
the earliest work in alignment research presents evidence for
interpersonal and intrapersonal multimodal alignment
between movement and speech channels (Condon &
Ogston, 1966). Since then, research has continued to explore
intrapersonal alignment, both in speech (Reitter, Moore, &
Keller, 2010) and movement (Beek, Peper, & Daffertshofer,
2002). However, body movement and speech have both
been studied extensively within the interpersonal alignment
literature as well, generally within affect-neutral, positively
valenced, or task-oriented settings.
Studies of body movement alignment have spanned a
wide variety of behaviors, including gesture (Bernieri &
1

These specific terminologies are laid out here for the purpose
of the present paper, rather than trying to resolve the emerging
terminological debate within the field.

1121

Rosenthal, 1991), stepping (Miles, Griffiths, Richardson, &
Macrae, 2010), and overall levels of body movement
(Paxton & Dale, in press). Often, work on body movement
alignment incorporates elements of social psychology, like
investigating how interpersonal alignment affects liking
(Chartrand & Bargh, 1999). Broadly speaking, these
findings generally cast the phenomenon as a pervasive and
relatively automatic process that can be enhanced with
liking or rapport. However, limited research suggests that
higher-level social factors may inhibit bodily alignment
(Miles et al., 2010; Paxton & Dale, under revision).
Individuals also align over numerous measures of speech.
Over time, interacting individuals have been shown to use
more similar acoustic features (Kousidis & Dorran, 2008),
sentence structures (Cleland & Pickering, 2003), and even
respiratory patterns (McFarland, 2001). The tendency
toward alignment during interaction is so powerful that
individuals even align to simulated partners (Krämer, Kopp,
Becker-Asano, & Sommer, 2012). These and related
findings of interpersonal alignment in speech have led some
to suggest that this is an automatic tendency driven in part
by shared cognitive representations (Brennan et al., 2010).
These past findings point to a distinct temporal structure
of speech and movement during interaction. Recently,
researchers have begun to emphasize the importance of
investigating interpersonal multimodal alignment on a large
scale (e.g., Delaherche & Chetouani, 2010; Louwerse et al.,
2012). These questions allow researchers to more fully
understand the complex, interdependent structure of
communication channels during interaction.

Dynamics of Interpersonal Alignment
Mechanistic models of body mechanics (e.g., interlimb
coordination; Haken et al., 1985) have influenced recent
work on the dynamics in interpersonal interaction (e.g.,
Miles et al., 2010). Researchers have begun to explore the
forms and functions of alignment, going beyond earlier
studies simply investigating its existence. Such work is
dedicated to exploring the time course of alignment with the
belief that—like many other phenomena—alignment is
neither static nor uniform across contexts.
Like the work discussed above, research on interaction
dynamics has also focused on speech and movement
channels. From gaze patterns and postural sway (Shockley,
Richardson, & Dale, 2009) to speech production (Tilsen,
2009), researchers have found support for dynamical
interpersonal and intrapersonal alignment, both unimodal
and multimodal. Individuals’ patterns of alignment change
with task demands (Sebanz, Bekkering, & Knoblich, 2006),
social context (Miles, Lumsden, Richardson, & Macrae,
2011), and even physical environment (Richardson et al.,
2007), providing further support for claims of context
dependence in alignment (Riley et al., 2011).

within individuals, and recent trends have begun to situate
this alignment within the context of multimodal interaction.
Yet, so far, the dynamics of multimodality in alignment
have remained relatively unexplored. Moreover, research on
alignment thus far has sampled only a small percentage of
the total space of human communicative contexts, focusing
primarily on task completion or friendly interactions.
Our primary goal in the present study is to add to the
growing literature on the dynamics of multimodal
alignment. We aim to extend various elements of previous
findings on unimodal and multimodal alignment, both interand intrapersonally. The present study explores participants’
speech patterns and their relation to body movement with
three initial hypotheses. First, we hypothesize that
individuals will align to one another’s speech patterns but
will not demonstrate in-phase synchrony of speech, due to
the natural constraints of turn-taking. Second, we anticipate
that individuals will exhibit multimodal intrapersonal
synchrony, tending to move and speak at the same time.
Third, although we anticipate that individuals will be most
likely to move while speaking (and the reverse), we expect
that analyses will reveal some evidence for interpersonal
multimodal alignment (e.g., due to nodding).
The current project extends previous work on multimodal
alignment further by situating the research within different
conversational contexts, namely affiliation and argument.
As part of a larger line of research investigating alignment
in various contexts, the present research brings a focus on
asymmetric contexts—interactions in which individuals
have conflicting, differing, or opposing goals—to bear on
questions of multimodal alignment. Previous research has
demonstrated that conflict significantly decreases levels of
interpersonal bodily synchrony (Paxton & Dale, under
revision). We continue to explore alignment during conflict
in the present project. Compared to non-asymmetric
contexts, we anticipate that argument will affect alignment
in several ways: first, that individuals will demonstrate a
more rigid turn-taking structure (possibly, e.g., to satisfy
implicit social demands for reciprocity); second, that levels
of intrapersonal multimodal synchrony will remain
consistent; and third, that levels of interpersonal multimodal
alignment will decrease. This pattern of results—balancing
stable conversational structures with sensitivity to
contextual factors—would reinforce claims of contextdependent, emergent properties of human interactions.
In addition to analyzing these data, the present study also
hopes to begin work towards descriptive models of
interpersonal and intrapersonal multimodal and unimodal
alignment in different conversational contexts. After
presenting our analyses, we highlight our findings in
visualization networks of multimodal alignment dynamics.

Method
Corpus

The Present Study
Previous work has pointed to distinct patterns of
organization of speech and body movement between and

The data presented here were collected by the authors as
part of a larger corpus comparing interpersonal alignment

1122

during argument and affiliation. The corpus comprised over
35 naïve participant dyads engaged in different
conversational settings, collected from the University of
Memphis and the University of California, Merced. As a
further exploratory analysis building on previous findings
(Paxton & Dale, in press; Paxton & Dale, under revision),
the present analyses were performed on a subset of the
participants from the University of California, Merced,
based on uniformity of experimental conditions. The audio
data had not yet been analyzed, separately or in conjunction
with body movement.

Participants
24 undergraduate participants (mean age=20.14 years) were
recruited as 12 dyads (6 female, 6 mixed-sex) through the
school’s online subject pool system. Participants signed up
independently and were unable to see their partner’s identity
beforehand. Only one dyad reported having known one
another prior to participation. One mixed-sex dyad was
dropped from present analyses because their opinions were
too similar to achieve any argument during the experiment.

opposing views (e.g., one pro-life, the other pro-choice) and
for which both participants indicated strong feelings was
chosen. Up to two additional argumentative prompts were
chosen using the same criteria and were given to
participants if they could not continue the argumentative
conversation on the first topic for the entire time.

Materials
Movement data were collected automatically using a framedifferencing method (FDM; Paxton & Dale, in press).
Participants sat facing one another during their
conversations and were captured in profile in a single frame
on a high-definition camcorder (Canon VIXIA HF M31).3
The videos were downsampled at 8Hz to a series of still
frames. The FDM tracked movement by registering changes
in pixels across frames (see Figure 1 for toy visualization)
and applying a filter to remove extraneous pixel changes
(e.g., due to fluctuations in light sources). For additional
detail on the FDM, see Paxton and Dale (in press). See also
Grammer, Honda, Jüette, and Schmitt (1999) for related
methods.

Procedure
Upon arrival, participants were separated and individually
completed a number of questionnaires prior to interacting
with one another. One of the questionnaires was an opinion
survey that included a number of sociopolitical (e.g.,
abortion, death penalty, legalization of marijuana) and
university-specific (e.g., a campus rule forbidding freshmen
students from bringing cars to campus) issues. The opinion
survey posed the issues as open-ended, opinion-neutral
questions. For each item, participants were given several
lines to write their opinion and were directed to indicate the
strength of that opinion on a Likert-style scale from 1 (feel
very weakly) to 4 (feel very strongly).
Participants were randomly assigned to one of two
conditions based on the order of the two target
conversations that they were prompted to have. All dyads
held a brief introduction conversation without the
experimenter present (~3min) and two target conversations
(10min each). Half of the dyads were given an affiliative
prompt first and an argumentative prompt second; half of
the dyads experienced the reverse order. After each target
conversation, participants were separated to complete postconversation measures. Participants were not informed in
advance of the conversation topics. After holding both target
conversations, participants were thanked and debriefed.
For the affiliative conversation, dyads were instructed to
discuss
popular
media
that
both
participants
enjoyed.2 Experimenters identified the argumentative
prompt for each dyad based on participants’ responses to the
opinion survey. The topic for which participants expressed
2

Due to experimenter error, one dyad’s affiliative prompt was
based on a sociopolitical topic on which both agreed. However,
close inspection of the data confirmed the affiliative nature of the
conversation.

Figure 1: Sample FDM sequence of interacting dyad,
aggregated over multiple frames for visualization purposes.
Speech data were collected using individual lapel
microphones (Audio-Technica ATR 3350) and a mixer
(Azden CAM-3) so that each participant’s audio was
captured on a separate channel. The present research used
on/off speech states as the measure for speech. On/off
speech states were obtained for each participant using the
sound finder function in Audacity. Decibel cutoffs were
individually determined for each dyad in order to maximize
sensitivity and specificity.
Samples of data obtained from each interaction type are
graphed in Figure 2. Each sample includes 250sec of
interaction. Taken from the same dyad, the figures graph
changes in body movement as red and blue lines, with
speech events depicted as boxes of corresponding color
behind the lines.

Results
The present analyses tested for unimodal and multimodal
interpersonal and intrapersonal alignment. Crosscorrelations were calculated for interpersonal unimodal
(e.g., participant A’s movement to participant B’s
movement), interpersonal multimodal (e.g., participant A’s
speech to participant B’s movement), and intrapersonal
3

The experimenter sat beside the camcorder, outside of the
participants’ immediate range of vision, in order to monitor the
equipment unobtrusively and to ensure the participants did not
stray from the assigned topic.

1123

Figure 2: Sample body movement time series of a single dyad during 250s of interaction during an affiliative (left) and
an argumentative (right) conversation. Speech data are represented as shaded boxes of corresponding colors.
multimodal (e.g., participant A’s movement to participant
A’s speech) channels within a +/- 3000ms range, yielding a
series of cross-correlation
coefficients (r). Using cross-correlation coefficients
permitted us to investigate both in-phase synchrony and
longer-phase alignment trends within the data.
The data were primarily analyzed using a series of linear
mixed-effects models (Baayen, 2008), using dyad and
condition as random effects unless otherwise noted. All
main and interaction terms were standardized prior to being
entered into the models. As standardized values, the crosscorrelation coefficients can be interpreted as beta weights, a
measure of effect size (Keith, 2005).

Unimodal Alignment
Movement Previous analyses of the corpus found evidence
for in-phase bodily synchrony (Paxton & Dale, under
revision).4 To ensure that the subset analyzed here exhibited
similar patterns, our first model predicted interpersonal
body movement alignment (rmov) with conversation type
(affiliative or argumentative) and time lag (125ms
increments). Results confirmed that the subset of dyads
conformed to broader patterns within the whole corpus.
Increases in time lag (i.e., comparing movement further
removed in time) significantly predicted a drop in rmov (ß=.25; p<.0001), providing evidence for in-phase interpersonal
synchrony. Changes in rmov were also significantly predicted
by conversation type (ß=-.19; p<.0001), with lower levels of
movement synchrony in argumentative conversations.
Interestingly, while only trending toward significance in
analyses of the entire corpus, the interaction term between
conversation type and time lag reached significance in this
subset of the data (ß=.14; p<.01): Interlocutors’ body
movements were more tightly synchronized during
affiliative conversations, reaching higher peak rmov and
falling more sharply as time lag increased.
4

One dyad included in the present analyses was excluded from
analyses in Paxton and Dale (under revision), due to incomplete
data for other analyses.

Speech The second model tested interpersonal speech
alignment during different conversation contexts, using
conversation type and time lag (125ms) to predict
interpersonal speech alignment (rspeech). As anticipated,
increases in time lag predicted increases in rspeech (ß=.15;
p<.0001), while argumentative conversation type
significantly predicted a decrease in rspeech (ß=-.44;
p<.0001). The interaction term was also significant (ß=-.11;
p<.001). Together, these results suggest that interlocutors
generally respected the turn-taking structure during all
conversations but were more likely to exhibit overlapping
speech during affiliative conversations.
To better situate these results, we performed
complementary analyses comparing participants’ speech
patterns during different conversation types, accounting for
condition, conversation number, speaker, and dyad
membership as random effects. In a model predicting turn
length with conversation type, argumentative conversations
predicted slightly but significantly longer turn lengths
compared with affiliative conversations (ß=.04; p<.005).
Another model predicted total number of speech events in a
conversation by both participants using conversation type
and found that argumentative conversations had
significantly fewer speech events than affiliative
conversations (ß=-.31; p<.0001).

Multimodal Alignment
Interpersonal Next, we predicted interpersonal multimodal
cross-correlation coefficients (rmulti) with conversation type
and time lag. The main effect for conversation type was
again significant, with argumentative conversations
predicting a significant drop in rmulti (ß=-.21; p<.0001).
Neither time lag (ß=-.02; p=.39) nor the interaction term
(ß=.01; p=.76) reached significance.
Intrapersonal Our final model predicted intrapersonal
multimodal cross-correlation coefficients (rself) with
conversation type and time lag. As predicted, we found no
significant effect of conversation type on rself (ß=.01;
p=.75), suggesting that intrapersonal alignment may be less

1124

sensitive to conversation context than interpersonal
alignment. Increases in time lag again significantly
predicted decreases in rself (ß=-.34; p<.0001), suggesting
that interlocutors were more likely to be moving when
talking and vice-versa. As with unimodal body movement
alignment, the significance of time lag as a main effect
provided evidence for the existence of in-phase synchrony.
The interaction term also reached significance (ß=.09;
p<.05): Although participants exhibited in-phase
multimodal intrapersonal synchrony, individuals’ own body
movements and speech events were more tightly connected
during affiliative conversations.

Network Visualizations of Interaction
To create the network visualizations of interpersonal
interaction, we used body movement (M) and speech (S)
time series data rather than cross-correlation coefficients.
The networks were intended to capture relationships as they
occur in time. We created two independent networks, one
for affiliative interactions and the other for argumentative
interactions (Figure 3). Connection strengths were presented
as beta weights obtained through a series of linear mixedeffects models.5 All models used condition, conversation
number, and dyad as random effects; the intrapersonal
models (M1:S1 and M2:S2) included participant as an
additional random effect. Models used the nodes as
predictors of other nodes, according to their connections
(e.g., predicting M1 with M2).

Figure 3: Network visualizations for affiliative (top) and
argumentative (bottom) interactions. Colors correspond to
those used in Figure 2. Connection strengths are shown as
beta weights obtained from a series of linear mixed-effects
models and can be interpreted as effect sizes. All
connections are significant (p<.001).

Discussion
While we often intuitively acknowledge that conversational
contexts affect the course of an interaction, our results
suggest that there are fundamental differences in
interpersonal dynamics during different contexts. During
conflict, interpersonal body movement synchrony
diminishes. Interlocutors have a more rigid turn-taking
structure with fewer and longer turns. Dyads use fewer
instances of any overlapping speech, including events like
laughter and verbal tracking. Interpersonal multimodal
alignment—when one interlocutor is talking and the other is
simultaneously moving—drops. Furthermore, in many of
these cases, the effect size of conversation type on these
measures is quite large, suggesting a very strong impact of
context on these aspects of interaction.
On the other hand, some types of behavior exhibit
relatively more stable properties across context.
Interlocutors multimodally synchronize their own speech
and movement, tending to move and speak at the same time
regardless
of
conversational
contexts.
However,
intrapersonal multimodal synchrony can still be affected by
context through interaction effects. We believe this
reinforces a view of interpersonal interaction as inherently
5

The automated speech analysis produces off states frequently,
as it prioritizes ignoring non-target speech. This can minimize the
magnitude of the negative correlations, since there are frequent off
states that match in time during an interaction (e.g., pauses).

context-dependent, although the effects may be quite small
for some elements or in some contexts.
Our findings paint conversation as a highly complex
interpersonal communication structure. While there are
some relatively stable elements within it (e.g., intrapersonal
multimodal alignment), other elements are very sensitive to
conversational contexts (e.g., interpersonal bodily
synchrony). While complementary behaviors align across
interactions, argument as a conversation context appears to
exhibit additional constraints on alignment patterns. Based
on these exploratory analyses, interpersonal communicative
structures appear to be self-organizing within the interaction
and with strong regard to the overall context.
The corpus analyzed here provides a rich source of
interaction data in multiple conversational contexts. We
intend to continue to mine these data in order to better
understand the nature of multimodal communication and
interaction and to collect additional corpora on other
conversation contexts. In doing so, we hope to more fully
develop the interaction network presented here. Future
directions will pursue the creation of more predictive
models of interpersonal (e.g., Mehler, Lücking, & Weiß,
2010) and intrapersonal (e.g., Tilsen, 2009) multimodal
alignment that can shape additional experimental work as

1125

models like the HKB (Haken et al., 1985) have shaped
intrapersonal unimodal alignment.

Acknowledgments
The authors would like to thank undergraduates J.P.
Gonzales and Stephanie Frewen for their help in data
collection and preparation, respectively. This research was
supported partially by NSF grants BCS-0826825 and BCS0926670.

References
Baayen, R. H. (2008). Analyzing linguistic data: A practical
introduction to statistics using R. Cambridge: Cambridge
University Press.
Beek, P. J., Peper, C. E., & Daffertshofer, A. (2002).
Modeling rhythmic interlimb coordination: Beyond the
Haken–Kelso–Bunz model. Brain and Cognition, 48(1),
149–165.
Bernieri, F., & Rosenthal, R. (1991). Interpersonal
coordination: Behavior matching and interactional
synchrony. In R. Feldman & B. Rime (Eds.),
Fundamentals of nonverbal behavior: Studies in emotion
and social interaction. New York: Cambridge University
Press.
Brennan, S. E., Galati, A., & Kuhlen, A. K. (2010). Two
minds, one dialog: Coordinating speaking and
understanding. In B. H. Ross (Ed.), The psychology of
learning. Burlington: Academic Press.
Chartrand, T. L., & Bargh, J. A. (1999). The chameleon
effect: The perception–behavior link and social
interaction. Journal of Personality and Social Psychology,
76(6), 893–910.
Cleland, A. A., & Pickering, M. J. (2003). The use of lexical
and syntactic information in language production:
Evidence from the priming of noun-phrase structure.
Journal of Memory and Language, 49(2), 214–230.
Condon, W. S., & Ogston, W. D. (1966). Sound film
analysis of normal and pathological behavior patterns.
Journal of Nervous and Mental Disease, 143(3), 1–10.
Delaherche, E., & Chetouani, M. (2010). Multimodal
coordination: Exploring relevant features and measures.
Proceedings of the Second International Workshop on
Social Signal Processing (pp. 47–52). ACM.
Garrod, S., & Pickering, M. J. (2004). Why is conversation
so easy? Trends in Cognitive Sciences, 8(1), 8–11.
Grammer, K., Honda, M., Jüette, A., & Schmitt, A. (1999).
Fuzziness of nonverbal courtship communication
unblurred by motion energy detection. Journal of
Personality and Social Psychology, 77(3), 487–508.
Haken, H., Kelso, J. S., & Bunz, H. (1985). A theoretical
model of phase transitions in human hand
movements. Biological Cybernetics, 51(5), 347-356.
Keith, T. Z. (2005). Multiple regression and beyond.
Boston: Pearson Education.
Kopp, S. (2010). Social resonance and embodied
coordination in face-to-face conversation with artificial
interlocutors. Speech Communication, 52(6), 587–597.

Kousidis, S., & Dorran, D. (2009). Monitoring convergence
of temporal features in spontaneous dialogue speech.
Proceedings of the First Young Researchers Workshop on
Speech Technology. Dublin, Ireland.
Krämer, N., Kopp, S., Becker-Asano, C., & Sommer, N.
(2013). Smile and the world will smile with you - The
effects of a virtual agent’s smile on users’ evaluation and
behavior. International Journal of Human-Computer
Studies, 71(3), 335–349.
Louwerse, M. M., Dale, R., Bard, E. G., & Jeuniaux, P.
(2012). Behavior matching in multimodal communication
is synchronized. Cognitive Science, 36(8), 1404–1426.
McFarland, D. H. (2001). Respiratory markers of
conversational interaction. Journal of Speech, Language
and Hearing Research, 44(1), 128-143.
Miles, L. K., Griffiths, J. L., Richardson, M. J., & Macrae,
C. N. (2010). Too late to coordinate: Contextual
influences on behavioral synchrony. European Journal of
Social Psychology, 40(1), 52–60.
Miles, L. K., Lumsden, J., Richardson, M. J., & Macrae, C.
N. (2011). Do birds of a feather move together? Group
membership and behavioral synchrony. Experimental
Brain Research, 211(3-4), 495–503.
Mehler, A., Lücking, A., & Weiß, P. (2010). A network
model of interpersonal alignment in dialog. Entropy,
12(6), 1440–1483.
Norris, S. (2004). Analyzing multimodal interaction: A
methodological framework. New York: Routledge.
Paxton, A., & Dale, R. (in press). Frame-differencing
methods for measuring bodily synchrony in conversation.
Behavior Research Methods.
Paxton, A., & Dale, R. (under revision). Argument disrupts
interpersonal alignment.
Reitter, D., Moore, J. D., & Keller, F. (2006). Priming of
syntactic rules in task-oriented dialogue and spontaneous
conversation. Proceedings of the Conference of the
Cognitive Science Society (pp. 685–690). Vancouver,
Canada.
Riley, M. A., Richardson, M. J., Shockley, K., &
Ramenzoni, V. C. (2011). Interpersonal synergies.
Frontiers in Psychology, 2, 38.
Richardson, M. J., Marsh, K. L., Isenhower, R. W.,
Goodman, J. R. L., & Schmidt, R. C. (2007). Rocking
together: Dynamics of intentional and unintentional
interpersonal coordination. Human Movement Science,
26(6), 867–891.
Sebanz, N., Bekkering, H., & Knoblich, G. (2006). Joint
action: bodies and minds moving together. Trends in
Cognitive Sciences, 10(2), 70–76.
Shockley, K., Richardson, D. C., & Dale, R. (2009).
Conversation and coordinative structures. Topics in
Cognitive Science, 1(2), 305–319.
Tilsen, S. (2009). Multitimescale dynamical interactions
between speech rhythm and gesture. Cognitive Science,
33(5), 839–879.

1126

