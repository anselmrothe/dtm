UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A joint interference effect in picture naming

Permalink
https://escholarship.org/uc/item/7k23c6w1

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Gambi, Chiara
Van de Cavey, Joris
Pickering, Martin

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

A Joint Interference Effect in Picture Naming
Chiara Gambi (c.gambi@sms.ed.ac.uk)
Department of Psychology, 7 George Square
Edinburgh, EH8 9JZ U.K.

Joris Van de Cavey (joris.vandecavey@ugent.be)
Department of Experimental Psychology, 2 Henri Dunantlaan
Gent, 9000 Belgium

Martin J. Pickering (martin.pickering@ed.ac.uk)
Department of Psychology, 7 George Square
Edinburgh, EH8 9JZ U.K.

Abstract
In two experiments we provided evidence for a joint
interference effect in picture naming. Participants took longer
to name pictures when they believed that their partner
concurrently named pictures than when they believed their
partner was silent (Experiment 1) or concurrently categorized
the pictures as being from the same or from different semantic
categories (Experiment 2). However, picture naming latencies
were not affected by beliefs about what one’s partner said.
These findings are consistent with the idea that speakers
represent whether another speaker is preparing to speak, but
not what they are preparing to say.
Keywords: joint task; co-representation; agent-conflict;
language production; picture naming.

In this paper we report results from two experiments that,
for the first time, combined a highly constrained language
task (picture naming), with a manipulation of the context in
which the task is performed (i.e., whether the participant
speaks concurrently with her partner or on her own). A
similar rationale has been used by researchers who
compared solo and joint SR compatibility effects (see
Knoblich, Butterfill, & Sebanz, 2011 for a review), but it
has never been applied to picture naming.
A well-known SR compatibility effect is the Simon effect.
People are faster responding to “right” stimuli with their
right hand and to “left” stimuli with their left hand
(congruent trials) than they are responding to “right” stimuli
with their left hand and to “left” stimuli with their right hand
(incongruent trials). For example, people respond more
quickly to the color of a stimulus when the stimulus (e.g.,
the photograph of a hand) is pointing towards the response
hand than when the stimulus is pointing away from the
response hand (Sebanz, Knoblich, & Prinz, 2003).
A similar effect occurs when participants respond only
with one hand, but they take turns with another participant
who is seated next to them (i.e., they are slower when the
pictured hand points towards the other participant than when
it points towards themselves). This joint interference effect
is interesting because the Simon effect is not observed (or is

reduced) if participants respond with one hand and they
perform the task on their own.
The joint Simon effect has been interpreted as evidence
that participants represent their partner’s potential response
and that this representation interferes with their own
response on incongruent trials (because the two responses
are incompatible, in the same way as a response with one’s
right hand is incompatible with a response given with one’s
left hand). We refer to this as the co-representation account
of joint interference effects. Interestingly, joint
compatibility effects were found when participants sat alone
but were led to believe another person performed the task
with them. This occurred even when no feedback was
available (Atmaca, Sebanz, & Knoblich, 2011).
The co-representation account has been challenged. Here
we are particularly interested in an alternative account put
forward by Wenke et al. (2011), the agent-conflict account.
According to this account, representing that one’s partner is
(potentially) about to respond on the current trial interferes
with one’s own response. However, this occurs because
there is a conflict regarding whose turn it is to respond,
rather than because of incompatibility between one’s own
and one’s partner’s response. In fact, congruent responses
should lead to similar amounts of interference as
incongruent responses.
Joint interference effects have been almost exclusively
investigated in manual tasks (e.g., Simon task, Flanker task,
SNARC task), with only two studies using verbal responses
(Philipp & Prinz, 2010; Pickering & MacLean, 2013) and
none looking at picture-naming responses. Importantly,
picture-naming responses are subject to varying degrees of
congruency. For example, if one participant names the
picture of an apple, her partner could either concurrently
produce the same word (i.e., apple), or they could
concurrently produce an unrelated word (e.g., blouse), or a
related word (e.g., banana).
These different degrees of congruency do matter in solo
tasks, as shown by several picture-word interference studies.
Speakers who name pictures while ignoring distractor words
are fastest when the distractor word is the picture’s name.

2362

They are slower when the distractor is a different word and
slowest when it is a different but semantically related word.
The difference in naming latencies between trials with
unrelated distractors and trials with related distractors is due
to interference between co-activated lexical representations
(Levelt, Roelofs, & Meyer, 1999).
In our study, participants saw pairs of pictures rather than
picture-word pairs. When distractor words are replaced by
distractor pictures, semantic interference effects generally
disappear (Damian & Bowers, 2003), possibly because
distractor picture names are not routinely retrieved or their
activation is too weak to out-weight facilitatory effects at
the conceptual level. We therefore asked participants to
name both pictures in a pair, a task that is subject to
semantic interference effects (Aristei, Zwitserlood, & Abdel
Rahman, 2012).We asked whether the time they took to
respond might be affected by a representation of their
partner’s concurrent response.

account can be extended to naming responses, it could be
taken to predict that speakers represent the content of their
partner’s response and activate the corresponding lexical
representations.
500 ms

400 ms

1500 ms
Mary: blue

2000 ms
1000 ms

+

John: red

INSTRUCTION
SCREEN

DIFFERENT
M: "apple blouse"
J: "blouse apple"

Mary: blue

SAME
M: "apple blouse"

John: blue

J: "apple blouse"

Mary: blue
John: no

NO
M: "apple blouse"
J: "
"

DIFFERENT

A

M(M)

Apple

Blouse

M(J)

Apple

Blouse

Apple

Blouse

M(J)

Apple

Blouse

M(M)

Apple

Blouse

M(J)

Apple

Co-representation
SAME

Mary(Mary)

Apple

Blouse

M(M)
Mary(John)

Apple

Blouse

NO

Experiment 1
In Experiment 1, a red and a blue picture were
simultaneously displayed to two participants seated in
different rooms. Before the pictures appeared, an instruction
screen showed the names of the two participants
accompanied by the words red, blue, or no. Red and blue
corresponded to “go” trials: the participant was instructed to
name the picture presented in the given color first, and then
also name the other picture. No corresponded to “no-go”
trials: The participant was instructed to give no response.
We varied the order in which the other participant (the
partner) was concurrently naming the pictures (Partner’s
task), as follows. On trials on which the two participants
were assigned the same color, they named the pictures in the
same order, therefore producing the same verbal response
(SAME condition). On trials on which the two participants
were assigned different colors, they named the pictures in
reversed order, therefore producing different verbal
responses (DIFF condition). Finally, when either of the
participants was assigned a “no-go” cue, one participant
named the pictures while their partner produced no response
(NO condition). See Figure 1 (top) for examples (with apple
in blue, blouse in red).
In addition, we introduced a second manipulation,
orthogonal to Partner’s task. Participants saw either two
semantically related (e.g., apple – banana) or two unrelated
pictures (e.g., apple – blouse). This served two purposes.
The first was to provide a manipulation check. When two
semantically related lexical items are activated concurrently
(e.g., when speakers are asked to say “apple” and “banana”
in close proximity), they interfere with one another (Aristei,
et al., 2012). We therefore expected longer latencies when
participants named two related than when they named two
unrelated pictures (a main effect of semantic relatedness).
Most importantly, we expected Partner’s task to affect
naming latencies. Specifically, if the co-representation

B

DIFFERENT /
SAME

Agent conflict

M(M)
Mary(Mary)

Apple

Blouse

Blouse

Apple

M(J)

Word

M(M)

Apple

Blouse

NO

Mary(John)

Word

M(J)

C

Mary(John)

Apple

Word

DIFFERENT /
SAME / NO

No-representation

Mary(Mary)

Blouse

Blouse

M(M)

Apple

Blouse

M(J)

Figure 1: Sample trial (top) and hypothesized effects
according to the three accounts.
Note that, because the speakers always named both
pictures, their utterances always contained the same lexical
items. However, when the order differed, the picture that the
speaker named second was the picture that their partner
named first.
Therefore, in the DIFF condition the representation of the
partner’s response might enhance the activation of the
second picture’s name. This would in turn result in greater
competition between the two pictures’ names. Instead, when
the order is the same, the first picture’s name was the word
that one’s partner also named first. Therefore, its activation
level might be raised and competition with the second
picture’s name could be reduced. Overall, we should find
longer naming latencies in the DIFF condition than in the
SAME condition.

2363

This scenario is presented in Figure 1 (panel A). The
nodes represent lemmas in Mary’s mental lexicon. On the
right is a snapshot of the activation level of the nodes apple
and blouse just before the onset of the word “apple” when
Mary is preparing to utter “apple blouse” (unrelated case),
under the different conditions. The degree of activation is
indicated by the thickness of the circles. Pointed arrows are
excitatory connections, rounded arrows are inhibitory
connections.
In addition, the degree of relatedness might also matter
(and this was the second purpose of the relatedness
manipulation). Specifically, if other-representations are
content-specific, the semantic interference effect could be
enhanced in the DIFF compared to the SAME condition.
Alternatively, speakers might not represent the content of
their partner’s response, but they might represent whether
their partner responds on the current trial or not (agentconflict account). If so, the relationship between self- and
other-representations would not affect processing, and hence
naming latencies would be comparable in the SAME and
DIFF conditions. For the same reason, there should be no
interaction between Relatedness and Partner’s task.
However, naming latencies should be longer in the SAME
and DIFF conditions than in the NO condition. This
scenario is presented in Figure 1 (panel B).
Finally, people might not represent other people’s
responses. Note that our participants could not interact:
They named pictures alongside each other, but could not
hear each other. Whereas several studies have shown that
non-interacting participants display joint interference effects
(see above), they all used manual responses. We do not
know whether the same would be true for verbal responses,
particularly because language is perhaps more tightly linked
to communicative situations compared to manual actions. If
the Partner’s task manipulation has no effect (i.e., no
difference between the SAME, DIFF, and NO conditions),
we would conclude that another person’s utterances are not
represented under the conditions tested in our experiment.
This scenario is presented in Figure 1 (panel C) as the norepresentation account.

Method
Participants Twelve pairs of previously unacquainted
participants were recruited from the University of
Edinburgh student community. All reported to be native
English speakers and had no speaking or reading
difficulties. They were paid £6 in return for participation.
Materials Fifty line drawings of everyday objects and
animals were paired twice to yield 50 picture-picture pairs
(25 semantically related, 25 semantically unrelated).
Design and Procedure Partner’s task (henceforth, Partner;
SAME vs. DIFF vs. NO) and Relatedness (unrelated vs.
related) were manipulated within participants and within

items. An item was defined in terms of the first named
picture (so apple-blouse and blouse-apple counted as
different items). Partner varied on a trial-by-trial basis.
Each participant named a given item once per condition.
Pictures were presented into 4 different blocks of 100 trials
each. Each block comprised an equal number of trials in
each condition for both participants. The order of
presentation was pseudo-randomized, separately for each
pair and for each block, with the constraint that the same
picture never appeared on two consecutive trials. (The order
of blocks was counterbalanced across pairs). In addition, we
counterbalanced within each block and for each participant
the color of the first named picture (blue or red) and the
position of the cue (top or bottom half of the screen).
Participants were tested in adjacent soundproof booths.
They were seated in front of computer monitors connected
to the same machine in the control room (so stimulus
presentation was simultaneous). There was a window
between the two rooms, but participants could perceive each
other only peripherally when facing the monitors.
Upon entering the lab, participants were introduced to one
another and taken into the booths. After learning the picture
names individually, they were told that they would “work
together”; instructions were delivered to both participants at
the same time in the control room. Participants then returned
to the booths and, after performing 20 practice trials, began
the experimental phase. A sample trial is shown in Figure 1
(top). A session lasted about 1 hour.
Recording and Data Analysis An inaudible beep marked
stimulus presentation and was recorded together with the
participants’ responses (on three separate channels), using a
multi-channel M-Audio FireWire 1814 device (inMusic,
Cumberland, RI, www.m-audio.com) and Adobe Audition
(Version 4.0; sampling rate: 48000 Hz). Beep onsets were
automatically tagged using Audacity (Version 1.2.5).
Recordings were pre-processed to reduce background noise.
Speech onsets were tagged using the Silence finder
algorithm in Audacity and manually checked (for lip
smacks, etc.). Naming latencies were defined as the time
from beep onset to the onset of the participant’s response.
The data were analyzed using Generalized Linear mixedeffects models (Bayeen, Davidson, & Bates, 2008) in R
(Version 2.7.2) with a logistic link function for categorical
data (Jaeger, 2008). All predictors were contrast-coded. For
Partner, we defined two planned contrasts: naming vs. no
compared the DIFF and SAME conditions against the NO
condition; same vs. different compared the SAME against
the DIFF condition.
Fixed and random effects were selected using backward
selection. If the model with full random structure did not
converge we simplified it by removing higher order terms
(first by subjects, then by items). The alpha-level for

2364

Table 3: Mean latencies in Exp. 1.

likelihood-ratio tests was set to .05 for fixed effects, to .1 for
random effects1.
Latencies were analyzed only if both pictures were named
correctly. Incorrect responses included: naming errors (the
wrong name was used), disfluencies, order errors (the name
of the second picture was uttered first and vice versa),
missing responses. Latencies longer than 3000 or shorter
than 300 ms were considered outliers and excluded.
Latencies more than 3 standard deviations from the byparticipant mean (1.5%) were replaced with the cut-off
value.

DIFF
Unrelated
Related
Tot
Semantic
interference

Table 1: % incorrect in Exp. 1.
SAME
6.8%
5.3%

NO
6.3%
4.9%

Estimate
SE
Z
-3.10
.18
-16.97
.24
.11
2.23
-.23
.08
-2.75
-.31
.15
-2.05
Explained variance estimate
.48
.48
.56

Interestingly, the likelihood of producing an incorrect
response was affected by Partner (χ2(2) = 13.10, p<.01):
They produced more incorrect response when their partner
was naming than when their partner was silent and also
fewer incorrect responses in the SAME than in the DIFF
condition (see Table 1 and 2).
Naming latencies Participants took longer to name
semantically related than unrelated pictures (χ2(1) = 11.32,
p<.001). Crucially, Partner affected naming latencies (χ2(2)
= 7.80, p<.05): Latencies were longer when the partner was
naming than when he was silent. However, the DIFF and
SAME conditions did not differ. Finally, Relatedness and
Partner did not interact (see Table 3 and 4).

1

Analyses that included random slopes for the factor of interest
(Partner), for both items and subjects, yielded the same pattern of
results as the ones reported here.

Tot

869
886
877

855
872
864

864
880

-12

-17

-17

-16

Estimate
874
14
1
16

SE
24
5
4
5

t
36.72
2.79
.17
3.36

Explained variance estimate
11980
3150

Discussion

Table 2: Best fit for accuracy data in Exp. 1.
Predictor
Intercept
naming vs. no
same vs. different
related vs. unrelated
Random effect
Subjects: intercept
Items: intercept
Items: Relatedness

869
881
875

Predictor
Intercept
naming vs. no
same vs. different
related
vs.
unrelated
Random effect
Subjects: intercept
Items: intercept

Accuracy Speakers produced (marginally) fewer
incorrect responses when naming related than unrelated
pictures (χ2(1) = 3.54, p= .06).

DIFF
7.9%
8.1%

NO

Table 4: Model for naming latencies in Exp. 1.

Results

Unrelated
Related

SAME

Experiment 1 showed that beliefs about another’s task can
affect the latency of picture-naming responses, and are thus
not consistent with the no-representation account. We take
this as evidence that speakers represented that their partner
was about to speak. More precisely, our results do not
support the co-representation account. Though participants
made more errors when their partner prepared an
incongruent (DIFF) than a congruent (SAME) response, this
pattern was not confirmed by latency data. In addition,
while there was a clear semantic interference effect, which
replicated previous findings (Aristei, et al., 2012), the effect
was no greater in the DIFF (12 ms) than in the SAME
condition (17 ms). These results are consistent with the
agent-conflict account, as participants took longer to
respond when they believed their partner also prepared to
respond.
However, we must consider alternative explanations. Note
that the slowest conditions (SAME and DIFF) are the ones
in which two “go” instructions are displayed on the screen.
Participants might be distracted by their partner’s instruction
more if it is a “go” instruction than if it is a “no-go”
instruction, perhaps because “go” instructions are more
similar to each other than they are to “no-go” instructions.
This might cause interference between memory
representations for one’s own and the partner’s instructions.
Participants rarely performed their partner’s task by
mistake, which seems to suggest that they had little trouble
remembering instructions. However, this occurred more
often in the DIFF (on 2.3% of trials speakers named the
pictures in their partner’s order) than in the NO condition
(on 1.2% of trials speakers gave no response). But more
importantly, this explanation cannot account for the fact that
latencies were equally long in the SAME as in the DIFF

2365

condition (as in SAME instructions were identical). We
return to this issue after Experiment 2.
We conclude that participants experienced interference
whenever their partner responded concurrently, because
they represented whether it was their partner’s turn to
respond. But what sort of mechanism could be responsible
for this interference effect? The process of “imagining” that
one’s partner is about to respond might draw away
attentional resources from the picture-naming task. If this is
the case, “imagining” one’s partner performing any task
should slow down latencies to the same extent as
“imagining” them naming.
However, it is also possible that interference arises
because the same mechanisms (i.e., language production
mechanisms) are used to represent one’s partner naming
response and to prepare one’s own naming response. If this
is the case, we predict less interference when one’s partner
is preparing a different (non-naming) task than when one’s
partner is preparing a naming response. Experiment 2 was
designed to decide between these alternative explanations.

Experiment 2
In Experiment 2 we replaced “no-go” trials with a semantic
categorization (CAT) task. The SAME and DIFF conditions
were exactly the same as in Experiment 1. In the CAT
condition, partners were instructed to judge whether the two
pictures belonged to the same semantic category or to
different semantic categories. They responded by saying
“yes” or “no” into the microphone.
Thus, all trials required a response from both participants.
If imagining one’s partner performing any task was driving
the effect we observed in Experiment 1, we should now find
no difference between the SAME, DIFF and CAT
conditions. Note that both the CAT task and the naming task
involve visual processing of the pictures and retrieval of the
concepts associated with the depicted entities from memory.
In addition, both tasks require articulation of an overt verbal
response.
Crucially, however, only the naming task engages
language production mechanisms (and specifically the
retrieval of the picture’s name). Therefore, if the
interference effect in Experiment 1 is due to a representation
that one’s partner is preparing a naming task, we should
replicate it in Experiment 2.

Method
Sixteen new participants from the University of Edinburgh
student community were recruited. Materials, design and
procedure were as in Experiment 1, except that the CAT
condition replaced the NO condition. For the semantic
categorization task, participants were told that when they
saw the word question (which replaced the word no) next to
their name, they were to respond to the following question:
“Are the two pictures from the same category?” Data were

analyzed as in Experiment 1; latencies exceeding the 3SDthreshold amounted to 1.7% of the data.

Results and Discussion
Categorization Task Participants responded correctly on
94.7% of the unrelated trials and on 93.6% of the related
trials (a non-significant difference).
Accuracy Speakers produced (marginally) more incorrect
naming responses to related than unrelated pictures (χ2(1) =
2.98, p=.08). More importantly, Partner did not affect the
likelihood of producing an incorrect response (see Table 5).
Table 5: % incorrect in Exp. 2.

Unrelated
Related

DIFF
5.6%
7.2%

SAME
6.3%
7.1%

CAT
6.0%
5.8%

Naming latencies Participants took longer to name
semantically related than unrelated pictures (χ2(1) = 11.04,
p<.001). As in Experiment 1, Partner affected latencies
(χ2(2) = 6.54, p<.05): They were longer when participants
believed their partner named pictures than when they
believed their partner categorized the pictures. However, the
DIFF and SAME conditions did not differ and Relatedness
and Partner did not interact (see Table 6 and 7).
Table 6: Mean latencies in Exp. 2.
DIFF
Unrelated
Related
Tot
Semantic
interference

SAME

NO

Tot

881
898
889

879
907
893

874
885
880

878
897

-17

-28

-11

-19

Note that in Experiment 2 two “go” instructions were
displayed on every trial, including in the CAT condition;
therefore, interference could not have been due to greater
interference between memory representations for more
similar instructions.
The results of Experiment 2 are not consistent with the
co-representation account. As in Experiment 1, naming
latencies were very similar in the DIFF and SAME
condition. In addition, and unlike in Experiment 1, the
likelihood of incorrect responses was very similar in the two
conditions (and did not differ significantly from the CAT
condition, either). Finally, the semantic interference effect
was not larger in the DIFF than in the SAME condition.

2366

Table 7: Model for naming latencies in Exp. 2
Predictor
Intercept
naming vs. no
same vs. different
related vs. unrelated
Random effect
Subjects: intercept
Subjects: Size2
Items: intercept
Items: Relatedness

Estimate
SE
t
884
24
36.77
12
5
2.47
3
4
.70
19
5
3.48
Explained variance estimate
16490
13080
46670
4380

Most importantly, we found that naming latencies are
longer when speakers believe that their partner is also
naming a picture than when they believe their partner is
performing a semantic categorization task. Given that the
two tasks share all processing stages except lexical retrieval,
we conclude that the process of naming pictures is inhibited
by the belief that another speaker is concurrently retrieving
the pictures’ names.

Conclusion
We showed that people represent their partner’s task in a
joint picture-naming task. The evidence is not consistent
with the co-representation account of joint task effects.
Participants did not form content-specific representations of
their partner’s response. It is possible that this finding is
limited to the conditions tested in this study. Interlocutors
might form content-specific representations when engaged
in a conversation (when they rarely speak at the same time).
In addition, the amount of practice and repetition that
characterizes picture naming experiments could have
masked content-specific effects (perhaps because activation
was already at ceiling). Future studies should consider these
limitations.
However, our results are consistent with a version of the
agent-conflict account, in which interference in naming
responses is due (at least partly) to the belief that one’s
partner is preparing a naming response (as opposed to any
response). This is consistent with the idea that people
represent others’ utterances using some of the mechanisms
they use in preparing their own utterances (i.e., language
production mechanisms; Pickering & Garrod, in press).

Edinburgh scholarship. J. Van de Cavey is supported by an
FWO scholarship.

References
Aristei, S., Zwitserlood, P., & Abdel Rahman, R. (2012).
Picture-induced semantic interference reflects lexical
competition during object naming. Frontiers in
Psychology, 3, doi: 10.3389/fpsyg.2012.00028
Atmaca, S., Sebanz, N., & Knoblich, G. (2011). The joint
flanker effect: sharing tasks with real and imagined coactors. Experimental Brain Research, 211, 371-385.
Bayeen, R. H., Davidson, D. J., & Bates, D. (2008). Mixedeffects modeling with crossed random effects for subjects
and items. Journal of Memory and Language, 59, 390412.
Damian, M. F., & Bowers, J. S. (2003). Locus of semantic
interference in picture-word interference tasks.
Psychonomic Bulletin & Review, 10, 111-117.
Jaeger, T. F. (2008). Categorical data analysis: Away from
ANOVAs (transformation or not) and towards logit mixed
models. Journal of Memory and Language, 59, 434-446.
Knoblich, G., Butterfill, S., & Sebanz, N. (2011).
Psychological research on joint action: Theory and Data.
In B. Ross (Ed.), The psychology of learning and
motivation. Burlington: Academic Press.
Levelt, W. J. M., Roelofs, A., & Meyer, A. S. (1999). A
theory of lexical access in speech production. Behavioral
and Brain Sciences, 22, 1-75.
Philipp, A. M., & Prinz, W. (2010). Evidence for a role of
the responding agent in the joint compatibility effect.
Quarterly Journal of Experimental Psychology, 63, 21592171.
Pickering, M. J., & Garrod, S. (in press). An integrated
theory of language production and comprehension.
Behavioral and Brain Sciences.
Pickering, M.J. & MacLean, J. (2013). Representing others’
words: Just like one’s own? Unpublished manuscript.
Sebanz, N., Knoblich, G., & Prinz, W. (2003). Representing
others'actions: Just like one's own? Cognition, 88, B11B21.
Wenke, D., Atmaca, S., Holländer, A., Liepelt, R., Baess,
P., & Prinz, W. (2011). What is shared in joint action?
Issues of co-representation, response conflict, and agent
identification. Review of Philosophy and Psychology, 2,
147-172.

Acknowledgments
We would like to thank Eddie Dubourg and Ziggy
Campbell. C. Gambi is supported by a University of

2
Size refers to the Size of the first named picture. Pictures were
either relatively big (e.g., apple in Figure 1) or small (e.g., blouse
in Figure 1). Size did not interact with any other factors in the
analyses reported in this paper.

2367

