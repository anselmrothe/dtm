UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Cognitive Robotics, Enactive Perception, and Learning in the Real World

Permalink
https://escholarship.org/uc/item/6t72608j

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)

Authors
Morse, Anthony F.
Ziemke, Tom

Publication Date
2007-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Cognitive Robotics, Enactive Perception, and Learning in the Real World
Anthony F. Morse (Anthony.Morse@his.se)
Tom Ziemke (Tom.Ziemke@his.se)
SCAI Lab, University of Skövde,
School of Humanities and Informatics,
SE-541 28 Skövde, Sweden
claiming that the former was reasonably tight, in terms of
theoretical explanation, while the latter left far too much
unspecified and unconstrained. In updating our interpretations
of Newell’s first suggestion, even implemented simulations
leave much unspecified and unconstrained by comparison to
concrete robotic agents which must account for processing
from the sensory surface all the way to motor output, only
then can we claim to have a complete account of the
phenomenon studied (cf. Brooks, 1989; Pfeifer & Scheier,
1999). That is not to claim that isolated or ungrounded
models cannot provide significant contributions to cognitive
theories, rather that such models typically remain silent on
issues of integration and grounding. Hence, they are also at
odds with the emerging paradigm of embodied cognition (e.g.
Varela et al., 1991; Clark, 1997; Pfeifer & Scheier, 1999;
Gibbs, 2005).
Newell’s (1973) second and third suggestions were to
“accept a single task and model all of it.” (ibid, p. 303), or “to
stay with the diverse collection of small experimental tasks,
as now, but to construct a single system to perform them all…
[and crucially] …it must truly be a single system to provide
the integration that we seek.” (ibid, p. 305). Despite huge
progress in modeling techniques, convincing answers to
Newell’s call for integration are very rare.
The natural way forward then for cognitive modeling, in
our opinion, is − still pretty much in the spirit of Allen
Newell’s original view of cognitive modeling, although not
necessarily following his choice of symbolic modeling
approach – to aim for integrative and cumulative modeling. It
is here that we believe modeling with situated and embodied
agents is central, not only as real world modeling forces a
shift in our focus from ungrounded theories of isolated
phenomena to grounded and integrated models accounting for
complete processing, but also for theoretical reasons in the
embodied cognition paradigm (Morse & Ziemke, In Press).

Abstract
Robotic cognitive modeling in the real world requires a level of
integration and grounding rarely seen in more abstract
modeling. However, like Newell we believe this is exactly the
kind of integration needed to promote scientific cumulation in
the cognitive sciences. We present a neural model of learning
compatible with Noë’s account of enactive perception. We
highlight that accounts of enactive perception tend to
oversimplify the problem of identifying contingent
relationships and introduce a novel way to address the problem
of marginal regularities. Finally, we describe a general (nontask specific) model and present a number of real-world robotic
experiments demonstrating a wide range of integrated
psychological phenomena.
Keywords: Cognitive robotics; enactive perception; learning;
cognitive modeling; association; reservoir systems; dynamic
liquid association; liquid state machine; echo state network.

Introduction: A Call for Integration
Although many of the pioneers of AI and cognitive science
were very optimistic about the prospects of creating humanlike general AI (Feigenbaum & Feldman, 1963; Simon,
1957), the field has not come even close to fulfilling such
early promises. We believe that this is in part due to a lack of
research directed specifically at the problem(s) of integration.
Newell (1973) claimed that this limited progress was, in part,
the result of using methods that do not support integrative and
cumulative modeling: “Suppose that in the next thirty years
we continued as we are now going. Another hundred
phenomena give or take a few dozen, will have been
discovered and explored. Another forty oppositions will have
been posited and their resolution initiated. Will psychology
then have come of age?” (ibid, pp. 287-288). Newell’s
opinion at the time, and borne out over the following thirty
years, was that the answer must be no: “Far from providing
the rungs of a ladder by which psychology gradually climbs
to clarity, this form of conceptual structure leads rather to an
ever increasing pile of issues, which we weary of or become
diverted from, but never really settle.” (ibid, p. 289). Clearly,
Newell’s comments and suggestions are still relevant to all
forms of cognitive modeling from computational
neuroscience to cognitive robotics, and are not limited solely
to the symbolic methods he pursued himself.
Newell made three specific suggestions for the future
development of psychological theories and models; “The first
suggestion is to construct complete processing models rather
than the partial ones we now do.” (ibid, p. 301). He contrasted
an implemented simulation with an abstract flow diagram,

Perception, Action, and the Environment
Real world robotic embodiment plays a wider role than
merely focusing our attention on matters of internal
integration. The unit of analysis that many cognitive scientists
consider relevant to our understanding of cognition has
shifted from the view of cognition as purely internal
computation that, at least to some degree, can be reduced to
mapping sensory input to motor output, to the view of
cognition as situated and embodied action that spans brain,
body and environment (Clark, 1997; Clark & Chalmers,
1998; Hutchins, 1995; Suchman, 1987; Varela, Thompson, &
Rosch, 1991). Thus embodiment calls our attention to the

485

actions for evaluation, and recognizing categories in an agent
relative way.
Initially the enactive theory of perception would appear
open to computational / robotic modeling; acquiring and
representing sensorimotor profiles from experience would
seem straight forward as they present contingent and hence
contextually consistent relationships between our actions and
our sensory data. One possible method to use would be
statistical analysis from which consistent relationships in data
are supposed to emerge; however, there are at least two
problems complicating the application of such methods in
modeling enactive perception. Firstly, there is a temporal
problem as the result of an action may not be immediate, and
may in fact result from a sequence of actions leading to a
variation on the credit assignment problem (i.e. which subset
of the actions performed is actually responsible for this
sensory change). Secondly, and more problematically, there is
the problem of marginal regularity. There are relatively few
sensorimotor contingencies that emerge directly between our
senses and actions, rather these contingencies lie between
patterns of relationships in sensory data and patterns of
relationships in action. For example the regularities or
affordances offered by a cup are not consistently between the
same retinal pixels and the same muscular actions. Initially
this is worrying as by Noë’s theory it is the identification of
sensorimotor profiles that enables recognition of the objects
in the first place. Acknowledging this problem turns Noë’s
account on its head; now it seems that you must have
persistent recognition of the object over time (allowing for
varying sensory contact) in order to perceive the
contingencies it affords to your action. The initial object
recognition or tracking need not be a conscious perception in
itself but achieving this prior recognition presents a serious
problem for computational / robotic modeling. As Kirsh
(1992) put it, “what if so little of a regularity is present in the
data that for all intents and purposes it would be totally
serendipitous to strike upon it? It seems … that such a
demonstration would constitute a form of the poverty of
stimulus argument” (ibid, p. 317). Such marginal regularities
are present in the overwhelming majority of tasks requiring
any kind of categorization, recognition, or decision making
process based in either real world or any other non-trivial data
set (Clark & Thornton, 1997). In recent years the reservoir
systems approach has been developed with the potential to
alleviate both problems, as discussed in more detail in the
next section.

relationship between an agent or organism and its
environment. One prominent example of this is the enactive
approach (Noë, 2004; O'Regan & Noë, 2001) in which
perception is thought to be both dependent upon, and
constituted by our possession of sensorimotor knowledge, i.e.
“practical knowledge of the ways movement gives rise to
changes in stimulation.” (Noë, 2004). Thus sensorimotor
knowledge is not simply factual knowledge about a domain
but is intimately about the relationship between an agent and
its environment.
It is clear is that the strong dynamic relationship between
the developing agent and its environment is co-determining
and thus to better understand cognition, modeling in real
environments with robotic/embodied agents is important.
Without refuting the importance of evolution, it seems
reasonably clear that much of our human world knowledge
and general cognitive ability is either derived from, or heavily
shaped by, life-time experiences. The models discussed in
this paper therefore address ontogenetic developmental
models of human learning demonstrated on a real robot.
Models displaying cognitive abilities of one kind or another
typically capture either knowledge or skill in a limited
domain. In attempting to develop an integrative approach, and
following Noë’s enactive view, we can first collapse these
two distinctions together; knowledge is skill, and skill is
knowledge. Next, and still following Noë’s enactive view,
there is no special treatment of any particular modality or
domain of data, so as long as contingencies in the agent’s
interactions with its environment are captured, perception of,
and hence skill within any domain should be forthcoming.
Our starting point for an integrated modeling approach then is
to model a form of learning compatible with Noë’s view of
enactive perception. This then provides means for a real
robotic agent to learn from its experience in noisy and
unstructured environments in a domain general way, thereby
integrating knowledge and skills from several domains of
interaction.

Enactive Perception and Marginal Regularities
According to the enactive view, there are regularities in the
world which produce stereotypical changes in sensory stimuli
relative to actions performed. To use an example from (Noë,
2004) a plate may appear to us as elliptical from some
viewpoints but we know it to be in fact round and perceive it
as such. This means that we know how it would look if we
moved a little this way or a little that way, and we would be
surprised if we did move and found those expectations
contradicted. Our perception of a plate is to recognize that
some part of our sensory input corresponds to a sensorimotor
profile typical of plates – how we would expect our sensory
input to change as a result of various actions. This is, Noë
claims, how our perception gains content, though without an
account of affect it is not clear how this content becomes
meaningful. Despite this, it is clear that such knowledge could
be extremely useful to an embodied cognitive agent; for
example simulating or anticipating the effects of possible

Reservoir Systems
Two primary forms of reservoir system are the biologically
derived Liquid State Machine (LSM) (Maass, Natschlager, &
Markram, 2002a, 2002b, 2002c) and the more
computationally efficient Echo State Network (ESN) (Jaeger,
2001a, 2001b, 2002). Although descriptions of reservoir
systems can be quite complicated, what they all have in
common is a sparse randomly interconnected network of
artificial neurons in which activity coming into the system
reverberates around the network, and would decay to a null

486

method from others both technically and theoretically.
Practically, this method does not require the kind of explicit
training of readout units that both forces a localist
interpretation and can be seen as fixing or providing the
interpretations of those sensory states for which training is
provided. The relationships statistically captured are driven
by contextual consistencies in the sensorimotor experience of
the agent and therefore conform to the generation of enacted
sensorimotor profiles. That is to say, sensorimotor
contingencies between an agent and its environment will be
captured by the statistical analysis so long as the entities
involved in the contingency are made separable by the
reservoir. This is significantly more powerful than relying on
the contingencies being between separable entities in the first
place. Finally, the method remains uncommitted to any
particular form of embodiment or sensory apparatus and
should therefore be considered general purpose, requiring no
task or embodiment-specific knowledge in advance.
Using this method in modeling enactive perception, the two
major problems highlighted in the previous section are
significantly alleviated. Firstly, the current state and trajectory
of the reservoir are contingent not only on current events, but
also recent events, thereby enabling contingencies between
actions in the recent past and sensory changes, to be captured.
Secondly, sensory input is continually transformed and
warped in the high-dimensional space of the reservoir such
that many of the marginal regularities are exposed. This is not
to claim that these problems have been solved, rather to reiterate, they are alleviated to some degree, but should still be
considered as limitations to this model.
At this point the original direction of Noë’s claims can be
re-instated. Having begun forming sensorimotor profiles
based on experienced contingencies between separable
entities, such relationships can not only be used to predict the
sensory effects of various actions, but could also strengthen
the recognition / separation of these entities by feeding back
into the reservoir (Jaeger, 2001b).

state if the input were stopped. Reservoir systems implement
a high dimensional space and cast the input data into it. To
explain the role of the reservoir, Maass et al draw an analogy
between the activity reverberating around a reservoir and the
ripples on a pond. Input to either system (e.g. dropping a
stone into the water, or providing input to the reservoir)
causes activity (or ripples) which then reverberate around the
system, decaying over time. Just like the ripples on the pond,
this residual activity is far from random and can be used to
tell us something about the disturbance(s) that originally
caused it, thus it carries information about those
disturbance(s).
Reservoir systems also have interesting temporal dynamics
as a disturbance at time t will persist within the reservoir
lasting at least as long as the decay period (the time taken to
reach a null state should input cease), and with feedback can
potentially last even longer.
Normally a reservoir system, although untrained, would
utilize trained perceptrons to act as readout units. These single
layer perceptrons can only recover information from the
reservoir so long as it is linearly separable; however, this need
not be simple linear separation in the original input space but
rather in the transformed and dynamic state space of the
reservoir. In this standard guise reservoir systems would seem
to tackle, at least in part, both of the problems identified with
synthetically modeling Noë’s (2004) account of enactive
perception, though they still require the explicit training of
readout perceptrons.

Dynamic Liquid Association
Readers less interested in the implementation details of the
model should pass over this section. The Dynamic Liquid
Association (DLA) model (Morse, 2005a, 2005b, 2006) both
removes the need for training readouts from the reservoir and
implements an auto-associative network capturing multiple
psychological phenomena. We know that the basic limitation
of any simple statistical or experience-based learning system
to accurately capture relationships is that the entities those
relationships are between are linearly separable. Therefore if
it is possible to train a single layer perceptron to act as a
readout unit to a reservoir, responding differentially to the
presence of a particular input feature, then, even without
doing so we know (given the limitations of single layer
perceptrons) that that feature must be linearly separable
somewhere within the reservoir. By this logic the supervised
training of readout units only localizes responses but does not
provide any greater separation than is already present. Thus if
we apply a context sensitive statistical or experience based
learning algorithm directly to the microcircuit, it should
behave as if all the features which could in principal be
trained for identification by single layer perceptrons were
explicitly represented anyway, i.e. the perceptron training
becomes an unnecessary step. That means the limits imposed
on statistical learning by requiring separation are extended to
include all those features of the sensory data stream which are
made linearly separable by the continual transformations of
the reservoir. This is an important step differentiating this

Auto-Association
Following connectionist work on spreading activation
models, a simple learning algorithm has been introduced for
the ongoing construction of Interactive Activation and
Competition (IAC) networks (Morse, 2003). Standard IAC
models (McClelland, Rumelhart, & Group, 1986; Page, 2000;
Young & Burton, 1999) consist of a number of localist
(independently interpretable) units connected via designed
connectivity to other localist units, such that activity spreads
between related feature units while incompatible features
compete. Unlike many architectures derived through
evolution or gradient descent supervision, these networks
function in a manner introspectively similar to mental
activity, in that each locally represented thought, concept or
idea primes related thoughts, concepts, and ideas as activity
spreads between units via structured interconnections (Morse,
2005a, 2005b; Young & Burton, 1999). To autonomously
develop similar architectures in conjunction with a reservoir

487

system, a unit sensitive to the current context is generated by
autonomous pattern recognition over the current state of the
reservoir. In the models presented here this is achieved using
Adaptive Resonance Theory (ART) networks (Carpenter &
Grossberg, 1987) as they allow for the ongoing identification
of consistent patterns in subsections of an otherwise varying
input vector.
The autonomously identified patters are then fully
connected to the reservoir and to motor action units by
connections initially weighted at 0, and subject to normalized
Hebbian and anti-Hebbian plasticity. It is these plastic
connections that provide a simple statistical analysis and are
intended to capture the contingencies present between actions
and sense data in the contexts in which each particular pattern
unit is active. As already discussed, the reservoir is intended
to provide separation between complex features in the
sensory data. Statistics directly between the reservoir and
actions would not be context sensitive as the same units are
involved in many different relationships. That is to say, the
statistical relationships are mediated by the autonomous
identification of sub-states of the reservoir such that different
sub-states can result in different statistical relationships
between the transformed sensory data and actions.

the behaviour emerging from the agent is significantly more
complex than that of a Braitenberg vehicle (Braitenberg,
1984). Agents frequently approach obstacles, only turning or
backing away at the very last moment. Some experienced
agents 1hour+ were even able to perform extremely close
range movement without colliding.

Experiments - Cognitive Robotics

Classical Conditioning. Following Pavlov’s (1927)
description of classical conditioning, a conditioned response
will be elicited by any stimulus that is conditioned by
repeated exposure to a pairing of that stimulus with one
normally eliciting a reflex response. To demonstrate this with
complex sensory input, the DLA agent’s robotic embodiment
is modified to include visual input from a pan-tilt camera, and
an additional input (a button press), which is directly
connected to the motor output so as to force the agent to
move forward. This hard-wired behavior can be considered
analogous to a reflex response such as the salivation response
elicited in the presence of food by Pavlov’s dogs. Thus, when
the new input button is pressed, the robot necessarily (by
hard-wired reflex) moves forward.
The initial random behaviour of the agents gradually
became organized to produce obstacle avoidance as before.
Following this, a large bold black arrow on a white card was
held up in front of the moving camera pointing vertically up.
The agent produced no particular response to the presence or
absence of the card in the visual input, and thus we have the
same situation as in Stage 1 of Pavlov’s experiments (see
Figure 2). Presentation of the upward pointing arrow and the
button push, eliciting a move forwards reflex response, were
then paired (made to co-occur while the agent remains in
open space, i.e. it is not forced to crash) providing the agent
with experience of the visual arrow input whilst moving
forward, thus, mirroring the situation in Stage 2 of classical
conditioning. Given experience of this pairing (≈2-5 min,
≈500 to 1000 time steps) the agent was then able to
consistently reproduce forward movement responses on
presentation of the upward pointing arrow alone (without the
additional hard wired button press). This produces Stage 3 of
classical conditioning where the reflex response has been

Figure 1: 15 second time laps pictures showing (left) initial
collisions form which the robot learned (right) to successfully
navigate a real office environment without further collisions

The DLA model described above is used as the control
system for a real robot such that input from three forward
facing infrared proximity sensors and six contact sensors
(covering the front left and right, rear left and right, and left
and right sides) provided input randomly but sparsely to the
reservoir. In an additional set of experiments low-resolution
input from a camera was also fed via random connectivity
into the same reservoir. Four motor neurons were also set up
in a winner-takes-all relationship corresponding to
acceleration in the forward, backward, turn left, and turn right
directions. Weak noise was used to initiate behaviour.
Finally a motivation neuron was included, active whenever
any of the contact sensors became active. This neuron had
plastic reciprocal connections inverting the polarity of
incoming connections and amplifying their strengths. The
effect of this neuron is to inhibit units in positive relations to
it, while amplifying those in negative relationships. The
practical upshot of this is to prevent actions leading to activity
in the contact units, i.e. to prevent behavior expected to lead
to a collision. For further details and full experimental
analysis see Morse (2006).

Summary of Results
Obstacle Avoidance. Placed in both simple and complex
environments, the robot, initially having no task-specific
knowledge wanders randomly around, occasionally colliding
with obstacles. Following a short period (≈5 min, ≈1000 time
steps) with occasional collisions, the agent’s behavior selforganizes to produce collision avoidance (see Figure 1). This
result is not maintained when the reservoir is removed from
the architecture and statistical learning applied directly to the
input and output streams. Although the task here is simple,

488

unlike that produced by subsumption architectures (Brooks,
1989) in which more complex behaviours take over from
simple behaviours; however, in this case the hierarchical
ordering of behaviour is a product of experience rather than
design.

successfully transferred to stimuli which previously elicited
no discernable external response (see Figure 2).
The same method was repeated to condition a reverse
(backward) response to a downward pointing arrow.
Following a further 500 – 1000 time steps (≈2-5 min)
experiencing the pairing the agent was found to be
differentially responsive to upward and downward pointing
arrows producing either forward or backward motion
respectively. The agent was then conditioned for left and right
pointing arrows to produce left and right turns respectively.
The agent could then be driven by remote control simply by
holding the arrow up in front of the camera and turning it to
point in one of the four conditioned directions. This is not
only a demonstration of conditioned learning but also a
demonstration of complex object recognition, in that the
differences between an upward pointing arrow, and a
downward pointing arrow, being both size and position
variant are non trivial.

Discussion and Conclusion
As an integrated model, the DLA robot readily reproduces
multiple psychological phenomena without requiring the
imparting of task-specific knowledge from the designer.
Rather, by learning the sensorimotor contingencies of its
experience, the agent is subsequently able to direct its
behavior in a manner informed by its past experiences.
Although Noë claims that sensorimotor knowledge is not
necessarily for the guiding of behavior, he readily admits that
“the knowledge in question is practical knowledge; it is
know-how. To perceive you must be in possession of
sensorimotor bodily skill.” (Noë, 2004).
To recapitulate the conditioning experiments in enactive
terms, the agent first experiences contingencies between
active contact sensors and high valued infrared sensor values.
Subsequently, rising infrared values from one side of the
robot paired with moving in that direction produces the
expectation of contact sensor activation. Although not part of
Noë’s account, placing a negative value on these expectations
can guide the robot’s behaviour away from such situations,
promoting the selection of actions which have different
expectations. This has been demonstrated on a real robot in a
real untailored environment, i.e. not only in tidy robot arenas
with high contrast obstacles but further in unstructured
environments with hand held visual cues and noisy sensors.
The presentation of visual stimuli in contingent relationships
with the agent’s movements provides further opportunities for
perceiving the world. The agent expects to see appropriately
oriented arrows as it moves, or vice versa, to move in
directions appropriate to the orientation of visually
discriminated arrows. By Noë’s account, this is exactly the
kind of knowledge required for enactive perception.
Following Newell’s (1973) suggestions for cognitive
modeling, integration can be considered paramount to
developing both cumulative models and theories. In the
experiments briefly presented here (for further details see
Morse, 2006) the focus has been on an embodied model of
learning, therefore classical and operant conditioning were
primary psychological targets, being archetypal theories of
human and animal learning. Further to this, the inclusion of
numerous other psychological features such as; semantic and
repetition priming, overt and covert recognition, phobic
acquisition, systematic desensitization, and sequence
learning, all detailed in Morse (2006), demonstrates a level of
integration rarely obtained by a single model. DLA is further
argued to be both compatible with Noë’s account of enactive
perception, and able to highlight difficulties in the application
of such accounts, such as the problem of marginal regularities
here addressed by the inclusion of a reservoir system.

Figure 2: The three stages of Classical Conditioning. Stage 1: an
unconditioned stimulus elicits an unconditioned response via a hardwired reflex connection; the conditioned stimulus elicits no
particular response. Stage 2, conditioned and unconditioned stimuli
are repeatedly paired. In Stage 3, the conditioned stimulus now
elicits the conditioned response independently of the reflex-response
stimulus.

Operant Conditioning. In operant conditioning (Skinner,
1938), the consequences of any behavior can result in
modifications to the production of that behavior. In the robot,
this is demonstrated by changes in the behaviors leading to
obstacle avoidance. Controlling the movement of the robot by
presenting arrow stimuli to the camera, initially has the
potential to cause collisions; however, on being ‘driven’ into
obstacles the agent modifies its responses such that the
remote control will eventually only work in situations where
the visual ‘instruction’ will not cause a collision. If following
the arrow is likely to cause a collision, the agent reverts to
avoidance behaviors and then, once the collision has been
avoided, the agent reverts back to the behavior associated
with the presented arrow. This hierarchy of behaviour is not

489

McClelland, J., Rumelhart, D., & Group, P. R. (1986).
Parallel Distributed Processing: Explorations in the
Microstructure of Cognition. Volume 2: Psychological and
Biological Models. Cambridge, MA.: MIT Press.
Morse, A. (2003). Autonomous Generation Of Burton’s IAC
Cognitive Models. Paper presented at the European
Conference of Cognitive Science 2003.
Morse, A. (2005a). Psychological ALife: Bridging The Gap
Between Mind And Brain; Enactive Distributed
Associationism & Transient Localism. In A. Cangelosi, G.
Bugmann & R. Borisyuk (Eds.), Modeling Language,
Cognition, and Action: Proceedings of the ninth conference
on neural computation and psychology (Vol. 16, pp. 403407): World Scientific.
Morse, A. (2005b). Scale Invariant Associationism, Liquid
State Machines, & Ontogenetic Learning In Robotics. Paper
presented at the Developmental Robotics (DevRob05).
Morse, A. (2006). Cortical Cognition: Associative Learning
in the Real World: DPhil Thesis, Department of Informatics,
University of Sussex, UK.
Morse, A., & Ziemke, T. (In Press). On the Role(s) of
Synthetic Modelling in Cognitive Science. Journal of
Pragmatics & Cognition, special issue on mechanism and
autonomy: what can robotics teach us about human
cognition and action?
Newell, A. (1973). You can’t play 20 questions with nature
and win: Projective comments on the papers of this
symposium. Visual information processing, 135–183.
Noë, A. (2004). Action in Perception: Cambridge, Mass.:
MIT Press.
O'Regan, K., & Noë, A. (2001). A sensorimotor account of
visual perception and consciousness. Behavioral and Brain
Sciences, 24, 939-1011.
Page, M. (2000). Connectionist Modelling in Psychology: A
Localist Manifesto. Behavioural and Brain Sciences, 23,
443-512.
Pavlov, I. (1927). Conditioned Reflexes: An Investigation of
the Physiological Activity of the Cerebral Cortex.
Translated by Anrep GV: London, Oxford University Press.
Simon, H. A. (1957). Models of Man: New York, NY: Wiley.
Skinner, B. F. (1938). The Behaviour of Organizma: An
Experimental Analysis. New Jersey: Prentice Hall.
Suchman, L. A. (1987). Plans and Situated Action: The
Problem of Human-Machine Communication: New York:
Cambridge University Press.
Varela, F., Thompson, E., & Rosch, E. (1991). The embodied
mind: Cognitive science and human experience: Cambridge,
MA: MIT Press.
Young, A., & Burton, A. (1999). Simulating Face
Recognition: Implications for Modelling Cogntion.
Cognitive Neuropsychology, 16(1), 1-48.

Acknowledgments
This work has been supported by a European Commission
grant to the project “Integrating Cognition, Emotion and
Autonomy” (ICEA, IST-027819, www.iceaproject.eu) as part
of the European Cognitive Systems initiative.

References
Braitenberg, V. (1984). Vehicles: Experiments in Synthetic
Psychology: MIT Press.
Brooks, R. A. (1989). How to build complete creatures rather
than isolated cognitive simulators. In K. VanLehn (Ed.),
Architectures for Intelligence (pp. 225-239). Hillsdale, NJ:
Lawrence Erlbaum Associates.
Carpenter, G., & Grossberg, S. (1987). A massively parallel
architecture for a self-organizing neural pattern recognition
machine. Computer Vision, Graphics, and Image
Processing, 37(1), 54-115.
Clark, A. (1997). Being there - putting brain, body and world
together again: Cambridge, MA: MIT Press.
Clark, A., & Chalmers, D. J. (1998). The Extended Mind.
Analysis, 58, 10-23.
Clark, A., & Thornton, C. (1997). Trading spaces:
Computation, representation, and the limits of uninformed
learning. Behavioral and Brain Sciences, 20(01), 57-90.
Feigenbaum, E., & Feldman, J. (1963). Computers and
Thought. In: New York: McGrall Hill.
Hutchins, E. (1995). Cognition in the wild: Cambridge, MA:
MIT Press.
Jaeger, H. (2001a). The echo state approach to analysing and
training recurrent neural networks. GMD Report 148,
German National Institute for Computer Science.
Jaeger, H. (2001b). Short term memory in echo state
networks: GMD Report 152, German National Institute for
Computer Science.
Jaeger, H. (2002). Adaptive Nonlinear System Identification
with Echo State Networks. Paper presented at the Neural
Information Processing Systems (NIPS) 2002.
Kirsh, D. (1992). From Connectionist Theory to Practice. In
Davis (Ed.), Connectionism: Theory and Practice. New
York: O.U.P.
Maass, W., Natschlager, T., & Markram, H. (2002a).
Computational models for generic cortical microcircuits.
Computational Neuroscience: A Comprehensive Approach,
CRC-Press.
Maass, W., Natschlager, T., & Markram, H. (2002b). A model
for real-time computation in generic neural microcircuits.
Paper presented at the Neural Information Processing
Systems (NIPS) 2002.
Maass, W., Natschlager, T., & Markram, H. (2002c). RealTime Computing Without Stable States: A New Framework
for Neural Computation Based on Perturbations. Neural
Computation, 14(11), 2531-2560.

490

