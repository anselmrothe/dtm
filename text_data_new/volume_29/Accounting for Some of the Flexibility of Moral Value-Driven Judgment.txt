UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Accounting for Some of the Flexibility of Moral Value-Driven Judgment

Permalink
https://escholarship.org/uc/item/589828jk

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)

Author
Bartels, Daniel M.

Publication Date
2007-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Accounting for Some of the Flexibility of Moral Value-Driven Judgment
Daniel M. Bartels (d-bartels@northwestern.edu)
Department of Psychology, Northwestern University
2029 N. Sheridan Road, Evanston, IL 60208-2710 USA
Abstract

Deontology and Consequentialism

An influential account of moral choice suggests that one class of
moral values—protected values (PVs)—rules like “do no harm”
motivate a kind of rigidity in moral cognition: PV-driven choices
are sensitive to the difference between doing and allowing harm,
but are relatively less sensitive to the amount of harm imposed
(Baron & Spranca, 1997). Related work on people’s reasoning
about moral dilemmas suggests a kind of flexibility—moral
judgment is influenced by many factors. Participants in the current
studies evaluated governmental administrators’ decisions to
knowingly do harm to a resource to mitigate greater harm or to
allow the greater harm to happen. When evaluated in isolation,
approval for choices to do harm were interactively shaped by PVs
and participants’ tendency to rely on intuition (rather than
deliberative thinking). When both choices were evaluated
simultaneously, total harm—but not the do/allow distinction—
influenced rated approval. These results suggest PV-driven
judgment is more flexible than previously supposed and are
broadly consistent with an “Affect-Backed Normative Theory”
(Nichols, 2002) account of moral judgment.

Normative ethical theories in philosophy offer bases for judging
acts as morally forbidden, permissible, or obligatory. The most
prominent classes of normative ethical theory in philosophy are
deontology (Davis, 1993) and consequentialism (Pettit, 1993); I
will present strict versions of each to accentuate the contrast.
Both types of theory hold that the goodness of the consequences
produced by an act is relevant for determining its moral status,
but they differ in key respects. Strict consequentialism
acknowledges only one moral currency—the goodness of
consequences. Deontology invests significant weight in a
second factor—adherence to moral rules—allowing this factor
to outweigh the goodness of consequences (Kagan, 1998). Strict
deontology attributes intrinsic significance to rules classifying
acts as per se impermissible—for example, knowingly doing
harm. This rule forbids an agent from performing harmful
actions. Consequentialism removes the agent and rules—what
matters is that the best consequences be produced. In many
contexts, consequentialist judgment is similar to deontological
judgment for harmful acts—doing harm leads to worse
consequences, other things equal—but strict consequentialism
treats proscriptions for harmful acts as rules of thumb that must
be broken in cases where doing so produces better
consequences.
Importantly, these two normative positions imply different
cognitive processes. Absolutist deontological reasoning checks
certain qualities of actions (but not their consequences) against a
set of norms that must be honored. The output of this process is
that some acts are judged wrong in themselves, and thus are
morally unacceptable (even as means to morally-obligatory
ends (see Davis, 1993). Conversely, strict consequentialist
reasoning is focused on ends—whatever values an individual
adopts, this perspective mandates that one brings about the best
consequences by any means (Pettit, 1993). Because the only
inputs to this reasoning process are the consequences (and not
their causes), the morally right course of action is the one that
produces the best outcome. Of course, moral cognition is
remarkably flexible—these two classes of reasoning do not
exhaust the processes at work in moral cognition (see Haidt &
Joseph, 2004). Accounting for this flexibility within the context
of reasonably-constrained models is an important goal for
cognitive science.

Keywords: Moral judgment, reasoning, decision making

Introduction
Understanding the processes at work in value-driven judgment
is an active endeavor in psychology; a number of descriptive
frameworks have been developed in recent years with this aim
in mind (Baron & Spranca, 1997; Tetlock, 2002; Cushman et
al., 2006; Greene et al., 2001; Nichols & Mallon, 2006). The
current studies aim for cross-pollination between these
frameworks by testing their predictions concerning the use of
two distinct moral judgment processes that pervade discussions
in moral philosophy and psychology: deontology (adhering to
moral rules of right and wrong) and consequentialism
(balancing costs and benefits). Although many perspectives in
moral psychology implicate these processes, little theoretical
synthesis has been achieved, perhaps because few studies seek
descriptive generalization across contexts (i.e., different types of
judgment and choice situations).
The current studies are a modest step towards a synthesis,
incorporating multiple methods and theoretical viewpoints to
examine the flexibility of value-driven judgment. Specifically,
these studies examine whether people’s judgment in domains
governed by a specific type of moral value—a protected
value—is always rigidly focused on rules, or whether moral
judgment is more flexible. They examine how task constraints
can shift participants’ evaluative focus to rules and to
consequences and examine one way in which individual
differences can inform an investigation of the processes
underlying moral judgment.

Dilemmatic Reasoning
Philosophers often discuss morality by constructing ethical
dilemmas intended to distill real world problems to their
“essential” features. Ethical dilemmas typically pit moral rules
vs. consequences: less acceptable actions result in better

83

with a dilemma where billions of people would die from a virus
released into the atmosphere unless the fat man is pushed from a
footbridge, 68% of participants judged that such an action
violates a moral rule, but only 24% judged that the action was
morally wrong, all things considered. Judging an act permissible
or impermissible thus appears to be influenced by whether
violations of moral rules evoke affective reactions and by
whether sufficient attention is directed to consequences favoring
violating a moral rule. It appears that non-affect backed rules
operate as a normative consequentialist theory might treat all
commonsense moral rules: if the consequences favor a harmful
action, infringing them may be required and thus, morally
justified, producing dissociation between weak and all-in
impermissibility. The operation of affect-backed rules is more
consistent with a rigid deontology: violating these rules is
forbidden except in the most extreme circumstances.
Accounts of dilemmatic reasoning intend to explain de facto
morally-motivated processes, describing sensitivities to certain
features of dilemmas that generalize across people. However, in
trying to develop parsimonious theories about the fundamental
(i.e., not context-dependent) laws of human thought that
generalize across a wide range of content domains, these
experimenters may be exhibiting what Tetlock et al. (1996) refer
to as the anti-context and anti-content biases. The current studies
take a different approach, using as key variables not only issues
related to the content of the scenarios under consideration, but
also making use of individual differences to inform a processbased account of moral judgment.
Whereas many dilemmatic reasoning studies make use of
only one type of problem—threats to human life, studies in the
judgment and decision making literature confront participants
with threats to different kinds of resources that are not
universally treated has having moral, or intrinsic cross
individuals or cultures. The literatures on “sacred values” (e.g.
Tetlock, 2002) and “protected values” (e.g. Baron & Spranca,
1997) focus on the restrictive tradeoff rules participants appear
to have for cherished resources, and suggest that strongly held,
situation-specific values engender nonconsequentialist decision
strategies.

consequences; more acceptable actions result in worse
consequences. Whereas philosophers have used dilemmas to
develop normative arguments, moral psychologists have used
them to develop descriptive accounts of moral cognition.
Recent studies have asked participants to judge the
permissibility of acts that do harm to one person to prevent harm
to others. Such judgments appear to be influenced by whether
(a) the action elicits a strong emotional reaction, (b) sufficient
consequences favoring the sacrifice (i.e., many lives to be
saved) exist, and by (c) individual differences in propensity to
allow emotional reactions to drive judgment (to name a few;
Cushman et al., 2006; Greene et al., 2001; Nichols & Mallon,
2006). For example, researchers have compared reactions to the
“trolley problem”—where a protagonist may flip a switch to
divert runaway train car threatening to kill five railway workers
onto a track where it will kill only a single railway worker—to
reactions to the “footbridge problem”—where the only way to
save the five railway workers is to stop the train by pushing a fat
man off a footbridge onto the tracks below. People tend to judge
flipping the switch permissible, but pushing the fat man
impermissible. Greene et al. (2001) argue that people’s aversion
to pushing the fat man is attributable to an emotional reaction
elicited by the up-close and “personal” nature of the act that
differs from the “impersonal” nature of flipping the switch.
They argue that (rarely-observed) consequentialist judgments
for “personal” dilemmas are produced by actively suppressing
the affectively pre-potent, deontology-consistent response to
judge “personal” harm impermissible (c.f., Mikhail, 2007).
In contrast to a strictly emotion-based account, Nichols (2002;
Nichols & Mallon, 2006) argues that moral cognition depends
on an “affect-backed normative theory” that contains a set of
proscriptive moral rules that serve to establish preconditions for
actions to be viewed as morally wrong. The rules are often
accompanied by affect or reliant on affect to bring them on-line.
This account suggests three processes interactively shape moral
judgment: cost-benefit analysis, checking to see whether the
action violates a moral rule, and an emotional reaction. To
support the claim that moral judgment is mediated by affective
response, Nichols (2002; Nichols & Mallon, 2006) presents two
kinds of evidence. First, Nichols (2002) found that conventional
norm violations that elicited affective reactions (e.g. spitting at
the table) were judged as less permissible than rule violations
that did not (e.g. playing with your food), and this was
especially true for participants high in disgust sensitivity.
Second, Nichols and Mallon (2006) developed trolley-like and
footbridge-like dilemmas of minimized emotional force (by
substituting teacups for humans) and found a distinction
between judgments of whether the protagonist broke a rule—
what they call “weak impermissibility”, and judgments of
whether the action was morally wrong, all things considered—
what they call “all-in impermissibility”. They show that
violations of affect-backed rules are more likely to generate
judgments of all-in impermissibility than violations of nonaffect-backed rules.
However, Nichols and Mallon also found that even affectbacked moral rules can be overwhelmed by good/bad
consequences of great magnitude. For example, when presented

Protected Values
One distinguishing characteristic of morally-motivated choice is
its (often nonconsquentialist) use of moral rules. For example,
when people have sacred or protected values (PVs), proposed
tradeoffs for these values elicit moral outrage (Tetlock, 2002).
People refuse such tradeoffs on moral grounds and exhibit an
outright refusal to consider costs and benefits (e.g.” You can’t
put a price on a human life.”). The PV framework (Baron &
Spranca, 1997) suggests that for scenarios entailing the
exchange of a cherished resource (for which people have a PV),
people may reason differently (i.e., they make use of moral
rules) than when reasoning about resources not tied to one’s
moral values. Baron and Spranca (1997) describe PVs as a
subset of deontological rules that are tied to affect—rules that
concern actions, like “do no harm”, but not the consequences of
those actions.

84

towards the permissibility of rule-violating act or to the act’s
consequences. The current studies expand on these findings,
relating accounts of dilemmatic reasoning to an investigation of
PV-driven judgment. The current studies use the models of
dilemmatic reasoning as a basis for new predictions about the
context-sensitive role of moral rules in moral judgment across
content domains and different types of people.
One idea motivating the current research is that PVs share
important properties with “affect-backed rules.” Notably, PVs
are intimately tied to strong emotions—proposed tradeoffs of
sacred goods can elicit extreme anger (Tetlock, 2002). But just as
even affect-backed rules can be overwhelmed if sufficient
attention is directed to consequences favoring violating PVs,
Bartels and Medin (2007) show that people’s willingness to
accept tradeoffs of PVs varies depending on where attention is
focused, a factor that varies substantially across contexts. In
particular, they report that in contexts that direct attention
towards net benefits brought about by doing harm, people with
PVs were more willing to engage in the harmful action than
people without PVs.

The PV framework offers a way to pick out the domains for
which we might expect deontology (i.e., for moral rules to drive
judgment, rather than consideration of consequences). To assess
whether a participant has a PV for a given resource, participants
are presented with statements concerning the acceptability of
tradeoffs for some value (e.g., fish species), as below:
Causing the extinction of fish species.
a) I do not object to this.
b) This is acceptable if it leads to some sort of benefits
(money or something else) that are great enough.
c) This is not acceptable no matter how great the benefits.
People who endorse “c” are counted as having a PV for that
resource (Ritov & Baron, 1999). Endorsement of PVs has been
linked to what Baron et al. call “omission bias”—a preference for
indirect harm caused by omissions (i.e., failure to act) over equal
or lesser harm caused by acts. Consider, for example, a scenario
where the only way to save 20 species above a dam in a river is
to open a dam that will kill 2 species downstream. In this
situation, some participants say they would not open the dam.
Some even say they would not want to cause the loss of a single
species (even though not opening the dam leads to the loss of all
20 species). These sorts of nonconsequentialist responses are
more likely for people who endorse a PV for the relevant
resource.
So, how rigid are people’s apparently nonconsequentialist
preferences in domains governed by PVs? Do people who
endorse a PVs care less about the consequences than people who
do not? There are some reasons to think so. First, the
measurement itself suggests a lack of concern with
consequences—“no matter how great the benefits”. Second, the
evidence linking PVs to a large omission bias is consistent with
commitment to moral prohibition. That the preference for
omission over action in contexts where action produces better
consequences is greater for people with PVs suggests PV-driven
preference is consistent with a rigid deontology (Baron &
Spranca, 1997).
Are people absolutist deontologists for domains governed by
PVs? Probably not. PVs tend to be about particularly important
issues—those issues where it seems like consequences should
matter most. It seems reasonable that the people who care more
about not harming the resource (people with PVs) might also
care more about the ultimate consequences of an act. Based on
this logic, Bartels and Medin (2007) used two different
preference elicitation procedures to examine the contextsensitivity of PV-driven (non)consequentialist preference. Using
a procedure that focused attention on whether an action that
causes harm should be taken to maximize net benefits, people
without PVs appear more consequentialist than people with PVs.
Using a procedure that highlights the net costs averted by such
actions, the trend reverses—people with PVs appear more
consequentialist than people without PVs. The context sensitivity
apparent in Bartels & Medin (1997) makes sense if we assume
that people with PVs for a resource care more than people
without PVs about both not doing harm to the resource and
about the ultimate consequences of actions in the domain.
Moral value-driven preference appears to be influenced by the
presence or absence of a PV and by whether attention is directed

Joint vs. Separate Evaluation Preference Reversals
The current studies examine whether PV-driven focus on moral
rules and consequences is subject to a different set of task
constraints. Study 1 asks participants to evaluate decisions under
separate evaluation conditions. Study 2 presents the same
participants with two decisions in joint evaluation, inviting a
comparison between options before rendering judgment.
Previous research demonstrates that attributes that appeal to
one’s intuitive sensibilities, and attributes that are otherwise easy
to evaluate, drive preference in separate evaluation (where a
number of otherwise useful comparisons are not made available),
whereas attributes with greater normative significance that
appeal to “colder”, more logical sensibilities drive preference in
joint evaluation. For example, Hsee and Leclerc (1998) asked
three groups of participants to assign buying prices to an ice
cream product. One group was asked to evaluate a 7 oz. serving
of ice cream presented in a 5 oz. cup, a second group was asked
to evaluate an 8 oz. serving in a 10 oz. cup, and a third group
assigned buying prices to both. Participants in the first condition
were willing to pay more for 7 oz. serving than participants in the
second condition were willing to pay for the 8 oz. serving. In
separate evaluation, participants incorporated feelings about the
overfilled/underfilled attribute of the product into their evaluative
judgment. Of course, buying prices in the joint evaluation were
greater for the 8 oz. serving than the 7 oz. serving. The joint
evaluation condition affords participants with a richer evaluative
context, where participants are able to pick out the most
important attribute for setting a buying price.
Bazerman and Messick (1998) argue that one way to assess
the normative status people give to deontology-relevant vs.
consequentialism-relevant attributes is to present scenarios in
joint evaluation. They suggest consequentialism better captures
people’s lay normative theory for a host of problems, so
sensitivity to deontological considerations would be diminished
in joint evaluation. In the current studies, by collecting judgments
in both separate (Study 1) and joint evaluation (Study 2), one can

85

governed by PVs than for domains not governed by PVs,
because the administrator is described as knowingly doing
harm, violating a PV. Violations of PVs evoke moral outrage,
(Baron & Spranca, 1997), and because attributes that appeal to
intuitive faculties drive judgment in separate evaluation, I
predicted that moral outrage to would drive disapproval1.

better assess whether the deontology-consistent judgments
rendered for violations of PVs are the result of emotional
reaction vs. whether people invest the doing/allowing harm
distinction with enough normative significance to outweigh
consquentialist considerations.

Intuitive and Deliberative Thinking Styles

Hypothesis 2 I expected that the effect predicted in Hypothesis
1 would be moderated by individual differences. Specifically,
the advantage in approval for acts in domains not governed by
PVs over acts in domains governed by PVs should be larger for
people who report greater reliance on intuitive (as opposed to
deliberative) thinking. Deliberative thinkers may over-ride the
pre-potent response to render strong disapproval of PV
violations.

A shared theme of the models of dilemmatic reasoning is that
intuition and deliberation shape moral judgment. In particular,
some suggest that sensitivity to violations of moral rules is often
more reflexive than reflective. The current studies use an
adapted version of Epstein’s (1996) Rational versus Experiential
Inventory (REI) to measure differences in intuitive/deliberative
thinking styles. This measure consists of two subscales: the
Need for Cognition scale, which measures enjoyment of and
reliance on deliberation and the Faith-in-Intuition scale, which
measures enjoyment of and reliance on intuition.
Recall (a) that the affect-backed rules account of moral
judgment suggests the importance of affect for bringing moral
rules on-line, but that even affect-backed rules can be
overwhelmed and (b) that Greene et al. (2001) view some
consequentialist judgments as the product of deliberatively overriding the affectively pre-potent, deontology-consistent
response. If PVs are like affect-backed rules, one might expect
intuitive thinkers, those who “trust their feelings”, to be those
whose judgment will be influenced by PVs in the standard way
(i.e., induce focus on the impermissibility of knowingly doing
harm and away from the consequences of the action). In
contrast, deliberative thinkers might be more likely to ignore or
over-ride intuitions generated by violations of their affectbacked PVs and thus render more consequentialist judgments.

Hypothesis 3 The relationship between rule violation and
approval will be stronger for domains governed by PVs than for
other domains. When participants perceive that a PV has been
violated, they should more strongly disapprove of the decision
than when they perceive that some other (non-affect-backed)
moral rule has been violated—such violations should be
sufficient for strong disapproval. For other domains, participants
may be willing to support a decision that violates a moral rule if
the benefits brought about are great enough.
Hypothesis 4 In joint evaluation, I predicted that actions would
be met with more approval than omissions. Consistent with the
predictions of Bazerman and Messick (1998), I expected the
consequentialism-relevant attributes—80% are lost with the
action; all 100% are lost with the omission–would be large
enough to sway even participants whose judgments might be
consistent with a rigid deontology in other contexts.

Hypotheses
The current studies examine whether PVs operate like affectbacked rules in evaluation of others’ decisions and whether
emotional influences of PVs on judgment will be stronger for
intuitive thinkers. In these studies, participants rate (dis)approval
of decisions made by government administrators, and in Study
1, judge whether the administrators’ decision have broken a
moral rule (judgments of “weak impermissibility”).
For each problem described to participants, each of two
administrators is described as facing a choice about whether to
intervene and knowingly do harm to a resource to mitigate even
greater harm or to merely allow the harm to happen. One
administrator is described as motivated by a desire to do no
harm, and so 100% of the anticipated harm to the resources
occurs. I refer to this class of choices as “omission”. The other
administrator knowingly does harm, first calculating that by
intervening, he or she will kill 80% of the resources under
consideration, and based on this analysis, he or she chooses to
intervene. I refer to this class of choices as “action”.
Study 1 presents the decisions in separate evaluation—on a
given trial participants evaluate just the omission or the action.
Study 2 presents the decisions in joint evaluation—both the
omission and action are evaluated on a single trial.
Hypothesis 1 For separate evaluation judgments, I predicted
that actions would be evaluated more negatively for domains

Method
Participants Forty-eight undergraduates (25 women; 23 men)
participated in Study 1 for partial course credit. Participants
completed the study tasks at their own pace. They were tested
individually but in a small group setting (typically, 1 to 4 Ps per
session). Usually, other participants were completing the study
in the same room. Those participants who wrote their contact
information on a sign-up sheet for Study 2 were contacted about
participating in the second round of data collection, conducted
61 to 71 days later. Thirty-two of the original 48 (18 women, 14
men) Ps participated in Study 2 in exchange for $5
compensation. Sixteen of Study 2’s Ps were run in one session;
the others participated individually in a small group setting. No
differences were observed on any of the variables between the
large- and small-group setting Ps in Study 2.

1

I did not predict a difference in judgments of approval for omissions in
separate evaluation because Haidt and Baron [1996] demonstrated that
omission bias—a preference for allowing harm over doing harm—did
not generalize to evaluations of third-party decisions made by people
occupying especially responsible social roles, like that of a government
administrator.

86

1-8 = Strongly (Dis)Approve

Figure 1: Approval Ratings
6

Prohibition-Driven Omission

Cost-Benefit-Driven Action

Joint Eval, All Ps

5

Sep Eval, Deliberative Ps

4

Sep Eval, Intuitive Ps

Sep Eval, Intuitive Ps
Sep Eval, Deliberative Ps
Joint Eval, All Ps

03

No PV

PV

No PV

PV

assignment of action type to the problems was randomized so
that on every other trial, participants evaluated omission/action.

Materials and Procedure In each session, Ps completed one of
three packets that differed only in the pseudo-randomized
ordering of items within each type (PV items, RationalExperiential Inventory items [only Study 1], judgment
scenarios). First, Ps responded to 30 PV items—seven that
corresponded to the judgment scenarios intermixed with 23
unrelated PV items. Then, Ps responded to an adapted, 20-item
version of the REI. Finally, Ps evaluated two governmental
administrators’ choices for seven problems. The 14 judgment
scenarios crossed two types of decisions (omission, action) with
seven problems (birds, children, dolphins, fish, jobs, poor,
trees). The two versions of the children problem appear below:
(Name) is considering a vaccination program.
Epidemiologists estimate that vaccinating 600 children will
prevent them from dying from an epidemic of a new infectious
disease. The vaccine itself will kill some number of children
because it sometimes causes the same disease. Because this
disease progresses rapidly, a decision must be made quickly,
and the government’s options are severely constrained.

Results and Discussion
To assess whether moral judgment differs according to whether
the domain is governed by protected values, I restrict analyses to
within-Ps effects where I first separate the items for which each
participant endorsed a PV (referred to as “PV”) from the items
for which she did not (referred to as “No PV”).
Rule Violations and Approval (Study 1) For each participant,
I computed correlations between judgments of moral rule
violation (i.e., “weak impermissibility”) and approval ratings
across PV and No PV items. I predicted that violations of moral
rules would elicit strong disapproval ratings for domains
governed by PVs and found that, in general, when participants
perceived rule violations, they disapproved of the
administrators’ decisions. This relationship held for No PV
items (M = -.66), and as predicted, was stronger for PV items
(M = -.84; Wilcoxon signed ranks test for related samples Z = 2.42, p < .05). This finding is consistent with the idea that PVs
function like affect-backed rules in driving moral judgment.

Julie does not want to kill any of the children with the
vaccine. So, the vaccine is not administered. The 600 children
die.

Approval Ratings—Separate Evaluation (Study 1) For each
participant, I calculated four averages: the average approval
rating she assigned to omissions/actions on items for which she
endorsed/did not endorse a PV. Recall that in the current design,
actions resulted in better consequences (80% loss) than
omissions (100% loss). In Study 1, this comparison was
unavailable to participants, leaving emotional reactions to drive
(dis)approval. I expected that for domains governed by PVs,
decisions to knowingly do harm on the basis of explicit costbenefit reasoning would be considered offensive (violations of
PVs—rules like “do no harm”—elicit anger), eliciting greater
disapproval from participants than similar choices made for
other domains. I also predicted that this tendency would be
more pronounced for intuitive thinkers, who might be less likely
to over-ride their emotional reaction than deliberative thinkers.
The results of a 2 (Decision: Action/Omission) x 2 (Domain: No
PV/PV) repeated-measures ANOVA reveal effects of each
factor (F’s(1,43) = 19.98 and 19.87, p’s < .001, ηp2’s = .32) and
a reliable interaction (F(1,43) = 12.58, p < .001, ηp2 = .23). As
predicted, actions were evaluated more favorably for No PV
domains than for PV domains (M’s = 5.39 vs. 4.30; See Figure
1). Also, the correlation between this difference score and
participants’ REI scores was strongly negative (r(43) = -.54, p <

Rich wants to save the children from the disease. He first
calculates that administering the vaccine will kill 480
children. Knowing that doing so will kill many children, he
chooses to vaccinate the children.
In Study 1, after reading about the administrator’s decision,
Ps were asked to assess whether or not the administrator broke a
moral rule. The item read, “By (not) administering the vaccine,
(Julie) Rich broke a moral rule.” Ps indicated agreement on a -3
(Strongly Disagree) to +3 (Strongly Agree) scale. Then, in
Study 1, Ps were asked, “How do you feel about (Julie’s) Rich’s
decision?” Ps indicated (dis)approval by circling a partitioning
mark on a scale ranging from “Strongly Disapprove” (coded as
1 for the analyses that follow) to “Strongly Approve” (coded as
8). In Study 2, Ps read about both decisions before being asked
to evaluate each. Before the first decision presented on a page,
Ps read “Suppose this problem has been assigned to (Name)”,
and then read “Now suppose, instead, that this problem has
been assigned to (Name)” between the first and second
decision. Ps then rated (dis)approval of each decision as they did
in Study 1.
For each of the three packets, the order of problems (i.e.,
birds, children, etc.) was randomized. For Study 1, the

87

.001), indicating that deliberative thinkers showed a smaller
effect. Approval ratings for omissions did not appreciably differ
across these domains (M’s = 4.17 vs. 4.07), nor did difference
scores for omissions relate to thinking styles (r(43) = .15, p >
.10).

References
Baron, J., & Spranca, M. (1997). Protected values.
Organizational Behavior and Human Decision Processes, 70,
1-16.
Bartels, D. M., & Medin, D. L. (2007). Are morally-motivated
decision makers insensitive to the consequences of their
choices? Psychological Science, 18, 24-28
Bazerman, M. H., & Messick, D. M. (1998). On the power of a
clear definition of rationality. Business Ethics Quarterly, 8,
477-480
Cushman, F., Young, L., & Hauser, M. (2006). The role of
conscious reasoning and intuition in moral judgments:
Testing three principles of harm. Psychological Science, 17,
1082-1089.
Davis, N. (1993). Contemporary deontology. In P. Singer (Ed.),
A companion to ethics (pp. 205-218). Malden, MA:
Blackwell Publishing.
Epstein, S., Pacini, R., DenesRaj, V., & Heier, H. (1996).
Individual differences in intuitive-experiential and analyticalrational thinking styles. Journal of Personality and Social
Psychology, 71, 390-405.
Greene, J. D., Sommerville, R. B., Nystrom, L. E., Darley, J.
M., & Cohen, J. D. (2001). An fMRI investigation of
emotional engagement in moral judgment. Science, 293,
2105-2108.
Haidt, J., & Baron, J. (1996). Social roles and the moral
judgement of acts and omissions. European Journal of Social
Psychology, 26, 201-218.
Haidt, J., & Joseph, C. (2004). Inuitive ethics: How innately
prepared intuitions generate culturally variable virtues.
Daedalus, 55-66.
Hsee, C. K., & Leclerc, F. (1998). Will products look more
attractive when presented separately or together? Journal of
Consumer Research, 25, 175-186.
Kagan, S. (1998). Normative Ethics. Boulder, CO: Westview
Press
Mikhail, J. (2007). Universal moral grammar: theory, evidence
and the future. Trends in Cognitive Sciences, 11, 143-152
Nichols, S. (2002). Norms with feeling: Towards a
psychological account of moral judgment. Cognition, 84,
221-236.
Nichols, S., & Mallon, R. (2006). Moral dilemmas and moral
rules. Cognition, 100, 530-542.
Pettit, P. (1993). Consequentialism. In P. Singer (Ed.), A
companion to ethics. Malden, MA: Blackwell Publishing.
Ritov, I., & Baron, J. (1999). Protected values and omission
bias. Organizational Behavior and Human Decision
Processes, 79, 79-94.
Tetlock, P. E. (2002). Social functionalist frameworks for
judgment and choice: Intuitive politicians, theologians, and
prosecutors. Psychological Review, 10, 451-471.
Tetlock, P. E., Peterson, R., & Lerner, J. S. (1996). Revising the
value pluralism model: Incorporating social content and
context postulates. In C. Seligman, J. Olson & M. Zanna
(Eds.), Ontario symposium on social and personality
psychology: Values. Hillsdale, NJ: Earlbaum.

Approval Ratings—Joint Evaluation (Study 2) Actions were
evaluated more favorably than omissions in joint evaluation,
regardless of the presence/absence of PVs and differences in
thinking styles. The results of a 2 (Decision: Action/Omission) x
2 (Domain: No PV/PV) repeated-measures ANOVA reveals a
large effect for Decision (F(1,26) = 44.12, p < .001, ηp2 = .63), a
marginal effect of the presence/absence of PVs (F(1,26) = 3.36,
p = .08, ηp2 = .11) and no reliable interaction (F(1,26) = 2.26, p
> .10, ηp2 = .08). The marginal effect of PVs in the ANOVA is
influenced by an unanticipated difference in approval ratings for
omissions across No PV and PV domains (M’s = 5.61 and 5.70,
paired-t(1,26) < 1.00). Omissions received higher approval
ratings for No PV domains (M = 3.96, SD = 1.00) than for PV
domains (M = 3.55, SD = 1.03, paired-t (1,26) = 2.09, p < .05).
This effect, though unexpected, is consistent with Bartels and
Medin’s (2007) finding that in a procedure that highlighted net
costs and benefits, people endorsing PVs appeared more
sensitive to consequentialist considerations than people without
PVs. Study 2’s joint evaluation context allows for comparisons
of both deontology-relevant and consequentialist considerations.
Given this more enriched evaluative context, attention given to
the contrast in consequences appears to overwhelm constraints
against doing harm that participants would otherwise treat as
having normative significance.

Summary and Conclusions
Research that asks participants to judge the permissibility of
sacrificing a human life for the benefit of others suggested such
judgments are influenced by (a) whether violations of moral
rules evoke strong affective reactions (b) by whether sufficient
attention is directed to consequences favoring the act, and (c) by
individual differences in propensity to allow emotional reactions
to drive judgment. The current results suggest a similar set of
principles govern PV-driven judgment. The observed results
suggest PVs might operate as constituents of an affect-backed
normative theory, offering some cross-pollination between
theoretical frameworks. The current studies were motivated by
the idea that a better understanding of PV-driven judgment and
choice can only be achieved through more thorough scrutiny of
the processes PVs motivate. Previous theory suggested PVs
motivate rigid, nonconsequentialist judgment and choice. By
demonstrating the context-sensitivity of PV-driven moral
judgment, the present findings qualify previous conclusions,
suggesting a more flexible PV-driven judge.

Acknowledgments
NSF-HSD award 0527396 supported this work. I thank Doug
Medin, Lance Rips, Reid Hastie, Craig Joseph, Jason Jameson,
and mosaicLabbers for helpful comments and discussion.

88

