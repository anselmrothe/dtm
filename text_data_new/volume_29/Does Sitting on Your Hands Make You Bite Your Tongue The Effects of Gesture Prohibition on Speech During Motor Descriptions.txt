UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Does Sitting on Your Hands Make You Bite Your Tongue? The Effects of Gesture Prohibition
on Speech During Motor Descriptions

Permalink
https://escholarship.org/uc/item/6g02f7bv

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)

Authors
Hostetter, Autumn B.
Alibali, Martha W.
Kita, Sotaro

Publication Date
2007-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Does Sitting on Your Hands Make You Bite Your Tongue?
The Effects of Gesture Prohibition on Speech During Motor Descriptions
Autumn B. Hostetter (abhostetter@wisc.edu)
Department of Psychology, University of Wisconsin–Madison
1202 W. Johnson Street, Madison, WI 53706 USA

Martha W. Alibali (mwalibali@wisc.edu)
Department of Psychology, University of Wisconsin–Madison
1202 W. Johnson Street, Madison, WI 53706 USA

Sotaro Kita (s.kita@bham.ac.uk)
School of Psychology, University of Birmingham
Edgbaston, Birmingham B15 2TT UK

Abstract
Several theories of gesture production predict that speech
production is affected when gestures are prohibited. The
present study sought evidence for these predictions by asking
participants to describe how to complete three motor tasks.
Half of the participants were prohibited from gesturing during
their descriptions. We found that participants who were free
to gesture described a higher percentage of key events with
semantically rich verbs (e.g., cross, fold) than participants
who were not free to gesture. Participants who were free to
gesture were also less likely to begin their sentences with the
word “and” than participants who were not free to gesture.
There were no effects found for other measures of the amount
and content of speech produced. Thus, the effects of gesture
prohibition on speech production are compatible with the idea
that gesturing helps speakers package their thoughts into
planning units for speaking.
Keywords: gesture prohibition; speech production

Introduction
Hand gestures that mimic the meaning of speech are
frequently produced with descriptions of spatial and motoric
events (Alibali, 2005; Krauss, 1998), and they are
particularly prevalent with speech describing how to
complete motor tasks (Feyereisen & Havard, 1999;
Hostetter & Alibali, 2007). This co-occurrence is likely due
to the isomorphism between spatio-motor images and
representational gestures. Both spatio-motor images and
gestures convey meaning globally; the meaning of a
particular feature (e.g., handshape) can only be interpreted
within the larger meaning of the whole. Similarly, images
and gestures both convey meaning synthetically, in that they
do not rely on analytic rules like syntax to achieve their
meaning (see McNeill, 1992, for discussion).
Because representational gestures rely on the same
principles to convey meaning as images, they are a natural
means of expressing spatio-motor images during speaking.
Rather than having to transfer the global and synthetic
properties of mental images into the local and analytic

format necessary for speech production, gestures are able to
express these properties directly. It seems likely that
gesture’s ability to directly convey imagistic components of
thought may be helpful to speakers who are trying to
describe spatio-motor events.
How might gestures be helpful for speakers? A number of
possibilities have been considered in the literature. One
possibility is that speakers express some aspects of their
spatio-motor images in gestures, rather than in speech (e.g.,
Church & Goldin-Meadow, 1986). Indeed, some data
suggests that speakers use gestures to convey information
that is not also included in their speech (Melinger & Levelt,
2004). According to this view, speakers use a combination
of gestures and speech to express ideas. When one modality
(i.e., gesture) is unavailable, speakers may increase their
reliance on the other modality (i.e., speech) to fully convey
their meaning. This would be manifested in an increase in
the information expressed in speech when gestures are not
allowed compared to when they are allowed. Indeed,
Graham and Heywood (1975) found that speakers used
more words to express spatial relations when gesture was
prohibited than when gesture was allowed.
However, the opposite prediction is made by other
theories that focus on how gestures facilitate speech
production. One such theory, the Information Packaging
Hypothesis (Kita, 2000), holds that gestures facilitate the
packaging of spatio-motor information into the linear format
required by speech. Forming an image with the hands can
focus attention on particular features of the image to
mention, thus helping speakers break their thoughts down in
a way that can be linearly segmented. According to this
view, speakers who cannot gesture may choose not to speak
about spatial information at all. In fact, Rimé, Schiaratura,
Hupert, and Ghysselinckx (1984) found through a
computerized content analysis that speech produced when
gestures were allowed contained a higher degree of vivid
imagery than speech produced when gestures were not
allowed.

1097

But what happens when a speaker is required to talk about
spatial information and is not allowed to gesture? The
Information Packaging Hypothesis predicts that such
speakers will struggle more to organize their rich spatiomotor ideas in the linear stream of speech. Thus, speakers
who cannot gesture may include less spatial information in
their speech than speakers who can gesture and they may
organize the spatial information they do include less
efficiently.
Another theory that focuses on the facilitative benefits of
gesture is the Lexical Access Hypothesis (Krauss, Chen, &
Gottesman, 2000), which holds that gesture helps speakers
to retrieve words that express spatial information. Forming
an image with the hands acts as a cross-modal prime to
activate the desired word in the speaker’s mental lexicon.
According to this hypothesis, if speakers are not able to
gesture, they may have more trouble finding the words they
need to express spatio-motor ideas. In support of this idea,
Rauscher, Krauss, and Chen (1996) found that speakers
produced a higher proportion of filled pauses (e.g., um, uh)
that did not fall at syntactic junctures when they could not
gesture than when they could gesture. Non-juncture pauses
are considered a sign of trouble accessing lexical items, and
Rauscher et al. interpreted their relative increase when
gestures are prohibited as evidence that gestures facilitate
lexical retrieval.
The purpose of the present study was to further
investigate the effects of gesture prohibition on speech
production. We asked participants to describe three motor
tasks: how to wrap a package, how to tie a shoe, and how to
change an automobile tire. Half of the participants were
prohibited from gesturing during their descriptions. We
tested the effects of gesturing on a number of speech
variables to determine if either the amount or the content of
speech differed between the two conditions.
If speech and gesture work as mutually compensating
channels of expression, then speakers who are prohibited
from gesturing should use more detailed speech than
speakers who are allowed to gesture, because individuals
who cannot gesture will compensate by including more
information in their speech. In contrast, if gestures help
speakers plan and produce speech, then speakers who are
allowed to gesture should use more detailed spatial language
than speakers who are not allowed to gesture.
Furthermore, if gestures facilitate speech production by
helping speakers package spatio-motor information, then
speakers who are prohibited from gesturing should show
more difficulty at syntactic junctures than speakers who are
allowed to gesture, as these are places where speakers are
engaged in conceptual planning of the next syntactic unit.
Additionally, speakers who are allowed to gesture should
package spatio-motor information into units for speech
production in a more efficient way. Thus, when gestures are
allowed, the number of spatio-motor terms per speech unit
should be higher, and/or the spatio-motor terms should be
semantically richer (in either case, the speech expresses
spatio-motor information more densely).

Finally, if gestures facilitate speech production by
facilitating retrieval of words that express spatio-motor
ideas, then speakers who are prohibited from gesturing
should produce more filled pauses that are not at syntactic
junctures than speakers who are allowed to gesture,
replicating Rauscher et al. (1996).

Method
Participants
Twenty-six participants (19 female, 7 male) volunteered to
participate. They were recruited via the undergraduate
Psychology research pool at the University of WisconsinMadison and received extra credit in their Introductory
Psychology course in exchange for their participation. All
were native English speakers.

Procedure
Participants arrived for testing with a confederate who
pretended to be another participant. They were told that they
would participate in a study about how people describe
spatial information in different situations. No mention was
made of the specific focus on the influence of gesture
prohibition on speech production. The experimenter
pretended to randomly assign the participant to the
“speaker” role and the confederate to the “listener” role. The
speaker and listener were then seated on either side of a
screen, so that they could not see one another.
The experimenter then explained that the speaker would be
asked to describe three motor tasks to the listener, who
would rate the quality of the descriptions. During the
descriptions, the speakers’ hands or feet would be
immobilized. We chose to restrain participants’ feet in the
control condition in order to equalize any attentional effects
of having to maintain a specific posture across conditions.
Participants were randomly assigned to either the feet
restrained or hands restrained condition.
Participants in the hands restrained condition were given a
25 x 60 x 2 cm wooden board to place across their laps. On
the top of this board, there were several strips of Velcro.
The participants were also given cotton gloves to wear that
had the opposite side of the Velcro attached to the palms
and fingers. They were asked to place their hands on the
board, so that the two sides of the Velcro adhered. In this
way, they were discouraged from moving their hands during
the task without being forcefully restrained. Participants in
the feet restrained condition were given a similar wooden
board to place underneath their feet. This board was
equipped with two straps that the participant slipped over
their feet.
Once participants were properly restrained, they were
asked to describe how they would complete three motor
tasks in the same fixed order. First, they described how they
would tie a shoe. Second, they described how they would
wrap a package, and finally, they described how they would
change a tire on a car. Participants were asked to describe
each task in as much detail as they could and to take as
much time as they needed.

1098

The experimenter sat across from the participant and
listened as each task was described. A hidden video camera
recorded the participants’ descriptions.
At the conclusion of the experiment, all participants were
debriefed about the true purpose of the experiment as well
as the hidden video camera. They were given the
opportunity to withdraw their video data from the study. All
declined.

Coding
The descriptions given by each participant were transcribed
verbatim. We then segmented speech into syntactic units. A
unit was defined as a main clause and its associated
dependent clauses. For example, “You get some wrapping
paper/ and you lay it flat on the table” was coded as two
units, whereas “You take a piece of paper that is big enough
to wrap around the box” was coded as a single unit. We
chose the syntactic unit as our level of analysis because each
syntactic unit is planned separately (Bock & Cutting, 1982);
thus, if gestures facilitate speech planning and production,
then gestures should influence speech at the level of the
syntactic unit.
Outcome measures
Amount of speech. As a measure of how much the
participants in each condition spoke, we counted the total
number of words each participant produced during each
description. We also counted the total number of units
produced.
Speech content. If speakers use gestures to help plan and
produce utterances about spatio-motoric information, then
speakers who can gesture may produce more spatio-motor
terms in speech than speakers who cannot gesture. In
contrast, if speakers use gestures to express spatio-motoric
information so that it does not have to be encoded in speech,
then speakers should convey less spatio-motor information
in speech when they are allowed to gesture. To test this
prediction, we counted the number of spatial motor terms

(SMTs) produced in each description. SMTs were defined
as words that denote a spatial or motoric property, relation,
or motion. For example, the unit “so they make a little
triangle” was coded as containing three SMTs: make, little,
and triangle. Similarly, the unit “um, you wanna take your
left shoelace” was coded as containing 2 SMTs: take and
left. We then calculated SMTs per unit.
Participants varied greatly in the SMTs they used to
describe individual events. For example, when talking about
the first step in tying a shoe, one speaker said “you cross the
laces over one another” while another speaker said “you put
one lace over the other.” Although we counted both cross
and put as SMTs, these two verbs differ in the amount of
specific spatial information they convey, or in their
“richness.” We therefore decided to more closely examine
the verbs speakers used to describe two of the key events
from each of the three motor tasks, for a total of six events.
From the package-wrapping task, we chose the first folding
event, when the paper is first wrapped around the box, and
the end-folding event, when the triangle-shaped piece on the
end of the box is folded upwards. From the shoe-tying task,
we chose the event when the laces are originally crossed and
the event when the two laces are intertwined by pulling one
through the other. For the tire-changing task, we chose the
event when the old tire is taken off and the event when the
new tire is put on. Table 1 displays information about the
variety of words used to describe each of these key events.
We classified each verb as “rich” or “generic” on the basis
of whether or not the verb conveyed information about
manner or configuration. For example, the term “put” was
coded as generic, but the word “cross” was encoded as rich
because it conveys specific information about manner (see
Breedin, Safffran, & Schwartz, 1998).
Conceptual planning load. According to the Information
Packaging Hypothesis (Kita, 2000), gestures help speakers
to package their thoughts into units for speaking. According
to this view, speakers who cannot gesture may have more
trouble deciding which spatio-motor ideas should be

Table 1: Verbs Used to Describe Six Key Events
Number of
speakers who
described
23
23
17
21

Most common
verb (% who
used that verb)
Take (57%)
Put (83%)
Cross (71%)
Put (43%)

Task
Tire
Tire
Shoe
Shoe

Event
Take old tire off
Put new tire on
Cross laces
Intertwine laces

Package

Fold over box

26

Fold (62%)

Package

Fold end up

20

Fold (70%)

1099

Rich Verbs
Pull, remove, slide, lift
Replace, slip, place
Cross, criss-cross, pull
Tie, tuck, pull, wrap,
loop, cross
Fold, wrap, lift, cover,
place, pull
Fold, flip, push, pull,
square

Generic Verbs
Take, get, move
Put
Make, put
Put, bring
Put, bring, do, take,
make
Take, do, make

Table 2: Speech Output as a Function of whether the Hands or Feet were Restrained
Gesture Prohibited
M
SD
Amount of Speech
Words
Units
Content of Speech
Spatial Motor Terms (SMTs)
SMTs / Unit
% events described with rich verbs
Conceptual Planning
% units starting with “and”
Lexical Access Difficulties
Filled pauses
% Non-juncture filled pauses

Gesture Allowed
M
SD

t

p

292.31
33.23

107.03
10.47

341.46
37.38

204.42
16.56

0.77
1.24

0.45
0.22

76.54
2.21
45

30.52
0.46
36.3

85.62
2.09
71

46.50
0.37
27.8

0.59
0.69
3.87

0.56
0.50
0.012

55.2

12.99

38.8

13.5

3.07

0.005

9.77
27

6.38
19

10.31
29

9.94
15

0.16
0.24

0.87
0.81

expressed in what way in each unit. Thus, speakers who
cannot gesture may need to spend more time for conceptual
planning at the beginning of each utterance in order to make
the decisions about what to mention that gesture normally
helps with along the way. The need for additional planning
time might be manifested by the addition of the word “and”
to the beginning of each unit (compare “take them one in
each hand/ cross them over/ loop one underneath the other/
pull it tight” to “ well you have your two strings/ and
crossthem/ and you put one underneath the other one/ and
pull 'em tight”). We calculated the percentage of units that
began with the word “and” as a measure of speakers’ need
for additional planning time.
Lexical access difficulties. One implication of the Lexical
Access Hypothesis (Krauss et al., 2000) is that speakers
should have more difficulty accessing lexical items when
they are unable to gesture. Following Rauscher, Krauss, and
Chen (1996), we used the proportion of filled pauses that
did not occur at syntactic junctures as a measure of
difficulties retrieving lexical items. To derive this measure,
we first identified all filled pauses (e.g., um, uh, er), and we
then classified each as either occurring at a syntactic
juncture (e.g., “Uh, first you take one end”) or not (e.g.,
“You wanna take off um the bolts”). For each participant,
we then calculated the proportion of filled pauses that were
non-juncture filled pauses. We also calculated the total
number of filled pauses each participant produced.

Results
Is speech affected when speakers are prohibited from
gesturing? To investigate this question, we collapsed the
dependent variables described above across the three motor
tasks. We then used independent-samples t-tests to compare
the frequency of each behavior when gestures were allowed
and when gestures were prohibited. Table 2 displays the
results of all comparisons.
We first compared the amount of speech produced in each
condition, by examining the number of words produced and
the number of units produced. There was wide variability in

how much participants talked, with the total number of
words being produced ranging from 99 to 701 words when
gestures were allowed and from 149 to 487 words when
gestures were not allowed. However, participants who were
free to gesture did not produce more speech than
participants who were not free to gesture.
We next compared the content of speech when gestures
were allowed and when gestures were prohibited. To
review, theories that focus on how gesture and speech work
together to express information predict more detailed spatial
content when gesture is prohibited; theories that focus on
how gestures facilitate speech production predict more
detailed spatial content when gesture is allowed. There was
no difference in the number of SMTs produced in the two
conditions (range in gesture-allowed condition: 24-168;
range in gesture-prohibited condition: 36-128), nor was
there a difference in the rate of SMTs produced per unit of
speech. However, a comparison of the percentage of key
events described with rich verbs yielded a significant
difference between conditions. Speakers who were free to
gesture described a larger percentage of the key events with
rich verbs (M = 71%, SD = 27.8) than did the speakers who
were not free to gesture (M = 45%, SD = 36.3), t(25) = 3.87,
p = .012.
We next considered whether there was evidence that
gesture contributes to the conceptual planning of utterances.
The Information Packaging Hypothesis holds that gestures
help speakers segment spatio-motor ideas into the linear
system of speech, and therefore, predicts that speakers
should have increased difficulties packaging speech when
gestures are prohibited. Packaging problems might result in
the production of speech that contains fewer SMTs per unit;
however, we found no difference between the SMTs per
unit produced by speakers who could gesture and the SMTs
per unit produced by speakers who could not gesture.
Packaging problems might also result in the need for
additional planning time at the beginning of each speech
unit, a need that could be met by beginning units with the
word “and.” We found that speakers who could not gesture
were more likely to begin units with the word “and” (M =

1100

speech production. First, the Information Packaging
Hypothesis (Kita, 2000) holds that gestures are a means of
packaging spatio-motor information into the linear system
of speech. The Information Packaging Hypothesis predicts
that speakers who are unable to gesture should be less
successful in packaging spatio-motor information into units
for speech production. Accordingly, spatio-motor content
was described in a richer way when gestures were allowed
compared to when gestures were not allowed.
Second, the Lexical Access Hypothesis (Krauss et al.,
2000) can also explain why speakers who were allowed to
gesture produced richer speech than those who were not
allowed to gesture. The Lexical Access Hypothesis suggests
that gestures facilitate the retrieval of words from the mental
lexicon. According to this view, speakers should produce
less rich speech when they are unable to gesture, because
they will have more trouble accessing rich lexical items that
are more infrequent in the language. This is in line with the
present finding that speakers were more likely to produce
rich verbs to describe key events when they were allowed to
gesture.
The two theories make different predictions about other
aspects of speakers’ behavior. The Information Packaging
Hypothesis predicts that speakers who cannot gesture
should have more trouble at syntactic boundaries, as they
decide what to include in the upcoming unit. This prediction
was supported by the present data. Speakers whose hands
were restricted began a higher percentage of their syntactic
units with the word “and” than did speakers whose hands
were not restricted. Our claim is that the addition of the
word “and” to the beginning of syntactic units is a pausing
tactic that gives speakers additional time to plan the rest of
the upcoming utterance.
The Lexical Access Hypothesis claims that speakers
should produce less fluent speech when gestures are not
allowed compared to when they are allowed. More
specifically, speakers should produce a higher proportion of
non-juncture pauses when they are unable to gesture,
because non-juncture pauses are those that are most closely
associated with lexical retrieval problems. We found no
evidence to support this prediction. The speakers in our
study who were unable to gesture did not produce more
filled pauses (in total, or at non-junctures) than the speakers
who were able to gesture. This is a failure to replicate the
findings reported by Rauscher et al. (1996). One possible
reason for the discrepancy is the difference in design
between the two studies. Rauscher et al. used a withinsubjects design that may have been more robust against the
natural between-speaker variability in speech fluency.
The two significant findings that emerged in this study are
quite subtle. Given the strong predictions made by a variety
of theoretical frameworks regarding the inextricable relation
between gesture and speech, it is perhaps surprising that we
did not find more striking differences in either the amount
or the content of speech when gestures were inhibited
compared to when gestures were allowed. There are at least

55.2%, SD = 12.99) than were speakers who could gesture
(M = 38.8%, SD = 13.5), t(25) = 3.07, p = .005.
We also investigated whether there was evidence for
difficulties in lexical access when gestures were prohibited.
The Lexical Access Hypothesis holds that gestures help
speakers retrieve lexical items, and therefore, predicts that
speakers should have increased difficulties retrieving lexical
items when gestures are prohibited. However, we found that
participants whose hands were restrained did not produce
more filled pauses or a higher percentage of non-juncture
filled pauses than participants whose hands were not
restrained. The total number of filled pauses produced
ranged from 2 to 36 by speakers who were free to gesture
and from 3 to 22 by speakers who were not free to gesture.
Finally, we more closely examined the behavior of those
individuals in the hands free condition who gestured during
their descriptions. Three of the participants did not gesture
at all during any of their three descriptions. The remaining
ten participants gestured from 3 to 33 times over the course
of their three descriptions. Rate of gestures per 100 words
was not correlated with rate of SMTs or rate of filled
pauses. It is worth noting that the findings reported above
remain unchanged if the three participants in the gestureallowed condition who did not gesture are excluded from
the analyses.

Discussion
This study aimed to provide evidence about the effects of
gesture prohibition on speech production. We tested the
predictions of three theories about how speech is affected
when gestures are prohibited.
First, some theories posit that speakers use gestures to
encode spatial information so that it need not be encoded in
speech. Accordingly, these theories predict that speakers
will compensate for their inability to gesture by producing
more detailed speech when gestures are prohibited. We
found no evidence to support this prediction. Speakers did
not produce more words, more speech units, or more spatiomotor terms when they were not allowed to gesture than
when they were. However, this should not be taken as
evidence that such theories of gesture production are
invalid. It is still quite possible that speakers do produce
gestures and speech as complements to one another. What
the present data suggest is that speakers do not compensate
for their inability to gesture by enhancing their speech.
Instead, the present data indicate that speakers actually
produce less detailed speech when describing spatio-motor
events in the absence of gesture. This is manifested in the
production of more semantically rich verbs by speakers
were allowed to gesture than by speakers who were not
allowed to gesture. This is in line with theories that suggest
a facilitative role for gestures in the speech production
process. Speakers who are able to gesture talk about spatiomotoric events in a more detailed way than speakers who
are not able to gesture.
This increased richness of speech content is compatible
with two specific theories about how gestures facilitate
1101

two possible explanations for the lack of more striking
effects.
First, prohibiting speakers from gesturing is not an easy
task. Rime et al. (1984) found that prohibiting movement of
the forearms and hands led to increased movement in other
parts of the body, including the eyes, lips, fingers, and legs.
Although we did not systematically measure movement in
our study, our anecdotal impression certainly coincides with
the report given by Rime et al. Speakers have a difficult
time being still while they speak; restraining their arms does
not keep them from moving other body parts. We consider it
quite possible that these non-hand movements accomplish
the same functions as do hand gestures, thus potentially
weakening our manipulation. However, since we know of
no ethical way to completely immobilize a speaker, this
problem is not easily overcome.
Second, it is not clear whether all speakers find gestures
beneficial for the same reasons or in the same ways. For
example, Hostetter and Alibali (2007) suggest that speakers
with weak verbal skills may use gestures to facilitate speech
production while speakers with strong verbal skills may use
gestures to supplement their speech and make speech more
engaging. It is possible that inhibiting gestures does have
more profound negative effects on speech production than
those observed here, but only for speakers with weak verbal
skills. In the present experiment, speakers with strong verbal
skills who were prohibited from gesturing may have been
able to compensate for their inability to gesture by
producing more detailed speech. Meanwhile, speakers with
weak verbal skills who were prohibited from gesturing may
have been unable to compensate for the lack of gesture and
produced less detailed, less fluent, and less efficiently
packaged speech. However, in the entire sample, such
effects may have been washed out by the inclusion of
individuals who show both types of patterns. Unfortunately,
we do not have information about the verbal skills of the
participants in this study, so we cannot test this possibility
in the present data.
In conclusion, the present experiment provided new
evidence that prohibiting gestures influences speech
production. Although we did not find differences in many of
the variables we considered, we did find differences in both
the richness of the verbs produced and in how easily
information was packaged into syntactic units. Thus, we
found support for the idea that gesture plays a role in speech
production, and specifically, for the idea that gesture

facilitates the packaging of spatio-motor information into
units. It seems, then, that sitting on your hands does
influence your tongue, though it does not make you bite it
completely.

Acknowledgments
This research was funded by a grant from the University of
Wisconsin Graduate School Research Committee to Martha
Alibali. We thank Katie Barofsky and Karin Ockuly for
their help with data collection and transcription.

References
Alibali, M. W. (2005). Gesture in spatial cognition: Expressing,
communicating, and thinking about spatial information. Spatial
Cognition and Computation, 5, 307-331.
Bock, K., & Cutting, J. C. (1982). Regulating mental energy:
Performance units in language production. Journal of Memory
and Language, 31, 99-127.
Church, R., & Goldin-Meadow, S. (1986). The mismatch between
gesture and speech as an index of transitional knowledge.
Cognition, 23, 43-71.
Feyereisen, P., & Havard, I. (1999). Mental imagery and
production of hand gestures while speaking in younger and older
adults. Journal of Nonverbal Behavior, 23, 153-171.
Graham, J. A., & Heywood, S. (1975). The effects of elimination
of hand gestures and of verbal codability on speech
performance. European Journal of Social Psychology, 5, 189195.
Hostetter, A. B., & Alibali, M. W. (2007). Raise your hand if
you're spatial: Relations between verbal and spatial skills and
representational gesture production, Gesture, 73-95.
Kita, S. (2000). How representational gestures help speaking. In D.
McNeill (Ed.), Language and gesture (pp. 162-185). Cambridge,
UK: Cambridge University Press.
Krauss, R. M. (1998). Why do we gesture when we speak? Current
Directions in Psychological Science, 7, 54-60.
McNeill, D. (1992). Hand and mind: What gestures reveal about
thought. Chicago: University of Chicago Press.
Melinger, A., & Levelt, W. J. M. (2004). Gesture and the
communicative intention of the speaker. Gesture, 4, 119-141.
Rauscher, F., Krauss, R. M., & Chen, Y. (1996). Gesture, speech,
and lexical access: The role of lexical movements in speech
production. Psychological Science, 7, 226-231.
Rimé, B., Schiaratura, L., Hupert, M., & Ghysselinckx, A. (1984).
Effects of relative immobilization on the speaker's nonverbal
behavior and on the dialogue imagery level. Motivation and
Emotion, 8, 311-325.

1102

