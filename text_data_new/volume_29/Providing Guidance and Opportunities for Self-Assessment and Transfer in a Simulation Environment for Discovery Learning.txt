UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Providing Guidance and Opportunities for Self-Assessment and Transfer in a Simulation
Environment for Discovery Learning

Permalink
https://escholarship.org/uc/item/08m0w0qz

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)

Authors
Tan, Jason
Skirvin, Nathan
Biswas, Gautam
et al.

Publication Date
2007-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Providing Guidance and Opportunities for Self-Assessment and Transfer in a
Simulation Environment for Discovery Learning
Jason Tan1, Nathan Skirvin1, Gautam Biswas1, Kefyn Catley2
{jason.tan, nathan.skirvin, gautam.biswas, kefyn.catley }@vanderbilt.edu
1

Department of EECS / ISIS, Vanderbilt University
Department of Teaching and Learning, Vanderbilt University
Nashville, TN 37235 USA

2

to answer questions and solve problems in different situations. Researchers have confirmed that “exploratory learning” with understanding and the ability to transfer in learning environments can lead to “effective learning” (Schwartz
and Bransford, 2005).
In our work on middle school science education, we have
been developing a teachable agent environment called
Betty’s Brain, where students teach the computer agent,
Betty, by creating concept maps. The concept map representation and reasoning mechanisms are geared towards teaching and learning about interdependence among entities in
river ecosystems. Analysis of student answers to post-test
questions on balance (equilibrium) made it clear that students did not quite grasp this concept and how it applied to
river ecosystems. We realized that to understand balance,
students had to be introduced to the dynamic behavior of
river ecosystems.
Analyzing dynamic systems behavior can be very challenging for middle school students who do not have the
relevant mathematical background or the maturity to grasp
these concepts. To scaffold the process of learning about
temporal effects, we have developed a simulation-based
learning environment that supports scientific discovery
learning in the domain of river ecosystems. This support
comes in the form of guidance, scaffolding, and mechanisms for self-assessment and transfer. The addition of a
simulation environment allows students to explore and conduct experiments about dynamic processes in the ecosystem.

Abstract
This paper describes the use of a computer-based simulation
learning environment to teach fifth grade students about dynamic processes in a river ecosystem using guided discovery
learning techniques. The simulation is framed in an established learning-by-teaching environment called Betty’s Brain,
where students create causal concept maps to understand interdependence and balance. Students were asked to manipulate and observe the simulation and teach the computer agent,
Betty, using the knowledge acquired from the simulation. We
present the design of the simulation environment and the implementation of its components: the simulation engine, the
display and control interface, tools for guided discovery
learning, and the self-assessment subsystem. A study was run
on two classrooms to examine the effectiveness of the system.
Student understanding was measured by pre to post test differences. Students using the system showed significant learning gains in important concepts. We also studied student
learning behaviors in the simulation environment, and found
that those who better utilized the self-assessment system performed better on the post test.
Keywords: simulations, scientific discovery learning, selfassessment, transfer, learning by teaching

Introduction
Constructivist approaches to education involve experiences
through observation, exploration, and performance (Dewey,
1938). Tools provided as scaffolds can support learners in
constructing their own knowledge with less frustration,
more motivation, and more efficiency and innovation in the
learning process. These tools can enhance activity and
thinking (Vygotsky, 1978). Computers have become a versatile tool through which one can gain a wide variety of
learning experiences. In particular, the integration of simulation, graphics, and animation enables users to experience
and witness processes that might not otherwise be readily
observable and comprehendible. This makes computerbased simulations a powerful tool for learning.
In simulation-based learning environments “the main task
for the learner is to infer the characteristics of the model underlying the simulation” (de Jong and van Joolingen, 1998).
In other words, the simulation environment provides learners with observations and experiences that they must attempt to explain, assimilate, and combine with their existing
knowledge. One way to do this is to provide scaffolds and
guidance that help users generate hypotheses, use interactive
controls to manipulate the simulation, run experiments and
verify the hypotheses, and then apply the learnt knowledge

Scientific Discovery Learning

1539

Scientific discovery learning environments provide the
learner with resources and tools that help them deduce properties of a specific domain by running controlled experiments or by accessing relevant data that is made available in
the environment (van Joolingen and de Jong, 1997). They
promote a constructivist form of learning where students
start with a science problem, formulate hypotheses, design
experiments, and then discover the relations needed to solve
the problem. The learning environment may include additional resources and tools to help students focus on the primary entities and relations that are important to the discovery learning process (de Jong and van Joolingen, 1998).
Simulations can form the core of scientific discovery
learning. They represent models of the underlying phenomena or domain of study (Wilensky, 2006). The primary task
for the student is to run experiments with the simulation and
inspect the observed behaviors to discover the model or

parts of the model. The experiments require setting simulation parameters within allowable ranges, and using the behaviors generated for testing different operating conditions.
Researchers believe that this approach has great potential
for education (Sandoval and Reiser, 2003).
de Jong and van Joolingen (1998) conducted a survey of
pedagogical simulations and identified several common impediments to their effective use. This included: (i) hypothesis generation, where students had difficulty successfully
forming and understanding what a hypothesis is, and failed
to adapt their hypotheses to available data; (ii) design of experiments, where students created experiments that had no
relationship with the variables of the hypothesis, or modified too many variables, which produced useless or incomprehensible results; and (iii) interpretation of data, where
students had trouble finding regularities in data or interpreting the meaning of data supporting the hypothesis. Moreover, being novices in scientific reasoning, students tended
to force the data to fit their pre-conceptions and misconceptions. In some situations the lack of a problem solving context often hindered students from self-assessing and evaluating their own learning as they worked in the simulation environment.

yond their capabilities. The tools should also assist learners
in interpreting data generated by the simulation.

Self-Assessment
Effective learning requires learners to have the ability to assess their learning, receive feedback, and revise their learning process (Bransford, 2000). Assessment should occur
not only after, but during the learning process. A simulation
environment should include methods for formative assessment, which allow learners to monitor their learning and enable them to correct their mistakes and misconceptions.
Self-assessment mechanisms can assist learners in developing metacognitive skills.

Transfer

Designing Simulation Environments for Learning about Dynamic Processes

The transfer of acquired knowledge into other representations and problem solving situations promotes deep learning
(Bransford and Schwartz, 1998). This is particularly important in a simulation learning environment, where the learner
has to understand the underlying rules of the simulation
model. If the learning environment helps the student to apply this to other problems, students will have the opportunity to explicitly think about and encode these rules, rather
than merely observing the rules in action.

Implementing the Simulation-Based Learning
Environment

The goal of a simulation-based learning environment is to
promote deep learning where students not only learn direct
relations between observed variables, but develop an understanding that allows them to transfer this knowledge to other
problem-solving situations. Deep learning requires the ability to understand, interpret, and reason with the underlying
model of the dynamic processes that govern the simulation
(Biswas, et al., 2005). To address the difficulties of scientific discovery learning in simulation environments discussed in the last section, and to ensure that students can
apply their learnt knowledge to problem solving tasks, we
developed four primary design goals.

In this section, we describe the implementation of a simulation-based learning environment called Ranger Joe based on
the design goals listed above.

Guided Exploration
Learning of complex phenomena involves iterative cycles of
goal setting, planning, executing, and assessment (White,
1999). For scientific discovery learning, this translates to
cycles of hypothesis generation, design of experiments, and
interpretation of data. Simulation environments must provide resources and tools that guide learners through conducting experiments. The learner should be free to explore
but the learning environment should guide the user towards
those experiments which reveal the important aspects of the
“inspectable” model.

Figure 1: The Ranger Joe environment

Guided Exploration and Self Assessment

Scaffolding

Ranger Joe features the Challenge Zone, which guides the
learner’s interaction with the simulation. It provides important background information to help students generate relevant hypotheses and suggestions for setting up and running
experiments. The Challenge Zone poses a series of questions to help focus student learning. The questions may direct students to run the simulation under specific conditions
and compute specific parameter values or infer relations between entities to interpret the results of the simulation.

Scaffolding provides a framework for exploration in a
meaningful context. Scaffolding must also provide tools and
resources to aid students with tasks that are currently be-

1540

Students’ scores are based upon the number of attempts
they need to correctly answer a Challenge Zone question.
This gives the students the opportunity to assess their understanding of the simulation, and do it by understanding the
phenomena rather than just guessing an answer.

original entity. A new, temporal version of Betty’s Brain
incorporates reasoning mechanisms for such cycles of behavior. Users specify cycles as they teach Betty and include
information about time.

Figure 2: The Challenge Zone
Figure 3. Betty’s Brain

Scaffolding

Implementing the Simulation Model

The Ranger Joe environment also provides a set of tools that
help facilitate the understanding of dynamic processes in
terms of two parameters: the baseline value and cycle time.
The baseline calculator helps students compute the average
value of a time-varying behavior by collecting information
from multiple situations. The cycle time calculator helps
students compute the average cycle time (rate of change) by
noting the occurrence of a sequence of peaks and valleys.
Students are also presented with hypertext resources that
provide supplemental information about the river ecosystem
and simulation. These support tools provide information
about questions posed in the Challenge Zone.

Transfer

The interacting entities in an ecosystem are typically modeled as differential equations or discrete time state space
models. However, this type of model is better suited for expert analysis rather than as a simulation model for a learning
environment. In order to create a simulation model with
underlying rules that can be inspected through exploration,
we have employed a multi-agent approach using NetLogo.
NetLogo is a modeling environment well suited for simulating complex systems using the emergent behavior of independent agents (Wilensky 1999). It provides a robust multiagent programming language and the ability to generate visual representations of the simulated environment and graphs
of the included entities.
This approach accurately reflects the ecological system by
modeling each entity (fish, algae, macroinvertebrates, etc.)
in the river as an agent. Rules associated with each type of
agent are executed at each time step during the simulation.
These agent-specific rules can be relatively simple but still
capture the complexities of a dynamic system. When all of
the agents interact with one another in an environment, the
sum total of their behaviors defines the visible behavior of
the real-world system.
Some of the agent actions are randomized (e.g., fish
movement) or they occur with a certain probability (e.g.,
macroinvertebrate reproduction). Since the simulation is
stochastic, every simulation run may provide a slightly different result quantitatively, even when the simulation parameters are unchanged. This reflects what would happen
in a real world environment. Students quickly realize that
they may need to run the simulation several times and make
multiple measurements or take measurements at several
points to compute average behaviors or derive qualitative
relations that correctly define expected parameter values and
relations between entities. This is an important lesson they
learn about scientific experimentation, hypothesis generation, and verification while working in this environment.

In order to provide a mechanism for students to transfer the
knowledge they learned from the simulation into another
representation, the simulation environment is linked to
Betty’s Brain. The interface to the system is illustrated in
Figure 3. The teaching process is implemented as three primary components: (i) teach: Students explicitly teach Betty
using a concept map representation, (ii) query: Students use
a template to generate questions to see how much Betty has
understood, and (iii) quiz: Students send Betty to take a quiz
and observe her performance on a set of predefined questions. Once taught, Betty uses qualitative reasoning methods
to reason through chains of links (Forbus 1984; Biswas, et
al. 2005) to answer questions, and, if asked, explain her reasoning using text and animation schemes. Betty also provides feedback on students’ teaching behaviors to help students develop metacognitive strategies (Tan, Biswas, and
Schwartz 2006).
Detailed descriptions of Betty’s Brain and the results of
several studies run with fifth-grade students are described
elsewhere (Tan, Biswas, and Schwartz 2006). While the
ideas of interdependence and causal chains were taught by
the previous versions of Betty’s Brain, dynamic and temporal ideas were not. It may be the case that an entity decreases, affects other entities, which then cycle back to the

1541

where they are presented with a fuller simulation including
the additional elements of dissolved gases (oxygen and carbon dioxide) and algae. After answering a new set of Challenge Zone questions, students return to teach Betty about
the additional cycle. Students have completed the entire
task when Betty passes the final temporal quiz.

The River Ecosystem Models
Two models are developed for this simulation. The first
models the fish-macroinvertebrates cycle. The second models a more complex system by including algae, dissolved
oxygen, and dissolved carbon dioxide. By breaking the system dynamics into multiple cycles, the students might progressively learn the relationships that produce the dynamic
cyclic behavior (balance), and not be overwhelmed by the
complexity of the model.
The fish-macroinvertebrates cycle is a simple predator/prey model, in which one species consumes the other. A
very clear cyclical behavior emerges, where the decrease in
one entity results in the increase in the other, until some
minimum or maximum threshold is reached and the growth
is reversed. In the second model, algae and macroinvertebrates form a second predator/prey cycle.
The idea of balance is inherent in this system. Should an
extreme amount of either entity exist, the system will go out
of balance, resulting in a deviation from the regular, cyclical
growth behavior.

Results

Experimental Design

The pre and post tests consisted of 3 free response questions
(Q1-Q3) and 7 multiple choice questions (Q4-Q10). In previous studies, students using Betty’s Brain showed significant improvement on post test questions that required understanding the concept of interdependence (Leelawong,
2005). However, they did not show significant improvement in understanding the concept of balance in an ecosystem. The 3 free response questions on the pre and post tests
for this study focused on gauging students’ understanding of
balance. An example of these questions is, “Explain in your
own words what it means for a river to be in “balance.”
These free response questions carried a maximum value of 6
points each. The multiple choice questions were directed at
extracting information from graphs. Students’ answers
were scored as either right or wrong (1 or 0 points). Students were also given 5 additional questions on the post test
which involved reasoning with a cyclical graph (similar to
the concept map representation they used to teach Betty).
An example of one of these questions is, “If the amount of
fish increases, what would happen to the number of macroinvertebrates after 10 days?” These were questions Q11Q15. All of the non-multiple-choice questions were handgraded by two coders according to a pre-defined rubric.
The table below presents the results of paired t-tests
which determine if there was a significant increase from pre
to post test scores across all students. The mean pre test
score and post test score are reported for each category of
questions. We did not look at the total score because an aggregated value of both free response and multiple choice
questions would require a scaling of how much each type of
questions is worth in relation to the other.

46 students from two fifth-grade classrooms participated
in the study. The students were given a pre test which contained questions on concepts such as balance, causality,
chains of events, and reading and interpreting graphs. Students were given five 50-minute sessions to work with the
Betty’s Brain/ Ranger Joe system each day. Each student
was required to go through two iterations of teaching Betty,
using the simulation, and going back to teach Betty temporal information learned from the simulation. A week after
using this system, the students were given a post test. The
post test contained the same questions as the pre test along
with 5 additional questions related to reasoning with a concept map containing cycles. Analysis of the pre and post
tests would determine if the system improved students’ understanding of balance and dynamic processes.
In the first iteration, students teach Betty about fish and
macroinvertebrates. The relationships between the entities
at this point are direct relationships with no temporal information. Betty must pass a quiz which asks questions about
how fish and macroinvertebrates are related. After successfully getting Betty to pass the first quiz, the student is
prompted to visit Ranger Joe.
In the students’ first visit to Ranger Joe, the simulation
only displays fish and macroinvertebrates. The students are
guided through the simulation environment by the Challenge Zone questions and hypertext resources. After using
the simulation to answer all of the Challenge Zone questions
correctly, the students return to Betty and add temporal information to the concept map to teach Betty about the fish
and macroinvertebrate cycle. Betty must again pass a quiz,
but this time, the questions involve temporal information.
In the second iteration, students add the concepts of oxygen, carbon dioxide, and algae to the concept map. Again,
Betty must pass a non-temporal quiz containing information
on these concepts. Students then return to Ranger Joe,
1542

Table 1: Paired t-test results for pre to post test scores
(n=46, df=45)
Free
response
Multiple
choice

Pre
(std dev)
9.26
(3.87)
4.41
(1.34)

Post
(std dev)
11.50
(3.48)
5.04
(1.41)

t
4.020

Sig
(1-tailed)
< 0.001

2.655

0.006

There was a highly significant improvement from pre to
post on the free response questions (p<0.001) and a significant improvement on the multiple choice questions
(p<0.01).
Table 2 presents the results of additional paired t-tests determining which questions had significant increases from
pre to post test. (For the multiple choice questions Q4-Q10,
the mean represents the percentage of students answering
correctly).

Table 2: Paired t-test results for individual questions with
significant pre to post test differences (n=46, df=45)
Q1
Q3
Q5
Q6
Q9
Q10

Pre
(std dev)
2.93 (2.3)
2.61 (1.86)
0.70 (0.47)
0.46 (0.50)
0.24 (0.43)
0.46 (0.50)

Post
(std dev)
3.72 (2.13)
3.74 (1.88)
0.85 (0.36)
0.65 (0.48)
0.41 (0.50)
0.63 (0.49)

t

14, and 15 all showed a positive correlation with the Challenge Zone score. Questions 6, 7, 14, and 15 were highly
significant at the p<0.01 level. The others were significant
at the p<0.05 level.
Finally, we wanted to see how the students’ usage of the
self-assessment questions correlated with their behaviors in
Betty’s Brain. Three actions: quiz request, query, and resource access where analyzed and their correlations with the
Challenge Zone score are listed below in Table 5.

Sig
(1-tailed)
0.013
0.001
0.035
0.014
0.022
0.037

2.299
3.644
1.855
2.280
2.070
1.834

Table 5: Pearson correlation results between Challenge
Zone score and frequency of activities in Betty’s Brain

When looked at individually, the two questions that
showed the most improvement from pre to post were questions 1 (p<0.05) and 3 (p<0.005). These were both free response questions that dealt with the concept of balance.
Students also showed significant improvement on multiple
choice questions 5, 6, 9, and 10 (p<0.05). Many of the other
multiple choice showed no significant improvement due to a
ceiling effect.
In addition to the pre to post test score differences, we
looked at the students’ usage of the Ranger Joe Challenge
Zone self-assessment questions. We hypothesized that students using the self-assessment mechanism more effectively
would have been able to learn more from the simulation environment and therefore, perform better on the post test.

Quiz requests
Queries
Resource Accesses

Free response
Multiple choice
Cyclical graph
reasoning

Sig
(1-tailed)
0.139
0.083

0.414

0.004

By using the simulation environment and teaching Betty,
students showed significant gains from pre to post test. Use
of the system improved their performance overall on both
the free response and the multiple choice questions (Table
1). Students showed the most improvement on Q1 and Q3
(Table 2). These questions show that students were able to
better understand and explain a chain of events leading to a
system going out of balance over time. Students who used
Betty without the Ranger Joe simulation system did not exhibit this understanding (Leelawaong, 2005).
Almost all of the questions without a significant increase
from pre to post did not show this gain because of a ceiling
effect. These questions dealt primarily with reading information from a graph, which shows that the 5th graders were
quite conversant in using graphs.
The use of the Challenge Zone self-assessment mechanism in the simulation environment varied. Some students
used the questions to carefully conduct experiments in the
simulation and find answers. These students received
higher Challenge Zone scores because they tended to get the
answer correct on the first or second try. Others did not see
the value in self-assessment, and tried to guess on some of
the more difficult questions. These students received lower
Challenge Zone scores because it took multiple attempts for
them to get the answers right. Students who did well on the
Challenge Zone tended to score higher on the cyclical graph
reasoning questions (see Table 3). In particular, those who
utilized the self-assessment mechanism more effectively
were able to reason with multiple cycles over longer periods
of time on Q13, Q14, and Q15 (see Table 4).
Finally, we wanted to see how well students were able to
transfer the knowledge they learned from the simulation into
another representation. The students’ use of the simulation
environment should have an effect on their performance in
Betty’s Brain. In past studies with Betty’s Brain, we have
looked at the students’ concept map quality, behavior patterns, and their ability to construct similar concept maps in
other domains (Biswas, 2005; Tan, 2006). Because most of

There was a significant positive correlation between the
Challenge Zone score and the five extra post test questions
(p<0.005). However, there was no significant correlation
between the Challenge Zone score and the free response
questions or the multiple choice questions.
The next table lists the individual questions that showed a
positive correlation between students’ Challenge Zone score
and that individual post test question score.
Table 4: Pearson correlation results between Challenge
Zone score and individual post test question scores
Q2
Q3
Q6
Q7
Q8
Q13
Q14
Q15

Pearson
correlation
0.285
0.284
0.384
0.340
0.274
0.257
0.374
0.352

Sig
(1-tailed)
0.080
0.115
0.085

Discussion

Table 3: Pearson correlation results between Challenge
Zone score and post test score
Pearson
correlation
0.163
0.207

Pearson
correlation
-0.198
-0.170
-0.194

Sig
(1-tailed)
0.027
0.028
0.004
0.010
0.033
0.042
0.005
0.008

Free response questions 2 and 3; multiple choice questions 6, 7, and 8; and cyclical graph reasoning questions 13,
1543

the students had similarly complete concept maps at the end
of the study, the concept maps were not graded. We were
able to look at some of the behavioral data as it related to
the students’ use of the simulation environment and Challenge Zone self-assessment (see Table 5).
Students use the quiz request action in Betty’s Brain to
get Betty to take a quiz. We have found in previous studies
that better students use the quiz request function less (Tan,
2006). It is more effective to spend time carefully constructing and debugging the concept map than to repeatedly
ask Betty to take the quiz after each small edit to the map.
Students who are less successful in teaching Betty revert to
this trial-and-error pattern. Quiz requests were slightly
negatively correlated (at p<0.10 level) with the Challenge
Zone score. This suggests that the students who used the
simulation more effectively were able to successfully teach
Betty with less quiz attempts.
When teaching Betty, students have access to a set of hypertext resources which provide domain knowledge to assist
the student with constructing a correct concept map. Resource accesses were slightly negatively correlated (at
p<0.10 level) with the Challenge Zone score, suggesting
that students who used the simulation more effectively did
not need to access the resources as much in order to construct the concept map representation. This may be because
they learned more from the simulation environment than the
other students.
Further analysis on the students’ behavior and performance in Betty’s Brain needs to be completed in order to better understand how well the students were able to encode
the model they learned from the simulation into the representation used to teach Betty.

Conclusions
Our simulation-based learning environment provides students with guidance for scientific discovery learning, and
opportunities for self-assessment and transfer as they
learned about river ecosystems. Students using the learning
environment showed significant improvement from pre to
post test on important topics and demonstrated understanding of dynamic processes and balance in an ecosystem. Although all of the students used the same learning environment, we were able to evaluate the role of self-assessment
by studying the students’ Challenge Zone use and correlating self-assessment with learning gains. We found that students who were better at assessing their own learning were
able to perform better on the post test. Additionally, they
seemed to be better equipped to transfer what they learned
into another representation.
Our use of simulations suggests that when embedded in a
well-designed learning environment, they can be a useful
tool for scientific discovery learning. Based on these findings, we plan to improve on our current system and run a
more in-depth study which further investigates the students’
ability to transfer knowledge learned in the simulation.

Acknowledgments
This work has been supported by a NSF REESE Award
#0633856 and a Department of Education IES grant
#R305H060089.

References
Biswas, G., Schwartz, D., Leelawong, K., Vye, N., and
TAG-V (2005). Learning by teaching: A new agent paradigm for educational software, Applied Artificial Intelligence, special issue on Educational Agents, 19(3), 363392.
Bransford, J.D. & Schwartz, D.L. (1999). Rethinking transfer: A simple proposal with multiple implications. Review
of Research in Education, vol. 24, pp. 61-101.
Bransford, J.D., Brown, A.L., et al., Eds. (2000). How People Learn. Washington, D.C., National Academy Press.
Forbus, K. (1984), Qualitative Process Theory, Artificial Intelligence, 24.
de Jong, T., & van Joolingen, W. R. (1998). Scientific discovery learning with computer simulations of conceptual
domains. Review of Educational Research. 68(2): 179201.
van Joolingen, W. R., & de Jong, T. (1997). An extended
dual search space model of scientific discovery. Instructional Science, 25, 307-346.
Leelawong, K. (2005). Using the Learning-by-Teaching
Paradigm to Design Learning Environments. Ph.D. Thesis. Department of EECS, Vanderbilt University, Nashville, TN.
Sandoval, W. A., & Reiser, B. J. (2003). Explanation-driven
inquiry: Integrating conceptual and epistemic scaffolds
for scientific inquiry. Sci Ed, 88, 345-372.
Schwartz, D.L. Bransford, J.D. & Sears, D.L. (2005). Efficiency and innovation in transfer. J. Mestre (Ed.), Transfer of learning from a modern multidisciplinary perspective (pp. 1 - 51). CT: Information Age Publishing.
Tan, J., Biswas, G., and Schwartz, D. (2006), Feedback for
metacognitive support in learning by teaching environments. The 28th Annual Meeting of the Cognitive Science
Society, Vancouver, Canada, 828-833.
Vygotsky, L.S. (1978). Mind in society: The development of
higher psychological processes. Cambridge, MA: Harvard University Press.
White, B., Shimoda, T., and Frederiksen, J. (1999). Enabling students to construct theories of collaborative inquiry and reflective learning: Computer support for metacognitive development. International Journal of Artificial
Intelligence in Education, vol. 10, pp. 151-182.
Wilensky, U. (1999). “NetLogo” [Computer software].
Evanston, IL: Northwestern University, Center for Connected Learning and Computer-Based Modeling,.
Wilensky, U. & Reisman, K. (2006). Thinking like a wolf,
a sheep or a firefly: Learning biology through constructing and testing computational theories - An embodied
modeling approach. Cognition & Instruction, 24(2), 171209.

1544

