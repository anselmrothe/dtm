UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
LSA: First dimension and dimensional weighting

Permalink
https://escholarship.org/uc/item/0p3526z0

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)

Authors
Hu, X.
Cai, Z.
Franceschetti, D.
et al.

Publication Date
2003-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

LSA: First dimension and dimensional weighting
Hu, X.1 , Cai, Z.1 ,Franceschetti, D.2 ,
Penumatsa, P. Graesser, A. C.1 , Louwerse, M. M.1 ,McNamara, D. S.1
and the Tutoring Research Group1
1,

1

Department of Psychology;
2
Department of Physics;
The University of Memphis, Memphis, TN 38152

Abstract
We report two discoveries concerning Latent Semantic Analysis (LSA). First, we observed the
special properties of the …rst dimension of the
LSA space. Second, we observed that dimensional
weighting plays an important role in LSA analysis.
Based on the …rst discovery, we examined the cosine
matches without the …rst dimension. Based on the
second discovery, we explored di¤erent dimensional
weighting schemes. Based on these observations, we
recommend a new algorithm for LSA cosine computation such that LSA becomes more sensitive to
relevant similarities and di¤erences.

Introduction
Latent Semantic analysis (LSA) is an application
of principal component analysis (PCA) to natural
language understanding. LSA reduces a large body
of documents into a compact matrix representation
(with only a few hundred columns), such that each
row of this matrix represents a word (or a token)
that appears in the document collection in the form
of vector. Such a mathematical representation of
words as vectors captures the semantic relationship
between words in the following way. If two words
appear in the same document environment, accompanied by similar words, then the vector representations of the two words are similar; that is, the
normalized dot product (cosine) of the two vectors
is close to 1. In the same way similarity measures
can be obtained for sentences, paragraphs, or documents. For instance, for any document consisting
of a list of words, the vector corresponding to a sentence, paragraph, or document is simply a weighted
vector summation of the vectors for the included
words. This property of a reduced semantic matrix
representation has proven to be extremely useful in
a range of applications in natural language processing (s ee Foltz, K ints ch , an d Landauer, 1998 for an
overview).
LSA was originally developed for information retrieval (Deerwester, Dumais, Furnas, Landauer, &
Harshman, 1990). In those cases, for any query document, the corresponding vector (called the query
vector) is used to obtain a cosine match with all rows
of the document vectors corresponding to the documents in corpus. By using the cosine match as the

587

similarity measure, best candidates can be obtained
and they are assumed to be semantically similar to
the query document. Application of this use of LSA
has been documented in several of our current applications (Olde, Franceschetti, Karnavat, Graesser, &
TRG, 2002; Graesser, Hu, Person, Jackson, & Toth,
2002).
Later uses of LSA went beyond document retrieval
to measure semantic similarity between documents
(Wiemer-Hastings, Wiemer-Hastings, Graesser, &
TRG, 1999). This use of LSA is more e¢cient than
retrieving documents from the original corpus. To
determine the semantic similarity of two documents,
only one cosine is needed and is computed from
word vectors, whereas thousands of cosine calculations must be completed for document retrieval. In
recent years, researchers have used LSA to compute
semantic similarities in a variety of applications. For
instance, LSA has been used as an automated essay
grader, comparing student essays with ideal essays
(Foltz et al., 1998). It has been used as a measurement of coherence between successive sentences
(Foltz et al., 1998). LSA has in fact shown to perform as well as students on the TOEFL test (Test
Of English as a Foreign Language) (Landauer & Dumais, 1997) and can even be used for understanding metaphors (Kintsch, 2000). Finally, LSA has
played a crucial role in the representation of world
knowledge in intelligent tutoring systems. For example, LSA is used in AutoTutor, an intelligent tutoring system that has tutorial conversations with
students on a variety of topics. Currently, AutoTutor has been developed for computer literacy and
conceptual physics (Graesser, VanLehn, Rose, P., &
Harter, 2001). AutoTutor uses LSA to give meaning to a student answer and to match that answer to
ideal good answers and bad answers (Graesser et al.,
2000; Franceschetti et al., 2001; Olde et al., 2002).
In our work on AutoTutor, we observed that when
similarity is computed between two documents, the
cosine value is positively related to the size of these
documents. We believe this is an unfortunate artifact and demonstrate here that the performance
of LSA can be improved signi…cantly by excluding
the …rst dimension of the LSA space in calculating
the cosines between document vectors. To explain

how the performance of LSA can be improved, we
…rst brie‡y describe the mathematical foundations
of LSA and how LSA is used in AutoTutor. We
then prove two theorems that show how LSA document matching can be improved. Finally we provide
some general recommendations on how LSA should
be used in natural language understanding applications.

Mathematical foundations of LSA
The fundamental assumption of LSA is that the
semantics of any natural language are expressed
by the way humans use that language. According to this assumption, the meaning of words is
entirely based on co-occurrences with other words.
When the language units share a common environment, they must be similar in meaning. Two words
have similar meaning if they accompanied by similar
words(Landauer & Dumais, 1997). For example, for
a given corpus of text that contains N distinct document environments de…ned as sentences, paragraphs,
or clusters of paragraphs, any word that occurs in
the corpus can be expressed as a N -dimensional vector. Each value in the vector is either non-zero (a
function of how the words appear in the paragraph)
or a zero value (in case the word is not in the paragraph).
Consider Grolier’s (1996) Multimedia Encyclopedia as a corpus. This corpus contains 44; 227
paragraphs with a total of 76; 948 distinct words.
A vector with 44; 227 dimension can be used to
uniquely identify each of the distinct words. In this
vector, the components along each dimension are
computed as a function of how often each word is
used in the corresponding paragraph. For example, assume the mth word is table (in the list of
76; 948 distinct words) and appears in paragraph
n, then the vector representation for table has a
non-zero value at the nth element with a value of
f (m; n) £ G (m) £ L (m; n), where: f (m; n) is a
function of the frequency of table occurring in the
m th paragraph; G (m) represents the weight of the
m th word independent of the paragraph in which
it occurs; and L (m; n) represents the weight of the
m th word (i.e., table) depending on the paragraph
in which it occurs (i.e., the nth paragraph). In the
literature (Deerwester et al., 1990), G (m) is called
global weighting and L (m; n) is called local weighting.
Assume matrix A is obtained for a given corpus, then singular value decomposition (SVD) is performed on A. SVD will produce three matrices: Two
orthonormal matrices, U and V, and one diagonal
matrix § = diag (¾ 1 ; :::; ¾ r), with elements appearing in descending order, where r is the rank of A;
such that
A = U§V | :
(1)
LSA truncates the U matrix into k columns (from

588

original m columns) and use the truncated U matrix, called term-matrix (throughout the remainder
of this paper we will use U k to denote the truncated
matrix). Each row of the U k is a k dimensional realvalued vector. Similarly, the V matrix is also truncated into k columns from original n columns. The
truncated V is called document-matrix (denoted as
V k ).
In LSA, words are represented as real-valued vectors from the truncated U matrix. For any paragraph, a real-valued vector can be computed from
the vectors that correspond to the terms in the paragraph. The formula for obtaining the vector is in the
form
x| WUk ¤ k ;
(2)
where x is an m £ 1 vector with entries either 1
or 0, W is a m £ m diagonal matrix and ¤k is a
diagonal k £ k matrix. After vectors are obtained
for each paragraph, then the similarity of the two
paragraphs are measured as the cosine value of the
two vectors:
|

s (x1 ; x2 ) =

(x|1 WU k ¤ k ) (x|2 WU k ¤k )
:
kx|1 WUk ¤ k k kx|2 WU k ¤ k k

(3)

Accordingly, we can formulate the following de…nition:
De…nition Assuming all words are indexed from
1 to m; x1 = (x 1;1 ; :::; x 1;m ) and x 2 =
(x 2;1 ; :::; x 2;m ) are two m£1 vectors corresponding
two documents, such that
½
1
word j is in document i
x i;j =
0 word j is not in document i
The similarity of the two documents is de…ned as
Equation (3).
When k = r then U k = U and ¤ k = ¤:

Observations and Theorems
In this section, we …rst report observations and results related to the special properties of the …rst dimension of LSA space. Then we extend these …ndings to explore alternative ways of computing similarity measures using di¤erent dimensional weighting.

The importance of the …rst dimension in
LSA
As described earlier, the intelligent tutoring system
AutoTutor evaluates student answers by comparing
them with ideal good answer information and bad
answers as de…ned by experts in the system’s course
curriculum scripts. Physics textbooks are used to
create an LSA space (see (Franceschetti et al., 2001;
Olde et al., 2002)). Based on the LSA cosine match

between student answers and ideal good and bad answers, AutoTutor decides whether or not a student
answered the question correctly and determines its
next tutorial dialog move. However, the length of
the utterances in the student answer and AutoTutor’s expectation show an interesting phenomenon.
We randomly selected n (n = 1; 2; 4 : : : 512) words
from a random po ol of student contributions. That
is, we mimicked student contributions with variable numbers of words. Next, we randomly selected
m (m = 1; 2; : : : 512) words from the expectations.
Mimicking the authored physics curriculum scripts
with variable numbers of words, entries in Table 1
show the average cosine match values (100 cosines
computed per cell) between the student contributions and the curriculum scripts. Notice that the
average cosine match between documents with 256
words is 0:450, while the average cosine between documents with 16 words is 0:086. Our …rst observation
below is:
Observation 1 The cosine match between documents is monotonically related to the number of
words contained in documents.

4
16
64
256
512

4
0:033
0:047
0:079
0:106
0:107

16
0:051
0:085
0:129
0:167
0:176

64
0:093
0:146
0:244
0:324
0:336

256
0:130
0:211
0:345
0:458
0:483

512
0:152
0:245
0:390
0:509
0:535

to the eigenvalue ¾ 21 can be written as (Power Approximation Method)
(AA| )
x
°
x1 = lim °
| k+1 °
k!1 °
°(AA )
x°
k+1

(4)

where x is any vector in Rm that is not orthogonal to x1 . We can always …nd an x not orthogonal
to x1 from the m orthonormal vectors e1 ; :::; em ,
where the i th entry of e i; (i = 1; :::; m) is 1 and
all other entries are 0. Therefore, we can assume
that all entries of x are non-negative. Because all
entries of A are also non-negative, from (4) we
can see that all entries of AA | are non-negative.
Since §x1 are the only two unit eigenvectors corresponding to ¾ 21 , the …rst column of U is one of
these two vectors, their entries are either all nonnegative or all non-positive.
To further examine the numerical details, we have
examined the arithmetic means of the elements in
each column of the U matrix. We observed that the
…rst column has a signi…cantly larger mean than all
other dimensions. This can be measured by the ratio between the mean of the …rst dimension and the
length of the vector of the means for all the dimensions. Denote m i as the mean for ith dimension. For
example, for
physics LSA space (with 338 dimenÁ the
qP
338
2
sions), m 1
i=1 m i = 0:833:Table 3 shows the
same observation we found across corpora.

Table 1: Average cosine values for di¤erent document sizes.
To understand the above observation, we must
further examine the LSA spaces that are used in
AutoTutor. We observed that the …rst dimension of
each term vector always had the same sign (either
all non-negative or all non-positive). We found the
same phenomenon within a variety of other corpora
(encyclopedia, science texts, etc.).
The following theorem provides a formal proof for
the above observation.
Theorem 1 Let A be an m £ m real matrix with all
entries non-negative. Suppose the singular value decomposition of A is (1), where § = diag (¾ 1 ; :::; ¾ r ).
Assume ¾ 1 > ¾ 2 ¸ :::; ¾ r . Then all entries in the
…rst column of U are of the same sign (all nonnegative or all non-positive).
Proof The …rst column of U is actually an eigenvector with a unit length corresponding to the
maximum eigenvalue ¾ 21 of the symmetric matrix
AA| . Since ¾ 21 is greater than all other eigenvalues of AA| , an eigenvector of AA | corresponding

589

Space

# Dim

Physics
Science
Encyclopedia
Narrative

338
365
496
277

1 st Dim
Mean
0:00528
0:00348
0:00079
0:00139

Mean
length
0:00634
0:00479
0:00119
0:00270

Table 2: Relative magnitude of …rst dimension for
selected LSA spaces.
Observation 2 The magnitude of the …rst component of any document vector is signi…cantly larger
than all other components.
Observation 2 and Theorem 1 explain Observation
1. For any document with a large number of terms,
the LSA vector of the document is simply the vector summation of the term vectors. While all other
dimensions may cancel out, the …rst dimension just
keeps adding up, so it is the value of the …rst dimension that drives the cosine value. This critical
observation led us to further examine the numerical
properties of the …rst dimension. We observed that
the common words, such as is, the, and an, have
larger values in their …rst dimension and rare words

Ratio
0:833
0:726
0:669
0:516

have smaller values. In LSA, a measure of the commonness is measured by weights. We examined one
of the LSA spaces used in the AutoTutor project.
The correlation between the …rst dimension and the
weights was as high as ¡0:757 (6679 words). Table
3 shows corresponding quantities for all other corpora for which we have created LSA spaces. These
spaces include physics textbooks, science textbooks,
Grollier’s encyclopedia, the encyclopedia augmented
with WordNet, and a large sample of narrative texts.
Observation 3 Weights and the …rst dimension of
the LSA vectors are negatively related.
Space Name
Physics
Science
Encyclopedia
Narrative

4
16
64
256
512

64
¡0:06
0:03
0:06
0:17
0:14

256
0:07
0:01
0:02
0:12
0:12

2. If ¤ = §; then Equation (3) is equivalent to similarity comparison using original term-document
matrix.
|

=
=

The above observations and theorems point out
that the …rst dimension of LSA vector needs special treatment. Table 4 shows the cosine matches
between document vectors without the …rst dimension. We observed that removing the …rst dimension
made a greater di¤erence when the documents being compared were large. Notice in Table 4, that
when documents have fewer than 64 words (which is
more than most student contributions and expectations in AutoTutor), the LSA cosine matches are no
longer a function of the document size. In contrast,
when documents have more than 64 words, the cosine matches increase as a function of document size,
but in a smaller magnitude than when the …rst dimension is retained.
16
¡0:02
0:00
0:09
0:10
0:13

1. If ¤ is a unit matrix, then Equation (3) is equivalent to weighted word matching.

Proof : 1) Assume k = r and ¤ is unit matrix,

#1
¡0:757
¡0:686
¡0:573
¡0:500

Table 3: Correlations between the weight for words
and the …rst dimension for the words.

4
¡0:02
¡0:03
0:01
0:05
0:04

explored di¤erent dimensional weighting in the computation of the cosine match between documents.
First, we examined Equation (3). We obtained the
following formal results that link di¤erent measures
of similarity in a uni…ed formulation.
Theorem 2 From the De…nition before, assuming
k = r; then

512
¡0:00
0:11
0:08
0:22
0:20

Table 4: Average cosine values as a function of document size. For these cosines, the …rst dimension
was removed, otherwise, the computations for the
cosines were the same as those in Table 1.

The importance of dimensional
weighting in LSA
In the previous section, we showed the importance
of the …rst dimension of LSA vector. This …nding
motivated us to explore di¤erent ways to obtain cosine matches between documents. In particular, we

590

(x|1 WU¤) (x|2 WU¤)
|
|
(x1 WU¤¤U | Wx2 ) = (x1 WUU | Wx2 )
|
|
|
|
(x1 WWx2 ) = (x1 W) (x2 W)

so,
|

|

(x|1 WU¤) (x|2 WU¤)
(x|1 W) (x|2 W)
=
:
|
|
|
|
kx1 WU¤k kx2 WU¤k
kx1 Wk kx2 Wk

Notice that

x|1 WWx2 =

X© ¯
ª
w 2i ¯ word i is in both documents

2) Assume k = r and ¤ = §;

|

(x|1 WU§) (x|2 WU§ )
|
|
= x1 WU§ § | U | Wx2 = x1 WU§V | V§ | U | Wx2
|
|
= (x|1 WU§V| ) (x|2 WU§V | ) = (x|1 WA) (x|2 WA)
therefore:
|

|

(x|1 WU¤) (x|2 WU¤)
(x|1 WA) (x|2 WA)
=
:
kx |1 WU¤k kx|2 WU¤k
kx|1 WAk kx|2 WAk
LSA truncates the U matrix into U k ; which only
contains a few hundreds columns and is used as an
approximation of the original term-document matrix
A. Theorem 2 and the De…nition presented before
can be understood as the following: (1) without dimensional weighting, the LSA cosine matching approximates weighted keyword matching, and (2) using dimensional weighting with singular values, LSA
cosine matching approximates context similarity.
To explore other possible dimensional weighting,
we examined cosines using several di¤erent dimensional weighting schemes: Consider the dimensional
weight matrix ¤ = diag (¸ 1 ; :::; ¸ k ) ;
¸1
¸1
¸1
¸i
¸i
¸i

=
=
=
=
=
=

0; ¸ i = ¾ i ¡ ¾ k+ 1; i ¸ 2
0; ¸ i = 1; i ¸ 2
0; ¸ i = ¾ i; i ¸ 2
¾ i ¡ ¾ k+1 ; i ¸ 1
1; i ¸ 1
¾i ; i ¸ 1

(5)
(6)
(7)
(8)
(9)
(10)

Figure 1 gives examples showing LSA performance
for the two dimensional weighting schemes. We systematically changed the number of dimensions used
in the LSA space (from 100 to 400; step size 10).
Using the weighting in (5) above, for related words
(for example, cat and dog) and non-related words
(for example, force and blue), the cosine match
is a smooth function of the number of dimensions
used. This is not true for the weighting method (6)
above.Researchers have previously examined the op1
0.8
0.6
0.4
0.2
0
-0.2
-0.4

toTutor pro ject proposal, pair-wise dissimilar to the
sentences in set I. We computed the LSA cosines
between the similar pairs and dissimilar pairs. The
discriminability was then calculated by
d=

Ã

m1 ¡ m2

,r

sd21 + sd22
2

!

where m1 and sd1 are the mean and standard deviation for the cosine matches between set I and set II
(cosine for the similar pairs), and m2 and sd2 are the
mean and standard deviation for the cosine matches
between set III and set II (cosine for the dissimilar pairs) The results of these analysis are shown
in Table 5. We observed better performance for the
weighting method (5) in the sense that it provides
the highest discriminability.

-0.6
"Cat" and "Dog": Not Weighted

"Cat" and "Dog": Weighted

"Blue" and "Force": Not Weighted

"Blue" and "Force": Weighted

keep 1 st dim
Figure 1: Cosines with two di¤erent types of dimensional weighting.
timization of the number of dimensions used for an
LSA space (Landauer & Dumais, 1997). It has been
shown that there exists a range of about 300 § 100
dimensions that provide the best overall LSA performance for most corpora. The above observation
of smooth curves as function of number of dimensions shows that with particular dimensional weighting scheme, there may be di¤erent ways in which the
optimum range of dimensions can be selected.
In addition, the smoothness of the LSA similarity between words when the number of dimensions
change makes intuitive sense. For example, one
would not expect huge di¤erences between cosine
matches using 300 dimensions and 310 dimensions.
The smoothness of the curve we observed here is due
to di¤erential weighting of the dimensions. For example, if one chose 300 dimensions, then the weight
for the 300 th dimension is the smallest (approaching
zero, it plays the least amount of importance in the
computation). We have also explored other dimensional weighting schemes such as (7), (8), (9), and
(10) de…ned above. Weighting method (5) performs
the best in the sense that it provides the smoothest
function.
We further explored the dimensional weighting on
3 carefully selected sets of documents from AutoTutor. Each set had 42 sentences. Sentences in set
I and set II were from the AutoTutor curriculum
script for conceptual physics. Each sentence in set
I was an expected answer of a question; the corresponding sentence in set II was an alternative answer. Therefore sentences in set I and set I I were
similar in meaning in a pair-wise manner. Sentences
in set II I were randomly selected from the NSF Au-

591

remove 1 st dim

not weighted
method (9)
3:408
method (6)
3:573

weighted
method (8)
2:422
method (5)
6:297

Table 5: Discriminability for the four di¤erent
weighting methods.

Conclusions
In this paper, we made two important observations
about LSA. The …rst observation concerns the nonnegative (or non-positive) …rst component of LSA
vectors. This is the primary reason that the similarity measure between any two documents is an
increasing function of the document size. The second important observation (outlined in Theorem 2)
is that there is a uni…ed mathematical expression
for three di¤erent types of document similarity measures, namely (1) LSA cosine match, (2) weighted
word match, and (3) context similarity match. Such
a uni…ed representation led us to explore di¤erent dimensional weighting methods. We observed that the
weighting methods described in (5) outperformed
other weighting methods, including the method used
in the lion’s share of LSA literature. Speci…cally, it
provided the smallest cosine changes as a function
of the dimension of the space and better discriminations between sentence pairs. With the above two
…ndings, we have the following recommendations for
future use of LSA.
Theorem 1 states that the …rst non-zero components of all the LSA term vectors will have the
same sign. When constructing a vector to represent a document by summing term vectors, the …rst
component thus will grow in magnitude while the
other components need not. We therefore recommend against interpreting absolute cosine values as

a measure of the "match" between documents. Instead, we recommend establishing statistical distributions of the cosine match values for the target space
and using these relative units to judge the similarity. For example, for documents x1 and x2 with
document sizes n1 and n2 , if a baseline (theoretical or empirical) distribution with mean ¹n1; n2 and
sdn 1;n 2 is obtained, then the relative similarity can
be computed as the following relative score:
s (x 1; x2 ) ¡ ¹n1 ;n2
sdn1 ;n2

(11)

where s (x1 ; x2 ) is de…ned in Equation (3). This
recommendation is particularly useful when LSA is
used in applications where the similarity measures
are used as selection criteria, as in AutoTutor. It
is not necessary in document retrieval applications,
where the similarity measure is primarily used for
ordering the potential candidates.
Even if one uses relative score in the form of Equation (11), we recommend using dimensional weighting, especially the dimensional weighting matrix in
the form of (5). By using dimensional weighting
scheme outlined in (5), the similarity measures are
relatively robust with respect to number of dimensions used the LSA space. Furthermore, weighting scheme (5) removes the in‡uence of the …rst dimension and thereby increases discriminability when
similarity is used in selecting candidates among multiple alternatives.

Acknowledgments
The Tutoring Research Group (TRG) is an interdisciplinary research team comprised of approximately 35 researchers from psychology, computer science, physics, and education (visit
http://www.autotutor.org).
This research conducted by the authors and the TRG was supported
by the National Science Foundation (REC 0106965),
the DoD Multidisciplinary University Research Initiative (MURI) administered by ONR under grant
N00014-00-1-0600, and Institute of Education Sciences (IES R3056020018-02). Any opinions, …ndings and conclusions or recommendations expressed
in this material are those of the authors and do not
necessarily re‡ect the views of ONR, NSF, or IES.
We would like to thank Tom Landauer and Walter
Kintsch by supplying a number of corpora used in
this study.

References
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). Indexing by latent semantic analysis. Journal of the
American Society For Information, 141, 391407.

592

Foltz, P. W., Kintsch, W., & Landauer, T. K. (1998).
The measurement of textual coherence with latent semantic analysis. Discourse Processes,
25, 285-307.
Franceschetti, D., Karnavat, A., Marineau, J., McCallie, G. L., Olde, B. A., Terry, B. L., &
Graesser, A. C. (2001). Development of
physics text corpora for latent semantic analysis. In Proceedings of the 23rd annual conference of the cognitive science society (p. 297300). Mahwah, NJ: Erlbaum.
Graesser, A., Hu, X., Person, P., Jackson, T., &
Toth, J. (2002). Modules and information retrieval facilities of the human use regulatory affairs advisor (HURAA). In M. Driscoll & T. C.
Reeves (Eds.), Proceedings for e-learning 2002:
World conference on e-learning in corporate,
government, healthcare, and higher education
(p. 353-360). Montreal, Canada: AACE.
Graesser, A., VanLehn, K., Rose, C., P., J., & Harter, D. (2001). Intelligent tutoring systems
with coversational dialogue. AI Magazine, 22,
39-51.
Graesser, A., Wiemer-Hastings, P., WiemerHastings, K., Harter, D., Person, N., & TRG.
the. (2000). Using latent semantic analysis to
evaluate the contributions of students in AutoTutor. Interactive Learning Environments,
8, 128-148.
Kintsch, W. (2000). Metaphor comprehension: A
computational theory. Psyhonomic Bul letin
and Review, 7, 257-266.
Landauer, T. K., & Dumais, S. T. (1997). A solution to plato’s problem: The latent semantic
analysis theory of the acquisition, induction,
and representation of knowledge. Psychological Review, 104, 211-240.
Olde, B., Franceschetti, D., Karnavat, A., Graesser,
A., & TRG. (2002). The right stu¤: Do you
need to sanitize your corpus when using latent semantic analysis? In W. G. Gray &
C. Schunn (Eds.), Proceedings of the 24rd annual conference of the cognitive science society
(p. 708-713). Mahwah, NJ: Erlbaum.
Wiemer-Hastings, P.,
Wiemer-Hastings, K.,
Graesser, A., & TRG. (1999). Improving an
intelligent tutor’
s comprehension of students
with latent semantic analysis. In S. La joie
& M. Vivet (Eds.), Arti…cial intelligence in
education (p. 535-542). Amsterdam: IOS
Press.

