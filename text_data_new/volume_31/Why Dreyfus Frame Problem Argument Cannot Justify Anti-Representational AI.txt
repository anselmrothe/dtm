UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Why Dreyfus’ Frame Problem Argument Cannot Justify Anti-Representational AI

Permalink
https://escholarship.org/uc/item/4kc1g3qv

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)

Author
Salay, Nancy

Publication Date
2009-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Why Dreyfus’ Frame Problem Argument Cannot Justify AntiRepresentational AI
Nancy Salay (salay@queensu.ca)
Department of Philosophy, Watson Hall 309
Queen‘s University, Kingston, ON
K7L 3N6

Abstract

disembodied cognitive models will not work, and this
conclusion needs to be heard. By disentangling the ideas of
embodiment and representation, at least with respect to
Dreyfus‘ frame problem argument, the real locus of the
general polemic between traditional computationalrepresentational cognitive science and the more recent
embodied approaches is revealed. From this, I hope that
productive debate will ensue.
The paper proceeds in the following way: in section I, I
describe and distinguish the logical version of the frame
problem and the philosophical one that remains unsolved; in
section II, I rehearse Dreyfus‘ negative argument, what I‘ll
be calling his frame problem argument; in section III, I
highlight some key points from Dreyfus‘ positive account of
a Heideggerian alternative; in section IV, I make my case
against the anti-representational conclusions Dreyfus wants
to draw from his frame problem argument; and, in section
V, I speculate on the nature of an embodiment-directed AI
not hampered by the frame problem.

Hubert Dreyfus has argued recently that the frame problem,
discussion of which has fallen out of favour in the AI
community, is still a deal breaker for the majority of AI
projects, despite the fact that the logical version of it has been
solved. (Shanahan 1997, Thielscher 1998). Dreyfus thinks
that the frame problem will disappear only once we abandon
the Cartesian foundations from which it stems and adopt,
instead, a thoroughly Heideggerian model of cognition, in
particular one that does not appeal to representations. I argue
that Dreyfus is too hasty in his condemnation of all
representational views; the argument he provides licenses
only a rejection of disembodied models of cognition. In
casting his net too broadly, Dreyfus circumscribes the
cognitive playing field so closely that one is left wondering
how his Heideggerian alternative could ever provide a
foundation explanatorily robust enough for a theory of
cognition. As a consequence, he dilutes the force of his
legitimate conclusion, that disembodied cognitive models will
not work, and this conclusion needs to be heard. By
disentangling the ideas of embodiment and representation, at
least with respect to Dreyfus‘ frame problem argument, the
real locus of the general polemic between traditional
computational/representational cognitive science and the
more recent embodied approaches is revealed. From this, I
hope that productive debate will ensue.

I: The Frame Problem

Keywords:
representation;
cognition,
embodiment;
embodied; enactive; Dreyfus; Heidegger; frame problem

Introduction
Hubert Dreyfus has argued recently that the frame problem,
discussion of which has fallen out of favour in the AI
community, is still a deal breaker for the majority of AI
projects, despite the fact that the logical version of it has
been solved. (Shanahan, 1997; Thielscher, 1998). Dreyfus
thinks that the frame problem will disappear only once we
abandon the Cartesian foundations from which it stems and
adopt, instead, a thoroughly Heideggerian model of
cognition, in particular one that does not appeal to
representations. But, as I shall argue, Dreyfus is too hasty
in his condemnation of all representational views; the
argument he provides licenses only a rejection of
disembodied models of cognition. In casting his net too
broadly, Dreyfus circumscribes the cognitive playing field
so closely that one is left wondering how his Heideggerian
alternative could ever provide a foundation explanatorily
robust enough for a theory of cognition. As a consequence,
he dilutes the force of his legitimate conclusion, that

The first version of the frame problem (McCarthy & Hayes,
1969), the logical frame problem, refers to a technical
difficulty that emerged out of the logicist framework within
which early models of AI were conceived. One necessary
feature of any AI system is the ability to handle change. For
a logicist AI system, ‗to handle change‘ means to make
correct inferences about which features in its world model to
update and which to ignore when something in the
environment changes. For example, if a ball is moved from
one box to another, the system will need to update the ball‘s
location and the empty/full status of the respective boxes,
but it ought to leave its representations of the other items in
the room alone, e.g. the fridge will remain white, the stove
will remain off, and so on. A logicist system, however, will
not infer anything unless it has a rule telling it to do so; thus,
the system will need explicit rules, frame axioms, which
dictate both what will change and what will stay the same
for a given action. But not only is creating a comprehensive
set of frame axioms impossible in a world-scale system –
more on this in a moment – even were it doable, the
resulting knowledge base would be prohibitively large, from
an inferential perspective; for M possible actions and N
attributes, M x N frame axioms would be needed. The

1198

intuitive solution, that the system should just assume that
properties remain unchanged unless explicitly stated
otherwise – this is known as the commonsense law of inertia
– turned out to be extremely difficult to formalise for
technical reasons that we will not go into here. Recently,
Murray Shanahan‘s circumscriptive event calculus has been
accepted as providing a workable solution. (Shanahan,
1997)
Now while this solves the logical frame problem, the
challenge of logically representing the commonsense law of
inertia, it doesn't solve the related problem I alluded to
above: How does a system engineer produce the correct set
of axioms, the list of properties that do change given an
action, in a way that can scale to real life? In other words,
how will the engineer anticipate which objects and attributes
are relevant to which actions before the situations in which
those actions take place have occurred? As Zenon Pylyshyn
(1987) puts it,
Another way to put this problem is to say that Artificial
Intelligence must face the problem of determining the
relevance of facts it knows to some problem at hand.
This, the problem of relevance, is what many believe lies
at the heart of the frame problem, and which will continue
to be a serious problem long after all the minor technical
problems (e.g., concerning the need for "frame axioms")
have been dealt with. (p. x)
The axiom approach for solving this deeper relevance
problem won‘t work for two general reasons; first, because
anything at all might be affected by an action, it is not
possible to write a closed and comprehensive set of such
axioms – call this the holism problem (Fodor, 2000); and,
second, even were there a way around the holism issue, no
amount of frame axioms could ever help such a system
decide which of the axioms is relevant to the current
situation – call this the infinite regress problem. Dreyfus
(2008) focuses on this second aspect of the problem:
But a system of frames isn‘t in a situation, so in order to
select the possibly relevant facts in the current situation
one would need frames for recognizing situations like
birthday parties, and for telling them from other situations
such as ordering in a restaurant. But how, I wondered,
could the computer select from the supposed millions of
frames in its memory the relevant frame for selecting the
birthday party frame as the relevant frame, so as to see the
current relevance of an exchange of gifts? (p.1138)
The holism problem and the infinite regress problem
taken together constitute the philosophical version of the
frame problem that has yet to be solved. The very fact that
this problem exists for most contemporary AI approaches is
a sign, Dreyfus thinks, that something very fundamental has
been overlooked.

II: The Problem with Cartesian-Based AI
Dreyfus argues that the frame problem will arise for any
approach that is founded on a Cartesian ontology,

specifically one in which cognitive agents impose meaning
on the brute meaningless facts of the world. Following
Heidegger, Dreyfus argues that meaning, which is always
situated, arises out of holistic, dynamic, inter-relations
between agents and their environment. Meaning arises, as a
whole, out of activity, and never individually, as a result of
assignments:
To say a hammer has the function of being for hammering
leaves out the defining relation of hammers to nails and
other equipment, to the point of building things, and to
our skills – all of which Heidegger called readiness-tohand – and so attributing functions to brute facts couldn‘t
capture the meaningful organization of the everyday
world. (Dreyfus, 2008, p.1138)
For Heidegger, we (mostly) don‘t experience objects as
things to which we need to assign a meaning, rather we
experience things in situations as ‗ready-to-hand‘, as
somethings, embedded within meaningful situations. Thus
the question "what feature of this object is relevant here?"
cannot even be formulated on a Heideggerian view, since
that would require seeing the object as something, rather
than just seeing it.
On the Cartesian view, in contrast, relevance problems
arise because there is always an extra step, that of deciding
how to apply meanings in actual situations. Since meanings
vary, sometimes dramatically, from context to context, not
every context-free meaning attribution will apply in a
particular situation. There are countless examples of this;
indeed, the failure of computational linguistics to handle
lexical knowledge in any general way is a testament to the
intractability of this relevance problem, but here is one for
discussion. The colour of traffic lights, one might think, has
a highly circumscribed meaning in the contexts within
which they are found, namely traffic systems; red means
stop and green means go to all individuals using the system.
There are times, however, when the authority of the traffic
light is overridden, for example when there has been an
accident in the middle of an intersection, and drivers must
respond to the traffic officer‘s signals instead of and
possibly in contradiction to the traffic light signals. In such
situations once must decide that the colour of the traffic
light is no longer relevant, or that its relevance has been
over-ridden by something that has greater relevance. On the
Heideggerian view, in contrast, there are no analogous
relevance decision problems because on that view one
doesn‘t first assign meanings to the constituents of a
situation and then reason about what to do next; rather, one
simply responds1 to the situation at hand.
1

Of course how we characterise this response will make an
enormous difference in what sort of view we end up with. If we
think that the response is made up of a bunch of lower level
‗decisions‘ of some sort, we might have the relevance problem all
over again – see footnote 2. I say ―might‖ because there are
different ways in which this story might go. Dreyfus thinks that
Freeman is on to one such story.

1199

Thus, on the Cartesian view of meaning, even once the
meaning of an object is assigned or established in some
way, one must still always ask, what is the meaning here, in
this context, for this individual, with this purpose? And this
is just to ask ―what is it about this object or this action that
is relevant here?‖2 An appeal to the overall body of
meanings, whether stored in a conceptual hierarchy or in a
system of inter-connected concept relations, cannot help for
the same reason that appealing to meta-frames does not help
on the logicist approach; an infinite regress of metameanings will be needed in order to determine what is
currently relevant.
Dreyfus concludes that the Cartesian view, and all
approaches that are essentially founded upon it, cognitivist
AI, for example, must be abandoned. More positively, he
suggests that AI projects in general still have a future: if
they are founded on a Heideggerian ontology, the frame
problem will not arise.

III: Heideggerian AI According to Dreyfus
To evade the frame problem, then, instead of viewing
cognitive agents as founts of intentionality, we need to see
intentionality as arising out of tightly connected dynamic
couplings, that is, embodied relations between agents and
their environments. On a Heideggerian view, embodiment
takes the form of being-in-the-world, what Dreyfus
describes as ‗background coping‘.
This relation he
emphasises ―...is not representational at all and does not
involve any problem solving.‖ (Dreyfus, 2008, p. 1150)
Thus, even though we do engage in inferencing over
representations in high-level linguistic thinking, for example
when we are solving a mathematical equation, or sometimes
when situations break down, for example when the bulb in a
flashlight burns out and we suddenly become aware of the
flashlight as a flashlight, we are misguided if we think that
it's that kind of process all the way down; Heidegger and
Dreyfus think that the high-level, off-line, thinking that we
take to be paradigmatic of cognition, e.g. planning a
vacation, is derivative of background coping. If we want to
find out how we think, we first need to investigate how we
manage to do anything at all.
... the Heideggerian claim is that action-oriented coping,
as long as it is involved (online, Wheeler would say) is
not representational at all and does not involve any
problem solving, and that all representational problem
solving takes place offline and presupposes involved
background coping. Showing in detail how the
representational un-ready-to-hand in all its forms depends
upon a background of holistic, nonrepresentational coping
is exactly the Heideggerian project....
Indeed, a
Heideggerian Cognitive Science would require working
2

Note that these ‗decisions‘ are not necessarily made consciously.
There needs to be some mechanism, on the Cartesian account,
agential and/or sub-agential, according to which the choice is made
to treat this or that feature as relevant in the current situation.

out an ontology, phenomenology, and brain model, that
denies a basic role to any sort of representation — even
action oriented ones — and defends a dynamical model
like Merleau-Ponty‘s and van Gelder‘s that gives a
primordial place to equilibrium and in general to rich
coupling. (Dreyfus, 2008, p. 1150)
Dreyfus eschews all disembodied approaches, then,
because they all fall prey to the frame problem. But he goes
on to reject all representational accounts because, on the
Heideggerian view, embodiment is characterised by
background coping, and this is a non-representational
relation.
A non-representational view that Dreyfus singles out
favourably is Walter Freeman's neuro-dynamical account of
cognition.(Freeman, 2000) The central point he makes
relevant to our discussion is that the cognitivist
interpretation of brain neurology, as representational and
computational, is inconsistent with the evidence; a nonrepresentational dynamic systems interpretation does a
better job of explaining the raw data.
Freeman takes two complementary observations to be
instructive with respect to this debate. On the one hand, the
neuronal patterns that cognitivists want to identify as the
vehicles of representation, amplitude modulation (AM)
patterns, change over time sometimes subtly and sometimes
dramatically: "... even with fixed experimental conditions
and invariant stimuli, the constancy of the pattern for each
class of odorant holds for only a few days." (Freeman, 2000,
pp. 76-77) On the other hand, evidence shows that these
same AM patterns are invariant with respect to widely
divergent stimuli: "in the cognitivist view, each AM pattern
represents an odorant. ... [but] they cannot be
representations of odorants, because it is impossible to
match them either with stimuli or with pulse patterns from
receptor activation that convey stimuli to the cortex."
(Freeman, 2000, p. 89) AM patterns, Freeman thinks, are
more a product of the unique neuronal properties of an
individual brain, which change over time and as a
consequence of experience, than of external stimuli. Thus,
instead of thinking about AM patterns as representations of
objects in an animal's world, say of carrot or celery for a
rabbit, Freeman urges that we should use the language of
dynamic systems theory to think of the patterns as attractor
basins that stand for ‗significances‘ and the clusters of
neurons that promote movement to such basins as attractors
for those basins. For Dreyfus, this difference in pattern
interpretation is central because it provides empirical
support for Heidegger‘s insistence that background coping
does not involve any representations at all.
Thus Freeman contends that each new attractor does not
represent, say, a carrot, or the smell of carrot, or even
what to do with a carrot. Rather, the brain‘s current state
is the result of the sum of the animal‘s past experiences
with carrots. What in the physical input is directly picked
up and resonated to when the rabbit sniffs, then, is the

1200

affords-eating,73 and the brain state is directly coupled
with (or in Gibson‘s terms resonates to) the affordance
offered by the current carrot. (Dreyfus, 2008, p. 1153)
By using dynamic systems theory, rather than a
computational or information theoretic approach to
interpreting these patterns, we not only do a better job of
explaining the data, Dreyfus argues, we also gain a tool for
explaining how experiences draw us towards other
experiences, something that static cognitivist theories cannot
do. Thus, on Freeman‘s account, we get a neurological
explanation of the phenomenological resolution to the frame
problem; a consequence of the tightly inter-connected
relation between experience, perception, and meaning is that
agents don‘t ‗pick out‘ the relevant features of a situation,
rather, those features ‗solicit‘ the agent: ―In coping in a
particular context, say a classroom, we learn to ignore most
of what is in the room, but, if it gets too warm, the windows
solicit us to open them.‖ (Dreyfus, 2008, p. 1158)
If we accept Freeman‘s interpretations of the neurological
data, it looks like Dreyfus might have some empirical
support for moving beyond embodiment to a general antirepresentationalism. But, as I shall argue in the next
section, although they are both right in pointing out the
limitations of the cognitivist view, Dreyfus is hasty in
concluding that all representational models of cognition will
suffer the same fate.

IV: Dreyfus Concludes Too Much
As I‘ve noted, Dreyfus‘ observation that the frame problem
stems not from any particular model of cognition per se, but
from a prior ontological commitment to a disembodied view
of meaning provides us with a deep and important insight.
But instead of concluding that we ought to be focusing on
embodied AI approaches, he wants more; Dreyfus thinks
that the new basis for cognitive science ought to be fully
Heideggerian, not just embodied. And this, he argues,
entails that our cognitive science will be fundamentally nonrepresentational.
But the fact that a Heideggerian embodiment relation –
background coping – is fundamentally non-representational
is not in itself an argument that we must see it that way as
well. The frame problem argument licenses only the move
to views on which meanings arise for systems out of their
on-going inter-relations with their environment; it is silent
about the specific nature of those on-going inter-relations.
We need an independent argument for the claim that the
non-representational feature of background coping is
essential to it being an embodied relation. In other words,
Dreyfus needs to show us that being a representational
relation and an embodied relation are at base incompatible.
He does not provide this support anywhere.
To be charitable, we might read Dreyfus‘ use of the term
‗representation‘ as ‗cognitivist representation‘. Since the
frame problem argument does show us that the cognitivist
representational relation is incompatible with the embodied

one, he‘d be justified in his insistence that embodiment is
essentially non-representational. But if we don‘t agree that
all accounts of representation are, or must be, cognitivist,
the frame problem argument will not justify a blanket
dismissal of them. Nor would a Heideggerian appeal, even
were we to move with Dreyfus beyond embodied AI to
Heideggerian AI, since the term ‗representation‘ as it is used
today is far more nuanced and complex than it was in
Heidegger‘s day, when it did mean roughly what the
cognitivist conception does. Fred Keijzer (1996), for
example, suggests that the internal states of a cognitive
system responsible for guiding long-term behaviour should
be seen as representations, though since they do not have the
conceptual and computational baggage of cognitivist
representations, we might want to use some new
terminology to refer to them: ―However, at the same time,
these internal states are so different from the cognitive
science‘s traditional notion of representation that it is
perfectly reasonable to hold that they are not
representations.‖ (p.283) Evan Thompson (2007) suggests
that representations emerge out of ‗brain-body-world‘
interactions:
Representational 'vehicles' (the structures or processes
that embody meaning) are temporally extended patterns
of activity that can crisscross the brain-body-world
boundaries, and the meanings or contents they embody
are brought forth or enacted in the context of the system's
structural coupling with its environment. (p.59)
And Rolf Pfeifer (1995) thinks of embodied representations
as the ―weight configurations of a neural network‖ (p.59) in
conjunction with the physical make-up of the system and
how it interacts with its environment. And there are many
more variations besides. The point is that not all appeals to
representation are commitments to internal content kernels
that, in virtue of their content, play a role in an inferential
network. To suppose that they are is to have just missed the
shift in thought that has been occurring over the past decade
or so. Of course, Dreyfus must be aware of the fact that not
all accounts of representation are cognitivist ones. Michael
Wheeler, in developing a quasi-scientific fleshing out of
Heideggerian ‗thrownness‘, takes great pains to distinguish
his ‗action-oriented representations‘ from the traditional
variety. Instead of evaluating whether Wheeler‘s
representationalism slips in a commitment to a disembodied
view, however, Dreyfus (2008) rejects it out of hand:
Merely by supposing that Heidegger is concerned with
problem solving and action oriented representations,
Wheeler‘s project reflects not a step beyond Agre but a
regression to aspects of pre-Brooks GOFAI. Heidegger,
indeed, claims that skilful coping is basic, but he is also
clear that all coping takes place on the background coping
he calls being-in-the-world that doesn‘t involve any form
of representation at all. (p. 1145)
But Dreyfus isn‘t just claiming that, in everyday coping,
we don‘t use representations; Dreyfus ultimately wants to

1201

conclude that cognition itself ought to be understood as a
non-representational activity.
The post-Cartesian agent manages to cope with the world
without necessarily representing it. A dynamical approach
suggests how this might be possible by showing how the
internal operation of a system interacting with an external
world can be so subtle and complex as to defy description
in representational terms—how, in other words, cognition
can transcend representation. (Dreyfus, 2008, p. 1147)
Given what he‘s argued successfully thus far, however, this
is just implausible. It doesn‘t follow from the fact that one
relation or process is derivative of another that they will
share all of their properties. Dancing is derivative of
movement, but dancing can have all sorts of structural
properties, for example, being organised according to a
piece of music, without movement also having those
properties. Even if we accept that cognition as a process
develops out of a more fundamental embodied relation with
the environment in which representations do not figure, we
cannot assume, as he does, that representations will play no
role in our account of the derived cognitive process.
Finally, the empirical support that Freeman‘s neurodynamical position seems to offer Dreyfus‘ antirepresentationalism is illusory. Freeman‘s position is in
opposition to cognitivist interpretations of neurological data
– Freeman is clear about this. Thus, his account, if
accepted, could serve as empirical reinforcement for
Dreyfus‘ dismissal of cognitivist approaches, in short for his
frame problem argument, but not as support for a rejection
of non-cognitivist models on anti-representational grounds.
Dreyfus is hasty, then, in moving beyond the conclusion
of his argument to a rejection of all representational
accounts.
If he pleaded a cognitivist reading of
‗representation‘, we‘d have a rationalisation for his overreaching, but the fact that the term has been evolving rather
publicly undermines that plea and places his wholesale
rejection on very shaky grounds.

‗having a physical instantiation‘ is insufficient because it‘s
entirely possible for a system to be physically grounded
through its body but at the same time gain knowledge about
its environment in another way, perhaps through an
externally created knowledge base, rather than through
experience, over time, and via its own body parts.
As mentioned earlier, on an embodied view the frame
problem really can‘t be formulated since, as a dynamic
approach, situations aren‘t seen as discrete units of
experience in the context of which decisions (about
relevance or about anything at all) have to be made. Rather,
situations are the backdrop for the continually evolving
dynamic coupling of systems. Thus, in an important sense,
embodied systems are never in situations; they are
constantly moving and reacting in response to the pushes
and pulls in their environment. Perhaps the litmus test for
minimal embodiment, then, is whether it is possible to
formulate the frame problem on a given view; if it is, then
the view is not minimally embodied in the sense required to
defeat the frame problem argument. But how do we know if
our theory of embodiment has gone too far, as I‘ve argued
Dreyfus‘ Heideggerian version does? And what precisely
does ‗going too far‘ mean? An objective measure of this is
unlikely. Nevertheless, a balance is required between the
intuitions motivating traditional disembodiment and the new
insights that have arisen in response to its shortcomings.
While we are shifting paradigms and striving for new ways
of thinking about the phenomena, we need to do so within
the context of a theory that is explanatorily perspicuous with
respect to cognition, since this is the process we ultimately
want to understand. The minimal sense of embodiment that
I think is best suited for this task is the concept of
autopoietic embodiment that Humberto Maturana and
Francisco Varela (1980) and more recently Evan Thompson
(2007) have developed. But I‘ll have to leave to another
paper the task of determining whether that view is stronger
than it needs to be in order to underwrite a theory of
cognition.

V. Conclusions and Speculations
Even if we cannot accept all of Dreyfus‘ conclusions, we
can still act on the viable one and replace our disembodied
cognitive models with those that are embodied. What we
still need is a working definition of embodiment, one that is
general enough to apply to non-human and possibly even
non-organic systems, but specific enough to provide
direction and cohesion to embodied approaches. As we‘ve
seen, we can‘t accept Dreyfus‘ Heideggerian suggestion,
since it is entangled with the idea of antirepresentationalism, and the frame problem argument does
not warrant such a strong reading of embodiment.
There are many suggestions for how we might develop an
embodied cognitive science – see Tom Ziemke‘s (2003)
lucid and comprehensive overview for an up-to-date list –
but not all of these will meet the minimum criterion set by
the frame problem. For example, the idea of embodiment as

1202

References
Barsalou, L. (1999). Perceptual symbol systems.
Behavioral and Brain Sciences, 22, 577–660.
Brooks, R. A. (1991). Intelligence without reason.
Proceedings of the International Joint Conference on
Artificial Intelligence. (pp. 569–595). San Mateo, CA:
Morgan-Kaufman.
Clark, A. (1997). Being there: putting brain, body, and
world together again. Cambridge: MIT Press.
Clark, A. & Toribio, J. (1994). Doing without
representing? Synthese, 101, 401–431.
Dreyfus, H. (2002). Intelligence without representation –
Merleau-Ponty‘s critique of mental representation.
Phenomenology and the Cognitive Sciences, 1, 367–
383.

Dreyfus, H. (2007). Response to McDowell. Inquiry, 50
(4), 371-377.
Dreyfus, H. (2008). Why Heideggerian AI failed and how
fixing it would require making it more Heideggerian.
Artificial Intelligence, 171(18), 1137–1160.
Fodor, J.A. (1987). Modules, frames, fridgeons, sleeping
dogs, and the music of the spheres. In Z. Pylyshyn
(Ed.), The robot's dilemma: the frame problem in
artificial intelligence. Norwood, NJ: Ablex.
Fodor, J.A. (2000). The mind doesn't work that way.
Cambridge: MIT Press.
Freeman, W. (2000). How brains make up their minds.
New York: Columbia University Press.
Keijzer, F. (1998). Doing without representations which
specify what to do. Philosophical Psychology, II(3),
269-302.
Keijzer, F. (2002). Representation in dynamical and
embodied cognition. Cognitive Systems Research, 3,
275–288.
Kelso, J. A. S. (1995). Dynamic patterns: the selforganization of brain and behaviour. Cambridge: MIT
Press.
Kirsh, D. (1990). When is information explicitly
represented?
In P. Hanson (Ed.), Information,
language, and cognition. Vancouver: University of
British Columbia Press.
Maturana, H. & Varela, F. (1980). Autopoiesis and
cognition. Dordrecht, The Netherlands: Reidel.
McCarthy, J. & Hayes, P.J. (1969). Some philosophical
problems from the standpoint of artificial intelligence.
In D.Michie and B.Meltzer (Eds.), Machine
intelligence, 4. Edinburgh: Edinburgh University Press.
McDowell, J. (2007). Response to Dreyfus. Inquiry, 50
(4), 366–370.
Pfeifer, R. (1995). Cognition – perspectives from
autonomous agents. Robotics and Autonomous Systems,
15, 47-70.
Preston, B. (1993). Heidegger and artificial intelligence.
Philosophy and Phenomenological Research, 53 (1),
43-69.
Pylyshyn, Z.W. (Ed.) (1987). The robot's dilemma: the
frame problem in artificial intelligence. Norwood, NJ:
Ablex.
Shanahan, M. (1997). Solving the frame problem: a
mathematical investigation of the common sense law of
inertia. Cambridge: MIT Press.
Shanahan, M. (2008). The frame problem. The Stanford
Encyclopedia of Philosophy (Fall 2008 Edition),
Edward N. Zalta (Ed.),URL =
<http://plato.stanford.edu/archives/fall2008/entries/fra
me-problem/>.
Thelen, E., and Smith, L. (1994). A dynamic systems
approach to the development of cognition and action.
Cambridge: MIT Press.
Thielscher. M. (1998). Introduction to the fluent calculus.
Electronic Transactions on Artificial Intelligence, 2(3–
4), 179–192.

1203

Thompson, E. (2007). Mind in life: Biology,
phenomenology, and the sciences of the mind.
Cambridge: Harvard University Press.
Van Gelder, T. (1995). What might cognition be, if not
computation? Journal of Philosophy, XCII(7), 345381.
Varela, F., Thompson, E., and Rosch, E. (1991). The
embodied mind. Cambridge: MIT Press.
Wheeler,
M.
(2008).
Cognition
in
context:
phenomenology, situated robotics and the frame
problem. International Journal of Philosophical
Studies, 16 (3), 323-349.
Ziemke, T. (2003). What‘s that thing called embodiment?
In Alterman & Kirsh (Eds.), Proceedings of the 25th
Annual Conference of the Cognitive Science Society.
Mahwah, NJ: Lawrence Erlbaum.

