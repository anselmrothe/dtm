UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Memory in Language: Language in Memory

Permalink
https://escholarship.org/uc/item/6ph8n3js

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)

Authors
Culicover, Peter
Dennis, Simon
Howard, Marc
et al.

Publication Date
2009-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Memory in Language: Language in Memory
Peter Culicover (culicove@ling.ohio-state.edu)
Department of Linguistics, The Ohio State University,
1712 Neil Ave, Columbus, OH 43210 USA

Simon Dennis (simon.dennis@gmail.com)
Department of Psychology, The Ohio State University,
1835 Neil Ave, Columbus, OH 43210 USA

Marc Howard (marc@chb.syr.edu)
Department of Psychology, Syracuse University
Huntington Hall, Syracuse, NY 13244, USA

Richard Lewis (rickl@umich.edu)
Department of Psychology, University of Michigan,
525 East University, Ann Arbor, MI 48109-1109, USA

Brian McElree (brian.mcelree@nyu.edu)
Department of Psychology, New York University,
6 Washington Place, New York, NY 10003, USA

Keywords: memory, language, sentence processing, dependency, memory load,

Although constraints of memory have long been assumed
to affect sentence processing (Miller & Chomsky, 1963) and
contingencies in language have long been known to affect
human memory (Miller & Selfridge, 1950), the connection
between models of sentence comprehension and memory remains less than clear and there is often too little dialogue
between memory and language researchers. In this symposium, we aim to bring together researchers studying sentence
processing that are interested in the constraints imposed by
memory (Culicover, Lewis, McElree) and researchers studying memory that work on incorporating language sensitive
representations into their models (Dennis, Howard).
The prima facie evidence for the potential value of a closer
relationship is substantial. For instance, Fedorenko, Gibson,
and Rohde (2006) found that when a working memory load
is imposed during sentence comprehension, object extracted
relative clauses, which are typically more difficult to process,
are affected more than subject extracted relative clauses. This
effect is magnified if the nouns present in the working memory set are similar to those in the sentence. In addition, when
a licensor for a negative polarity item appears in an inaccessible position (such as ”no” in ”A man who had no beard was
ever thrifty”) it nonetheless impacts both the reading time and
the probability of judging the string grammatical (Vasishth,
Brussow, Lewis, & Drenhaus, 2008) suggesting that sentence
processing is subject to the kinds of similarity phenomena
that are observed ubiquitously in working memory studies.
Models of short term memory have progressed substantially since the observations of Miller and Chomsky (1963)
observations. Buffer models have been replaced by direct access models and the cue dependent nature of memory is now

more broadly appreciated. The importance of this change in
perspective was underlined by McElree, Foraker, and Dyer
(2003). Using a response signal procedure, they found that
while increasing the length of a dependency impacts accuracy
it does not affect the rate of formation accrual the signature
pattern of a direct access memory process.
The broad implication is that the detailed nature of sentence processing emerges from the interaction of general principles of human memory with the specialized task of language comprehension. Conversely, many memory experiments involve sequences of word stimuli and the role of language is well known. For instance, Howard, Addis, Jing, and
Kahana (2007) showed that semantic relatedness affects the
conditional response probability of producing items in free
recall. Furthermore, models of free recall performance such
as the temporal context model (Howard & Kahana, 2002) can
be used to construct lexical representations directly from language corpora.
The purpose of the symposium is to bring together leading
researchers from both domains to discuss how memory and
language models can be brought into closer alignment.
Peter Culicover: Linguists working on the syntax of natural languages conventionally categorize instances of unacceptability as ’ungrammaticality’ (except for clear cases of
semantic anomaly). However, there has been a recent trend
(even among some linguists, e.g. Kluender, Sag, myself) in
favor of taking a somewhat narrower view of what constitutes
’ungrammaticality’, and to attribute some types of unacceptabilty to the processing complexity associated with particular
syntactic constructions. Typically, such complexity is characterized in terms of ’memory limitations’ (as in the work of
scholars such as Gibson and Lewis), the most familiar cases
involving nested dependency in relative clauses. I consider a

1002

range of syntactic constructions that are candidates for such
a processing account, and explore the question of how processing would have to proceed in order for a general and reasonably principled complexity account to be plausible. The
main goal is to highlight the syntactic phenomena that might
profit from continuing investigations into how discourse representations are constructed in the course of processing, how
such representations are represented in memory, and what
takes place in accessing, retrieving and otherwise manipulating their components.
Simon Dennis: In models of human episodic memory, it
is common to consider retrieval to be the consequence of
both item and contextual cues. However, the features of these
representations are often assumed to be drawn independently
from some distribution, without regard to semantic or syntactic contingencies that may have led to their formation. In this
talk, I will outline a version of the syntagmatic paradigmatic
model (Dennis, 2004, 2005) that builds contextual representations from exposure to a corpus. The contextual representations formed capture syntactic constituent and semantic role
information that may help bridge the gap between models of
memory and sentence comprehension.
Marc Howard: In recent years, mathematical models
of episodic memory have increasingly utilized a graduallychanging state of temporal context as the cue for recall from
episodic memory. I will describe a recent extension to the
temporal context model that attempts to model the development of semantic representations from a gradually-changing
state of temporal context. The model, referred to as pTCM,
uses the current state of context—a weighted sum of the semantic representations of recently-presented items—as a cue
to generate a prediction about what word will be presented
next. The semantic representation of a word is formed by
averaging the predictions that precede its presentation. The
model performs comparably to LSA when trained on a corpus of naturally-occurring text. The semantic representation generated by pTCM performs best when temporal context changes gradually over time and extends across sentence
boundaries.
Richard Lewis: I present a theoretical framework that
construes linguistic processing and its behavioural manifestations as rational processes bounded by constraints on cognitive architecture such as short term memory, noise, and perceptual motor bottlenecks. The framework is supported by
new computational modelling techniques for deriving adaptive control strategies given a specific set of cognitive constraints and some probabilistic environment. The presentation will provide summarize evidence on the nature of memory constraints in sentence comprehension, and then show
how incorporating these constraints in boundedly rational
control models provides a novel and detailed integration of
memory based and experience based approaches to language
comprehension.
Brian McElree: Understanding language routinely requires comprehenders to establish dependencies between ele-

ments that span several words, phrases, or even clauses. I will
first present an overview of studies indicating that the same
mechanism identified in basic research on recognition memory underlies sentence comprehension. Specifically, that a)
the interpretations of expressions with various types of nonadjacent dependencies are mediated by content-addressable
memory representations that are retrieved with a cue-driven,
direct-access operation, and b) although this type of retrieval
operation enables the rapid recovery of past analyses, it is
highly susceptible to interference from other constituents in
memory that match the cues used for retrieval. A second line
of research explores the hypothesis that memory retrieval is
required whenever a constituent needed to resolve a dependency is outside of focal attention. Although studies using a
range of cognitive tasks suggest that the span of focal attention is extremely limited, research has not directly examined
the effective span of focal attention in comprehension, and
whether it interacts with linguistic structure and linguistic devices for focusing information. I will report initial studies
that investigate the span of focal attention with time course
measures of retrieval speed during on-line comprehension.

References
Dennis, S. (2004). An unsupervised method for the extraction
of propositional information from text. Proceedings of the
National Academy of Sciences, 101, 5206-5213.
Dennis, S. (2005). A memory-based theory of verbal cognition. Cognitive Science, 29(2), 145-193.
Fedorenko, E., Gibson, E., & Rohde, D. (2006). The nature of working memory capacity in sentence comprehension: Evidence against domain specific resources. Journal
of Memory and Language, 54(4), 541-553.
Howard, M. W., Addis, K., Jing, B., & Kahana, M. K. (2007).
Semantic structure and episodic memory. In T. K. Landauer, D. S. McNamara, S. Dennis, & W. Kintsch (Eds.),
Handbook of Latent Semantic Anlaysis (p. 121-142). Mahwah, New Jersey: Erlbaum.
Howard, M. W., & Kahana, M. J. (2002). A distributed representation of temporal context. Journal of Mathematical
Psychology, 46, 268-299.
McElree, B., Foraker, S., & Dyer, L. (2003). Memory structures that subserve sentence comprehension. Journal of
Memory and Language, 48, 67-91.
Miller, G. A., & Chomsky, N. (1963). Finitary models of language users. In D. R. Luce, R. R. Bush, & E. Galanter
(Eds.), Handbook of Mathematical Psychology (vol. ii).
New York: John Wiley.
Miller, G. A., & Selfridge, J. A. (1950). Verbal context and
the recall of meaningful material. American Journal of Psychology, 63, 176-185.
Vasishth, S., Brussow, S., Lewis, R., & Drenhaus, H. (2008).
Processing polarity: How the ungrammatical intrudes on
the grammatical. Cognitive Science, 32, 685-712.

1003

