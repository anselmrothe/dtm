UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Prediction of Human Eye Fixations using Symmetry

Permalink
https://escholarship.org/uc/item/9tq6j18t

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)

Authors
Koostra, Gert
Schomaker, Lambert R.B.

Publication Date
2009-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Prediction of Human Eye Fixations using Symmetry
Gert Kootstra (G.Kootstra@ai.rug.nl)
Lambert R.B. Schomaker (L.Schomaker@ai.rug.nl)
Artificial Intelligence, University of Groningen
Nijenborgh 9, 9747 AG Groningen, the Netherlands
Abstract
Humans are very sensitive to symmetry in visual patterns.
Reaction time experiments show that symmetry is detected
and recognized very rapidly. This suggests that symmetry is a
highly salient feature. Existing computational models of saliency, however, have mainly focused on contrast as a measure
of saliency. In this paper, we discuss local symmetry as a
measure of saliency. We propose a number of symmetry
models and perform an eye-tracking study with human participants viewing photographic images to test the models. The
performance of our symmetry models is compared with the
contrast-saliency model of Itti, Koch and Niebur (1998). The
results show that the symmetry models better match the human data than the contrast model, which indicates that symmetry can be regarded as a salient feature.
Keywords: visual perception; overt visual attention; symmetry; saliency; saliency models

Figure 1: Examples of images containing symmetrical
forms. The second column shows the human fixation density maps, the third shows the contrast-saliency maps, and
the last shows the symmetry-saliency maps. The preference
of humans to fixate on the center of symmetry is correctly
reproduced by the symmetry model. The bright regions are
the parts of the maps above 50% of its maximum value.

Introduction
While viewing the world, humans constantly make eye
movements to fixate on interesting parts of the visual field.
In this way, the relevant information can be viewed with
high resolution, while irrelevant information can be ignored.
The process of focusing attention by making an eye movement is called overt visual attention. The eye movements
are influenced both top-down, emerging for instance from
past experiences, personal interests, and the task, as well as
bottom-up, purely from the stimulus. In the current research,
we are interested in the stimulus-driven influences on eye
movements, without semantic control. What in the stimulus
causes our eyes to move to a certain location? What causes
certain parts of the visual field to catch attention? More specifically, we investigate how well the locations of eye fixations can be predicted on the basis of local symmetry.
Most current models of visual attention base their predictions on contrast in the image. The model of Itti and Koch.
(Itti & Koch, 2001; Itti, Koch, & Niebur, 1998), for instance, is based on contrasts in luminance, color and orientation. Their saliency model is an implementation of the
feature integration theory of human visual search (Treisman
& Gelade, 1980). The saliency model of Itti and Koch has
been compared to human eye fixations by Parkhurst, Law
and Niebur (2002). They tested the model on photographic
images and showed that it matches the human fixation
points significantly better than expected by chance. Ouerhani, von Wartburg, Hügli and Müri (2004) also found a
positive correlation between the model and human fixations.
Also other saliency models, like the model of Le Meur,
Le Callet, Barba and Thoreau (2006) are based on contrast
calculations. They found a positive correlation between their
model and human data that was slightly higher than the per-

formance of Itti and Koch’s model. Privitera and Stark
(2000) investigated a set of simpler saliency operators including other features than contrast. The operators were also
found to predict human fixation points to some extent. It
must be noted that Privitera and Stark also used a simple
symmetry operator that weakly resembled the human data.
However, even though most existing models are based on
contrast, Figure 1 shows that humans have a clear preference to fixate on the center of symmetry for some images
(second column). This can not be explained using contrast.
Instead, the contrast model gives high response at the
boundaries, where the flowers contrast with the background
(third column). This apparent deficiency in current vision
models was the motivation for the present study.
The human response, shown in Figure 1, suggests that
humans pay attention to symmetrical forms. In this paper we
will investigate if eye fixations can be predicted on the basis
of local symmetry. As can be appreciated in the last column
in Figure 1, our symmetry saliency model does predict fixations in accordance with the human data. We will show that
this is not only valid for images that contain explicit symmetrical forms like those in Figure 1, but more generally in
complex photographic images with different types of content.
Symmetry is a prominent visual feature in our daily environments. Many living organisms, for instance, have clear
left-right symmetry in their bodies. This symmetry is even

56

Figure 2: The basis of our symmetry models. (a) gives three examples of pixel pairs whose gradients are compared by the
symmetry operator. The geometry of the contribution of a pixel pair is shown in (b) and further explained in the text. (c)
gives an overview of the multi scale setup of the symmetry models.
(Itti et al., 1998) is also compared with the human data. In
this section, the developed symmetry-saliency models are
explained, followed by a description of the eye-tracking
studies and the method to compare the models with the human data.

an indication of the fitness of the individual. For instance,
manipulated images of faces with enhanced symmetry are
judged more attractive than the original faces (Grammer &
Thornhill, 1994). Also in art, symmetry is usually preferred
over asymmetry (Tyler, 2000). According to Gestalt psychologists symmetry improves the figural goodness
(Palmer, 1991). Because symmetry is so prominent, it is
likely that it plays a role in the visual system.
It is also known that humans are sensitive to symmetry.
Symmetrical patterns are detected very rapidly, especially
when having multiple axes of symmetry (Palmer & Hemenway, 1978). Also recognition performances increase
with symmetrical patterns (Royer, 1981). The increase in
performance might be explained by the intrinsic redundancy
present in symmetrical forms, which gives rise to simpler
representations (Barlow & Reeves, 1979). Humans furthermore have the tendency to interpret symmetrical regions as
figure, and asymmetrical regions as background (Driver,
Baylis, & Rafal, 1992).
Symmetry also influences eye movements. Fixations on
symmetrical forms are concentrated at the center (Richards
& Kaufman, 1969) of the form, or at the crossing points of
the symmetry axes (Kaufman & Richards, 1969). For images with a single symmetry axis, the fixations are concentrated along this axis, whereas the fixations are more spread
out for non-symmetrical images (Locher & Nodine, 1987).
These studies, however, use simple stimuli with only one
symmetrical pattern presented.
In this paper, we investigate the role of local symmetry on
overt visual attention in complex photographic scenes, and
compare it to the role of contrast. We propose extended
symmetry-saliency models, and compare their performances
with the contrast saliency model of Itti and Koch, henceforth referred to as the contrast model. We show that the
symmetry models correspondd significantly better with the
human eye fixations.

Symmetry operators
Our models are based on the isotropic symmetry and radial
symmetry operator of Reisfeld, Wolfson and Yeshurun
(1995), and on the color symmetry operator of Heidemann
(2004). We extended the operators to multi-scale symmetrysaliency models.
The isotropic symmetry operator (Reisfeld et al., 1995)
calculates the amount of symmetry at a given position, x,
based upon gradients of the intensity in surrounding pixels.
This is done by comparing pairs of pixels i and j at positions
xi and xj, where x = ( xi + x j ) / 2 (see Figure 2a). Every pixel

pair contributes to the local symmetry by
c(i, j ) = d (i, j , σ ) ⋅ p(i, j ) ⋅ mi ⋅ m j

(1)

Where mi is the magnitude of the gradient at point i,
d (i, j , σ ) is a Gaussian weighting function on the distance
between the two pixels with standard deviation σ, and the
symmetry measurement p(i, j ) is calculated by

p(i, j ) = (1 − cos(γ i + γ j ) ) ⋅ (1 − cos(γ i − γ j ) )

(2)

Where γ i = θi − α is the angle between the direction of the
gradient angle θi and the angle α of the line between pi
and pi (see Figure 2b). The first term in equation (2) has a
maximum value when γ i + γ j = π , which is true for gradients that are mirror symmetric with respect to p. Using only
this term would result in high values for points on a straight
edge, which are not considered symmetrical. To avoid this
problem, the second term demotes pixel pairs with similar
gradient orientation. In this way, the contributions of all
pixel pairs, Γ( p) , within the radius, r, are summed up to
give the isotropic symmetry value for p:
(3)
Miso ( x, y ) = ∑ ( i , j )∈Γ ( p ) c(i, j )

Methods
To investigate the role of symmetry in visual attention, we
developed a number of symmetry-saliency models and compared them with human eye tracking data. To establish a
point of reference, the contrast-saliency model of Itti et al.

To make the symmetry operator more sensitive to symmetrical patterns with multiple axes of symmetry, Reisfeld

57

Figure 3: Examples of images used, one for each category: flowers, animals, street scenes, buildings and nature.
lar symmetry values. Finally, the feature maps are combined
into a symmetry saliency map by resizing all feature maps
to the same size and summing them.

et al. (1995) developed the radial symmetry operator as an
extension of the isotropic symmetry operator. First, the orientations of the contribution of the pixel pairs are calculated
by ϕ (i, j ) = (θ i + θ j ) / 2 . Next, the symmetry orientation is

4

S = ⊕ N (Ms )
s =0

determined as φ ( p) = ϕ (i, j ) for (i, j ) that give the highest

(7)

Where ⊕ is the summation operator that resizes all parts to
the same size, and Ms is the symmetry feature map at scale
s. This procedure results in three symmetry saliency maps:
S iso for isotropic symmetry, S rad for patterns with multiple
symmetry axes, and S col which uses color information.

contribution c(i, j ) . This value is then used to promote the
contributions of pixels pairs with dissimilar orientations:
(4)
Mrad = ∑ (i , j )∈Γ ( p ) c(i, j ) ⋅ sin 2 (ϕ (i, j ) − φ ( p) )
The two symmetry operators mentioned above work on
intensity values only. Since some color transitions are not
detectable in gray-valued images, Heidemann (2004)
adapted the isotropic symmetry operator to the color symmetry operator. This operator uses three color channels, red,
green and blue. Equation (3) is adapted so that not only the
gradients of pixels in one channel, but also between different channels are compared:
(5)
Mcol ( x, y ) = ∑ ∑ c(i, j , ki , k j )

Eye-tracking experiment
We recorded human fixation data in an eye-tracking experiment using the Eyelink head-mounted eye-tracking system (SR research). Fixation locations were extracted using
the accompanied software. The images were displayed fullscreen with a resolution of 1024 by 768 pixels on an 18’’
CRT monitor of 36 by 27 cm at a distance of 70 cm from
the participants. The visual angle was approximately 29º
horizontally by 22º vertically. Before the experiment, the
eye tracker was calibrated using the Eyelink software. The
calibration was verified prior to each session, and recalibrated when needed.
Since we are interested in the bottom-up components of
visual attention, the participants were asked to freely view
the images. We did not give them a task, since that would
give a strong bias on the eye movements. Our approach is
similar to (Kootstra, Nederveen, & De Boer, 2008; Le Meur
et al., 2006; Parkhurst & Niebur, 2003).
31 students (15 female) of the University of Groningen
took part for course credits. The age of participants ranged
from 17 to 32. All had normal or corrected-to-normal vision. In the experiment, 99 images in five different categories were presented, 19 images of natural symmetrical objects, 12 images of animals in a natural setting, 12 images of
street scenes, 16 images of buildings, and 40 images of
natural environments (see Figure 3). Only the images in the
first category were selected for their symmetrical content.
The other categories represent a wide variety of images,
containing natural and cultural scenes, with organic and
rectilinear shapes. All these images were taken from the
McGill calibrated colour image database (Olmos & Kingdom, 2004). The experiment was split up into sessions of
approximately 5 minutes. Between the sessions, the experimenter had a short relaxing conversation with the participants, in order to get them motivated and focused for the
next session. Before starting a new session, the calibration
of the eye tracker was verified. After each presented images,
drift was measured and corrected if needed using the Eye-

( i , j )∈Γ ( ki , k j )∈K

where K contains all combinations of color channels, and
c(i, j , ki , k j ) is the symmetry contribution calculated by
comparing pixel i in color channel ki with pixel j in color
channel kj. Furthermore, equation (2) is altered to:
p(i, j ) = cos 2 (γ i + γ j ) ⋅ cos 2 (γ i ) ⋅ cos(γ j )
(6)

so that the function gives the same result for gradients that
are rotated 180°. The second term keeps the same functionality as the second term in equation (2).

Symmetry-saliency models
The human visual system is thought to process information
on multiple spatial scales. We therefore adapted the above
described operators to multi-scale saliency models, similarly
to (Itti et al., 1998).
The process to calculate the symmetry maps is depicted in
Figure 2c. First, five spatial scales of the input image are
created by progressively applying a Gaussian filter followed
by a downscaling of the image by a factor of two. The different scales are then processed to symmetry feature maps
using the symmetry operators as discussed in the previous
section, where we use r = 24 and σ = 36 . Next, the five
feature maps are normalized using the normalization operator, N, used in (Itti et al., 1998). This normalization consists
first of scaling the feature map values to the range [0..1],
and then multiplying the feature map with (1 − m) 2 , where
m is the average value of all local maxima in the map. This
normalization promotes feature maps that contain a small
number of symmetrical patterns that really stand out, as opposed to feature maps that contain many patterns with simi-

58

Figure 4: (a) The correlations results with the individual fixation-distance maps. The groups give the results for the different
image categories. The error bars are the 95% confidence intervals. The horizontal gray bars with the solid line show the mean
and 95% confidence interval of the inter-participant correlation. The dashed lines show the correlation of the human data
with random fixations (close to zero). (b) The correlation results with the combined fixation-distance maps, representing the
consensus among the participants. The patterns in both plots are similar, but with higher correlations for the later.

link software. The participants could decide when to continue and were allowed to take a short break.

the values in these maps. The correlation coefficient has a
value between –1 and 1. A ρ of 0 means that there is no correlation between the two maps, which is true when correlating with random fixation-distance maps. Values for ρ close
to zero indicate that a model is a poor predictor of human
fixation locations. Positive correlations show that there is
similar structure in the saliency map and the human fixation
map.
This method calculates the correlation for individual participants. However, the photographic images viewed by the
participants are highly complex stimuli that generate many
fixations, with substantial variation among participants. Because of this variation, the correlations of individual fixation-distance maps with the saliency maps will be low.
However, some of the fixations are shared by all participants. To see how well our models predict this consensus
among participants, we also calculate the correlation coefficient for the combined fixation-distance maps,

Analysis methods
To compare the saliency models with the human data, we
use two methods. A correlation method similar to that used
in (Le Meur et al., 2006; Ouerhani et al., 2004), and a fixation-saliency method, similar to that used in (Parkhurst et
al., 2002).
The correlation method correlates the saliency maps with
fixation-distance maps calculated from the human fixation
data. For every single trial of every participant, the fixationdistance map is calculated using the inverse distance transform. The distance transform of the human data gives the
distance to the nearest human fixation for all the pixels in
the image. At the points of fixation, the values are thus zero,
and they increase linearly for pixels farther from the fixations. We then take the inverse of the distance transformation. This is done by subtracting all distance values from the
maximum distance value. The inverse distance map, which
we call the fixation-distance map, therefore contains high
values close to the fixation points, and lower values at
points far from the fixations, and can be seen as a probability distribution for fixations. This is slightly different from
the approach in (Kootstra et al., 2008; Le Meur et al., 2006;
Ouerhani et al., 2004), where a fixation density map is calculated using Gaussian kernels. Our method puts emphasis
on the location of fixations, rather than on their density.
Moreover, our method is parameter free, i.e., there is no
width of the kernel to be set.
The value of the comparison between the saliency map
and the fixation-distance map is then given by the correlation coefficient, ρ:
( ( F ( x, y ) − µ F ) ⋅ ( S ( x, y ) − µ S ) )
∑
x, y
ρ=
(8)
σ F2 ⋅ σ S2

N

Ftot = ∑ i =1 F i , using equation (8).
The second comparison method, the fixation-saliency
method, measures the saliency value, according to the saliency models, at the points of human fixation relative to the
average saliency value at a large number of randomly chosen locations:

λi = s( f i )

∑

1000
j =1

s (rnd)

(9)

Where λi is the fixation-saliency value for the ith fixation,
fi is the ith human fixation location and rnd is a randomly
determined location in the image, excluding the borders.
s(p) is the average saliency value in a patch of the saliency
map, S, centered at point p and with a radius of 28 pixels. If

λi > 1, the saliency at human fixation points is higher than in
the rest of the image, meaning that the given saliency model
has predictive powers.

Results

where F is the fixation-distance map, S is the saliency map

In Figure 4a, the results of the correlation between the individual fixation-distance maps and the saliency methods are

and µ and σ2 are respectively the mean and the variance of

59

Figure 5: The plots give the saliency values at the fixation points relative to the average saliency in the image for the five
image categories. Time, measured in fixation units is plotted on the horizontal axis. The fixation-saliency values for the three
symmetry models and the contrast model of Itti and Koch are shown. The horizontal dotted line gives the expected value
when fixations are random (i.e., 1.0), and the error bars are the 95% confidence intervals.
Figure 4b shows a very good match between the symmetry
models and the human data for all images, suggesting that
the common fixations of the participants are captured. The
difference with the contrast-saliency model is significant.
Figure 5 shows the fixation-saliency values as a function
of the fixation number. For all image categories, the symmetry models have high values for the early fixations, and
gradually dropping values for later fixations. This shows
that humans first fixate on highly symmetrical parts of the
images. The fixation-saliency scores for the contrast model,
on the other hand, are more or less constant over time, except for the animal category. For the images containing
natural symmetries, the difference with the contrast saliency
model is highly significant for all fixations. But also for
most other categories, the symmetry models score better,
especially at the first few fixations. For the street-scene, the
building, and the nature categories, symmetry at early fixations is significantly higher than contrast. For later fixations,
the difference is less apparent, but still in favor of the symmetry models, and significant for the nature category. For
the animal category, we see a different picture. There, the
symmetry and contrast model do not significantly differ,
with higher scores for the contrast model. Furthermore, the
contrast values are not constant for this category, but drop
over time. This difference can be explained by the fact that
the animal images contain objects (the animals) that are
highly distinguishable from their, often, more or less uniform and blurry background. In contrast, the fore- and background in the other categories are less distinct, and more
cluttered.

shown. The five groups of bars contain the results for the
different image categories. Within each group, the bars
show the mean correlation coefficients for the saliency
models. The error bars give the 95% confidence intervals on
the mean. The scores of the saliency methods are plotted
along with the inter-participant correlation, and the correlation of the human data with random fixations. The first is
depicted by the horizontal gray bar with a solid mid-line,
giving the mean and 95% confidence interval. The correlation with random fixations is depicted by the horizontal
dashed line, which is, as expected, virtually zero for all
categories. All means and confidence intervals are calculated using multi-level bootstrapping. Significant differences can be appreciated by looking at the 95% confidence
intervals.
The inter-participant correlation is calculated for every
image by correlating the fixation-distance maps of the participants with those of all other participants, resulting in a
similarity measure among participants. The plot shows that
there is variability among the participants. The saliency
methods are also faced with this variability, which pulls
down the correlation values. The inter-participant correlation can therefore be used to put the scores of the saliency
methods into perspective. The correlation scores of the
models can be higher when the variation is high.
Looking at the bars and the 95% confidence intervals in
Figure 4a, it can be appreciated that the performance of the
three symmetry models is significantly higher than that of
the contrast model. This is not only true for the natural symmetry images containing explicit symmetrical forms, also
for the other categories the symmetry models significantly
outperform the contrast model. For all categories, the performance of the symmetry models is in the same range as
the inter-participant score. Among the three symmetry models, there is no significant difference in performance.
The values in Figure 4a are relatively low, caused by the
variability among participants. Figure 4b shows the results
of the comparison of the human data with the combined
fixation-distance maps. This gives the similarity between
the saliency models and the consensus among participants.

Discussion
We proposed three symmetry saliency models to investigate
the role of local symmetry in guiding eye fixations. To test
the performance of the models, we conducted an eyetracking study, and evaluated the prediction of the models
with the human data. We used the contrast saliency model
of Itti and Koch (Itti et al., 1998) to compare our results.
The results clearly show that human eye fixations can be
significantly better predicted with our symmetry models

60

Kaufman, L., & Richards, W. (1969). Spontaneous Fixation
Tendencies for Visual Forms. Perception & Psychophysics, 5(2), 85-88.
Kootstra, G., Nederveen, A., & De Boer, B. (2008, 1-4 September 2008). Paying Attention to Symmetry. In proceedings of the British Machine Vision Conference, Leeds,
UK.
Le Meur, O., Le Callet, P., Barba, D., & Thoreau, D. (2006).
A Coherent Computational Approach to Model BottomUp Visual Attention. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 28(5), 802-817.
Locher, P. J., & Nodine, C. F. (1987). Symmetry Catches
the Eye. In J. K. O'Regan & A. Lévy-Schoen (Eds.), Eye
Movements: From Physiology to Cognition. NorthHolland: Elsevier Science Publishers B.V.
Olmos, A., & Kingdom, F. A. A. (2004). McGill Calibrated
Colour Image Database, http://tabby.vision.mcgill.ca.
Ouerhani, N., von Wartburg, R., Hügli, H., & Müri, R.
(2004). Empirical Validation of the Saliency-based Model
of Visual Attention. Electronic Letters on Computer Vision and Image Analysis, 3(1), 13-14.
Palmer, S. E. (1991). Goodness, Gestalt, Groups, and Garner: Local Symmetry Subgroups as a Theory of Figural
Goodness. In G. R. Lockhead & J. R. Pomerantz (Eds.),
The Perception of Structure. Essays in Honor of Wendell
R. Garner (pp. 23-40). Washington, DC: American Psychological Association.
Palmer, S. E., & Hemenway, K. (1978). Orientation and
Symmetry: Effects of Multiple, Rotational, and Near
Symmetries. Journal of Experimental Psychology: Human Perception and Performance, 4(4), 691-702.
Parkhurst, D. J., Law, K., & Niebur, E. (2002). Modeling
the Role of Salience in the Allocation of Overt Visual Attention. Vision Research, 42, 107-123.
Parkhurst, D. J., & Niebur, E. (2003). Scene Content Selected by Active Vision. Spatial Vision, 16(2), 125-154.
Privitera, C. M., & Stark, L. W. (2000). Algorithms for Defining Visual Regions-of-Interest: Comparison with Eye
Fixations. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 22(9), 970-982.
Reisfeld, D., Wolfson, H., & Yeshurun, Y. (1995). ContextFree Attentional Operators: The Generalized Symmetry
Transform. International Journal of Computer Vision, 14,
119-130.
Richards, W., & Kaufman, L. (1969). "Center-of-Gravity"
Tendencies for Fixations and Flow Patterns. Perception &
Psychophysics, 5(2), 81-84.
Royer, F. L. (1981). Detection of Symmetry. Journal of
Experimental Psychology: Human Perception and Performance, 7(6), 1186-1210.
Treisman, A. M., & Gelade, G. (1980). A FeatureIntegration Theory of Attention. Cognitive Psychology,
12(1), 97-136.
Tyler, C. W. (2000). The Human Expression of Symmetry:
Art and Neuroscience. In proceedings of the ICUS Symmetry Symposium, Seoul.

than with the contrast model. Our models show a significantly better correlation with the human data, comparable
with the inter-participant correlation, for the individual participants. Particularly when we look at the combined fixation-distance maps, we see a very good match, showing that
the proposed models predict the general consensus. The
proposed models are general models in the sense that they
do not only perform well on the images containing natural
symmetrical objects, but also on images that are not selected
for their symmetrical content. Moreover, the fixationsaliency analysis shows that especially early fixations are on
highly symmetrical content, suggesting that symmetry is a
good predictor of involuntary eye fixations, which are presumably more bottom-up controlled.
A possible explanation that symmetry is a good predictor
of eye fixations is that eye movements are object oriented.
Symmetry is known to play a role in figure-ground segregation (Driver et al., 1992). Local symmetry can therefore
serve as a bottom-up cue for the presence of an object in the
image, which is then further inspected by making a fixation.
We expected the radial symmetry model to outperform
the isotropic symmetry model since psychophysical studies
showed that humans are more sensitive to forms with multiple symmetry axes (Palmer & Hemenway, 1978). However,
this did not result in a significant increase in performance.
This can be explained by the fact that the isotropic model
already gives a stronger response to patterns with multiple
symmetry axes. Also color does not improve the model.
Contrast obviously also plays a role in bottom-up attention. In future research, we would like to study the combination of contrast and symmetry for the prediction of fixations.
To conclude, the symmetry models that we developed are
good predictors of visual attention, suggesting that humans
pay attention to symmetry.

References
Barlow, H. B., & Reeves, B. C. (1979). The Versatility and
Absolute Efficiency of Detecting Mirror Symmetry in
Random Dot Displays. Vision Research, 19, 783-793.
Driver, J., Baylis, G. C., & Rafal, R. D. (1992). Preserved
figure-ground segregation and symmetry perception in
visual neglect. Nature, 360, 73 - 75.
Grammer, K., & Thornhill, R. (1994). Human (Home
sapiens) Facial Attractiveness and Sexual Selection: The
Role of Symmetry and Averageness. Journal of Comparative Psychology, 108(3), 233-242.
Heidemann, G. (2004). Focus-of-Attention from Local
Color Symmetries. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(7), 817-830.
Itti, L., & Koch, C. (2001). Computational Modelling of
Visual Attention. Nature Reviews Neuroscience, 2(3),
194-203.
Itti, L., Koch, C., & Niebur, E. (1998). A Model of Saliency-Based Visual Attention for Rapid Scene Analysis.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(11), 1254-1259.

61

