Predictions for narrow
range of parameter
100

80

80

60

60

40

40

20

20

Correct responses
(POW)

100

0

0

5

10

15

20

Correct responses
(EXP)

100

0

0

5

10

15

20

100

80

80

60

60

40

40

20

20

0

updated at each stage. At each stage, the information gained
from all prior stages is used to improve the design at the current stage. Thus, the problem to be solved in adaptive design
optimization (ADO) is to identify the most informative design at each stage of the experiment, taking into account the
results of all previous stages, so that one can infer the underlying model and its parameter values in as few steps as
possible.

Predictions for wide
range of parameters

0

5

10
15
Lag time

20

0

Finding optimal adaptive designs in a general setting requires simultaneous high-dimensional optimization and integration. This can be an exteremely difficult task given that
the integration can be intractable for the complex, nonlinear
models.
0

5

10
15
Lag time

Despite these computational challenges, the flexibility and
efficiency of adaptive designs have made them popular in various fields. For example, in astrophysics, ADO has been used
in the design of experiments to distinguish between dark energy models (Heavens et al., 2007). ADO has also been used
in designing phase I and phase II clinical trials to ascertain
the dose-response relationship of experimental drugs (Haines
et al., 2003; Ding et al., 2008), as well as in estimating psychometric functions (Kujala & Lukka, 2006; Lesmes et al.,
2006). However, because identification of optimal adaptive
designs in general settings was not possible until recently,
most of this prior work focused on problems that were sufficiently simple (e.g. linear models with normal error) so that
analytical solutions could be found.

20

Figure 1: Scatter plot of the probabilities of each number of
correct responses, out of 100 Bernoulli trials, predicted by
power model (top row) and exponential model (bottom row)
for a narrow range of parameters (left column) and a wide
range of parameters (right column). Darker colors represent
higher probabilities.

inspection is meant to illustrate the more complicated problems that arises in practice. In actual retention experiments, it
would be highly unusual to have enough prior information to
restrict the model parameters to such a small range. When the
parameters are allowed to vary across a more realistic range,
the picture looks more like the second column of Figure 1. In
that case, an optimal design for discriminating the two models can not be found by visual inspection alone.
Given how closely the power and exponential models can
mimic one another when their parameters are allowed to
vary realistically, finding an optimal design for distinguishing them is no easy task. A principled approach to this
challenging problem can be found in Bayesian decision theory (Chaloner & Verdinelli, 1995). In the Bayesian framework, each possible experimental design is treated as a gamble whose payoff is determined by the outcome of an experiment carried out with that design. The idea is to estimate
the informativeness of hypothetical experiments carried out
with a given design, so that an ”expected utility” (i.e., informativeness) of that design can be computed. This is done by
considering every possible observation that could be obtained
from an experiment with that design and then evaluating the
relative likelihoods and utilities of those observations. The
design with the highest expected utility is then chosen as the
optimal design.
The Bayesian approach to experimental design optimization extends easily to the case of adaptively designed experiments, in which testing proceeds over the course of several
stages (e.g., trials), and the design of the experiment can be

We draw on a recent breakthrough in stochastic Bayesian
optimization (Müller et al., 2004) in offering a method for
finding optimal adaptive designs in a general setting, without relying on approximations nor simplifying assumptions.
The basic idea is to recast the problem as a probability density simulation in which the optimal design corresponds to the
mode of the distribution. This allows one to find the optimal
design without having to evaluate the integration and optimization directly. The density is simulated by Markov Chain
Monte-Carlo (Gilks et al., 1996), and the mode is sought by
gradually ”sharpening” the distribution with a simulated annealing procedure (Kirkpatrick et al., 1983).
In the remainder of the paper, we first give a formal account
of the Bayesian framework for ADO, including the computational algorithm for finding optimal designs via probability density simulation, and then we demonstrate the implementation of ADO in a computer-simulated retention experiment. Readers interested in only an example of its application
should jump to the section ”ADO for discriminating retention
models.” However, we wish to emphasize that this example is
just one of many potential applications of ADO. The computational algorithm is very general and works just as well with
other types of design variables. Readers interested in additional applications, such as optimization of categorization experiments, should see Myung & Pitt (in press).

94

Bayesian Adaptive Design Optimization

stage. The above updating scheme is applied successively
on each stage of experimentation, after an initialization with
equal model priors p(s=0) (m) = 1/K and a non-informative
parameter prior p(s=0) (θm ).

Adaptive design optimization within a Bayesian framework
has been considered at length in the statistics community
(Atkinson & Donev, 1992) as well as in other science and engineering disciplines (e.g. El-Gamal & Palfrey, 1996; Bardsley et al., 1996; Allen et al., 2003). The issue is essentially a Bayesian decision problem where, at each stage s =
(1, 2, . . .) of experimentation, the most informative design at
stage s (i.e., that design with the highest expected utility) is
chosen based on the outcomes of the previous experiments
{y1 , . . . , ys−1 }. The criterion for the informativeness of a design often depends on the goals of the experimenter. The experiment which yields the most precise parameter estimates
may not be the most effective at discriminating among competing models, for example (see Nelson, 2005, for a comparison of several utility functions that have been used for optimal
design of experiments in cognitive science).
Whatever the goals of the experiment may be, solving for
the optimal design is a highly nontrivial problem. Formally,
Bayesian ADO entails finding an optimal design ds∗ , at each
stage s of the experiment, which maximizes a utility function
Us (d) defined as
K

U(d) =

Computational Methods: Finding an optimal
design via probability simulation
Given the mutliple computational challenges involved in finding the design d ∗ , standard optimization methods such as
Newton-Raphson and Revelverg-Marquardt are insufficient.
However, a promising new approach to this problem has
been proposed in statistics (Müller et al., 2004). It is a
simulation-based approach that includes an ingenious computational trick that allows one to find the optimal design
without having to evaluate the integration and optimization
directly in Equation (1). The basic idea is to recast the problem as a simulation from a sequence of augmented probability
models.
To illustrate how it works, let us consider the design optimization problem to be solved at any given stage s of experimentation, and, for simplicity, we will suppress the subscript
s in the remainder of this section. According to the ingenious
trick of Müller et al. (2004), we treat the design d as a random variable and define an auxiliary distribution h(d, ·) that
admits U(d) as its marginal density. Specifically, assuming
that u(d, θm , y) is non-negative and bounded, we define

Z Z

∑ p(m)

u(d, θm , y) p(y|θm , d) p(θm ) dy dθm ,

m=1

(1)
where m = {1, 2, . . . , K} is one of a set of K models being
considered, d is a design, θm is a parameterization of model
m, and y is the outcome of an experiment with design d under model m with parameter θm . We refer to the function
u(d, θm , y) in Equation (1) as the local utility of the design
d. It measures the utility of a hypothetical experiment carried
out with design d, when the true model is m with parameter
θm , and the sample y is obtained. Thus, Us (d) represents the
expected value of the local utility function, where the expectation is taken over all models under consideration, the full
parameter space of each model, and all possible outcomes,
with respect to the model prior probability ps (m), the prior
parameter probability ps (θm ), and the sampling distribution
function p(y|θm ), respectively.
The model and parameter priors are updated at each stage
s = {1, 2, . . .} of experimentation. Specifically, upon the specific outcome zs observed at stage s of an actual experiment
carried out with design ds , the model and parameter priors to
be used to find an optimal design at the next stage are updated
via Bayes rule and Bayes factor calculation
ps+1 (θm ) =
ps+1 (m) =

p(zs |θm , ds ) ps (θm )
p(zs |θm , ds ) ps (θm ) dθm
p0 (m)
K
∑k=1 p0 (k) BF(k,m) (zs ) ps (θ)
R

h(d,y1 , θ1 , . . . , yK , θK ) =
"
K

α

#

∑ p(m) u(d, θm , ym )

p(y1 , θ1 , . . . , yK , θK |d)

(4)

m=1

where α(> 0) is the normalizing constant of the marginal distribution, and
K

p(y1 , θ1 , . . . , yK , θK |d) =

∏ p(ym |θm , d)p(θm )

m=1

is the joint prior distribution of observations y and parameters θ. Note that the subscript m in the above equation refers
to model m, not the stage of experimentation. For instance,
ym denotes an experimental outcome generated from model
m with design d and parameter θm . It can be shown that
marginalizing h(d, ·) over {y1 , θ1 , . . . , yK , θK } in Equation (4)
yields h(d) = αU(d). This means that the probability model
h(d) generates designs with probability proportional U(d).
Consequently, the design with the highest utility according to
U(d) can be found by taking the mode of a sufficiently large
sample of designs drawn from the marginal distribution h(d).
However, finding the global optimum could potentially require a very large number of samples from h(d), especially
if there are many local optima or if the design space is very
irregular or high-dimensional. To overcome this problem, we
augment the auxiliary distribution with independent samples
of y’s and θ’s given design d as follows:

(2)
(3)

where BF(k,m) (zs ) denotes the Bayes factor defined as the ratio of the marginal likelihood of model k to that of model m
given the realized outcome zs , where the marginal likelihoods
are computed using the updated priors from the preceding

J

hJ (d, ·) = αJ ∏ h(d, y1, j , θ1, j , . . . , yK, j , θK, j )
j=1

95

(5)

for a positive integer J and αJ (> 0). The marginal distribution of hJ (·) obtained after integrating out model parameters
and outcome variables will then be equal to αJ U(d)J . Hence,
as J increases, the distribution hJ will become more highly
peaked around its (global) mode corresponding to the optimal design d ∗ , thereby making it easier to identify the mode.
Following Amzal et al. (2006), we implement a sequential
Monte Carlo particle filter algorithm that begins by simulating hJ (d, ·) in Equation (5) for J = 1, and then increases J
incrementally on subsequent iterations on an appropriate simulated annealing schedule (Kirkpatrick et al., 1983; Doucet et
al., 2001).

1

Probability of True Model (EXP)

0.9

ADO for discriminating retention models

0.8
0.7
0.6
0.5
0.4
0.3
0.2

Optimal Adaptive Design
Random Sequential Design
Fixed 10pt Design

0.1

We conducted a computer simulation to illustrate the ADO
procedure, the purpose of which was to design an experiment to discriminate between the retention models in Table 1.
The point of the simulated experiment is to demonstrate how
quickly an optimal adaptive experiment can correctly identify
the true model, which in this case was EXP with parameters
a = 0.7103 and b = 0.0833.
The first step in conducting the simulation was to choose
a utility funtion (i.e., measure of model discriminability) that
was appropriate for the goal of the experiment. Since the goal
was to discriminate between models POW and EXP, we chose
to use a utility function based on the entropy of the posterior
model probabilities. Specifically, we set the utility of a design
d to be the mutual information (Cover & Thomas, 1991) between M and Yd , where M is a random variable representing
uncertainty about the true model, and Yd is a random variable
denoting the result of an experiment carried out with design
d.
The next step was to solve for the optimal design (i.e.,
the time lags) to use in the initial stage of the experiment,
based on the initial (i.e., prior) model and parameter probabilities. We considered designs with one test phase (i.e., recall
of the study items) after a single time lag, and a fixed binomial sample size of n = 30 at each stage of experimentation.
Using equal prior probabilities on POW and EXP, and independent beta priors on a and b in each model1 , we used the
just-described algorithm to find the time lag, d1 , that maximized U1 (d). We then generated data from the true model at
t = d1 , and updated the prior probabilities accordingly. These
updated distributions were then used as the priors for finding
the optimal design at the next stage, so that the process could
be repeated. We continued this process for eight stages of the
experiment. A typical profile of the posterior model probability ps (EXP) as a function of stage s is shown by the solid
black line in Figure 2. The optimal adaptive design identifies

0

0

2

4
Stage of Experiment

6

8

Figure 2: Posterior model probabilities in the simulated experiments.
the correct model with over 0.95 probability after just three
stages or 90 Bernoulli observations.
To help illustrate the logic of the ADO procedure in the
simulated experiment, the predictions of the two models at
each of the first five stages (trials) are shown in Figure 3. Recall that our goal in ADO is to choose, at each stage, the lag
time at which the predictions of the models differ the most,
and then update model probabilities based on the relative likelihoods of the observed outcome. For example, in stage 2 of
the simulated experiment, depicted by the second column of
graphs in Figure 3, 96.4 seconds was found to be the optimal design. It can be seen from the stage-2 scatter plots that
EXP predicts fewer correct responses than POW at lag times
around 96.4 seconds. When zero correct responses were observed (an outome that is much more likely under EXP than
under POW, as indicated by the colors at the tips of the arrows in Figure 3) the odds of EXP are increased from 0.35 to
0.7 in stage 3 and the odds of POW are decreased from 0.65
to 0.3. As this process continues, the predictions of the models narrow around the observed outcomes and the posterior
probability of the true model (EXP) approaches 1.
For comparison of ADO against a different sequential design scheme, we also conducted several simulated experiments with randomly generated designs. These randomly designed experiments proceeded in the manner described above,
except that the lag time at each stage was chosen randomly
between zero and 100 seconds. The solid gray line in Figure
2 shows a typical posterior model probability curve obtained
in these experiments. The line shows that the true model
could be identified with 0.90 probability after 6 stages or 180
Bernoulli trials; that’s nearly twice as many as were required
when ADO was used.2

1 We used priors a ∼ Beta(2, 1) and b ∼ Beta(1, 4) for the power
model and a ∼ Beta(2, 1) and b ∼ Beta(1, 80) for the exponential
model over each model’s parameter space, 0 < a, b < 1. These priors reflect conventional wisdom about these retention models based
on many years of investigation. The choice of priors does indeed
change the optimal solution, but the importance of this example is
the process of finding a solution, not the actual solution itself.

2 We repeated the simulations several times in order to verify that

96

Stage 1
Stage 2
Stage 3
Stage 4
Stage 5
Optimal design: 16.1s Optimal design: 96.4s Optimal design: 96.8s Optimal design: 4.3s Optimal Design: 59.9s
Correct responses: 7 Correct responses: 0 Correct responses: 0 Correct responses: 12 Correct responses: 2
30
30
30
30
30
prPOW)=0.5
pr(POW)=0.02
pr(POW)=0.30
pr(POW)<0.001
pr(POW)=0.65
20

20

20

20

20

10

10

10

10

10

0

0

30

50

100

pr(EXP)=0.5

0

0

50

100

30

0

0

50

100

30

0

0

50

100

30

20

20

20

20

10

10

10

10

10

0

50
Lag Time

100

0

0

50
Lag Time

100

0

0

50
Lag Time

100

0

50

100

pr(EXP)>0.999

20

0

0

pr(EXP)=0.98

pr(EXP)=0.70

pr(EXP)=0.35

0
30

0

50
Lag Time

100

0

0

50
Lag Time

100

Figure 3: Predictions of the power and exponential models in the first five stages (i.e., trials) of the simulated experiment. The
predictions are based on the prior parameter estimates at each stage. For example, the stage 2 column of scatter plots depicts
outcome probabilities at each lag time, and for each model, according to parameter estimates that were updated based on the
results observed in stage 1. The text above and inside the graphs provide information about the prior probabilites of each model,
the optimal designs for discriminating the models, and the observed outcomes (correct responses) at each stage of the simulated
experiment. Arrows denote the number of correct responses at the optimal lag time
To compare ADO against a more principled approach than
randomly selecting lag times at each stage, we conducted additional simulations using a typical design from the retention
literature. While there is no established standard for the set
of lag times to test in actual retention experiments, a few conventions have emerged. For one, previous experiments utilize
what we call ‘fixed designs,’ in which the set of lag times at
which to assess retention are specified before experimentation begins, and held fixed for the duration of the experiment.
Thus, there is no Bayesian updating between stages as there
would be in a sequential design. The lag times are typically
concentrated near zero and spaced roughly geometrically. For
example, Rubin et al. (1999) used a design consisting of 10
lag times: (0s, 1s, 2s, 4s, 7s, 12s, 21s, 35s, 59s, 99s). To get a
meaningful comparison between this fixed design and the sequential designs that we just tested, we generated data at each
stage from the same model as in the previous simulations,
but with just 3 Bernoulli trials at each of the 10 lag times in
the Rubin et al. design. That way, each stage of the experiment included the same number of total observations as in the
experiments with just one lag time at each stage. Posterior
model probabilities and parameter estimates were computed
after each stage but not carried over as priors for subsequent
stages. The obtained posterior model probabilities from a typical simulation are shown by the dashed line in Figure 2.

The results of these simulations clearly demonstrate the advantage of the optimal adaptive design. In the experiment
designed with ADO, the true model could be identifed with
over 0.95 probability after just three stages or 90 Bernoulli
observations. In contrast, the experiments that did not use
ADO required twice as many observations (6 stages or 180
Bernoulli trials) to produce a similar level of evidence in favor of the true model. This marked improvement in efficiency
is even more striking in light of the fact that the ADO experiment was optimized over just one of many possible design
variables. Even better designs could potentially be obtained
by optimizing over additional design variables, such as the
number of lag times to test at each stage, and the number of
Bernoulli trials to allocate to teach lag time at each stage.
It is worth noting that the preceding simulations are intended only as a proof-of-concept. Obviously, in this simple
case, an optimal design could be found via comprehensive
grid searches. However, the approach that we have demonstrated here generalizes easily to much more complex problems in which brute-force searchers would be impractical or
impossible.

Other Applications of ADO
The design optimization scheme that we have presented here
can be applied in a variety of settings besides the simple retention experiment described above. Simply put, any experiment in which observations can be made sequentially, and
for which the goal is to distinguish among closed form mathematical models, could potentially benefit from the applica-

the results were not due to random chance. Occasionally the random approach stumbled upon a good design and performed very
well, and sometimes even an optimal design was hindered by unlikely experimental outcomes, but the reported results reflect typical
performances of the two designs.

97

tion of ADO. For a different example, consider the following
application to an experiment in the field of cognitive development.

ing of design points for model discrimination. Computers
& Chemistry, 20, 145-157.
Chaloner, K., & Verdinelli, I. (1995). Bayesian experimental
design: A review. Statistical Science, 10(3), 273-304.
Cover, T., & Thomas, J. (1991). Elements of information
theory. John Wiley & Sons, Inc.
Ding, M., Rosner, G., & Müller, P. (2008). Bayesian optimal
design for phase ii screening trials. Biometrics, 64, 886894.
Doucet, A., Freitas, N. de, & Gordon, N. (2001). Sequential
monte carlo methods in practice. Springer.
El-Gamal, M., & Palfrey, T.(1996). Economical experiments:
Bayesian efficient experimental design. International Journal of Game Theory, 25, 495-517.
Gilks, W. R., Richardson, S., & Spiegelhalter, D. (1996).
Markov chain monte carlo in practice. Chapman & Hall.
Haines, L., Perevozskaya, I., & Rosenberer, W. (2003).
Bayesian optimal designs for phase i clinical trials. Biometrics, 59, 591-600.
Heavens, A., Kitching, T., & Verde, L. (2007). On model
selection forecasting, dark energy and modified gravity. Monthly Notices of the Royal Astronomical Society,
380(3), 1029–1035.
Kirkpatrick, S., Gelatt, C., & Vecchi, M.(1983). Optimization
by simulated annealing. Science, 220, 671–680.
Kujala, J., & Lukka, T.(2006). Bayesian adaptive estimation:
The next dimension. Journal of Mathematical Psychology,
50(4), 369-389.
Lesmes, L., Jeon, S.-T., Lu, Z.-L., & Dosher, B. (2006).
Bayesian adaptive estimation of threshold versus contrast
external noise functions: The quick tvc method. Vision Research, 46, 3160-3176.
Müller, P., Sanso, B., & De Iorio, M. (2004). Optimal
bayesian design by inhomogeneous markov chain simulation. Journal of the American Statistical Association,
99(467), 788–798.
Myung, J. I., & Pitt, M. (in press). Optimal experimental
design for model discrimination. Psychological Review.
Nelson, J. (2005). Finding useful questions: On bayesian diagnosticity, probability, impact, and information gain. Psychological Review, 112(4), 979–999.
Opfer, J., & Siegler, R. (2007). Representational change and
children’s numerical estimation. Cognitive Psychology, 55,
165-195.
Rubin, D., Hinton, S., & Wenzel, A.(1999). The precise time
course of retention. Journal of Experimental Psychology,
25(5), 1161–1176.
Rubin, D., & Wenzel, A.(1996). One hundred years of forgetting: A quantitative description of retention. Psychological
Review, 103(4), 734–760.
Tang, Y., Cavagnaro, D. R., Myung, J. I., & Pitt, M. A.(2009).
Adaptive design optimization for developmental psychology experiments. (Presented at The 42nd Annual Meeting
of the Society for Mathematical Psychology)

ADO for Developmental Psychology
It is widely believed that the abstract representation of numbers by children transitions from a logarithmic scale to a linear scale as they grow(Opfer & Siegler, 2007). Verifying this
hypothesis leads to the problem of discriminating between
linear and logarithmic models in experiments. Typical experiments designed to elicit such numerical representations use
a “number-to-position” task, in which children are shown a
number between zero and 1000 and asked to estimate its position on a line. The choice of numbers to be shown at each
stage of the experiment constitutes a design variable that can
be controlled by the experimenter, hence choosing appropriate numbers to show (i.e. designs) in order to quickly and
reliably discriminate linear and log models could be seen as a
design optimization problem. Simulation results have shown
that using ADO to find the optimal set of numbers to show
in each trial greatly reduces the number of trials required to
reveal the data-generating model (Tang et al., 2009).

Conclusion
Computational models are precise in their predictions about
human behavior. Our experimental methods for probing cognition to assess the accuracy of their predictions are much less
so. Adaptive design optimization is a tool that can not only
improve the informativeness of an experiment in discriminating models, but also increase the efficiency of data collection.
We are currently applying the method in experimentation and
expanding its application to more complex experimental designs and models.

Acknowledgements
This research is supported by National Institute of Health
Grant R01-MH57472 to JIM and MAP. We wish to thank
Hendrik Küeck and Nando de Freitas for valuable feedback
and technical help provided for the project, and Michael Rosner for the implementation of the design optimization algorithm in C++.

References
Allen, T., Yu, L., & Schmitz, J. (2003). An experimental design criterion for minimizing meta-model prediction errors
applied to die casting process design. Applied Statistics,
52, 103-117.
Amzal, B., Bois, F., Parent, E., & Robert, C. (2006).
Bayesian-optimal design via interacting particle systems.
Journal of the American Statistical Association, 101(474),
773–785.
Atkinson, A., & Donev, A. (1992). Optimum experimental
designs. Oxford University Press.
Bardsley, W., Wood, R., & Melikhova, E.(1996). Optimal design: A computer program to study the best possible spac-

98

