UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Hard to put your finger on it: Haptic modality disadvantage in conceptual processing

Permalink
https://escholarship.org/uc/item/3390k2jx

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)

Authors
Connell, Louise
Lynott, Dermot

Publication Date
2009-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Hard to Put Your Finger on it:
Haptic Modality Disadvantage in Conceptual Processing
Louise Connell (louise.connell@manchester.ac.uk)
Dermot Lynott (dermot.lynott@manchester.ac.uk)
School of Psychological Sciences, University of Manchester
Oxford Road, Manchester M13 9PL, UK
and inhibit how we perceive an entity in the real world
should also influence how we conceive of that entity during
language comprehension. Several studies demonstrate
instances where this is indeed the case (Goldstone &
Barsalou, 1998; Pecher, Zeelenberg & Barsalou, 2003). For
example, Spence, Nicholls and Driver (2001; see also
Turatto, Galfano, Bridgeman & Umiltà, 2004) asked people
to indicate the left/right location of a series of perceptual
stimuli, and found that switching modalities from one trial
to the next (e.g., from a visual light flash to an auditory
tone) incurred a processing cost. Pecher et al. (2003)
investigated whether this switching cost effect extended to
conceptual processing by asking people to verify a series of
object properties from different modalities, presented as text
onscreen. They found that people were slower to verify a
property in a given modality (e.g., auditory leaves:rustling)
after verifying a property in a different modality (e.g., visual
apple:shiny) than after verifying a property in the same
modality (e.g., auditory blender:loud), and that this effect
was not due to associative priming. Pecher et al. (see also
van Dantzig, Pecher, Zeelenberg & Barsalou, 2008)
concluded that these switching costs during language
comprehension, like those found by Spence et al. during
perceptual tasks, resulted from the re-allocation of attention
from one modality-specific brain system to another.
Modality-specific perceptual simulation has also been
implicated in the processing of single words. In both
behavioural and cognitive neuroscience research, while finegrained sensory distinctions have been long been noted,
more recent work has highlighted the continuity between
conceptual and perceptual knowledge with respect to the
different sensory modalities. For example, Gonzáles and
colleagues (2006) found that passively reading scent-related
words (e.g., cinnamon) increased activation in the primary
olfactory areas of the piriform cortex (similar to
Pulvermüller's 2005 finding of motor cortex activation for
action words). Regarding visual processing, Simmons et al.
(2007) showed that verifying colour properties in text (e.g.,
that a banana is yellow) led to activation in the same region
of the left fusiform gyrus in the visual cortex as a perceptual
task that involved judging colour sequences. Similarly,
Newman, Klatzky, Lederman and Just (2005) examined
visual and haptic modalities by asking participants to
compare various objects and found differential activation in
the inferior extrastriate and intraparietal sulcus depending
on whether visual features (e.g., which is bigger? pear OR
egg) or haptic features (e.g., which is harder? potato OR
mushroom) formed the basis for comparison. Further
comparisons by Goldberg, Perfetti and Schneider (2006)

Abstract
Recent neuroimaging research has shown that perceptual and
conceptual processing share a common, modality-specific
neural substrate, while work on modality switching costs
shows that they appear to share some of the same attentional
mechanisms. In two experiments, we employed a modality
detection task that displayed modality-specific object
properties (e.g., shrill, warm, crimson) for extremely short
display times and asked participants to judge whether each
property corresponded to a particular target modality
(auditory, gustatory, haptic, olfactory, or visual). Results
show that perceptual and conceptual processing share a haptic
disadvantage: people need more time to detect expected
information regarding the sense of touch than any other
modality. These findings support the assertions of embodied
views that the conceptual system uses the perceptual system
for the purposes of representation and are discussed with
reference to differences in endogenous attentional control.

Introduction
It has become increasingly clear of late that cognition
cannot be successfully studied by marginalising the roles of
body, world and action. Embodied cognition research
represents a recent trend to cease viewing conceptualisation
and mental representation in terms of abstract information
processing, but rather as perceptual and motor simulation.
While early, influential work in cognitive psychology
advocated symbolic knowledge structures (Collins &
Quillian, 1969; Newell & Simon, 1972; Tulving, 1972),
recent years have witnessed a multidisciplinary convergence
of opinion from cognitive psychology (Barsalou, 1999;
Glenberg, 1997), linguistics (Gibbs, 2003; MacWhinney,
1999) and artificial intelligence (Anderson, 2003; Chrisley,
2003) that cognition is situated in, rather than independent
from, its environment. Embodied theories of cognition hold
that conceptual thought is grounded in the same neural
systems that govern sensation, perception and action
(Glenberg & Kaschak, 2002; Johnson-Laird, 1983; Pecher
& Zwaan, 2005) and one of the most influential views is
Barsalou’s (1999, 2008) Perceptual Symbol Systems
account. According to this theory, concepts are essentially
partial recordings of the neural activation that arises during
perceptual and motor experiences.
These recordings
(known as perceptual symbols) can later be re-enacted as a
perceptual simulation of that concept.
One of the most important elements of the embodied
view, separating it from other theories of mental
representation, is the dependence of conception on
perception. In other words, the same factors that facilitate

762

found that verification of colour, sound, touch and taste
properties activated cortical regions respectively associated
with encoding visual, auditory, haptic and gustatory
experiences. In sum, such studies illustrate that perceptual
experience and conceptual knowledge share a common
neural substrate.

detection of emotionally affective words at near-subliminal
thresholds (Dijksterhuis & Aarts, 2003). Participants will be
presented with unimodal object properties (i.e., perceived
through one sense alone, such as shiny, echoing) for
extremely short display times and asked to judge whether
the property corresponds to a target modality (e.g., visual).
By measuring accuracy rates for a range of increasing
display times above the subliminal threshold, we can
examine whether the perceptual haptic disadvantage also
emerges during conceptual processing.

The Current Study
If the conceptual system uses perceptual simulations for the
purposes of representation, then it follows that one should
expect perceptual phenomena to emerge in conceptual
processing.
One such phenomenon is the haptic
disadvantage in perceptual processing, relative to vision and
audition. When people are asked to respond to the arrival of
a perceptual stimulus, they are generally slower to detect
haptic stimuli (e.g., finger vibration) than visual (e.g., light
flash) or auditory (e.g., noise burst) stimuli, even when they
are told which modality to expect (Spence et al., 2001;
Turatto et al., 2004). In other words, asking people to focus
their attention on the sense of sight, hearing or touch allows
information from the relevant modality to be processed
faster than that from other modalities, but expected haptic
stimuli take longer to process than expected visual or
auditory stimuli.
So why should haptic processing be disadvantaged?
There are obvious physiological differences in processing
stimuli from different perceptual modalities, with
differential latencies for transduction in the skin, retina, and
cochlea and for transmission of their respective signals to
the somatosensory, visual and auditory cortices. However,
since the retina is actually the slowest of the three in
converting a stimulus to an electrical signal and delivering it
to the brain, these physiological differences alone cannot
explain the haptic disadvantage in stimulus perception.
Rather, the haptic modality appears to be disadvantaged
when it comes to the resolution of the raw sensory signal
into a recognisable percept. Researchers have speculated on
a number of reasons why this might be the case. The haptic
modality may be special in requiring an internal, bodyfocused representation, in contrast to the visual or auditory
modalities requiring a representation of the external world,
and hence may require a different attentional perspective
(Martin, 1995; Spence et al., 2001). For example, if
something is being felt by touch, it is (by definition) located
on the body's surface, and there may be costs involved in
shifting attentional perspective to something that is seen or
heard some distance away. Alternatively, there may be an
adaptive advantage in coupling attention longer to visual
and auditory modalities than to haptic (Turatto et al., 2004).
In this account, approaching threats could be efficiently
detected by keeping attention focused on sight or sound, but
waiting to detect a potential threat by touch is unlikely to
have evolved as a useful attentional mechanism.
The current study aims to investigate if the haptic
disadvantage in perceptual processing also emerges during
conceptual processing. In two experiments, we use a
modality detection task to examine conceptual processing of
modality-specific words. The modality detection task is a
variant of that used to examine the positive/negative

Experiment 1
In this modality detection task, participants will first see
blocks for each modality (auditory, gustatory, haptic,
olfactory, visual) for an extremely short display time at the
threshold of subliminal perception (17ms), then the blocks
will be repeated for increasing display times (33ms, 50ms,
67ms, 100ms). We expect accuracy rates to improve from
near-chance performance over successive repetitions, both
because of practice effects and because longer display times
increase the probability of successful detection, but we
expect performance differences between modalities. In
particular, we predict faster detection of visual and auditory
properties (more accurate detection at earlier display times)
than haptic properties (i.e., the haptic disadvantage). Since
the sense of taste presumably requires as much of an
internal body representation as the sense of touch, Spence et
al.'s (2001) notion of attentional perspective suggests that
gustatory accuracy should be similar to haptic accuracy.
Likewise, since taste is not particularly useful in detecting
an approaching danger, Turatto et al.'s (2004) idea of
attentional adaptation for threat detection would suggest that
the gustatory modality should have similar accuracy to
haptic.

Method
Participants Forty-five native speakers of English, with no
reported reading or sensory deficits, participated in the
experiment for course credit or a fee of £5. Participants
were recruited via university email lists and notice boards
and through the university’s research volunteering website.
Three participants’ data were removed prior to analyses; two
due to pressing incorrect buttons during the experiment and
one due to a consistently high error rate (>80%).
Materials A set of 200 words were taken from Lynott &
Connell's (2009) modality exclusivity norms: 100 test items
and 100 fillers. These norms comprise 423 adjectives, each
describing an object property, with mean ratings (0-5) of
how strongly that property is experienced through each of
five perceptual modalities (auditory, gustatory, haptic,
olfactory, visual) plus a number of other useful statistics.
For this experiment, test items were selected to be unimodal,
and consisted of 20 words from each modality, where each
word had the highest score in the target modality (minimum
strength rating of 3) and all other modalities were at least
one full point lower on the the ratings scale (see Table 1 for
examples). Only 17 and 15 words met this criterion for the
haptic and olfactory modalities, respectively, and so

763

morphological variants of existing words were included
(e.g., odorous, malodorous) to ensure balanced blocks of 20
items per modality; data relating to these variants were
removed prior to analysis. There were no differences
between modalities in British National Corpus (BNC) word
frequency, orthographic length, or target modality strength
ratings of test words (all Bonferroni comparison ps>.18). In
addition, we used the English Lexicon Project database
(Balota et al., 2007) to examine further lexical
characteristics of the test words. Seventeen of our test
words were not featured in the eLexicon database
(distributed across modalities), but tests on those words
present showed that there were no differences between
modalities in the lexical decision time or accuracy of each
word, nor in the number of orthographic, phonological or
phonographic neighbours of each word (all Bonferroni
comparison ps>.2).
Twenty filler items were selected per modality so that
each filler word had a low strength rating (less than 2) on
the target modality. This meant that all fillers had
significantly lower strength on the target modality than the
corresponding test words (all Bonferroni comparison
ps<.001). However, there were no differences between test
and filler words in BNC frequency or orthographic length
(all Bonferroni ps>.25).

Design A two-factor repeated measures design employed the
factors of modality (auditory, gustatory, haptic, olfactory,
visual) and display duration (17ms, 33ms, 50ms, 67ms,
100ms). As per Dijksterhuis and Aarts (2003), the
proportion of correctly detected words per participant per
condition are subjected to analyses of variance. Effect sizes
are reported as generalized eta-squared ( 2G), which allows
direct comparison of within- and between-participants
designs (Olejnik & Algina, 2003).

100
90

Accuracy (%)

80

Gustatory
bitter
bland
palatable
salty
tangy

Haptic
chilly
itchy
silky
ticklish
warm

Olfactory
aromatic
fragrant
musky
perfumed
stinky

82.4

75.8

70

Auditory
Gustatory
Haptic
Olfactory
Visual

69.7

60
5053.3

Table 1: Sample words for each modality used in
Experiments 1 and 2.
Auditory
bleeping
echoing
loud
shrill
squeaking

80.5

40
17

Visual
crimson
dazzling
flickering
pale
shiny

33

50

67

100

Display Duration (ms)

Figure 1: Percentage of correctly-detected words per
modality and duration in Experiment 1 (yes/no task). Error
bars represent 95% confidence intervals for withinparticipant designs (Loftus & Masson, 1994), calculated per
display duration, and for clarity are only shown for the
haptic modality.

Procedure Participants were instructed that they would be
asked to judge whether or not words appearing onscreen
could be experienced through a particular sense (heard,
tasted, felt through touch, smelled or seen). They were told
that words would appear onscreen one at a time and be
covered very quickly by a row of Xs, and that they should
press “Yes” (the comma key) if the word could be perceived
through that sense or “No” (the full stop key) if it could not.
Stimuli were arranged into blocks of test and filler words for
each modality; since all test items pertained to the given
modality and all fillers did not, there was an equal ratio of
yes:no responses within each block. At the start of each
block, participants were told which sense they would be
making judgements about.
When participants had
completed all five modality blocks with a display duration
of 17s, the same five blocks were repeated at 33ms, 50ms,
67ms, and 100ms. Items were presented randomly within
each block, with each trial beginning with a central fixation
(250ms), followed by a word (displayed for different
durations depending on the block), followed by a mask (a
row of Xs) until the participant responded. Response times
(RTs) were measured from mask onset to keypress.

Results & Discussion
Responses to test words less than 200 ms or more than three
standard deviations away from a participant's mean per
display duration were removed as outliers (3.7% of data).
The percentage of correctly detected test words per modality
per display time is shown in Figure 1, where 50% represents
performance at chance level.
There was an overall main effect of modality [F(4, 164) =
14.00, p < .0001, 2G = .06]. Planned contrasts between
haptic and other modalities showed a distinct haptic
disadvantage: people were indeed worse at detecting haptic
words than any other modality (all ps < .001). As expected,
there was also a main effect of display duration [F(4, 164) =
89.25, p < .0001,  2G = .26], with people becoming more
accurate with each increasing duration up to 67ms (all
ps<.001), and performance levelling out between 67ms and
100ms (p = .599). The interaction between factors was not
significant [F<1,  2G = .01].
In order to examine when the haptic disadvantage first
appears, and whether relative performance changes when
more time is given to process the word, we examined each

764

display duration separately. At 17ms, modalities differed in
performance [F(4, 164) = 3.21, p = .014,  2G = .03]: in
planned contrasts, accuracy for haptic words was
significantly worse than for all other modalities (all ps < .
03). The same pattern emerged for 33ms [F(4, 164) = 5.88,
p < .001,  2G = .07; all contrast ps < .02], 50ms [F(4, 164) =
8.51, p < .001,  2G = .09; all contrast ps < .004], and 67ms
[F(4, 164) = 8.16, p = .001,  2G = .09; all contrast ps < .
004]. By 100ms, where accuracy had begun to plateau out,
performance still varied by modality [F(4, 164) = 4.25, p
= .004,  2G = .05]; people continued to be significantly less
accurate in detecting haptic words than auditory or gustatory
words (ps < .002), and marginally less accurate than
olfactory (p = .072) and visual (p = .104) words.
When accuracy at the 17ms display duration was
compared to chance (50%) in one-sample t-tests,
performance was significantly better for auditory [t(41) =
3.70, p = .001], gustatory [t(41) = 3.86, p < .001], olfactory
[t(41) = 4.12, p < .001], and visual [t(41) = 4.22, p < .001]
modalities, but not for haptic [t(41) = 1.00, p = .321]. At
33ms, accuracy for haptic words reached a level above
chance [t(41) = 5.51 p < .001]. Since performance
consistently improved with longer display durations, we do
not report further above-chance statistics.
In summary, results show a distinct haptic disadvantage in
conceptual processing. More time is needed for the
successful processing of haptic information than any other
modality. Even when a word is displayed for only 17ms,
and people are not necessarily conscious of having read it,
they can successfully detect auditory, gustatory, olfactory
and visual modalities at a rate above chance. Haptic words,
on the other hand, need to be displayed for longer (33ms)
before they can be reliably detected.
This haptic
disadvantage, ranging between 4 and 15 percentage points,
remains consistent across increasingly longer display times
up to 100ms, where performance begins to plateau out and
the differences between modalities become less pronounced.
Since accuracy for both gustatory and olfactory modalities
closely followed that for auditory and visual, and remained
significantly better than haptic accuracy throughout, neither
the attentional perspective nor threat detection explanations
for the haptic disadvantage can adequately explain the
results. We return to this issue in the General Discussion.

Method
Identical to Experiment 1, with the following exceptions:
Participants Forty-six new participants took part. Data
from two participants were excluded prior to analysis due to
equipment malfunction during testing.
Procedure Following calibration of the unidirectional
microphone (worn as part of a headset), participants were
instructed to say “yes” as clearly as possible if the word
could be perceived through the target sense or remain silent
if it could not (constituting a “no” response). RTs were
measured from the mask onset to the registration of a voice
response. If no response was made within 1500ms, it was
considered a “no” response and the next trial was presented.

100
90
80

Accuracy (%)

70

73.1

75.1

60

Auditory
Gustatory
Haptic
Olfactory
Visual

55.7

50

76.8

40
30

29.5

20
17

33

50

67

100

Display Duration (ms)

Figure 2: Percentage of correctly-detected words
per modality and duration in Experiment 2 (go/no-go task).
Error bars are as Figure 1.

Results & Discussion
Responses due to disfluencies (e.g., lip pops, coughs) were
excluded from analysis. Responses to test words less than
200 ms or more than three standard deviations away from a
participant's mean per display duration were removed as
outliers (1.7% of data). Figure 2 shows the percentage of
correctly detected test words per modality per display time.
As in Experiment 1, the main effect of modality [F(4,
172) = 16.54, p < .0001,  2G = .03] resulted from a haptic
disadvantage: people were less accurate in detecting haptic
words than words from the other modalities (all planned
contrast ps < .001). Accuracy improved as display duration
increased [F(4, 172) = 12.74, p < .0001,  2G = .35], with
significant improvements up to 50ms (planned contrast ps
< .001) and no significant change between 50-67ms (p = .
519) or 67-100ms (p = .266). The interaction of modality
and display duration was marginal [F(16, 688) = 1.62, p = .
058,  2G = .01].

Experiment 2
Since the task in Experiment 1 required pressing “yes” and
“no” buttons in response to stimuli, participants would have
experienced haptic feedback from their fingers on every
trial. It could be argued that this feedback, and the
expectation of such feedback, could have swamped the
haptic simulators and interfered with the simultaneous
processing of haptic words (similar to e.g., Kaschak et al.,
2005, for visual motion processing), potentially contributing
to the haptic disadvantage. In this experiment, we employ a
verbal go/no-go task where participants respond with a
voice trigger rather than a button press. If the haptic
disadvantage effect is more than a mere artifact of the
button-pressing task, then we should see it replicated in the
current experiment.

765

Further investigation of the timeline of the haptic
disadvantage also replicated Experiment 1. At 17ms,
accuracy differed across modalities [F(4, 172) = 2.99, p = .
020 ,  2G = .02], with planned contrasts showing lower
accuracy for haptic words than any other modality (all ps
< .03). Haptic performance remained consistently worse
than other modality words at 33ms [F(4, 172) = 12.94, p
< .001,  2G = .08; all contrast ps < .01], and 50ms [F(4, 172)
= 6.59, p < .001,  2G = .03; all contrast ps < .001]. At 67ms
[F(4, 172) = 4.92, p = .001, 2G = .03], where overall
performance had begun to plateau, haptic accuracy was
similar to that of olfactory words (p = .23), but still worse
than the remaining modalities (all ps < .03). By 100ms,
haptic responses were again less accurate than all other
modalities [F(4, 172) = 6.35, p < .001,  2G = .03; all contrast
ps < .02].
Comparison to chance performance showed that people
were generally more conservative in the current go/no-go
task than in the previous experiment's yes/no task, which
was not unexpected given that uncertain participants tend to
withhold their responses in go/no-go tasks (thus registering
an incorrect “no” to target items), whereas, in a yes/no task,
they must press one of the two available buttons (thus
carrying a 50% chance of being correct). At 17ms, people
detected words at below-chance accuracy for all modalities:
auditory [t(43) = -2.79, p = .008], gustatory [t(43) = -2.85, p
= .007], haptic [t(43) = -5.12, p < .001], olfactory [t(43) =
-2.59, p = .013], and visual [t(43) = -3.16, p = .003]. By
33ms, performance had risen above chance for auditory
[t(43) = 6.49, p < .001], gustatory [t(43) = 8.37, p < .001],
olfactory [t(43) = 5.87, p < .001] and visual [t(43) = 4.07,
p < .001] words, but not haptic [t(43) = 1.37, p = .178],
which took until 50ms to achieve above-chance accuracy
[t(43) = 6.76, p < .001].
In short, the replication of the haptic disadvantage effect
using a voice-trigger task confirms that the results of
Experiment 1 were not due to the fact that participants
registered responses by pressing buttons, but rather are due
to differences in the conceptual processing of modalityspecific words.

neural substrate, while work on modality switching costs
shows that they appear to share the same attentional
mechanisms.
Our results further demonstrate that
perceptual and conceptual processing share a haptic
disadvantage: people need more time to detect expected
information regarding the sense of touch because of
modality-specific differences in attentional control.
Two attentional mechanisms are at play in our modality
detection task: endogenous control (where participants
consciously focus attention on the target modality) and
exogenous control (where the modality involved in
processing a word automatically and obligatorily grabs
attention). In any given block, therefore, endogenous and
exogenous control are in competition: endogenous control
attempts to focus continuously on the target modality while
exogenous control flickers between the target modality (test
items) and other modalities (filler items). We propose that
the haptic disadvantage described in this paper arises from
difficulties in haptic endogenous control: people find it
more difficult to sustain attentional focus on the haptic
modality than on any other which leaves haptic blocks more
prone to exogenous disruption and hence leads to lower
accuracy in detection of haptic stimuli.
Endogenous control of attention towards a particular
perceptual modality creates anticipatory activation in the
relevant area of the cortex (Foxe, Simpson, Ahlfors &
Saron, 2005). However, attentional control may vary in
strength. Strong endogenous control means that conscious
attention is anchored effectively in a specific modality and
that stimuli from other modalities, while grabbing
exogenous control during their processing, cannot hold onto
attention and endogenous focus quickly returns to the target
modality in preparation for the next stimulus. Weak
endogenous control, on the other hand, means that
conscious attention is not well-anchored and that stimuli
from other modalities, when they wrest exogenous control
away during their processing, are able to disrupt endogenous
focus enough that attention may not be on the target
modality when the next stimulus appears. We propose that
the haptic modality suffers from weaker endogenous control
of attention than the other perceptual modalities, which
means more time is needed to detect words successfully, and
thus the haptic modality lags behind in accuracy rates across
display times.
So how did this haptic disadvantage in endogenous
attentional come into being? Spence et al.'s (2001)
speculation that haptic processing is special because it
requires an internal attentional perspective is not borne out
by the results. Taste is detected inside the mouth, and hence
also requires body-focused attention, but gustatory
information was processed as quickly as visual and auditory
information. Turatto et al.'s (2004) suggestion of the
attentional system having evolved to stay coupled longer to
visual and auditory modalities than haptic due to an adaptive
advantage in threat detection was also not supported: taste is
of little use in detecting approaching danger but did not
share the haptic disadvantage. However, threat detection is
not the only reason that adaptive advantages may have
emerged for certain modalities, and we would speculate that
Turatto et al.'s account may be partially correct. Being able

General Discussion
In this paper, we have demonstrated that a phenomenon
observed during perception – the haptic disadvantage – also
emerges during conceptual processing. Results showed that
the processing of modality-specific information is rapid and
automatic, with above-chance performance after just 17ms
exposure in Experiment 1 and 33ms in Experiment 2.
Haptic information, however, is the hardest to process.
Even with extra time to process the word, people are less
accurate at detecting properties that pertain to the sense of
touch than to hearing, taste, smell or vision, and this effect
emerged even though the strength on the given modality and
the lexical decision times for each word were equal across
modalities. These findings support the assertions of
embodied theories that the conceptual system utilises the
perceptual system for the purposes of representation.
Neuroimaging research has shown that perceptual and
conceptual processing share a common, modality-specific

766

to sustain attentional focus on a particular sensory modality
(i.e., endogenous control) is also useful in hunting, where
efficacious looking, listening and even smelling for traces of
prey could afford an adaptive advantage.
Similarly,
contaminant detection (visual, olfactory and gustatory
information) and mate selection (visual and olfactory
information) will be most successful if attention can be
deliberately and consciously turned towards these cues. In
other words, the attentional system may have evolved to
stay coupled at length to visual, auditory, olfactory and
gustatory modalities because of their usefulness in detecting
stimuli that affect the ability to survive and reproduce,
whereas sustained attentional focus on the haptic modality
brought no such adaptive advantage. If such attentional
mechanisms evolved as part of our perceptual systems, and
these same attentional and perceptual systems are utilised
during conceptual processing and language comprehension,
then it should come as no surprise that modality-specific
differences, such as the haptic disadvantage, emerge with
linguistic as well as sensory stimuli.

regions. Journal of Neuroscience, 26, 4917-4921.
Goldstone, R., & Barsalou, L.W. (1998). Reuniting
perception and conception. Cognition, 65, 231-262.
González, J., Barros-Loscertales, A., Pulvermüller, F.,
Meseguer, V., Sanjuán, A., Belloch, V., & Ávila, C.
(2006). Reading cinnamon activates olfactory brain
regions. Neuroimage, 32, 906-912.
Johnson-Laird, P. N. (1983). Mental models. Cambridge,
MA: Harvard University Press.
Loftus, G. R., & Masson, M. E. J. (1994). Using confidence
intervals in within-subject designs. Psychonomic Bulletin
& Review, 1, 476-490.
Lynott, D. & Connell, L. (2009). Modality exclusivity
norms for 423 object properties. Behavior Research
Methods, 41, 558-564.
MacWhinney, B. (1999). The emergence of language from
embodiment. In B. MacWhinney (Ed.), The emergence of
language. Mahwah, NJ: Lawrence Erlbaum.
Martin, M. G. F. (1995). Bodily awareness: A sense of
ownership. In J. L. Bermudez, A. Marcel, & N. Eilan
(Eds.), The body and the self. Cambridge, MA:MIT Press.
Newell, A. & Simon, H. A. (1972). Human Problem
Solving. Englewood Cliffs, NJ: Prentice-Hall.
Newman, S. D., Klatzky, R. L., Lederman, S. J., Just, M. A.
(2005). Imagining material versus geometric properties
of objects: An fMRI study. Cognitive Brain Research, 23,
235-246.
Olejnik, S., & Algina, J. (2003). Generalized eta and omega
squared statistics: Measures of effect size for some
common research designs. Psychological Methods, 8,
434-447.
Pecher, D., Zeelenberg, R., & Barsalou, L.W. (2003).
Verifying properties from different modalities for
concepts produces switching costs. Psychological
Science, 14, 119-124.
Pecher, D., & Zwaan, R. A. (2005). Introduction to
grounding cognition. In D. Pecher & R. A. Zwaan (Eds.),
Grounding cognition: the role of perception and action in
memory, language, and thinking. Cambridge: CUP.
Pulvermuller, F. (2005) Brain mechanisms linking language
and action. Nature Reviews Neuroscience, 6, 576-582.
Simmons, W.K., Ramjee, V., Beauchamp, M.S., McRae, K.,
Martin, A., & Barsalou, L.W. (2007). A common neural
substrate for perceiving and knowing about color.
Neuropsychologia, 45, 2802-2810.
Spence, C., Nicholls, M. E. R., & Driver, J. (2000). The cost
of expecting events in the wrong sensory modality.
Perception & Psychophysics, 63, 330-336.
Tulving, E. (1972). Episodic and semantic memory. In E.
Tulving & W. Donaldson (Eds.), Organization and
memory. New York: Academic Press.
Turatto, M., Galfano, G., Bridgeman, B., & Umiltà, C.
(2004). Space-independent modality-driven attentional
capture in auditory, tactile and visual systems.
Experimental Brain Research, 155, 301-310.
van Dantzig, S., Pecher, D., Zeelenberg, R., & Barsalou,
L.W. (2008). Perceptual processing affects conceptual
processing. Cognitive Science, 32, 579-5.

Acknowledgments
This work was funded by grant RES-000-22-2407 from the
UK Economic and Social Research Council to the first
author.

References
Anderson, M. L. (2003). Embodied Cognition: A field
guide. Artificial Intelligence, 149, 91-130.
Balota, D. A., Yap, M. J., Cortese, M.J., Hutchison, K. A.,
Kessler, B., Loftis, B., Neely, J. H., Nelson, D. L.,
Simpson, G. B., & Treiman, R. (2007). The English
Lexicon Project. Behavior Research Methods, 39, 445459.
Barsalou, L. W. (1999). Perceptual symbol systems.
Behavioral and Brain Sciences 22, 577–660.
Barsalou, L. W. (2008). Grounded cognition. Annual
Review of Psychology, 59, 617–645.
Chrisley, R. (2003). Embodied artificial intelligence.
Artificial Intelligence, 149, 131-150.
Collins, A. M., & Quillian, M. R. (1969). Retrieval time
from semantic memory. Journal of Verbal Learning and
Verbal Behaviour, 8, 240-248.
Dijksterhuis, A., & Aarts, H. (2003). On wildebeests and
humans: The preferential detection of negative stimuli.
Psychological Science, 14, 14–18.
Foxe, J. J., Simpson, G. V., Ahlfors, S. P., & Saron, C. D.
(2005). Biasing the brain’s attentional set: I. Cue driven
deployments of intersensory selective attention.
Experimental Brain Research, 166, 370–392.
Gibbs, R. W. (2003). Embodied experience and linguistic
meaning. Brain and Language, 84, 1-15.
Glenberg, A. M. (1997). What memory is for. Behavioral
and Brain Sciences, 20, 1–55.
Glenberg, A. M., & Kaschak, M. P. (2002). Grounding
language in action. Psychonomic Bulletin & Review, 9,
558–565.
Goldberg, R. F., Perfetti, C. A., & Schneider, W. (2006).
Perceptual knowledge retrieval activates sensory brain

767

