UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Semantic Representations with Hidden Markov Topics Models

Permalink
https://escholarship.org/uc/item/17s6g6dm

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)

Authors
Andrews, Mark
Vigliocco, Gabriella

Publication Date
2009-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Learning Semantic Representations with Hidden Markov Topics
Models
Mark Andrews (m.andrews@ucl.ac.uk)
Gabriella Vigliocco (g.vigliocco@ucl.ac.uk)
Cognitive, Perceptual and Brain Sciences
University College London,
26 Bedford Way
London, WC1H 0AP
United Kingdom
Abstract

information is unavailable in bag-of-words models and
consequently the extent to which they can extract semantic information from text, or adequately model human semantic learning, is limited.
In this paper, we describe a distributional model that
goes beyond the bag-of-words paradigm. This model
is a natural extension to the current state of the art
in probabilistic bag-of-words models, namely the Topics
model described in Griffiths et al. (2007) and elsewhere.
The model we propose is a seamless continuation of the
Topics model, preserving its strengths — its thoroughly
unsupervised learning, its hierarchical Bayesian nature
— while extending its scope to incorporate more finegrained sequential and syntactic data.

In this paper, we describe a model that learns semantic representations from the distributional statistics of
language. This model, however, goes beyond the common bag-of-words paradigm, and infers semantic representations by taking into account the inherent sequential
nature of linguistic data. The model we describe, and
which we refer to as a Hidden Markov Topics model is
a natural extension of the current state of the art in
Bayesian bag-of-words models, i.e. the Topics model of
Griffiths, Steyvers, and Tenenbaum (2007), preserving
its strengths while extending its scope to incorporate
more fine-grained linguistic information.

Introduction
How word meanings are learned is a foundational problem in the study of human language use. Within cognitive science, a promising recent approach to this problem
has been the study of how the meanings of words can
be learned from their statistical distribution across the
language. This approach is motivated by the so-called
distributional hypothesis, originally due to Harris (1954)
and Firth (1957), which proposes that the meaning of
a word is given by the linguistic contexts in which it
occurs. Numerous large-scale computational implementations of this approach — including, for example, the
work of Schütze (1992), the HAL model (Lund, Burgess,
& Atchley, 1995), the LSA model (Landauer & Dumais,
1997) and, most recently, the Topics model (Griffiths
et al., 2007) — have succesfully demonstrated that the
meanings of words can, at least in part, be derived from
their statistical distribution in language.
Important as these computational models have been,
one of their widely shared practices has been to treat the
linguistic contexts in which a word occurs as unordered
sets of words. In other words, the linguistic context of
any given word is defined by which words co-occur with
it and with what frequency, but it disregards all finegrained sequential and syntactic information. By disregarding these types of data, these so-called bag-of-words
models drastically restrict the information from which
word meanings can be learned. All languages have strong
syntactic-semantic correlations. The sequential order in
which words occur, the argument structure and general
syntactic relationships within sentences, all provide vital
information about the possible meaning of words. This

The Topics Model
The standard Topics model as described in Griffiths and
Steyvers (2002, 2003); Griffiths et al. (2007) is a probabilistic generative model for texts, and is based on the
Latent Dirichlet Allocation (LDA) model of Blei, Ng,
and Jordan (2003). It stipulates that each word in a corpus of texts is drawn from one of K latent distributions
φ1 . . . φk . . . φK $ φ, with each φk being a probability
distribution over the V word-types in a fixed vocabulary. These distributions are the so-called topics that
give the model its name. Some examples, learned by a
Topics model described in Andrews, Vigliocco, and Vinson (In Press), are given in the table below (each column
gives the 7 most probable word types in each topic).
theatre
stage
arts
play
dance
opera
cast

music
band
rock
song
record
pop
dance

league
cup
season
team
game
match
division

prison
years
sentence
jail
home
prisoner
serving

air
aircraft
flying
flight
plane
airport
pilot

As is evident from this table, each topic is a cluster of
related terms that corresponds to a coherent semantic
theme, or subject-matter. As such, the topic distributions correspond to the semantic knowledge learned by
the model, and the semantic representation of each word
in the vocabulary is given by a distribution over them.

154

generated according to an elementary language model
— specifically a mixture of unigram distributions —
which are then hierarchically coupled with one another.
From this, it is evident that the standard model can be
extended by simply changing the elementary language
model on which it is based. There are multiple possible language models that could be used in this respect.
One possibility is to use a bigram language model as
described in Wallach (2006). Another possibility is to
use a language model based on a full phrase-structure
grammar as described in Boyd-Graber and Blei (2009).
However, in that work, the syntactic structure underlying the sentences in the texts is assumed to be known
in advance, and is provided by syntactically tagged corpus. In what follows, we describe a Topics models whose
elementary language model is a Hidden Markov model
(HMM). We refer to this as the Hidden Markov Topics
model (HMTM)1 .

wji
φ
xji
1 ≤ i ≤ nj

β
πj

1≤j≤J

α

Figure 1: A Bayesian network diagram of the standard
Topics model described in Griffiths et al. (2007) and elsewhere. Details are provided in the main text. Note that
β denotes the parameters of a V -dimensional Dirichlet
distribution, from which each of K topic distributions
are sampled.

Hidden Markov Topics model
In the HMTM, just as in the standard Topics model, each
wji is drawn from one of K topics, the identity of which
is determined by the latent variable xji . However, rather
than sampling each xji independently from a probability
distribution πj , as in the standard model, these latent
variabls are generated by a Markov chain that is specific
to text j. By direct analogy with the standard model,
across the texts, the parameters of these Markov chains
are drawn from a common set of Dirichlet distributions.
As such, the HMTM is a hierarchical coupling of HMMs,
defining a continuum of Hidden Markov models, all sharing the same state to output mapping.
In the HMTM, the likelihood of the corpus is

To describe the Topics model more precisely, let us
assume we have a corpus of J texts w1 . . . wj . . . wJ ,
where the jth text wj is a sequence of nj words, i.e.
wj1 . . . wji . . . wjnj . Each word, in each text, is assumed
to be sampled from one of the model’s K topics. Which
one of these topics is chosen is determined by the value
of a latent variable xji that corresponds to each word
wji . This variable takes on one of K discrete values,
and is determined by sampling from a probability distribution πj , which is specific to text j. As such, we can
see that each text wj is assumed to be a set of nj independent samples from a mixture model. This mixture
model is specific to text j, as the mixing distribution
is determined by πj . However, across texts, all mixing
distributions are drawn from a common Dirichlet distribution, with parameters α. Given known values for φ
and α, the likelihood of the entire corpus is given by
P(w1:J |φ, α) =

J
Y

P(w1:J |φ, α, γ) =

nj

Y X





nj
XY


{xj } i=1

P(wji |xji , φ)

nj
Y


P(xji |xji−1 , θ j )P(xj1 |π j ) .

i=2

(2)

Z

Here, θj and πj are the parameters of the Markov
chain of latent-states in text j. The θj is the K × K
state transition matrix (i.e. the kth row of θj gives the
probability of transitioning to each of the K states, given
that the current state is k), and πj is the initial distribution for the K states. The distribution over the πj and

dπj P(πj |α)


dπj dθj P(πj , θj |α, γ)

j=1

j=1



J Z
Y

(1)

P(wji |xji , φ)P(xji |πj ) ,

i=1 {xji }

In this, we see that the Dirichlet distribution P(πj |α)
introduces a hierarchical coupling of all the mixing distributions. As such, the standard Topics model is a hierarchical coupling of mixture models, effectively defining
a continuum of mixture models, all sharing the same K
component topics.
The standard Topics model is thus an example of a
hierarchical language model. It assumes that each text is

1
The HMTM bears some resemblance to a model described in Griffiths et al. (2007) that couples a Hidden
Markov model with a standard Topics model. There are,
however, substantial differences between these models. In
particular, the HMTM is designed to learn semantic representations by directly availing of the sequential information
in text. By contrast, the HMM based model described in
Griffiths et al. (2007) learns semantic representations using
a standard topics model, while the HMM is used to learn
syntactic categories.

155

θj , i.e. P(πj , θj |α, γ), is a set of independent Dirichlet distributions, where α are the parameters for the
Dirichlet distibution over πj , and γ 1 . . . γ K $ γ are the
parameters for the distribution over each of the K rows
of θj .

On convergence of the Gibbs sampler, we will
obtain samples from the joint posterior distribution
P(x1:J , α, β, γ|w1:J ). From this, other variables of interest can be obtained. For example, it is desirable
to know the likely values of the topic distributions
φ1 . . . φk . . . φK . Given known values for x1:J and β,
the posterior distribution over φ is simply a product
of Dirichlet distribution, from which samples are easily drawn and averaged. Further details of this Gibbs
sampler are provided in the Appendix.

β

φ

Demonstration
wj1

wj2

wji

wjnj

xj1

xj2

xji

xjnj

Here, we demonstrate the operation of the HMTM on
a toy problem. In this problem, we generate a data-set
from a HMTM with known values for φ, α and γ. We
can then train another HMTM, whose parameters are
unknown, with this training data-set. Using the Gibbs
sampler, we can draw samples from the posterior over
φ, α, β and γ, and then compare these samples to the
true parameter values that generated the training data.
In the example we present here, we use a “vocabulary”
of V = 25 symbols, and a set of K = 10 “topics”. As
is common practice in demonstrations of related models,
the “topics” we use in this example can be visualized
using the grids shown below.

1 ≤ i ≤ nj
πj

θj
1≤j≤J

α

γ

Figure 2: A Bayesian network diagram for the Hidden
Markov Topic model. Details are provided in the main
text.

Learning and Inference
From this description of the HMTM, as well as from
its Bayesian network diagram given in Figure 2, we can
see that the HMTM has four sets of parameters whose
values must be inferred from a training corpus of texts.
These are the K topic distributions, i.e. φ, the Dirichlet
parameters for the latent-variable Markov chains, i.e. α
and γ, and the Dirichlet parameters from which the topic
distributions are drawn, i.e. β 2 . The posterior over φ,
α, β and γ given a set of J texts w1:J is

Each grid, although shown as a 5 × 5 square, is just a
25 dimensional array, with 5 high values (darker) and 20
low values (lighter). Each of these arrays corresponds to
one of the topics distributions in our toy problem, i.e.
each one places the majority of its probability mass on
a different set of 5 symbols.
The γ 1 . . . γ K $ γ parameters in the HMTM are a
set of K K-dimensional Dirichlet parameters. Each γ k
is an array of K non-negative real numbers. A common
reparameterization of Dirichlet parameters, such as γ k
is as smk , where s is the sum of γ k and mk is the γ k
divided by s. For each set of K Dirichlet parameters, we
used an s = 3.0. The K arrays m1 . . . mK are depicted
below on the left.

P(φ, α, β, γ|w1:J ) ∝ P(w1:J |φ, α, γ)P(φ|β)P(α, β, γ),
(3)
where the likelihood term P(w1:J |φ, α, γ) is given by
Equation 2. This distribution is intractable (as is even
the likelihood term itself), and as such it is necessary
to use Markov Chain Monte Carlo (MCMC) methods to
sample from it.
There are different options available for MCMC sampling. The method we employ is a Gibbs sampler that
draws samples from the posterior over α, β, γ and x1:J .
Here, x1:J are the sequences of latent-variables for each
of the J texts, i.e. x1 , x2 . . . xJ , with xj = xj1 . . . xjnj .
This Gibbs sampler has the useful property of integrating
over φ, π 1:J and θ 1:J , which entails both computational
efficiency and faster convergence.
2
The β parameters can be seen as a prior over φ, but a
prior that will be inferred from the data rather than simply
assumed.

From these Dirichlet parameters, we may generate arbitrarily many sets of state-transition parameters for a 10

156

Figure 3: Averages of samples from the posterior distribution over the topic distributions (left) and locations of the
γ parameters (right). Shown on the top row are averages, over 10 draws, drawn after 20 iterations. Shown on the
lower row are averages, again over 10 draws, taken after 100 iterations. Compare with the true parameters shown
on the previous page.
state HMM. As an example, we show two such sets on
the right. As is evident, these parameters retain characteristics of the patterns found in the orginal Dirichlet
parameters. We can see that, on average, the state transition dynamics leads a given state to map to either the
state before it, or the state after it. For example, we can
see that, on average, state 2 maps to state 1 or state 3,
state 3 maps to state 2 or state 4, and so on. While this
average dynamical behavior is simple, it is not trivial,
and does not lead to fixed point or periodic trajectories.
Note also that the small differences in the transition dynamics can lead to quite distinct dynamical behaviors in
their respective HMMs.

samples drawn from the posterior over φ and m1 . . . mk ,
i.e the location parameters of γ 1 . . . γ k . On the top row
of Figure 3, we show averages of samples drawn after 20
iterations of the sampler. On the lower row, we show averages of drawn after 100 iterations. In both cases, these
averages are over 10 samples taken two iterations apart
in time. To the left in each case, are the inferred topics.
To the right are the inferred locations of the Dirichlet parameters. These inferred parameters can be compared to
the true parameters on the previous page. By doing so,
it is clear that even after 20 iterations of the sampler,
patterns in the topic distributions have been discovered.
After 100 iterations, the inferred parameters are almost
identical to the originals.
Although not shown, the Gibbs sampler also succesfully draws samples from the posterior over α, β 3 and
the scale parameter s for γ. In addition, we may use
draws from the posterior to estimate, using the harmonic
mean method, the marginal likelihood of the model under a range of different numbers of topic distributions.
Although, the harmonic mean method is not highly recommended, we have found that in practice it consistently
leads to an estimate of the correct number of topics.

On the basis of these φ and θ, and using an array
of K ones as the parameters α, we generated J = 50
training “texts” as follows. For each text j, we drew a
set of initial conditions and transition parameters for a
HMM from α and γ, respectively. We then iterated the
HMM for nj = 100 steps, to obtain a state trajectory
xj1 . . . xjnj . On the basis of the value of each xji , we
chose the appropriate topic (i.e. if xji = k we chose φk )
and drew an observation wji from it. The training thus
comprised a set of J symbol sequences, with each symbol
taken a value from the set {1 . . . 25}.

Learning Topics from Text

Using this as training data, we trained another HMTM
whose parameters were unknown. As described earlier,
the Gibbs sampler draws samples from the posterior distributon over x1:J , α, β and γ. From these samples,
we may also draw samples from the posterior over the
topics. In Figure 3, we graphically present some results
of these simulations. Show in this figure are averages of

In this final section, we present some topics learned by
a HMTM trained on a corpus of natural language. The
corpus used was a sub-sample from the British National
3
For the case of β, we used a symmetric Dirichlet distribution.

157

HMTM Topics

beer
guinness
alcohol
ale
whisky
spirits
wine
cider
pint
lager

sheep
cattle
meat
livestock
dairy
beef
pigs
animal
cow
pig

sugar
fruit
butter
bread
chocolate
milk
cream
water
lemon
egg

aircraft
plane
jet
airline
squadron
helicopter
fighter
hercules
airbus
falcon

film
movie
series
tv
story
television
soap
movies
drama
episode

say
know
talk
think
feel
understand
believe
speak
ask
explain

ship
boat
ferry
vessel
ships
navy
shipping
lifeboat
fleet
coastguard

Standard Topics

pub
drink
guinness
beer
drinking
wine
bar
alcohol
brewery
whisky

farm
agriculture
food
farming
sheep
agricultural
cattle
ministry
crop
pigs

fruit
add
fresh
butter
cooking
minutes
hot
food
bread
chicken

air
aircraft
flight
plane
airport
flying
pilot
fly
jet
airline

film
star
hollywood
movie
screen
stars
director
actress
actor
role

want
think
like
people
moment
happen
wanted
worried
believe
exactly

boat
island
sea
ship
crew
ferry
sailing
yacht
shipping
board

Table 1: Examples of topics learned by a HMTM (top row) that was trained on a set of documents taken from the
BNC. On the lower row, we show topics from a standard topics model also trained on a set of documents from the
BNC.
Corpus (BNC)4 . The BNC is annotated with the structural properties of a text such as sectioning and subsectioning information. The latter type of annotation
facilitates the processing of the corpus. To extract texts
from the BNC we extracted contiguous blocks that were
labeled as high-level sections, roughly corresponding to
individual articles, letters or chapters. These sections
varied in size from tens to thousands of words, and from
these we chose only those texts that were approximately
150-250 words in length. This length is typical of, for
example, a short newspaper article. Following these criteria, we then sampled 2500 individual texts to be used
for training. Of all the word types that occurred within
this subset of texts, we excluded words that occurred
less 5 times overall, and replaced their occurrence with
a marker symbol. This restriction resulted in a total of
5182 unique words.
We trained a HMTM with K = 120 topics using this
corpus. After a burn-in period of 1000 iterations, we
drew 50 samples from the posterior over the latent trajectories and β, with each sample being 10 iterations
apart. We used these to draw samples from the pos-

terior over the topics, which are then averaged, as described earlier. In the upper part of Table 1, we present
seven averaged topics from the HMTM simulation. For
the purposes of comparison, in the lower part Table 1
we present seven averaged topics taken from a standard
topics model. This standard topics model was trained
on a larger corpus, and is described in detail in Andrews
et al. (In Press). The topics in the standard model were
chosen by finding the topics that are the closest matching
to the HMTM topics we chose.
The side-by-side comparison provides an appreciation
for how the topics in a HMTM differ from the standard
model. In the HMTM, the topics are more refined in the
semantics, referring to more specific categories of things
or events. For example, we see that the first topic to the
left in the HMTM refers to alcholic beverages, specifically those associated with a (British) pub. By contrast,
the corresponding topic from the standard model is less
specifically about beverages and refers more generally
to things of, or relating to, pubs. In the next example,
we see that the topic from the HMTM refers to farm
animals. By contrast, the corresponding topic from the
standard model is less specifically about farm animals
and more related to agriculture in general. In all of the
examples shown, a similar pattern of results holds.

4

The BNC is a 100 million word corpus of contemporary
written and spoken English in the British Isles. According
to its publishers, the texts that make up the written component include “extracts from regional and national newspapers, specialist periodicals and journals for all ages and
interests, academic books and popular fiction, published and
unpublished letters and memoranda, school and university
essays”.

Conclusion
The aim of this paper is to demonstrate how to extend
the standard Topics model so as to learn more fine-

158

grained semantic representations from the statistics of
langauge. This is done by using the sequential statistical information of language. As mentioned, the sequential order in which words occur provide vital information about the possible meaning of words. This information is not available in the standard Topics model,
nor in most of its counterparts. In the examples shown,
we have seen that more refined semantic representations
can be learned when sequential information is taken into
account.
This general usefulness of sequential information can
be understood by way of a, albeit contrived, simple example. Words like horse, cow, mule are likely to occur
as subjects of verbs like eat, chew, while words like grass,
hay, grain are likely to occur as their objects. A model
that learns topics by taking into account this sequential
information may learn that words like horse, cow, and
mule etc., form a coherent topic. Likewise, such a model
may infer other topics based on words like eat, chew,
etc., or words like grass, hay, grain, etc. By contrast,
the standard Topics model, based on the assumptions
that the sequential information in a text is irrelevant,
is likely to conflate these separate topics into one single
topic referring to, for example, farms or farming.

variables, the conditional posterior over the share of the
probability mass between any two elements of mk is also
log-concave and can be sampled by ARS. As such, the
Gibbs sequentially samples from each latent variable xji ,
each scale parameter of the Dirichlet parameters given
by α, β, γ, and also from the share of probability mass
between every pair of elements of each location parameters of the Dirichlet parameters.

References
Andrews, M., Vigliocco, G., & Vinson, D. P. (In Press).
Integrating experiential and distributional data to
learn semantic representations. Psychological Review.
Blei, D., Ng, A., & Jordan, M. (2003). Latent dirichlet
allocation. Journal of Machine Learning Research,
3, 993-1022.
Boyd-Graber, J., & Blei, D. (2009). Syntactic topic
models. In Neural information processing systems.
Firth, J. R. (1957). A synopsis of linguistic theory 19301955. In Studies in Linguistic Analysis (special volume of the Philological Society, Oxford) (p. 1-32).
Oxford: Blackwell.
Gilks, W. R., & Wild, P. (1992). Adaptive rejection
sampling for gibbs sampling. Applied Statistics,
41 (2), 337-348.
Griffiths, T., & Steyvers, M. (2002). A probabilistic
approach to semantic representation. In Proceedings of the 24th annual conference of the cognitive
science society.
Griffiths, T., & Steyvers, M. (2003). Prediction and semantic association. In S. T. S. Becker & K. Obermayer (Eds.), Advances in neural information processing systems 15 (pp. 11–18). Cambridge, MA:
MIT Press.
Griffiths, T., Steyvers, M., & Tenenbaum, J. (2007).
Topics in semantic representation. Psychological
Review, 114, 211-244.
Harris, Z. (1954). Distributional structure. Word, 10 (23), 775-793.
Landauer, T., & Dumais, S. (1997). A solutions to
Plato’s problem: The Latent Semantic Analyis
theory of acquistion, induction and representation
of knowledge. Psychological Review, 104, 211-240.
Lund, K., Burgess, C., & Atchley, R. A. (1995). Semantic and associative priming in high-dimensional semantic space. In Proceedings of the seventeeth annual conference of the cognitive science society.
Schütze, H. (1992). Dimensions of meaning. IEEE Computer Society Press.
Wallach, H. (2006). Topic modeling: Beyond bag-ofwords. In Proceedings of the 23rd international
conference on machine learning (p. 977-984).

Appendix
The Gibbs sampler for the HMTM model draws samples
from P(x1:J , α, β, γ|w1:J ). It does so iteratively sampling from a given latent variable xji , assuming values
from all other latent variables, and for α, β, γ. The
conditional distribution over xji is given by
P(xji |x−[ji] , w1:J , α, β, γ)
(A.1)
∝ P(wji |xji , x−[ji] , w−[ji] , β)P(xji |x−[ji] , α, γ),
where we denote the set of latent variables excluding xji
by x−[ji] , and denote the set of observables excluding
wji by w−[ji] . Superficially, this conditional distribution appears identical to the conditional distributions in
the Gibbs sampler for the standard Topics model, as descibed in Griffiths and Steyvers (2002, 2003); Griffiths
et al. (2007). However, due to the non-independence in
the latent trajectory that results from the Markov dynamics, the term P(xji |x−[ji] , α, γ) must be calculated
as a ratio of Polya distributions, i.e.
P(xji |x−[ji] , α, γ) =

P(x1:J |α, γ)
.
P(x−[ji] , α, γ)

(A.2)

The Dirichlet parameters α, β, γ may also be sampled
by Gibbs sampling. For example, each γ k is reparameterized as smk (as described in the main text). Assuming
known values for all variables, the conditional posterior
distribution of s is log-concave and can be sampled using
Adaptive Rejection sampling (ARS), see Gilks and Wild
(1992). Likewise, assuming known values for all other

159

