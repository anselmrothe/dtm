UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Adaptive Emotion Reading Model

Permalink
https://escholarship.org/uc/item/6bf49133

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)

Authors
Bosse, Tibor
Memon, Zulfiquar A.

Publication Date
2009-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

An Adaptive Emotion Reading Model
Tibor Bosse, Zulfiqar A. Memon, Jan Treur ({tbosse, zamemon, treur}@few.vu.nl)
Vrije Universiteit Amsterdam, Department of Artificial Intelligence
De Boelelaan 1081, 1081 HV Amsterdam, the Netherlands

Abstract
This paper focuses on how capabilities to interpret somebody
else’s emotions can be modelled in an adaptive manner. First
a cognitive model to generate emotional states is described.
This model involves a recursive body loop: a converging
positive feedback loop based on reciprocal causation between
body states and emotions felt. By this model emotion reading
can be modelled taking into account the Simulation Theory
perspective on mindreading as known from the literature,
which assumes that the own emotions are involved in reading
somebody else’s emotions. It is shown how the model was
extended to an adaptive model within which a direct
connection between a stimulus and the emotion recognition is
created, which implies that in principle the observed emotion
can be recognised just before it is felt.
Keywords: Theory of Mind; mindreading; adaptive emotion
reading; emotion generation; modeling; simulation.

Introduction
From an evolutionary perspective, mindreading (or having a
Theory of Mind) in humans and some other kinds of
animals has developed for a number of aspects, for example,
intention, attention, emotion, knowing (e.g., Baron-Cohen,
1995; Bogdan, 1997; Dennett, 1987; Goldman, 2006;
Goldman & Spirada, 2004; Malle, Moses & Baldwin, 2001).
Two philosophical perspectives on having a Theory of Mind
are Simulation Theory and Theory Theory (Goldman,
2006). In the first perspective it is assumed that mindreading
takes place by using the facilities involving the own
cognitive states that are counterparts of the cognitive states
attributed to the other person. For example, the state of
feeling pain oneself is used in the process to determine
whether the other person has pain. The second perspective is
based on reasoning using knowledge about relationships
between cognitive states and observed behaviour. An
example of such a pattern is: ‘I hear that the person says
‘ouch!’. Having pain causes saying ‘ouch!’. Therefore the
person has pain’. The current paper addresses emotion
reading from a Simulation Theory perspective: an approach
is adopted that involves a person’s own emotions in the
process of reading the human’s emotions.
The concept of ‘body loop’ as put forward by Damasio
(1999) and formalised by Bosse, Jonker and Treur (2008) is
used as a point of departure. This perspective distinguishes
the (bodily) emotional response to a stimulus from feeling
the emotion (sometimes called the emotional feeling), which
is caused by sensing the own bodily response. Secondly, an
extension of this idea is adopted by assuming that the body
loop is not processed once, but in a recursive manner: a
converging positive feedback loop based on reciprocal

causation between emotional state (with gradually more
feeling) and body state (with gradually stronger expression).
This cycle is triggered by the stimulus and after an
indefinite number of rounds ends up in equilibrium for both
states. In this way a model is obtained in which the process
of generating an emotional feeling is not only assumed to be
carried by neurological structures, but equally well by body
states, in a reciprocal causation and convergence process.
By Bosse, Memon, and Treur (2008) and Memon and Treur
(2008), it was shown that such a recursive body loop model
for emotions is an appropriate basis to obtain an emotion
reading model from the Simulation Theory perspective. In
the current paper it is shown how the model based on a
recursive body loop can be extended to obtain an adaptive
model for emotion reading. The adaptation creates a
shortcut connection from the stimulus (observed facial
expression) to the imputed emotion, bypassing the own
emotional states. Some simulation results are discussed, and
some formally specified dynamic properties of adaptive and
non-adaptive emotion reading are shown, and it is discussed
how they were verified against simulation traces.

An Emotion Generation Model
The model to generate emotional states for a given stimulus
adopts from Damasio (1999) the idea of a ‘body loop’ and
‘as if body loop’, but extends this by making these loops
recursive. According to the original idea, emotion
generation via a body loop roughly proceeds according to
the following causal chain:
sensing a stimulus → sensory representation of stimulus →
(preparation for) bodily response → sensing the bodily response →
sensory representation of the bodily response → feeling the emotion

As a variation, an ‘as if body loop’ uses a causal relation
preparation for bodily response →
sensory representation of the bodily response

as a shortcut in the causal chain. In the model used here an
essential addition is that the body loop (or as if body loop)
is extended to a recursive (as if) body loop by assuming that
the preparation of the bodily response is also affected by the
state of feeling the emotion (also called emotional feeling):
feeling the emotion → preparation for bodily response

as an additional causal relation. This idea is supported by
the following quote from Damasio (2003, p. 91-92):
‘In other words, feelings are not a passive perception or a flash in time,
especially not in the case of feelings of joy and sorrow. For a while after an
occasion of such feelings begins – for seconds or for minutes – there is a
dynamic engagement of the body, almost certainly in a repeated fashion,
and a subsequent dynamic variation of the perception. We perceive a series
of transitions. We sense an interplay, a give and take.’

1006

Both the bodily response and the emotional feeling are
assigned a level or gradation, expressed by a number, which
is assumed dynamic. The causal cycle is modelled as a
positive feedback loop, triggered by the stimulus and
converging to a certain level of emotional feeling and body
state. Here in each round of the cycle the next body state has
a level that is affected by both the level of the stimulus and
of the emotional feeling state, and the next level of the
emotional feeling is based on the level of the body state. In
the more detailed model described below, the combined
effect of the levels of the stimulus and the emotional state
on the body state is modelled as a weighted sum (with equal
weights 0.5 in this case). This implies a pattern of gradual
generation (and extinction) of an emotion upon a stimulus.
In the description of the detailed cognitive model the
temporal relation a →
→ b denotes that when a state property
a occurs, then after a certain time delay (which for each
relation instance can be specified as any positive real
number), state property b will occur. In this language
(called LEADSTO) both logical and numerical calculations
can be specified, and a dedicated software environment is
available to support specification and simulation; for details
see (Bosse, Jonker, Meij, & Treur, 2007). The specification
(both informally and formally) of the model for emotion
generation based on a recursive body loop is as follows.

LP8 From sensory representation of body state to emotion
If a sensory representation for facial expression f with level v occurs,
then emotion e is felt with level v.
srs(f, v) →
→ emotion(e, v)
LP9 Imputation
If a certain emotion e is felt, with level ≥ th and a sensory representation for
s occurs, then emotion e will imputed to s. Here, th is a (constant) threshold
for imputation of emotion. In the simulations shown, th is assumed 0.95.
srs(s) & emotion(e, v) & v≥th →
→ imputation(s, e)

In the imputation state, the experienced emotion e is
related to the stimulus s, which triggers the emotion
generation process. Note that this state makes sense in
general, for any type of stimulus s, as usually a person does
not only feel an emotion, but also has an awareness of what
causes an emotion; what by Damasio (1999) is called a state
of conscious feeling also plays this role. This state that
relates an emotion felt to any triggering stimulus will play
an important role in the emotion reading process.

LP1 Sensing a stimulus
If stimulus s occurs then a sensor state for s will occur.
s→
→ sensor_state(s)
LP2 Generating a sensory representation of a stimulus
If a sensor state for s occurs, then a sensory representation for s will occur.
sensor_state(s) →
→ srs(s)
1

LP3 From sensory representation and emotion to preparation
If a sensory representation for s occurs and emotion e has level v,
then preparation state for facial expression f will occur with level (1+v)/2.
srs(s) & emotion(e, v) →
→ preparation_state(f, (1+v)/2)
If no sensory representation for s occurs and emotion e has level v,
then preparation state for facial expression f will occur with level v/2.
not srs(s) & emotion(e, v) →
→ preparation_state(f, v/2)
LP4 From preparation to body modification
If preparation state for facial expression f occurs with level v,
then the face is modified to express f with level v.
preparation_state(f, v) →
→ effector_state(f, v)
LP5 From body modification to modified body
If the face is modified to express f with level v,
then the face will have expression f with level v.
effector_state(f, v) →
→ own_face(f, v)
LP6 Sensing a body state
If facial expression f with level v occurs,
then this facial expression is sensed.
own_face(f, v) →
→ sensor_state(f, v)
LP7 Generating a sensory representation of a body state
If facial expression f of level v is sensed, then a sensory representation for
facial expression f with level v will occur.
sensor_state(f, v) →
→ srs(f, v)
1

Here, it is assumed that the relative effects of both antecedents are the
same. However, the formula (1+v)/2 can as well be replaced by the more
generic formula w1+w2*v with weights w1 and w2. Such an extension
also enables the modeller to distinguish different types of emotions (e.g.,
fear may develop faster than happiness).

Figure 1: Example simulation of emotion generation

1007

Instead of a recursive body loop, a variation of the model
as described can be made to model a ‘recursive as if body
loop’; see (Damasio, 1999) for evidence for a causal
relation between preparation state and sensory
representation. This can be incorporated by replacing the
temporal relations LP4, LP5. LP6, LP7 by the following:
LP4* From preparation to sensory representation of body state
If preparation state for facial expression f occurs with level v, then a
sensory representation for facial expression f with level v will occur.
preparation_state(f, v) →
→ srs(f, v)

Based on the model, a number of simulations have been
performed; for an example, see Figure 1 (here the time
delays within the temporal LEADSTO relations were taken
1 time unit). In this figure, where time is on the horizontal
axis, the upper part shows the time periods in which the
binary logical state properties s, sensor_state(s), srs(s),
imputation(s, e) hold (indicated by the dark lines):
respectively from time point 0, 1, 2 and 9. Below this part
for the other state properties values for the different time
periods are shown (by the dark lines). For example, the
preparation state for f has value 0.5 at time point 3, which is
increased to 0.75 and further at time points 9 and further.
The graphs show how the recursive body loop approximates
converging states both for emotion and facial expression.

Emotion Reading
Based on the model for emotion generation presented in the
previous section, in this section a model for emotion reading
(for the Simulation Theory perspective) is discussed. Such a
model for emotion reading should essentially be based on a
model to generate the own emotions. Indeed, the model
presented in the previous section can be specialised in a
rather straightforward manner to enable emotion reading.
The main step is that the stimulus s that triggers the
emotional process, which until now was left open, is
instantiated with the body state of another person; to make it
specific, a facial expression f of another person is
considered: s = othersface(f). Indeed there is strong
evidence that (already from an age of 1 hour) sensing
somebody else’s facial expression leads (within about 300
milliseconds) to preparing for and showing the same facial
expression (Goldman & Sripada, 2004, pp. 129-130).
Furthermore, for the sake of illustration, following the
emotion imputation, a communication about it is prepared
and performed. This extension is not essential for the
emotion reading capability, but just shows an example of
behaviour based on emotion reading.
LP10 Communication preparation
If emotion e is imputed to s, then a related communication is prepared
imputation(e, s) →
→ preparation_state(say(your emotion is e))
LP11 Communication
If a communication is prepared, then this communication will be
performed.
preparation_state(say(your emotion is e))
→
→ effector_state(say(your emotion is e))

The model described in the previous section has been
extended by the above two temporal relations in LEADSTO

format, and used for simulation. An example simulation
trace was obtained that for a large part coincides with the
one shown in Figure 1 (with the other person’s facial
expression f as the stimulus), with an extension as shown in
Figure 2. Here also the time delays within the additional
temporal LEADSTO relations were taken one time unit.
s
sensor_state(s)
srs(s)
imputation(s, e)
preparation_state(say(your emotion is e))
effector_state(say(your emotion is e))
time

0

5

10

15

20

25

30

35

Figure 2: Trace extension for emotion reading

Adaptive Emotion Reading
As a next step, the model for emotion reading is extended
by a facility to learn a direct connection between the
stimulus (the other face) and the emotion imputation. Such a
connection creates an emotion reading process that in
principle bypasses the generation of the own emotion. The
learning principle to achieve such an adaptation process is
inspired by the well-known learning principle at a
neurological level that connected neurons that are frequently
activated simultaneously strengthen their connecting
synapse (e.g., Gleitman, 1999). In an analogous manner,
within the model presented here an extra state is included
that represents the sensitivity of a direct connection between
the sensory representation of the stimulus (the other face)
and the emotion imputation. If this sensitivity is high, the
imputation will directly follow the sensory representation of
the stimulus, as is expressed by the following temporal
relationship.
LP12 Imputation shortcut
If the imputation sensitivity is high and a sensory representation for s
occurs, then emotion e will imputed to s.
srs(s) & srs_stimulus_imputation_sensitivity(high)
→
→ imputation(s, e)

The adaptation process itself and the persistence of the
sensitivity level is described by the following relationships.
LP13 Imputation sensitivity adaptation
If the imputation sensitivity is sen1 and a sensory representation for s
occurs and an imputation occurs, then the imputation sensitivity is the value
sen2 next to sen1.
srs(s) & imputation(s, e) &
srs_stimulus_imputation_sensitivity(sen1) &
next_value(sen1, sen2)
→
→ srs_stimulus_imputation_sensitivity(sen2)
LP14 Imputation sensitivity persistence
If the imputation sensitivity is sen1 and no increase occurs, then it will
remain the same.
srs_stimulus_imputation_sensitivity(sen1) &
next_value(sen1, sen2) &
not srs_stimulus_imputation_sensitivity(sen2)
→
→ srs_stimulus_imputation_sensitivity(sen1)
srs_stimulus_imputation_sensitivity(high)
→
→ srs_stimulus_imputation_sensitivity(high)

1008

Adaptive Simulation
Based on the model for adaptive emotion reading presented
in the previous section, a number of simulations have been
performed; for an example, see Figure 3. In this figure, the
imputation sensitivity state has initial value set to low,
represented by srs_stimulus_imputation_sensitivity(low) in the
upper part. The adaptation phase consists of two trials,
where as soon as the person imputes emotion e to the target
stimulus s (which is the observation of the other person’s
face), the imputation sensitivity level goes up, i.e., from low
to medium to high, in accordance with the temporal
relationship LP13 described above. Note that the sensitivity
state keeps its value in the adaptation phase until the person
(again) imputes emotion e to target, as described by
temporal relationship LP14, but retains its final value, i.e.
high, after the adaptation phase of two trials.

Note in the lower part of Figure 3, the values of other
state properties gradually increase as the person observes
the stimulus, following the recursive feedback loop
discussed in Section 3. These values sharply decrease as the
person stops observing the stimulus, as described by the
temporal relationship LP3 in Section 3. After the adaptation
phase, and with the imputation sensitivity at high, the person
imputes emotion e to the target stimulus directly after
occurrence of the sensory representation of the stimulus, as
shown in the third trial in the upper part of Figure 3. Note
here that even though the person has adapted to impute
emotion e to the target directly after the stimulus, the other
state property values continue to increase in the third trial as
the person receives the stimulus; this is because the
adaptation phase creates a connection between the sensory
representation of the stimulus and emotion imputation
without eliminating the recursive feedback loop altogether.

Verification of Properties

Figure 3: Simulation results for adaptive emotion reading

To verify whether the overall behaviour of the model is
according to expectations, some hypotheses (in terms of
logical dynamic properties) have been identified, formally
specified, and verified for simulation traces. These
properties express proper emotion reading, and some of
them are meant to distinguish emotion reading in a situation
before adaptation and after adaptation. In particular, before
an accomplished adaptation process, upon occurrence of a
stimulus, first the emotion has to be felt before the emotion
reading takes place. After an adaptation process, the
emotion reading takes place before the emotion is felt and
therefore it will take place faster.
The modelling approach for temporal expressions is
based on the Temporal Trace Language TTL (cf. Bosse,
Jonker, Meij, Sharpanskykh & Treur, 2009). This reified
temporal predicate logical language supports formal
specification and analysis of dynamic properties, covering
both qualitative and quantitative aspects. TTL is built on
atoms referring to states, time points and traces. A state of a
process for (state) ontology Ont is an assignment of truth
values to the set of ground atoms in the ontology. The set of
all possible states for ontology Ont is denoted by
STATES(Ont). To describe sequences of states, a fixed time
frame T is assumed which is linearly ordered. A trace γ over
state ontology Ont and time frame T is a mapping γ : T →
STATES(Ont), i.e., a sequence of states γt (t ∈ T) in
STATES(Ont). The set of dynamic properties DYNPROP(Ont)
is the set of temporal statements that can be formulated with
respect to traces based on the state ontology Ont in the
following manner. Given a trace γ over state ontology Ont,
the state in γ at time point t is denoted by state(γ, t). These
states can be related to state properties via state(γ, t) |= p
which denotes that state property p (from sort SPROP(Ont))
holds in trace γ at time t. Based on these statements,
dynamic properties can be formulated in a sorted first-order
predicate logic, using quantifiers over time and traces and
the usual first-order logical connectives such as ¬, ∧, ∨, ,

1009

∀, ∃. A special software environment has been developed
for TTL, featuring a Property Editor for building TTL
properties and a Checking Tool that enables formal
verification of such properties against a set of traces.
Using the TTL environment, the following Global
Properties (GP’s) have been identified, formalised and
automatically verified (first an abbreviation is introduced to
count how often a state holds in a certain time period):
Abbreviations
state_holds_times_between(S:SPROP,0,tb,te:TIME,γ:TRACE) ≡
¬ [ ∃t1:TIME tb<t1<te & state(γ, t1) |= S ]
state_holds_times_between(S:SPROP,n+1,tb,te:TIME,γ:TRACE) ≡
∃t1:TIME tb<t1<te & state(γ, t1) |= S &
¬[ ∃t2:TIME tb<t2<t1 & state(γ, t2) |= S ] &
state_holds_times_between(S, n, t1, te, γ)
GP1a Input-Output Correlation Timing
In trace γ, if at time point t1 the person perceives a facial expression of
another person, then within time duration D this leads to communication
about the person’s emotional state.
GP1a(t1:TIME, γ:TRACE, D:REAL) ≡
state(γ, t1) |= othersface(f)
[ ∃t2:TIME t1<t2<t1+D &
state(γ, t2) |= effector_state(your emotion is e) ]

This first property checks whether the process of
responding (verbally) to the stimulus is performed correctly.
As could be expected, this property indeed turned out to
hold for any t1. In the situation before learning, it holds for
D=36, and after learning it holds already for D=6.
GP1b Input-Output Correlation During Learning
If in trace γ between tb and te the person perceives a facial expression of
another person for n (different) time points, then within time duration D
this leads to communication about the person’s emotional state.
GP1b(tb, te:TIME, n:INTEGER, γ:TRACE, D:REAL) ≡
state_holds_times_between(othersface(f), n, tb, te, γ)
[ ∃t:TIME te<t<te+D &
state(γ, t) |= effector_state(your emotion is e) ]

This property also holds for all time points, and for n=3
and D=6: in all situations that the person perceived the
stimulus three times, this resulted in a response within 6
time points.
GP2 Successful Associative Learning
If in trace γ between tb and te state property S1 and S2 hold together for n
(different) time points, then eventually a relation between these states will
be learned.
GP2(tb, te:TIME, n:INTEGER, γ:TRACE) ≡
∀S1,S2:SPROP
state_holds_times_between(S1∧S2, n, tb, te, γ)
[ ∃t:TIME ∃w:REAL te<t<te+D &
state(γ, t) |= sensitivity_for_relation_between(w, S1,S2) & w>δ ]

This property holds for n=2 (and for D=1), which
confirms that the associative learning is directly successful
after two trials. Note that here δ is a certain sensitivity
threshold, which can be considered to depend on n. Thus, an
example instance of sensitivity_for_relation_between(w, S1, S2)
could be the predicate srs_stimulus_imputation_sensitivity(high).
GP3a Emotion reading with own feeling
In trace γ, if at time point t1 a stimulus occurs, then there is a point in time
that the emotion is recognised whereas it is felt as well.
GP3a(t1:TIME, γ:TRACE) ≡

state(γ, t1) |= othersface(f)
∃t2:TIME, v:REAL [ t1<t2<t1+D & v>th &
state(γ, t2) |= effector_state(your emotion is e) &
state(γ, t2) |= emotion(e, v) ]
GP3b Emotion reading without own feeling
In trace γ, if at time point t1 a stimulus occurs, then there is a point in time
that the emotion is recognised whereas it is not felt (yet).
GP3b(t1:TIME, γ:TRACE) ≡
state(γ, t1) |= othersface(f)
∃t2:TIME, v:REAL [ t1<t2<t1+D & v≤0.1 &
state(γ, t2) |= effector_state(your emotion is e) &
state(γ, t2) |= emotion(e, v) ]

These properties can be used to distinguish the phase
when the person performs emotion reading with an
experienced emotion from the phase without an experienced
emotion. Checks pointed out that the second phase is
entered at time point 126. To conclude, although not proven
exhaustively, the above checks have pointed out that the
model satisfies a number of relevant expected properties. In
addition, they allow the modeller to fine-tune the precise
temporal aspects of the simulated emotion reading process.

Discussion
A person’s observations of another person’s body, for
example facial expressions, are used by the person as a basis
for emotion recognition. Here, a specific emotion
recognition process can be modelled in the form of a
prespecified classification process of facial expressions in
terms of a set of possible emotions; see, e.g., (Cohen, Garg
& Huang, 2000; Malle, Moses & Baldwin, 2001; Pantic &
Rothkrantz, 1997 & 2000). Indeed, a model based on such a
classification procedure is able to perform emotion
recognition. However, within such an approach the imputed
emotions will not have any relationship to a person’s own
emotions. The model for emotion reading presented in the
current paper combines the person’s own emotion
generation with the emotion reading process as also claimed
by the Simulation Theory perspective on mindreading, (e.g.,
Goldman, 2006; Goldman & Sripada, 2004). In addition,
adaptive facilities within the model allow the person to learn
a direct classification without involving the own emotions.
In (Goldman, 2006, pp. 124-132), a number of possible
emotion reading models from the Simulation Theory
perspective are sketched and discussed. For his model 1, a
generate and test process for emotional states was assumed,
where on the basis of a hypothesized emotional state an own
facial expression is generated, and this is compared to the
observed facial expression of the other person. In the
assessment of this model, the hypothesis generation process
for a given observed face was considered as less
satisfactory. Models 2 and 3 discussed by Goldman (2006)
are based on a notion of ‘reverse simulation’. This means
that for the causal relation from emotional state to (the
preparation of) a facial expression which is used to generate
the own facial expressions, also a reverse relation from
prepared own facial expression to emotional state is
assumed, which is used for the mind reading process. A

1010

point of discussion concerning these models is that it is
difficult to fit them to the Simulation Theory perspective:
whereas the emotional states and facial expression
(preparation) states used for mindreading are the same as
used for the own emotions and facial expressions, the causal
relations between them used in the two cases are not the
same. Model 4 is based on a so-called ‘mirroring process’,
where a correlation between the emotional state of the other
person and the corresponding own emotional state is
assumed, based on a certain causal chain between the two.
However, the relation of such a causal chain with the causal
relations used to generate the own emotional states and
facial expressions is not made clear.
The approach adopted in the current paper has drawn
some inspiration from the four models sketched (but not
formalised) by Goldman (2006), as briefly discussed above.
The recursive body loop (or as if body loop) introduced here
addresses the problems of model 1, as it can be viewed as an
efficient and converging way of generating and testing
hypotheses for the emotional states. Moreover, it solves the
problems of models 2 and 3, as the causal chain from facial
expression to emotional state is not a reverse simulation, but
just the causal chain via the body state which is used for
generating the own emotional feelings as well. Finally,
compared to model 4, the models put forward here can be
viewed as an efficient manner to obtain a mirroring process
between the emotional state of the other person on the own
emotional state, based on the machinery available for the
own emotional states.
Models for emotion reading by a person can be of two
types: either they make use of the own emotion states of the
person, or they are independent of them. In principle models
according to the Simulation Theory perspective are of the
first type, whereas models according to the Theory Theory
perspective or based on a specific classification procedure
can be of the second type. In (Bosse, Memon, & Treur,
2008) a non-adaptive cognitive model for emotion reading
is described based on a recursive body loop according to the
Simulation Theory perspective. Moreover, it is shown how
this model can be used by an ambient software agent in
order to estimate a person’s emotion level. This software
agent is adaptive in the sense that at run-time the parameter
in the emotion reading model can be tuned to the person. By
Memon and Treur (2008) it is shown how this non-adaptive
model according to the Simulation Theory perspective can
be related to a biological (neurological) model. In contrast
to the above models, the current paper addresses a cognitive
model for emotion reading which itself is adaptive. This
adaptivity allows the model to gradually incorporate
relations that bypass the own emotion states, and thus at
run-time provides a model for emotion reading independent
of the own emotion state. As this learnt pathway bypasses
the own emotion generation process, and its body-related
part, it may be faster. Moreover, as own emotions are not
involved anymore, it may be argued that the learnt model
for emotion reading by itself is not a model from the

Simulation Theory perspective, whereas the model for the
learning process to obtain this model is. As a final remark, it
may be considered that the learnt model (or part of) is
innate, and is only further tuned by the learning process.

References
Baron-Cohen, S. (1995). Mindblindness. MIT Press
Bogdan, R.J. (1997). Interpreting Minds. MIT Press
Bosse, T., Jonker, C.M., Meij, L. van der, Sharpanskykh, A,
and Treur, J. (2009). Specification and Verification of
Dynamics in Cognitive Agent Models. Int. Journal of
Coop. Information Systems, vol. 18, 2009, pp. 167-193.
Bosse, T., Jonker, C.M., Meij, L. van der, and Treur, J.
(2007). A Language and Environment for Analysis of
Dynamics by Simulation. International Journal of
Artificial Intelligence Tools, vol. 16, 2007, pp. 435-464.
Bosse, T., Jonker, C.M., and Treur, J., (2008). Formalisation
of Damasio's Theory of Emotion, Feeling and Core
Consciousness. Consciousness and Cognition Journal,
vol. 17, 2008, pp. 94–113.
Bosse, T., Memon, Z.A., and Treur, J. (2008), Adaptive
Estimation of Emotion Generation for an Ambient Agent
Model. In: Aarts, E., et al. (eds.), Ambient Intelligence,
Proc. of the 2nd European Conf. on Ambient Intelligence,
AmI'08. Springer LNCS, vol. 5355, 2008, pp. 141–156.
Cohen, I., Garg, A., and Huang, T.S. (2000). Emotion
recognition using multilevel HMM. In: Proc. of the NIPS
Workshop on Affective Computing, Colorado, 2000.
Damasio, A. (1999). The Feeling of What Happens: Body,
Emotion and the Making of Consciousness. Harcourt
Brace & Co., NY, 1999.
Damasio, A. (2003). Looking for Spinoza: Joy, Sorrow, and
the Feeling Brain. Harcourt Brace & Co., NY, 2003.
Dennett, D.C. (1987). The Intentional Stance. MIT Press.
Cambridge Mass.
Gleitman, H. (1999). Psychology. W.W. Norton &
Company, New York, 1999.
Goldman, A.I. (2006). Simulating Minds: the Philosophy,
Psychology and Neuroscience of Mindreading. Oxford
University Press.
Goldman, A.I., and Sripada, C.S. (2004). Simulationist
models of face-based emotion recognition. Cognition, vol.
94, pp. 193–213.
Malle, B.F., Moses, L.J., Baldwin, D.A. (2001). Intentions
and Intentionality. MIT Press.
Memon, Z.A., and Treur, J. (2008), Cognitive and
Biological Agent Models for Emotion Reading. In: Jain,
L., et al. (eds.), Proceedings of the 8th IEEE/WIC/ACM
Int. Conference on Intelligent Agent Technology, IAT'08.
IEEE Computer Society Press, 2008, pp. 308-313.
Pantic, M., and Rothkrantz, L.J.M. (1997). Automatic
Recognition of Facial Expressions and Human Emotions.
In: Proceedings of ASCI’97, ASCI, Delft, pp. 196-202.
Pantic, M., and Rothkrantz, L.J.M. (2000), Expert System
for Automatic Analysis of Facial Expressions, Image and
Vision Computing Journal, vol. 18, pp. 881-905.

1011

