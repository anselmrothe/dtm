UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Necessity of Ordinary Experience

Permalink
https://escholarship.org/uc/item/6457p29q

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Author
Flanagan, Robin

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

The Necessity of Ordinary Experience
Robin Flanagan (FlanaganR@wcsu.edu)
Department of Psychology, WCSU, 181 White Street
Danbury, CT 06810 USA

interpret on its own (see (Brooks, 2008) for example); it
only means that silicon-based machinery isn’t necessarily
constrained by the same physical qualities that constrain
biological organisms. A lot of very interesting work in
artificial intelligence, does examine cognition while taking
biological constraints into consideration, and these lines of
research have been extremely fruitful, which should help to
support the idea that implementation does indeed matter.
The embodied cognition paradigm already assumes,
however, that implementation is a critical element of any
intelligent system.

Abstract
I argue in this paper that ordinary experience is not only a
nice part of everyday life; it is a necessity for the
development of human knowledge. I begin by looking at why
the particular biological machinery that defines our nervous
system matters. I then examine the particular machineries
that constrain but also foster the development of human
knowledge. Finally, I examine the kinds of activities that
foster the development of knowledge, given the constraints of
the given machinery, and conclude that activities that are
repeated often and that involve meaningful interaction with
an inherently meaningful environment form a plausible basis
for the formation of knowledge within the particular neural
net machinery that evolution has produced for us.

The Basic Machinery

Keywords: Learning; neural networks; embodied cognition;
practice; education; development; instructional technology

That leads to the examination of the actual elements of the
biological machinery from which the nervous system is
constructed. There are, of course, very few elements in the
biological machinery. The main element is an ordinary
neuron, which is not too dissimilar from other cells in the
biological organism. Like other cells in the biological
organism the neuron is best at responding to elements in the
immediate surroundings. In other words, the neuron is best
at noticing what’s in its immediate neighborhood and
responding by secreting to its immediate neighborhood.
However, the neuron can take on very unusual shapes,
and these shapes, make them particularly good for
communicating with each other, by redefining what is meant
by “its immediate neighborhood”. The maximized surface
area of the neuron (the dendrites) allows the neuron to
receive multiple messages simultaneously from other
neurons or from the environment. The other part of the
neuron’s unusual shape (the axon) can sometimes be quite a
long extension of the cell body. The axon is the main tool
that the neuron has at its disposal for communicating to
other neurons or to the muscles. So just by changing its
shape the neuron has the ability to get information from, and
have an effect on, parts of the nervous system and ultimately
parts of the body that are not apparently in its immediate
neighborhood.
This is important because the main technique that neurons
have for getting information, and for sending information,
involves the idea of simple local processing. So it’s
important to note that “local” for the neuron has been
redefined to include connections to quite distant elements of
the nervous system and the biological organism. In fact, in
the case of the photoreceptors, “local” involves light waves
arriving in the immediate vicinity from potentially
extremely distant locations. Simple local processing is the
kind of processing that single-celled organisms developed at
the very beginning of organized life, to detect things in their

Mind and world in short have been evolved together, and in
consequence are something of a mutual fit.
(James, 1948, p. 4)

The Implementation Problem
The implementation question is the notion that once a
system of knowledge has been completely and accurately
articulated, it shouldn’t matter in what kind of machinery
the system is implemented. This was a major assumption of
cognitive science for quite a long time, and to its credit it
was a very useful and fruitful assumption. If we assume
that there is no important difference between carbon-based
machinery and silicon-based machinery, and this is a very
reasonable assumption, we can investigate and test
knowledge systems on silicon-based machinery, machinery
which is much easier to control, much easier to completely
specify, and much easier to manipulate in ethical ways.
However, this assumption has two gaping holes in it: how
does the knowledge get into the machinery (most biological
organisms have no programmers to install useful data
structures or programs, while most silicon-based machines
do have programmers), and how does the knowledge get
interpreted (most silicon-based machines have intelligent
“users” to interpret the output; most biological organisms
must interpret the knowledge for themselves).
If, instead of ignoring implementation, we examine how
the actual machinery works, we find that there are many
important constraints derived directly from the machinery
that actually help us to understand how the knowledge gets
incorporated into the machinery and how the “knowledge”
in the system gets interpreted. This, of course, does not
mean that a silicon-based machine couldn’t learn and

724

couldn’t be more important. A relatively more predictable
pattern of activation that is meaningful or important to the
organism is pretty close to a basic definition of intelligent
behavior, or knowledge. Again, operant conditioning may
seem too basic and non-cognitive when discussed in the
absence of a mind or within the context of training animals,
as behaviorism often is; yet surely the ability to increase the
probability of activating a useful pattern of neurons when it
becomes clear that the pattern is, in fact, useful, could form
the basis of an endogenous back-propagation system, the
exogenous form of which is such an essential aspect of so
many artificial neural nets (see, for example, McClelland
and Rumelhart (1988)). Whether it forms the basis of the
feed backward system or not, most would agree that
knowledge that is more probable, rather than less probable,
to become available at the appropriate time is the main point
of learning and education.
The third important mechanism was hypothesized by
Hebb sixty years ago (1949), and established more recently
in empirical neuroscience research ((Isaac, Buchanan,
Muller, & Mellor, 2009) for example). Hebb theorized that
neurons that become activated simultaneously would be
subsequently more likely to activate each other. This has
been found at least in the case of the NMDA receptor, a
receptor that requires simultaneous messages in order to
allow permanent, structural changes to occur at the synapse
(see (Isaac, et al., 2009) for example). This is perhaps a
more general mechanism upon which both Pavlovian
association and Skinnerian contingency are both built.
Long-term potentiation has been the chief candidate for this
process. Long-term potentiation involving the NMDA
receptor requires more than one converging pathway to
neural activation. Also important is the idea that this
process is dependent on an overwhelmingly huge stimulus,
or an often repeated activation before it makes permanent
changes to the synapse. If long-term potentiation (or any
kind of wiring) developed after every mere exposure the
neural net would be in constant flux without the ability to
store meaningful knowledge (something to keep in mind
when considering so-called “smart” genes and genetic
modifications). The ability of neurons to strengthen their
association when they find themselves simultaneously
activated over time is essential to both forms of
conditioning, as well as learning to perceive and to act on
any reliable invariance in the internal and external
environment. Invariance in the environment, by definition,
provides almost endless repeated activation in response to
objects and events that are important, or, at least, enduring.
Finally, with lots of neurons activated simultaneously in
response to an event in the environment, distributed
“representation” is possible: that is, a distributed set of
neurons together form a concept. This is important as a
storage mechanism, but it is even more important as a
means of developing categories and abstract concepts.
When lots of neurons, rather than a single neuron, become
activated by a particular stimulus, and then another large
group of neurons becomes activated by a slightly different

immediate environment, and through very simple rules
made decisions about how to act on their environment. The
typical example is a bacterium floating through water.
When it detects a particular toxin in the environment, it
activates its flagellum and flaps away from the toxin. The
cool thing about simple local processing is that when many
organisms are using simple local processing at the same
time, intelligent behavior can emerge at the level of the
group or colony, without any programmer or leader or
teacher.
Because there is no programmer or leader or teacher to
direct the nervous system this is an incredibly useful quality
to include in any description or explanation of biological
intelligence to account for the undoubtedly quite intelligent
behavior of this leaderless system.
So, the basic elements of which our nervous system is
composed consist of billions of very simple agents,
performing simple local processing in which “local” has
been redefined to include any “neighbor” to which a
neuron’s unusual shape can give it access, including, for
example, any light wave event within the visual vicinity of
the amazing biological eye. This massively parallel system
of simple agents acts without a leader, without a
programmer, without a teacher; yet intelligent and useful
behavior emerges over time. We turn next to the question
of how knowledge, or intelligent behavior, can emerge in
such a system.

How knowledge develops in such a system
While no single model of the human nervous system has
been universally accepted, we have established the basic
building blocks and parameters from which it must be built.
Several of the basic mechanisms with which such a neural
network could store knowledge have also been identified.
The first important mechanism was established about a
hundred years ago by Pavlov (2009)and articulated more
fully in the sea slug by Kandel and his colleagues (Hawkins,
Greene, & Kandel, 1998, for example). The ability of the
nervous system to associate a previously non-meaningful
stimulus with an already meaningful stimulus may seem
rather minor and non-cognitive when discussed within the
context of dog saliva and sea slugs, and yet this is an
amazingly useful mechanism. Association between a
stimulus that is already meaningful and a previously
meaningless stimulus can produce symbols, where a symbol
means anything that stands for something else. Surely this
is the basis of the nervous system’s ability to use language
and, more generally, abstract symbols. Abstract symbols
are, by definition, meaningless stimuli on their own which
have taken on meaning by association with something
already meaningful.
The second important mechanism was robustly
established during the half century of American behaviorist
research (Staddon & Cerutti, 2003). Operant conditioning
increases the probability of a pattern of neural activity to
reoccur if that pattern has proven to be useful (that is, if it
has been reinforced). In a probabilistic network, this

725

stimulus, any overlapping active neurons get twice the
opportunity to wire together with each other, and so
subsequently are even more likely to activate each other.
This overlapping set comes to stand for (or “mean”) the
precise similarity between the two stimuli, not as an analogy
but as a literal overlapping commonality. This is a
profoundly important part of our machinery if we want to be
able to explain the human genius for categorization,
abstraction, and creativity.
Very few psychologists admit that these crude
mechanisms are useful for more than motor skill learning
and perception. Yet, what other mechanisms have been
identified in the nervous system to account for lasting
changes? I am aware of none. So, leaving physical skill
learning and perception aside for the moment (although
they’re quite important) let’s examine, briefly, how verbal,
spatial, or declarative knowledge could develop in such a
system, although the research in this area is ongoing and not
at all settled yet.
These mechanisms certainly do look better suited to the
implementation of non-declarative knowledge than of
declarative knowledge. Non-declarative knowledge can
build up over time through normal interactions and
perceptions, and even without conscious awareness or
attention. But how do we explain the (seemingly) more
cognitive, conscious and occasionally instantaneous
category: declarative knowledge? How could declarative
knowledge be implemented in such a system?
Unlike all other organisms, human beings have a rich set
of verbal (as well as visual) symbols at their disposal. One
possibility is that words become associated (through
classical conditioning mechanisms) with “concepts” already
established in the neural network through Hebbian synapses.
In fact, Bloom and her colleagues found that as soon as
children are reliably able to refer to objects in their
environment, jointly with their caregiver, vocabulary
suddenly blossoms (Lifter & Bloom, 1989). GoldinMeadow found that as soon as learners were capable of
gesturing appropriately during problem-solving, that the
correct words almost immediately followed (2003). It
seems that in humans, at least, language is produced almost
simultaneously with the ability to identify and perceive a
referent. If this is the case, this is a powerful addition to the
simple machinery with which we have to work: to be able
to have a word associated with each distinction we are
capable of perceiving or acting upon.
Once a word is in place (associated with a meaningful
distinction) the neural net can use the activation of a word in
place of the primary experience: the word can initiate a
cascade of neural activity that is very similar to the cascade
that would be produced by the primary experience. At this
point, a coach, or a teacher, or a friend, or a parent can use a
word (“hot”) to produce the same neural activity that might
have been produced by a similar (“hot”) experience, thus
allowing learning to take place without the primary
experience. Clearly the primary experience, or some critical
conjunction of important partial experiences, must have

occurred at some point. But learning can quickly be
produced in the absence of the primary experience once the
word is in place. From here it is a not impossibly large leap
to the nervous system supplying the words internally in the
absence of an external coach, teacher, friend or parent.
These internally activated words, then, could form the basis
of explicit knowledge and rational thought.
Re-activation of sensory-motor cortex, followed by a
cascade of neural activity similar to primary activation, has
repeatedly been found to be the case with stored concepts
(see, for example, the visual imagery work of Kosslyn
(2005) and the motor imagery work of Jeannerod (1994)).
The research on mirror neurons has even indicated that
watching someone else’s behavior can trigger a cascade of
neural activity that is similar to the neural activity involved
in one’s own primary experience (Brass & Rüschemeyer,
2010).
The other aspect of declarative knowledge, the apparent
ability of explicit knowledge to be examined consciously,
needs more explanation, and probably a completely separate
paper. Briefly, though, the main advantage of implicit, or
non-declarative knowledge, is that it is so well integrated
into the neural network that it is ready for use without any
conscious reflection. That is of course its main liability as
well, because without conscious reflection there is no room
for “free will”, no room for new responses, and no room for
transfer of knowledge to novel situations. How, then, does
declarative knowledge gain this apparently conscious
element? There is perhaps no hotter topic in philosophy of
mind these days (see Metzinger (2009) for example), so I
will not presume to solve this problem for all time.
However, an intriguing possibility, and one that is in line
with what is known about the biological constraints of the
human nervous system, was put forth decades ago by
Antonio Damasio (1989). He pointed out that a mechanism
in the hippocampus allowed incoming messages to be,
essentially, bounced back to the sensory store from which
they had just come. Because incoming sensory information
must reach the hippocampus in a cohesive timeframe, the
bouncing back must also occur in tandem, restimulating the
same sensory stores as the original experience. He did not
discuss verbal stimulation in particular, but because we
know that verbal information stimulates the same sensory
store as heard language (Hubbard, 2010), this mechanism
should work for verbal information as for any other sensory
stimulation. What does this ability to bounce an experience
back for re-experiencing buy us? Just this: it allows for the
opportunity, as any multi-neuron synaptic junction would,
for the original stimulus to be affected by other elements
rather than triggering an automatic and unalterable cascade
of activity. Implicit knowledge does not need to go through
this bounce-back process because it’s already usable, and in
many cases, already crystallized. Explicit knowledge,
however, differs from implicit knowledge in the “second
chance” it gives its network, and of course the environment,
to affect the cascade of activity in a new or more subtle way.
This explicit second chance may not result in fast, or

726

graceful, processing and activity (that is the strength of
implicit knowledge), but it gives our neural net the
opportunity to bring old symbols, old categories and old
knowledge to bear on a new situation. The analogy I have
used with students is very over-simplified, but may help to
illustrate this distinction. If a sensory stimulus is like a
pebble and our neural network is like a pond, then implicit
knowledge is the set of waves that travel across the pond
without hindrance when the pebble is dropped into its
center, and explicit knowledge is the set of waves that
results from the pebble’s original waves encountering a
partial barrier that bounces back some of the waves allowing
them to interact again with the out-moving waves. The
explicit is more complicated, more interesting, more filled
with information (in the information theory sense), but the
implicit is more graceful and efficient.
So, it’s possible for both non-declarative and declarative
knowledge to develop within the severe constraints built
into the biological machinery about which we already know.

the characteristics that tend to appear simultaneously tend
not to be arbitrary co-occurrences, but rather quite
meaningful and successful co-occurrences. In other words,
if our nervous system happens upon a set of co-occurring
characteristics in the natural world, they are extremely likely
to be the product of a long and successful line of evolution,
and therefore be the opposite of arbitrary or capricious.
All else being equal, then, the set of repeated cooccurrences we encounter will tend to be meaningful, not
meaningless, co-occurrences, and therefore very useful for
us to learn to perceive, “chunk” and to be able to act on.
Our physical environment is full of non-arbitrary cooccurrences. The physical laws at work here are the same
physical laws that have shaped our planet for billions of
years and that have driven evolution of all the living species
we encounter since life began on this planet. And the cooccurrences of living things in a particular environment are
also non-arbitrary because these living things have had to
survive within the same environment for millions of years.
So the living organisms that we encounter have been
successful not just in our particular physical environment,
but in our particular ecological niche as well.

Activities that Foster Development
What kinds of activities, then, foster knowledge
development in such a system, with so few clear
mechanisms for plasticity?
Imagine the elaborately
connected human nervous system moving about in the
environment with all of its electrical activity visible for
observation. Notice that the nervous system is constantly
active and that what changes is the relative activity of the
system: relative both in time and space. This system does
not passively await inputs, but constantly changes in
response to the particular interactions it has with its
environment. It should be clear at this point, that a system
such as this one has no “input” device. It has, rather, the
ability to make small adjustments in real time in response to
real events. This system will only be as useful as the
meaningful distinctions to which it can attend and respond.
What activities will, naturally, produce patterned and
intelligent behavior?
Perhaps obviously, the neural network will store reliable
patterns detected in the environment: if a set of neurons is
consistently firing together, they will begin to wire together,
thus storing a united response to a unified set of stimuli.
There are two major sources for such reliable patterns: the
natural invariances in the physical world, and the sets of
actions that produce reliable (or meaningful) results for the
organism (contingent activities). Notice how perfectly these
sources match our two major learning mechanisms:
associative conditioning and operant conditioning.

Contingent Activities
Held and his colleagues found quite a while ago that
contingent experiences were necessary for the normal
development of kittens (Held & Hein, 1963). In his elegant
set of experiments, in which kittens were literally yoked
during their daily visual stimulation and were able to move
around the visual stimuli based on just one of the yoked
kittens’ movements, Held showed that kittens with
completely equal visual stimulation, and deprivation,
developed completely different visual capabilities
depending only on whether the visual stimulation was
contingent on the kitten’s own activity.
Fox and Oakes updated Held’s experiments by doing a
similar set of experiments using undergraduates, instead of
kittens, and video games, instead of a yoked carousel
experience (Fox & Oakes, 1984). In this set of experiments,
undergraduates were virtually yoked to each other while
they played one of two versions of a video game. In one
version of the game, the undergraduates’ success at
destroying elements of the virtual world was completely
contingent on their motor behavior: if their aim and timing
was good, they were able to blow up a lot of objects; if their
aim and timing was poor, they had little success. In the
second version of the game, undergraduates experienced the
same (yoked) number of apparent successes, but the success
had nothing to do with their motor behavior: it depended
completely on the success of the undergraduate to which
they had been virtually yoked. However, the second version
of the game was designed to make it look like the success
was contingent on the player’s skill: elements were slowed
or speeded up in order to make appropriate, successful,
contact. When tested afterwards all of the undergraduates
felt as though they had succeeded: consciously they felt like
their actions mattered. But the undergraduates who played

Invariance in the Environment
Why does the physical world provide such a rich source of
useful invariances (or correlations) for the nervous system?
The short answer is “evolution”. Because the particular
physical environment in which we all develop is the product
of multiple, simultaneous lines of successful evolution,
within the same set of physical constraints based on the
physical structure and physical laws of this particular planet,

727

the non-contingent form of the game were not as successful
at a subsequent, unrelated, lexical decision task.
Notice that “contingent experience” is any experience in
which the organism’s actions are related, reliably, to the
feedback the organism receives, whether or not the
organism is consciously aware of this reliable relationship.

that personal feedback rather than task-related feedback
interferes with the mastery orientation of children solving
challenging problems (Dweck & Leggett, 1988). Because of
this difference in available stereotypically female and male
after school activities, Flanagan and Canada provided
school-age female students with one hour a week of afterschool activities in which the students got natural feedback
for both invariance in the environment and their own
contingency (Flanagan & Canada, 2010). These students
did computer programming (Scratch (Group) or Lego
Mindstorms (Lab, 1999)) or building scale models (Google
Sketch (Google, 2010) or physical craft materials) for eight
weeks. At the end of the eight weeks the students had
significantly better spatial reasoning skills than a similar
control group, and felt significantly more confident about
doing math and using computers.

Both Invariance and Contingency
Diamond and Rosenzweig and their colleagues looked at
both elements at once. They found that rats that grew up in
an environment with lots of new, physical and social
interactions, developed more useful and heavier brains
(Rosenzweig, Bennett, & Diamond, 1972). Interestingly,
when the interaction was eliminated, by having rats near
enough to watch but not interact with all the stimulation, the
rats’ brains did not become as useful or heavy. Most
importantly, however, these “enriched” lab rats had brains
that were significantly less useful, heavy, and wellconnected than rats raised in the wild (where both
invariants, and contingency are much more widely
available) (Huck & Price, 1975; Zhao, Toyoda, Wang, &
Zhuo, 2009) .
Flanagan (1996) showed that in a normal classroom
setting, third graders who did an activity that involved
contingent rather than non-contingent feedback for just
fifteen minutes were subsequently significantly less likely to
give up in a challenging but possible puzzle. Furthermore,
third graders who used physical rather than virtual materials
were significantly more likely to be able to build on that
knowledge.
Natural feedback refers to feedback that is not dependent
on a teacher, programmer or author, but that is instead
inherent in the activity itself. So dropping objects of
different weights does not require a teacher to give positive
or negative feedback; the gravity of the physical world gives
this feedback naturally. Most interactions with the natural
world provide such feedback, but natural feedback is not
limited to the natural or physical world:
computer
programming, for example, provides natural feedback
because the programmer does not need a teacher or
authority to provide positive or negative reinforcement – the
programmed code either works or it doesn’t. All else being
equal, though, the natural world is the safer bet since cooccurrences in the natural world are the product of
evolution, and interactions with the natural world follow the
laws of physics. Artificial, or authored, environments
depend completely on the author, or programmer to provide
meaningful co-occurrences, and to provide meaningful
feedback – these must be deliberately incorporated, while in
the natural world they are already an integral part.
Natural feedback is also less available in stereotypically
female hobbies than in stereotypically male hobbies.
Playing with water pistols provides natural feedback – either
you get wet or you don’t. Many stereotypically female
hobbies depend on the opinions offered by peers or
authority figures: does this look pretty? Have I pleased
you? Is this good? Dweck and her colleagues have found

Ordinary Experiences
In environments that consist of inherently meaningful cooccurrences and opportunities for consistently meaningful
feedback the nervous system thrives.
Repetition, or
practice, in such environments should produce robust, wellorganized, functional nervous systems. The practice effect
is well-established, but shouldn’t be ignored: too often we
turn to the conceptual or technological shortcut when mere
practice in a meaningful environment would do more good.
Imagine a basketball team that got an hour or two of lecture
a week and then several readings in order to get ready to
play the season; imagine an orchestra that got an hour or
two of lecture a week and then had to read their musical
scores as homework for getting ready for their concert
season. This sounds ridiculous, of course. But we expect
our students to learn more “cognitive” skills this way even
though it shouldn’t work given the mechanisms available,
and routinely fails to work (see (Sahiner, 1987) for
example). If we accept the mechanisms we’ve been given,
cognitive education should begin to look more like physical
and musical education.
“Baby Einstein” media have recently been (finally)
recalled because they probably do more harm than good
(Lewin, 2009). As cognitive scientists we owe anxious
parents the benefit of our expertise, and must point out that
ordinary interactions with people and meaningful objects are
better suited to the developing nervous system than
“educational” consumer media. Because, (un)fortunately,
constrained by the biological machinery with which we are
born there is no magical input portal for pouring fully
formed knowledge systems into the human mind: there are
just a few simple mechanisms that must incorporate
knowledge through lots of simple, ordinary, meaningful
encounters over a long period of time.

Conclusion
The human nervous system is the product of millions of
years of evolution within an ecology that has simultaneously
been evolving. So it makes sense that the human nervous
system should be optimized for operating within the

728

particular natural and physical world we call “earth”.
Indeed when we look at the particular mechanisms actually
available to the human nervous system for learning and
developing a solid knowledge base, these mechanisms seem
to be ideal for detecting and learning naturally occurring
invariances in our ordinary environment, as well as for
learning actions that turn out to be important and
meaningful to the nervous system itself. These are the very
elements that Lloyd argued were the minimum essential
requirements for anything we would consider to be a “mind”
(1989). Furthermore, these mechanisms work best when the
applicable neurons are activated simultaneously over a
significant period of time.
Activity that involves important co-occurrences that are
meaningful for the organism over significant periods of time
are more succinctly termed “ordinary” experiences and are
the foundation of our solid and meaningful neural network.
We would be wise to build on this framework rather than
attempting to circumvent it. Practice in real environments
in real time has long been the accepted practice in athletics
and music. It is time for other human endeavors to follow
the same advice.

Hawkins, R. D., Greene, W., & Kandel, E. R. (1998). Classical
conditioning, differential conditioning, and second-order
conditioning of the Aplysia gill-withdrawal reflex in a
simplified mantle organ preparation. Behavioral Neuroscience,
112(3), 636-645. doi: 10.1037/0735-7044.112.3.636
Hebb, D. (1949). The organization of behavior.
Held, R., & Hein, A. (1963). Movement-produced stimulation in
the development of visually guided behavior. Journal of
Comparative and Physiological Psychology, 56(5), 872-876.
doi: 10.1037/h0040546
Hubbard, T. L. (2010). Auditory imagery: Empirical findings.
Psychological Bulletin, 136(2), 302-329. doi: 10.1037/a0018436
Huck, U. W., & Price, E. O. (1975). Differential effects of
environmental enrichment on the open-field behavior of wild
and domestic Norway rats. Journal of Comparative and
Physiological
Psychology,
89(8),
892-898.
doi:
10.1037/h0077160
Isaac, J. T. R., Buchanan, K. A., Muller, R. U., & Mellor, J. R.
(2009). Hippocampal place cell firing patterns can induce longterm synaptic plasticity in vitro. The Journal of Neuroscience,
29(21), 6840-6850. doi: 10.1523/jneurosci.0731-09.2009
James, W. (1948). Psychology. Cleveland, OH: The World
Publishing Company.
Jeannerod, M. (1994). The representing brain: Neural correlates of
motor intention and imagery. Behavioral and Brain Sciences,
17(2), 187-245.
Kosslyn, S. M. (2005). Mental images and the brain. Cognitive
Neuropsychology,
22(3-4),
333-347.
doi:
10.1080/02643290442000130
Lab, M. M. (1999). Lego Mindstorms: The Lego Group.
Lewin, T. (2009, 10/23/2009). No Einstein in Your Crib? Get a
Refund, New York Times.
Lifter, K., & Bloom, L. (1989). Object knowledge and the
emergence of language. Infant Behavior & Development, 12(4),
395-423. doi: 10.1016/0163-6383(89)90023-4
Lloyd, D. (1989). Simple Minds. Cambridge, MA: MIT Press.
McClelland, J. L., & Rumelhart, D. E. (1988). Explorations in
parallel distributed processing: A handbook of models,
programs, and exercises. Cambridge, MA US: The MIT Press.
Metzinger, T. (2009). The ego tunnel: The science of the mind and
the myth of the self. New York, NY US: Basic Books.
Pavlov, I. P. (2009). Conditioned reflexes: An investigation of the
physiological activity of the cerebral cortex (1927). In B. F.
Gentile & B. O. Miller (Eds.), Foundations of psychological
thought: A history of psychology. (pp. 441-453). Thousand
Oaks, CA US: Sage Publications, Inc.
Rosenzweig, M. R., Bennett, E. L., & Diamond, M. C. (1972).
Brain changes in response to experience. Scientific American,
226(2), 22-29.
Sahiner. (1987). A Private Universe: Harvard-Smithsonian Center
for Astrophysics, Science Education Department, Science Media
Group.
Staddon, J. E. R., & Cerutti, D. T. (2003). Operant conditioning.
Annual Review of Psychology, 54, 115-144. doi:
10.1146/annurev.psych.54.101601.145124
Zhao, M.-G., Toyoda, H., Wang, Y.-K., & Zhuo, M. (2009).
Enhanced synaptic long-term potentiation in the anterior
cingulate cortex of adult wild mice as compared with that in
laboratory mice. Molecular Brain, 2.

Acknowledgments
Thanks to James Schmotter of Western Connecticut State
University and the CSU-AAUP for funding much of this
work.

References
Brass, M., & Rüschemeyer, S.-A. (2010). Mirrors in science: How
mirror neurons changed cognitive neuroscience. Cortex: A
Journal Devoted to the Study of the Nervous System and
Behavior, 46(1), 139-143. doi: 10.1016/j.cortex.2009.04.005
Brooks, R. A. (2008). Intelligence without representation. In W. G.
Lycan & J. J. Prinz (Eds.), Mind and cognition: An anthology
(3rd ed.). (pp. 298-311). Malden: Blackwell Publishing.
Damasio, A. R. (1989). Time-locked multiregional retroactivation:
A systems-level proposal for the neural substrates of recall and
recognition. Cognition, 33, 25-62.
Dweck, C. S., & Leggett, E. L. (1988). A Social-Cognitive
Approach to Motivation and Personality. Psychological Review,
95(2), 256-273.
Flanagan, R. (1996). Learning through direct vs. indirect
experience: The role of interactivity and physicality in media
effects. 57, ProQuest Information & Learning, US. Available
from EBSCOhost psyh database.
Flanagan, R., & Canada, T. (2010). Ordinary experience helps girls
develop their spatial reasoning. Paper presented at the
Association of Psychological Science, Boston, MA.
Fox, P. E., & Oakes, W. F. (1984). Learned helplessness:
Noncontingent reinforcement in video game performance
produces a decrement in performance on a lexical decision task.
Bulletin of the Psychonomic Society, 22(2), 113-116.
Goldin-Meadow, S. (2003). Hearing gesture: How our hands help
us think. Cambridge, MA US: Belknap Press of Harvard
University Press.
Google. (2010). Google Sketchup.
Group, L.-l. K. Scratch Programming Language. Cambridge, MA:
MIT Media Lab.

729

