UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Hubel Weisel model for hierarchical representation of concepts in textual documents

Permalink
https://escholarship.org/uc/item/57w312q3

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Ramanathan, Kiruthika
Shi, Luping
Chong Chong, Tow

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

A Hubel Weisel model for hierarchical representation of concepts in textual
documents
Kiruthika Ramanathan (kiruthika_r@dsi.a-star.edu.sg),
Shi Luping (shi_luping@a-star.edu.sg), Chong Tow
Chong (chong_tow_chong@dsi.a-star.edu.sg)
Data Storage Institute, (A*STAR) Agency for Science,
Technology and Research, DSI Building, 5 Engineering Drive 1,
Singapore 117608

the existing memory, such that the new memory can be
acquired without damage to the old ones.

Abstract
Hubel Weisel models of the cortex describe visual
processing as a hierarchy of increasingly sophisticated
representations. While several models exist for image
processing, little work has been done with Hubel Weisel
models out of the domain of object recognition. In this
paper, we describe how such models can be extended to
the representation of concepts, resulting in a model that
shares several properties with the PDP model of
semantic cognition. The model that we propose is also
capable of incremental learning, in which the knowledge
is stored in the strength of the neuron connections.
Degradation of old knowledge occurs as new knowledge
is introduced to the system in a fashion that simulates
decay theory in short term memory. The simulation
model therefore captures several properties of cognitive
conceptual memory, including generalization patterns,
the role of rehearsal and, hierarchical representation.

Related work

Introduction
There exist several bottom-up approaches to
hierarchical models of object recognition that are based
on the visual cortex. They make use of Mountcastle’s
(1978) theory of uniformity and hierarchy in the
cortical column and the model of simple to complex
cells of Hubel and Weisel (1965), modeling how simple
cells from neighboring receptive fields feed into the
same complex cell, meaning that the complex cell has
phase invariant response.
In this paper, we consider the following question. If
the structure of the cortical column is uniform and
hierarchical in nature and if the model of simple to
complex cells can be used to model the visual cortex as
discussed in prior works, then can such a model also be
used to represent other modalities of information such
as the concepts derived from text? We are therefore
aiming to design a bottom up hierarchical memory for
the representation of concepts, much the same way as it
is designed for the representation of images. In this
paper, we will define a concept as being a keyword in a
document.
To deal with the dynamic nature of concept inputs,
we look at incremental learning of concepts from two
aspects relevant to concept representation from text – (a)
with respect to new incoming features and (b) training
of hierarchies. To perform this, we apply a set of
geometric approximations to the incremental inputs and

Mountcastle (1978) showed that parts of the cortical
system are organized in a hierarchy and that some
regions are hierarchically above others. In general,
neurons in the higher levels of the visual cortex
represent more complex features with neurons in the IT
representing objects or object parts (Hubel and Weisel,
1965). Hubel Weisel models have therefore been
developed for object recognition (Cadieu et al., 2007;
Fukushima, 2003) proposing a hierarchy of feature
extracting simple (S) and complex (C) cells that allow
for positional invariance. The combination of S-cells
and C-cells, whose signals propagate up the hierarchy
allows for scale and position invariant object
recognition.
The idea of feature based concept acquisition has
been well studied in psychological literature. Sloutsky
(2003) discusses how children group concepts based on,
not just one, but multiple similarities, which tap the fact
that those basic level categories have correlated
structures (or features). This correlation of features is
also discussed in McClelland and Rogers (2003) who
argue that information should be stored at the individual
concept level rather than at the super ordinate category
level allowing properties to be shared by many items.
Our model is related to Hubel Weisel approaches in
that it implements a hierarchical modular architecture
for bottom-up propagation of conceptual information.
To our knowledge, however, this is the first
implementation of a Hubel Weisel approach to nonnatural medium such as text, and has attempted to
model hierarchical representation of keywords to form
concepts.

System architecture
The system that we describe here is organized in a
bottom up hierarchy. This means that the component
features are represented before the representation of
concept objects. Our learning algorithm exploits the
property of this hierarchical structure. Each level in the

1106

hierarchy has several modules. These modules model
cortical regions of concept memory. The modules are
arranged in a tree structure, having several children and
one parent. In our paper, we call the bottom most level
of the hierarchy level 1, and the level increases as one
moves up the hierarchy. The keywords from a
document form the inputs to the system. These are
directly fed to level 1. Level 1 modules resemble simple
cells of the cortex, in that they receive their inputs from
a small patch of the input space. Several level 1
modules tile the input space, possibly with overlap. A
module at level 2 covers more of the input space when
compared to a level 1 module. It represents the union of
the input space of all its children level 1 modules.
However, a level 2 module obtains its inputs only
through its level 1 children. This pattern is repeated in
the hierarchy. Thus, the module at the tree root (the top
most level) covers the entire input space, but it does so
by pooling the inputs from its child modules. In the
visual cortex, the level 1 can be considered analogous
to the area V1 of the cortex, level 2 to area V2 and so
on.

Learning the first batch of information
To understand how the model learns, let us consider
the inputs and outputs of a single module mk,i in level k
of the system as shown in Figure 1a. Let x, representing
connections {xj} be the input pattern to the module mk,i.
x is the output of the child modules of mk,i from the
level k-1, and a represent the weights of the competitive
network. The vector a is used to represent the
connections {aj} between x and the cells in the module
mk,i. The output of a neuron in the module mk,i is given
∑
by
.

(a)

(b)

Figure 1a. Inputs and outputs to a single module mk,i. b. The
concatenation of information from the child modules of the
hierarchy to generate inputs for the parent module

During learning, each neuron in mk,i competes with
other neurons in the vicinity. Of the large number of
inputs to a given module, a neuron is activated by a
subset of them. The neuron then becomes the spatial
center of these patterns. To ensure that there are no
garbage neurons, we adopt in our creation of the
module, a model of Growing SOM (GSOM)
(Alahakhoon et al., 2000).
When all the modules at level k finish training, the
second stage of learning occurs. This comprises the

process by which the parent modules learn from the
outputs of the child modules. Here, consider the case
shown in Figure 1b where the module 3 is the parent of
modules 1 and 2. Let x(1) be the output vector of
module 1 and x(2) be the output vector of module 2. x(i)
represents the Euclidean distance from the input pattern
to the each output neuron i of the child modules. The
input to module 3,
||
, is the
concatenation of the outputs of modules 1 and 2. A
particular concatenation represents a simultaneous
occurrence of a combination of concepts in the child
module. Depending on the statistics of the input data,
some combinations will occur more frequently, while
others will not. During the second stage of learning, the
parent module learns the most frequent combinations of
concepts in the levels below it. A GSOM is again used
in the clustering of such combinations. The learning
process thus defined can be repeated in a hierarchical
manner.

Incremental learning
In this and the subsequent sections of the paper, we
will use the terms batch 0 to represent the first batch of
documents. Batch 1 refers to the subsequent set of
documents. Once the system learns the documents in
batch i, only the hierarchical structure and the neuron
architecture are retained. All other information
regarding the documents presented is discarded.
Incremental learning poses a challenge in Hubel
Weisel based computational models due to three
reasons. (1) Damage to the knowledge represented by
old neurons which is fundamental in competitive
learning. (2) Propagation of information in the
hierarchical architecture. The number of output neurons
of each child node increases with the introduction of the
incremental batch. The input dimensions of the parent
node are therefore changed and incompatible with the
dimensions of the previous batch. (3) The irregularity
in the input data dimensions. Where keywords are
defined as concepts to be processed by the system, the
keywords in an incremental batch will not be a subset
of those in the previous batch. The architecture
therefore has to provide rules for the generation and
growth of new modules with respect to incoming
incremental data.
Preventing Damage to Old Memories: This problem
is tackled using a sampling method using pseudo data
inspired from Liu et al (2008). The algorithm
implemented summarizes data distribution in a cluster
map. Given neuron a in a GSOM of N neurons,
consider the closest neuron b, a,b Є N , their midpoint
is given by
2 . We generate a random set of
vectors around neuron a, bounded on both sides by
2 . These pseudo vectors generated during the

1107

training of batch k implicitly reconstruct the data used
to train batches 0 to k-1.

For ease of analysis, assume that d=1
1

Incremental learning in a hierarchy Let us consider
Figure 2, where the modules and are child modules
of .. At batch 0, the training sets xα and xβ, consisting
of p0 patterns each are used to generate the neurons yα
and yβ.
iЄp0,

,

||

,

1

′

′

′

′

′

(1)

,

is passed to node . The vectors xα, xβ and
discarded.

are then

Where

,
(3)

1
1
′

Let
(a)

We obtain

(b)

√2√1

Figure 2. (a) Incremental learning stages. At batch 0, the training
patterns at level 1,
and cluster to form the neurons
. For simplicity, we consider that only one neuron is generated
after training batch 0. (b) Batch 1 and the approximation of the
pseudo vectors ,
and

,

,

||

(2)

,

However, during the training of batch 1, the measure
for
should be the distance to ′ and ′ , the
updated neuron vectors. A set of adapted pseudo
vectors ′ should therefore be approximated.
In Euclidean space, we can we can visualize the
problem as shown in Figure 3,

y'
D

x

R

θθ
d

y

Figure 3. Approximation of incremented pseudo vector for levels
2 and above in the hierarchy

We consider two cases, (a) y’ is not the winning
neuron for the pattern x. (b) y’ is the winning neuron.
Case (a).

is not the winning neuron, i.e, R<<d

(4)
,,

Where

When batch 1, consisting of p1 vectors is now
and are approximated from and
introduced,
respectively and used along with the new batch to train
the GSOM modules α and β. After training, the neurons
and
adapt to ′ and ′ . A
of the level 1 nodes
set of pseudo data are approximated from the neuron
.
From equation 2,
represents the Euclidean of
and
from and respectively, i.e, for a set of p0’
pseudo data,
iЄp0’,

′

and

1
(11)

sin

2

1

2
(5)

Substituting (5) into (4), we obtain
2 1

(6)

Substituting (6) into (3), we obtain
1

(7)

Solving (7), we have
1

(8)

2 , we
obtain
1
. In implementation, to satisfy (8) we
use the inequality (9) to assign the value of .
Based on Figure 3, if we approximate a

2

=

(9)

We can therefore conclude that, a value specified
by inequality (9) can be used to re-generate the dataset
, where ,
′ and ′
, 0,1, … , , …
,
is not the winning neuron.
Case (b):

is the winning neuron

If y’ is the winning neuron, a random value of ,
0
can be used to regenerate ,
,
′ |, where y′α is the winning neuron.

1108

Dealing with the problem of new input dimensions:
A rule based approach of creating a new module to
process the new keywords is preliminarily proposed to
deal with the dynamically increasing input dimensions.
A module is trained and connected to parents only if the
number of concepts that it represents increases above a
predefined threshold. In order to avoid overcrowding,
heuristic rules have been put into place such that a
parent has atmost three children.

concept cluster, the concept of penguin is separate from
that of other clusters. This is shown in Figure 6.

Experimental results
To illustrate the cognitive properties of the training
model, we train the system using 21 concepts from 200
documents. The concepts included ideas such as “birds”,
“animals”, “flowers”, “trees” etc, same as the ones used
by McClelland and Rogers (2003). The following
preprocessing was performed to the documents. First,
the contents were analyzed and the stopwords removed.
The concept terms were stemmed and grouped using
Wordnet (Fellbaum, 1998) before a tf.idf weighing
scheme was used to select the most relevant concepts to
the batch. For visualization purposes,
Hierarchical identification of concepts from wiki
documents

Figure 4. The number of concept neurons at the top of the
hierarchy vs. the number of features per bottom level module

In this section we observe how our hierarchical model
captures the properties of semantic cognition outlined
by McClelland and Rogers (2003, 2008). The training
data used by McClelland and Rogers is intuitively
designed based on common sense knowledge. Our
system, on the other hand is trained using information
from 200 text descriptors of the concepts from
wikipedia. The snippets varied in length from 50 word
descriptions to 500 word descriptions. Figure 4
illustrates the number of concept neurons at the top
level as a function of the ratio of the number of features
to each level 1 module and the total number of features.
When there are only two layers in the hierarchy, a
larger number of concept neurons (16) are generated.
The number of concept neurons converges to between 6
and 8 for all other architectures. Typically, for a six

Figure 5. Euclidean distance between various concepts vs. the
number of training epochs

In Figure 5, we observe the evolution of the
Euclidean distance between concepts. The training
shows empirical properties of convergence. The
distances between the various concepts are stable after
500 epochs of training. We can also observe promising
results from the concept representation point of view.
For instance, the Euclidean distance between “pine”
and “oak”, for instance, is larger than the Euclidean
distance between “birch” and “oak”, which belong to
the same family.
Figure 6.a shows the top two levels of a five level
hierarchy of hierarchy of concepts obtained (10
concepts per GSOM module and 160 concepts used in
training). We observe, as is the result in McClelland
and Rogers that similar concepts tend to be near each
other in space. For instance, “canary” and “sparrow”
tend to be closer to each other, but far from “penguin”.
In some cases, super ordinate terms, such as “bird”,
“tree” etc are mined as part of the hierarchy. There are
some interesting observations that can be made here.
We can see that the highest level (level 5) shows
general concepts while level 4 shows the concepts one
level lower. i.e., while the neuron 1 refers to “animals”,
the neuron box “2” refers to more detailed
differentiation of neuron 1. Further to this, the system
also shows some intermediate level categorization
characteristics that taps item frequency effects. In
McClelland and Rogers’ paper, they describe it as the
process by which certain descriptive terms such as
“tree”, “bird” and “dog” tend to be acquired earlier than
the super ordinate terms such as “plant” or “animal” or
more specific terms such as canary, pine or poodle. The
general consensus for this is that parents use certain
intermediate level words more frequently when
speaking to children. As such, intermediate concepts,
based on their frequency of usage, are also clustered
more tightly into intermediate groups within super
ordinate concepts.

1109

In the experimental
e
data,
d
some conncepts were ussed
more frequuently than oth
hers of the sam
me sub category.
These incllude birch (12 instances) vs.. 10 instances of
pine and maple,
m
16 instaances of rose vs.
v 3 instances of

daisy and
a 7 instancess of sunflowerr. It is seem thhat the
more frrequent conceppts are tied toggether with thee super
ordinatte concept neuuron (dog is tieed with animal, rose
with floower, sparrow with bird) andd so on.
cod

goat
dog

pig

flounder

salmon
sunfish

tulips
Saalmon, fish, cod
sunflower
roses

(b)

daisy
Roses, flowers,

Dog, animals
penguin
oak, tree

Birch, tree

Birch/oak
Sparrow, birds

birch
pine

pine
oak

maple

(a)

(c))

Figure 6 (a) Hierarchical representation off the 21 concepts in the memory syystem. (b) Groupiing due to second
dary characteristiics –
mon etc. (c) Associiative retrieval off concepts based on
o activation of hiigh correlation neeurons
“leaves” grooups all trees, “reed” for rose, salm
in the hierarchy.

At the lower
l
level, we
w can also observe
o
alternaate
similarity grouping wh
hich are basedd on individuual
features. For
F instance, at
a level 1 moodule, one migght
observe neeurons denotin
ng concepts such
s
as “leavees”
grouping birch,
b
maple and
a oak. The concept “grow
w”
groups objects of all cateegories. “Red” indicates objects
that are redd – rose, cod, salmon. One such lower layyer
representattion is shown in
i Figure 6.c.
From thhese results, one
o can expecct this model to
perform a sort of associiative retrieval of the kind thhat
humans peerform. For insstance, if one sees the conceept
“gold”, one would th
hink of perhaaps “ring” and
a
subsequenttly “marriage”” and then “fam
mily”, “childreen”
and so fortth. In the samee way, firing off the neuron “rred”
at level 1 will
w lead to firring of correspponding nodess at
various levvels. This willl lead to otherr concepts beiing
fired, bothh sequentially
y and parallelyy. For instance,
probing thhe model with
h “red” leads to
t the sequenttial
firing as deescribed in Fig
gure 6.b.
Each cooncept can, in
n turn, be usedd as a probe to
activate reelevant neuron
ns. In this seense, the moddel
describes the
t human chaain of thought process.
p
Workk is
in progresss to study how
w this process can be modelled
and how the
t firing can
n be controlledd by the wiriing
strength beetween modulees.

Incremen
ntal learning performancce
Overview of incrementtal learning with
w
5 concep
pts
and 3 battches: In this section, insteaad of introduciing
all the cooncepts are on
ne go, conceppts are learnt in
batches. The
T figure belo
ow shows the evolution of the
t
incrementaal concept nettwork at the top
t level for the
t
first three batches (a) tu
ulips and sunfflowers only (b)
(
sparrow annd sunfish (c) salmon.
s
In batcch 1, the top levvel

f
generallizes, creating concept repreesentation of flower
and aniimal. Lower laayers now portrray the differenntiated
concepts. In batch 2, the conceept of salmonn was
introduuced. At this juuncture, the old informationn from
sunfishh is sub grouuped under fiish and sparrrow is
consideered a separatee entity. These experiments suuggest
merit in
i the approximations that we have desscribed
earlier in the paper.

Figgure 7. The top-levvel evolution of in
ncremental conceept
repreesentation. (a) Battch 0 tulip and su
unflower. (b) Batcch 1:
sparrow and
d sunfish. (c)Batch
h 2: salmon

Hierarrchical represeentation in in
ncremental leaarning
In this experiment, concepts
c
weree introduced one
o by
b
withh “birch” andd “cod”. At some
one, beginning
juncturre concepts arre reintroducedd to investigaate the
effect of
o new data annd rehearsal onn old data. Soome of
the resuults obtained arre discussed inn this section.
Figurre 8a shows thhe evolution of
o how the conncepts
“birch”” and “betula”” are representted (“Betula” is the
scientiffic name for “birch”).
“
Birchh is the first cooncept
that was introduced to the system
m. At batch 0, the
distincttion between the
t concepts “birch”
“
and “bbetula”
appearss in level 3 of
o the hierarcchy. To the syystem,
“betulaa” and “birch” are two distinnct concepts, though
t
with a low Euclideann distance of 39.81.
3
When batch 2
is introoduced, the disstinction betweeen the two conncepts
moves one level loweer to level 2 annd eventually too level
1. How
wever, as morre concepts arre introduced to the
system, the presence of the new infformation makkes the
system lose the distinnction betweenn the concepts “birch”

1110

and “betula”, and the Euclidean distance between the
concepts reduces to 0 at batch 7. However, at batch 10,
when the concept of birch is reintroduced, the
Euclidean distance between the two terms increases

before gradually decreasing to 0 once again. A similar
result is also observed in the relationship between terms
“canary” and “islands”.

Figure 8. Evolution of the hierarchical and Euclidean relationship between the concepts (a)“birch” and “betula” vs “canary” and
“islands” (b) “Birch” and “cod” vs. “birch” and “dog” (c) “dog” and “flounder” vs. “goat” and “dog”

Figure 8b shows the representation of the concept
“birch” with respect to the concepts “dog” and “cod”.
“Birch” is introduced to the system at batch 0, “cod” at
batch 1 and “dog” at batch 4. The differentiation
between the concepts “birch” and “cod” is at level 4
and converges to level 3. By batch 8, the concepts of
“dog” and “cod” are of the same distance from “birch”.
At this juncture, the system at level 3 no longer
distinguishes between “birch”, “cod” and “dog”, but
makes a distinction between “plant” and “animal”.
Figure 8c shows a similar relationship of concepts “dog”
with the concepts “flounder” and “goat”. The flounderdog distinction converges to level 2 (from Figure 8b,
we can see that the plant-animal distinction occurred at
level 3) while the dog-goat distinction converges to
level 1. The Euclidean distance between the concept
terms “dog” and “goat” converges to approximately
700 which is close to the value that is obtained through
batch learning (from Figure 5).

Conclusions and further work
In summary, our model attempts to propose a
hierarchical Hubel Weisel model for the acquisition of
concepts from text such that the concepts are
represented in a hierarchical connectionist network. The
result is a new framework that we have applied in two
scenarios. The first is concept acquisition where we
have shown that the system is able to represent
everyday concepts in a hierarchical fashion, in a
manner similar to the PDP model. The system was
interestingly also able to perform chain retrieval, in that
when “red” was given as a probe to the system, it was
able to retrieve “robin” and by association “sparrow”.
Secondly, we have modeled information approximation
and incremental learning, which models some
properties of short term memory.
There are several directions for further work in this
area. In addition to the pertinent issues of improving
computation time and processing algorithms to make
the system able to handle large sets of data, one
important direction is the incorporation of semantic
information into the hierarchical architecture. As of

now, this information is ignored and only the statistical
properties of keywords are taken into consideration in
the generation of the concept hierarchy. Work is under
process to integrate semantic information into the
model. Work is also under progress to include common
sense knowledge in the model. We expect that these
additions will make the model more cognitively
accurate. In addition to this, we are also incorporating
other aspects of cognition such as attention; interest etc
to study the generation and behavior of the cognitive
map.
References
Cadieu, C., Kouh, M., Pasupathy, A., Conner, C., Riesenhuber, M., &
Poggio, T.A. (2007). A Model of V4 Shape Selectivity and
Invariance. J ournal of Neurophysiology 98, 1733-1750
George D, Hawkins J (2005), A hierarchical Bayesian Model of
Invariant Pattern Recognition in the Visual Cortex, International
Joint Conference on Neural Networks, 3, 1812-1817
Vernon Mountcastle (1978), An Organizing Principle for Cerebral
Function: The Unit Model and the Distributed System, The
Mindful Brain (Gerald M. Edelman and Vernon B. Mountcastle,
eds.) Cambridge, MA: MIT Press.
Hubel D and Weisel T (1965), Receptive fields and functional
architecture in two non striate visual areas (18 and 19) of a cat,
Journal of NeuroPhysiology 28, pp229-289
Fukushima K(2003), Neocognitron for handwritten digit recognition
1, Neurocomputing , 51C, 161-180
Alahakhoon D, Halgamuge S K, Srinivasan B (2000), Dynamic Self
Organizing maps with controlled growth for Knowledge discovery,
IEEE Transactions on neural networks, 11(3), pp601-614
Liu M, Liu Y C, Wang X L (2008), IGSOM: Incremental clustering
based on self organizing map, International Conference on
Intelligent Information hiding and multimedia Signal Processing,
IIHMSP’08, pp 885-890
Fellbaum C (1998), Wordnet: An electronic lexical database, MIT
Press
Mc Clelland J L and Rogers T T (2003), The parallel distributed
processsing approach to semantic cognition, Nature Reviews
Neuroscience, 4(4), pp310-322
Sloutsky VM (2003), The role of similarity in the development of
categorization, Trends in Cognitie Sciences, 7, 246-251
Rogers T T, McClelland J L (2008), Precis of Semantic Cognition, a
Parallel distributed Processing approach, Brain and Behavioral
Sciences, 31, pp 689-749

1111

