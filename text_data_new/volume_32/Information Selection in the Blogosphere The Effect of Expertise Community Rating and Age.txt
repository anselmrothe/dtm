UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Information Selection in the Blogosphere: The Effect of Expertise, Community Rating, and
Age

Permalink
https://escholarship.org/uc/item/4v26g8fp

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Winter, Stephan
Kramer, Nicole C.
Appel, Jana
et al.

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Information Selection in the Blogosphere:
The Effect of Expertise, Community Rating, and Age
Stephan Winter (stephan.winter@uni-due.de)
Nicole C. Krämer (nicole.kraemer@uni-due.de)
Jana Appel (jana.appel@stud.uni-due.de)
Kathrin Schielke (kathrin.schielke@stud.uni-due.de)
University of Duisburg-Essen
Department of Social Psychology: Media and Communication
Forsthausweg 2,
47057 Duisburg, Germany
research on selective exposure focused on content features
such as the relevance of the topic (e.g. Knobloch, Zillman,
Gibson, & Karrh, 2002; Zillmann, Chen, Knobloch, &
Callison, 2004), information on the authors has not been
taken into account yet. Similarly, models of online
information seeking (e.g. Pirolli & Card, 1999; Schamber &
Bateman, 1996; Tombros, Ruthven, & Jose, 2005) consider
factors like title, currency and layout. With respect to
theoretical modelling, it can be asked whether these models
have to be amended by aspects of social cognition with
regard to the authors.
Against this background, we wanted to investigate the
effect of source cues – self-reported expertise, community
rating, and age of authors – on the perception of credibility
and the selection of online science information. Who do
Internet users trust? And whose information do they select?
Our examination focuses on weblogs (or blogs), which can
be defined as “frequently updated websites where content is
posted on a regular basis and displayed in reverse
chronological order” (Schmidt, 2007). These websites are
popular means of science communication in the Web (e.g.
www.scienceblogs.com). Therefore, they are increasingly
used by laypersons for obtaining information on sciencerelated issues. As exemplary scenario for our study, we
chose the discussion on the effects of violent media contents
on children and adolescents.

Abstract
The World Wide Web offers a lot of information that has
been provided by laypersons instead of experts or
professional journalists. This raises the question how Internet
users perceive credibility of online authors and which
information on the source influences the users’ selection and
processing of texts. Our study investigated the effect of selfreported expertise, community rating, and age of weblog
authors. In an online laboratory experiment, information
seeking behavior of 60 participants on a science weblog was
analyzed. As exemplary scenario, the discussion on the
effects of violent media contents on children was chosen.
Results showed that authors with a high level of expertise
(operationalized by the author’s self-reported profession)
were rated as more credible and their texts were selected for
further reading more frequently. This suggests that selfreported expertise emerges as a strong cue for information
selection, whereas there was only partial evidence for the
importance of community ratings.
Keywords: Credibility, Selective Exposure, Persuasion,
Source Cues, Information Processing.

Introduction
The Internet is today’s largest source of information and
communication. As Metzger (2007) points out, “more
information from more sources is available and more easily
accessible now than ever before” (p. 2078). Although this
can definitely be seen as a major advancement, it might also
lead to the problem that users get lost in the digital world
and do not know how to find the content they need, e.g.
when searching for information on science related issues.
This phenomenon of “information overload” in the Internet
(Eppler & Mengis, 2004) has brought new attention to the
issue of credibility and quality of information – especially
since the World Wide Web is rapidly developing in the
direction of user-generated-content (Web 2.0, O’Reilly,
2005). For example, in blogs and forums “any user can say
anything about any topic” (Van der Heide, 2008,
p. 30). Thus, one can increasingly find information that has
been provided by laypersons instead of experts or
professional journalists and therefore may be less reliable.
This raises the question how Internet users perceive
credibility of online authors and which information on the
source influences the users’ selection and processing of
information in the World Wide Web. While previous

Credibility and Information Selection
in the Web
While several studies examined the general credibility of the
Internet as a medium (Stavrositu & Sundar, 2008; Metzger,
Flanagin, Eyal, Lemus, & McCann, 2003) or the credibility
of different web sites as a whole (e.g. Walther, Wang, &
Loh, 2004), this study focuses on the credibility of authors
within a certain web site, which means that the analyzed
message sources here are persons. According to the theory
of social information processing (Walther, 1992),
impressions
of
persons
in
computer-mediated
communication are formed on the basis of verbal, linguistic,
and textual manipulations – even though a lot of
information that would be visible in face-to-face
communication is missing. These impressions, primarily
based on text-based cues, accrue over time and lead to a

802

relatively elaborate evaluation of other persons. In this
context, Walther (1996) stated that, due to the absence of
other cues, basic personal information might even be more
important than in face-to-face situations (hyperpersonal
communication).
Van der Heide (2008) distinguishes between system
generated cues (e.g. the number of posts in a forum or the
number of friends on the social networking site Facebook),
aggregated feedback systems (such as reputation or rating
systems) and self-disclosure behaviors (e.g. self-report of
profession and age) as relevant types of heuristically
valuable information about computer-mediated message
senders. While system generated cues and aggregated
feedback systems are based on information that has been
provided by other users or the computer system itself, selfdisclosures are easier to manipulate by the authors
themselves. This means that someone might claim to be an
expert although he is not. On the other hand, selfdisclosures are “an efficient, direct, and visible method of
communicating one’s qualification” (Van der Heide, 2008,
p. 24) and might therefore be particularly important.
As “authority is no longer a prerequisite for content
provision on the Internet” (Metzger, 2007, p. 2078), it
seems reasonable that people use these information on the
author and his/her estimated credibility as a criterion for
information selection. However, it has not been analyzed yet
if these cues are a relevant factor for laypersons who are
seeking information on science-related everyday issues in
the Internet.

considerations on persuasion research, we hypothesize that
the information on the expertise of the author influences
rating of the source and selective exposure to the
corresponding message:
H1a: Sources with a high level of self-reported expertise
will be perceived as more credible.
H1b: The texts of authors with a high level of self-reported
expertise will be selected more often than the ones of the
low-expertise-sources.
Expertise Attributed by Others (Community Ratings)
Next to self-reports, expertise can also be expressed through
the statements of other users. Therefore, collaborative
filtering, e.g. rating systems (1 to 5 stars) or popularity
indications (most e-mailed, number of views), is also likely
to influence information choice. As these ratings are
difficult to manipulate, they provide valuable information
on the qualities of the user. Walther et al. (2009) showed
that comments of friends on social networking sites are even
more important for impression formation than selfgenerated statements. Furthermore, according to Chaiken
(1987), people use the heuristic that, if many agree with an
opinion, the opinion is probably correct. In this line,
community ratings should produce a bandwagon effect
(Sundar & Nass, 2001) in that articles or elements which
already have a positive rating are clicked more frequently.
On the other hand, individuals sometimes seek
distinctiveness from others (Brewer, 1991), which would be
an explanation for the opposite effect. Previous research has
supported the idea of the bandwagon effect: In an
experiment on selective exposure, Knobloch-Westerwick,
Sharma, Hansen, and Alter (2005) found that online articles
with better explicit recommendations were read longer.
Additionally, Resnick, Zeckhauser, Swanson, and
Lockwood (2006) showed that sellers with a high rating at
the auction website Ebay were able to sell products for
higher prices than users without a positive reputation.
In this context, we hypothesize that:

Expertise
Persuasion research in the tradition of the Yale studies (e.g.
Hovland, Lumsdaine, & Sheffield, 1949) shows that
messages presented by persons with a high level of
expertise are more likely to influence other people (Wilson
& Sherell, 1993). Therefore, dual-models of information
processing (Chaiken, 1987; Petty & Cacioppo, 1986)
include expertise of the source as one major factor – which
is especially relevant if the level of elaboration is low.

H2a: Authors with a high community rating are perceived
as more credible than authors with a low community rating.
H2b: Texts of sources with a high community rating are
more likely to be chosen.

Expertise Communicated via Self-Report On a weblog on
science-related issues, self-reports, which may consist of a
short self-description and the profession of the author, are
able to provide important cues on the expertise of the
author. This information is able to serve as a heuristic
(“experts are usually correct”). As humans are cognitive
misers (Fiske & Taylor, 1991) who do not include more
cues than necessary for their decisions, it seems plausible to
assume that this aspect is already relevant for the selection
of information. For an investigation of newsbots such as
Google News, Sundar, Knobloch-Westerwick, and Hastall
(2007) demonstrated that source credibility cues (name of
the medium in which a certain article was found, e.g. New
York Times vs. tabloid newspaper) – which can be seen as
an equivalent of expertise information on the level of
persons – influenced perceived message credibility and
likelihood of clicking. Following these results and

Social Comparison (Age)
Furthermore, social comparison (Festinger, 1954) may be
relevant for selection. According to Festinger’s theory,
people are motivated to evaluate their opinions and abilities
in comparison to similar persons, e.g. people with the same
socio-demographic background (age, gender, education,
etc.). The (positive or negative) results of this comparison
process have been shown to influence self-evaluations and
behavior (Mussweiler, 2001). In order to gain information
that is relevant for social comparison, people should choose
content that is connected to similar persons. In an
experiment with an online news magazine, Knobloch-

803

Westerwick and Hastall (2006) already demonstrated that
recipients more often choose news with protagonists of the
same sex and that young readers prefer texts about sameage-characters. As similar effects can be expected for text
authors, we hypothesize that:

between authors and texts were systematically rotated to
avoid effects of the different topics and formulations.

Dependent Measures
As dependent measures, information selection and rating of
the information and the source’s credibility were assessed. It
was coded which of the texts were chosen (in which order)
and how long the texts were read. Furthermore, it was
assessed whether the participants decided to get more
information on the author. Credibility was measured with a
scale based on research by Berlo, Lemert, and Mertz (1969)
and Gierl, Stich, and Strohmayr (1997), including items like
“trustworthy”, “experienced” and “altruistic”.

H3a: Users perceive sources of similar age as more
credible.
H3b: Users choose texts that were written by sources of
similar age.

Method
Sample
In order to investigate these hypotheses, we created an
online laboratory experiment in which 60 German
participants were asked to search for information on a
science weblog. As exemplary scenario, the website dealt
with the controversy on the effects of violent media contents
on children and adolescents. To ensure that this topic was
personally relevant, participants were parents with children
between the age of 2 and 18. Subjects were recruited via
different channels, e.g. newspaper ads, postings in forums
for parents and flyers which were distributed in schools.
Participants (30 female, 30 male) were between the age of
22 and 47 (M = 36.93; SD = 6.54). 26.7 % of them had a
university degree, 31.7 % finished high school with a
qualification for university entrance and 41.7 % finished
high school without this degree.

Figure 1: Screenshot of the weblog
(Title: “Violence in the Media”)

Stimulus Material

Procedure

As stimulus material a blog platform (see figure 1) was
created. On the overview page, 16 summaries of articles
(with a headline, short description and information on the
author) were shown. By clicking on the summary, the user
was able to read the whole article – furthermore, it was
possible to get more information on the author.

Data were collected in a laboratory at the University of
Duisburg-Essen. First, the participants filled out an online
questionnaire in which their previous knowledge on the
topic, their media usage, need for cognition (Cacioppo &
Petty, 1982) and self-efficacy with regard to Internet and
Web skills (Eachus & Cassidy, 2006) were assessed. After
that, they were told to search for information on the topic by
reading the weblog. In order to create a selection situation,
time was limited to four minutes. The sessions were saved
with a screen-recording software. After that, the participants
filled out a post-questionnaire in which they rated the
credibility of the authors.

Independent Measures
As independent measures, the information on the author
(self-reported expertise, rating, age) was systematically
varied as within-subject factors. Expertise was
operationalized via profession (professions with a close
connection to the topic, e.g. psychologist (high) vs.
professions without a connection to the topic, e.g. banker
(low)). Sex, rating and age were also varied (rating: five or
four stars vs. one or two stars / age: 24-27 years vs. 42-45
years). As a result, there were 16 combinations of author
information that were shown below the headlines of the
summaries. For every combination, a fictitious “character”
was created (e.g. “Dr. Thomas Moos, 42, media scholar,
community rating: 2 out of 5 stars” or “Jens Kohwall, 27,
insurance broker, community rating: 5 out of 5 stars”).
Headlines and texts were written in a neutral tone (e.g.
“New studies on the effects of first-person shooters” or
“Survey on children’s media usage”), and connections

Results
Usage of the weblog
The participants of the study selected an average of 5.68
articles (SD = 1.99) during four minutes of reading time.
Average reading time per article was 28.60 seconds (SD =
12.52). 25 % of the participants wanted to see further
information on the author.

H1: Self-Reported Expertise
H1 predicted that authors with a high level of self-reported
expertise (with a profession that has a close connection to

804

the topic) are perceived as more credible (H1a) and that
their texts are selected more often (H1b). To test these
hypotheses, we conducted an analysis of variance
(ANOVA) with repeated-measures in which the values for
the authors were grouped according to their level of
expertise. This revealed a significant effect of self-reported
expertise on credibility ratings, F (1, 59) = 98.040, p = .000,
ηp² = .624. As table 1 shows, the credibility scores for highexpertise authors are higher than for the low-expertise
sources. Therefore, H1a has been supported by the data.

p = .087, ηp² = .049: Participants selected an average of 1.40
texts of high-rating-authors (SD = 1.01) in comparison to an
average of 1.15 texts of low-rating-authors (SD = .88).

H3: Social Comparison (Age)
H3 stated that the participants would perceive sources of the
same age as more credible and choose their texts more
often. For this analysis, the sample was separated into two
age groups (from 22 to 38 years and from 39 to 47 years)
via median split. With regard to credibility ratings (H3a),
the analysis of variance revealed a significant effect of the
author’s age, in the group of older participants (F (1, 29) =
14.920, p = .001, ηp² = .340) as well as in the group of
younger participants (F (1, 29) = 8.696, p = .006, ηp² =
.231). However, in contrast to our hypothesis, mean values
(see table 2) show that older authors were generally
perceived as more credible in both age groups. The effect of
author’s age on credibility rating was significant for the
whole sample, F (1, 59) = 23.041, p = .000, ηp² = .281. For
the number of clicks (H3b), no significant effects emerged.

Table 1: Descriptive statistics for the effect of
self-reported expertise on credibility score,
number of clicks and reading time (in seconds)
M
153.35

SD
19.45

N
60

Credibility Score
Low Expertise
Number of clicks
High Expertise

115.30

24.47

60

3.13

1.44

60

Number of clicks
Low Expertise
Reading Time (s)
High Expertise

2.55

1.53

60

79.43

34.69

60

67.67

41.32

60

Credibility Score
High Expertise

Table 2: Descriptive statistics for the effect
of age on credibility score
Sample

Reading Time (s)
Low Expertise

Age
22-38

For the number of clicks, ANOVA also revealed a
significant effect of expertise, F (1, 59) = 4.145, p = .046,
ηp² = .066. The mean values (see table 1) show that texts
that were attached to authors with a high level of selfreported expertise were selected more often for further
reading. This means that H1b can also be supported.
However, it has to be noted that the effect size is low.
With regard to reading time, the mean values (see table 1)
indicate that texts of high-expertise-authors were read
longer. However, ANOVA did not show a significant effect.

Age
39-47

Total
Sample

H2: Community Rating
H2 predicted that the participants prefer authors with a high
community rating. However, with regard to credibility
evaluations, no significant result was revealed (H2a). For
the number of clicks (H2b), the mean values indicate that
texts of authors with a high rating were selected more often
(M = 3.07; SD = 1.52) than texts of authors with a low
rating (M = 2.62; SD = 1.45). However, this trend was not
significant. As a result, H2 is not supported by these data. In
further exploratory analyses, we found that, if only the
authors with a low level of self-reported expertise are taken
into account, community rating has a positive, marginally
significant effect on the number of clicks, F (1, 59) = 3.020,

M

SD

N

Cred., Young
Authors

129.43

15.94

30

Cred., Old
Authors

133.63

17.16

30

Cred., Young
Authors

133.10

16.34

30

Cred., Old
Authors

141.13

18.25

30

Cred., Young
Authors

131.27

16.11

60

Cred., Old
Authors

137.38

17.96

60

Discussion
Against the background of the rise of Web 2.0 formats in
which a lot of content is produced by laypersons instead of
experts, we aimed to answer the question how online users
perceive credibility and which factors determine their
selection of online science information. For this purpose, the
present study investigated the effect of expertise (as selfreports and community ratings) and age of weblog authors.
Our analysis showed that self-reported expertise has a
strong influence on the perception of credibility: As
hypothesized in H1, the participants preferred texts of

805

authors who had a profession with a close connection to the
topic, e.g. psychologists or media scholars. Furthermore,
their texts were chosen more frequently for further reading.
These results are in line with studies from (offline)
persuasion research (e.g. Wilson & Sherell, 1993) and dualmodels of information processing (Petty & Cacioppo, 1986;
Chaiken, 1987) in which expertise of the source is one
important factor. From our findings, we can conclude that
expertise as heuristically valuable information is already
relevant in the earlier stage of information selection:
Following the heuristic that “experts are usually correct”,
online users assess the credibility of the author and the
estimated quality of the text before choosing an article.
While Sundar et al. (2007) showed that this is true for
newspaper sources, the present study indicates that expertise
cues are also relevant if the message sources are persons.
Therefore, it seems that online users prefer declared experts
to “normal” people (who may be personally concerned with
regard to the topic) even in websites that are dedicated to
user-generated-content.
However, statements of other users on the expertise of the
authors, expressed by community ratings (H2), did not have
a significant effect on credibility rating and information
selection. Obviously, the display of rating stars did not
produce a bandwagon effect, as it was found for online
articles (Knobloch-Westerwick et al., 2005) and for the
credibility of Ebay sellers (Resnick et al., 2006). This is all
the more astonishing as previous research on social
networking sites (Walther et al., 2009) has shown that
information given by other people is seen as more important
than self-descriptions. The lack of impact might be due to
the fact that it was not clear to the participants what exactly
the ratings indicated and by whom (e.g. how many people)
the evaluation had been given. The cue concerning selfreported expertise (profession of the author) has obviously
been more important because the participants trusted in the
correctness of these self-reports: It is also possible that they
perceived it as an objective fact (possibly verified by the
blog owner) rather than a subjective assessment made by the
author. Furthermore, the costs and consequences of the
decision to choose an article or not are smaller than e.g.
when deciding to buy a product on Ebay. As a result, the
considerations may be less careful, which would lead to a
decreased importance of community ratings. However, if
only the authors with a low self-reported expertise were
taken into account, community ratings produced a
marginally significant effect: Texts of authors with a high
rating were selected more often than texts with a low rating.
This suggests that community rating does not matter when
the level of self-reported expertise is high. But if the level of
expertise is low, ratings seem to make a difference in that
people with a better rating are selected more often.
Our analysis for H3 showed that the age of weblog
authors has a significant influence on credibility ratings and
that older authors are generally perceived as more credible.
This is in contrast to our assumptions that users prefer
sources of similar age, based on social comparison theory

(Festinger, 1954; Mussweiler, 2001). While KnoblochWesterwick and Hastall (2006) found a social comparison
effect on the selection of news articles according to the age
of protagonists, there seems to be no such effect for blog
authors. An explanation could be that users of a science
weblog are mainly concentrating on the quality of
information (which can e.g. be deducted from a profession
with a close connection to the topic, a high rating and
maybe higher age due to more professional experience)
rather than seeking personal information on the author.
Possibly, other websites, such as social networks in which
detailed personal information and pictures are included, are
more likely to foster social comparison processes (see
Haferkamp & Krämer, 2010). The effect that older authors
are seen as more credible may be explained by the topic of
“violent media effects”, in which experiences with childrearing are helpful. For other topics (e.g. pop music or
Internet technology), the relationship between age and
source credibility may be different.
In summary, self-reported expertise of the author emerges
as a strong cue for the perception of online science
information, whereas there is only partial evidence for the
importance of community ratings and age. In line with
Sundar et al. (2007), these results demonstrate that the
“information scent” of articles is not restricted to its content
or formal features (position or layout): Information on the
author, especially expertise, must also be taken into account.
In order to achieve further insights into these processes,
future research should investigate the effects of sources in
combination with other variables, such as different message
types and different levels of motivation of information
seeking. In the present study, texts have been written in a
neutral style, which might have created a slightly artificial
situation that differs from the normal situation in the
blogosphere. If variations of content are included, the
analysis of user behavior may show the interdependencies
between several important factors of information selection.

Acknowledgments
The present study was funded by the Deutsche
Forschungsgemeinschaft
(DFG,
German
Research
Foundation) in the Special Priority Program “Science and
the General Public” (Kr 2240/2).

References
Berlo, D., Lemert, J., & Mertz, R. (1969). Dimensions for
evaluating the acceptability of message sources. Public
Opinion Quarterly, 33, 563-675.
Brewer, M. B. (1991). The social self: On being the same
and different at the same time. Personality and Social
Psychology Bulletin, 17, 475-482.
Cacioppo, J. T. & Petty, R. E. (1982). The need for
cognition. Journal of Personality and Social Psychology,
42, 116-131.
Chaiken, S. (1987). The Heuristic Model of Persuasion. In
M. Zanna, J. Olson, & C. Herman (Eds.), Social

806

Influence: The Ontario Symposium (Vol. 5, pp. 3-39).
Hillsdale, NJ: Lawrence Erlbaum Associates.
Eachus, P. & Cassidy, S. (2006). Development of the Web
User Self-efficacy Scale, (WUSE). Issues in Informing
Science and Information Technology, 3, 199-209.
Eppler, M. J. & Mengis, J. (2004). The concept of
information overload: A review of literature from
organization science, accounting, marketing, MIS, and
related disciplines. The Information Society, 20, 325-344.
Festinger, L. (1954). A theory of social comparison
processes. Human Relations, 7, 117-140.
Fiske, S. T. & Taylor, S. E. (1991). Social cognition (2nd
Ed.). New York: McGraw-Hill.
Gierl, H., Stich, A., & Strohmayr, M. (1997). Einfluss der
Glaubwürdigkeit einer Informationsquelle auf die
Glaubwürdigkeit der Information. [The influence of
source credibility on information credibility.] Marketing
Zeitschrift für Forschung und Praxis, 19, 27-31.
Haferkamp, N. & Krämer, N.C. (2010). Social comparison
2.0. Examining the effects of online profiles on social
networking sites. Paper presented at the annual meeting of
the International Communication Association, Singapore.
Hovland, C. I., Lumsdaine, F. D., & Sheffield, F. D. (1949).
Experiments on Mass Communication. Princeton, NJ:
Princeton University Press.
Knobloch-Westerwick, S. & Hastall, M. (2006). Social
Comparisons with News Personae: Selective Exposure to
News Portrayals of Same-Sex and Same-Age Characters.
Communication Research, 33, 262-284.
Knobloch-Westerwick, S., Sharma, N., Hansen, D. L., &
Alter, S. (2005). Impact of Popularity Indications on
Readers' Selective Exposure to Online News. Journal of
Broadcasting & Electronic Media, 49, 296-313.
Knobloch, S., Zillmann, D., Gibson, R., & Karrh, J.A.
(2002). Effects of Salient News Items on Information
Acquisition and Issue Perception. Zeitschrift für
Medienpsychologie, 14 (N.F. 2) 1, 14-22.
Metzger, M. J. (2007). Making sense of credibility on the
Web: Models for evaluating online information and
recommendations for future research. Journal of the
American Society for Information Science and
Technology, 58, 2078-2091.
Metzger, M., Flanagin, A., Eyal, K., Lemus, D., & McCann,
R. (2003). Credibility for the 21st century: Integrating
perspectives on source, message, and media credibility in
the contemporary media environment. Communication
Yearbook, 27, 293-335.
Mussweiler, T. (2001). Focus of comparison as a
determinant of assimilation versus contrast in social
comparison. Personality and Social Psychology Bulletin,
27, 38-47.
O´Reilly, T. (2005). What Is Web 2.0 - Design Patterns and
Business Models for the Next Generation of Software.
Available:
http://www.oreillynet.com/pub/a/oreilly/tim/news/2005/0
9/30/what-is-web-20.html?page=5.

Petty, R. E. & Cacioppo, J. T. (1986). Communication and
Persuasion. Central and Peripheral Routes to Attitude
Change. New York: Springer Verlag.
Pirolli, P. & Card, S. K. (1999). Information Foraging.
Psychological Review, 106, 643-675.
Resnick, P., Zeckhauser, R., Swanson, J., & Lockwood, K.
(2006). The value of reputation on eBay: A controlled
experiment. Experimental Economics, 9, 79-101.
Schamber, L. & Bateman, J. (1996). User criteria in
relevance evaluation: Toward development of a
measurement scale. Journal of the American Society for
Information Science, 33, 218-225.
Schmidt, J. (2007). Blogging Practices: An analytical
framework.
Journal
of
Computer-Mediated
Communication, [On-line serial], 12.
Available:
http://jcmc.indiana.edu/vol12/issue4/schmidt.html
Stavrositu, C. & Sundar, S. S. (2008). If Internet credibility
is so iffy, why the heavy use? Cyberpsychology &
Behavior, 11, 65-68.
Sundar, S. S., Knobloch-Westerwick, S., & Hastall, M.
(2007). News cues: Do indicators of newsworthiness by
newsbots affect our perception of news stories? A crosscultural study in Germany, the Netherlands, and the U.S.
Journal of the American Society of Information Science
and Technology, 58, 366-378.
Sundar, S. S. & Nass, C. (2001). Conceptualizing sources in
online news. Journal of Communication, 51, 52-72.
Tombros, A., Ruthven, I., & Jose, J. M. (2005). How users
assess Web pages for information seeking. Journal of the
American Society for Information Science and
Technology, 56, 327-344.
Van Der Heide, B. (2008, May). Persuasion on the ‘net: A
synthetic propositional framework. Paper presented at the
annual meeting of the International Communication
Association in Montreal, Canada.
Walther, J. B. (1992). Interpersonal effects in computermediated interaction: A relational perspective.
Communication Research, 19, 52-90.
Walther, J. B. (1996). Computer-mediated communication:
Impersonal, interpersonal, and hyperpersonal interaction.
Communication Research, 23, 3-43.
Walther, J. B., Van Der Heide, B., Hamel, L., & Shulman,
H. (2009). Self-generated versus other-generated
statements and impressions in computer-mediated
communication: A test of warranting theory using
Facebook. Communication Research, 36, 229-253.
Walther, J. B., Wang, Z., & Loh, T. (2004). The effect of
top-level domains and advertisements on health web-site
credibility. Journal of Medical Internet Research, 6.
Wilson, E. J. & Sherrel, D. L. (1993). Source effects in
communication and persuasion research: A meta-analysis
of effect size. Journal of the Academy of Marketing
Science, 21, 101-112.
Zillmann, D., Chen, L., Knobloch, S., & Callison, C.
(2004). Effects of lead framing on selective exposure to
Internet news reports. Communication Research, 31, 5881.

807

