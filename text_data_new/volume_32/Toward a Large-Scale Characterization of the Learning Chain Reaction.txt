UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Toward a Large-Scale Characterization of the Learning Chain Reaction

Permalink
https://escholarship.org/uc/item/42r832m4

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Author
Samsonovich, Alexei

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Toward a Large-Scale Characterization of the Learning Chain Reaction
Alexei V. Samsonovich (asamsono@gmu.edu)
Krasnow Institute for Advanced Study, George Mason University, 4400 University Drive MS 2A1
Fairfax, VA 22030-4444, USA
Abstract
Designing an agent that can grow cognitively from a child to
an adult human level of intelligence is the key challenge on
the roadmap to human-level artificial intelligence. To solve
this challenge, it is important to understand general
characteristics of the expected learning process at a level of
mathematical models. The present work makes a step toward
this goal with a simple abstract model of a long-term learning
process. Results indicate that this process of learning is
characterized by two distinct regimes: (1) limited learning and
(2) global learning chain reaction. The transition is
determined by the set of initially available learning skills and
techniques. Therefore, the notion of a ‘critical mass’ for a
human-level learner makes sense and can be determined
experimentally.
Keywords: human-level artificial intelligence; self-regulated
learning; teachable systems.

Introduction
Since the 1956 Dartmouth conference, researchers are
taking seriously the challenge of creating machines capable
of general human-level intelligence, human-like learning
and self-improvement (McCarthy et al., 1955), yet the
distance toward this goal has hardly decreased since 1955
(in fact, it is practically difficult to evaluate to make any
judgment). Main progress made since the beginning is
visible in reassessment of the difficulties, in emergence of
new approaches, e.g., cognitive architectures (Newell, 1990;
Gray, 2007), and in a new understanding of the goal. Yet,
despite growing interest to the field, it is not clear what the
final goal is and what would be the impact of achieving it.
Here four different views and associated with them
schools of thought can be named: (1) computational
neuroscience, that tries to understand how the brain works
in terms of neurophysiological
mechanisms and
neuroarchitectures; (2) cognitive modeling, pursuing higherlevel computational description of human cognition and
behavior based on more abstract cognitive architectures; (3)
human-level artificial intelligence, aiming at generally
intelligent artifacts that can replace humans at work; and (4)
a new emergent paradigm (that can be called “machine
consciousness”, or “human-like learners”, etc.) aiming at
artificial minds that can be understood by humans
intuitively, that can learn like humans, from humans and for
human needs. All these are fundamental scientific problems.
While (1) and (2) are focused on understanding the roots of
human cognition, and therefore allow for validation of their
progress using brain-and-behavior data, (3) and (4) are
oriented toward creation of an expected phenomenon that
does not exist yet: a computational replica of the human
mind capable of human-like learning and cognitive growth.

From this point of view, defining a success criterion for
(3) and (4) is the key challenge on the roadmap to a humanlevel learner. Examples of practically inefficient guiding
criteria include ambitious global challenges like the Turing
test (Turing, 1950; Korukonda, 2003) and limited
challenges like beating humans in chess or poker. As a
result of the missing clear understanding of the overarching
goal, the original high spirit appears to be lost in specific,
incremental steps that together did not produce a quantum
leap and seem to lead nowhere. On the other hand, both,
computational neuroscience and microelectronics made
tremendous progress in recent decades. Today there is no
big mystery in functioning of the brain elements, in the
sense that the generally accepted view of the brain is that of
a natural information processing device. Capabilities of the
modern computing hardware are approaching or have
already exceed computational resources of the human brain,
therefore, hardware is not a bottleneck on the road to
human-level intelligence. Then, what are we missing?
Several recent conferences, e.g., the BICA symposia
(Samsonovich, 2008, 2009, members.cox.net/bica2010)
addressed this question. To summarize the situation: it
becomes clear that biologically inspired cognitive
architectures (BICA) provide the most promising approach
to creating a functional replica of the human mind, and the
key to solving this challenge is in replication of the human
ability to learn. In other words, having a machine that can
be taught virtually anything that a human child can be
taught would imply the achievement of the grand
overarching goal. Therefore, in order to successfully steer
research toward this goal, it is necessary (a) to better
understand principles and characteristics of human learning
at a big scale: e.g., in educational practice, rather than in
limited behavioral laboratory experiments, and (b) to match
the same characteristics of learning in artifacts.
From this point of view, the overarching goal should be
formulated as a challenge in terms of scalability laws
characterizing the learning process over a long period of
time, when previously learned knowledge and skills enable
the acquisition of new knowledge and new learning
techniques, and so on. This large-scale process of
bootstrapped learning can be visualized as a chain reaction
(e.g., as depicted in Figure 1). Understanding details of this
bootstrapped learning process, such as principles of selfregulated learning (SRL: Zimmerman, 2002), is vital for
achieving the goal. At the same time, description of the
general laws and their mathematical understanding is
possible at an abstract level. Indeed, this is a task of high
priority on the roadmap toward general human-level
artificial intelligence, or artificial human-like minds.

2308

Similarly to the history of the study of the nuclear chain
reaction, models and theories are needed to describe and
understand the learning chain reaction in detail in order to
make it achievable. The key task is to clarify the difference
between solutions leading to small incremental steps and
solutions leading to a big quantum leap, and to identify
conditions (e.g., the ‘critical mass’ of the initially available
capabilities, if it makes sense) that make the leap possible.
The present work presents an attempt to start developing
the necessary theory by constructing and simulating a
general model of large-scale learning of the above sort,
using learning of an abstract curriculum as an example.

Methods
First, in this section a challenge scenario is outlined that can
serve as a goal for designing a general-purpose human-like
learner and at the same time as an example justifying
abstract modeling. Then, based on this scenario, an abstract
model is constructed that will be studied numerically in the
next section.

An Example Challenge Scenario

(examples: an integer number, a variable, a function) and
gradually moving to their practical usage and to complex
constructs based on them, and (ii) to demonstrate
improvement of meta-learning skills over time, in particular,
SRL skills (Zimmerman, 2002). The agent performance
improvement over time at the task level will be measured
using standard tests and metrics used for evaluation of
student academic progress. The agent will be evaluated not
only based on its task-level learning and problem solving
performance, but also based on changes in the approach to
problem solving, using general metrics for SRL (Winne &
Perry, 2000). A particularly interesting question relates to
the measure of initial intelligence (the “critical mass”) that
enables learning of the curriculum.
Generalizations: This example of a challenge scenario
entails a generalization. Taking a step to the abstract level, it
makes sense to introduce of elements of an abstract
curriculum as a system of interrelated abstract units (skills,
facts, etc.) that the agent may have the ability to learn,
depending on its current knowledge and experience. This is
done below.

The following challenge scenario for an artificial learner can
be viewed as an operational definition of certain key aspects
of the human learning ability that needs to be replicated in
artifacts: in particular, scalability, robustness, cross-domain
transferability, and most importantly, its metacognitive
nature. This paradigm will also allow experimenters to
measure the “critical mass” of initial knowledge and skills
that enable human-level bootstrapped learning.
Settings: The agent is embedded in a virtual learning
environment with the study material (the textbook and
supplementary materials normally available to students)
encoded electronically and made available to the agent. All
interactions between human instructors and the agent are
mediated by an interface implemented at a symbolic level.
The agent is expected to make progress in study of the
curriculum, being taught by human instructor(s) via the
high-level symbolic interface. The domain of study can be
limited, e.g., to high school algebra. The agent together with
instructors will go through chapters of the textbook (e.g.,
Algebra 1 and Algebra 2: Larson et al., 2007a,b) in their
electronic representation, will learn new concepts, will ask
questions, will take quizzes, will do exercises, and will be
able to explain the learned material.
Approach: The approach to implement an artificial student
capable of learning how to solve high school algebra
problems can be based on a biologically-inspired cognitivemetacognitive architecture implementing principles of SRL
(e.g., Samsonovich, De Jong and Kitsantas, 2009;
Samsonovich, 2010).
Metrics and Criteria: The challenge for the agent is (i) to
demonstrate a long-term ability to learn the curriculum stepby-step, starting from basic concepts available initially

T1
T8

T5

T9

T7

T2
T6

T3
T4
P1, S1

P2, S2

P3, S3

Figure 1: A possible curriculum structure. Solid circles
(black and blue) represent available techniques.

Designing an abstract Model
A simple abstract model of a learning process consistent
with the above scenario can be defined as follows. Elements
of the model are abstract problems P, abstract facts and
techniques T applicable to the problems, and abstract
solutions S understood as subsets of techniques associated
with problems. It is assumed that the set of techniques {T}
is ordered based on their mutual dependence, excluding a
possibility of circular dependence. This ordering can be
understood as resulting from a process of adding new
techniques one by one to the set, as follows. Given a set of
available i-1 techniques, the new technique Ti is defined as
an abstract function fi of a subset of the available i-1
techniques:

2309

Ti = f i ({T j : 0 < j < i, Wij = 1}),

W

ij

j

= m, (1)

where W is a randomly generated, sparse Boolean matrix
with zero elements for all i≤j. To simplify the modeling
study, it is assumed that each row of W (except the first m
rows) has exactly m nonzero elements that are uniformly
sampled in the part of the row before the diagonal. Each of
the first m rows has the maximal number of nonzero
elements satisfying (1). Therefore, a typical matrix W looks
like the example plotted in Figure 2 B.

Next, it is assumed that the set of problems {P} is ordered
similarly, and solutions S to problems are described by a
matrix W' defined by a duplicate of (1), only with a different
parameter m':

S i = {T j : 0 < j < i, Wij′ = 1},

 W ′ = m′,
ij

(2)

j

To simplify the study, each problem is assumed to have
exactly one solution, which is defined as a certain set of m'
techniques. Of course, in real life, knowing a set of
techniques that together are sufficient for solving a given
problem does not imply the ability to solve the problem: one
needs to know in what order and how to apply those
techniques. For now, however, we assume that this part of
solution is available automatically whenever each of the set
of techniques that are necessary to solve the problem is
mastered. Therefore, a typical matrix defining solutions of
problems looks like the plot in Figure 2 A.
Naturally, there should be also an abstract notion of
applicability of techniques to problems: each technique is
either applicable to a given problem or not. In this sense, a
solution must be a subset of techniques applicable to the
given problem. The matrix of applicability A = {Aij} tells us
whether Ti is applicable to Pj. The matrix A is again
generated as a Boolean random matrix with given sparsity.

Model Dynamics
From the learner’s perspective, each problem in this
simplistic model is either solved or not. Similarly, each
technique has three possible states: unavailable, available
(yet not mastered), and mastered. The model dynamic rules
of learning are defined as follows. A technique becomes
(and forever remains) available when all techniques on
which it depends are mastered. A technique becomes (and
forever remains) mastered when it is successfully used to
solve a problem. A technique can be used to solve a
problem when it is available. A problem that is already
solved is not considered again (indeed, with exactly one
solution for each problem, its consideration would change
nothing).

Experimental Paradigm

Figure 2: Examples of the abstract curriculum materials
used in simulations. A: Example set of solutions for each of
the 2,000 abstract problems. B: Example set of
dependencies among the 2,000 abstract techniques.

The abstract learning paradigm is the following. Given a set
of n techniques and n problems (having the same n just
simplifies the consideration), of which initially k techniques
are available and zero problems are solved, the learner tries
to solve as many problems as possible by randomly picking
problems that are not solved yet and trying to apply
available techniques to them. This is done in discrete steps.
At each step, one problem is selected, and if the solution is
contained within the set of available applicable techniques,
then the problem is considered solved, and those techniques
that constitute the solution become
mastered.
Simultaneously, states of all techniques are updated based
on the above stated dynamic rules: this may result in the
emergence of new available techniques. The process ends
after a fixed number of steps Nmax. The progress made in

2310

learning is measured by the final numbers of solved
problems, available and mastered techniques.

Simulation Results and Analysis
The model described above was simulated on a computer
with the following parameters. The number of techniques
and problems was n = 2,000, the maximal number of
dependencies for techniques m = 3, the maximal length of a
solution = 10, the sparsity of the applicability matrix = 0.05,
the number of steps Nmax = 50,000, and the number of
initially available techniques was k = 200 and k = 20 in two
sets of trials. The results were averaged over 10+10 trials.
Results of simulations are summarized in Figure 3.
Figure 3A shows the dependence of the number of finally
available solutions (blue) and techniques (red) as functions
of the number k of initially available techniques that was
varied gradually from 20 to 200. Figure 3B shows multiple
learning trajectories for two different values of k: 20 and
200. In this case, the numbers of available solutions (blue)
and techniques (red) are plotted as functions of the number
of learning attempts.
Results indicate two distinct regimes: “limited learning”
(k < 70) and “global chain reaction” (k > 100). As Figure 3A
shows, limited learning is characterized by the number of
finally available solutions and techniques that is
commensurate with the number of initially available
techniques. By contrast, the chain reaction regime is
characterized by the number of finally available solutions
and techniques that is close to the maximal given number of
solutions and techniques n. Figure 3B demonstrates clear
clustering of the learning trajectories corresponding to the
two regimes.
The qualitative observation of the two regimes is robust
with respect to variation of parameters of the model. The
transition is clearly visible in Figure 3 and shows a tendency
to become sharper as the number of learning steps and the
number of problems and techniques n are increased (not
shown in Figure 3). The null hypothesis, that dynamics of
learning produces results linearly increasing as a function of
the amount of initial intelligence, can be ruled out based on
the sigmoid appearance of the curve in Figure 3A: residuals
of the linear fit exhibit the standard error several times along
a substantial fraction of the curve (not shown in Figure 3A).
A more rigorous validation of the result will be presented
elsewhere.
Speaking generally, the observed chain reaction behavior
resembles the phenomenon of percolation, which is
characterized by threshold dynamics (a phase transition: see
Ziman, 1979). The present limited study, however, does not
allow for a detailed investigation of the phase transition
characteristics.

Discussion
Figure 3: Simulation results. A: Results of learning as a
function of initially available techniques. The dotted lines
indicate the standard error. B: Learning curves for different
numbers of initially available techniques.

The presented study and its results are limited in many
aspects. For example, from the cognitive psychological
perspective, this approach may seem to be fundamentally
lacking in an appreciation for the difficulty in understanding
human cognition and learning and how they vary as a
function of many variables (experience, context, personality
differences, etc.). It is nevertheless always reasonable to

2311

start with a simple general model, and then correct the
findings by including missing details. This was the
motivation behind the choice of the presented model and its
simulation. In the present version, the model and its
simulation results do not tell us precise details about how
humans learn throughout their lifetimes, and this was not the
intent. On the other hand, the present work opens a new
topic in cross-disciplinary discussions: only through
meaningful interactions between hardcore computer
scientists and mathematicians and psychologists and
cognitive neuroscientists we will be able to achieve the
overarcing goals described in the Introduction. Certainly,
more formal analysis informed by all related disciplines will
benefit artificial intelligence, psychology and neuroscience,
as well as other disciplines.
The study and its analysis presented here constitute a first
step of its kind, paving the way to finding general scalability
criteria for intelligent learning systems intended to achieve
the human level of performance. In this respect, it would be
very interesting to compare results of this study with
available data from educational studies and from machine
learning: this will be done elsewhere. The questions to be
addressed in future studies include: Do we see the same
kinds of limited learning and global learning chain reaction
regimes in human children? How this model might be made
less abstract and more related to existing specific models of
learning? How to include cross-discipline learning into the
model? How to specifically describe the role of
metacognition within the abstract formalism? And so on.
Returning to the topic of the Introduction, the present
study outlined a new kind of scalability criteria that will be
useful guiding the design of human-level learners. The
knowledge that learning dynamics have a threshold nature
allows in principle to identify the threshold (the “critical
mass”) for a given learning system based on its abstract
modeling and then to set the achievement of this “critical
mass” as the goal of development.

Conclusions
Designing an agent that can master one specific cognitive
skill in specific settings may be feasible today based on
traditional approaches in artificial intelligence, regardless of
the level of the selected cognitive skill. At the same time,
designing an agent that can grow cognitively from a child to
an adult human level of general intelligence is challenging.
To solve the challenge, it is important to understand the
different nature of the two tasks in terms of mathematical
characteristics of the expected learning process. The present
work addressed this goal with a simple abstract model of a
long-term learning process. Results indicate that this process
of learning is characterized by two distinct regimes: (1)
limited learning and (2) a learning chain reaction that
extends through the entire learning material.
The transition between the two regimes is determined by
the ‘mass’ of initially available learning skills and
techniques. Therefore, the notion of a ‘critical mass’ for a
human-level learner makes sense and can be determined

experimentally. Therefore, this criterion can be used to
guide research in human-level teachable systems.
Stated simply, while an intelligent learning agent may be
successful in learning starting from any amount of initial
knowledge, it needs to begin with a “critical mass” of
knowledge and skills in order to successfully self-teach to
learn much more than was conceived during its design – up
to the human level of general knowledge.

Acknowledgments
I am grateful to members of the GMU BICA team, Drs.
Kenneth De Jong and Anastasia Kitsantas, who fueled my
thinking about the problem addressed by this study. This
work is supported by the Center for Consciousness and
Transformation, George Mason University.

References
Gray, W. D. (Ed.) (2007). Integrated Models of Cognitive
Systems. Series on Cognitive Models and Architectures.
Oxford, UK: Oxford University Press.
Korukonda, A. R. (2003). Taking stock of Turing test: a
review, analysis, and appraisal of issues surrounding
thinking machines. International Journal of HumanComputer Studies, 58: 240-257.
Larson, R., Boswell, L., Kanold, T. D., & Stiff, L. (2007a).
Algebra 1. Evanston, IL: McDougal Littell.
Larson, R., Boswell, L., Kanold, T. D., & Stiff, L. (2007b).
Algebra 2. Evanston, IL: McDougal Littell. ISBN
9780618595419.
McCarthy, J., Minsky, M.L., Rochester, N., & Shannon,
C.E. (1955/2000). A proposal for the Dartmouth summer
research project on artificial intelligence. In Chrisley, R.,
& Begeer, S. (Eds.). Artificial Intelligence: Critical
Concepts. Vol. 2, pp. 44-53. London: Routledge.
Newell, A. (1990). Unified theories of cognition.
Cambridge, MA: Harward University Press.
Samsonovich, A. V. (Ed.). (2008). Biologically Inspired
Cognitive Architectures: Papers from the AAAI Fall
Symposium. AAAI Technical Report FS-08-04. Menlo
Park, CA: AAAI Press. ISBN 978-1-57735-396-6.
Samsonovich, A. V. (Ed.). (2009). Biologically Inspired
Cognitive Architectures II: Papers from the AAAI Fall
Symposium. AAAI Technical Report FS-09-01. Menlo
Park, CA: AAAI Press. ISBN 978-1-57735-435-2.
Samsonovich, A. V. (2010). A human-inspired cognitive
architecture supporting self-regulated learning in problem
solving. In: Raja, A. & Josyula, D. (Eds.). Metacognition
for Robust Social Systems: Papers from the 2010 AAAI
Workshop, AAAI Technical Report WS-10-07. Menlo
Park, CA: AAAI Press (forthcoming).
Samsonovich, A. V., Ascoli, G. A., & De Jong, K. A.
(2006). Human-Level Psychometrics for Cognitive
Architectures. In Proceedings of the Fifth International
Conference on Development and Learning (ICDL5), CDROM. Bloomington, IN: Indiana University, 2006.
Samsonovich, A. V., De Jong, K. A., and Kitsantas, A.
(2009). The mental state formalism of GMU-BICA.

2312

International Journal of Machine Consciousness 1 (1):
111-130.
Turing, A. M. (1950). Computing machinery and
intelligence. Mind, LIX: 433-460.
Winne, P.H., & Perry, N.E. (2000). Measuring selfregulated learning. In P. Pintrich, M. Boekaerts, & M.
Seidner (Eds.), Handbook of self-regulation. Orlando, FL:
Academic Press.
Ziman, J. M. (1979). Models of Disorder: The Theoretical
Physics of Homogeneously Disordered Systems.
Cambridge, UK: Cambridge UP.
Zimmerman, B. J. (2002). Becoming a self-regulated
learner: An overview. Theory into Practice 41 (2): 64-70.

2313

