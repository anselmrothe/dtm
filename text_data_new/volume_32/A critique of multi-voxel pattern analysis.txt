UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A critique of multi-voxel pattern analysis

Permalink
https://escholarship.org/uc/item/4914w4rs

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Anderson, Michael
Oates, Tim

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

A critique of multi-voxel pattern analysis
Michael L. Anderson (michael.anderson@fandm.edu)
Department of Psychology, Franklin & Marshall College
Lancaster, PA 17604 USA

Tim Oates (oates@cs.umbc.edu)

Department of Computer Science, University of Maryland, Baltimore County
Baltimore, MD 21250 USA
Abstract
Multi-voxel pattern analysis (MVPA) is a popular analytical
technique in neuroscience that involves identifying patterns in
fMRI BOLD signal data that are predictive of task conditions.
But the technique is also frequently used to make inferences
about the regions of the brain that are most important to the
tasks in question, and our analysis shows that this is a
mistake. MVPA does not provide a reliable guide to what
information is being used by the brain during cognitive tasks,
nor where that information is. This is due in part to inherent
run to run variability in the decision space generated by the
classifier, but there are also several other issues, discussed
here, that make inference from the characteristics of the
learned models to relevant brain activity deeply problematic.
These issues have significant implications both for many
papers already published, and for how the field uses this
technique in the future.
Keywords: neuroscience, machine learning, inference,
philosophical issues.

Introduction
Multi-voxel pattern analysis (MVPA) is an increasingly
popular analytical technique in neuroscience. MVPA
involves searching through the Blood Oxygenation Level
Dependent (BOLD) signal data produced in fMRI
experiments to identify patterns that are highly predictive of
task conditions. To illustrate, consider a simple experiment
in which participants are asked to view pictures representing
various object categories (e.g. faces, houses, chairs, shoes,
etc.). One early MVPA study showed it was possible to
determine, by looking only at BOLD data, which class of
object an experimental participant was viewing when that
data was collected (Haxby et al., 2001). The technique has
since been used to predict the orientation of lines being
viewed by a participant (Haynes & Rees, 2005), to
differentiate between lying and truth-telling (Davatzikos et
al., 2005), and to predict which action a participant was
about to take (Haynes et al., 2007), among many other
things (see Pereira, Mitchell & Botvinick, 2009; Norman et
al., 2006; Haynes & Rees, 2006 for reviews of the technique
and its applications).
This is indeed impressive, and we expect that MVPA will
have many important experimental and diagnostic
applications (Lao et al., 2004). It has become commonplace
to make certain inferences about the way differences in
BOLD signal patterns correspond to differences in mental
states. For instance, by finding the set of voxels that are
most predictive of a certain task outcome, studies have
claimed to discover the “cognitive states associated with
perception of tools and dwellings” (Shinkareva et al., 2008),

“localizable task-specific representations of freely chosen
intentions” (Haynes at al., 2007), and the regions of the
brain that “contain information” (Preston et al., 2008)
relevant to the cognitive or perceptual task under
investigation.
To put it bluntly, however, such inferences are at best
misleading and at worst entirely unwarranted. The issues
dovetail with, but are distinct from, the more general
concerns about the unreliability of “reverse inference” from
neuroimaging data (Poldrack, 2006), and have significant
implications both for how we ought to interpret some of the
many papers already published, and for how the field
applies this technique in the future.
Of course, not every MVPA study is governed by the
logic that we will criticize here. For instance, Mitchell et al.
(2008) take something like the opposite approach, and see if
they can predict the pattern of brain activity that will be
caused by listening to novel words. Here the point of the
study is not to discover which brain regions are responsible
for understanding; rather, they are testing the hypothesis that
meanings of words are based on sets of “semantic features”
that can be inferred from word co-occurrence in language
corpora. McDuff, Frankel & Norman (2009) are likewise
focused on hypothesis testing, in their case about the
characteristics of targeted memory retrieval. We think that
MVPA has a very promising future both as a diagnostic
tool, and as a useful dependent variable—in part because the
technique is sensitive to contingencies beyond classical
single-voxel effects—but that for the reasons outlined in this
paper it is a very poor tool for reliably localizing
information or identifying cognitive states.

Information and the brain
There are three general ways in which information could
inhere in the BOLD signal. First, the information could be
non-local, that is, carried by irreducibly relational features
of the signal like regional co-variance. We might expect this
to occur when large-scale neural synchrony is the relevant
aspect of brain activity (Varela, et al., 2001; Gross et al.,
2004). Second, it could be local and distributed, that is, the
information could be carried by the activity of individual
voxels, and the information-carrying voxels could be spread
throughout the brain. We might expect this for cognitive
processes that require the cooperation of many different
brain regions. Third, the information could be local and
concentrated, that is, carried by individual voxels that are
grouped together in one or a few clumps. This might
happen when the work done by local neural circuits is most
important to the cognitive task(s) in question. In this essay,
we will consider the performance of MVPA in all three

1511

ssituations, and discuss whatt can, and cannnot, be inferrred
f
from
features of
o the learned model
m
in each case.
c

L
Local,
distrib
buted inform
mation
Consider the problem of differentiatinng between the
C
t
f
following
two patterns
p
of hyp
pothetical voxeel-level activatiion
d
data,
each pressented with tw
wo versions of the same patteern
tyype (see Figurre 1). In this simple
s
example each of the 25
“
“voxels”
can be
b in one of tw
wo states (activve or inactive,, if
y
you
like). Supppose “brain scans” like these had beeen
o
observed
durinng an experimeent in which participants
p
weere
If
a
asked
to classiify pictures ass “living” or “non-living”.
“
thhese judgmentts reliably corrresponded to the
t two patternns,
r
respectively,
coould we use MVPA
M
to read the mind of the
t
p
participants?

Pattern typee 1, version 1

Pattern type 1, version 2

Pattern typee 2, version 1

Pattern type 2, version 2

Figure 1: Simple patterrns for use in MV
VPA test

Now, when thee ratio of patterrn versions wiithin each patteern
N
tyype is 1:1, every voxel is in
n both of its possible
p
states in
e
every
task conddition. That is:: no voxel is byy itself predictiive
o any cognittive state, and thus in thhis condition all
of
innformation is non-local. In
n this conditioon linear MVP
PA
c
cannot
distinguuish between th
hese two patterns; it is blindd to
n
non-local
inforrmation (Kamittani & Tong, 2005;
2
Normann et
a 2006). For linear classifieers, since the evvidence providded
al.
b each voxell is integrated
by
d separately, linear
l
MVPA is
s
successful
onlyy when there are individuall voxels that are
a
s
sensitive
to thee difference between
b
classes. In general, a
(binary) linear classifier overr an input space of dimensionn n
loooks like this:
preediction

siggn b

w

x

where the ith weight
w
w
is wi and
d the ith compoonent of the inpput
v
vector
(the listt of numbers th
hat describe thhe patterns to be
c
classified)
is xi and the bias value
v
is b. If the
t sum abovee is
p
positive,
the instance is classified one way;; if it is negativve,
thhe instance is classified
c
the other
o
way.
However, manipulating
m
th
he version raatio changes the
t
s
situation
from one in which no voxel is more
m
informatiive
thhan any other—
—a situation in
n which linear classifiers faill—
too one in whicch there is ind
deed a set of voxels,
v
scatterred
thhrough the patterns,
p
that are informattive about claass
m
membership.
T is to say, although theree is still non-loccal
That
innformation in the patterns—
—and it is arguaable that the noonloocal co-variancce structure is the crucial, rellevant distinctiion
b
between
these patterns—the initial test sittuation is one in

which there nevertheeless is also relevant local innformation,
distribuuted across manny voxels.
For our
o analysis off the performannce of MVPA with local,
distribuuted information, we gennerated 20 seets of 80
“scans””—that is, 20 datasets, eachh containing 400 instances
of eachh pattern typee. Patterns were
w
corruptedd with 5%
noise—
—a 5% chance for each voxell that it will bee in a state
inconsiistent with the pattern. For each
e
dataset, we
w used 40
of the 80 scans for training
t
and 400 for test, andd classified
them using a Support Vectoor Machine. Because
classifiication accuraccy roughly trackks the relative proportion
of patteern versions, ouur scans contaiined a 4:1 ratioo of pattern
versionns within eacch type, and classificationn accuracy
averageed 80%.
Thuss, our hypothetical experimennt would have produced
p
a
solid predictive success; we wouldd be able to teell, 80% of
the tim
me, which task condition
c
the participant
p
wass in just by
lookingg at the fMRI data. But whaat, if anything, would we
be perm
mitted to concluude about the local
l
neural coonditions—
represeentations, infformation conntent, activitty, etc.—
contributing to the differences
d
in cognitive
c
taskss (thinking
about or
o judging the difference between
b
livingg vs. nonliving things)?
t
Althoough any of thhe input compoonents could coontribute to
the preediction breakiing one way or
o the other inn an given
case (aand it needn’tt be the sam
me componentss for each
instance), in practice there can be a small numberr of voxels
that coontribute most to the classifi
fier performancce because
they (liiterally) carry the
t most weighht—that is, theey have the
highestt values of wi. In linear MVPA,
M
this sett of highly
weighteed voxels is coonsidered the “m
most informatiive”.
Figurre 2 shows a map of the voxels that were
w
most
informaative for distinnguishing between pattern typpes 1 and 2
in datasset 1.

Figurre 2: Most inform
mative voxels forr an MVPA classsification

i the proper interpretation of these resuults in the
What is
contextt of MVPA? These
T
are the voxels
v
that, hadd they been
in a diffferent state, would have beenn most likely too cause the
classifiier to place the pattern in the other class. Buut consider
the folllowing inferennce, an inferencce of similar structure
s
to
those being
b
made inn the MVPA literature:
l
if thhe state of
these voxels
v
had been different in the right way—
—and note
this piccture providess no informatioon about whaat the right
way iss—the brain would have been in the relevantly
differennt state (or thhe participant would
w
have been
b
in the
differennt cognitive sttate). This infference does not
n follow,
becausee if covariancce is the cruccial cognitivelly relevant
propertty of the activitty here, then all
a the other voxxels would
also bee different wheen the brain/participant is inn the other
state: thhey will be covvarying with a different set of
o partners.
And, evven if covariaance is not the crucial property—if the
relevannt informationn is the locall information—
—it seems
pretty clear
c
that it isnn’t all or onlyy the voxels in the “most

1512

innformative” seet that would need
n
to be in a different statee to
tuurn one patternn into the otherr.
Likewise, coonsider a similaar inference (vversions of whiich
c also easilyy be found in the literature):: the informatiion
can
c
contained
in thhese voxels is the informatioon crucial to the
t
d
difference
betw
ween the cogniitive states under investigatiion
(jjudging living vs. non-living
g things). This inference is allso
u
unwarranted,
foor similar reaso
ons. For one strong
s
possibillity
is that the relevvant informatio
on is carried by
b the covariannce
s
structure
of thee patterns, and
d this non-locaal informationn is
n contained in
not
i the set of “m
most informativve” voxels. And
A
e
even
if the locaal information is
i what is relevvant here, we can
c
s from the reesults above th
see
hat the set of most
m
informatiive
v
voxels
does nott consist of all or only the vooxels carrying the
t
r
relevant
inform
mation.
The uncertaiinty of inferen
nces about brrain or cognitiive
s
states
based onn which voxells are most higghly weightedd is
d
driven
home even
e
more strrongly when one
o looks at the
t
s
stability
of the set of highly weighted voxeels over multipple
trrials of the saame task. Fig
gure 3 shows the most highhly
w
weighted
voxells from the first three datasetss.

Figure 3:
3 Most informattive voxels for thhree different
classificcation runs

Obviously, the highly weighted voxels varyy from run to ruun.
O
T get a better quantitative handle on thee stability of the
To
t
h
highly
weighted voxel set, we
w counted the number of tim
mes
e
each
voxel waas among the top 10 most highly
h
weighteed.
O
Overall,
every voxel was in this
t set at leastt twice, and noone
m
more
than 12 times.
t
24 vox
xels were in thhe set betweenn 6
a
and
12 times,, and 22 bettween 6 and 10 times. The
T
c
characteristics
of the classification model
m
can vaary
driven in paart by noise in the trainiing
c
considerably,
innstances, but also
a by the facct that the algorrithm needs onnly
f
find
some of the
t features th
hat discriminatte between som
me
innstances of the
t
patterns some
s
of the time. It is not
n
g
guaranteed
to find
f
all of the relevant
r
differeentiating featurres,
n the best. The
nor
T conclusion
n seems obvioous, but is worth
s
stating
clearly: when any vox
xel can make it into the “moost
innformative” seet, and many voxels
v
are morre or less equaally
liikely to end up
u there, this should make us a bit uneaasy
a
about
their acttual informativeness. If theere is somethiing
s
stable
to the cognitive sttates differenttiating the taask
c
conditions,
thee set of most informative
i
vooxels is certainnly
n tracking it, nor can it therrefore be a reliiable indicator of
not
thhe location of the cognitively
y relevant inforrmation.
oss-validation does
d
not alleviaate
In should be noted that cro
thhis issue. Crooss-validation consists
c
of a faamily of methoods
d
designed
to preevent over-fittiing of the moddel to what couuld
b an unusuaally biased saample. Typicaally, it involvves
be
b
building
multipple models based on multiplee partitions of the
t
s
sample,
and avveraging resullts over the raange of differeent
p
partitions
(Pereeira et al., 2009
9). For instance, K-fold crossv
validation
invoolves splitting the training data
d
into K parrts,

K of the parttitions. We
and training K times on a rotating K-1
perform
med 10-fold crross validationn on our 20 traaining sets,
and fouund similar variability in thee set of most innformative
voxels in each fold. The mean num
mber of inclusioons among
the top 10 most highlyy weighted voxxels was 4 (SD
D 2.83). 23
of the voxels were among the 100 most highlyy weighted
voxels in at least one fold.

Non-loocal informaation
So, thaat seems to be the situation when
w
informatiion is local
and disstributed. Whhat about whenn the only infoormation is
non-loccal, that is, whhen the ratio off pattern versioons is 1:1?
It turnss out it is poossible to classsify these pattterns with
100% accuracy, appplying MVPA using a suppport-vector
machinne with a polyynomial kernell of degree two. Can we
concludde anything in this case abouut the neural coonditions—
represeentations, inforrmation, activiity, etc.—contrributing to
the diffferences in coggnitive tasks?
One is of course tem
mpted to simplly dismiss the possibility.
p
b
of its posssible states
In our examples everry voxel is in both
in everry task condittion. That iss: no voxel iss by itself
predictiive of any coggnitive state, and
a thus no innference to
the speecial status of activity
a
in anyy voxel could possibly
p
be
supportted by the predictive suuccess of MV
VPA. Yet
researchers do extracct “most inforrmative” voxell sets even
u
polynom
mial kernels (e.g. Davatzikkos et al.,
when using
2005), so it is wise to consider the matter
m
more carrefully.
In linnear classifierss identifying most
m important voxels for
the classsifier is easy—
—the features in
i the decisionn space that
have thhe highest weights are the most
m
important,, and these
features have a 1:1 correspondencee with componnents of the
v
that is, with the vooxel values fed into the
input vector,
classifiier. But non‐llinear SVMs use “kernel functions”,
f
wherebby a vector inpput is projecteed into a kernnel‐specific
high‐diimensional spaace, and the im
mportance of eaach feature
is deterrmined in thatt space. The orriginal space for
f a given
vector x has one dimeension for eachh component of
o the input
vector, and the vallue of that feature—its
f
poosition on
dimenssion i—is just xi. In contrastt, a polynomial kernel of
degree 2 (K2) projectss the input vecctor into a spacce having a
u
(unordered) pair of features in
dimenssion for each unique
the vecctor, and the value of eachh of those features—its
positionn on dimensionns (i,j)—is xi * xj.
Thuss, the K2 classiffier over an inpput space of diimension n
looks more
m
like this:
prediction

sign b

,
,

In factt, we can gett exactly thiss situation byy manually
projectiing our input vectors into the polynomiaal space—
turningg them in this case from n‐ddimensional vectors into
n+((n2‐n)/2)
‐
dimensioonal vectors—
—and using linnear SVMs
to classsify them. Thhis procedure will produce the same
decision surface as using
u
K2 in thee original spacce, but will
allow us to directlly inspect the resulting weights
w
to
determiine which featuures were mostt important.
How
wever, given thhe nature of noon‐linear SVM
Ms, relating
features to individuaal components of the input vectors is
inherenntly problematiic. For note thhat what gets weighted
w
in
the deecision functioon is the prooduct of each pair of

1513

components. So, if a given product turns out to be important
to the classifier, shall we attribute this importance to just
one of the components, or to both? Either decision seems
likely to give misleading results. Nevertheless, for the sake
of the discussion, let’s adopt the simple rule that when a
given feature is highly weighted, both components (voxels)
will be counted as “informative”. Given this, we can
examine the frequency with which voxels are informative,
and track the voxels that are frequently informative.
To test this procedure when using K2, we generated 40,
10x10 versions of the standard patterns from Figure 1, 20 of
each pattern type, with a 1:1 ratio of versions, and a noise
level of 5%. We projected each of these patterns into the
5,050‐dimensional feature space of K2, and trained a linear
SVM on the set. Then we found the top 500 highest
weighted features, and projected these back onto the 10x10
pattern following the rule above. Now, it is perfectly
legitimate to make the following inference from this
procedure: the highly weighted voxels are the ones that, had
they been in a different state, would have been most likely
to cause the classifier to place the pattern in the other class.
The trouble is, the weighting is often taken to tell us
something about the relative importance of each voxel to the
intrinsic difference between the patterns (and to the
underlying cognitive states), and no such inference is
warranted in this case.
First, there is a basic problem of interpretation given that
the important features are in fact products of two voxels—
so, every time a voxel is deemed informative, it has a
partner with which it was important, and the set itself gives
no information about the distribution of these partners.
Second, it is clear in this case (because there is no local
information) that the relevant information differentiating
between the patterns is non-local, carried in the covariance
structure of the pattern, and this information is not contained
in the set of frequently informative voxels. Third, the most
highly-weighted features are not those that contain the most
information. As in the linear case, they are the features that
contained sufficient information to drive the classifier on a
given set of training examples. Fourth and finally, as should
not be surprising, the set of informative features and
informative voxels is highly unstable in this case, as well.
To explore the stability of the set of important features
when using K2, we generated 10x10 versions of the standard
patterns above, creating 100 sets of 40 (20 of each pattern)
with a noise level of 5%. We projected each of these
patterns into the 5,050‐dimensional feature space of K2, and
trained a linear SVM on each of the 100 sets. From each of
these 100 sets, we extracted the top 500 most important
features. Doing a pair‐wise comparison of the most
important features from each set revealed that, on average,
only 101.08 (SD 16.94) of these features (20.21%) were
common between each pair. Moreover, the common features
varied from pair to pair. Doing a 5‐wise comparison of the
most important features sets reveals an average of just 0.81
(SD 1.09) of the features (0.16%) are shared across all five
sets. Note that despite the instability of the “most
informative” feature sets, classification accuracy in all cases
was 100%.

Given the high degree of variability in the features
considered most important, it seems certain that the set of
frequently informative components (voxels) is likewise
unstable. To confirm this, we generated 500 training sets of
the 10x10 patterns, and, following the procedure above,
found the top 500 most important features for each set.
Then, we counted the number of times each individual
component of the input vector was included in a pair that
was in this important feature set. On average, each
component was included in the set 10.00 times (SD 0.39).
No component averaged fewer than 9 inclusions, or more
than 11.00. Once again, if there is some stable difference
between the cognitive states in the two task conditions, the
set of most informative voxels is certainly not tracking it,
nor can it therefore be a reliable indicator of the location of
the cognitively relevant information.
Admittedly, this example was based on a very simple rule
for mapping features in the multi-dimensional space to
components of the original vector, and it is true that more
sophisticated procedures for uncovering the most
informative components have been developed (Davatzikos
et al., 2005; Lao et al., 2004). But insofar as these
techniques still depend in one way or another on identifying
the most highly weighted features in a multi-dimensional
space, and insofar as this set is not determinate for a given
classification task, then the results of such analyses need to
be interpreted with extreme caution.
Before moving on with the remainder of the analysis, it is
worth pausing to summarize the findings. In the case where
there is local information relevant to distinguishing patterns,
linear MVPA does not reliably find it; and in the case where
there is relevant non-local information, carried for instance
by covariance patterns, linear MVPA cannot find it, and
non-linear MVPA models can make it look as if they were
using local information. More importantly, having
discovered some features whose state matters most to the
classification decision is not the same as having discovered
the brain regions whose activity matters most (or even
relatively more) to the participant (or her brain). Indeed,
these two sorts of information need have no regular
correspondence to one another; one need not track, be a
reliable indicator of, or be otherwise instructive about the
nature, scope or location of the other.

Local, concentrated information
How is this disconnect possible? Consider first an example
from the MVPA literature meant to showcase the power of
the technique. Haynes and Rees (2005) were able to use
MVPA to correctly identify the orientation of visuallypresented lines, even when the stimuli were presented
briefly and masked so that the participant did not
consciously perceive them. That is an intriguing result, and
may tell us something interesting about the operation of V1
(the ROI they used to make the predictions). But note the
broader implication for the method: since the participants
cannot judge the orientation of the lines, they cannot be in
whatever cognitive state gives the ability to judge the
orientation of the lines. Thus, MVPA can be used to infer
features of the task environment from characteristics of the

1514

BOLD signal, without being a reliable indicator of the
cognitive state of the participant.
Now consider extending the experiment in the following
straightforward way: while the visual stimulus is being
shown (and masked), experimenters play an auditory tone
from which the participant could reliably infer the
orientation of the line. If, as seems likely in this particular
case, the most informative voxels for the pattern classifier
would remain in V1, this outcome would provide a clear
instance in which the information used by the participant
and the information used by the classifier would not have
the expected relation.
But is such an outcome really possible? In fact, this
hypothetical example points in the direction of a wellknown fact about the way classification algorithms perform.
Numerous theoretical results and a tremendous amount of
empirical evidence in machine learning demonstrate that
there is no universally best learning algorithm (Wolpert,
1996). Every algorithm has a bias that is appropriate for
some problems and inappropriate for others. This is true for
the brain, and the same is true of kernels. There is no
universally best kernel, and changing from one kernel to
another can lead to large changes in the learned decision
surface and thus to changes in what features in the data set
seem to be important.
The relevance of this problem for MVPA is that a
particular set of stimuli may elicit different patterns of
activity, call them pattern A and pattern B, in different parts
of the brain, and one kernel may be able to detect pattern A
but not pattern B, whereas another kernel may be able to
detect pattern B but not pattern A. Thus, when relating
“most informative features” to “most important activity”,
the area of the brain implicated in the experiment will
change depending on which kernel is used.
To make this concrete, consider two patterns with 20
binary features (f1 - f20) in which for every instance of the
first (positive) pattern the following two conditions hold:
(a) Either f19 = 1 and f20 = −1, or f19 = −1 and f20 = 1
(b) The sum of the first 5 bits is less than or equal to zero
For every instance of the second (negative) pattern, the
following two conditions hold:
(a) Either f19 = 1 and f20 = 1, or f19 = −1 and f20 = −1
(b) The sum of the first 5 bits is greater than zero
The values of the other bits are chosen uniformly at random
from {−1, 1}. Condition (a) is the logical exclusive or
(XOR) function on bits 19 and 20 and is easily learned by
the polynomial kernel of degree two (the class label is
−sign(f19 * f20)) but is impossible to learn with a linear
kernel. Condition (b) is easily learned with a linear kernel
(the class label is 1 if f1+f2+f3+f4+f5 ≤ 0 and is -1 otherwise),
but is extremely difficult for the polynomial kernel of
degree two because it has access to individual feature fi only
as fi * fi which is 1 regardless of the value of fi.
We created 100 datasets based on the above rules and
trained an SVM with a linear kernel on both the original
feature space and the feature space constructed for the

polynomial kernel of degree two. In the latter space, the
feature corresponding to f19 * f20 had an average weight of
3.64. The remaining 209 features had average weights in the
range (0.05, 0.10). In the former case, the average weights
for features f1 through f5 were 1.92, 1.94, 1.94, 1.93, and
1.94. The remaining 15 features had average weights in the
range (0.03, 0.10). Clearly, the choice of kernel can have a
dramatic impact on which features are deemed important
and, in the case of MVPA, which voxels are implicated in
various cognitive tasks.
Thus, although much of this paper was spent detailing the
worrying instability and potential deceptiveness of the most
informative voxel set when information is non-local or
distributed, the fact is that even if MVPA were perfectly
reliable at the task of finding the most informative features
in a data set, the inference from this to the brain activity
most important determining the outcome in given task
would remain fairly weak. This is because inference from
most informative features to most important activity
apparently relies on the unwarranted additional assumption
that the pattern classification algorithm and the brain are
classifying on a relevantly similar basis. While of course no
one claims that the success of MVPA shows that the brain is
implementing an identical classifier, the issue is that the
hypothesis space is different for different classifiers, and so
different information will be relevant to each. What is
relevant in the brain, and what is relevant to classifying an
image of the brain, need not bear much relation.

Conclusion
There are very many challenges to the task of reliably
relating the features (of the BOLD signal) most important to
classification success to the features (of brain activity) most
important to cognitive states/outcomes.
By way of
summation, consider this general list of possible ways in
which these features might fail to relate as expected.
(1) The highly informative elements of the pattern as
discerned by MVPA are distributed in the brain in such a
way that the brain is anatomically or functionally incapable
of integrating the information. If people are nevertheless
capable of making the relevant discrimination, it must have
been on the basis of different information.
(2) There may well be classes of stimuli that differ in
ways undetectable to subjects (under any presentation
condition, conscious or otherwise), but which nevertheless
create patterns in the BOLD signal allowing for successful
classification by MVPA. Consider in this regard an
experiment run by Hung et al. (2005). Macaques passively
viewed picture stimuli in eight different categories while
undergoing direct recording of neural activity using
microelectrode arrays. Hung et al. were able to successfully
classify the stimuli with a linear SVM taking the multi-unit
activity as input. But here the macaques did not—indeed, in
all likelihood could not—classify the stimuli, because they
had not been trained to do so. In this case, the SVM might
have been making distinctions that the (untrained) macaques
were not.
(3) Stimuli may differ along more than one dimension,
both of which lead to differences in the BOLD signal.
MVPA classification could rely on patterns relating to one

1515

dimension, while participants use information relating to the
other. That is, even when there is information in the BOLD
signal that is theoretically accessible by (or that is tracking
information accessible by) the participant, this may not be
the information that is being used by the participant.
(4) The MVPA classifier may be using a kernel that is
significantly different from what is implemented in the
brain. As we saw, classifiers with different kernels trained
on the very same data will extract different features, and
thus come to different decisions about which features (and
which elements of the input vectors) are most important.
(5) Since there will always be a set of highly informative
voxels produced by the MVPA classifier, the existence of
such a set won’t tell us whether the relevant information in
the brain is local and concentrated, local and distributed,
non-local, or some combination of these.
The discussion also raises a much more general issue. As
we noted at the outset, MVPA offers an exciting new way to
investigate the operation of the brain, by looking at the
predictive value of (typically widely) distributed patterns of
activity. The problematic inferences generally come in the
attempt to reduce such patterns to local features of brain
activity. But if the best predictor of cognitive states is not
the location of an activated region, but rather the patterns of
cooperation and coactivation between them—as the success
of MVPA might be said to indicate, and as has been argued
for independent reasons (Anderson, 2008; Sporns, et al.,
2004; Uttal, 2001)—then perhaps it is time to pay more
heed to the patterns than to the partners. We are just
beginning to develop the tools to make such an investigation
fruitful and rigorous—including not only MVPA but other
forms of statistical pattern analysis, machine learning, graph
theory, etc.—and it seems a shame instead to use these tools
in the service of localization projects for which they are
ultimately ill-suited. New tools often come with the
opportunity to re-consider the strengths of theoretical
perspectives and paradigms, and these are offering a chance
to look beyond localization, to what other perspectives on
brain organization might have to offer.

References
Anderson, M. (2008). Circuit sharing and the
implementation of intelligent systems. Connection
Science, 20(4): 239-51.
Davatzikos, C., Ruparel, K., Fan, Y., Shen, D., Acharyya,
M., Loughead, J., Gur, R. & Langleben, D.D. (2005).
Classifying spatial patterns of brain activity with machine
learning methods: Application to lie detection.
NeuroImage, 28(3): 663-68.
Gross, J., Schmitz, F., Schnitzler, I., Kessler, K., Shapiro,
K., Hommel, B & , Schnitzler, A. (2004). Modulation of
long-range neural synchrony reflects temporal limitations
of visual attention in humans. Proceedings of the National
Academy of Sciences-USA, 101(35): 13050-13055
Haxby, J.V., Gobbini, M. I., Furey, M.L., Ishai, A.,
Schouten, J.L. & Pietrini, P. (2001). Distributed and
overlapping representations of faces and objects in ventral
temporal cortex. Science, 293: 2425-30.
Haynes, J-D. & Rees, G. (2006). Deconding mental states
from brain activity in humans. Nature Reviews

Neuroscience, 7:523-34.
Haynes, J.-D. & Rees, G. (2005). Predicting the orientation
of invisible stimuli from activity in human primary visual
cortex. Nature Neuroscience, 8(5): 686-91.
Haynes, J.-D., Sakai, K., Rees, G., Gilbert, S., Frith, C. &
Passingham, R.E. (2007). Reading hidden intentions in
the human brain. Current Biology, 17: 232-28.
Hung, C.P., Kreiman, G., Poggio, T. & DiCarlo, J.J. (2005).
Fast readout of object identity from macaque inferior
temporal cortex. Science, 310: 863-6.
Kamitani, Y. & Tong, F. (2005). Decoding the visual and
subjective contents of the human brain. Nature
Neuroscience, 8(5): 679-85.
Kay, K.N., Naselaris, T., Prenger, R.J. & Gallant, J.L.
(2008). Identifying natural images from human brain
activity. Nature, 452: 352-6.
Lao, Z., Shen, D., Xue, Z., Karacali, B., Resnick, S. &
Davatzikos, C. (2004). Morphological classification of
brains via high-dimensional shape transformation and
machine learning methods. NeuroImage, 21 (1): 46-57.
McDuff, S.G.R., Frankel, H.C. & Norman, K.A. (2009).
Multivoxel pattern analysis reveals increased memory
targeting and reduced use of retrieved details during
single-agenda source monitoring.
Journal of
Neuroscience, 29(2):508-516.
Mitchell, T.M., Shinkareva, S.V., Carlson, A., Chang, K.M., Malave, V.L., Mason, R.A. & Just, M.A. (2008).
Predicting human brain activity associated with the
meanings of nouns. Science, 320: 1191-5.
Norman, K.A., Polyn, S.M., Detre, G.J. & Haxby, J.V.
(2006). Beyond mind-reading: Milti-voxel pattern
analysis of fMRI data. TRENDS in Cognitive Sciences,
10(9): 424-30.
Pereira, F., Mitchell, T., Botvinick, M. M. (2009). Machine
learning classifiers and fMRI: a tutorial overview.
NeuroImage, 45: S199-S209.
Poldrack, R.A. (2006). Can cognitive processes be inferred
from neuroimaging data? Trends in Cognitive Sciences,
2006; 10(2): 59-63.
Preston, T.J., Li, S., Kourti, Z. & Welchman, A.E. (2008).
Multivoxel pattern selectivity for perceptually relevant
binocular disparities in the human brain. The Journal of
Neuroscience, 28(44): 11315-27.
Shinkareva, S.V., Mason, R.A., Malave, V.L., Wang, W.,
Mitchell, T.M. & Just, M.A. (2008). Using fMRI brain
activation to identify cognitive states associated with
perception of tools and dwellings. PLOS One, 3(1):
e1394. doi:10.137/journal.pone.0001394
Sporns, O., Chialvo, D., Kaiser, M. & Hilgetag, C.C.
(2004). Organization, development and function of
complex brain networks. Trends in Cognitive Sciences, 8:
418-425.
Uttal, W. (2001). The New Phrenology. Cambridge: MIT
Press.
Varela, F., Lachaux J.P., Rodriguez E. & Martinerie J.
(2001). The brainweb: phase synchronization and largescale integration. Nature Rev. Neuroscience, 2(4): 229-39.
Wolpert, D. (1996). The lack of a priori distinctions
between learning algorithms. Neural Computation, 8(7):
1341-1390.

1516

