UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Do Tutors’ Content Knowledge and Beliefs About Learning Influence Their Assessment of
Tutees’ Understanding?

Permalink
https://escholarship.org/uc/item/1c169462

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Herppich, Stephanie
Wittwer, Jorg
Nuckles, Matthias
et al.

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Do Tutors’ Content Knowledge and Beliefs About Learning Influence
Their Assessment of Tutees’ Understanding?
Stephanie Herppich (herppich@ipn.uni-kiel.de)
Institute for Science and Mathematics Education at the University of Kiel
Olshausenstrasse 62, 24098 Kiel, Germany

Jörg Wittwer (wittwer@ipn.uni-kiel.de)
Institute for Science and Mathematics Education at the University of Kiel
Olshausenstrasse 62, 24098 Kiel, Germany

Matthias Nückles (matthias.nueckles@ezw.uni-freiburg.de)
University of Freiburg, Department of Educational Science, Instructional and School Research
Rempartstrasse 11, 79098 Freiburg, Germany

Alexander Renkl (renkl@psychologie.uni-freiburg.de)
University of Freiburg, Department of Psychology, Developmental and Educational Psychology
Engelbergerstrasse 41, 79085 Freiburg, Germany

gleaning diagnostically relevant information about a tutee.
For example, Chi, Siler, and Jeong (2004) examined
tutoring in biology and found that tutors appeared to be
relatively accurate in knowing the tutees’ correct
understanding but they failed to assess the tutees’ incorrect
understanding including their false beliefs and flawed
mental models. The researchers interpreted this finding as
evidence that tutors mainly used their own normative
perspective as a basis for estimating what the tutees did and
did not know. Similar findings were obtained by Graesser,
Person, and Magliano (1995), who showed that tutors rarely
diagnosed a tutee’s incorrect understanding. Instead, their
actions were largely based on a curriculum script that
determined which skills and concepts were to be learned by
the tutees (see also Putnam, 1987).
In light of these findings, the question arises as to what
influences the tutors’ assessment of a tutee’s understanding.
In this article, we shed light on two characteristics of tutors
that might impact their assessment of tutees. Specifically,
we look at the tutors’ content knowledge and beliefs about
learning. To theoretically elucidate the role of these tutor
characteristics, we draw on research in the field of human
tutoring and classroom teaching.

Abstract
Research has established that tutors often have difficulty with
accurately assessing a tutee’s understanding. However, it is a
completely open question which characteristics of tutors
might affect their assessment. In an empirical study with
N = 22 tutor-tutee dyads, we used a methodology developed
by Chi, Siler, and Jeong (2004) to examine the influence of
the tutors’ content knowledge and beliefs about learning on
their assessment accuracy. Results replicated previous
research in showing that tutors overestimated a tutee’s correct
understanding and underestimated a tutee’s incorrect
understanding. In addition, more accurate assessments were
positively related with tutees’ learning. Finally, content
knowledge had a positive impact on assessment accuracy,
whereas beliefs about learning were not strongly associated
with assessment accuracy. Thus, assessing a tutee’s
understanding seems to be important for the effectiveness of
human tutoring. Moreover, the results suggest that the tutors’
assessment accuracy is largely influenced by their content
knowledge.
Keywords: assessment accuracy; beliefs about learning;
content knowledge; human tutoring

Introduction
In educational psychology, it is widely acknowledged that
for learning to be effective instruction should be tailored to
a learner (Kalyuga, 2007). However, such learner-tailored
instruction makes it necessary to assess a learner’s
individual understanding. Therefore, the ability to collect
diagnostically relevant information about a learner is a
central component that constitutes teaching competence
(Wittwer & Renkl, 2008).
One-to-one tutoring is a form of instruction where tutors
can make intensive use of the possibility of freely
interacting with a tutee in order to assess a tutee’s
understanding. Accordingly, tutors can be expected to have
a detailed “model of the student” (Putnam, 1987). However,
research has shown that tutors often have difficulty with

Tutors’ Content Knowledge
There is widespread agreement that having a deep
understanding of a subject matter is an important condition
for effective teaching. Research has shown that teachers
with higher content knowledge show, for example, a greater
understanding of important concepts in a domain and of the
relationships among them (e.g., Borko & Putnam, 1996).
However, the question as to how content knowledge
specifically affects the assessment of learners in the process
of teaching has not been the object of much research (cf.
Baumert & Kunter, 2006). For example, Krauss et al. (2008)
found that teachers with higher content knowledge tended to

314

have more knowledge about a learner’s misconceptions. The
influence of this knowledge on the teachers’ practices in
classroom, including their assessment of the learners, was,
however, not examined.
Similarly, in the context of tutoring, little is known about
the relationship between the tutors’ content knowledge and
their assessment of tutees. For example, Schmidt et al.
(1993) found that tutors with higher content knowledge
were generally more effective in promoting tutees’ learning
when compared to tutors with lower content knowledge.
The researchers attributed this finding to the fact that tutors
with more content knowledge engaged in content-related
activities that helped tutees to acquire knowledge. Even so,
the role of the tutors’ assessment practices for the tutees’
learning was not investigated in this study.
Overall, the findings suggest that tutors with higher
content knowledge might assess a tutee’s individual
understanding more accurately than tutors with lower
content knowledge. This is assumed to be because tutors
with more content knowledge normally have a deeper
understanding of the concepts to be learned by a tutee
(Borko & Putnam, 1996). Accordingly, tutors can be
expected to show a more differentiated understanding of a
tutee’s conceptual knowledge (Nickerson, 1999). For
example, tutors with higher content knowledge might be
more likely to think at a deeper level about the conceptual
aspects of a tutee’s comprehension difficulties (Chi,
Feltovich, & Glaser, 1981). Similarly, tutors with higher
content knowledge might more likely infer from a tutee’s
particular
misunderstanding
which
related
misunderstandings and misconceptions can occur (Person et al.,
1994).

pedagogical activities observed in classroom (e.g., Leuchter
et al., 2006). It can be assumed that this is because teachers
might not be completely accurate in self-assessing their
beliefs about learning. Another explanation is that the
teachers’ beliefs might be too distal to strongly shape their
teaching practices.
In the context of tutoring, it is a completely open question
as to how the tutors’ beliefs about learning influence their
assessment of tutees. In line with the findings obtained in
research on classroom teaching, it can be assumed that a
constructivist view of learning supports the accuracy with
which tutors assess a tutee’s understanding. This is because
tutors with a constructivist view of learning as opposed to
tutors with a transmission view of learning see tutees as
being actively involved in learning. Thus, it is supposed that
tutors with a constructivist view of learning provide tutees
with opportunities to be active and constructive on their
own. As a result, the tutors should get insights into the
tutees’ understanding and learning progress during the
course of tutoring.

Research Questions
We present an empirical study in which we examined
human tutoring in biology to shed light on the role of the
tutors’ content knowledge and beliefs about learning in
assessing a tutee’s conceptual understanding. We addressed
the following research questions:
1) How accurately do tutors assess a tutee’s correct
understanding and a tutee’s incorrect understanding?
2) Is the tutors’ assessment accuracy positively associated
with the tutees’ learning?
3) Does the tutors’ content knowledge positively influence
their assessment accuracy?
4) Does the tutors’ orientation towards a constructivist
view of learning positively influence their assessment
accuracy?

Tutors’ Beliefs About Learning
Apart from the teachers’ content knowledge, their beliefs
about how learners learn might also influence their teaching
(Borko & Putnam, 1996). These beliefs can be roughly
divided into two different views of learning: a transmission
view of learning and a constructivist view of learning. A
transmission view of learning focuses on the contents to-belearned and emphasizes the role of transmitting knowledge
to the learner. In contrast, a constructivist view of learning
places a learner’s own knowledge-construction activities at
the center of instruction and emphasizes the role of
supporting a learner’s learning.
Research has provided evidence that such beliefs have an
impact on teaching and learning. For example, Staub and
Stern (2002) found that teachers with a constructivist view
of learning were more successful in enhancing a learner’s
problem solving. In addition, Dubberke et al. (2008) showed
that the teachers’ beliefs strongly guided their teaching
practices. For example, teachers with a transmission view of
learning less often engaged in activities to support the
learners’ knowledge acquisition than teachers with a
constructivist view of learning.
Despite these findings, there is also research showing that
the teachers’ beliefs are not necessarily associated with their

Method
Sample and Design
A total of N = 22 dyads of tutors and tutees participated in
the empirical study. Tutors were university students of
biology. Of the tutors, 18 were female and 4 were male.
Their mean age was 22.64 years (SD = 2.79). Tutees were
K-7 students from Realschulen (i.e., schools from the
middle track of the German school system). Of the tutees, 9
were female and 13 were male. Their mean age was 12.64
years (SD = 0.49). The tutors and the tutees did not know
each other before tutoring.
We examined the accuracy with which the tutors assessed
a tutee’s individual understanding. We also analyzed the
impact of their assessment accuracy on tutees’ learning.
Finally, we investigated the influence of the tutors’ content
knowledge and beliefs about learning on their accuracy at
assessing a tutee’s individual understanding.

315

by Chi et al. (2004), we assessed a tutee’s conceptual
understanding at the global level of mental models.
To code the tutees’ and the tutors’ drawings and
explanations of the human circulatory system, we adapted a
classification scheme originally developed by Azevedo,
Cromley, and Seibert (2004). On the basis of this
classification scheme, the drawings were assigned to one of
twelve categories. The categories reflect distinguishable
types of correct and incorrect mental models with categories
0 to 9 indicating different types of incorrect mental models
and with categories 10 to 11 indicating a correct mental
model.

Materials
Textbook (Tutee and Tutor) In the tutoring session, the
tutor and the tutee engaged in a dialogue on the basis of a
passage about the human circulatory system, which was
previously used by Chi et al. (2001). We adapted this
passage for the present study by deleting and reformulating
some sentences. Each of the remaining 59 sentences of the
passage was printed on a separate sheet of paper. The
sentences were presented to the tutor and the tutee in a ring
binder.
Content Knowledge Test (Tutor) The test consisted of 18
multiple-choice items. Each correct answer was assigned 1
point. The test measured not only the tutors’ knowledge
about basic concepts to be discussed in tutoring, but also
their knowledge about advanced concepts of the human
circulatory system, about the relationships among these
concepts, and about the relevance of these concepts for life
processes. Hence, answering the test required different
levels of knowledge. Accordingly, item difficulty ranged
from .41 to .95 (M = .64, SD = .16).

Procedure
Each tutoring session was divided into three phases: pre-test
phase, tutoring phase, and post-test phase. It lasted about 3
hours.
Pre-Test Phase In the pre-test phase, the tutors completed
the content knowledge test. The tutees completed the
misconceptions test. In addition, the tutees were asked to
draw the blood path of the human circulatory system in the
outline of a human body and to explain the blood path as
they knew it. Afterwards, both the tutors and the tutees
individually read the passage about the human circulatory
system.

Beliefs About Learning Questionnaire (Tutor) The
questionnaire was adapted from Staub and Stern (2002). On
a 4-point Likert scale ranging from 1 (= strongly disagree)
to 4 (= strongly agree), the tutors indicated their agreement
with 19 statements. Agreement with 9 out of the 19
statements indicated a constructivist view of learning. The
agreement with the remaining 10 statements indicated a
transmission view of learning. The statements indicating a
transmission view of learning were reversed so that the
mean agreement with a constructivist view of learning could
be computed, with higher scores showing a more
constructivist view of learning.

Tutoring Phase The dyads of tutors and tutees read each
sentence of the passage about the human circulatory system
and engaged in a dialogue about each sentence. After the
33th sentence, tutoring was interrupted and the dyads were
separated. The tutees were asked to draw and explain the
blood path of the human circulatory system. To measure
what the tutors thought that the tutees would know about the
blood path, the tutors were required to draw and explain the
tutees’ mental model of the human circulatory system. After
accomplishing this task, tutoring was continued.

Misconceptions Test (Tutee and Tutor) The test consisted
of 25 multiple-choice items that addressed concepts about
the human circulatory system at the local level of
propositions (cf. Chi et al., 2004). The items were adapted
from tests originally developed by Sungur and Tekkaya
(2003) and by Michael et al. (2002) or constructed on the
basis of the literature on misconceptions of the human
circulatory system (e.g., Pelaez et al., 2005). The items
covered concepts about the human circulatory system that
were explicitly or implicitly mentioned in the textbook. A
correct answer indicated a scientifically correct
understanding of the concept. Each of the incorrect answers
indicated a specific type of incorrect understanding of the
concept.

Post-Test Phase After completing the tutorial dialogue, the
dyads of tutors and tutees were separated again and asked to
draw and explain the blood path of the human circulatory
system. Afterwards, the tutees completed the misconceptions test. The tutors also received the 25 items of the
misconceptions test and were asked to indicate how the
tutee would answer each of the items. Finally, the tutors
filled in the beliefs about learning questionnaire.

Results
The following results concerning the tutors’ assessment
accuracy and the tutees’ learning are based on the data
collected in the post-test phase.

Drawings of the Human Circulatory System (Tutee and
Tutor) On a sheet of paper, the outline of a human body
was displayed. The tutees were asked to draw the blood path
of the circulatory system into the human body and to
explain the blood path. The explanations were audiotaped.
By using this methodology, which was originally developed

Tutors’ Assessment Accuracy
In a first step, we examined the accuracy with which the
tutors assessed what the tutees did and did not know at the
level of propositions (i.e., misconceptions test) and at the

316

Tutors’ Content Knowledge, Beliefs
Learning, and Assessment Accuracy

level of mental models (i.e., drawings of the circulatory
system).

In a last step, we determined the relation between the tutors’
content knowledge and beliefs about learning on the one
hand and their assessment accuracy on the other hand. To
measure the assessment accuracy at the level of
propositions, we used the number of answers that the tutors
correctly assumed the tutees to give to each of the items of
the misconceptions test. To measure the assessment
accuracy at the level of mental models, we used the
difference between the category number of a tutee’s mental
model and the category number of a tutor’s drawing of the
tutee’s mental model. Content knowledge and beliefs about
learning were not significantly related with each other,
r = .25, p = .26.

Misconceptions Test On average, the tutees had a correct
understanding of 49% (SD = 11%) of the concepts and an
incorrect understanding of 43% (SD = 13%) of the
concepts1.
Generally, the tutors assumed tutees to have a correct
understanding of 58% (SD = 12%) of the concepts and to
have an incorrect understanding of 26% (SD = 5%) of the
concepts. Hence, the tutors significantly overestimated the
tutees’ correct understanding of the concepts, t(21) = -2.43,
p = .02, η2 = .22 (strong effect), and significantly
underestimated the tutees’ incorrect understanding of the
concepts, t(21) = 6.10, p = .01, η2 = .64 (strong effect).
When we specifically looked at whether the tutors knew
how the tutees would answer each of the items of the
misconceptions test, we found that the tutors knew the
tutees’ precise answers for 43% (SD = 11%) of all items.

Content Knowledge In the content knowledge test, the
tutors answered, on average, 64% (SD = 21%) of the items
correctly. The number of correctly answered items was
positively and significantly correlated with the accuracy
with which the tutors assessed the tutees’ understanding at
the level of propositions, r = .47, p = .03. It was also
positively and significantly correlated with the accuracy
with which the tutors assessed the tutees’ understanding at
the level of mental models, r = .48, p = .02. Hence, the
tutors with higher content knowledge were clearly more
accurate in assessing the tutees’ understanding.

Drawings Of the tutees, 64% drew and explained an
incorrect mental model, whereas 36% drew and explained a
correct mental model.
The tutors assumed the tutees to have an incorrect mental
model in 18% of all cases and assumed the tutees to have a
correct mental model in 82% of all cases. Thus, the tutors
tended to assume the tutees to have more often a correct
mental model than the tutees actually had and to have less
often an incorrect mental model than the tutees actually had,
χ2(1, N = 22) = 2.79, p = .09, φ = .36 (medium effect).
When we further looked at the categories into which the
drawings of the tutees and the tutors fell, we found that, on
average, the tutees’ mental models were assigned to
category 7 (M = 7.36, SD = 3.19). The tutors’ drawings of
the tutees’ mental models were, on average, assigned to
category 10 (M = 10.27, SD = 0.88). The difference between
the average category of the tutees’ mental models and the
average category of the tutors’ drawings of the tutees’
mental models (M = -2.91, SD = 3.25) was significant,
t(21) = -4.20, p = .01, η2 = .46 (strong effect). Hence, the
tutors largely overestimated the tutees’ understanding at the
level of mental models.

Tutors’ Assessment
Learning

Accuracy

and

About

Beliefs About Learning When answering the beliefs about
learning questionnaire, the tutors achieved a mean score of
2.76 points (SD = 0.44). Hence, the tutors, on average,
tended to show a constructivist view of learning. The
correlation between the tutors’ beliefs about learning and
their accuracy at assessing what tutees knew at the level of
propositions just failed to reach the 10%-level of statistical
significance, r = .35, p = .11. The correlation between the
tutors’ beliefs about learning and their accuracy at assessing
what tutees knew at the level of mental models was not
significant, r = .12, p = .59. Obviously, the tutors’ beliefs
about learning were not generally associated with the
accuracy with which the tutors assessed the tutees’
understanding.

Tutees’

Discussion
The present study examined the accuracy with which tutors
assessed a tutee’s understanding of the human circulatory
system. We found that the tutors significantly overestimated
the tutees’ correct understanding of important concepts
related to the human circulatory system and significantly
underestimated the tutees’ incorrect understanding of these
concepts. A similar pattern of results was obtained when we
looked at the tutors’ assessments of the tutees’ mental
models of the human circulatory system. Again, the tutors
assumed the tutees to have a more complete understanding
than they actually had. Overall, our findings replicate the
results of Chi et al. (2004) and suggest that tutors seriously
fail to take into account a tutee’s alternative understanding.

In a next step, we examined the importance of the tutors’
assessment accuracy for the tutees’ learning. To do so, we
computed the correlation between the tutors’ assessment
accuracy at the level of propositions and the tutees’
understanding at the level of mental models. The correlation
was significant, r = .59, p = .01. Hence, the tutors’
assessment accuracy was substantially associated with
tutees’ learning.
1
To reduce the probability of guessing the correct answer in the
misconceptions test, the tutees were asked to check the option
“don’t know” in case of uncertainty. Thus, correct and incorrect
answers do not add up to 100%.

317

As already discussed by Chi et al. (2004), tutors appear not
to carefully assess what tutees do and do not know. Instead,
they seem to exhibit a bias towards imputing their own
normative perspective to the tutees (Hinds, 1999; Nickerson,
1999).
However, our results also show that the accuracy with
which the tutors assessed a tutee’s understanding largely
depended on their content knowledge. In other words, tutors
with more content knowledge were more accurate in
assessing a tutee’s conceptual understanding both at the
level of propositions and at the level of mental models. It
can be argued that this is likely to be because tutors with
more content knowledge assess and categorize a tutee’s
understanding of concepts at a deeper level (Nickerson,
1999). This might allow the tutors to discriminate a tutee’s
understandings and misunderstandings more accurately (Chi
et al., 1981).
In addition, we found that the tutors’ beliefs about
learning seemed to be less important for their assessment
accuracy. This finding, however, has to be interpreted with
caution. In our study, nearly all tutors showed an orientation
towards a constructivist view of learning. Therefore, the
variance of this tutor characteristic apparently was too small
to yield any significant result.
Even though we observed differences in the accuracy with
which the tutors assessed a tutee’s understanding, we do not
know yet which assessment strategies they used to collect
diagnostically relevant information about a tutee. Prior
research has already provided evidence for differences in
tutorial actions between more experienced tutors and less
experienced tutors. For example, Cromley and Azevedo
(2005) found that more experienced tutors more often
engaged in cognitive scaffolding. Less experienced tutors,
in contrast, more often delivered information to the tutees.
Following Chi et al. (2001), it is plausible to assume that
these tutorial moves might help or hinder tutors in assessing
what a tutee knows. For example, when asking questions
(i.e., asking for information) instead of providing
explanations (i.e., generating information on one’s own),
tutors might have more cognitive resources left for assessing
a tutee’s understanding (see also Wittwer, Nückles, &
Renkl, 2010). Thus, to shed light on the question which
moves of tutors positively and negatively influence their
assessments of tutees, we are currently analyzing the
tutoring protocols collected during the tutoring sessions.
Related to this is the question how the tutors in our study
adjusted their tutorial moves on the basis of their
assessments. Our results show that the tutors’ assessment
accuracy was positively associated with the tutees’ learning.
This suggests that the tutors might have used their
assessments of what a tutee does and does not know in order
to individualize instruction. It can be conjectured that the
assessments, for example, influenced the tutors in deciding
to move on to the next sentence of the textbook or to ask a
question in order to elicit knowledge-construction activities
from a tutee. Again, our content analysis of the tutoring

protocols could clarify how the tutors adapted their moves
to a tutee’s specific understanding.
What are the implications of our study and what are the
directions for future research? First, our findings suggest
that it seems to matter who serves as tutor. Obviously, tutors
with higher content knowledge can more accurately assess
what a particular tutee does and does not know. As a result,
these tutors acquire knowledge about a tutee’s knowledge
which they can use to support the tutee’s learning 2. Hence
the concrete effectiveness of human tutoring might vary,
amongst other things, as a function of tutor characteristics
such as a tutor’s content knowledge and tutoring experience
(Cromley & Azevedo, 2005; though tutoring has generally
been shown to be effective: Cohen, Kulik, & Kulik, 1982).
Second, our study seems to indicate that, in general, tutors
with lower content knowledge have more difficulty with
taking into account a tutee’s particular understanding. At
first glance, this finding might contradict the notion that
peer tutors who normally do not possess considerably more
knowledge than their tutees can also be responsive to their
tutees’ needs. However, such responsive behavior might not
primarily result from the tutors’ accurate assessments of the
tutees’ knowledge. Instead, it can be argued that tutors in
peer tutoring share with their tutees a similar understanding
of the learning task and, thus, might encounter the same
comprehension difficulties. As a result of this common
ground (Chi et al., 2004), the tutor and the tutee are more
likely to “automatically” possess a mutual understanding.
Hence, peer tutors might not be required to deliberately
assess a tutee’s understanding at all.
Third, our results show that, on average, the tutors largely
overestimated a tutee’s understanding. It was assumed that
this finding can be attributed to the tutors’ bias to impute
their own normative perspective to the tutees. Although our
study suggests that having more content knowledge reduces
the risk of overestimating a tutee’s understanding, there
might be a trade-off between the tutors’ content knowledge
and their assessment accuracy under some circumstances.
For example, Nathan and Petrosino (2003) found that preservice teachers with higher content knowledge had
problems with correctly estimating the difficulty of
mathematical problems for learners. This was assumed to be
a result of the pre-service teachers’ discipline-specific
perspective on the mathematical problems. Accordingly, it
might well be that tutors who have, due to their high content
knowledge, a more discipline-oriented view of the subject
matter are particularly prone to an egocentric bias. In this
case, it can be expected that tutors with such knowledge are
less accurate instead of more accurate in assessing a tutee’s
understanding.

2

In a mediation analysis, we found that the tutors’ content
knowledge influenced the tutees’ learning. This effect was
significantly mediated by the tutors’ assessment accuracy.

318

mathematics
teachers.
Journal
of
Educational
Psychology, 100, 716-725.
Leuchter, M., Pauli, C., Reusser, K., & Lipowsky, F.
(2006). Unterrichtsbezogene Überzeugungen und
handlungsleitende Kognitionen von Lehrpersonen
[Teaching-related beliefs and practice-guiding cognitions
of teachers]. Zeitschrift für Erziehungswissenschaft, 9,
562-579.
Michael, J. A., Wenderoth, M. P., Modell, H. I., Cliff, W.,
Horwitz, B. et al. (2002). Undergraduates’ understanding
of cardiovascular phenomena. Advances in Physiology
Education, 26, 72-84.
Nathan, M. J., & Petrosino, A. J. (2003). Expert blind spot
among preservice teachers. American Educational
Research Journal, 40, 905-928.
Nickerson, R. S. (1999). How we know―and sometimes
misjudge―what others know: Imputing one’s own
knowledge to others. Psychological Bulletin, 125, 737759.
Pelaez, N. J., Boyd, D. D., Rojas, J. B., & Hoover, M. A.
(2005). Prevalence of blood circulation misconceptions
among prospective elementary teachers. Advances in
Physiology Education, 29, 172-181.
Person, N. K., Graesser, A. C., Magliano, J. P., & Kreuz, R.
J. (1994). Inferring what the student knows in one-to-one
tutoring: The role of student questions and answers.
Learning and Individual Differences, 6, 205-229.
Putnam, R. T. (1987). Structuring and adjusting content for
students: A study of live and simulated students tutoring
of addition. American Educational Research Journal, 24,
13-48.
Schmidt, H. G., Van der Arend, A., Moust, J. H. C., Kokx,
I., & Boon, L. (1993). Influence of tutors’ subject-matter
expertise on student effort and achievement in problembased learning. Academic Medicine, 68, 784-791.
Staub, F. C., & Stern, E. (2002). The nature of teachers’
pedagogical content beliefs matters for students’
achievement gains: Quasi-experimental evidence from
elementary mathematics. Journal of Educational
Psychology, 94, 344-355.
Sungur, S., & Tekkaya, C. (2003). Students’ achievement in
human circulatory system unit: The effect of reasoning
ability and gender. Journal of Science Education and
Technology, 12, 59-64.
Wittwer, J., Nückles, M., & Renkl, A. (2010). Using a
diagnosis-based approach to individualize instructional
explanations in computer-mediated communication.
Educational Psychology Review, 22, 9-23.
Wittwer, J., & Renkl, A. (2008). Why instructional
explanations often do not work: A framework for
understanding the effectiveness of instructional
explanations. Educational Psychologist, 43, 49-64.

Acknowledgments
We would like to thank our research assistants Julian Etzel,
Tatjana Scharping, Anika Schoneville, and Raoul
Zimmermann for their help with many practical aspects of
the project. This research was supported by grants from the
German Science Foundation DFG (WI 3348/2-1).

References
Azevedo, R., Cromley, J. G., & Seibert, D. (2004). Does
adaptive scaffolding facilitate students’ ability to regulate
their learning with hypermedia? Contemporary
Educational Psychology, 29, 344-370.
Baumert, J., & Kunter, M. (2006). Stichwort: Professionelle
Kompetenz von Lehrkräften [Keyword: Professional
competence
of
teachers].
Zeitschrift
für
Erziehungswissenschaft, 9, 469-520.
Borko, H., & Putnam, R. (1996). Learning to teach. In D.
Berliner & R. Calfee (Eds.), Handbook of educational
psychology. New York: Macmillan.
Chi, M. T. H., Feltovich, P., & Glaser, R. (1981).
Categorization and representation of physics problems by
experts and novices. Cognitive Science, 5, 121-152.
Chi, M. T. H., Siler, S., & Jeong, H. (2004). Can tutors
monitor students’ understanding accurately? Cognition
and Instruction, 22, 363-387.
Chi, M. T. H., Siler, S., Jeong, H., Yamauchi, T., &
Hausmann, R. G. (2001). Learning from human tutoring.
Cognitive Science, 25, 471-533.
Cohen, P. A., Kulik, J. A., & Kulik, C. (1982). Educational
outcomes of tutoring: A meta-analysis of findings.
American Educational Research Journal, 19, 237-248.
Cromley, J. G., & Azevedo, R. (2005). What do reading
tutors do? A naturalistic study of more and less
experienced tutors in reading. Discourse Processes, 40,
83-113.
Dubberke, T., Kunter, M., McElvany, N., Brunner, M., &
Baumert, J. (2008). Lerntheoretische Überzeugungen von
Mathematiklehrkräften:
Einflüsse
auf
die
Unterrichtsgestaltung
und
den
Lernerfolg
von
Schülerinnen und Schülern [Beliefs of mathematics
teachers: Impact on teaching practices and students’
achievement]. Zeitschrift für Pädagogische Psychologie,
3/4, 193-206.
Graesser, A. C., Person, N. K., & Magliano, J. P. (1995).
Collaborative dialogue patterns in naturalistic one-on-one
tutoring. Applied Cognitive Psychology, 9, 495-522.
Hinds, P. J. (1999). The curse of expertise: The effects of
expertise and debiasing methods on predictions of novice
performance. Journal of Experimental Psychology:
Applied, 5, 205-221.
Kalyuga, S. (2007). Expertise reversal effect and its
implications for learner-tailored instruction. Educational
Psychology Review, 19, 509-539.
Krauss, S., Brunner, M., Kunter, M., Baumert, J., Blum, W.,
Neubrand, M. et al. (2008). Pedagogical content
knowledge and content knowledge of secondary

319

