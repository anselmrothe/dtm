Factors Influencing Categorization Strategy in Visual Category Learning
Sujith Thomas (sujith@cse.iitk.ac.in)
Department of Computer Science and Engineering
Indian Institute of Technology Kanpur
India 208016

Harish Karnick (hk@cse.iitk.ac.in)
Department of Computer Science and Engineering
Indian Institute of Technology Kanpur
India 208016
Abstract
Studies in visual category learning show that participants use
different category generalization strategies. Some studies report a preference for a rule-based strategy, while others report
a preference for a similarity-based strategy. We conducted category learning experiments in which we varied three variables
— family resemblance of a category, saliency of the defining
rule and presentation of transfer stimulus after a delay. Our
results show that these factors influence the choice of category
generalization strategy. Our study offers a possible explanation
for the divergent results in the literature.

Figure 2: Two transfer stimuli for the target category in Figure 1. Stimulus A has the defining feature, but stimulus B is
more similar to the target category instances.
a similarity-based strategy. In our study, we try to identify the
variables that influence the choice of categorization strategy.

Keywords: visual category learning; supervised learning;
generalization; family resemblance; defining rule saliency

Studies on Supervised Learning
In the studies conducted by Xu and Tenenbaum (2007) and
Schmidt (2009), observational learning paradigm was used.
In each trial, participants were informed about the correct
category of three stimuli. The participants were then asked
to pick other items that might belong to the same category. Participants preferred a similarity-based generalization. Thomas and Karnick (2014) showed that participants
preferred similarity-based generalization even when the target category had a defining feature.
Feldman (2000) showed that categories based on a simple rule (conjunction of boolean features) were learned more
easily. Ashby, Maddox, and Bohil (2002) also showed
that a rule-based category structure was easier to learn.
In (Conaway & Kurtz, 2014; Rabi, Miles, & Minda, 2015;
Deng & Sloutsky, 2015), a category could be generalized using either a rule-based strategy or a similarity-based strategy.
The results showed that participants preferred a rule-based
strategy over a similarity-based strategy.
The studies discussed above indicate that participants use
different strategies for visual category generalization. In this
study, we try to identify the variables that influence the choice
of categorization strategy. Kloos and Sloutsky (2008) showed
that participants were able to learn the defining rule when the
family resemblance was low, but not when the family resemblance was high. So, family resemblance could be one factor
that influences the choice of categorization strategy.
The study conducted by Yim, Castro, Wasserman, and
Sloutsky (2014) showed that a category is easier to learn
when the defining rule can be identified more easily. So,
the ease with which the defining rule can be identified (rule
saliency) might also affect the categorization strategy.

Introduction
Studies in visual category learning investigate how humans
learn and generalize visual categories. Let us say that the
items shown in Figure 1 are true instances of a category. The
true instances have a common defining feature — the blue five
pointed star. Now consider the transfer stimuli shown in Figure 2. Stimulus A has the defining feature (blue star). Stimulus B does not have the defining feature, but is more similar to
the true instances shown in Figure 1. For the transfer stimuli
in Figure 2, two generalization strategies are possible: defining rule-based strategy and similarity-based strategy.

Figure 1: True instances of a target category. All the instances
have a common defining feature (blue five pointed star).
In this article, we refer to categorization based on a defining feature as rule-based strategy. If a rule-based strategy is
preferred, then participants will generalize stimulus A to the
target category, but not stimulus B. On the other hand, if a
similarity-based strategy is preferred, then stimulus B will be
generalized to the target category, but not stimulus A.
Observational learning is a supervised learning paradigm
where the participants are informed about the correct category
of the training stimuli. Participants are then asked to categorize the transfer stimuli. No feedback is provided. Some studies in the observational learning literature report a preference
for a rule-based strategy; while others report a preference for

596

In the studies that report similarity-based generalization,
the training and transfer stimuli are usually presented simultaneously (Xu & Tenenbaum, 2007; Schmidt, 2009; Thomas
& Karnick, 2014). In (Feldman, 2000), training stimuli were
presented simultaneously, but the transfer stimuli were presented in a separate testing block. In (Conaway & Kurtz,
2014; Rabi et al., 2015), the training stimuli were presented
one by one, and the transfer stimuli were presented in a separate testing block. The results in (Feldman, 2000; Conaway
& Kurtz, 2014; Rabi et al., 2015) show a preference for rulebased generalization, and in these studies the transfer stimuli
were presented in a separate testing block. So, presenting the
transfer stimuli separately could be another factor that might
be influencing the choice of categorization strategy.
So the three variables that could be influencing the choice
of categorization strategy are: family resemblance of a category, saliency of the defining feature (we will refer to this as
rule saliency) and delayed presentation of transfer stimuli. If
manipulating the above variables affects categorization strategy, then that would explain the divergent results in the visual
category learning literature.
In Experiment 1, we trained a regression model using behavioral data to predict the rule saliency. In Experiment 2,
we studied the effect of family resemblance and rule saliency
on category generalization. In Experiment 3, we studied the
effect of presenting the transfer stimulus after a delay. In the
next section, we explain how we quantified the notion of family resemblance and rule saliency. Quantification allows us to
study the effect of the independent variables as they change
in a continuous manner.

stances of a category. First, the entropy for each of the ten
stimuli dimensions is calculated as follows:
6

dimEnt( j) =

∑ −P(x jk ) log2 P(x jk )

(1)

k=1

where dimEnt( j) is the entropy of the jth dimension, x jk are
the distinct values that the jth dimension can take and P(x jk )
is the probability with which the value x jk occurs among the
instances of a category. The entropy of a category is found by
averaging the entropy across all the dimensions as follows:
entropy(C) =

1
10

10

∑ dimEnt( j)

(2)

j=1

where entropy(C) is the average entropy of the ten dimensions for category C. In our study, the value returned by
Equation 2 is considered to be the category entropy. Category entropy will be low when there is less variability (high
family resemblance) among the category instances. Category
entropy will be maximum when the non-defining feature values are unique for every instance of the category. The maximum possible value of category entropy in our study is 2.07,
because we have six distinct values for each dimension and
there can be at most eight non-defining features.
We trained a regression model using behavioral data to
quantify the notion of rule saliency. We explain this in more
detail in the next section.

Experiment 1
In our experiments, the defining feature is the part of the figure that is common across all the instances of a category. Visual saliency is defined as the perceptual quality that makes
a feature stand out from other features and grab our attention
(Itti, 2007). In order to quantify visual saliency, we have followed the intuition that participants would take less time to
detect the defining feature when it is more salient.
Participants were shown six stimuli like the ones depicted
in Figures 1 and 3. Their task was to correctly identify the
defining feature as quickly as possible. Participants could
choose the defining feature from among the four options that
appeared on the screen: boundary, star, polygon and spikewheel.
In order to change the rule saliency, the area covered by
the defining feature, and its color were randomly varied. The
time taken by a participant to detect the defining feature was
noted. This time was then used to quantify the visual saliency
as follows:

Family Resemblance and Rule Saliency
In Figure 1, the family resemblance between the true instances is high. But, the defining rule (blue star) is not easy
to spot. In other words, the rule saliency is low.
Figure 3 shows another set of target category instances.
Here the family resemblance between the instances is low.
But, the defining rule (red polygon) stands out from the other
features; therefore, the rule saliency is high.

Figure 3: True instances of a target category. All the instances
have a common defining feature (red polygon).
In our study, we use abstract figures having ten dimensions,
where each dimension can take six distinct values. The stimuli dimensions are — number of outer boundary sides, outer
boundary color, number of points in the star, star boundary
color, star fill color, number of inner polygon sides, inner
polygon boundary color, inner polygon fill color, number of
spike-wheel points and spike-wheel color.
We use the concept of entropy to quantify family resemblance of a category. We estimate the entropy using the in-

saliency(i) =

max p − time p (i)
max p − min p

(3)

where saliency(i) is the saliency of the defining feature for
the ith trial, max p is the maximum response time for the pth
participant, min p is the minimum response time for the pth
participant, and time p (i) is the response time for the ith trial

597

by the pth participant. In Equation 3, saliency(i) will give a
value of one for the trial in which a participant took minimum
time to respond. Saliency would be zero for the trial in which
a participant took maximum time. The saliency value given
by Equation 3 will lie in the range [0, 1] for all trials.

For each trial the di value for each of the six properties is
found. Then the maximum and minimum values for each di
across all the trials is found. The features for training a linear
regression model for saliency is calculated as follows:
di −min(di )

fi = e max(di )−min(di ) − 1

Participants and Procedure
There were 22 adult participants (Males=16, Females=6,
Mean age=24.27, S.D.=4.33). Participants were volunteers
from the student community. Each participant had to respond
to 35 trial questions. The first 15 questions were the practice
trials, and the remaining 20 were the experimental trials. So
the experiment gave us 440 (22 participants × 20 trials) data
points to train our regression model.
In each trial, six abstract figures were shown. These figures
were randomly generated. A part of the figure was randomly
selected to be the defining feature, and was given a random
size and color. Participants were expected to correctly identify the defining feature across the six abstract figures. The
response times for only the correct responses were recorded.
The maximum time permitted for the participants to identify
the common defining feature was 20 seconds.

(5)

where i ranges from 1 to 6, and min(di ) and max(di ) are the
minimum and maximum values of di respectively. The main
function of Equation 5 is to scale the di values so that the
corresponding features fi lies in the range [0, e − 1]. Features
similar to those described in this section have been previously
used to detect salient objects and regions of interest in images
(Osberger & Rohaly, 2001; Tian, Wan, & Yue, 2010).

Results
The features obtained using Equation 5 were used to train
a linear regression model for predicting the saliency values
calculated using Equation 3. The linear regression model was
trained using the gradient descent algorithm.
After training, the rule saliency values predicted by the
saliency model for the category instances in Figures 1, 3 and
4 were .05, .98 and .41 respectively. A value close to zero
means that rule saliency is low, and a value close to one means
that the rule saliency is high. The stimuli in Figures 1, 3 and 4
were not part of the training or the test data. This shows that
the values predicted by the regression model are reasonable.

Modeling Rule Saliency
In this section, we describe the features that were used to train
the linear regression model. First, the color value of the different parts of figures were converted from the RGB space to
the Lab space. The advantage of Lab space is that the perceptual difference between two colors is proportional to the
Euclidean distance between them.
The L value in Lab space corresponds to the lightness component of the color, and the a and b values correspond to the
color information. We have used the following six properties
of the different parts of the figure: L value, a value, b value,
color-saliency, area and perimeter. The color-saliency is estimated using the technique mentioned in (Gelasca, Tomasic,
& Ebrahimi, 2005).
For each of the above properties the difference (di ) between the average defining feature values and the average
non-defining feature values was found as follows:

|vi − wi | if i = 2 or i = 3
di =
(4)
vi − wi
otherwise

Experiment 2
In this experiment, we studied the effect of rule saliency and
category entropy on the generalization behavior. Figure 4
shows a sample screenshot. In each trial, six true and six false
instances of a category, and a transfer stimulus were shown.
Participants had to decide whether the transfer stimulus was
a true instance of the category. A participant could respond
by clicking Yes, No or Can’t decide.

where i ranges from 1 to 6 for each of the six properties mentioned above, vi is the average value of the ith property for the
defining feature and wi is the average value of the ith property
for the non-defining features. For each of the six property
values Equation 4 indicates how different the defining feature
is from the non-defining features.
In Equation 4, the absolute value of the difference is taken
for properties 2 and 3. These two properties correspond to the
a and b values in the Lab color space. If the a and b values of
the defining feature are very different from the non-defining
features, then it might make the defining feature more salient.
Here the sign of the difference should not matter. But the sign
of the difference matters for the remaining four properties.

Figure 4: Sample screenshot for Experiment 2. The defining
feature is the pink spike-wheel.
In each trial, the target and contrast category instances
were generated randomly. A part of the stimuli (boundary,
star, polygon or spike-wheel) was randomly selected to be
the defining feature. A multinomial distribution was used
to assign values to each of the non-defining dimensions. A

598

Results

Table 1: The four types of transfer stimuli.
Generated From
Defining Feature
Transfer type 1 Target Category
Present
Transfer type 2 Target Category
Absent
Transfer type 3 Contrast Category
Present
Transfer type 4 Contrast Category
Absent

Our results show that participants were able to correctly identify the type 1 stimuli as true instances, and type 4 stimuli as
false instances of the target category. The overall percentage
of correct, incorrect and can’t decide responses for type 1 and
type 4 stimuli were 78.75%, 11.25% and 10.0% respectively.
The 3D bars in Figure 5 show how the categorization strategy varied for transfer stimuli types 2 and 3. There were
640 datapoints. The median values of category entropy and
rule saliency across Experiments 2 and 3 were used to divide the 640 datapoints into four sets. The four sets corresponded to the high and low values of rule saliency and category entropy. The percentages of similarity-based responses
and rule-based responses for each set was found. Figure 5
shows that similarity-based responses (black bar) were preferred over rule-based responses (green bar).
The rule saliency values were obtained from the saliency
model, and lie in the range [0, 1]. Figure 6 shows the effect
of rule saliency on rule-based responses (green line). We divided the 640 datapoints into five equal sized bins (each having 128 datapoints). In Figure 6, the numbers in brackets denote the datapoints in each bin. Figure 6 plots the percentage
of rule-based responses for each of the five bins. In Figure 6,
the x error bars denote the range of rule saliency in each bin.
Figure 6 also shows the effect of rule saliency on similaritybased responses (black line). The x error bars for similaritybased responses (black line) align with the x error bars for
rule-based responses (green line) because they correspond to
the same five bins. Figure 6 shows that similarity-based responses were preferred over the rule-based responses for different values of rule saliency.
In our study, rule saliency and category entropy are continuous variables. For each trial, rule-based response is a binary variable. Point-biserial correlation coefficient gives the
correlation between a continuous variable and a binary variable. We use Pearson product moment correlation because it
is equivalent to the point-biserial correlation. The Pearson
correlation between rule saliency and rule-based responses
was found to be not significant (r(638) = .09, ns) 1 .
Figure 7 shows the effect of category entropy on rule-based
responses (green line), and similarity-based responses (black
line). The figure shows that similarity-based responses are
preferred over rule-based responses; except when entropy is
very high. When entropy is very high, the non-defining feature values of category members are very different from each
other. In this scenario, similarity will be mostly determined
by the defining feature. Hence, for high values of category
entropy there will be no difference between similarity-based
categorization and rule-based categorization.
The Pearson correlation between category entropy
and rule-based responses was found to be significant (r(638) = .12, p < .01). The maximum value that
the category entropy took in Experiment 2 was 1.85
(maximum possible value is 2.07).

multinomial distribution associates a probability with each of
the six values that a dimension can take. An instance was
generated by assigning to each dimension a value that was
sampled from the multinomial distribution. In each trial, a
random multinomial distribution was assigned for both target
and contrast categories.
Category entropy was calculated based on the category instances that were generated in each trial. Category entropy
captures the variability present in the instances that were
shown in each trial. So, once the instances are generated,
the multinomial distribution from which the instances were
generated is not important. In each trial, the saliency of the
defining feature was varied by changing its color and size.
Four types of transfer stimuli were used in our experiment.
Transfer types 1 and 2 were generated using the target category multinomial distribution. Transfer types 3 and 4 were
generated using the contrast category multinomial distribution. For the transfer types 2 and 3, the defining features were
from the opposite category (i.e. the category from which they
were not generated). Table 1 gives the details of the four types
of transfer stimuli that were used in our experiments.
The transfer types 1 and 4 were the true and false instances
of target category respectively. Transfer type 2 stimuli were
similar to the true instances, but did not have the defining
feature. Transfer type 3 stimuli had the defining feature, but
were not similar to the true instances. If participants used a
rule-based strategy, then transfer type 3 stimuli would be generalized to the target category, but not transfer type 2 stimuli.
The opposite would happen if participants used a similaritybased strategy.

Participants and Procedure
The 40 adult participants (Males=26, Females=14, Mean
age=22.00, S.D.=3.21) in Experiment 2 were volunteers from
the student community. Each participant responded to 10
practice trials followed by 20 experimental trials. No feedback was given to the participants during the trials. 800 data
points (40 participants × 20 trials) were obtained from the experiment. There were 640 datapoints corresponding to type
2 and type 3 transfer stimuli. Participants were made to respond to more type 2 and type 3 stimuli because these reveal
more about participant generalization behavior. The maximum response time limit was 40 seconds. The generalization
strategy of the participants was the dependent variable.

1 Here,

599

d f = 638 because there were 640 datapoints.

The results of Experiment 2 show that participants preferred similarity-based responses over rule-based responses.
Category entropy had a significant effect on categorization
strategy; but rule saliency did not.

Effect of Entropy on Responses
Similarity-based (Exp 2)
Rule-based (Exp 2)
Similarity-based (Exp 3)
Rule-based (Exp3)

Participant Responses (%)

80

(120)

60
(120)
40

(120)

(120) (120)

(128)

(128)
20

(128)

0
0.0

(128)

(128)

0.5
1.0
1.5
Entropy of Target/Contrast Category

2.0

Figure 7: Effect of category entropy on participant responses
for transfer types 2 and 3.
Figure 5: Rule-based and similarity-based responses for
transfer stimuli types 2 and 3 in Experiments 2 and 3.

Participant Responses (%)

60

0
0.0

(120)

(120)

(120)

Results

(120)

(120)
(128)

20

We had a new set of 50 adult participants (Males=30, Females=20, Mean age=21.88, S.D.=3.73). The participants
were volunteers from the student community. Each participant responded to 10 practice trials followed by 16 experimental trials. No feedback was given to the participants during the trials. We obtained 800 data points (50 participants ×
16 trials) from the experiment. Out of 800 data points, 600
corresponded to type 2 and type 3 transfer stimuli.

Effect of Rule Saliency on Responses
Similarity-based (Exp 2)
Rule-based (Exp 2)
Similarity-based (Exp 3)
Rule-based (Exp3)

80

40

Participants and Procedure

(128)

0.2

(128)

(128)

Our results show that participants were able to correctly categorize the type 1 stimuli as true instances, and type 4 stimuli
as false instances of the target category. The overall percentage of correct, incorrect and can’t decide responses for type 1
and type 4 stimuli were 87.5%, 10.0% and 2.5% respectively.
Figure 5 shows how the categorization strategy varied for
transfer stimuli types 2 and 3. There were 600 datapoints.
The median values of rule saliency and category entropy were
used to divide the 600 datapoints into four sets, that corresponded to the high and low values of rule saliency and category entropy. Figure 5 shows that rule-based responses (red
bar) were greater than similarity-based responses (blue bar)
when either rule saliency or category entropy was high.
Figure 6 shows the effect of rule saliency on categorization strategy. The 600 datapoints were divided into 5 equal
sized bins (each having 120 datapoints). Figure 6 shows that
for higher values of saliency rule-based responses (red line)
were preferred over the similarity-based response (blue line).
The rule-based responses were found to be correlated with
saliency (r(598) = .15, p < .001) 2 .
Figure 7 shows that rule-based responses (red line) were
preferred over similarity-based responses (blue line) for high
values of entropy. The rule-based responses were found to be
correlated with entropy (r(598) = .20, p < .0001).

(128)

0.4
0.6
0.8
Defining Rule Saliency

1.0

1.2

Figure 6: Effect of rule saliency on participant responses for
transfer types 2 and 3.

Experiment 3
In Experiment 2, all stimuli were shown simultaneously. In
Experiment 3, we wanted to study whether showing the transfer stimulus after a time delay will lead to a preference for
rule-based responses.
In Experiment 3, participants were shown true and false
instances of a category for a maximum duration of 40 seconds
— just as in Experiment 2. When participants clicked the
next button the screen would go blank for six seconds, and
then the transfer stimulus would be presented. As before, a
participant could select one of the three options — Yes, No
or Can’t decide. Apart from the delayed presentation of the
transfer stimulus, every other detail remained the same as that
in Experiment 2.

2 Here,

600

d f = 598 because there were 600 datapoints.

References

In Experiment 2, the number of rule-based and non-rulebased responses for type 2 and type 3 stimuli were 181 and
459 respectively (total 640 datapoints). In Experiment 3, the
number of rule-based and non-rule-based responses for type
2 and type 3 stimuli were 309 and 291 respectively (total 600
datapoints). The difference between the observed frequencies of responses across the two experiments was found to be
significant (χ2 (1, N = 1240) = 68.88, p < .0001).
The results of Experiment 3 show that presenting the transfer stimulus after a delay caused a significant increase in rulebased responses.

Ashby, F. G., Maddox, W. T., & Bohil, C. J. (2002).
Observational versus feedback training in rule-based and
information-integration category learning. Memory & cognition, 30(5), 666–677.
Conaway, N., & Kurtz, K. J. (2014). Now you know it, now
you dont: Asking the right question about category knowledge. In Proceedings of the 36th annual meeting of the
Cognitive Science Society (pp. 2062–2067).
Deng, W. S., & Sloutsky, V. M. (2015). The development of
categorization: Effects of classification and inference training on category representation. Developmental psychology,
51(3), 392–405.
Feldman, J. (2000). Minimization of boolean complexity in
human concept learning. Nature, 407(6804), 630–633.
Gelasca, E. D., Tomasic, D., & Ebrahimi, T. (2005). Which
colors best catch your eyes: a subjective study of color
saliency. In First international workshop on video processing and quality metrics for consumer electronics.
Itti, L. (2007). Visual salience. , 2(9), 3327. (revision 72776)
Kloos, H., & Sloutsky, V. M. (2008). What’s behind different kinds of kinds: Effects of statistical density on learning
and representation of categories. Journal of Experimental
Psychology: General, 137(1), 52–72.
Osberger, W. M., & Rohaly, A. M. (2001). Automatic detection of regions of interest in complex video sequences. In
Photonics west 2001-electronic imaging (pp. 361–372).
Rabi, R., Miles, S. J., & Minda, J. P. (2015). Learning categories via rules and similarity: Comparing adults and children. Journal of experimental child psychology, 131, 149–
169.
Schmidt, L. A. (2009). Meaning and compositionality as
statistical induction of categories and constraints. Unpublished doctoral dissertation, Massachusetts Institute of
Technology.
Thomas, S., & Karnick, H. (2014). Is word generalization for
novel concepts modelled by similarity or by formal concepts? In Cognitive 2014, the sixth international conference on advanced cognitive technologies and applications
(pp. 274–280).
Tian, M., Wan, S., & Yue, L. (2010). A color saliency model
for salient objects detection in natural scenes. In Advances
in multimedia modeling (pp. 240–250). Springer.
Todd, J. J., & Marois, R. (2004). Capacity limit of visual
short-term memory in human posterior parietal cortex. Nature, 428(6984), 751–754.
Xu, F., & Tenenbaum, J. B. (2007). Sensitivity to sampling
in bayesian word learning. Developmental science, 10(3),
288–297.
Yim, H., Castro, L., Wasserman, E. A., & Sloutsky, V. M.
(2014). The interactions of category structure and supervision in category learning: a comparative approach. In
Proceedings of the 36th annual meeting of the Cognitive
Science Society (pp. 1814–1819).

Discussion and Conclusion
The aim of our study was to identify the variables that cause
participants to prefer one categorization strategy over another.
When the training and transfer stimuli were presented simultaneously, participants preferred similarity-based generalization. When transfer stimuli was presented after a six seconds
delay, participants preferred rule-based responses for higher
values of rule saliency and category entropy.
Our Experiment 2 results are consistent with the results
in (Xu & Tenenbaum, 2007; Schmidt, 2009; Thomas & Karnick, 2014), that report a preference for similarity-based generalization. In (Kloos & Sloutsky, 2008), the transfer stimuli
was presented in a separate testing block. Our Experiment 3
results show that participants prefer similarity-based generalization when category entropy is low, but rule-based generalization is preferred when entropy is high. This result is consistent with (Kloos & Sloutsky, 2008). In (Feldman, 2000;
Conaway & Kurtz, 2014; Rabi et al., 2015), transfer stimuli was presented in a separate testing block, and participants
preferred rule-based generalization. The Experiment 3 results
can explain the above results as well.
When transfer stimulus is presented after a delay, participants use their short term memory to remember the training stimuli. Visual short term memory has a limited capacity (Todd & Marois, 2004). So, remembering all the features
of the training stimuli would become difficult. This might be
causing the participants to use a simple rule-based strategy,
which puts less demand on the short term memory.
We have quantified family resemblance using category entropy, which does not take into account the saliency of different features. This allows us to independently vary category
entropy and rule saliency. But, when defining rule saliency is
increased it should lead to greater family resemblance. Category entropy does not take this effect into account.
Many studies in the literature make use of artificial categories. Artificial stimuli allow us to study the learning behavior in the absence of prior knowledge about the categories.
In this study, we have manipulated three variables: family
resemblance, rule saliency and delayed presentation of transfer stimulus. Our results show that these variables influence
the choice of categorization strategy. Our study offers possible explanations for the divergent results in the visual category learning literature.

601

