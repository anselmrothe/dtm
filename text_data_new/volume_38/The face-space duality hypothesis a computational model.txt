The face-space duality hypothesis: a computational model
Jonathan Vitale (jonathan.vitale@student.uts.edu.au)
Mary-Anne Williams (mary-anne.williams@uts.edu.au)
Benjamin Johnston (benjamin.johnston@uts.edu.au)
University of Technology, Sydney
QCIS Centre - Innovation and Enterprise Research Lab, 15 Broadway - Ultimo NSW 2007
Abstract

Can Valentine’s framework support an integral understanding of identity and expression processing? Calder and colleagues (2001) demonstrated that a multidimensional space
derived from a principal component analysis (PCA) (Turk
& Pentland, 1991) can provide a set of components being either identity-independent, expression-independent or
identity-expression-interdependent (Calder & Young, 2005).
However, in order to perform identity and expression recognition, the authors used two distinct latent discriminant analysis (LDA) modules, one choosing the best components to
support identity recognition, whereas the other choosing the
best components to support expression classification.
In this paper, we show that a single face-space with a
twofold representation supports both identity and expression
recognition. The structure of this face-space can be realized
in a parsimonious way by integrating both identity and expression in a single model. We demonstrate the computational validity of our hypothesis through a rigorous mathematical presentation and related experiments. This work further supports the integral nature of identity and expression, at
least from a computational perspective.

Valentine’s face-space suggests that faces are represented in a
psychological multidimensional space according to their perceived properties. However, the proposed framework was initially designed as an account of invariant facial features only,
and explanations for dynamic features representation were neglected. In this paper we propose, develop and evaluate a computational model for a twofold structure of the face-space, able
to unify both identity and expression representations in a single
implemented model. To capture both invariant and dynamic
facial features we introduce the face-space duality hypothesis
and subsequently validate it through a mathematical presentation using a general approach to dimensionality reduction.
Two experiments with real facial images show that the proposed face-space: (1) supports both identity and expression
recognition, and (2) has a twofold structure anticipated by our
formal argument.
Keywords: face perception; face processing; face-space; duality hypothesis; dimensionality reduction

Introduction
As an explanation of findings in face perception, Valentine
used formal models of concept representations to propose that
faces are represented in a psychologically plausible multidimensional space, i.e. the face-space (Valentine, 1991). Faces
are points of this space based on their perceived properties.
Valentine’s formal models have been used to explain results
of many human experimental studies, as well as computational simulations (Calder, Burton, Miller, Young, & Akamatsu, 2001; Lee, Byatt, & Rhodes, 2000; Rhodes et al.,
2011).
These models were initially designed to only account for
coding identity-related features, such as sex, distinctiveness,
age and attractiveness (Calder et al., 2001). For example,
the feature ‘eyebrows’ can vary from marked to delicate, thus
possibly being one of the perceivable features crucial for coding the sex of a face. However, dynamic aspects of faces,
such as facial expressions, were neglected. Early brain lesion
and neuroimaging studies suggested that face identity and expression are not integral dimensions1 , instead they are represented and processed by separate systems that process faces
in parallel (Bruyer et al., 1983; Tranel, Damasio, & Damasio, 1988). However, contemporary understanding indicates
that identity and expression are more closely connected than
previously thought, suggesting that common-codings can respond to both of them and so processes of their perception
can interact (Ganel, Valyear, Goshen-Gottstein, & Goodale,
2005; Kadosh et al., 2016; Rhodes et al., 2015).

Findings in Face Perception
Valentine’s ‘face-space’ framework (Valentine, 1991) is a notable cognitive model for face representation. According to
this framework, facial representations are encoded in a multidimensional psychological space. The dimensions of this
space are assumed to encode properties of the facial signals that better discriminate one face from another. The distance between two representations underlies their dissimilarity from a psychological perspective.
Identity and expression are two forms of facial information
crucial for many social skills. Identity is considered an invariant feature of face, whereas expression a dynamic one. Traditional cognitive models of face perception suggest a complete separation of identity and expression systems after the
completion of a structural encoding stage (Bruce & Young,
1986). Accordingly, Haxby et al. (2000) argue that invariant
features, such as identity, and dynamic features, such as facial
expression, are computed by separate regions of the brain.
However, new evidence from recent findings indicates that
these systems operate interdependently (Pell & Richards,
2013). For example, Ganel and Goshen-Gottstein (2004)
found that familiarity of faces increases the perceptual interdependence of identity and expression recognition. They
suggested that differences between the facial configurations
of individuals should lead to systematic differences in the

1 Here and for the rest of the paper we use the term ‘integral’ to
define properties exhibiting inter-dependencies and not completely
separable.

514

way emotions are expressed by these individuals. For this
reason, every individual can express each facial expression
in a unique way. Knowledge of the identity of the observed
subject can therefore facilitate the process of his or her facial
expression.
We found a similar effect in developing a computational
model inspired by simulation-theories (Vitale, Williams,
Johnston, & Boccignone, 2014). In this endeavour, we suggested to first pre-process the observed face so to reduce its
identity-related information, while preserving motor components information (i.e. its dynamic features). This facilitates
the recognition of facial expression based on feed-forward
mechanisms of internal motor simulation. A similar account
was recently supported by a human study of Ipser and Cook
(2015).
Calder et al. (2001) demonstrated the validity of integral identity and expression representations from a computational perspective. They submitted digital images of faces
showing different identities and facial expressions to PCA
thus obtaining representations based on components of a lowdimensional space. Their results demonstrated that this common representation can support both identity and expression
recognition and that the representations of identity and expression partially overlap.

functions CE (·) → R and CI (·) → R, respectively providing the number of correctly classified facial expressions and
identities from a set of observations encoding faces, and a
permutation function σ over the coordinates (y1 , y2 , . . . , yd )
of a point y defined as:
 1

y
y2
y3
. . . yd
σ=
(1)
yd yd−1 yd−2 . . . y1
We make use, here and for the rest of the paper, of the superscript ˜· to denote a point or set of points to which is applied
the permutation in (1). Then, given a set of perceived observations Φ and the associated multidimensional spatial representations Y , the mapping function S is defined such that:
1. S(Φ) 7→ Y ;
2. CE (Y )  CE (Φ);
3. CI (Ỹ )  CI (Φ);
In other words, the face-space duality hypothesis assumes
a function S mapping the perceived faces onto a psychological multidimensional space having a twofold structure. This
new representation allows the encodings Y = {y1 , y2 , . . . , yn }
and associated permutations Ỹ = {ỹ1 , ỹ2 , . . . , ỹn } to support
significantly higher recognition rates than the original input
representation Φ.
The rationale behind this idea is that the dual face-space,
by maximising the separation between dynamic and invariant features of the face in a single multidimensional representation, will order the components of the resulting space
in such a way that the first ones will mostly correlate with
dynamic features of the face, whereas the latter will mostly
correlate with invariant features of the face. Therefore, the
resulting face-space would provide a single multidimensional
representation (as per point (i)) where invariant and dynamic
features of the face are interdependent (as per point (ii)), but
preserving a certain degree of separation able to support subsequent classification processes (as per point (iii)).
We investigate our hypothesis from a computational perspective, validating it using a mathematical analysis of a general dimensionality reduction framework used in face recognition, by limiting the analysis to facial identity and expression only. We further confirm our approach with experimental
results.

The Face-Space Duality Hypothesis
In the previous section we provided studies supporting:
i A multidimensional spatial representation of faces as a
plausible model for explaining many crucial effects in
face perception;
ii An interdependence between representations of invariant
and dynamic facial features;
iii That enhancing the separation between invariant and dynamic components of the face during face processing facilitates their classification.
Given this brief summary, we introduce the ‘face-space duality hypothesis’, suggesting that faces: (i) are encoded in
a multidimensional face-space, (ii) under a common integral
representation (iii) having a twofold structure: one supporting invariant features of the face (e.g. identity), whereas the
other supporting dynamic features of the face (e.g. expression), thus contributing to their correct classification.
This hypothesis arises from the need to accommodate the
apparently contrasting points (ii) and (iii) under a single representation based on a multidimensional space (i), which is an
extension of the face-space framework proposed by Valentine
(1991).
Consider a perceived face φi and a corresponding ddimensional point yi of a multidimensional psychological
space. The spatial representation yi encodes most of the original information of the input face φi and can be obtained
through the mapping function S(φi ) 7→ yi . We introduce the

Dimensionality Reduction Models
The function S introduced above can be modeled as a dimensionality reduction function. A dimensionality reduction
function maps a high-dimensional signal onto a point of a
low-dimensional space. For example, consider an image of a
face having resolution 100 × 100 pixels. This observed signal
is represented by a set of pixels and can be posed as a column
vector φi of dimension D = 10000. Dimensionality reduction models provide a mapping function S : RD ×1 → Rd×1 ,
with d  D, such that the low-dimensional representation

515

yi = S(φi ) is able to explain the observed data φi (Yan et al.,
2007).
Linear dimensionality reduction techniques make use of a
linear projection matrix V ∈ RD ×d in order to map the high
dimensional observed sample onto the low-dimensional target space. So the projection yi of an observation φi can be
computed as yi = V > φi . When V is an orthogonal matrix,
an approximation of the original observation can be reconstructed from its projection: φi ≈ V yi . The projection matrix
V can be estimated by solving an objective function. This objective function models desired constraints that the structure
of the target low-dimensional space is required to satisfy.
For the purposes of this paper we limit our investigation
to provide an implementation of our model through linear
dimensionality reduction techniques.

estimated. Finally, the overall mapping matrix able to reduce
the dimensionality from dimension D to dimension d and performing the constraints specified in the objective function (2)
is given by:
Voverall = VPCAV ?
(3)

Model implementation
Consider a set Φ of N observations of frontal faces. Each
observation consists of D pixel values, represented by a D ×1
vector. In this work we limit the investigation of observations
varying only in identity and facial expression.
We set a dimension d < N  D and we estimate a mapping matrix VPCA ∈ RD ×d by submitting the samples Φ to a
PCA, thus obtaining the corresponding PCA-encodings X =
> Φ. We aim to estimate another mapping matrix V ? ∈
VPCA
Rd×d , such that the final overall matrix Voverall = VPCAV ? validates our hypothesis.
We denote the identity class of the sample xi with I (xi )
and the facial expression class of the sample xi with E (xi ).
Considering the previously introduced scenario, we start
by designing the appropriate similarity and penalty matrices.
Invariant features of the face extend to more regions of the
face than dynamic ones, thus explaining most of the variance
in the considered dataset of observations (Turk & Pentland,
1991). This means that during facial expression classification
the identity can potentially introduce a bias on the samples
(the identity-bias), thus increasing their similarity and reducing their distance in the face-space even when they belong to
a different class of facial expression (Sariyanidi, Gunes, &
Cavallaro, 2015).
Therefore, as a first step we design our similarity matrix
to encourage pairs of samples associated with the same facial
expression to be in close proximity in the resulting space, and
our penalty matrix to provide a repulsive force between pairs
of samples belonging to the same identity. This would result
in maximising the separation between dynamic and invariant
components of the face, thus facilitating the classification of
facial expression.
Accordingly, we define the similarity matrix W E as:
(
1
, if E (xi ) = E (x j )
E
(4)
Wi j = nEi
0,
otherwise.

Graph-based dimensionality reduction framework
Yan et al. (2007) provided a general framework for unifying
many dimensionality reductions models. Since our approach
makes use of this framework, we first briefly introduce it below.
Let Φ = [φi , . . . , φn ] be a matrix of the N observations represented as column vectors with dimension D. The structure of the target low-dimensional space can be constrained
by a similarity matrix W and a penalty matrix W (p) . For
each pair of samples (φi , φ j ), the similarity matrix Wi j encodes the associated non-negative similarity measure, whilst
(p)
the penalty matrix Wi j encodes the associated penalty measure. This penalty measure can be used as a repulsive force
between pairs of samples to prevent samples with high similarity but belonging to different classes from being placed in
close proximity in the low-dimensional space (Kokiopoulou
& Saad, 2009).
Given these two graph structures, the optimal mapping matrix V ? can be found by solving the following objective function:
V ? = arg min
V ∈RD ×d

Tr(V > ΦLΦ>V )
Tr(V > ΦL(p) Φ>V )

(2)

with Tr(·) denoting the matrix trace operator and L, L(p) respectively being the resulting Laplacian matrices computed
from the similarity and penalty matrices W and W (p) (Yan et
al., 2007).
Unfortunately, there is no closed-form solution to this optimization problem (Ngo, Bellalij, & Saad, 2012). However,
the problem can be solved numerically with iterative algorithms whenever the matrix ΦL(p) Φ> is positive definite. The
resulting optimal solution V ? is unique up to unitary transforms of the columns (Ngo et al., 2012).
To ensure that matrix ΦL(p) Φ> is positive definite, the process is usually split into two phases. First, given a dimension
D 0 < N, the observations Φ are provided in input to a PCA,
0
and a first mapping matrix VPCA ∈ RD ×D is estimated. Then,
> Φ are provided as input to the objective
the samples X = VPCA
0
function in (2) and the optimal mapping matrix V ? ∈ RD ×d is

where nEi is the number of samples in X belonging to facial
expression class E (xi ).
Similarly, we define the penalty matrix W I as:
(
1
, if I (xi ) = I (x j )
I
Wi j = nIi
(5)
0,
otherwise.
where nIi is the number of samples in X belonging to identity
class I (xi ).
By using the proposed similarity and penalty matrices,
the resulting Laplacians becomes L = IN − W E and L(p) =
IN − W I , with IN a N × N identity matrix. Hence, these

516

lowing objective function:
arg max
V ∈Rd×d

Figure 1: Some examples of prototypes. On the left are two
prototypical identities (F05 and M07) in which expressionrelated features are reduced, whereas on the right are two examples of prototypical facial expressions (happiness and surprise).

V ∈Rd×d

Tr(V > X(IN −W E )X >V )
Tr(V > X(IN −W I )X >V )

arg min
V ∈Rd×d

Tr(V > X(IN −W I )X >V )
Tr(V > X(IN −W E )X >V )

(8)

By reminding that a centring matrix is symmetric idempotent and Tr(AA> ) = kAk22 , it is possible to note that the objective function (6) attempts in minimising the distances between the encodings Y = V > X and their corresponding protoE
typical facial expressions Yproto
= V > XW E , while maximising their distances with respect to the prototypical identity
I
Yproto
= V > XW I , overall facilitating expression recognition.
Conversely, the objective functions (7, 8) attempts in minimising the distances between the encodings Y = V > X and
I
their corresponding prototypical identities Yproto
= V > XW I ,
while maximising their distances with respect to the protoE
typical facial expression Yproto
= V > XW E , overall facilitating
identity recognition.
Since the eigenvectors of V ? are estimated from the same
matrix G (ρ? ) = M E − ρ? M I in both the objective functions
(6,7), the components of the two spaces are the same, but
differing by order. In other words, given V ? as the optimal
matrix resulting from objective function (6) we can easily get
the optimal matrix of the objective function (7) Ṽ ? , defined
as a matrix with the same columns of V ? , but arranged in an
inverse order.
Finally, given the matrix VPCA ∈ RD ×d and the matrix
?
V ∈ Rd×d we can estimate the final mapping matrix Voverall
of the face-space through equation (3), which leads respec>
tively to the mapping Y = Voverall
X and the associated permu>
tated mappings Ỹ = Ṽoverall X as suggested by our hypothesis.
Note that we are not claiming that this is the only way to
implement the proposed mapping function S (for example it
can be generalised to non linear models), and neither that human brain implement the suggested dual face-space in this
way. However, this is a viable computational implementation of the proposed model able to support our hypothesis. In
the remainder of this paper we further support the face-space
duality hypothesis with experimental data.

(6)

We solve the objective function in (6) with the iterative algorithm proposed in (Ngo et al., 2012). Given the matrices
M E = X(IN − W E )X > and M I = X(IN − W I )X > the optimal mapping matrix V ? can be found through the algorithm
(1).
Data: Matrices M E , M I , a maximum number of
iterations K and a tolerance ε.
Result: A mapping matrix V of dimension D × d.
V ← ID×d ;
for i ← 1 to K do
ρ←

(7)

Using simple properties of trace and eigenvalues, it follows
that the objective function in (7) can be equivalently posed as:

Laplacians behave as block centring matrices. These matrices
remove respectively the corresponding prototypical facial expression (i.e. an average identity showing the averaged facial
expression) and the corresponding prototypical identity (i.e.
the considered identity showing a neutral facial expression)
from samples X (see figure (1)).
The objective function in (2) becomes:
arg min

Tr(V > X(IN −W E )X >V )
Tr(V > X(IN −W I )X >V )

Tr(V > M E V )
;
Tr(V > M I V )

G (ρ) ← M E − ρM I ;
Compute the smallest (for minimisation) or largest
(for maximisation) d eigenvalues [λ1 , . . . , λd ] ≡ Λ of
G (ρ) and associated eigenvectors [v1 , . . . , vd ] ≡ V ;
if | ∑dj=1 Λ| < ε then
break;
end
end
Algorithm 1: Newton-Lanczos algorithm for optimization of objective function (6).

Experiments
We further validate our hypothesis using images from
the Karolinska Directed Emotional Faces (KDEF) dataset
(Lundqvist, Flykt, & Öhman, 1998). The dataset contains
static images of 70 subjects—35 female and 35 male—
exhibiting 7 different prototypical facial expressions of basic
emotions (anger, disgust, fear, happiness, neutral, sadness and
surprise). The pictures are taken in different face orientations
and in two different sessions (A and B).
We used the frontal pictures taken in session A. The facial region was extracted from the images and its resolution

From the algorithm (1), it can be seen that the optimal mapping matrix V ? is the set of eigenvectors associated with the
smallest eigenvalues of G (ρ? ), with ρ? being the result of the
trace ratio in (6) when posing the optimal solution V ? . If,
instead of taking the eigenvectors associated with the smallest eigenvalues, we take the eigenvectors associated with the
largest eigenvalues, we get the optimal solution for the fol-

517

(a) Expression recognition task

(b) Identity recognition task

(c) Components used in recognition tasks

Figure 2: Results of the proposed experiments.
reduced to 80 × 80 pixels. Eyes and mouth were at approximately the same position. Illumination variations were reduced by applying a simple equalization process to the images.
We first pre-processed the data by submitting the pixels of
the images in input to a PCA as explained previously. In the
first experiment we retained the components able to explain
95% of the variance of the original data resulting in 200 components, while in the second experiment we retained the data
explaining the 85% (so reducing the number of components
and allowing better readability) resulting in 100 components.
We performed two experiments: the first to test the ability
of the proposed face-space in supporting identity and expression recognition, and the second to demonstrate the twofold
nature of the resulting face-space.

space. The recognition rates in each dimension were averaged among each cross-validation test.
The results for facial expression and identity recognition
are shown, respectively, in figures (2a) and (2b). It is clear
that this face-space derived by a single integrated process can
support both identity and expression recognition, whereas a
simple PCA cannot overcome the baseline performance.

Validation of face-space twofold structure
We were able to confirm our hypothesis on the twofold structure of the face-space using data.
We estimated the mapping matrix Voverall as per equations
(6, 3) using the full dataset as training data.
Given the matrix Voverall = [v1 , . . . , vd ] the minimum set
of expression components for a sample φi is the smallest set
Vimin = [v1 , . . . , vk ] such that yi = Vi>
φ is classified with the
min i
correct expression label E (φi ) through nearest neighbour algorithm, as in the previous experiment.
Similarly, given the matrix Ṽoverall = [vd , . . . , v1 ] the minimum set of identity components for a sample φi is the smallest
set Ṽimin = [vk , . . . , v1 ] such that yi = Ṽi>
φ is classified with
min i
the correct identity label I (φi ) through nearest neighbour algorithm, as in the previous experiment.
For each sample φi we computed its minimum set of expression and identity components. Then, we set nEk the number of times the component k was included in the minimum sets of expression components and nI
k the number of
times the component k was included in the minimum sets
of identity components. We computed the final results for
expression and identity and for each component k as fkE =

Support of identity and facial expression recognition
In the first experiment we test the ability of our model to support subsequent processes of identity and facial expression
recognition. We used a 10-fold cross validation approach.
For each iteration we divided the data by taking one fold as
test and the rest for model training.
With each training data we estimated the mapping matrix
Voverall as per equations (6, 3). Then we mapped each test
data onto the corresponding face-space, thus obtaining the en>
>
codings Y E = Voverall
Φ and Ỹ I = Ṽoverall
Φ respectively used
during the expression and identity recognition tasks.
The classification was performed using the nearest neighbour algorithm. For each sample φi , xi and yi we computed
the Euclidean distances with respect to the centroids of each
class in the corresponding space (i.e. the prototypical identities in the case of identity recognition or the prototypical
expressions in the case of facial expression recognition) and
selected the label associated with the centroid having lower
distance to the sample. We repeated this process for each
dimension k by taking only the first k components of the encodings xi and yi . In classifying the raw observations φi for
a baseline comparison, we considered all the pixel values of
the input images as coordinates of points in a D-dimensional

nE

nI

log( Nk × 100 + 1) and fkI = log( Nk × 100 + 1). Here N is
the number of samples in the dataset (i.e. 490) and we used
the logarithm for better readability of the results. The resulting log-frequencies are illustrated in Figure (2c).
From the results two peaks placed in the extremes of the
face-space components are clearly visible. There are expression components clearly independent from identity ones
(components #1 to #52), components shared among expression and identity classification tasks (components #53 to #99)

518

and just one identity component independent from expression ones (component #100). These results are in agreement
with the study of Ganel and Goshen-Gottstein (2004), which
suggest that expression is perceptually separable from identity, but identity is not perceptually separable from expression. This experiment further supports the proposed twofold
structure of face-space as suggested by the face-space duality
hypothesis.

Ipser, A., & Cook, R. (2015). Inducing a concurrent motor load reduces categorization precision for facial expressions. Journal of Experimental Psychology Human Perception & Performance.
Kadosh, K. C., Luo, Q., de Burca, C., Sokunbi, M. O., Feng,
J., Linden, D. E., & Lau, J. Y. (2016). Using real-time
fmri to influence effective connectivity in the developing
emotion regulation network. NeuroImage, 125, 616–626.
Kokiopoulou, E., & Saad, Y. (2009). Enhanced graph-based
dimensionality reduction with repulsion laplaceans. Pattern Recognition, 42(11), 2392–2402.
Lee, K., Byatt, G., & Rhodes, G. (2000). Caricature effects,
distinctiveness, and identification: Testing the face-space
framework. Psychological Science, 11(5), 379–385.
Lundqvist, D., Flykt, A., & Öhman, A. (1998). The karolinska directed emotional faces (kdef). CD ROM from Department of Clinical Neuroscience, Psychology section,
Karolinska Institutet, 91–630.
Ngo, T. T., Bellalij, M., & Saad, Y. (2012). The trace ratio
optimization problem. SIAM review, 54(3), 545–569.
Pell, P. J., & Richards, A. (2013). Overlapping facial expression representations are identity-dependent. Vision research, 79, 1–7.
Rhodes, G., Jaquet, E., Jeffery, L., Evangelista, E., Keane,
J., & Calder, A. J. (2011). Sex-specific norms code face
identity. Journal of Vision, 11(1), 1.
Rhodes, G., Pond, S., Burton, N., Kloth, N., Jeffery, L., Bell,
J., . . . Palermo, R. (2015). How distinct is the coding of
face identity and expression? evidence for some common
dimensions in face space. Cognition, 142, 123–137.
Sariyanidi, E., Gunes, H., & Cavallaro, A. (2015). Automatic
analysis of facial affect: A survey of registration, representation, and recognition. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 37(6), 1113–1133.
Tranel, D., Damasio, A. R., & Damasio, H. (1988). Intact recognition of facial expression, gender, and age in
patients with impaired recognition of face identity. Neurology, 38(5), 690–690.
Turk, M., & Pentland, A. (1991). Eigenfaces for recognition.
Journal of cognitive neuroscience, 3(1), 71–86.
Valentine, T. (1991). A unified account of the effects
of distinctiveness, inversion, and race in face recognition.
The Quarterly Journal of Experimental Psychology, 43(2),
161–204.
Vitale, J., Williams, M.-A., Johnston, B., & Boccignone, G.
(2014). Affective facial expression processing via simulation: A probabilistic model. Biologically Inspired Cognitive Architectures, 10, 30–41.
Yan, S., Xu, D., Zhang, B., Zhang, H.-J., Yang, Q., & Lin,
S. (2007). Graph embedding and extensions: a general
framework for dimensionality reduction. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(1),
40–51.

Conclusions
In this paper we extended Valentine’s face-space framework
to better explain recent findings in face perception.
In alignment with recent studies in face processing, we
suggested that the nature of identity and expression dimensions is highly interdependent. We proposed that the structure
of the face-space can result from a single process integrating
invariant and dynamic features of the face. From this process
results a twofold structure. Reading the components of this
space from the first to the last facilitates classification of dynamic features of the face, such as facial expression, while
reading them from the last to the first better supports classification of invariant features, such as the identity.
We validated the face-space duality hypothesis, from a
computational perspective, through a formal mathematical
presentation, by considering a general framework of dimensionality reduction. The hypothesis was further supported by
experimental data.
This framework might serve as the basis and inspiration for
future empirical work on face processing and linked to neural
approaches.

References
Bruce, V., & Young, A. (1986). Understanding face recognition. British journal of psychology, 77, 305–327.
Bruyer, R., Laterre, C., Seron, X., Feyereisen, P., Strypstein,
E., Pierrard, E., & Rectem, D. (1983). A case of prosopagnosia with some preserved covert remembrance of familiar
faces. Brain and cognition, 2(3), 257–284.
Calder, A. J., Burton, A. M., Miller, P., Young, A. W., &
Akamatsu, S. (2001). A principal component analysis of
facial expressions. Vision research, 41(9), 1179–1208.
Calder, A. J., & Young, A. W. (2005). Understanding the
recognition of facial identity and facial expression. Nature
Reviews Neuroscience, 6(8), 641–651.
Ganel, T., & Goshen-Gottstein, Y. (2004). Effects of familiarity on the perceptual integrality of the identity and
expression of faces: the parallel-route hypothesis revisited.
Journal of Experimental Psychology: Human Perception
and Performance, 30(3), 583.
Ganel, T., Valyear, K. F., Goshen-Gottstein, Y., & Goodale,
M. A. (2005). The involvement of the fusiform face area
in processing facial expression. Neuropsychologia, 43(11),
1645–1654.
Haxby, J. V., Hoffman, E. A., & Gobbini, M. I. (2000).
The distributed human neural system for face perception.
Trends in cognitive sciences, 4(6), 223–233.

519

