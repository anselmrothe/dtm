Beyond Markov: Accounting for Independence Violations in Causal Reasoning
Bob Rehder (bob.rehder@nyu.edu)
Department of Psychology, New York University
6 Washington Place, New York, NY 10003 USA
Abstract

properties (e.g., have the same causal strength) in this work,
symmetry entails that 𝐷(𝑌! , 𝑌! ) = 𝐷(𝑌! , 𝑌! ), which will
therefore be abbreviated 𝐷(𝑌).
Analogously, 𝐷(𝑌! , 𝑌! |𝑥 ! )—the difference in (the logit
of) the probability of 𝑌! given the presence or absence of 𝑌! ,
conditioned on 𝑥 ! —is defined as,

Although many theories of causal cognition are based on causal
graphical models, a key property of such models—the independence relations stipulated by the Markov condition—is
routinely violated by human reasoners. Two accounts of why
people violate independence are formalized and subjected to
experimental test. Subjects’ inferences were more consistent
with a dual prototype model in which people favor network
states in which variables are all present or all absent than a
leaky gate model in which information is transmitted through
network nodes when it should normatively be blocked. The
article concludes with a call for theories of causal cognition
that rest on foundations that are faithful to the kinds of causal
inferences people actually draw.

𝐷(𝑌! , 𝑌! |𝑥 ! )    =   𝑙𝑜𝑔𝑖𝑡  (𝑝(𝑦!! |𝑦!! , 𝑥 ! ))    −   𝑙𝑜𝑔𝑖𝑡  (𝑝(𝑦!! |𝑦!! , 𝑥 ! ))  

The last 25 years has seen a dramatic increase in research
asking how causal knowledge influences many acts of cognition, including reasoning, categorization, decision making,
and learning. Theory in this area has been advanced by use
of the formalism known as causal graphical models that
provides a good first order approximation of human abilities. Nevertheless, there is now considerable evidence that a
defining feature of these models—the Markov condition—is
routinely violated by reasoners. This article describes these
violations and then compares two new models that account
for them, albeit in different ways. The diverging predictions
of these models are then tested in a new experiment.

and is abbreviated 𝐷(𝑌|𝑥 ).
The normative predictions for 𝐷(𝑌) and 𝐷(𝑌|𝑥 ! ) are presented in the left chart of Fig. 1 assuming that the common
cause graph is parameterized such that 𝑝(𝑥 ! )    =    .50, that
the generative “causal power” associated with both 𝑋 → 𝑌!
and 𝑋 → 𝑌! is .80, that there are weak alternative causes of
𝑌! and 𝑌! such that 𝑝(𝑦!! |𝑥 ! ) = 𝑝(𝑦!! |𝑥 ! ) = .20, and that
multiple causal influences integrate according to a standard
noisy-or function (Cheng, 1997). The figure illustrates the
independence relations that characterize a common cause
network: Whereas 𝐷(𝑌)    > 0, 𝐷(𝑌|𝑥 ! )    = 0, that is, the 𝑌s
are independent conditioned on 𝑥 ! .
Normative Model

Y1
X
Y2

Independence Violations in Causal Reasoning
The causal networks in Figs. 1 and 2 provide two prominent examples of how human reasoners violate the independence relations associated with causal graphic models.
Starting with the common cause network in Fig. 1 in which
𝑋 is a common cause of 𝑌! and 𝑌! , whereas 𝑌! and 𝑌! are
unconditionally dependent (e.g., one can reason from the
presence of 𝑌! to the likely presence of 𝑋 and then to the
likely presence of 𝑌! ) the Markov condition stipulates that
they are independent conditioned on  𝑋 (i.e., 𝑌! ⊥ 𝑌! |𝑋). In
this case 𝑋 “screens off” the 𝑌s from one another (Pearl,
2000; Spirtes, 2000). Because it will be useful to characterize the sign and magnitude of the dependence between two
variables, I define a function 𝐷 (for “delta”) that characterizes the difference in the probability of one variable given
the presence or absence of another. Moreover, I define that
difference in terms of probabilities transformed into log
odds. For example, 𝐷(𝑌! , 𝑌! ) is defined as,
𝐷(𝑌! , 𝑌! )    =   𝑙𝑜𝑔𝑖𝑡  (𝑝(𝑦!! |𝑦!! ))    −   𝑙𝑜𝑔𝑖𝑡  (𝑝(𝑦!! |𝑦!! ))   (1)
where 𝑦!! denotes 𝑌! = 𝑥 (e.g., 𝑦!! means 𝑌! is present). Because within-network causal relations will have the same

(2)

!

Empirical Results

2.10

36

1.40

24

0.70

12

0.00

0

-0.70

-12

D(Y)

D(Y|X=1)

D(Y)

D(Y|X=1)

Figure 1. Normative predictions for the common cause model
on the left and the corresponding empirical rating from Rehder &
Waldmann (2015). Error bars are 95% confidence intervals.

The right chart of Fig. 1 illustrates how that independence
relation is typically violated. Rehder & Waldmann (2015)
instructed subjects on causal relations that formed a common cause model in the domain of economics, meteorology
or sociology. For example, for economics subjects were told
that “low interest rates causes small trade deficits” (𝑋 → 𝑌! )
and that “low interest rates causes high retirement savings”
(𝑋 → 𝑌! ). The variables senses involved in the causal links
were varied (e.g., high rather than low interest rates sometimes played the role of 𝑋). Subjects were then presented
with a series of inferences in which they were asked to rate
(on a 0-100 scale) the probability of one variable conditioned on others. We found that the empirical analog of
𝐷(𝑌|𝑥 ! ) (the difference between 𝑟𝑎𝑡𝑖𝑛𝑔(𝑦!! |𝑦!! , 𝑥 ! ) and
𝑟𝑎𝑡𝑖𝑛𝑔(𝑦!! |𝑦!! , 𝑥 ! )) was about 18 points (Fig. 1).

1853

2.00%

Y1#
X#
Y2#

The Leaky Gate Model

24%

1.00%

12%

0.00%

0%

!1.00%

!12%

!2.00%

!24%

D(Y)%

D(Y|X=1)%

D(Y)%

D(Y|X=1)%

Figure 2. Normative predictions for a common effect model
and the corresponding empirical ratings from Rehder &
Waldmann (2015). Error bars are 95% confidence intervals.

Fig. 2 presents a common effect network in which 𝑋 is
now a common effect of 𝑌! and 𝑌! . For this network the
independence relations are the reverse of those for a common cause network: Whereas 𝑌! and 𝑌! are unconditionally
independent (𝑌! ⊥ 𝑌! ) they become dependent conditioned
on 𝑋. And, assuming that 𝑌! → 𝑋 and 𝑌! → 𝑋 are generative
and operate independently then 𝑌! and 𝑌! exhibit an explaining away relationship when 𝑋 is present (the presence of 𝑌!
makes 𝑌! less likely and vice versa). These predictions—
𝐷(𝑌) = 0 and 𝐷(𝑌|𝑥 ! ) < 1—are presented in the left chart
of Fig. 2 assuming the same parameterization as in Fig. 1
(i.e., 𝑝(𝑦!! )    =    .50, causal powers of .80, and 𝑝(𝑥 ! |𝑦!! 𝑦!! ) =
.20). And the right chart exhibits how those relations are
typically violated: Rehder & Waldmann found that
𝐷(𝑌) > 0. As an aside, note that, as predicted, 𝐷(𝑌|𝑥 ! ) was
significantly negative, although see Rottman & Hastie
(2014) for evidence that explaining away is usually weaker
than predicted by the normative model. The pattern of independence violations in Figs. 1 and 2 was also observed by
Rehder (2014a) using a forced-choice task.
A number of rationalizations of these independence violations have been offered. For instance, Park & Sloman
(2013) showed that those that arise with a common cause
network are sometimes partly due to subjects’ beliefs that
the two causal links could be disabled by a common factor
(also see Ali, Chater, & Oaksford, 2011; Lagnado & Sloman, 2004; Fernbach & Rehder, 2013; Mayrhofer & Waldmann, 2015; Rehder, 2014b; Walsh & Sloman, 2008). However, this account fails to explain the violations that occur
with the common effect network in Fig. 2 (although it may
explain why those violations were numerically about half
the size of those in Fig. 1). Relatedly, Rehder & Burnett
(2005) explained the large variety of Markov violations they
observed by assuming that all variables were related by an
underlying common cause (an assumption justified on the
basis of the fact that the variables were features of a category; also see Rehder, 2014b). However, this account also fails
to explain the results from Rehder & Waldmann (2015),
which tested materials that were not category features.
To address these descriptive failures of the normative
model, I present two new models of causal reasoning—the
leaky gate model and the dual prototype model. For each I
first show that it accounts for the independence violations
shown in Figs. 1 and 2. Because the models make the same
predictions for those simple networks, I describe two new
causal networks for which their predictions diverge. An
experimental test of those networks is then reported.

The leaky gate model is based on the intuition that information flows along the directed edges of a causal network—
in a manner that is reminiscent of “spreading activation”
accounts of memory—even in situations where that flow is
normatively blocked. It accomplishes this by introducing
additional statistical structure associated with every triple of
variables 𝐴, 𝐵, and 𝐶 in the network that are related such
that 𝐴 − 𝐵 − 𝐶 (where the edges represents a directed edge
in either direction).
𝑌! , 𝑋, and 𝑌! are so related in the networks in Figs. 1 and
2. The joint distribution specified by the leaky gate model
for those networks is derived from the joint specified by the
normative model under parameterization 𝜃, referred to as
𝑝! (𝑋, 𝑌! , 𝑌! |𝜃). The leaky gate model’s joint distribution,
𝑝!" , augments 𝑝! with an additional energy function, 𝜖𝑋𝑌1𝑌2
(Koller & Friedman, 2009),
𝑙𝑜𝑔(𝑝!" (𝑋, 𝑌! , 𝑌! |𝜃, 𝑎)  ) ∝    𝜖!!! !! + 𝑙𝑜𝑔(𝑝! (𝑋, 𝑌! , 𝑌! |𝜃)  )
𝜖!!! !!

𝑎,
=
0,

𝑋 = 𝑌! = 𝑌!
otherwise

(3)
(4)

where 𝑎 is a free parameter ≥ 0. 𝜖!!!!! represents an expectation (whose magnitude is represented by 𝑎) that 𝑋, 𝑌! , and
𝑌! will tend to have the same value, that is, to be all present
or all absent. Eq. 3 yields a proper joint distribution after
exponentiation and normalization.
The introduction of 𝜖!!!!! is sufficient to reproduce the
pattern of independence violations shown in Figs. 1 and 2.
When 𝑎 = .75 and the common cause network is instantiated
with same parameters as in Fig. 1, the leaky gate model predicts 𝐷(𝑌|𝑥 ! ) = .75 rather than 0. For the common effect
network in Fig. 2, it predicts 𝐷(𝑌) = 1.37 rather than 0.
The leaky gate model generalizes to more complex networks. Consider the elaborated common cause network in
Fig. 3 in which the two effects (𝑌! and 𝑌! ) are themselves
the causes of two other variables (𝑍! and 𝑍! ). In this network, there are three triples of connected variables:
𝑌! − 𝑋 − 𝑌! , 𝑋 − 𝑌! − 𝑍! , and 𝑋 − 𝑌! − 𝑍! . Thus, the leaky
gate model’s joint distribution for this network is,
𝑙𝑜𝑔(𝑝!" (𝑋, 𝑌! , 𝑌! , 𝑍! , 𝑍! |𝜃, 𝑎))
∝    𝜖!!! !! + 𝜖!!! !! + 𝜖!!! !!
+ 𝑙𝑜𝑔(𝑝! (𝑋, 𝑌! , 𝑌! , 𝑍! , 𝑍! |𝜃))

(5)

𝜖!!! !! =

𝑎,
0,

𝑋 = 𝑌! = 𝑍!
otherwise

(6)

𝜖!!! !! =

𝑎,
0,

𝑋 = 𝑌! = 𝑍!
otherwise

(7)

and 𝜖𝑋𝑌1𝑌2 is given by Eq. 4.
The predictions for the elaborated common cause network are shown in Fig. 3 for both the normative and leaky
gate model and a number inference types (it also includes
the predictions of the dual prototype model presented in the
next section). The top row of charts presents predictions
regarding the independence of the 𝑌s—𝐷(𝑌) and 𝐷(𝑌|𝑥 ! )
—but also those regarding the independence of the 𝑍s—
𝐷(𝑍) and 𝐷(𝑍|𝑥 ! ). The bottom charts compare 𝐷(𝑍|𝑥 ! )
with a new kind of inference, namely, 𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ) in

1854

Normative Model
2.1

Leaky Gate Model
6.0

Y
Z

1.4

Y1#

Z1#

X#
Z2#

Y
Z

4.0

Empirical Results
36

Y
Z

2.4

Y

2.0

1.2

12

0.0

0.0

0.0

0

-0.7

-2.0

D(•|X=1)

-1.2

D(•)

-12

D(•|X=1)

D(•)

D(•|X=1)

D(•)

1.5

0.18

1.5

24

1.0

0.12

1.0

16

0.5

0.06

0.5

8

0.0

0.00

0.0

0

-0.5

D(Z|X=1)

D(Z|X=Y1=Y2=1)

-0.06

D(Z|X=1)

D(Z|X=Y1=Y2=1)

-0.5

Z

24

0.7

D(•)

Y2#

Dual Prototype Model
3.6

D(Z|X=1)

D(Z|X=Y1=Y2=1)

-8

D(Z|X=1)

D(•|X=1)

D(Z|X=Y1=Y2=1)

Figure 3. Predictions of the normative model, the leaky gate model, and the dual prototype
model for the elaborated common cause network. The “•” in, say, D(•) represents either Y or Z.
Empirical results are shown in the last column. Error bars are 95% confidence intervals.
which 𝑍! and 𝑍! are screened off from one another by three
variables (𝑋, 𝑌! , and 𝑌! ) rather than one.
Using the same parameters as in Fig. 1, the normative
model predicts that 𝐷(𝑌) > 𝐷(𝑍). Because the causal relations are probabilistic, the 𝑌s provide more information
about each other than the 𝑍s. Nevertheless, 𝐷(𝑌|𝑥 ! ) =
𝐷(𝑍|𝑥 ! ) = 0, because knowledge of 𝑋 renders the 𝑍s independent along with the 𝑌s. In contrast, the leaky gate model
(𝑎 = .75) predicts that the 𝑌s and 𝑍s are no longer independent. Nevertheless, the magnitude of the dependence between
the 𝑌s—𝐷(𝑌|𝑥 ! )—is greater than that between the  𝑍s—
𝐷(𝑍|𝑥 ! ) —which in turn is close to 0. This is so because the
probabilistic links between the 𝑌s and 𝑍s further attenuates
the flow of information.
Whereas the normative model predicts 𝐷(𝑍|𝑥 ! ) =
𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ) = 0, the leaky gate model predicts 𝐷(𝑍|𝑥 ! ) >
𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ) ≈ 0. When the flow of information is blocked
by three variables, virtually no information gets transmitted
between the 𝑍s even when the gates are leaky.
Eqs. 5-7 can also be applied to the elaborated common effect model in Fig. 4 in which the causes 𝑌! and 𝑌! are the
effects of 𝑍! and 𝑍! . That figure also shows that whereas the
normative model predicts that 𝐷(𝑌) = 𝐷(𝑍) = 0 (i.e., the
causes are unconditionally independent), the leaky gate
model predicts instead that 𝐷(𝑌) > 𝐷(𝑍) > 0. It also predicts that explaining away will be weaker (i.e., 𝐷(𝑌|𝑥 ! ) and
𝐷(𝑍|𝑥 ! ) will be less negative) than in the normative model.
In summary, the key predictions of the leaky gate model
are that  𝐷(𝑌|𝑥 ! ) > 𝐷(𝑍|𝑥 ! ) ≥ 0 for the elaborated common
cause model, 𝐷(𝑌) > 𝐷(𝑍) > 0 for the elaborated common
effect model, and 𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ) ≈ 0 for both.

a network are either all present or all absent. Like the leaky
gate model, the joint distribution it defines, 𝑝!" , modifies
the one specified by the normative model, 𝑝! . For the simple common cause and common effect networks in Figs. 1
and 2, 𝑝!" is the same 𝑝!" and so reproduces the independence violations in Figs. 1 and 2 in the same manner. Differences between 𝑝!" and 𝑝!" arise for the networks in Figs. 3
and 4, however. In particular, Eq. 10 is analogous to Eq. 7
but elaborated with an alternative energy function,

The Dual Prototype Model

These predictions are tested in an experiment in which
subjects learned either the common cause network in Fig. 3
or the common effect network in Fig. 4 and then drew a
number of causal inferences, including those necessary to

The dual prototype model is based on the intuition that
causal inferences reflect a bias to expect that the variables in

𝑙𝑜𝑔(𝑝!" (𝑋, 𝑌! , 𝑌! , 𝑍! , 𝑍! |𝜃, 𝑎))
∝    𝜖!!! !! !! !!
+ 𝑙𝑜𝑔(𝑝! (𝑋, 𝑌! , 𝑌! , 𝑍! , 𝑍! |𝜃))
𝜖!!! !! !! !! =

𝑎,
0,

𝑋 = 𝑌! = 𝑌! = 𝑍! = 𝑍!
otherwise

(10)

(11)

𝜖!!!!!!!!! represents the expectation that all five variables
will tend to be all present or all absent.
Unlike the leaky gate model, the predictions of the dual
prototype model are less sensitive to the distance between
two variables in a network. For the extended common cause
network (Fig. 3) it predicts a smaller difference between
𝐷(𝑌) and 𝐷(𝑍), that 𝐷(𝑌|𝑥 ! ) ≈ 𝐷(𝑍|𝑥 ! ), and that
𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ) will be greater than 0 (and about equal to
𝐷(𝑍|𝑥 ! )). For the extended common effect network (Fig. 4)
it predicts that  𝐷(𝑌) = 𝐷(𝑍) > 0 and 𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ) > 0. The
prediction that 𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ) > 0 in both networks is especially notable because they imply the presence of independence violations between the 𝑍s even when they are separated by multiple “blockers.”

Overview of Experiment

1855

Normative Model
2.00

Leaky Gate Model
3.00

Y
Z

1.00

Y1#

Z1#

X#
Z2#

Y
Z

1.50

Empirical Results
24

Y
Z

0.75

Y

0.00

0.00

0

-1.00

-1.50

-0.75

-12

-2.00

-3.00

-1.50

D(•|X=1)

D(•)

D(•|X=1)

-24

D(•)

D(•|X=1)

1.00

0.50

1.00

20

0.50

0.25

0.50

10

0.00

0.00

0.00

0

-0.50

-0.25

-0.50

-10

-1.00

D(Z|X=1)

D(Z|X=Y1=Y2=1)

-0.50

D(Z|X=1)

D(Z|X=Y1=Y2=1)

-1.00

Z

12

0.00

D(•)

Y2#

Dual Prototype Model
1.50

D(Z|X=1)

D(Z|X=Y1=Y2=1)

-20

D(•)

D(•|X=1)

D(Z|X=1)

D(Z|X=Y1=Y2=1)

Figure 4. Predictions of the normative model, the leaky gate model, and the dual prototype
model for the elaborated common effect network. The “•” in, say, D(•) represents either Y or Z.
Empirical results are shown in the last column. Error bars are 95% confidence intervals.
compute the key quantities 𝐷(𝑌),  𝐷(𝑍), 𝐷(𝑌|𝑥 ! ), 𝐷(𝑍|𝑥 ! ),
and 𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ). On the basis of previous research I expected that the independence relations stipulated by the
normative model would be violated. The key question is
whether those violations match the pattern predicted by the
leaky gate model or the dual prototype model.

Method
Materials. Three domains were tested: economics, meteorology, and sociology. The five variables in each domain
are shown in Table 1. In the database of materials each variable had the two values shown in Table 1 (“high” or “low”
for interest rates) plus a third “normal” value. However, the
variables were described as binary to subjects (e.g., interest
rates were either high or normal) and the causal relations
were described as obtaining between the non-normal values
(e.g., low interest rates → small trade deficits). To control
for any domain knowledge that subjects might have brought
to the experiment, a between-subject factor controlled which
variable states were described as causally related and took
on the values ++++, ----, -+-+-, and +-+-+, where each +/picks out the value in Table 1 for variables 𝐴, 𝐵, 𝐶, 𝐷, and
𝐸, respectively.
As a further safeguard against subjects’ potential domain
knowledge, a second between-subject factor controlled the
assignment of these variables to the causal roles in Figs. 3
and 4. In the common cause condition, the roles of, 𝑋, 𝑌! ,
𝑌! , 𝑍! , 𝑍! were played by 𝐴, 𝐵, 𝐶, 𝐷, and 𝐸 for half the subjects and by 𝐴, 𝐷, 𝐸, 𝐵, and 𝐶 for the other half. In the
common effect condition, they were played by 𝐷, 𝐵, 𝐶, 𝐴,
and 𝐸 for half the subjects and by 𝐷, 𝐴, 𝐸, 𝐵, and 𝐶 for the
other half. This scheme balances the assignment of the variables to the roles of 𝑌 and Z so that any differences between
inferences involving 𝑌s and those involving 𝑍s cannot be
attributed to the particular variables involved.

The description of each causal relation consisted of two
sentences, one that stated that one variable caused another
and a second that described the mechanism responsible for
that relationship. Table 2 presents an example of one of the
sets of causal relations that formed a common cause network in the domain of economics. Subjects also viewed a
diagram of the causal relations and a third between-subject
variable controlled which of four versions of that diagram
was presented. The four versions of the common cause diagram are presented in Fig. 5 (names of the actual variables
replaced 𝑋, 𝑌! , etc.). These layouts were chosen to ensure
that the spatial distance between the two 𝑌s on the screen
was the same as between the 𝑍s and that the 𝑌s and 𝑍s appeared an equal number of times in each quadrant of the
screen. The four common effect diagrams were the same as
those in Fig. 5 with the arrows reversed.
Table 1
Variable Economics
A
Interest rates
B
C

Meteorology
Ozone levels

Sociology
Urbanization

(low+/high–)

(high+/low–)

(high+/low–)

Trade deficits

Air pressure

Interest in religion

(small+/large–)

(low+/high–)

(low+/high–)

Retirement
savings

Humidity

Socio-economic
mobility

(high+/low–)

(high+/low–)

D
E

(high+/low–)

Job mobility

Wind direction

(high+/low–)

(up+/ down–)

Income taxes

Air temperature

(low+/high–)

(low+/high–)

Interest in sports
Commitment to
the rule of law
(strong+/weak–)

Procedure. Subjects first studied several screens of information about the domain and then performed the inference test. The initial three screens presented a cover story
and a description of the domain’s five variables and their

1856

two values. A fourth screen presented the four causal links
and a fifth the presented the diagram of those links. When
ready, participants took a multiple-choice test of this
knowledge. While taking the test, participants could return
to the information screens they had studied; however, doing
so obligated them to retake the test.
Table 2

Y1#

Z1#

Y1#

Z1#

X#
Z2#

Z2#

Z2#

Y1#
X#

X#
Y2#

Y1#

Z1#

Y2#

Y2#

Z2#

Figure 5

Subjects were then presented with a test that included
those inferences needed to compute 𝐷(𝑌),  𝐷(𝑍), 𝐷(𝑌|𝑥 ! ),
𝐷(𝑍|𝑥 ! ), and 𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ). Each question presented a
particular economy (or society or weather system) whose
variables were shown in a layout like one of those in Fig. 5.
The values of variables whose state was known appeared in
its corresponding box, boxes for variables whose values
were unknown were blank, and the unknown variable whose
value was being requested was filled with “?????.” Subjects
were asked “What’s the probability that [this economy] has
X?” where X was the to-be-inferred variable (e.g., high interest rates). Subjects entered their response by moving a
tick on a rating scale whose ends were labeled “0%” and
“100%”. Each inference type was asked in both directions,
e.g., subjects were asked both 𝑝(𝑦!! |𝑦!! ) and 𝑝(𝑦!! |𝑦!! ). The
order of the 36 questions was randomized for each subject.
Subjects could refer to a printed sheet of the causal diagram
during the entire inference test.
Design and participants. The experiment consisted of a
2 (network type: common cause or common effect) × 3
(domain: economics, meteorology, or sociology)  × 2 (assignment of variables to roles) between-subject design. 96
New York University undergraduates received course credit
for participating and were randomly assigned to these 12
cells subject to the constraint that an equal number appeared
in each cell. The four level factors that controlled which
variable states were described as causally related and which
version of the diagram was presented were both randomly
assigned for each subject.

Initial analyses revealed no effects of the between-subject
counterbalancing variables and so the results are presented
in Figs. 3 and 4 collapsed over those factors. The common
cause and common effect conditions are reported separately.

Z1#

X#

Causal
relation Description
𝑋 → 𝑌! Low interest rates cause small trade deficits. The low cost of
borrowing money leads businesses to invest in the latest manufacturing technologies, and the resulting low-cost products are
exported around the world.
𝑋 → 𝑌! Low interest rates cause high retirement savings. Low interest
rates stimulate economic growth, leading to greater prosperity
overall, and allowing more money to be saved for retirement in
particular.
𝑌! → 𝑍! Small trade deficits cause high job mobility. The intense demand for exports means that entrepreneurs are forming many
new companies, and those new companies must hire workers
away from existing companies.
𝑌! → 𝑍! High retirement savings causes low income taxes. When retirement savings are high, states are likely to lower income taxes to
encourage spending and so stimulate the economy.

Results

Y2#

Common cause results. Consistent with previous research,
the 𝑌s were treated as dependent even when the state of
their common cause 𝑋 was known: Conditioned on 𝑥 ! , one
𝑌 was rated about 15 points more probable when the other 𝑌
was present versus absent. One new finding is that the 𝑍s
were also treated as dependent and that the magnitude of
those effects, 𝐷(𝑌|𝑥 ! ) and 𝐷(𝑍|𝑥 ! ), were about equal. Another is that the magnitude of the dependence between the
𝑍s was the same regardless of whether they were separated
by one or three blockers, 𝐷(𝑍|𝑥 ! ) ≈ 𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ). These
three differences scores were each significantly greater than
zero, ps < .0001, and not significantly different from one
another, ts (47) < 1. This pattern of results was predicted by
the dual prototype model but not the leaky gate model (Fig.
3). One unexpected result is that whereas all three models
predicted that 𝐷(𝑌) > 𝐷(𝑍), the corresponding empirical
judgments (29 and 26, respectively) did not differ from one
another significantly, t < 1. Notably, it was the dual prototype model that predicted the smallest difference between
𝐷(𝑌) and 𝐷(𝑍).
Common effect results. Also consistent with previous
findings, the causes of a common effect network were treated as unconditionally dependent: One 𝑌 was rated about 10
points more probable when the other 𝑌 was present versus
absent, p < .001. Although the degree of dependence between the 𝑍s—𝐷(𝑍)—was a bit smaller (7 points), it was
significantly different than 0, p < .01, and not significantly
different than 𝐷(𝑌), t(47) = 1.27, p = .29. In addition,
𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ) was significantly greater than 0, p < .001.
These results—𝐷(𝑌) ≈ 𝐷(𝑍) and 𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ) > 0—were
predicted by the dual prototype model but not the leaky gate
model (Fig. 4). One unexpected finding is that 𝐷(𝑌|𝑥 ! ) ≈
𝐷(𝑍|𝑥 ! ), t < 1, whereas the dual prototype model predicted
𝐷(𝑍|𝑥 ! ) > 𝐷(𝑌|𝑥 ! ). Notably, although these measures of
explaining away were negative, neither differed significantly from zero, ps > .14.
A model fitting exercise in which the three models were
fit to the subjects’ inferences corroborated the apparent superiority of the dual prototype model. Although space prohibits a full reporting of these results, I found that the common cause inferences were fit best by the dual prototype
model (𝐴𝐼𝐶 = 75.4), followed by the normative (90.0), and
leaky gate models (92.0). Common effect inferences were
also fit best by the dual prototype model (𝐴𝐼𝐶 = 63.4), followed by the leaky gate (70.8) and normative models (77.1).
The predictions of the fitted dual prototype model deviated
from the empirical ratings by an average of 4.1 and 3.1
points (on a 0-100 scale) in the common cause and common
effect conditions, respectively, and the correlations with
those ratings were .96 and .98.

Discussion
Empirically, this article has both replicated and extended

1857

previous findings. Once again, the two effects of a common
cause network were not treated as independent conditioned
on their cause. The new finding is that the same is true of
the effects of those effects: The 𝑍s were treated as dependent just like the 𝑌s. The two causes of a common effect
network (the 𝑌s) were again not treated as unconditionally
independent and, in a new result, so too were the causes of
those causes (the 𝑍s). Remarkably, in both networks the 𝑍s
were treated as dependent even when separated by multiple
“blockers” (𝐷(𝑍|𝑥 ! 𝑦!! 𝑦!! ) > 0).
Theoretically, this article has advanced our understanding
of such effects via the testing of two formal models. One
might have expected that independence violations arise because information flows along a network in cases where it
shouldn’t. Yet this intuition—formalized as the leaky gate
model—predicts that the magnitude of independence violations should decrease as the number of blockers between
two variables increases. The fact that the common cause
subjects treated the 𝑍s as dependent to the same degree regardless of whether they were separated by three blockers or
one provides striking evidence against “spreading activation” sorts of accounts.
Instead, reasoners behaved as if they overestimated the
likelihood of two prototypical networks states, one in which
all variables were present and another in which they were all
absent, a view which explains the absence of an effect of
either distance (𝑌s vs. 𝑍s) or the number of blockers on the
magnitude of independence violations. Because this tendency works in the opposite direction of explaining away in a
common effect network, the dual prototype model also accounts for the lack of significant explaining away in this and
past work (Rehder & Waldmann, 2015; Rehder, 2014a;
Rottman & Hastie, 2013).
It is important to note that the dual prototype model does
not imply that reasoners ignore the direction of causality in
their causal inferences, because that model’s joint distribution modifies rather than replaces the joint derived from the
normative model. Common cause and common effect networks are theoretically important because they are identical
ignoring the direction of causality. Yet in every experimental test of which I am aware (including this one) differences between those networks obtain (compare Figs. 3 and
4). The conclusion to be drawn from this work is not that
people aren’t sophisticated causal reasoners but rather that
their understanding of the statistics implied by causal networks differs from that of network theorists.
One might be tempted to interpret the dual prototype
model’s success as reflecting subjects’ beliefs that the causal links operated deterministically (and that the effect variables had no alternative causes), a situation that would normatively predict that variables were either all present or all
absent (at least for the common cause network). Yet when
these subjects were asked to infer 𝑋 given 0, 1, or 2 𝑌s (i.e.,
𝑝(𝑥 ! |𝑦!! , 𝑦!! ), 𝑝(𝑥 ! |𝑦!! , 𝑦!! ), or 𝑝(𝑥 ! |𝑦!! , 𝑦!! )) their ratings
(27, 57, and 82 in the common cause condition; 29, 64, and
97 in the common effect condition) reflected a belief that
the causal links were probabilistic rather than deterministic
(and the fitted causal power parameter was < .86 in both
conditions). Nevertheless, an experimental test of the dual

prototype model in which causal links are described as
probabilistic is called for.
Cognitive theories based on causal graphical models have
enjoyed great success accounting for causal-based judgments. Yet, the fact that people fail to honor the independence relations that such graphs express means that the theories rest on shaky foundations. It is time to move them onto
on a firmer foundation, one that embodies fundamental empirical facts regarding how people draw causal inferences.
Let the formalisms described herein represent modest proposals in that direction.

References
Ali, N., Chater, N., & Oaksford, M. (2011). The mental representation of causal conditional reasoning: Mental models or causal models. Cognition, 119, 403-418.
Cheng, P. (1997). From covariation to causation: A causal
power theory. Psychological Review, 104, 367-405.
Fernbach, P. & Rehder, B. (2012). Toward an effort reduction framework for causal inference. Argument and Computation, 4, 1-25
Koller, D. & Friedman, L. (2009). Probabilistic Graphical
Models: Principles and Techniques. Cambridge, MA:
MIT Press.
Lagnado, D.A. & Sloman S.A. (2004). The advantage of
timely intervention. Journal of Experimental Psychology:
Learning, Memory, & Cognition, 30:856–76
Mayrhofer, R. & Waldmann, M. R. (2015). Agents and
causes: Dispositional intuitions as a guide to causal structure. Cognitive Science, 39, 65-95.
Park, J. & Sloman, S. A. (2013). Mechanistic beliefs determine adherence to the Markov property in causal reasoning. Cognitive Psychology, 67, 186-216.
Pearl, J. (2000). Causality: models, reasoning, and inference. Cambridge, UK: Cambridge University Press.
Perales, J., Catena, A., & Maldonado, A. (2004). Inferring
non-observed correlations from causal scenarios: The role
of causal knowledge. Learning and Motivation, 35, 115–
135.
Rehder, B. (2014a). Independence and dependence in human
causal reasoning. Cognitive Psychology, 72, 54-107.
Rehder, B. (2014b). The role of functional form in causalbased categorization. Journal of Experimental Psychology:
Learning, Memory, and Cognition.
Rehder, B., & Burnett, R. C. (2005). Feature inference and
the causal structure of object categories. Cognitive Psychology, 50, 264-314.
Rehder, B., & Waldmann, M. (2015). Failures of Explaining
Away and Screening Off in Described versus Experienced
Causal Learning Scenarios. Submitted for publication.
Rottman, B., & Hastie, R. (2014). Reasoning about causal relationships: Inferences on causal networks. Psychological Bulletin, 140, 109-139.
Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, prediction, and search. New York: Springer-Verlag.
Walsh, C. R., & Sloman, S. A. (2008). Updating beliefs with
causal models: Violations of screening off. In G. H. Bower,
et al. (Eds.), Memory and Mind: A Festschrift for Gordon
Bower (pp. 345-358).

1858

