Facilitating Spatial Task Learning in Interactive Multimedia Environments
While Accounting for Individual Differences and Task Difficulty
Dar-Wei Chen (darwei.chen@gatech.edu)
School of Psychology, Georgia Institute of Technology,
654 Cherry Street, Atlanta, GA 30332-0170, USA

Richard Catrambone (richard.catrambone@psych.gatech.edu)
School of Psychology, Georgia Institute of Technology,
654 Cherry Street, Atlanta, GA 30332-0170, USA

Abstract
Two experiments examined the effects of interactive tutorial
features (compared to “passive” features) on learning spatial
tasks, an area seldom explored in interactivity research.
Experiment 1 results indicated that for simple spatial tasks,
interactive tutorials hindered learning for participants of
higher spatial ability but improved learning for lower-ability
participants. This interaction can be explained by
“compensation,” the notion that people of higher ability can
compensate for poor external support (passive tutorials) while
people of lower ability need the better support. It is likely that
the increased cognitive load of interactivity (Kalyuga, 2007)
hindered high-spatial participants on a relatively easy task. In
Experiment 2, task difficulty was increased, and the results
revealed that the interactive tutorial produced better learning
than the passive tutorial, regardless of spatial abilities. With
the relatively difficult task, the benefits of interactivity
became clearer because most people actually needed the
interactive features despite the associated cognitive load.
Keywords: user interactivity; learning technology; spatial
learning; multimedia; individual differences

Background
Education has changed markedly since “paper and pencil”
was the dominant form of learning. An internet connection
is now the only requirement for learners to reach previouslyinaccessible worlds of information. Therefore, not
surprisingly, technology-driven learning is on the rise. For
example, a survey by the Sloan Consortium showed that
almost three-quarters of universities report increasing
demand for online courses (Parry, 2010).
Improving the quality of technology-driven learning is
in the interest of educators and the public at large. One of
the more intriguing features of educational technology is
multimedia and any discussion about improving educational
technology would be incomplete without studying how
features of multimedia (materials that incorporate elements
such as text, images, animation, video, interactivity, etc.)
can be harnessed for positive learning outcomes.
One of the current discussions in technology-driven
learning concerns the impact of user interactivity on
learning outcomes. Interactivity can be defined as
“reciprocal activity between a learner and a multimedia
learning system, in which the [re]action of the learner is
dependent upon the [re]-action of the system and vice versa”
(Domagk, Schwartz, & Plass, 2010, p. 1025). Three aspects

of interactivity proposed by Moreno and Mayer (2007) are:
pacing (controlling speed of information presentation),
manipulating (controlling aspects of information
presentation), and navigating (selecting information
sources); these aspects were investigated in this study.
Some research suggests that students learn best when
material is interactive; that is, when the user has a relatively
high degree of control over the material and the actions of
the user and material are closely related to the actions of the
other. For example, Schwan and Riempp (2004) found that
people using interactive learning tools could accurately
complete knot-tying tasks with half of the practice time as
those using non-interactive tools; they posited that
interactivity allowed participants to tailor their information
gathering by helping them more easily distribute study time
to difficult tasks. A study by Khalifa and Lam (2002)
showed that the understanding exhibited by users in
“interactive conditions” was indicative of knowledge about
how concepts link together; the users in “passive
conditions” understood the material at merely a “list-like”
level. In research about learner control, students have been
found to better optimize their experiences and generalize
findings when in control (Gureckis & Markant, 2012).
In some ways, interactive materials could confer
educational benefits similar to those of “minimal guidance”
interventions, which generally hypothesize that people learn
more effectively when required to discover information on
their own, as opposed to being directly instructed.
Interactive
interventions
and
minimal-guidance
interventions allow learners to take control of the learning
process, leading them to assess their knowledge gaps and
methods to overcome them, both of which are productive
activities (Chi, 2000). For example, with some interactive
materials, a learner might have to choose a method of
gathering information that will yield the information he or
she needs; when directly instructed, learners tend not to
have to assess their own learning as much.
However, not all research demonstrates consistent
benefits for usage of interactive materials. Sometimes,
interactivity becomes a mental burden on learners by
introducing “non-essential extraneous processing load”
(Kalyuga, 2007). For example, Moreno and Mayer (2005)
found that interactivity helps people achieve “meaningful
learning” only when sufficient guidance exists within the

1925

interface. Therefore, interactivity seems not to be an
inherently positive feature in learning materials; instead, the
amount and aspects of interactivity are important keys in
creating useful multimedia instruction.
The present study provides a nudge in clarifying some
aspects of multimedia interactivity with respect to their
impact on learning of spatial tasks. The chosen task for this
study was partially solving Rubik’s Cube, a spatial task that
requires pattern recognition and inference-making. This task
is of interest because it is unlike tasks that are driven by
declarative knowledge; much of the existing interactive
learning research already focuses on declarative knowledge.
One of the goals in this study was to determine whether
some aspects of multimedia interactivity could be used to
foster understanding of tasks that are driven by spatial
pattern recognition and inference-making, as opposed to
primarily declarative knowledge (e.g. memorizing the
capitals of all fifty states) or procedures (e.g. operating a
vacuum cleaner). Another goal of this study is to more
realistically represent passive conditions by allowing users
the option of exercising basic control over videos as they
would have in most online learning interfaces. Many studies
in the past have created passive conditions in which users
did not have the option of controlling any aspect of videos,
situations that do not often arise with the modern internet.

Materials Two types of tutorials (shown in Figure 2):
 The interactive tutorial presented a series of Java applets
featuring an on-screen cube whose faces would turn
depending on user input. Each major step (and the moves
within each step) were presented in succession. Learners
could manipulate the learning pace through the use of
the “step-by-step” buttons, navigate cleanly between
move sequence animations through the use of the
indexed steps, and manually rotate the virtual cube.
 The “passive” tutorial included a series of videos that are
comparable in user control to videos commonly found on
websites. That is, the videos are played straight through
by default unless rewound, fast-forwarded, or paused.
All information available in the interactive tutorial was
available through the videos, although the videos
afforded less control in its presentation of information.

Experiment 1

Figure 2: Passive tutorial (L) and interactive tutorial (R)

Participants used either an interactive or a “passive” videobased tutorial to learn how to create “the cross” on Rubik’s
Cube. Creating the cross involves two types of pieces:
center pieces (in the middle of each cube face) and edge
pieces (pieces with colors on two sides, as opposed to corner
pieces that have colors on three sides). Participants were to
A) place edge pieces around the yellow center such that a
yellow “plus sign” was formed, and B) align the secondary
(non-yellow) colors of those edge pieces with the colors of
the adjacent centers. Figure 1 visually explains the cross.

Figure 1: “The cross,” shown from two angles
(Y = yellow, G = green, R = red, B = blue, O = orange)

Method
Participants Participants were 31 college students (18-22
years in age; 15 interactive, 16 passive) who had no selfreported prior experience in systematically learning the
Rubik’s Cube (e.g. looking up instructions online, learning
from a friend). The students received course credit for their
participation and were randomly assigned to conditions.

Design and Procedure The effects of tutorial type on
participants’ learning were examined with a betweensubject design. Before starting the tutorial, each participant
completed a demographics form and a spatial ability test
(on-screen paper is folded, has a hole punched through it,
and participants must predict what the paper looks like
when unfolded again; Ekstrom et al., 1976).
The study started with participants being allotted eight
minutes to read a Rubik’s Cube introduction while having a
cube available to use. After the reading period, participants
were given 20 minutes to access their assigned tutorial.
After the tutorial phase, participants moved onto the
assessment, in which they were given scrambled cubes (i.e.,
none of the edge pieces were already placed into the cross)
and instructed to construct the cross within a four-minute
period (cubes were physical, not virtual); inference-making
was tested as the scrambled cube configuration differed
from the cubes originally given to participants. Participants’
times were recorded if they were able to create the cross.
Assessment Scoring Scheme Performances on the
assessment were scored using a two-tiered scheme. This
scheme was developed by one of the researchers and
reviewed for face and external validity by 2007 Florida
Open Rubik’s Cube champion Andrew Chow (A. Chow,
personal communication, November 27, 2012). Participants
could score a maximum of six points in Tier 1. It
encompasses the first step of creating the cross without
regard for matching center colors. Points are awarded for
each individual yellow edge piece placed around the yellow

1926

center; the third and fourth edge pieces are weighted more
heavily because placing those sometimes involves moving
other edge pieces out of place, thus requiring more
awareness. Tier 2 has a maximum of four points, and it
involves matching the secondary colors of the edge pieces
to the colors of their adjacent centers. The possible scores
for a cube in Experiment 1 are shown in Table 1.

the option of allowing whole sequences of moves to play
consecutively without stopping, using the interactive applets
almost like videos. For a relatively easy task, such an
approach might have been sufficient.
However, even if participants did use the interactive
tutorial differently from the passive one, those differences
might not have revealed themselves in the scores. For
example, one of the main distinguishing features of the
interactive tutorial was the presence of “step-by-step
buttons” that allowed participants to scroll through
individual moves with self-selected pacing and relative ease.
However, as access to internet videos becomes increasingly
commonplace, especially among college students, perhaps
the passive condition’s tasks of rewinding or fastforwarding to search through a video is no longer much of a
mental burden relative to searching through the use of the
buttons. In fact, the interactive condition could even have
introduced its own larger mental burdens with the emphasis
it placed on user control of the tutorial (Kalyuga, 2007),
negating any potential benefits of the interactivity.

Table 1: Scoring possibilities for Experiment 1 (Tier 1
points must be complete before Tier 2 points can be earned).

Tier

Tier 1

Tier 2

Cross pieces
in place
1
2
3
4
4
4
4

Number of
matching
centers
0
0
0
0
1
2
4

Score
1
2
4
6
6
8
10

Results and Discussion
Main Effect of Tutorial Interactivity No significant
differences in performance were found between the
interactive condition participants (M = 8.33, SD = 2.29) and
passive condition participants (M = 8.81, SD = 2.23), F (1,
29) = 0.349, MSE = 5.10, p > 0.05, d = 0.21 (non-parametric
Mann-Whitney U test similarly non-significant, p = 0.572).
The difference in solve time between those in the interactive
condition (M = 93.29s, SD = 55.71) and those in the passive
(M = 84.5s, SD = 41.04) was also found to be nonsignificant, F (1, 11) = 0.157, MSE = 2,458.08, p > 0.05.
A few explanations are possible for the relative
similarity in performances between the two conditions. One
revolves around a limitation of the study: The given
assessment task might have been completed too easily by
the participants, leading to ceiling effects in the
achievement scores – more than two-thirds of the
participants (21 out of 31) achieved a perfect score. With a
majority of scores bunched at the high end of the scale,
leaving little room for performance differences to be
expressed in the data, any actual effects of the tutorial types
would have been difficult to find. Experiment 2 addressed
this issue by requiring participants to complete a more
difficult cube-related task within the same time constraints.
In terms of the study manipulation, the possibility
exists that for this relatively easy task, the differences
between the tutorials were not large enough to elicit
significantly different user actions (future studies should
implement manipulation checks regarding how learners
actually used the respective tutorials); that is, the way the
participants used the interactive tutorial might not have been
significantly different from how participants used the
passive tutorial. For example, interactive participants had

Individual Differences in Spatial Ability As might be
expected, a participant’s spatial ability had a significant
positive correlation with his or her achievement score (r =
0.415, p = 0.05); spatial ability also had a significant
negative correlation with solve time (r = -0.495, p = 0.016).
In short, these results indicate that participants of high
spatial ability generally performed better on the assessment
task and finished the task more quickly than those of low
spatial ability. In this study, “high-spatial” participants were
those with spatial abilities above the median and “lowspatial” participants were those lower than the median.
Furthermore, using a linear regression, spatial ability was
found to uniquely account for a significant amount of
achievement score variance that condition could not account
for itself (r-change = 0.154, p = 0.033). To explain findings
such as this one, Mayer and Sims (1994) posit that people
with high spatial ability are able to achieve – while using
fewer cognitive resources – the same understanding of the
multimedia instructions as people with low spatial ability;
therefore, they can transfer more of their resources to the
actual task at hand (the cube, in this case).
Interaction Between Condition and Spatial Ability. As
stated previously, the difference in mean achievement
scores between the two tutorial types was not statistically
significant. However, an ANOVA did demonstrate a
significant interaction between tutorial type and spatial
ability, F (1, 26) = 4.27, MSE = 4.12, p = 0.049. More
specifically, the interaction suggested that low-spatial
participants benefited more from interactivity than highspatial participants did; that is, high-spatial participants
scored about 46% higher than low-spatial participants when
using the passive tutorial, while the differences between the
participants were negligible when they used the interactive
tutorial (larger sample sizes are needed for post-hoc tests to
be conducted, however). Figure 3 illustrates this finding.

1927

Figure 4: The "first layer" shown from three angles.

Method
Participants Participants were 47 college students (18-22
years in age; 23 interactive, 24 passive) who had no selfreported prior experience in systematically learning the
Rubik’s Cube. The students received course credit for their
participation and were randomly assigned to conditions.
Materials. All of the content related to the cross was
identical to that of Experiment 1. The tutorials were
extended to include the additional concepts that participants
had to learn in order to create the first layer.

Figure 3: Interaction between condition and spatial ability
An explanation for this result is “compensation,” the
notion that people of high spatial ability can compensate for
ostensibly weaker external support (as experienced in the
passive condition) while people of low spatial ability benefit
from stronger external support (interactive condition). In
this particular study, high-spatial participants were likely
able to better mentally visualize and compensate for the
information that the passive condition did not present as
well, while low-spatial participants were significantly aided
by interactivity because they had lower capacities to
compensate and fill the information gaps themselves. In
other words, interactivity was unnecessary for high-spatial
participants, but made a positive difference for those on the
lower end of the spectrum. This conclusion aligns with a
finding from Hoffler and Leutner (2010) that high-spatial
people generally learned well from either multiphase
diagrams (low support) or animations (high support),
whereas low-spatial people needed the animations to
perform relatively better than they did with the diagrams.
Experiment 2 examined whether the effects of
interactivity and spatial ability would change on a more
difficult task (which also served to counter aforementioned
ceiling effects). It was hypothesized that, in a task that could
not be easily completed without good support, the benefits
of interactivity would become more evident because the
benefits would outweigh the associated cognitive load.

Design and Procedure. Participants experienced the same
procedures as the participants in Experiment 1. That is, even
though Experiment 2 required participants to learn more
material, they were still allowed just 20 minutes during the
tutorial phase and 4 minutes for the assessment. The time
limit was held constant between experiments to increase the
likelihood that the task for Experiment 2 was indeed more
difficult than the task for Experiment 1.
Assessment Scoring Scheme. The 10-point scheme for the
cross was kept in place, but 10 more points were added to
account for the additional points possible for adding corner
pieces; therefore, 20 points was the maximum possible
score for creating the first layer. Table 2 outlines the 10
additional points for the corner pieces.
Table 2: Scoring possibilities for corner pieces in
Experiment 2 (to be used if cross is completed).
Cross pieces
in place
4
4
4
4
4

Number of
matching
centers
4
4
4
4
4

Corner pieces
in place
0
1
2
3
4

Score

10
13
16
18
20

Experiment 2
Procedures for Experiment 2 were identical to those of
Experiment 1 with the exception of the task assigned to the
participants. The participants used either an interactive
tutorial or a passive tutorial to learn how to create “the first
layer,” which is one step further than the cross. Creating the
first layer involves creating the cross and then placing the
appropriate corner pieces around it such that the yellow side
is complete and the colors of the corner pieces match the
adjacent edge pieces. Figure 4 illustrates the first layer.

This scheme was reviewed by 2007 Florida Open
Rubik’s Cube champion Andrew Chow (A. Chow, personal
communication, May 11, 2015).

Results and Discussion
Main Effect of Tutorial Interactivity Participants using
the interactive tutorial (M = 11.04, SD = 6.06) significantly

1928

outperformed those using the passive tutorial (M = 7.46, SD
= 5.27), t (45) = 2.17, p = 0.04, d = 0.63 (see Figure 5).
Solve times were not compared due to the low number of
participants who were able to actually complete the task.

Figure 5: Assessment scores by condition and spatial ability
Participants using the interactive tutorial were
originally hypothesized to score more highly on the
assessment, and the results in Experiment 2 support that
hypothesis. The interactive condition participants likely
performed better because their tutorials encouraged user
control of the learning process (Zhang et al., 2006). For
example, as the interactive interface stopped after each
individual move of a sequence, participants might have been
relatively likely to self-question and reflect on their
understanding of the move. If they did not understand, the
“go back one move” button easily allowed participants to
see the move again. The videos in the passive condition did
not stop at the conclusion of individual moves unless the
participant stopped it, and precise rewinding to view a move
again was not as easy as in the interactive condition.
Another feature that encouraged reflection was the
cube rotation mechanism, which allowed interactive
condition participants to manually rotate the on-screen
Rubik’s Cube at any time to view the positions of any
pieces that were of interest. This feature helped participants
to become more aware of their knowledge gaps because
they had to deliberately find the information to fill those
gaps when they were stuck. In the passive condition, the
participants potentially received all of the same information
because their on-screen cubes were rotated automatically,
but the participants were perhaps less likely to know which
knowledge gap was being filled by the presented
information given their reduced control over the tutorial.
Both of these interactive features must be more deeply
examined in the future for a better understanding of how
learners used them for reflection. Of course, the interactive
features existed in the interactive condition tutorials for

Experiment 1 as well, but they produced different
performance effects because of the lower task difficulty in
Experiment 1. Interactivity proved to be useful for both
high- and low-spatial participants in Experiment 2 because
the task was difficult enough to require external support;
interactivity was not a superfluous feature for the highspatial participants like in Experiment 1. Interactivity was
not necessarily useful to high-spatial participants in
Experiment 1 because the mental burdens of interactivity
(Kalyuga, 2007) outweighed the benefits on a task they
likely could have done well on without much support; lowspatial participants needed the support (compensation).
The reflection presumably encouraged in the
interactive condition might have aided processes of
metacognition. In the passive condition, participants might
have known that a move was to be done, but be relatively
unsure about the reasoning because they were less likely to
be confronted with their gaps in knowledge at the right time.
Participants in the interactive condition were more likely to
learn why a move was done because the interface
encouraged them to think about it through A) the step-bystep emphasis in the interface, and B) the user control of
turning the cube for needed information. This deeper-level
knowledge was likely the reason that interactive participants
performed better on the assessment; the assessment required
transfer and fundamental knowledge because participants
were given a newly-scrambled cube for the assessment that
differed in initial state from the one they used in the tutorial,
diminishing the effectiveness of rote memorization.
Metacognitive processes demand mental resources,
and sometimes can hinder performance (Kalyuga, 2007).
However, the benefits of metacognition apparently
outweighed the drawbacks in Experiment 2, corroborating
the findings of other studies about the effectiveness of
metacognition (e.g., van den Boom, Paas, van Merrienboer,
& van Gog, 2004; Kramarski & Michalsky, 2010).
Individual Differences in Spatial Ability Spatial ability
was correlated significantly with assessment score (r = 0.45,
p < 0.01), in line with results from Experiment 1. However,
unlike Experiment 1, no statistically-significant interaction
was found between tutorial type and spatial ability, F (1, 43)
= 0.01, MSE = 28.00, p = 0.92. Therefore, it can be
concluded that the effects of tutorial type were relatively
similar across participants of varying spatial abilities.
Video Games and Spatial Ability. A significant correlation
was found between time spent playing video games and
assessment score (r = 0.44, p < 0.01). The data from the
present study do not indicate the nature of the causality,
although the correlation between video game hours and
spatial ability was also significant (r = 0.37, p < 0.05).

General Discussion
Perhaps the most intriguing finding from these two studies
is the difference in data patterns between Experiment 1 and
Experiment 2. With the relatively easy task in Experiment 1,

1929

the results were consistent with the theory of compensation:
high-spatial participants appeared to perform worse with the
interactive features while low-spatial participants performed
better with them (interaction effect), F (1, 26) = 4.27, MSE
= 4.12, p = 0.049. When the task increased in difficulty for
Experiment 2, the effectiveness of the interactive features
was no longer dependent on a participant’s spatial ability.
Instead, there was a main effect of tutorial type, with both
high-spatial participants and low-spatial participants
receiving an equal boost from the interactive features.
Interactivity is a broad concept not limited to the
aspects discussed here: pace, information manipulation, and
navigation. Future studies implementing different aspects of
interactivity could yield somewhat different results, and
more granular data regarding tutorial usage (e.g., how
aspects of interactivity affect user actions, cognitive load
data) could help researchers identify and describe more
exactly the low-level mechanisms driving the effects found
here. Another concern is that Rubik’s Cube is not
necessarily representative of the many types of tasks, or
even spatial tasks, that exist. Future research should identify
the aspects in interactivity that improve learning, the people
who benefit most from using those aspects of interactivity,
and situations in which interactivity is most appropriate.
In 2011, almost one-third of US college students had
taken at least one online course (Online Learning
Consortium, 2012), and that percentage is growing.
Countries like Great Britain (invested $100 million in 2011
to boost online learning) and Australia (20% growth
between 2007 and 2012) are also seeing online education as
a not just a reasonable alternative to “traditional” schools,
but a necessity in modern learning (International College of
Economics and Finance, 2012). Learning technologies offer
many conveniences over standard materials such as
portability and information access. They also provide
opportunities for user interaction that traditional textbooks
cannot match. However, as demonstrated in this paper,
interactivity is not “one size fits all.” Accounting for
individual differences and task-specific details can help
educators harness the powers of interactivity to improve
learning technologies and outcomes for all users.

References
Chi, M. T. H. (2000). Self-explaining: the dual processes of
generating inferences and repairing mental models. In R.
Glaser (Ed.), Advances in instructional psychology, (pp.
161–238). Mahwah, NJ: Lawrence Erlbaum Associates.
Domagk, S., Schwartz, R. N., & Plass, J. L. (2010).
Interactivity in multimedia learning: An integrated model.
Computers in Human Behavior, 26 (5), 1024-1033.
Ekstrom, R. B., French, J. W., Harman, H. H., & Dermen,
D. (1976). Kit of Factor-Referenced Cognitive Tests.
Princeton, NJ: Educational Testing Service.
Gureckis, T.M. & Markant, D.B. (2012). Self-Directed
Learning: A Cognitive and Computational Perspective.
Perspectives on Psychological Science, 7 (5), 464-481.

Hoffler, T. N. & Leutner, D. (2010). The role of spatial
ability in learning from instructional animations –
Evidence for an ability-as-compensator hypothesis.
Computers in Human Behavior, 27 (1), 209-216.
International College of Economics and Finance (2012). 8
countries leading the way in online education. ICEF
Monitor. Retrieved from: http://monitor.icef.com/2012/
06/8-countries-leading-the-way-in-online-education/
Kalyuga, S. (2007). Enhancing instructional efficiency of
interactive e-learning environments: A cognitive load
perspective. Educ. Psychol. Rev., 19, 387-399.
Khalifa, M. & Lam, R. (2002). Web-Based Learning:
Effects on Learning Process and Outcome. IEEE
Transactions on Education, 45 (4), 350-356.
Kline, K. & Catrambone, R. (2011). Learning from
Multiphase Diagrams: Effects of Spatial Ability and
Visuospatial Working Memory Capacity. In Proceedings
of the HFES 55th Annual Meeting, 570-574.
Kramarski, B. & Michalsky, T. (2010). Preparing preservice
teachers for self-regulated learning in the context of
technological pedagogical content knowledge. Learning
and Instruction, 20 (5), 434-447.
Mayer, R. E. & Sims, V. K. (1994). For whom is a picture
worth a thousand words? Extensions of a dual-coding
theory of multimedia learning. Journal of Educational
Psychology, 86 (3), 389-401.
Moreno, R. & Mayer, R. E. (2005). Role of guidance,
reflection, and interactivity in an agent-based multimedia
game. Journ. of Educational Psychology, 97 (1), 117-128.
Moreno, R. & Mayer, R.E. (2007). Interactive multimodal
learning environments. Educ. Psychol. Rev., 19, 309-326.
Online Learning Consortium. (2012). Changing Course: Ten
Years of Tracking Online Education in the United States.
Online Learning Consortium. Retrieved from:
http://onlinelearningconsortium.org/survey_report/changi
ng-course-ten-years-tracking-online-education-unitedstates/
Parry, M. (2010, January 26). Colleges See 17 Percent
Increase in Online Enrollment. The Chronicle of Higher
Education. Retrieved from: http://chronicle.com/blogs/
wiredcampus/colleges-see-17-percent-increase-in-onlineenrollment/20820
Schwan, S. & Riempp, R. (2004). The cognitive benefits of
interactive videos: Learning to tie nautical knots.
Learning and Instruction, 14, 293-305.
Van den Boom, G., Paas, F., van Merrienboer, J. J. G., &
van Gog, T. (2004). Reflection prompts and tutor
feedback in a web-based learning environment: effects on
students’ self-regulated learning competence. Computers
in Human Behavior, 20 (4), 551-567.
Zhang, D., Zhou, L., Briggs, R.O., Nunamaker, J.F. (2006).
Instructional video in e-learning: Assessing the impact of
interactive video on learning effectiveness. Information
and Management, 43, 15-27.

1930

