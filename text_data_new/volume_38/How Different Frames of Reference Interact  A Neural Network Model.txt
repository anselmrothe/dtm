How Different Frames of Reference Interact: A Neural Network Model
Weizhi Nan (nanwz@psych.ac.cn)ͣ,
Yanlong Sun (ysun@tamhsc.edu)ᵇ,
Xun Liu (liux@psych.ac.cn)ᵃ,
Hongbin Wang (hwang@tamhsc.edu)ᵇ
ᵃ Key Laboratory of Behavioral Science, Institute of Psychology,
Chinese Academy of Sciences, 16 Lincui Road, Chaoyang District, Beijing 100101, China
ᵇ Center for Biomedical Informatics, Texas A&M University Health Science Center,
2121 West Holcombe Blvd, Houston, TX 77030, USA
Abstract
It has been argued that people use multiple frames of
reference (FORs) for representing and updating spatial
relationships between objects in a complex environment.
When there are conflicts among representations of multiple
FORs, they compete to determine behavior. “Frame of
Reference-based Map of Salience” theory (FORMS) suggests
that FORs with high salience may be processed in priority.
Here, we report a computational neural network model for a
two-cannon task, which naturally involves multiple FORs
with different levels of salience: intrinsic frame of reference
(IFOR) and egocentric frame of reference (EFOR). The goal
is to investigate the computational neural mechanisms
underlying human spatial performance. Our simulation results
fit earlier behavioral results well. The model suggests
although multiple FORs may be initially represented
independently, they interfere with each other by the inhibitory
competition of neurons in the later process (in hidden layer)
for conflict resolution. Moreover, salience may modulate the
competition by prioritizing FORs with high salience levels.
These results represent a connectionist support for the
FORMS theory.
Keywords: frame of reference; inhibitory competition;
salience; neural network model

Introduction
People adopt multiple frames of reference (FORs) to
represent the spatial relationship of objects in a complex
environment (Klatzky, 1998; Mou & McNamara, 2002;
Piaget & Inhelder, 1956; Sun & Wang, 2014; Tamborello,
Sun, & Wang, 2012; Wang, Johnson, & Zhang, 2001; Zacks
& Michelon, 2005). Based on the relationship with the
observer, FORs can be classified into three types, egocentric
FOR (EFOR), intrinsic FOR (IFOR) and allocentric FOR
(AFOR) (Klatzky, 1998; Mou & McNamara, 2002;
Tamborello et al., 2012; Wang & Spelke, 2002). An EFORbased representation is anchored to the observer, which
needs to be updated following the movement of the
observer’s eye, head, body coordinates (Wang et al., 2001).
In an IFOR-based representation, an object or an object
group in the viewing environment but exogenous to the
observer is used as the reference point. For example, a table
is used as an IFOR anchor in the description “the cup is on
the table”. IFORs remain stable with the observer’s
movement but have to be updated when the reference object
moves. In an AFOR-based representation, the entire
environment, such as a room or a city, is taken as the

reference point. For a comprehensive review, see (Mou,
Fan, McNamara, & Owen, 2008; Mou & McNamara, 2002;
Sun & Wang, 2010, 2014; Tamborello et al., 2012;
Yamamoto & Philbeck, 2013).
Particularly of our interests is how multiple FOR
representations develop and interact with each other.
Mathematically, all FORs are equivalent. Depending on
specific situations, however, some FORs are more useful or
convenient. In comparison, EFOR is probably more
automatic and almost effortless, AFOR is quite stable but
computationally demanding, and IFOR is a balance between
flexibility and stability. “Frame of Reference-based Map of
Salience” theory (FORMS) states the human brain
represents spatial information simultaneously using multiple
FORs, each being a spatial map of salience (i.e. only salient
objects or locations are represented on each map), and that
human performance is determined by the interaction of all
relevant FOR-based representations (Sun & Wang, 2010,
2014; Tamborello et al., 2012; Wang et al., 2001; Wang,
Sun, Johnson, & Yuan, 2005; Yamamoto & Philbeck,
2013).
To understand the interaction among multiple FORs and
the effect of salience, Tamborello et al. (2012) designed a
two-cannon task (a modified and simplified version is
shown in Figure 1), in which two cannons (a red and a blue)
were surrounded by 8 pellets in red or blue color. The
salience of the cannons was determined by the pellet color
ratio such that the cannon with more same-color pellets was
more salient. In the task, when one of the pellets was
randomly chosen as the target pellet (flashing), the cannon
in the same color became the target cannon. Participants
were asked to rotate the target cannon to the location of the
flashing target pellet in the shortest way by pressing either
the left-arrow or the right-arrow button on the keyboard. An
analysis (Figure 3A and 3B) showed that reaction time (RT)
became longer when there was a conflict among different
FOR-based representations. RT in cannon angle 180°
condition was longer than RT in cannon angle 0° condition
(Cannon Angle Effect). In the cannon angle 180° condition,
RT for trials where the target pellet appeared in the minority
color group was longer than RT for those in the majority
color group (Salience Effect). RT in target cannon pointdown condition was longer than RT in target cannon pointup condition (Target Cannon Orientation Effect). And the
target cannon orientation effect was larger in cannon angle
180° condition than that in cannon angle 0° condition,

1457

Figure 1. A schematic illustration of the twocannon task. Middle: trial procedure. At the
beginning of each trial, a fixation cross was
presented at the center of the screen for 500 ms,
and subjects were asked to fixate on it, then
two cannons (one red and one blue) and eight
pellets (in either red or blue) were presented
together on the computer screen. After a onesecond pause, a randomly selected pellet would
flash as the target pellet. The participants’ task
was to use the arrow keys to rotate the cannon
in the same color of the target pellet toward the
target pellet as quickly as possible. Auditory
feedback was played for either the correct (a
shot sound) or incorrect (an alarm sound)
response; top left: cannon congruent (0°) and
cannon incongruent (180°); top right: pellet
color ratio (blue : red = 6:2, 4:4, and 2:6);
bottom left: likely target pellet (target occurs in
the majority color pellets) and less likely target
pellet (target occurs in the minority color
pellets); bottom right: target cannon orientation
up (12 o’clock is 0°, clockwise rotation, set
315°, 0°, and 45° as the target cannon up
condition) and down (set 135°, 180°, and 225°
as the target cannon down condition).
Table 1. The training data for the model. Each row represents a group of trials that covers different FORs with different
orientations and salience, different target pellet color. The first three columns (“FOR number”, “Cannon Angle” and “Pellet
Ratio B:R”) represent different conditions; Column “Weights” represents relative frequency; Column “Types” represents
how many trial types are contained in each group; the last ten columns represent specific input information, with salience
level ranging from 0.2 to 0.8, and cannon orientation or target location as up-right (UR), up (U), down-left (DL), or downright (DR); the last column is the response (right or left key).

which means there was an interaction between cannon angle
effect and target cannon orientation effect. These results
indicate that people encounter difficulties when they have to
process different conflicting FOR representations and that
they seem to prioritize processing of each FOR by salience.
How the conflicts occur raises an issue. In particular, how
are the conflicts among FORs represented and what is the
mechanism of conflict resolution? Previously, we have
hinted that spatiotemporal association and predictive
learning play a major role in such tasks (Sun & Wang,

2014). In the current study, we evaluate the hypothesis by
developing a neural network model of the two-cannon task.
The model is implemented in Leabra (local, error-driven and
associative, biologically realistic algorithm), a biologically
based computational modeling framework (O'Reilly, 1998;
O'Reilly, Munakata, Frank, & Hazy, 2012). Among all the
features, Leabra incorporates complex network dynamics
using bidirectional excitatory connections and fast pooled
inhibition, which makes it an ideal candidate for exploring
the interference among conflicting cognitive process.

1458

Training and Testing

Figure 2. The model of the two-cannon task. Three 3x3
FOR orientation layers (excluding the center neuron, the
other eight neurons corresponding to eight orientations: Uup, UR-up-right, R-right, DR-down-right, D-down, DLdown-left, L-left, UL-up-left) with three 1x1 salience layer;
one 3x3 target pellet location layer (excluding the center
neuron, the other neurons corresponding to eight locations)
with two 1x1 target pellet color layer (representing the color
of the target pellet); one 10x10 hidden layer; one 2x1 output
layer (Left, Right).

The Neural Network Model
Model Architecture and Connectivity
The model (Figure 2) consists of three levels of layers
(input, hidden and output). At the input level, there are nine
input layers. Each layer encodes one piece of stimulus
information independently. Three 3x3 layers are used to
encode the orientations of two IFORs and one EFOR (Blue
Cannon Orientation, Red Cannon Orientation and EFOR
Orientation in Figure 2). Three 1x1 layers aside the three
FOR input layers are used to encode their salience (Blue
Cannon Salience, Red Cannon Salience, EFOR Salience).
One 3x3 layer filled in gray color is used to encode the
locations of target pellet (Target Pellet Location). Two 1x1
layers aside the target location input layer are used to
encode the color of target pellet (Blue Target Pellet, Red
Target Pellet). At the hidden level, one 10x10 layer is used
to represent and process all the vision input information. At
the output level, one 2x1 layer is used to encode the
response.
The connections among the layers are all bi-directional.
The hidden layer has a self-recurrent excitatory connection
to itself. The weight of forward connection is stronger than
the weight of the feedback and self-recurrent excitatory
connection with a ratio of 2:1. The k-winner-take-all
parameter is used in the hidden layer (the percentage, k=
25%) and the output layer (the number of neurons, k= 1),
which controls the activation level of the two layers without
using explicit inhibitory neurons. All other parameters used
in the model take the default values of Leabra (O'Reilly et
al., 2012).

The training data set consists of 13 groups of trials (Table
1). The first 12 groups are used to train the three FORscoexisting conditions and the last group is for the single
EROR condition. The first 12 groups are generated based on
three independent variables: cannon angle (0°, 180°), the
pellet color ratio (blue: red = 2:6, 4:4, and 6:2), and target
pellet color (Blue, Red). The orientations of each cannon
(shown in “Blue Cannon Orientation” column and “Red
Cannon Orientation” column, same orientations for cannon
angle 0° condition, opposite orientations for cannon angle
180° condition) control the cannon angle (shown in
“Cannon Angle” column). The pellet color ratio (shown in
“Pellet Ratio B: R” column) will change the salience of each
cannon (shown in “Blue Cannon Salience” column and
“Red Cannon Salience” column. e.g. if blue: red= 2:6, we
set 0.2 for blue salience and 0.8 for red salience). The target
pellet color (shown in “Blue Target Pellet” column and
“Red Target Pellet” column) controls the target pellet color
of each trial and the blue/red target pellet also indicates the
correct response should be made by the blue/red cannon. In
each of the 12 groups, there are 36 trial types separated by
the cannon orientation and target pellet location (shown in
“Types” column. 8 cannon orientations and different target
pellet location excluding locations same with or opposite to
cannon and EFOR orientations). In order to increase the
robustness of salience effect, we set the salience groups 3
times more than the other groups (shown in “Weights”
column).
EFOR is always automatic activated as the point-up
orientation because in behavioral studies participants
(“observers”) were always facing with the computer screen.
When the target cannon points above the horizontal, the
target cannon’s IFOR matches the participant’s EFOR;
when it points below the horizontal, IFOR does not match
EFOR, causing a conflict between IFOR and EFOR (Target
Cannon Orientation Effect). Thus, we set the last group for
the EFOR, and specially train the single EFOR with an only
point-up orientation group. In EFOR group, there are 6 trial
types (1 EFOR orientation and 6 target pellet location
excluding locations same with or opposite to EFOR
orientation). In order to increase the robustness of the effect,
we set the weight of the EFOR group as 50. The total trial
number of training data is (1*10+3*2) *36+50*6=876. And
the testing data contains only three FORs coexisting groups,
and each group only repeats by one time, so the total trial
number of testing data is 12*36=432.
We set the maximum cycles of each trial as 200. If the
cycle reaches 200, the current training trial would stop and
switch to next trial. The stop criterion of the whole training
is the average sum square error (SSE) reaching 0 by four
times continuously. In the testing phase, when the activation
of output layer reaches a set threshold (0.85) a response is
said to have been made. The number of cycles is taken as
the measure of model RT.

1459

Thirty “simulated subjects”, each with randomly
initialized weights were trained and tested to get thirty
independent simulation results.

target pellet colors (red target pellet: 26.20 ± 0.79, blue
target pellet: 26.18 ± 0.91) showed no difference in the blue:
red = 4:4 condition, p > .05. No other significant effects
were obtained, ps > .05.

Simulation Results
Analysis
The dependent variable is the number of computational
cycles of each trial. The independent variables are cannon
angle (0°, 180°), target cannon orientation (Up, Down),
pellets color ratio (Blue: Red = 2:6, 4:4, 6:2), target pellet
color (Red, Blue). One 2 (cannon angle) × 3 (pellet color
ratio) × 2 (target pellet color) repeated-measures ANOVA
was performed on the cycles to search for the main effects
of cannon angle effect and the salience effect (Figure 3C.
and Table 2). A 2 (cannon angle × 2 (target cannon
orientation) repeated-measures ANOVA was performed on
the cycles to search for the main effects of target cannon
orientation effect, cannon angle effect, and their interaction
(Figure 3D and Table 3). A correlation analysis was
performed on the simulation results and the behavioral
results (Figure 4). According to the FORMS theory and the
earlier behavioral experimental results, we expected our
model could learn from the training data, and the simulation
results would have a positive correlation with earlier
behavioral experimental results. The effects of interest are
summarized below.

Figure 3 A. RTs for target pellet color, pellet color ratio
and cannon angle in behavioral results; B. RTs for target
cannon orientation and cannon angle in behavioral results;
C. Cycles for target pellet color, pellet color ratio and
cannon angle in simulation results; D. Cycles for target
cannon orientation and cannon angle in simulation results.

Cannon Angle Effect and Salience Effect
In Figure 3C and Table 2, there was a significant main
effect of cannon angle effect, F (1, 29) = 719.96, p < .001,
ηp2 = .96, indicating that the cycles in the cannon angle 0°
condition (23.51 ± 0.07) were fewer than the cycles in the
cannon angle 180° condition (26.28 ± 0.11). There was a
significant interaction of pellet color ratio and target pellet
color, F (2, 58) = 9.10, p < .001, ηp2 = .24. Post hoc
analysis suggested that the pattern as below: the cycles of
the red target pellet (24.81 ± 0.12) were marginally fewer
than that of the blue target pellet (25.09 ± 0.13) in the blue:
red = 2:6 condition, p = .09. In contrast, cycles of the red
target pellet (25.01 ± 0.13) had a trend to be larger than that
of the blue target pellet (24.77 ± 0.11) in the blue: red = 6:2
condition, p > .05. The cycles of the two target pellet colors
(red target pellet: 24.89 ± 0.11, blue target pellet: 24.82 ±
0.11) showed no difference in the blue: red = 4:4 condition,
p > .05. This interaction pattern only appeared significantly
in the cannon angle 180° condition and not in the cannon
angle 0° condition, as revealed by the significant three-way
interaction among cannon angle, target pellet color, and
pellet color ratio, F (2,58) = 47.84, p < .001, ηp2 = .62. Post
hoc analysis in cannon angle 180° condition suggested that
the pattern as below: the cycles of the red target pellet
(25.96 ± 0.99) were fewer than that of the blue target pellet
(26.82 ± 1.15) in the blue: red = 2:6 condition, p < .01. In
contrast, cycles of the red target pellet (26.63 ± 1.06) were
larger than that of the blue target pellet (25.90 ± 0.88) in the
blue: red = 6:2 condition, p < .01. The cycles of the two

Table 2: Cycles of target pellet color, pellet color ratio,
and cannon angle
Cannon
angle
180°
0°

Target
pellet
color
blue
red
blue
red

Pellet color ratio
blue: red
2:6
4:4
6:2
26.82±1.15 26.18±0.91 25.90±0.88
25.96±0.99 26.20±0.79 26.63±1.06
23.36±0.52 23.45±0.52 23.64±0.53
23.66±0.50 23.57±0.60 23.40±0.51

Table 3: Cycles of target cannon orientation and cannon

angle
Target cannon orientation
Down
Up

Cannon angle
0°
180°
23.63±0.52 26.87±1.03
22.89±0.48 25.16±0.65

Target Cannon Orientation Effect and Cannon
Angle Effect
The cycles results (Figure 3D and Table 3) showed
significant main effects for target cannon orientation effect

1460

and cannon angle effect [F (1, 29) = 75.82, p < .001, ηp2 =
.72; F (1, 29) = 717.79, p < .001, ηp2 = .96]. The interaction
of the two factors was also significant, F (1, 29) = 25.03, p
< .001, ηp2 = .46. Post hoc analysis suggested that target
cannon orientation effect in the cannon angle 180° condition
(1.71 ± 0.20) was larger than the effect in the cannon angle
0° condition (0.74 ± 0.13).

Correlation
A correlation analysis between the simulation results and
behavioral results was conducted. For cannon angle effect
and salience effect, we combined all the 12 conditions
(target color × cannon angle × pellet color ratio) together to
analyze the relation among these conditions. It is evident
results (Figure 4A) that there was a significantly positive
correlation, r = 0.53, p < .01, df = 358. For the cannon angle
effect and target cannon orientation effect, we combined the
4 conditions (target cannon orientation × cannon angle)
together to analyze the relation among these conditions.
Results (Figure 4B) showed that there was also a
significantly positive correlation, r =.56, p < .01, df = 118.

Figure 4 A. Correlation in target color, cannon angle and
pellet color ratio; B. Correlation in target cannon orientation
and cannon angle.

Discussion
The results from the neural network model of the twocannon task are consistent with our earlier interpretations of
the behavioral results based on the FORMS theory. The
model shows stable performance and replicates all major
effects with a close match to the behavioral results.
Importantly, these results provide a neural basis to our
theory where we can characterize the competition between

different FOR-based representations by inhibitory
competition among groups of neurons and predictive
learning.
According to the FORMS theory, multiple representations
with respect to different FORs may co-exist in a segmented
but competitive fashion. The salience of any particular
representation is driven by not only the perception of a static
scene (e.g., the ratio between blue and red pellets) but also
the prediction of the changing environment (e.g., a red
cannon would be more likely to be the target cannon
because there are more red pellets). This means that in a
complex environment with multiple spatial relationships,
representations anchored to different FORs have to be
constantly maintained and updated. The simulation results
from the neural network model suggest that the process of
maintenance and updating can take place distributive within
the same group of neurons (hidden layer), and therefore
afford the possibility of interference among different FORbased representations at the neural level.
The model showed the cannon angle effect, target cannon
orientation effect and the interaction between them. The
cannon angle effect was demonstrated by the fewer cycles in
the cannon angle 0° condition (the orientation of the
potential IFOR was the same as the orientation of the target
IFOR) than in the cannon angle 180° condition (the
orientation of the potential IFOR was opposite to the
orientation of target IFOR). And the target cannon
orientation effect was revealed by the fact that the cycles of
the target cannon point-up condition (the orientation of
target IFOR was the same as the orientation of EFOR) were
fewer than that of the target cannon point-down condition
(the orientation of target IFOR was opposite to the
orientation of EFOR). These two effects suggest that in the
hidden layer, there may be the same group of neurons
responsible for computing the output based on the different
FORs (blue cannon anchored to blue IFOR, red cannon
anchored to red IFOR and observer anchored to EFOR).
Therefore, when the orientation of the target IFOR was
same as the orientation of the potential IFOR or EFOR, the
response became faster. On the contrary, when their
orientations were different, inhibitory competition occurred,
and neurons took more time to focus on the target IFOR,
leading to slower responses. In addition, the interaction
between the two effects was revealed by the larger target
cannon orientation effect in cannon angle 180° condition
than that in cannon angle 0° condition. According to the
adding factor theory, when two processes occur in the same
phase, they will compete with each other for the cognitive
resource, resulting in longer RT (Liu, Banich, Jacobson, &
Tanabe, 2004; Liu, Park, Gu, & Fan, 2010; Sternberg,
1969). In this study, it is likely that some shared neurons for
FOR information processing in the hidden layer give rise to
these effects. The salience effect was demonstrated by fewer
cycles in the likely target pellet condition than that in the
less likely target pellet condition. We hypothesize that
neurons in the hidden layer learned to predict the cannon
with the same color of majority pellets as the target cannon.

1461

If the prediction was right, the response would be fast. If the
prediction was incorrect, it would take more time to update
the target cannon, so the response would be slow. The
salience level associated with each FOR might enhance the
competition and lead to a prioritization of the corresponding
FOR. However, it was the real-time prediction and
inhibitory competition that contributed to the extra
computational time for resolving potential conflicts (Sun &
Wang, 2014).
The above analysis suggests that the interaction among
neuron groups in the hidden layer may be responsible for
the modeling results. The next step is to perform further
data analysis, such as PCA and cluster analysis to evaluate
these predictions. Another step is to add the cue data to the
testing data set to make the model process the input
information in temporal sequences. Then the predictive
learning and the target decision making could be more
clearly separated in the model.

Conclusion
Our neural network model replicates the behavioral results
well, supporting the claim that representations with multiple
FORs co-exist and compete to determine performance.
Importantly, it suggests a plausible neural mechanism
underlying the FORMS theory. Depending on whether there
are conflicts among different FOR representations and
whether the actual outcome is consistent with the
expectation, competition takes place at different levels and
results in the engagement and disengagement of different
FOR-based representations. According to the salience
effect, the internal spatial representation of the environment
is always dynamically constructed and updated toward the
anticipated outcomes, rather than just representing static
associations of the current spatial configuration. These
results support our interpretation of spatiotemporal
association and predictive learning.

Acknowledgments
This work was partially supported by a Natural Science
Foundation of China grant (31328013), an Air Force Office
of Scientific Research (AFOSR) grant (FA9550-12-1-0457),
and an Office of Naval Research (ONR) grant (N00014-161-2111).

References
Klatzky, R. L. (1998). Allocentric and egocentric spatial
representations: Definitions, distinctions, and
interconnections. Paper presented at the Spatial
cognition.
Liu, X., Banich, M. T., Jacobson, B. L., & Tanabe, J. L.
(2004). Common and distinct neural substrates of
attentional control in an integrated Simon and
spatial Stroop task as assessed by event-related
fMRI. Neuroimage, 22(3), 1097-1106.
Liu, X., Park, Y., Gu, X., & Fan, J. (2010). Dimensional
overlap accounts for independence and integration

of stimulus-response compatibility effects. Atten
Percept Psychophys, 72(6), 1710-1720.
Mou, W., Fan, Y., McNamara, T. P., & Owen, C. B. (2008).
Intrinsic frames of reference and egocentric
viewpoints in scene recognition. Cognition, 106(2),
750-769.
Mou, W., & McNamara, T. P. (2002). Intrinsic frames of
reference in spatial memory. J Exp Psychol Learn
Mem Cogn, 28(1), 162-170.
O'Reilly, R. C. (1998). Six principles for biologically based
computational models of cortical cognition. Trends
in Cognitive Sciences, 2(11), 455-462.
O'Reilly, R. C., Munakata, Y., Frank, M., & Hazy, T.
(2012). Computational cognitive neuroscience:
PediaPress.
Piaget, J., & Inhelder, B. (1956). The child’s concept of
space. New York: Humanities Pr.
Sternberg, S. (1969). The discovery of processing stages:
Extensions
of
Donders'
method.
Acta
Psychologica, 30, 276-315.
Sun, Y., & Wang, H. (2010). Perception of space by
multiple intrinsic frames of reference. Plos One,
5(5), e10442.
Sun, Y., & Wang, H. (2014). Insight into others’ minds:
spatio-temporal representations by intrinsic frame
of reference. Frontiers in Human Neuroscience,
8(58), 1-11.
Tamborello, F. P., 2nd, Sun, Y., & Wang, H. (2012). Spatial
reasoning with multiple intrinsic frames of
reference. Experimental Psychology, 59(1), 3-10.
Wang, H., Johnson, T. R., & Zhang, J. (2001). The mind’s
views of space. Paper presented at the Proceedings
of the Third International Conference of Cognitive
Science.
Wang, H., Sun, Y., Johnson, T. R., & Yuan, Y. (2005).
Prioritized spatial updating in the intrinsic frame of
reference. Spatial Cognition and Computation,
5(1), 89-113.
Wang, R., & Spelke, E. (2002). Human spatial
representation: insights from animals. Trends in
Cognitive Sciences, 6(9), 376.
Yamamoto, N., & Philbeck, J. W. (2013). Intrinsic frames
of reference in haptic spatial learning. Cognition,
129(2), 447-456.
Zacks, J. M., & Michelon, P. (2005). Transformations of
visuospatial images. Behav Cogn Neurosci Rev,
4(2), 96-118.

1462

