Universals on Natural Language Determiners from a PAC-learnability Perspective
Giorgio Magri (magrigrg@gmail.com)
SFL UMR 7023 (CNRS, University of Paris 8)
61 rue Pouchet, 75017 Paris, France
Abstract
A classical conjecture in generative linguistics is that universal
restrictions on determiners in Natural Language (e.g. monotonicity, invariance, and conservativity) serve the purpose of
simplifying the language acquisition task. This paper formalizes this conjecture within the PAC-learnability framework.
Keywords: Natural Language determiners; PAC-learnability.

Introduction
Determiners and quantifiers. According to Natural Language Semantics (Montague, 1973; Heim & Kratzer, 1978),
a noun like animal and a predicate like tall denote properties,
namely subsets of the domain of quantification D. Determiners like some, every, no denote a function ℘(D) → ℘(℘(D))
that takes a property such as the one denoted by animal and
returns a collection of properties, exemplified in (1).
/
some(animal) = {B | animal ∩ B 6= 0}
every(animal) = {B | animal ⊆ B}
/
no(animal) = {B | animal ∩ B = 0}

(1)

The meaning of the sentence Some/every/no animal is tall
with the parse [ [NP Some/every/no animal ] tall ] is derived
compositionally as follows. The denotation of the NP is obtained by applying the property animal to the function denoted by the determiner. This yields a collection of properties
as in (1), called a generalized quantifier. The sentence is then
true iff the property tall belongs to that generalized quantifier.
Universal restrictions. Westerståhl (1989) formulates “the
basic question facing anyone who studies Natural Language
quantification” as follows: “Logically, the category of determiners is extremely rich. For example, even on a [domain D]
with two elements, there are 216 = 65.536 possible determiners. But, in natural languages, just a small portion of these
are [lexicalized]. Which ones [. . . ]? What are the constraints
on determiner denotations in natural languages?”. It has been
suggested (Westerståhl, 1989; Gamut, 1991; Keenan & Stavi,
1986; Barwise & Cooper, 1981; Benthem, 1983) that all lexical (i.e., not syntactically complex) determiners in Natural
Language satisfy the monotonicity, invariance and conservativity constraints defined below.
1 A determiner ∆ : ℘(D) → ℘(℘(D)) is:
(a) monotone+ (monotone− ) iff B ∈ ∆(A) entails B0 ∈ ∆(A),
for any properties A, B, B0 ⊆ D such that B ⊆ B0 (B ⊇ B0 ).
It is monotone iff it is either monotone+ or monotone− ;
(b) conservative provided B ∈ ∆(A) holds iff A ∩ B ∈ ∆(A),
for any properties A, B ⊆ D;

(c) invariant provided B ∈ ∆(A) holds iff π(B) ∈ ∆ π(A) ,
for any permutation π over the domain of quantification
D and any properties A, B ⊆ D.

DEF

UiL-OTS (Utrecht University)
Trans 10, 3512 JK Utrecht, The Netherlands
Here are some examples. The property dog is a subset of the
property animal. The determiner some is monotone+ because
Some dog is tall entails Some animal is tall. The determiner
no is monotone− because No animal is tall entails No dog
is tall. The determiners some, every and no are conservative because the sentence Some/every/no dog is tall is true iff
Some/every/no dog is a tall dog is true. The latter equivalence does not hold for only: Only dogs are tall is stronger
than Only dogs are tall dogs. And indeed only is not a determiner (rather an adverb), showing that the conservativity
universal for lexical determiners has empirical bite.
The challenge of a learnability approach. Gamut (1991)
points out that, although these universals are “a significant
contribution to the characterization of the notion of possible
human language” they do “not give any clue as to why this
should be so. [. . . ]. A formulation of a universal [. . . ] is
one thing; the explanation for it is something else.” This is
the problem addressed in this paper: how can these universal restrictions on the denotations of lexical determiners be
explained? Generative linguistics has focused mainly on the
conservativity universal, and has suggested that conservativity (and perhaps also the other universals) should be explained
from a learnability perspective. For instance, Hunter and Lidz
(2013) show that four- and five-year-olds fail to learn a novel
nonconservative determiner but succeed in learning a comparable conservative determiner, consistently with the learnability hypothesis.1 The question, then, is what might make conservativity and the other universals well suited from a learnability perspective.
Keenan and Stavi (1986) take on this issue. To start, they
n
note that there are 24 possible determiners over a domain
n
of quantification Dn of cardinality n but that only 23 are conservative. For instance, “in a model with only two individuals
there are [. . . ] 164 = 65.536 determiners [. . . but] only 512 of
these [. . . ] are conservative! The constraint then is extremely
strong [. . . ]: the language learner does not have to seek the
meaning of a novel determiner among all the logically possible [ones]. He only has to choose from among those ways
which satisfy conservativity.” This argument is rather weak.
If the effectiveness of conservativity were to be measured in
terms of its ability to prune the learner’s search space, then
the constraint is not at all “extremely strong” but rather quite
weak, as it does not alter the asymptotic exponential growth
of the search space as a function of the cardinality n of the
domain of quantification. Furthermore, this argument does
not explain why Natural Language enforces precisely conser1 But see Fox (1999) for an approach to conservativity not based
on learnability; and Piantadosi (2012) for modeling evidence that
conservativity does not improve learnabaility.

1494

vativity (together with the other universals) among the many
alternative possible ways of restricting the search space.
Keenan and Stavi also show that the family of conservative determiners coincides with the closure of the set
{some, every} w.r.t. to the operations of conjunction, disjunction and adjectival restrictions. They then observe that this
result “do[es] provide a basis for saying that the set of conservative determiners is cognitively apprehendable. Namely, we
do in fact understand the denotations of simple determiners
(every and some) [. . . ]. And we do have a cognitive grasp of
boolean operations and adjectival restriction, as [. . . ] we use
them in understanding meanings associated with essentially
all categories.” Again, the argument is rather weak. The determiner denoted by only is analogous to that denoted by every, just with the reversed set-inclusion (takes a property A
and returns properties B such that B ⊆ A rather than A ⊆ B).
Thus, only is not conservative as noted above, and yet not in
any way more complex than every. Why has Natural Language restricted determiners to the conservative ones rather
than, say, to the closure of the set {some, only}?

two concepts c, h ∈ Cn and any probability measure P over
Xn , the a
error ec,P (h) of h w.r.t. c relative to P is the probability P(c h)of the symmetric difference between c and h. A
concept class is learnable provided any concept in the class
can be identified from a labeled sample with high accuracy
(error smaller than ε) and with high confidence (larger than
1 − δ), as formalized in Definition 2 (Valiant, 1984; Kearns &
Vazirani, 1994; Kearns, 1999).

Meeting the challenge with PAC-learnability. These objections are not meant to challenge Keenan and Stavi’s learnability approach to universal restrictions on lexical determiners. Rather, they are meant to argue that this learnability approach needs to be cast within an explicit, formal learning
framework, contrary to what done by Keenan and Stavi and
much of the subsequent literature. This paper thus develops
this learnability approach to universal restrictions on determiners within the PAC learnability paradigm. As argued by
Natarajan (1991), “the PAC paradigm appears to be a good
model of the natural learning process while lending itself to
analysis” (but seeClark & Lappin, 2011 for discussion). It
shows that the entire class of determiners is not learnable,
not even according to a very weak PAC-learnability notion.
This result provides support for the hypothesis that Universal Grammar enforces restrictions on lexical determiners to
boost acquisition. It then shows that monotonicity has almost
no effects on learnability. Thus, there are restrictions that do
not affect learnability, reinforcing my point against Keenan
and Stavi’s naı̈ve approach. Finally, it looks at conservativity
and invariance, showing that those restrictions have a strong
learnability effect, allowing PAC-learnability from malicious
positive examples only.

and furthermore m(·, ·, ·) grows polynomially in 1ε ,

No PAC-learnability without restrictions
Generalized quantifiers
are not PAC-learnable. A sample
S
space is a set X = ∞
X
where X1 , X2 . . . are finite (to avoid
n
n=1
measurability problems) and disjoint sets, whose elements
are
S
called examples. A concept class on X is a set C = ∞
n=1 Cn
where each Cn ⊆ ℘(Xn ) is a collection of subsets of Xn ,
called concepts. A sample is an m-tuple x = (x1 , . . . , xm )
in Xnm = Xn × . . . × Xn . A concept c ∈ Cn can be identified
with its characteristic function c : Xn → {0, 1}. The labels assigned by a concept c to a sample x can be collected into the
boolean vector c(x) = (c(x1 ), . . . , c(xm )) ∈ {0, 1}m . For any

DEF 2 A concept class C is PAC-learnable with sample cardinality m : (ε, δ, n) ∈ (0, 1) × (0, 1) × N 7→ m(ε, δ, n) ∈ N provided there exists a learning function A of the form

A : (x, t) ∈ Xnm × {0, 1}m 7→ A(x, t) ∈ Cn

(2)

such that for any ε, δ ∈ (0, 1), any n ∈ N, any concept c ∈ Cn
and any probability measure P over Xn , the following condition holds with m = m(ε, δ, n)
n

o

Pm x ∈ Xnm ec,P A x, c(x) ≥ ε ≤ δ
(3)
1
δ

and n.2

A subset S ⊆ Xn is shattered by Cn provided {S ∩ c | c ∈ Cn } =
℘(S). The concept class Cn has Vapnik-Chervonenkis dimension VCD(Cn ) equal to d ∈ N provided there exists a shattered subset S ⊆ Xn with cardinality d but no shattered subset S ⊆ Xn with cardinality d + 1. VCD controls the sample
cardinality needed for PAC-learnability (Ehrenfeucht et al.,
1989): no learning function A satisfies condition (3) of PAClearnability with sample cardinality m = m(ε, δ, n) smaller
Cn )−1
than VCD(32ε
. With these preliminaries in place, let’s now
go back to generalized quantifiers, defined as follows.
3 Consider the sample space X = ∞
n=1 Xn where Xn =
℘(Dn ) is the collection of properties over a domain of quantification Dn of cardinality n. Consider the concept class
S
Q = ∞n=1 Qn where Qn =℘(Xn ) is the collection of generalized quantifiers q∈℘(℘(Dn )) over Dn .
DEF

S

By definition, Qn shatters Xn . And Xn has cardinality 2n .
Hence, VCD(Qn ) = 2n . Any learning function for Q thus
n −1
needs a sample cardinality m(·, ·, ·) larger than 232ε
and therefore not polynomial in n. I thus conclude that:
RESULT 1 The whole class Q of generalized quantifiers is
not PAC-learnable.

Not even uniformly weakly PAC-learnable. Let’s weaken
condition (3) to (4): the error is not required to be arbitrary
small (i.e., smaller than ε) but just smaller than chance, with
the polynomial q(·) controlling the improvement over chance.
DEF 4 A concept class C is weakly PAC-learnable with sample cardinality m : (ε, δ, n) ∈ (0, 1) × (0, 1) × N 7→ N provided
there exists a learning function (2) such that for any δ ∈ (0, 1),
2 The learning function A is often also required to be computable
in time polynomial in 1ε , 1δ and n (Valiant, 1984). In this paper, I ignore computational efficiency, and focus on a statistical perspective.

1495

any n ∈ N, any distribution P over Xn and any concept c ∈ Cn ,
condition (4) holds with m = m(ε, δ, n)

n
 1
1 o
≤ δ (4)
Pm x ∈ Xnm ec,P A x, c(x) ≥ −
2 q(n)

Where (*) holds because Sn is shattered by monotone quantifiers, as all properties in Sn have the same cardinality and thus
cannot be in a subset relation. And (**) holds because |Sn | is
the number of subsets of cardinality n2 out of n elements. 

and m(·, ·, ·) grows polynomially in 1ε , 1δ and n.
Weak and strong PAC-learnability are equivalent (Schapire,
1990), because of their distribution-independent nature (i.e.,
the learning function needs to succeed for any distribution P).
Thus, weak PAC-learning is only interesting for fixed probability distributions. Consider the uniform distribution U.
VCD also controls the sample cardinality for uniform weak
PAC-learnability (Blumer et al., 1989): no learning function
A satisfies (4) with sample cardinality m = m(ε, δ, n) smaller
Cn )−1 
. Reasoning as above, I thus obtain Result
than VCD(
1
1
32 2 − q(n)
2, that substantially strengthens Result 1.
RESULT 2 The class Q of generalized quantifiers is not
weakly PAC-learnable relative to the uniform distribution.
Results 1 and 2 lend support to the initial conjecture that UG
needs to enforce universal restrictions on the denotations of
quantifiers in order to make the acquisition of quantifiers feasible. The rest of the paper thus investigates the learnability
implications of these universals restrictions.

A restriction that
has no learnability effects. Given
a conS
S∞
cept class C = ∞
C
over
a
sample
space
X
=
X
n
n=1
n=1 n and
0 = S∞ C 0 over a possibly different
another concept class
C
n=1 n
S
0
0
sample space X 0 = ∞
n=1 Xn , C is PAC-riducible to C according to Pitt and Warmuth (1990) iff there exists a polynomial
p(·) and two PAC-reduction maps

Monotonicity does not help with learnability
Monotonicity has a modest learnability effect. A universal
restriction on quantifiers in Natural Language is that they be
monotone, according to Definition 1a. I thus focus on the
PAC-learnability of the class of monotone quantifiers.
S∞
DEF 5 Consider the sample space X = n=1 Xn where Xn =
℘(Dn ) is the collection of properties over a domain of quantification Dn of cardinality n. A generalized quantifier q ∈ Qn
is monotone+ provided that, if B1 ∈ q and B1 ⊆ B2 , then
B2 ∈ q, for any two properties B1 , B2 ∈ Xn ; it is monotone−
provided that, if B1 ∈ q and B2 ⊆ B1 , then B2 ∈ q; it is
monotone
provided it is either monotone+ or monotone− .
S∞
M
Q = n=1 QnM is the concept class of monotone quantifiers.
Result 2 says that the entire class Q of generalized quantifiers
is not weakly PAC-learnable w.r.t. the uniform distribution.
A construction by Kearns et al. (1994) can be readapted to
show that the subclass Q M of monotone quantifiers is instead
weakly PAC-learnable w.r.t. the uniform distribution. Monotonicity thus does lead to a learnability advantage. But the
advantage is very modest, as it relies on the assumption of
uniform distribution, which cannot be relaxed, by Result 3.
S∞
RESULT 3 The class Q M = n=1 QnM of monotone quantifiers
is not (weakly) PAC-learnable (w.r.t. arbitrary distributions).
Proof. As recalled, it is sufficient to show that the VapnikChervonenkis dimension VCD(QnM ) of monotone quantifiers
grows super-polynomially in n. Assume n is even and let
Sn ⊆ Xn be the subset of properties of cardinality n2 . Thus:
  n/2  n 2 n/2  n 
(∗)
√
(∗∗) n
VCD(QnM ) ≥ |Sn | = n = ∑ 2 ≥ ∑ 2 = 2n
2
k=0 k
k=0 k

0
R1 : x ∈ Xn 7→ R1 (x) ∈ X p(n)

0
R2 : c ∈ Cn 7→ R2 (c) ∈ C p(n)

such that for any example x ∈ Xn and any concept c ∈ Cn
we have that x ∈ c iff R1 (x) ∈ R2 (c), i.e. the behavior of the
transformed concept on the transformed examples is exactly
the behavior of the original concept on the original examples. If C is PAC-reducible to C 0 and C 0 is PAC-learnable,
then C is
PAC-learnable too (Pitt & Warmuth, 1990). Let
S
Q M,+ = ∞n=1 QnM,+ be the concept class of monotone+ quantifiers. Result 4 says that, despite the fact that Q M,+ is smaller
than Q M , it has no learnability advantages. This result shows
the importance of exploring the learnability implications of
universal restrictions within an explicit learnability framework. The proof is based on a technique from Kearns et al.
(1994) and Pitt and Warmuth (1990).
4 The subclass Q M,+ is PAC-reducible to the entire
class Q of monotone quantifiers.
RESULT

M

Proof. For any n, order the elements in the domain of quantification Dn into a sequence, so that Dn = (d1 , . . . , dn ). Let
p(n) = 2n. Define the reduction map R1 : x ∈ Xn 7→ R1 (x) ∈
X2n as follows: for i = 1, . . . , n, let di ∈ R1 (x) iff di ∈ x;
and for i = n + 1, . . . , 2n, let di ∈ R1 (x) iff di−n 6∈ x. Then,
R1 (y) ⊆ R1 (x) entails y = x. Define next the reduction map
M,+
R2 : q ∈ Qn 7→ R2 (q) ∈ Q2n
as follows: if q = {x1 , . . . , xm } ∈
M,+
Qn , then R2 (q) ∈ Q2n is the monotone+ quantifier that consists of the properties R1 (x1 ), . . . , R1 (xm ) as well as of all their
supersets. It is easy to check that x ∈ q iff R1 (x) ∈ R2 (q). 

Conservativity and invariance help learnability
A determiner ∆ is conservative (Definition 1b) provided that
B ∈ ∆(A) iff A ∩ B ∈ ∆(A), for any properties A, B ⊆ D. Assume that ∆ is furthermore invariant (Definition 1c). Thus,
whether it is the case that A ∩ B ∈ ∆(A) depends only on the
cardinality of A ∩ B. Thus, to learn from examples the denotations of quantified noun phrases projected by conservative
and invariant determiners means to learn the concept class
Q C,I defined below, that is the focus of this section.
6 Consider the sample space X = ∞
n=1 Xn where Xn =
℘(Dn ) is the collection of properties over a domain of quantification Dn of cardinality n. Consider the concept class
S
Q C,I = ∞n=1 QnC,I where QnC,I is the collection of those generalized quantifiers q ∈ ℘(Xn ) that are conservative and invariant, namely satisfy the following implication: if A ∈ q and
|A| = |B|, then B ∈ q, for any properties A, B ⊆ Xn .
DEF

1496

S

Plain PAC-learnability. A learning function A as in (2)
is consistent provided that for any labeled sample (x, t) ∈
Xnm × {0, 1}m , it returns a concept ĉ = A(x, t) that classifies the examples x = (x1 , . . . , xm ) according to the labels
t = (t1 , . . . ,tm ), i.e. ĉ(xi ) = ti . A consistent learning function satisfies the PAC-learnability condition (3) provided its
sample cardinality m is large enough (Blumer et al., 1989):


2 8VCD(Cn )
13
4
log ,
log
(5)
m(ε, δ, n) ≥ max
ε
δ
ε
ε
The Vapnik-Chervonenkis dimension of the concept class
Q C,I of conservative and invariant quantifiers is n + 1. The
bound in (5) is thus compatible with a polynomial sample
cardinality function m. I thus only need to construct a consistent learning function A for Q C,I . Assume A takes a labeled sample (x, t) ∈ Xnm × {0, 1}m and returns the generalized quantifier q = A(x, t) that contains the properties of a
certain cardinality i ∈ {0, 1, . . . , n} iff one of the properties in
the sample x with a positive label in t has cardinality i. This
learning function is obviously consistent. Hence:
RESULT 5 The class Q C,I of conservative and invariant quantifiers is PAC-learnable.
Result 5 contrasts sharply with Results 1 and 3, lending support to a learnability explanation of the universals of invariance and conservativity. I now strengthen Result 5 by looking
at more demanding PAC-learnability notions.
PAC-learnability from statistical information. According
to PAC-learnability (Definition 2), a learning function has
access to a labeled sample of a concept. I now consider
a stronger notion of PAC-learnability, whereby the learning
function has access only to statistical information about a
concept. For instance, statistical information concerning a
concept c ⊆ Xn over the set Xn of patients of age n could be
the probability w.r.t. a certain distribution P that a patient in c
is overweight. This probability is P{x ∈ Xn | Φ(x, c(x)) = 1}
where Φ(x,t) = 1 iff t = 1 and x is overweight.
DEF 7 A concept class C is PAC-learnable from the exact statistical information induced by Φ1 , Φ2 , . . . : X×{0, 1}→{0, 1}
with sample cardinality m : (ε, n) ∈ (0, 1)×N → m(ε, n) ∈ N
polynomial in 1ε , n provided there exists a learning function


A : ε, n, p ∈ (0, 1) × N × [0, 1]m 7→ A ε, n, p ∈ Cn (6)

The learning function (6) differs slightly from (2): the parameter δ has been suppressed because uninformative samples are already averaged out by the statistical information;
the parameter ε is provided to the learning function; the parameter n is provided as well (that was not necessary in (2),
because implicit in the sample x ∈ Xnm ). We have that:
RESULT 6 The class Q C,I of conservative and invariant generalized quantifiers is PAC-learnable from approximated statistical information.
Proof. Define the functions Φ0 , Φ1 , . . . : X × {0, 1} → {0, 1}
.
by setting Φi (x,t) = 1 iff t = 1 and |x| = i. Let m(ε, n) = n + 1
. ε
and τ = τ(ε, n) = 3n . Define the learning function A of the
form (6) as follows: for any parameters ε and n and for any
vector p ∈ [0, 1]m (whose m = n + 1 components are indexed
from 0 to n), let A(ε, n, p) be the generalized quantifier in
QnC,I that contains properties of cardinality i ∈ {0, . . . , n} iff
2ε
. For any q ∈ QnC,I , I can thus bound as follows:
pi ≥ 3n


n


|x| = i
q
P x ∈ q x 6∈ A ε, n, ( p̂0 . . . p̂qm ) = ∑ P x ∈ q q 2ε
p̂i < 3n
i=0
n
n


= ∑ P x ∈ q |x| = i = ∑ P x ∈ Xn Φi (x, q(x)) = 1
q

q

2ε
2ε
i=0, p̂i < 3n
i=0, p̂i < 3n
n
n
2ε
q
≤
p̂i + τ(ε, n) ≤
+
3n
q 2ε
q 2ε
i=0, p̂i < 3n
i=0, p̂i < 3n

∑

∑

ε
≤ε
3n

such that for any ε ∈ (0, 1), any n ∈ N, any concept c ∈ Cn ,
any probability P over Xn , the following condition holds


ec,P A ε, n, ( p̂c1 , . . . , p̂cm ) ≤ ε
(7)

If a property x of cardinality i does not belong to the quantifier
q, Φi x, q(x) =0 for any x and P{x∈Xn |Φi (x, q(x)) = 1}=0.
q
ε
Hence p̂i ≤ τ(ε, n) ≤ 3n
, and the following quantity is zero.



q
q
2ε
P x 6∈ q x ∈ A ε, n, ( p̂0 . . . p̂qm ) = P x 6∈ q p̂i ≥ 3n
, i = |x|

q
q
As the error eq,P A ε, n, ( p̂1 , . . . , p̂m ) is the sum of the two
terms just bounded, (7) holds.

PAC-learnability from misclassified examples. According to PAC-learnability (Definition 2), the learning function
is trained on a sample x = (x1 , . . . , xm ) ∈ Xnm together with
the corresponding correct labels c(x) = (c(x1 ), . . . , c(xm )) assigned by a target concept c. I now consider a stronger notion
of PAC-learnability, whereby some of the labels c(xi ) are altered. Given ξ ∈ {0, 1} and x ∈ Xn , define c(x, ξ) = c(x) iff
ξ = 1. Assume that ξ is sampled according to a Bernoulli
Bη with probability of success ξ = 1 equal to η ∈ [0, 1]. The
m-tuple (c(x1 , ξ1 ), . . . , c(xm , ξm )) is denoted by c(x, ξ).
DEF 8 A concept class C is PAC-learnable from misclassified
examples with sample cardinality function m : (ε, δ, n, η) ∈
(0, 1) × (0, 1) × N × [0, 12 ) 7→ N if there is a learning function

where p̂ci is exact statistical information, namely:

1
A : (ε, η, x, t) ∈ (0, 1)×[0, )×Xnm ×{0, 1}m 7→ A(ε, η, x, t) ∈ Cn
2

p̂ci = P{x ∈ Xn |Φi (x, c(x)) = 1}

(8)

Also, C is PAC-learnable from approximated statistical information provided (7) holds with (8) replaced by
| p̂ci − P{x ∈ Xn |Φi (x, c(x)) = 1}| ≤ τ
for some constant τ ∈ (0, 1] with

1
τ

≤ m(ε, n).

such that for any ε, δ ∈ (0, 1), any n ∈ N, any η ∈ [0, 12 ), any
concept c ∈ Cn , and any probability P over Xn , we have
n

o

≥ε δ
Pm × Bm
η (x, ξ) ec,P A ε, η, x, c(x, ξ)
and furthermore the sample cardinality
m(·, ·, ·, ·) grows poly
nomially in 1ε , 1δ , n and 1/ 12 − η .

1497

The misclassification rate η cannot be larger than 21 , otherwise learning would be impossible. As the complexity of the
learning task increases as η gets closer to the threshold 12 ,
the cardinality m of the
 sample is allowed to grow (polynomially) with 1/ 21 − η . The learning function A is provided
with the noise rate η and the accuracy parameter ε. PAClearnability from statistical information is known to entail
PAC-learnability from a misclassified sample (Kearns, 1998).
From Result 6 we thus have:
RESULT 7 The class Q C,I of conservative and invariant generalized quantifiers is PAC-learnable from misclassified examples.
PAC-learnability from positive, malicious examples. According to PAC-learnability (Definition 2), when the learning
function is trained on a target concept c, it is provided with
a sample x = (x1 , . . . , xm ) ∈ Xnm that in general contains both
positive examples xi ∈ c and negative examples xi ∈ c (where
c is the complement of c w.r.t. Xn ). I now consider a stronger
notion of PAC-learnability, whereby x is sampled w.r.t. a distribution concentrated on c so that the learning function receives only positive examples (Kearns et al., 1994).
DEF 9 A concept class C is PAC-learnable from positive examples only with sample cardinality m : (0, 1) × (0, 1) × N →
S
m
N
provided there exists a learning function A : ∞
n,m=1 Xn →
S∞
n=1 Cn such that for any ε, δ ∈ (0, 1), any n ∈ N, any concept
c ∈ Cn and any probability measures P, P concentrated over c
and c respectively, condition (9) holds with m = m(ε, δ, n)
(
)
m
m ec,P (A(x)) ≤ ε
P
x ∈ Xn
≥ 1−δ
(9)
ec,P (A(x)) ≤ ε
and furthermore the sample cardinality function m(·, ·, ·)
grows polynomially in 1ε , 1δ and n.
Consider next a noisy variant of this framework, whereby the
distribution P used to sample points from c is corrupted: with
probability µ, the example xi of the sample is chosen not according to the distribution P concentrated on c but according
to a distribution Qi over the entire Xn . The distribution Qi can
be chosen by a malicious adversary that knows the concept c,
the distribution P, the learning strategy A.
DEF 10 The concept class C is PAC-learnable from positive
examples only with malicious error rate µ : (ε, δ, n) ∈ (0, 1) ×
(0, 1) × N 7→ [0, 12 ) and sample cardinality m : (ε, δ, n) ∈
(0, 1) × (0, 1) × N 7→ N if there is a learning function
1
A : (ε, µ, x) ∈ (0, 1) × [0, ) × Xnm 7→ A(ε, µ, x) ∈ Cn
2

and furthermore the sample cardinality function m(·, ·, ·)
grows polynomially in 1ε , 1δ , and n
PAC-learnability from misclassified examples (Definition 8)
allows the error rate η to vary arbitrarily between 0 and 12 . In
the more demanding case of PAC-learnability with malicious
error, η is only required to vary between 0 and the malicious
error-rate µ(ε, δ, n) < 12 . I now show that:
RESULT 8 The class Q C,I of conservative and invariant quantifiers is PAC-learnable from positive examples only with
sample cardinality m and malicious error rate µ as follows:


4
ε
24n
(n + 1) + ln
, µ(ε, δ, n) ≤
(12)
m(ε, δ, n) ≥
ε
δ
8n
The proof rests on the following result (Kearns & Li, 1993).
Suppose the sample cardinality m is large, as in (13).
 4|C | 
24
n
log
(13)
m(ε, δ, n) ≥
ε
δ
Suppose furthermore that for some ε, δ ∈ (0, 1) and η ∈ [0, 4ε ),
the learned concept A(ε, µ, x) assigns a positive label to many
of the examples xi in the sample x = (x1 , . . . , xn ), as in (14).
m
n
O
ε o
δ
P̃k x |{xi |xi 6∈ A(ε, η, x)}| ≤ m ≥ 1 −
(14)
2
2
k=1
Then, A has small error relative to P in the sense that:
m
n
o
O
δ
P̃k x ec,P (A(ε, η, x)) ≤ ε ≥ 1 −
2
k=1

(15)

Proof. Consider the following learning function: A(ε, µ, x) is
the generalized quantifier in QnC,I that contains the properties
of cardinality i ∈ {0, . . . , n} iff the sample x contains at least
ε
4n m properties with cardinality i. As A does not depend on
µ, I will write just A(ε, x). Obviously |Q C,I | = 2n+1 , so that
(12) entails (13). For any sample x = (x1 , . . . , xm ), the quanε
tifier A(ε, x) classifies as negative at most 4n
m(n + 1) < 2ε m
of the m examples in x, so that (14) holds too. By the result
mentioned above, we thus have for any quantifier q ∈ Q C,I :
m
n
o
O
δ
P̃k x ∈ Xnm eq,P (A(ε, x)) ≤ ε ≥ 1 −
2
k=1
The following chain of inequalities finally proves (11).
m
n
O
 o
P̃k x ∈ Xnm eq,P A(ε, x) 0
k=1

such that for any ε, δ ∈ (0, 1), any n ∈ N, any η ∈
[0, µ(ε, δ, n)), any concept c ∈ Cn , any distributions P and
P concentrated over c and c, any additional m distributions Q1 , . . . , Qm over Xn , conditionN(11) holds, where m =
m(ε, δ, n), P̃k = (1 − µ)P + µQk and
is measure-product:
(
)
m
O
m ec,P (A(ε, µ, x)) ≤ ε
P̃k x ∈ Xn
≥ 1−δ
(11)
ec,P (A(ε, µ, x)) ≤ ε
k=1

1498

m
O

n
o
P̃k x ∃x 6∈ q s.t. x ∈ A(ε, x)
k=1


m
n O
∃x 6∈ q s.t.
≤∑
P̃k x
|x| = i, x ∈ A(ε, x)
i=0 k=1


m
 there is x 6∈ q s.t. |x| = i and 
n O
=∑
P̃k x the sample x contains at least

 ε
i=0 k=1
i
 4n m properties of cardinality

m
 x contains at least 
n O
ε
m properties in q
≤∑
P̃k x 4n


i=0 k=1
of cardinality i
≤

(10)



 x contains

n
δ
ε
m
≤∑
P̃k x at least 4n
≤ ∑ e−mε/24n ≤


2
i=0 k=1
i=0
properties in q
|
{z
}
n

Acknowledgments

m
O

I would like to thank G. Chierchia. This work was supported
by a Marie Curie Fellowship (PIEF-GA-2011-301938).

(∗)

References

In the penultimate step, I have noted that the probability (*) is
the probability that m Bernoulli trials each with a probability
ε
ε
overall yield at least 4n
m successes, which
of success µ ≤ 8n
−mε/24n
is bound by e
through Chernoff inequality.

Could a more sophisticated learning function than the one
considered in the preceding proof lead to a stronger result?
namely a higher noise tolerance or a smaller sample cardinality? No learning function
from positive examples only for a
S
concept class C = ∞
C
n=1 n can tolerate a rate of malicious error µ(ε, δ, n) ≥ VCD(εCn )−2 , as shown in Kearns and Li (1993).
The concept class Q C,I has Vapnik-Chervonenkis dimension
VCD(QnC,I ) = n + 1. Hence, the largest tolerable rate of malicious error for Q C,I is of the order of nε . The learning function
in the proof above thus tolerates the largest possible rate of
malicious error.
Furthermore, no learning function satisfies the plain PAClearnability condition (3) with sample cardinality m(ε, δ, n)
Cn )−1
smaller than VCD(32ε
, as recalled above. And Blumer et
al. (1989) show that m(ε, δ, n) cannot be smaller than 4ε log 2δ
either. PAC-learnability thus requires:
m(ε, δ) ≥ max

2 VCD(n) − 1 o
log ,
ε
δ
32ε

n4

(16)

Thus, the learning function in the preceding proof meets the
demanding condition of PAC-learnability from positive malicious examples while using a sample cardinality (12) that
asymptotically exceeds only by a factor n the lower bound
(16) needed for plain PAC-learnability.

Conclusion
I have looked at the conjecture informally made in the recent linguistic literature that universal restrictions on Natural
Language determiners serve the purpose of simplifying the
learning task. To start, I have looked at the monotonicity universal, and I have shown that it contributes only little to simplifying the learning task. This result shows the importance
of investigating the conjectured link between universals and
learnability within an explicit, formal learnability framework.
I have then focused on the universals of conservativity and
invariance. And I have provided support for the conjecture
that they crucially simplify the learning task, by showing that
the class Q C,I of conservative and invariant quantifiers has the
property that the simplest and most straightforward learning
strategy, namely the one considered in the proof of Result 8,
is the optimal one, namely the one that tolerates the largest
tolerable rate of malicious error. Furthermore, the class Q C,I
has the property that the presence of noise (even malicious
noise) does not require any substantial increase of the sample
cardinality compared to the noise-free case.

Barwise, J., & Cooper, R. (1981). Generalized quantifiers and
natural language. Linguistics and Philosophy, 4, 159–219.
Benthem, J. van. (1983). Determiners and logic. Linguistics
and Philosophy, 6.8, 447–47.
Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K.
(1989). Learnability and the vapnik-chervonenkis dimension. Journal of the ACM, 36.4, 929–965.
Clark, A., & Lappin, S. (2011). Linguistic nativism and the
poverty of the stimulus. Wiley-Blackwell.
Ehrenfeucht, A., Haussler, D., Kearns, M., & Valiant, L.
(1989). A general lower bound on the number of examples needed for learning. Information and Computation,
82.3, 247–251.
Fox, D. (1999). Reconstruction, binding theory, and the interpretation of chains. Linguistic Inquiry, 30.2, 157–196.
Gamut, L. T. F. (1991). Logic, language and meaning.
Chicago and London: The University of Chicago Press.
Heim, I., & Kratzer, A. (1978). Semantics in generative
grammar. Blackwell Textbooks in Linguistics.
Hunter, T., & Lidz, J. (2013). Conservativity and learnability
of determiners. Journal of Semantics, 30.3, 315–334.
Kearns, M. (1998). Efficient noise-tolerant learning from
statistical queries. Journal of the ACM, 45.6, 983–10061.
Kearns, M. (1999). PAC-learning. In R. A. Wilson &
F. C. Keil (Eds.), The MIT encyclopedia of cognitive sciences. Cambridge, MA: The MIT Press.
Kearns, M., & Li, M. (1993). Learning in the presence of
malicious errors. Journal on Computing, 22.4, 807–837.
Kearns, M., Li, M., & Valiant, L. (1994). Learning boolean
formulas. Journal of the ACM, 41.6, 1298–1328.
Kearns, M., & Vazirani, U. (1994). An introduction to computational learning theory. Cambridge, MA: The MIT Press.
Keenan, E. L., & Stavi, J. (1986). A semantic characterization of natural language determiners. Linguistics and
Philosophy, 9, 253–326.
Montague, R. (1973). The proper treatment of quantification
in ordinary english. In P. Suppes, J. Moravcsik, & J. Hintikka (Eds.), Approaches to natural language. Reidel.
Natarajan, B. K. (1991). Machine learning. A theoretical
approach. Morgan Kaufmann Publishers.
Pitt, L., & Warmuth, M. (1990). Prediction-preserving reducibility. Journal of Computer and System Science, 41.3,
430–467.
Schapire, R. E. (1990). The strength of weak learnability.
Machine Learning, 5.2, 197–227.
Valiant, L. (1984). A theory of the learnable. Communications of the ACM, 27.11, 1134–1142.
Westerståhl, D. (1989). Quantifiers in formal and natural languages. In D. Gabbay & F. Guenthner (Eds.), Handbook of
philosophical logic (Vol. 4, pp. 1–131). Reidel Publishing.

1499

