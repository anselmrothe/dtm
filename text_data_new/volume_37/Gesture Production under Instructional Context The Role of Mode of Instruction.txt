Gesture Production under Instructional Context:
The Role of Mode of Instruction
Melda Coskun (MELDA.COSKUN@Metu.Edu.Tr)
Cengiz Acartürk (ACARTURK@Metu.Edu.Tr)
Cognitive Science Department, Informatics Institute
Middle East Technical University
06800, Ankara, Turkey
Abstract

Deictic gestures are pointing movements of the arm or the
fingers, which refer to concrete entities in the environment
or refer to virtual entities that are not available in the
environment but are subjects of communication. Deictic
gestures have been proposed to construct a bridge between
speech and the entity that is referred to by speech.
Representational gestures, however, visualize picturable
aspects of speech. Accordingly, representational gestures
bear structural resemblance to what they represent.

We aim at examining how communication mode
influences the production of gestures under specific
contextual environments. Twenty-four participants were
asked to present a topic of their choice under three
instructional settings: a blackboard, paper-and-pencil, and a
tablet. Participants’ gestures were investigated in three
groups: deictic gestures that point to entities,
representational gestures that present picturable aspects of
semantic content, and beat gestures that are speech-related
rhythmic hand movements. The results indicated that
gesture production of the participants was influenced by the
mode of instruction (i.e., board, paper-and-pencil, tablet).

Gestures and Diagrams
There is a close relationship between diagrams and
gestures in terms of employing space and the spatial
relations between mental representations of objects of
interest during the course of communication. According to
Tversky, Jamalian, Giardino, Kang, and Kessell (2013),
gestures can be viewed as virtual diagrams in the air,
whereas diagrams are the permanent traces of gestures on
the surface. More generally, alongside language, gestures
and diagrams may be conceived, as communication
modalities that externalize common conceptual and spatial
mental representations (Acartürk, 2010).
Gestures may represent meaning by sketching virtual
diagrammatic elements in the air (e.g., lines, dots, boxes,
arrows), as stated by Tversky, et al. (2013). In Heiser and
Tversky (2004), the participants worked in pairs on a map to
find the most efficient route to rescue a certain number of
injuries. They produced gestures in a typical diagrammatic
element form; pointing a place (e.g., dot), tracing a path
between places (e.g., line), and tracing a place (e.g., box). In
particular domains, gestures carry domain-dependent roles.
For instance, gestures may highlight certain aspects of the
information represented in time-series line graphs: a
vertically oriented gesture may refer to an increase or a
decrease, whereas a bidirectional horizontal gesture may
refer to a durative state of the domain value (e.g., average
temperature). In time-series bar graphs, directional gestures
accompany verbal descriptions of trend information
(Alacam, Habel, & Acartürk, 2013).
These findings suggest that gestures, like static diagrams,
visualize thought by employing a set of content-free
geometric forms. Diagrammatic entities such as dots, lines,
boxes and arrows are basic content-free geometric forms,

Keywords: Gesture production; Multimodal communication;
Diagrams

Introduction
How gestures represent meaning has been subject to an
interdisciplinary debate in the past decades (Acartürk &
Alacam, 2012; Alibali & Nathan, 2007; Goldin-Meadow,
Kim & Singer, 1999; Goldin-Meadow & Wagner, 2005;
Rauscher, Krauss & Chen, 1996; Valenzeno, Alibali &
Klatzky, 2003, among many others). Gestures have been
proposed to enrich communication by supporting
information in a second modality, usually conveying
information that is not represented in speech. It has also
been claimed that gestures promote communication by
externalizing thoughts that are not formed well enough to
express verbally (Alibali & Goldin-Meadow, 1993), thus
reducing cognitive load of the speaker (Rauscher et al.,
1996).
A frequently used ontology of gesture types is based on a
threefold classification (McNeill, 1992): iconic and
metaphoric gestures (henceforth, representational gestures),
deictic gestures, and beat gestures. Beat gestures are merely
speech-related rhythmic movements. They have not been a
topic of intense research, since they have been conceived as
substantial to the information content in communication. On
the other hand, representational gestures and deictic gestures
have been conceived as closely related to the information
content of communication.

459

which represent meaning through geometric and gestalt
properties. In both gestures and diagrams, these geometric
forms refer to domain-specific entities.
Diagrams convey veridically (e.g., maps, architectural
plans) or inherently (e.g., organizational charts, flows) the
visible spatial relations by using diagrammatic constituents
and the spatial relations between the constituents. The
perpetual nature of diagrams allows them to represent
information in conventional forms (Tversky et al., 2013),
also by employing a wide range of perceptual processes,
such as, compare; contrast; highlight similarity, distance,
direction, shape, and size; rotate, group (Tversky, 2009).
Diagrams and gestures differ in their temporal
characteristics. A major difference between gestures and
diagrams is that gestures are momentary actions, whereas
diagrams are relatively permanent visual representations. On
the other hand, it is likely that the close relationship between
diagrams and gestures is based on their common roots in
cognitive sub-systems that are committed to processing
spatial information. In the present study, we focus on
gesture production in diagram-rich environments due to this
close coupling between the two modalities.

•
•
•

Board-and-boardmarker
Paper-and-pencil
Tablet-and-pen

In the rest of this paper, these interfaces will be shortly
named board, paper, and tablet as three modes of
instruction. The next section presents a broad overview of
diagrams and gestures in instructional settings.

Diagrams and Gestures in Instructional
Settings
Instructional settings are rich in diagrammatic
representations. Diagrams are indispensable constituents in
most instructional settings. Learning with multiple
representations (i.e., diagrams and text) has been a major
topic in cognitive science and related domains of research
(e.g., Glenberg & Langston, 1992; Hegarty & Just, 1993;
Hegarty, et al., 2005; Zahner & Corter, 2010), including
instructional science (e.g., Gyselinck, et al., 2000; Mayer,
2009; Eitel & Scheiter, 2014, among many others). The
findings mostly revealed a facilitating role of diagrams in
learning. Accordingly, the research focus has been directed
to the circumstances in which diagrams have those
facilitating roles.
Previous research on gestures and learning has also
revealed that nonverbal behavior might play a significant
role in instructional settings (Kendon, 1980; McNeill,
1992). The studies have shown that teachers’ nonverbal
behavior might influence learning processes, for instance,
by conveying teachers’ attitude toward students. Gesture
research in instructional science has a broad coverage of
topics, from foreign language education (e.g., Sime, 2006)
to mathematics education (Gerofsky, 2011). As a matter of
fact, instructional science has been a major domain, in
which gesture research has exhibited significant progress.
In the present study, we selected instructional setting as
a contextual environment for our study due its richness both
in the number and variety of diagrammatic representations
and gestures. We also selected three modes of instruction, as
described in detail below.

Gestures and Diagrams in Context
In spite of the intense interest in gesture production in
communication and its relation to linguistic utterances, the
variability of contextual environments has attracted limited
interest. The usual environment for gesture production has
been a participant in a conversation. For example, McNeill’s
(1992) descriptions of gestures were contextualized in a
dialogue environment, in which a participant produced
spontaneous gestures in verbal communication. Similarly,
Alibali (2005) employed a conversation as a contextual
environment to describe the importance of gestures in daily
communication, in which a man held two bags with his
hands, needed to drop the bags and freed his hands when the
conversation started.
The previous research on the interaction between
language and gestures has made significant contribution to
the study of human cognition. We propose that a further
investigation of gesture production under specific contextual
environments has the potential to contribute to cognitive
science research. Moreover, it is not known whether the
novel modes of communication (e.g., tablets and mobile
phones) influence communication in positive or negative
ways, by influencing gesture production More specifically, a
question remains to be answered about whether the specific
type of communication mode has any influence on gesture
production.
In the present study, we investigate how communication
mode influences the production of gestures under diagramrich (thus, expectedly gesture-rich) instructional settings.
For this, we employed three interfaces that allow sketching,
as well as production of gestures:

Experiment
Participants, Materials, and Design
Twenty-four content specialists, who had expertise
content knowledge in math or science and were experienced
in designing instructional products, (15 female, nine male)
volunteered to participate in the study (mean age M = 30,
SD = 3.7, age range was 25-38). The language of the
experiment was Turkish and the participants were native
Turkish speakers. Thirteen participants were experts in
Math and 11 participants were experts in General Science.
The experiment design consisted of three within-subject
conditions, namely the mode of instruction, as described in
the previous section; board, paper, and tablet. Participants

460

used whiteboard in the board condition, an A4-size paper in
the paper condition, and a SMART Podium ID422w
interactive pen display in the tablet condition (Figure 1).
Six groups were created by changing the order of these
sessions in each group. This order was randomized across
participants to counterbalance variance. The participants
were randomly assigned to each group. Instructions were
given to all participants before starting the experiment.
None of the participants were aware of the aim of the study.
They were told that it was investigated how different
technologies affect learning and the sessions would be
displayed by students later on, without providing any
information about interest in gestures.
The participants were asked to select a topic and teach it
as if there was a listener. They were also asked to the same
topic in three sessions, namely board, paper and tablet. All
the participants provided verbal informed consent for video
recording during the sessions. Time was constrained to
between 5 and 10 minutes for each session. The mean
number of gestures was measured as a dependent variable.

Gesture Annotation The speech-accompanying gestures
were annotated following the methodology developed by
Duncan (McNeill, 2005) based on the approach suggested
by McNeill (1992, 2005). Gestures without speech were not
included in the analysis. Type of gestures was determined
based on the following descriptions (Figure 2):
§ Representational Gesture: Gestures that presented
picturable aspects of semantic content. Various aspects of
gesture such as form, direction, motion trajectory of hand
played a certain role in the depiction of semantic content
(Alibali & Nathan, 2012).
o Gestures that represented action or concept of a
drawing or an object were assumed to be
representational gestures. For example, the speaker's
hand moved around a particle representation on the
board to represent electric field.
o Gestures that represented direction were accepted as
representational gestures. For instance, in one part of
the experiment the participant moved his hand forward
while showing an upward palm to represent an
outward direction.
§ Deictic Gesture: Pointing movements that indicate
physical, available objects, as well as physically
unavailable ones at the time of gesturing. McNeill (1992)
exemplifies deictic gestures which point unavailable
entities with the following example: The speaker points to
empty space to refer to a city rather than the physical
space as he is asking to the listener where he came from
before. In the present study, gestures that pointed a
particular part or all parts of a diagram were annotated as
deictic gestures.

Figure 1: The modes of instruction of the experiment.
Explanation of equivalent fractions in the board (left), paper
(middle) and tablet (right) session.

o The words referring to objects such as “this”, “here”,
“there” were often accompanied by deictic gestures.

The participants selected topics from four subjects of
Math and three subjects of General Science which were
listed below1:
• Math:
o
o
o
o
• Science:
o
o
o

o Gestures that traced a particular part or all parts of a
drawing were assumed to be deictic gestures.
o Gestures pointing a group of text, drawings or
formulas were assumed to be deictic gestures. It
created an impression on the audience that the speaker
was indicating things inside a particular area the
boundary of which was marked by the speaker’s hand
movements.

Geometry (6)
Algebra-I (5)
Precalculus (1)
Trigonometry (1)
Physics (4)
Chemistry (4)
Biology (3)

o Deictic gestures were typically performed with the
extended finger or hand in the board session whereas
they were mostly produced by the pen in paper and
tablet sessions.

By the end of the experiment, 24 participants were
recorded in three sessions and a total of 72 experiment
protocols were obtained. ELAN (ELAN, 2013) was used for
gesture and arrow annotations.

1

§ Beat Gesture: Speech-related rhythmic hand movements.
In the present study, following the literature (McNeill,
1992), beats were divided into two forms; discrete and
continuous. Beat gestures in discrete form were produced
when a syllable, word or clause was stressed and
disappeared right after the utterance. Beat gestures in
continuous form were produced throughout speech (word,
clause, and sentence). They often occurred in a series of a

The numbers in parentheses indicate the number of topics.

461

particular hand movement. Circular, continuous
movement of hand could be an example for a beat gesture
in continuous form.

Table 1: The mean number of gestures (standard deviations)
for subtypes of gestures

Board
Paper
Tablet

Deictic
31.25 (13.58)
29.92 (16.22)
26.75 (11.63)

Beat
Representational
20.79 (20.16)
3.54 (3.71)
17.00 (14.55)
2.13 (2.19)
14.38 (10.03)
1.13 (1.22)

Note Zero values were included in the analysis. The numbers in
parentheses indicate Standard Deviation

A three-way within-subjects ANOVA was conducted to
evaluate the effect of the instructional mode on the mean
number of gestures. Main effects and interaction effects
were tested for the mode of instruction (board, paper, tablet)
and the gesture type (deictic, representational, beat) using
multivariate criterion of Wilk's lambda (λ). A significant
main effect was obtained for the mode of instruction, λ =
.37, F(2, 22) = 6.44, p < .05, as well as for gesture type, λ =
.86, F(2, 22) = 69.40, p < .05. However, the interaction was
not significant.
Three paired-samples t tests were conducted to compare
the mean number of gestures among the conditions,
controlling for familywise error rate using Holm's sequential
Bonferroni approach. Gestures were significantly higher in
the board condition compared to the tablet condition, t (23)
= 3.66, p < .05. However, there was no significant
difference between the paper condition and the tablet
condition, t (23) = 1.67, p = .11 in the number of gestures.
The difference between the board condition and the paper
condition was also not significant, t (23) = 1.75 p = .09.
Three paired-samples t tests were conducted to compare
the number of gestures types, controlling for familywise
error rate using Holm's sequential Bonferroni approach. The
mean number of deictic gestures was significantly higher
compared to representational gestures, t (23) = 11.47, p <
.05, and beat gestures, t(23) = 3.86, p < .05. The mean
number of beat gestures was significantly higher than
representational gestures, t (23) = 5.97, p < .05.

Figure 2: A representational gesture illustrating “pull and
push ” action (left), a deictic gesture pointing to the particle
on the table (middle), a beat gesture which is a speechrelated rhythmic movement (right).
All the gesture annotations were initially performed by
the first author of study. Randomly-selected 25% of the all
gestures (880 of 3523) were annotated independently by a
second coder who was aware of the aim of the study, for
reliability analysis. Both coders were native Turkish
speakers and they annotated the participants’ gestures and
arrows by listening and watching their recordings. The
comprehension of the participants’ utterances also played
role in the annotation. Cohen’s kappa was used to calculate
inter-rater reliability between coders. The inter-rater
agreements of initial annotations were calculated as .75. The
value above .61 indicates substantial inter-rater agreement
and the value between .81 and .99 indicates almost perfect
agreement based on Landis and Koch (1977). Upon
discussion the coders re-annotated the gesture data and the
agreement was calculated as .96.

Results
Teaching Duration. A one-way within-subjects ANOVA
was conducted to evaluate a significant difference in the
duration of the teaching sessions. A comparison of the mean
durations (in minutes) among the tablet condition (M = 6.8,
SD = 2.29) the board condition (M = 6.5, SD = 2.22) and the
paper condition (M = 6.0, SD = 2.11) revealed no significant
difference, Wilk’s λ = .82, F(2, 22) = 2.4, p = .11.
This result indicates that teaching duration was not
affected by the instructional mode conditions. Therefore, it
was not included in the further analyses.

Discussion
The results of the experimental investigation revealed that
gesture production patterns of the participants were
influenced by the contextual environment (in this case, the
mode of instruction). In particular, the participants produced
more gestures when they used a board-and-boardmarker (in
a standing position), compared to the instructional context,
in which they used a tablet or paper-and-pencil (both in a
sitting position). There are at least two likely sources of the
findings about the mode of communication, as described
below.
The first is conventional vs. non-conventional modes of
communication. All the participants of the experiment were
expert educators, for whom the most frequently used type of
instructional setting was the blackboard setting during their
previous education. The other two modes, namely paper and
tablet, are not as frequent as the board setting as a mode of
communication currently. This aspect of instructional
settings may change by technological advances in future.

Mean Number of Gestures. The mean number of gestures
were analyzed under the three modes of instruction and
three types of gestures. The analysis revealed the results
shown in Table 1.

462

The second likely source of more-frequent gesture
production in the board setting is that the board setting has a
different physical configuration than the paper-and-pencil
setting and the tablet setting, in terms of participants’
position during the task (standing vs. sitting). The board
condition allows participants to control their bodily postures
and to produce arm gestures more freely. In other words, in
the standing position, the speakers may include their body
posture in a conversation more freely compared to the
sitting positions. The sitting positions may restrict the hands
of a speaker from engaging in a conversation. Moreover, the
possibility of a having technical problem (e.g., touching an
irrelevant location on the tablet screen) might have resulted
discomfort on the speakers’ side, which resulted in limited
number of gestures hand movements in the tablet setting.
Taking into account the facilitating role of gestures in
learning, these findings reveal an important advantage in
favor of the classical blackboard instructional setting over
the more recent tablet setting.
In terms of the number of gestures, gesture production
also exhibited significant differences among sub-types:
Deictic gesture was the most frequently produced gesture.
Participants produced significantly higher number of deictic
gestures than both representational gestures and beat
gestures. Representational gestures were the least produced
gesture type. These findings show that the deictic function
of gestures in the instructional context had a much larger
role than its representational function in the present study. A
likely reason for this finding is that the settings consisted of
boardmarkers and pens; hence, the speakers preferred to
visualize their thoughts in a concrete way by sketching
instead of producing representational gestures. It is also
likely that the picturable aspects of the content were limited,
which might have resulted in much less number of
representational gestures than deictic gestures. As a
consequence, a comparison between the two gestures types
in terms of their frequency of production may be
misleading, when one generalizes the findings obtained in
the present study to other contextual environments. A moreconstrained experiment design, which controls the spatial
content of the stimuli, would better address the role of
deictic vs. representational gestures in different contextual
environments.

results reveal differences in gesture production among these
three modes of instruction. In particular, deictic gestures are
more frequent compared to representational gestures.
Moreover, board is a more appropriate environment for
allowing gestures, both in terms of their number and variety.
In future work, we plan to address the limitations in the
present study, which were mostly due to difficulties in
setting up a more realistic experiment environment: The
participants were asked as if they made their presentation to
a classroom audience. A real classroom setting would
influence the gesture production patterns.
We also conjecture that future intrusion of tablet use in
daily life may result in changes in gesture patterns in tablets.
Finally, we conceived gesture production as a domaindependent. action in the present study The specific domain
of discourse (e.g., topics geometry or physics) , depending
on its semantic richness in spatial terms, may influence
production of different gesture types.
Acknowledgements: This work was partially funded by
Marie Curie Actions IRIS (ref. 610986, FP7-PEOPLE2013-IAPP) and METU Scientific Research Project scheme
BAP–08-11-2012-121 Investigation of Cognitive Processes
in Multimodal Communication. Thanks SEBIT Inc. for
technical support during data collection. Thanks our
reviewers for their useful comments and suggestions.

References
Acartürk, C. (2010). Multimodal comprehension of graphtext
constellations:
An
information
processing
perspective. Unpublished dissertation, University of
Hamburg.
Acartürk, C., & Alacam, O. (2012). Gestures in
communication through line graphs. In N. Miyake, D.
Peebles & R. P. Cooper (Eds.), Proceedings of the 34th
Annual Conference of the Cognitive Science Society. (pp.
66-71). Austin, TX: Cognitive Science Society.
Alacam, Ö., Habel, C., Acartürk, C. (2013). Gesture and
Language Production in Communication through Bar
Graphs. In M. Knauff, M. Pauen, N. Sebanz, & I.
Wachsmuth (Eds.), Proceedings of the 35th Annual
Conference of the Cognitive Science Society (pp. 17141719). Austin, TX: Cognitive Science Society.
Alibali, M. W., & Goldin-Meadow, S. (1993). GestureSpeech Mismatch and Mechanisms of Learning: What the
Hands Reveal about a Child′s State of Mind. Cognitive
Psychology, 25, 468-523.
Alibali, M. W. (2005). Gesture in Spatial Cognition:
Expressing, Communicating, and Thinking About Spatial
Information. Spatial Cognition & Computation, 5(4), 307331.
Alibali, M. W., & Nathan, M. J. (2007). Teachers’ gestures
as a means of scaffolding students’ understanding:
Evidence from an early algebra lesson. Video research in
the learning sciences, 349-365.

Conclusion and Future Work
Gestures are an integrated part of speech. The relationship
between verbal and nonverbal communication modalities,
such as the relationship between language and gestures, and
language and diagrams have been the topic of intense
research since the past several decades. Further research is
needed to extend research on multimodal communication,
by focusing on relationship between nonverbal modalities
(Flevares & Perry, 2001), as well as the production of
gestures under specific contextual environments. The
present study aimed at addressing and filling this gap in the
literature, by focusing on how the mode of instruction
(board, paper and tablet) influences gesture production. The

463

Alibali, M. W., & Nathan, M. J. (2012). Embodiment in
Mathematics Teaching and Learning: Evidence From
Learners' and Teachers' Gestures. Journal of the Learning
Sciences, 21(2), 247-286.
Eitel, A., & Scheiter, K. (2014). Picture or text first?
Explaining sequence effects when learning with pictures
and text. Educational Psychology Review, 27(1), 153.180.
doi 10.1007/s10648-014-9264-4.
ELAN (Version 4.6.2) [Computer software]. (2013)
Retrieved from http://tla.mpi.nl/tools/tla-tools/elan/. Max
Planck Institute for Psycholinguistics, The Language
Archive, Nijmegen, The Netherlands.
Flevares, L. M., & Perry, M. (2001). How many do you see?
The use of nonspoken representations in first-grade
mathematics lessons. Journal of Educational Psychology,
93(2), 330-345.
Gerofsky, S. (2011). Mathematical learning and gesture:
Character viewpoint and observer viewpoint in students'
gestured graphs of functions. Gesture, 10(2-3), 321-343.
Glenberg, A. M., & Langston, W. E. (1992).
Comprehension of illustrated text: Pictures help to build
mental models. Journal of Memory and Language, 31,
129-151.
Goldin-Meadow, S., Kim, S., & Singer, M. (1999). What
The Teacher's Hands Tell The Student's Mind About
Math. Journal of Educational Psychology, 91(4), 720730.
Goldin-Meadow, S., & Wagner, S. (2005). How Our Hands
Help Us Learn. Trends in Cognitive Sciences, 9(5), 234241.
Gyselinck, V., Ehrlich, M. F., Cornoldi, C., De Beni, R., &
Dubois, V. (2000). Visuospatial working memory in
learning from multimedia system. Journal of Computer
Assisted Learning, 16, 166–176.
Hegarty, M., & Just, M. A. (1993). Constructing mental
models of machines from text and diagrams. Journal of
Memory and Language, 32, 717-742.
Hegarty, M., Mayer, S., Kriz, S., Keehner, M. (2005). The
role of gestures in mental animation. Spatial Cognition
and Computation, 5, 333-356.
Heiser, J. & Tversky, B. (2004). Characterizing diagrams
produced by individuals and dyads. In T. Barkowsky
(Editor).
Spatial cognition: Reasoning, action,
interaction. Pp. 214-223. Berlin: Springer-Verlag.
Kendon, A. (1980). Gesticulation and speech: Two aspects
of the process of utterance. The Relationship of verbal
and nonverbal communication. Pp. 207-227. The Hague:
Mouton.
Landis, J. R., & Koch, G. G. (1977). The measurement of
observer agreement for categorical data. Biometrics, 159174.
Lausberg, H., & Sloetjes, H. (2009). Coding gestural
behavior with the NEUROGES-ELAN system. Behavior
Research Methods, Instruments, & Computers, 41(3),
841-849. doi:10.3758/BRM.41.3.591.
Mayer, R. E. (2009). Multimedia learning (2nd ed.).
Cambridge: Cambridge University Press.

McNeill, D. (1992). Hand and Mind: What gestures reveal
about thought. Chicago: University of Chicago Press.
McNeill, D. (2005). Gesture and thought. Chicago:
University of Chicago Press.
Rauscher, F. H., Krauss, R. M., & Chen, Y. (1996). Gesture,
Speech, and Lexical Access: The Role Of Lexical
Movements In Speech Production. Psychological Science,
7(4), 226-231.
Sime, D. (2006). What do learners make of teachers'
gestures in the language classroom? Review of Applied
Linguistics in Language Teaching, 44(2), 211-230.
Tversky, B. (2001). Spatial schemas in depictions. In
Spatial schemas and abstract thought, 79-111.
Tversky, B., Jamalian. A., Giardino. V., Kang. S., Kessell.
A., (2013). Comparing Gestures And Diagrams. In Online
Proceedings of TiGeR 2013: The combined meeting of the
10th international Gesture Workshop (GW) and the 3rd
Gesture and Speech in Interaction (GESPIN) conference.
Tversky, B. (2009). Spatial cognition: Embodied and
situated. In The Cambridge handbook of situated
cognition (pp. 201-217). Cambridge: Cambridge
University Press.
Valenzeno, L., Alibali, M. W., & Klatzky, R. (2003).
Teachers’ gestures facilitate students’ learning: A lesson
in symmetry. Contemporary Educational Psychology,
28(2), 187-204.
Zahner, D. C., & Corter, J. E. (2010). The process of
probability problem solving: Use of external visual
representations. Mathematical Thinking and Learning,
12(2), 177-204.

464

