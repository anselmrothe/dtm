Staying afloat on Neurath’s boat – Heuristics for sequential causal learning
Neil R. Bramley1 (neil.bramley@ucl.ac.uk), Peter Dayan2 (dayan@gatsby.ucl.ac.uk),
David A. Lagnado1 (d.lagnado@ucl.ac.uk)
1 Department
2 Gatsby

of Experimental Psychology, UCL, 26 Bedford Way, London, WC1H 0DS, UK
Computational Neuroscience Unit, UCL, Alexandra House, 17 Queen Square, WC1N 3AR, UK
Abstract

erative connections. We adopt Cheng’s power PC (1997)
parametrization for which the probability that a variable takes
the value 1 is a noisy-OR combination of the power or
strength S of any active causes in the model, together with
an omnipresent background cause B that is exogenous to the
model. S and B are assumed to be the same for all connections and components, and there is no other latent variable
(although see Buchanan, Tenenbaum, & Sobel, 2010).

Causal models are key to flexible and efficient exploitation
of the environment. However, learning causal structure is
hard, with massive spaces of possible models, hard-to-compute
marginals and the need to integrate diverse evidence over many
instances. We report on two experiments in which participants
learnt about probabilistic causal systems involving three and
four variables from sequences of interventions. Participants
were broadly successful, albeit exhibiting sequential dependence and floundering under high background noise. We capture their behavior with a simple model, based on the “Neurath’s ship” metaphor for scientific progress, that neither maintains a probability distribution, nor computes exact likelihoods.

Optimal structure learning
The likelihood of a datum (a complete observation, or the
outcome of an intervention) d given a noisy-or parametrized
causal model m over variables X, with strength and background parameters S and B is

“We are like sailors who on the open sea must reconstruct
their ship but are never able to start afresh from the bottom.
Where a beam is taken away a new one must at once be put
there, and for this the rest of the ship is used as support.”
(Quine, 1969, p3)

P(d|m, S, B) = ∏x∈X P(dx |d pa(x) , S, B)

(1)

∑y∈pa(x) dy

P(dx = 1|d pa(x) , S, B) = 1 − (1 − B)(1 − S)

(2)

Introduction
where pa(x) denotes the parents of variable x in the causal
model. We can thus compute the posterior probability of
model m ∈ M over a set of models M given a prior P(M) and
observations D. We can condition on S and B if known:

It is tremendously hard to learn causal models. Even in apparently simple circumstances, it is necessary to cope with
a huge diversity of complex, noisy and probabilistic interactions, and thus to integrate, often painfully, over extended experience. Optimal reasoning with distributional causal beliefs
places substantial demands on inference and storage. Nevertheless, in several studies (Bramley, Lagnado, & Speekenbrink, 2014; Coenen, Rehder, & Gureckis, 2014; Lagnado
& Sloman, 2004, 2006; Steyvers, 2003) it has been shown
that people can learn successfully from interventional data
in probabilistic scenarios. Existing experiments have largely
been confined to small structures, small data and semideterminism, thus limiting the computational demands and
the need for heuristics or approximations. Here, we report on
two experiments designed to tax learning more severely, with
a broad range of structures, long sequences of data points,
and substantial noise (Experiment 1) whose level and nature
participants have to infer as they learn (Experiment 2). We
thereby examine how people deviate from rational norms, and
explore what this can tell us about their psychological processes.

P(m|D, S, B) =

P(D|m, S, B)P(m|S, B)
∑m0 ∈M P(D|m0 , S, B)P(m0 |S, B)

(3)

or else marginalize over their possible values
R

P(m|D) =

S,B P(D|m, S, B)p(S, B)P(m) dS dB
R
0
0
S,B P(D|m , S, B)p(S, B)P(m ) dS

∑m0 ∈M

dB

(4)

If data arrive sequentially, we can either integrate them at
the end, or update our beliefs sequentially, taking the current
posterior as the new prior P(M) for the next datum1 .

Scope for approximation
Learning is hard because the number of possible graphs
grows rapidly with the number of components (3, 4 and 5variable problems have 25, 543, 29281 respectively) and there
is no known closed form update for densities over S and B in
noisy-OR models. To understand how people might mitigate
this computational explosion, we take inspiration from machine learning.

Representing causal structure
We adopt a ubiquitous framework for formalizing models
of causal structure – the parametrized directed acyclic graph
(Pearl, 2000). Arrows represent causal connections; and parameters encode the influence of parents (the source of an
arrow) on children (the arrow’s target). Such graphs can
represent continuous variables and any forms of causal relationship; here we focus on binary {0, 1} variables and gen-

Approximating with a few hypotheses One common approximation is based on a manageable number of individual
hypotheses, or particles (Liu & Chen, 1998), with weights
1 For the present, we ignore the related question of active learning
– i.e., the efficient selection of interventions. See the discussion.

262

corresponding to their relative likelihoods. Sophisticated
reweighting and resampling schemes allow particle filters impressive fidelity.
In rodent learning (Courville & Daw, 2007), and human
categorisation (Sanborn, Griffiths, & Navarro, 2010) and binary decision making (Vul, Goodman, Griffiths, & Tenenbaum, 2009), it has been proposed that people’s beliefs actually behave more like a single particle, capturing why individuals often exhibit fluctuating and sub-optimal judgements,
whereas group-level posteriors are smooth.

structs: the dissimilarity between bt−1 and a potential new bt ,
and the suitability of that bt for capturing dt . We quantified
dissimilarity in two ways. One is simple difference Eb∗t bt−1 ,
which is 1 iff bt is non-identical to bt−1 and 0 otherwise. The
second is the Edit distance Ebt bt−1 , which counts the number of edits (additions, subtractions, reversals of links) going
from bt−1 to bt (ranging from 0 to 6 for a 4 variable problem).
We quantified the suitabilities via two approximate likelihoods. One, Lbt (dt ), is the correct noisy-OR likelihood under a prospective new belief bt . The second, explanatory
inAdequacy Abt (dt ), just counts the number of component
states that the prospective model fails to explain.
We considered the eight viable combinations of these
constructs (singletons labeled E, E ∗ , L, A; pairs labeled
E ∗ L, E ∗ A, EL, EA). Each model can be taken to generate a
likelihood for a subject’s choices based on a softmax probability that the model assigns to a choice of bt . For instance,
for EA, this probability is

Local search A related simplification is to edit these particle hypotheses only locally – for instance adding, subtracting
and reversing individual connections to one’s current causal
structure in searching for changes that make the model more
likely (Cooper & Herskovits, 1992). This is approximate
since the complex dependencies between the connections imply that one cannot guarantee to be able to learn each one
separately (although see Fernbach & Sloman, 2009).
Prior assumptions People might also exploit simplifying
priors, for instance, expecting causal connections to be strong
(high Strength) and sparse (low Background noise) (Lu,
Yuille, Liljeholm, Cheng, & Holyoak, 2008), and structures
to be “well designed” (Bramley, Gerstenberg, & Lagnado,
2014), lacking redundant connections, or unconnected components. These would be sensible, since causal models simplify inference only to the extent that their structure reduces
the number of relata per variable. Mayrhofer and Waldmann (2011) suggest that people might favor deterministic causal structures, accommodating noisy data by assuming that causal connections are occasionally “broken”. Their
study assumed an absence of background noise; but one could
imagine an equivalent accommodation treating inexplicable
events as being ‘miraculous’. This suggests the heuristic
proxy for likelihood judgments for a model as a simple count
of the number of variables lacking explanation.

P(bt ) =

exp(Ebt bt−1 θ1 + Abt (dt )θ2 )
∑ exp(Ebt bt−1 θ1 + Abt (dt )θ2 )

(5)

with parameters θ1 and θ2 that can be fit to maximize the
likelihood. For the moment, we assume that subjects search
over all possible edits; how they actually perform this search
is an important question for the future.
a
A

E: 0
A: 2 (b,c)
C
c Cost: 0 + 2 = 2

a
A

E: 1 (+a→b)
A: 0
C
c Cost: 1 + 0 = 1

A
a

E: 3 (+a→b, +a→c, -b→c)
A: 0
C
c Cost: 3 + 0 = 3

B
b

+

A
a

B
b

B
b

C
c

Old belief
(bt-1)

Evidence
(dt)

B
b

Prospective new beliefs (bt)

Figure 1: A simple structure change model. The learner encounters
data that are not well explained by their model so they search for a
local change that improves it. By balancing the edit-cost E against
reduced inability to explain the latest outcome A, they opt to add a
connection a → b.

A class of simple structural change models
The resulting picture of a heuristic causal learner is reminiscent of Neurath and Quine’s (1969) metaphor for theory
change in science. Here, the theorist is cast as relying on
their theory to stay afloat, without the privilege of a drydock to make major improvements. At most local changes
to patch leaks and to improve the theory are possible, without
the whole space of possibilities ever being considered.
Similarly, we propose that causal learners might: (1) maintain only a single causal model (a single particle) bt−1 at time
t − 1; (2) search for local improvements (adding, subtracting, reorienting edges) in order to (3) (approximately) maximize the number of aspects of the new data dt for which
their model can account (Figure 1). Iterating this procedure leads to reasonable, though sub-optimal, causal structure
judgments without either representing more than one causal
model or remembering old evidence.
We parametrized a whole class of such models via two con-

Experimental rationale
To explore these approximations, we considered sequential
structure learning in appropriately difficult problems. If subjects really maintain only a single causal belief and make local edits, we expect sequential dependence, and a tendency to
get stuck in local optima. If they forget old evidence, relying
on the current structure itself, we expect to observe recency
effects whereby participants may return to judgments previously rejected. Finally, if they rely on generic priors we expect to see better performance when the true causal structure
is conformant.
We therefore designed two online studies based on the
paradigm used in Bramley, Lagnado and Speekenbrink
(2014) (demo at ucl.ac.uk/lagnado-lab/el/ns15a). Participants

263

interacted with a series of probabilistic causal systems involving 3-4 variables, repeatedly selecting interventions (or tests)
to perform in which any number of the variables were either
fixed on or off, while the remainder were left free to vary.
The tests people chose, along with the true underlying causal
model, S and B, jointly determined the data they saw. We systematically varied the number of connections between components in the problem set, along with S and B.

Participants were incentivized to report their best guess about
the structure, through receipt of a 10¢ bonus for each causal
relation (or non-relation) correctly registered at randomly selected time points throughout the task.
Participants completed instructions familiarizing them
with the task interface; the interpretation of arrows as (probabilistic) causal connections; the incentives for judgment accuracy; and the level of S and B in their condition. To train
participants on S and B, they were shown first 10 unconnected
components and forced to test them 5 times. The frequency
with which the components activated reflected the true background noise level. Then, they were shown a set of twocomponent causal systems where component “A” was a cause
of “B”, and were forced to test these systems 5 times by fixing component “A” on. This indicated that the frequency with
which “B” activated reflected the level of S combined with
the background noise they had already learned (e.g. 76% of
the time in condition 9).
After completing the instructions and correctly answering
comprehension checks, participants solved a practice problem drawn from the five three-variable problems. They then
faced the 10 test problems in random order, with randomly
orientated unlabeled components. They were given six tests
per three variable problem and eight tests per four variable
problem. After the final test for each problem they received
feedback telling them the true connections.

Experiment 1
In Experiment 1, we restricted ourselves to the effects of “expected” uncertainty (Yu & Dayan, 2003) by training subjects
explicitly on the true prevailing values of B and S.

Methods
Participants We recruited 150 participants (85 male,
mean±SD age 35 ± 10 from MTurk, split randomly between
9 conditions (group size 16.7 ± 3.4). They were paid $1.50
and received a bonus of 10c per correctly identified connection on a randomly chosen test for each problem (max=
$6.00, mean±SD $3.68 ± .75). The task took an average of
41 ± 20 minutes.
Design We included five 3-variable and five 4-variable
problems (see Figure 2). Within these, we varied the sparseness of the causal connections, ranging between a single connection (devices 1; 6) to fully connected structures (5; 10).
We included problems exemplifying three key types of causal
structure: forks (diverging connections), chains (sequential
connections) and colliders (converging connections).
There were three different levels of causal strength S ∈
[1, .85, .6] and three different levels of background noise B ∈
[0, .15, .4] making 3 × 3 = 9 between-subjects conditions. For
instance, in condition 1 (S = 1; B = 0) the causal systems were
perfectly deterministic, with nothing activating without being
intervened on, or caused by, an active parent, and connections
never failing to cause their effects. Meanwhile, in condition 9,
(S = 0.6; B = 0.4) the outcomes were very noisy, with probability .4 that a variable with no active parents would activate,
compared to a probability 1 − (1 − .6)(1 − .4) = 0.76 for a
variable with one active parent.

Results
Performance by condition We expected the quality of participants’ judgments to be bracketed by those of a random ( 31
per link, given the three possibilities) and a Bayes-optimal
observer. For the latter, we calculated the posterior distributions over the task using Bayesian integration based on the
outcomes the participants actually observed, calculating the
likelihoods using the true causal strength S and background
noise B, assuming a uniform prior at the start of each problem.
By reporting the MAP structure (guessing in the event of ties)
participants could have achieved accuracies ranging between
.84 ± 0.14 in condition 2 and .55 ± 0.09) in the nosiest condition, 9 (see Figure 3, blue circles). Optimal learning predicts
differences by condition, with a considerable reduction in accuracy going from no to high background noise, and a more
moderate reduction going from perfectly strong to highly unreliable causal connections.
Participants significantly outperformed chance in all nine
conditions (all p values < .05 for t-tests comparing to 13 ).
However they underperformed the Bayes-optimal observer
(t-test p values < .05) in all conditions bar condition 2 S =
0.85, B = 0 (p=0.07). Like the optimal observer, participants
became less accurate as noise increased, with a main effect
of Background noise F(2, 147) = 6.34, η2 = 0.07, p = 0.002
with lower performances for B = 0.1,t(147) = −2.23, p =
0.03 and B = 0.4,t(147) = −3.5p < .001 compared to B = 0,
but no main effect of Strength F(2, 147) = 1.2, p = 0.3.
Participants marked more causal connections per problem
than the optimal learner, mean±SD estimates 2.93±1.4 com-

Procedure The causal systems were represented as grey
circles on a white background. Participants were told that the
circles were components of a causal system of binary variables, but were not given any further cover story. Initially, all
components were inactive and no connection was marked between them. Participants performed tests by clicking on the
components, setting them at one of three states “fixed on”,
“fixed off” and “free-to-vary”, then clicking “test” and observing what happened to the “free to vary” components as a
result. The observations were of temporary activity (graphically activated components would turn green and wobble).
After each test, participants registered their best guess about
the underlying structure. They did this by clicking between
the components to select either no connection, or a clockwise
or anti-clockwise connection, (represented as black arrows).

264

a) Problem set experiment 1

pared to 2.75 ± 1.4, t(2998) = 3.5, p = 0.0005. The true
proportion was 2.6. The number of connections participants
marked on average was affected by both B and S, going from
2.78 ± 1.5 for B = 0 to 3.14 ± 1.4 for B = 0.4, and 2.77
(SD=1.4) for S = 0.6 to 3.01 (SD=1.4) for S = 1.

1.

2.

A

C

B

C

B

4.

A

C

7.

6.

Performance by problem Average accuracy on three variable problems was fractionally higher than on four variable problems .55 ± 0.34 compared to .52 ± 0.29, t(1463) =
2.0, p = 0.04, and tests were completed marginally quicker
with medians 12.3s and 14.6s. Due to the unrestricted timing of the study, test times were highly positively skewed.
Therefore, we tested for a difference between medians by
permutation test (Higgins, 2004), finding it significant p <
.0001. However, there was no main effect of the number of
connections on judgement accuracy F(1, 1498) − 2.1, η2 =
0.001, p = 0.14.
There was a significant main effect of device type
F(5, 1444) = 2.91, η2 = 0.007, p = 0.02 (see Figure 2). Accuracy was lowest for chains (devices 3; 8) 0.49 ± 0.28, and
highest for colliders 0.57 ± 0.30 (4; 9). Taking the chain
as treatment group, the main effect of device was driven by
higher accuracy on colliders (4; 9) t(1497) = 3.2; p = 0.001,
and marginally higher performance on singly- (1; 6) and
fully-connected (5; 10) structures.

3.

A

B

C

8.

5.

A

A

B

9.

C

B

10.

A

B

A

B

A

B

A

B

A

B

D

C

D

C

D

C

D

C

D

C

b) Participants’ averaged final judgments Experiment 1
1.

2.

A

C

B

A

D

3.

A

C

B

B

C

B

A

B

A

C

D

C

D

7.

6.

4.

A

C

B

B

A

C

D

8.

0.0

5.

A

C

B

B

A

B

C

D

C

9.

0.2

0.4

0.6

A

10.

0.8

1.0

Proportion selected

c) Averaged posteriors Experiment 1
1.

C

2.

A

B

3.

A

C

B

C

7.

6.

4.

A

B

C

8.

5.

A

B

9.

C

A

B

10.

A

B

A

B

A

B

A

B

A

B

D

C

D

C

D

C

D

C

D

C

0.0

0.2

0.4

0.6

0.8

1.0

Average posterior probability

Changing judgements Comparing participants’ sequences
of structure judgments indicates that they shift markedly less
frequently than the optimal observer, changing an average of
0.94 ± 1.3 connections after each test compared with 1.78 ±
1.5, χ2 (6) = 1920, p < .0001 (see Figure 3b)

Figure 2: a) The problems faced by participants. b) Weighted
average final judgments by participants. Darker arrows indicate
that a larger proportion of participants marked this link in their final model. c) Bayes-optimal marginal probability of each edge in
P(M|d1:T , S, B), averaged over participants.

Discussion

with ties broken randomly. This matches more closely the
subjects’ performances per condition, and also their patterns
of sequential judgment edits.

Participants identified causal connections above chance even
in the most complex and noisy situations we tested. Nevertheless, they were systematically less accurate than they
could have been. This is hardly surprising given the considerable complexity of the inferences, and invites comparison
with the heuristics discussed earlier. That response times do
not increase greatly going from three- to four-variable problems argues against explicit Bayesian-like calculations, as
these grow at least O(2N ) with increasing number of variables N. Nevertheless, that the ensemble behavior across all
participants resembles the (averaged) posteriors (Figure 2)
is in line with the idea that individuals’ judgments can be
plausibly thought of as individual particles. The strong sequential dependence in judgments argues firmly against their
representing the whole distribution. Finally, systematic overconnecting, especially for high B, fits with subjects’ failing
to compute the exact likelihoods even when they know the
parameters, but rather relying on more generic or heuristic
approximations.
As a hint that the heuristic models discussed above might
therefore offer a better model of the subjects’ behavior, the
green dots in figure 3 show the case of EA with θ1 = θ2 → ∞
(so that the MAP structure is chosen at each iteration), and

Modeling
To test the models more formally, we fit the likelihoods of
the various combinations, as in the example of equation 5, to
the judgments bt=1:T of all participants, for all problems. We
expect the resulting θ parameters to be such that lower dissimilarities and fewer explanatory inadequacies lead to more
probable selection. Judgments at t = 0 were assumed to be
an unconnected causal model, but starting evaluation at t = 1,
when a judgment was already in place, produces comparable
results.
We also considered two baseline models.
One is
a a parameter-free model that assumes each judgment
is a random draw from all possible causal models
p(bt = m) = Unif (M) (leading to a probability 31 for
each link). The other model is a variant of the Bayes-optimal
model that allows decision noise to corrupt choices from the
true posterior at t, P(M|D, S, B)t . For this, we considered
exp(P(M|D, S, B)t θ1 )
(6)
P(bt |D) =
∑m∈M exp(P(m|D, S, B)t θ1 )
controlled again by an inverse temperature parameter θ1 .
Separately, we estimated maximum likelihood S∗ and B∗

265

0.8

0.0

●

1

2

3

0.4

0

●

0.0

Participant Final Accuracy Experiment 1

●

●

●

0

1

0.0

●

●

●
2

●

●
3

●

●

●
●

●

0.6

Accuracy

Condition

0.6

●

0

0.15

0.4

0.4
B*

Four component structures

Neurath's ship
Chance●
●

●

●

0.0

●

●

0.85

0 Edit1distance
2 between
3 consecutive
4
5 judgements
6
1
2
3
Edit distance
Edit distance
Benchmark
Edit
distance
between
consecutive
judgements
Edit
distance between consecutive judgements
● Bayesian
●
0.8

0.8

Condition

●

S=1
S=.6
●
B=.4●
●

1

Actual

Four● component structures
●

0.6

1.0

0.0

S=1
S=.6
B=.15

0.4

0.4S*

0.8

●

0

S=1
S=.6
B=0

0.15

0.6

0.6
●

0

Best fitting S and B

0.8

●

c)
0.8

0.8
●

●

Proportion

1.0
0.4

●

0.4

●

0.2

●

●

Three component structures

0.0

●

●

0.4

0.8

●

Proportion

●

0.2

●

0.0

●
●

Proportion
Proportion
0.2 0.4 0.6

●

●

●
●

0.6

0.8

●

Four component structures
4-component problems

Three
componentproblems
structures
3-component

●

0.2

Accuracy

b)

Final accuracy Exp 1

●

Proportion
Proportion

a)

Accuracy

0.0

Edit distance between consecutive judgements

Participant Final Accuracy Experiment 1

B=.15

0.8

0.4

0.0

0.4

0.2
0.0

0.6
0.4

Proportion

B=0

B=.4
●

0.2

0.0

0.2

0.8

0.4

Figure 3: a) Mean final accuracy with standard errors. White circle: benchmark (greedy expected information gain maximizing) Bayesian
●
● interventions.
●
●
learner. Blue circles: Bayesian learner that maximizes over the posterior after seeing participants’
Green triangles: “Neurath’s
●
●
ship” simulation simply minimizing number of edits E and failures to explain A. Red squares: random ●guessing. b) Bars show average
0
1
2
3
4
6
number of edits (additions, subtractions or reversals of connections) between all t and t+1 judgments,
as5 compared
toActual
Bayesian, “Neurath’s
Edit distance between consecutive judgements
ship” and random choice simulations.
Boxplot
of best
S∗ and B∗ parameters assuming
learners soft-maximised over P(M|D, S∗ , B∗ ).
S=1 c)S=.6
S=1
S=.6
S=1 fitting
S=.6
●

●

●

1

●

0.0

Condition
parameters for participants assuming Equation
6.

●

●

0.85

0.6

Experiment 2

1
2 ef3
4
5
6
Fitting the models Altogether we fit 9 different0 (fixed
The fact that many participants are well captured by the
Edit distance between
consecutive judgements
fects) models separately to each of the 150 participants.
Modmodel that relies on heuristic likelihoods suggests that peoels were fit using maximum likelihood as implemented by R’s
ple will still be able to learn causal models well even if they
optim function, and compared using their BIC scores to acdo not know S and B parameters explicitly. We therefore
commodate their different numbers of parameters. Results
designed a second experiment (demo at ucl.ac.uk/lagnadoare detailed in Table 1.
lab/el/ns15b) to test this effect. Furthermore, by asking sub-

jects to re-register every link after every new test, we fixed
a potential shortcoming of Experiment 1, in which the inertia in judgments might have arisen from subjects’ response
laziness (i.e., not being bothered to change links) rather than
inferential heuristics.

Table 1: Experiment 1 - Models fitted to individuals’ judgments by
maximum likelihood. McFadden’s pseudo-R2 is reported, alongside
BIC, median soft-maximization weighting parameter estimates θs.
N best fit according to BIC, and average final judgment accuracy for
those best fit.
Model
Baseline
P(M|D, S, B)
L
A
E∗
E
E∗ L
E∗ A
EL
EA

BIC
104535
91629
97532
98152
80406
58892
73047
74202
51146
51665

Rsq
0
0.13
0.07
0.07
0.24
0.44
0.31
0.3
0.52
0.52

θ1

θ2

8
2.9
-1
-4.3
-2.2
-4.6
-4.4
-2.4
-2.4

3
-1.1
4
-1.3

N fit
0
0
0
1
1
35
1
1
60
51

Accuracy Participants

111 UCL undergraduates (mean±SD age
18.7 ± 0.9, 22 male) took part in Experiment 2 as part of a
course. They were incentivized as previously, but this time
with the opportunity to win Amazon vouchers rather than
money directly. They were split randomly into 8 conditions
mean size 13.8 ± 3.4.

.33
.33
.33
.49
.31
.63
.56

Design and procedure Experiment 2 used the same task interface as Experiment 1, but focused just on the three variable
problems. There were two background noise conditions B ∈
[.1, .25] and two causal strength conditions S ∈ [.9, .75]. However, unlike in Experiment 1, participants were not trained on
these parameters, but only told that: “the connections do not
always work”, and “sometimes components can activate by
chance”.
To assess the influence of laziness, we examined two reporting conditions between subjects: remain and disappear.
In the remain condition, judgments stayed on the screen into
the next test, so participants did not have to change anything if
they wanted to register the same judgement at t as at t − 1. In
the disappear condition, the previous judgment disappeared
as soon as participants entered a new test. They then had
explicitly to select what they wanted for every connection
after each test.2 At the end of the task, people were asked
to estimate, in 100 tries how often: “components turn on by
themselves?” (B) and “how often do the causal connections

Model fit results and discussion
These results show that the large majority of participants are
best described by variants of the structural change model
of causal judgment that simply balances judgment inertia
against a desire to accommodate the latest evidence. Participants were split fairly evenly between being better captured
by the true likelihoods L compared to the simple explanatory
inadequacy A proxy. Furthermore, estimated S∗ and B∗ values were less variable over conditions than the true values
and stronger and sparser on average (Figure 3c), in line with
the idea that participants relied on simplifying assumptions
over trained likelihoods. No participant was best described
by soft-maximising over the Bayesian posterior. Participants
with average accuracy levels at chance were predominantly
best captured by the E only model, indicating that their judgments were sequentially dependent but did not meaningfully
reflect the data. The better fit for models using edit distance
E rather than E ∗ suggests that participants do not just stick
with the same model, but rather tend to make local, rather
than drastic, changes.

2 We

also elicited additional judgments about expected outcomes
of interventions, confidence in individual connections and ’helpfulness’ of each outcome; however we do not report on these here for
space reasons.

266

work?”(S).

Second, while participants’ judgments showed high sequential dependence, they did occasionally change their
model abruptly. The theory of unexpected uncertainty (Yu
& Dayan, 2003), and substantial work on changepoint tasks
(Speekenbrink & Shanks, 2010) are associated with the notion that people will sometimes “start over” if they are having consistently poor predictions from their existing model
(Lakatos, 1976). Experiments in which the underlying structure changes over time would provide pointers.
Finally, we did not examine the selection of interventions,
but only how to learn from them. Participants’ interventions
were far from perfectly efficient – in 100 simulations of the
task, an active learning algorithm that selects interventions
greedily to minimize its expected uncertainty over the space
of possible structures, and updates beliefs optimally, achieves
considerably higher final accuracy (mean 0.81, see white circles in Figure 3) compared with what could be achieved given
the data participants actually saw (mean 0.69). This also
raises further important questions.

Results and modeling
Performance in Experiment 2 was comparable to the 3variable problems in Experiment 1. For example, mean±SD
accuracy in Experiment 2, [B = 0.1, S = 0.75] was .63 ± 0.27
and [B = 0.25, S = 0.75] was .58 ± 0.31 while Experiment 1
condition 5 [B = 0.15, S = 0.85] was .60±0.33. This suggests
that people can make reasonable structure judgments without knowledge of exact parameters. Supporting these conclusions – participants’ final judgments of S and B suffered
bias and variance: for B = {.1; .25} the mean±SD estimates
were {.37 ± .24; .48 ± .20} respectively; for S = {.9, .75},
mean±SD estimates were {.75 ± .21; .64 ± .23}.
As with Experiment 1, participants were affected by higher
levels of background noise B t(108) = 2.7, p = 0.008, but not
the reliability of the links themselves S t(106) = 0.88, p =
0.37, and there was no difference in performance between
the two judgment elicitation conditions t(108) = 0.67, p =
0.50. Analysis of variance revealed an effect of condition
on final judgment accuracy F(7, 103) = 2.87, η2 = 0.16, p =
0.008 with a significant interaction between S and judgment
type, with a .21 additional drop in accuracy going from S=0.9
to S=0.75 in the disappear condition compared to the remain
condition.
To check if the structure change model in Experiment 1
was driven by lazy reporting, we fit the models as before3 .
We found that once again the large majority of participants
were fit by variants of the structural change model, both when
judgments remain (47/53) and when they disappear (48/58),
this time with a larger proportion better fit by EL than EA
(32/47 for remain and 32/48 for disappear conditions), suggesting some sensitivity to the noisy-or aspect of the likelihoods at least for three variable problems. 5/53 and 10/53 in
the remain and disappear conditions respectively were best fit
by the model based on the Bayesian posterior P(D|M).

Acknowledgements PD was supported by the Gatsby
Charitable Foundation.

References
Bramley, N. R., Gerstenberg, T., & Lagnado, D. A. (2014). The order of things: Inferring causal structure from temporal patterns. In P. Bello, M. Guarini, M. McShane,
& B. Scassellati (Eds.), Proceedings of the 34th annual conference of the cognitive
science society (p. 236-242). Austin, TX: Cognitive Science Society.
Bramley, N. R., Lagnado, D. A., & Speekenbrink, M. (2014). Forgetful conservative scholars - how people learn causal structure through interventions. Journal of
Experimental Psychology: Learning, Memory and Cognition.
Buchanan, D. W., Tenenbaum, J. B., & Sobel, D. M. (2010). Edge replacement and
nonindependence in causation. In Proceedings of the 32nd annual conference of the
cognitive science society (pp. 919–924).
Cheng, P. W. (1997). From covariation to causation: A causal power theory. Psychological Review, 104, 367-405.
Coenen, A., Rehder, B., & Gureckis, T. (2014). Decisions to intervene on causal systems
are adaptively selected. In P. Bello, M. Guarini, M. McShane, & B. Scassellati
(Eds.), Proceedings of the 34th annual conference of the cognitive science society.
Austin, TX: Cognitive Science Society.
Cooper, G. F., & Herskovits, E. (1992). A Bayesian method for the induction of probabilistic networks from data. Machine learning, 9(4), 309–347.
Courville, A. C., & Daw, N. D. (2007). The rat as particle filter. In Advances in neural
information processing systems (pp. 369–376).
Fernbach, P. M., & Sloman, S. A. (2009). Causal learning with local computations.
Journal of experimental psychology: Learning, memory, and cognition, 35(3), 678.
Higgins, J. J. (2004). An introduction to modern nonparametric statistics. Brooks/Cole
Pacific Grove, CA.
Lagnado, D. A., & Sloman, S. (2004). The advantage of timely intervention. Journal
of Experimental Psychology: Learning, Memory & Cognition, 30, 856–876.
Lagnado, D. A., & Sloman, S. A. (2006). Time as a guide to cause. Journal of experimental psychology. Learning, memory, and cognition, 32(3), 451–60.
Lakatos, I. (1976). Falsification and the methodology of scientific research programmes.
In Can theories be refuted? (pp. 205–259). Springer.
Liu, J. S., & Chen, R. (1998). Sequential monte carlo methods for dynamic systems.
Journal of the American statistical association, 93(443), 1032–1044.
Lu, H., Yuille, A. L., Liljeholm, M., Cheng, P. W., & Holyoak, K. J. (2008). Bayesian
generic priors for causal learning. Psychological review, 115(4), 955.
Mayrhofer, R., & Waldmann, M. R. (2011). Heuristics in covariation-based induction of
causal models: Sufficiency and necessity priors. In Proceedings of the 33rd annual
conference of the cognitive science society (pp. 3110–3115).
Pearl, J. (2000). Causality. New York: Cambridge University Press (2nd edition).
Quine, W. v. O. (1969). Word and object. MIT press.
Sanborn, A. N., Griffiths, T. L., & Navarro, D. J. (2010). Rational approximations to
rational models: alternative algorithms for category learning. Psychological review,
117(4), 1144.
Speekenbrink, M., & Shanks, D. R. (2010). Learning in a changing environment.
Journal of Experimental Psychology: General, 139(2), 266.
Steyvers, M. (2003, June). Inferring causal networks from observations and interventions. Cognitive Science, 27(3), 453–489.
Vul, E., Goodman, N. D., Griffiths, T. L., & Tenenbaum, J. B. (2009). One and done?
optimal decisions from very few samples. In Proceedings of the 31st annual conference of the cognitive science society (Vol. 1, pp. 66–72).
Yu, A., & Dayan, P. (2003). Expected and unexpected uncertainty: ACh and NE in the
neocortex. Advances in neural information processing systems, 173–180.

General Discussion
In sum, people were able to learn complex causal models,
but exhibited strong sequential dependence and variability
in their judgments. These patterns were well-captured by a
heuristic model, inspired by “Neurath’s ship”, that maintains
a single model, and attempts to account for incoming evidence by making local changes. However, we have not yet
provided a plausible process model for the local search.
The model is still too simple in at least three respects. First,
it assumes no memory of past evidence beyond the insufficient statistic of the current causal model. It is likely that
subjects can remember some past experience, and combine
it with the current datum when updating their beliefs. Of
course, outside the lab setting, it is unlikely that our experience relevant to single causal models is adequately contiguous for this to be very useful in practice.
3 For P(D|M) we used importance sampling with 20,000 particles
to marginalize over S and B, updating a density for each over the
course of the 36 trials in the task.

267

