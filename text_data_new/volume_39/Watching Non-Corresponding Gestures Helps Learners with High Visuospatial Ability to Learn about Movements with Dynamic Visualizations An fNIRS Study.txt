Watching Non-Corresponding Gestures Helps Learners with High Visuospatial
Ability to Learn about Movements with Dynamic Visualizations: An fNIRS Study
Birgit Brucker (b.brucker@iwm-tuebingen.de) a
Björn de Koning (b.b.dekoning@fsw.eur.nl) b
Ann-Christine Ehlis (ann-christine.ehlis@med.uni-tuebingen.de) c
David Rosenbaum (david.rosenbaum@med.uni-tuebingen.de) c
Peter Gerjets (p.gerjets@iwm-tuebingen.de) a
a

Leibniz-Institut für Wissensmedien, Schleichstrasse 6, 72076 Tuebingen, Germany
Erasmus University Rotterdam, Burgemeester Oudlaan 50, 3062 PA Rotterdam, The Netherlands
c
Department of Psychiatry & Psychotherapy, University Hospital Tuebingen, Calwerstr. 14, 72076 Tuebingen, Germany
b

Abstract

with the hands (Marcus et al., 2013) or learning to classify
fish movements (Brucker et al., 2015). However, so far (1)
there is only a handful of studies investigating the
instructional potential of dynamic visualizations addressing
biological movement and most of them focus on handmanipulative tasks, and (2) it is yet unexplored to what
extent learning about biological movements from dynamic
visualizations can be enhanced by additional instructional
support. These aspects provided the basis for the present
study wherein we investigated the value of observing and
making gestures for learning to classify fish movement
patterns from dynamic visualizations.

This study investigates whether making and observing
(human) gestures facilitates learning about non-human
biological movements and whether correspondence between
gesture and to-be-learned movement is superior to noncorrespondence. Functional near-infrared spectroscopy was
used to address whether gestures activate the human mirrorneuron system (hMNS) and whether this activation mediates
the facilitation of learning. During learning, participants
viewed the animations of the to-be-learned movements twice.
Depending on the condition, the second viewing was
supplemented with either a self-gesturing instruction (Y/N)
and/or a gesture video (corresponding/non-corresponding/no).
Results showed that high-visuospatial-ability learners showed
better learning outcomes with non-corresponding gestures,
whereas those gestures were detrimental for low-visuospatialability learners. Furthermore, the activation of the inferiorparietal cortex (part of the hMNS) tended to predict better
learning outcomes. Unexpectedly, making gestures did not
influence learning, but cortical activation differed for learners
who self-gestured depending on which gesture they observed.
Results and implications are discussed.
Keywords:
Learning
about
movements;
visualizations; human mirror-neuron system;
functional near-infrared spectroscopy.

Gestures and Learning
It is by now relatively well-established that making and
observing gestures is beneficial for acquiring knowledge
about different scientific topics and spatial problem solving
(e.g., Chu & Kita, 2011; Cook & Goldin-Meadow, 2006). In
learning about movements from dynamic visualizations,
there is also increasing evidence that showing hands in
manual tasks (e.g. origami folding, Marcus et al., 2013) or
observing gestures in addition to the learning material
improves learning outcomes (Brucker et al., 2015; De
Koning & Tabbers, 2013). It is assumed that this is due to
the activation of brain regions (i.e., the human mirrorneuron system [hMNS]; Fogassi & Ferrari, 2011; Rizzolatti
& Craighero, 2004) involved in the observation,
understanding and imitation of other persons’ actions. This
is in line with the current hypothesis that the stimulation and
involvement of this hMNS might be beneficial for learning
about complex continuous aspects with dynamic
visualizations (Ayres et al., 2009; Van Gog et al., 2009).
Initial evidence for this comes from a study by Brucker
et al. (2015) wherein low- and high-visuospatial-ability
learners had to learn fish movement patterns from dynamic
visualizations whilst observing additional gestures that did
or did not correspond to the depicted movements. Results
showed better learning outcomes and higher cortical

dynamic
gestures;

Learning from Dynamic Visualizations
In recent years, dynamic visualizations such as animations
and videos have become a popular instructional tool to
visualize processes and phenomena that are dynamic in
nature (e.g., cardiovascular system, lightning formation, fish
movements). Obviously, dynamic visualizations are wellsuited for this purpose given that they explicitly depict
visuospatial information over time. Nevertheless, research
thus far indicates that dynamic visualizations are often not
superior to learning from static visualizations (e.g., CastroAlonso et al., 2016; Mayer et al., 2005). It appears that
dynamic visualizations are particularly effective for learning
about movements when biological movement is involved
(Hoffler & Leutner, 2007) like when learning to tie knots

168

activation in the inferior-frontal cortex (part of the hMNS)
for low-visuospatial-ability learners after watching gestures
that corresponded to the to-be-learned fish movements
compared to watching non-corresponding gestures. Highvisuospatial-ability learners achieved high learning
outcomes with both gestures. Unexpectedly, lowvisuospatial-ability learners who watched the noncorresponding gestures could also achieve high learning
outcomes if they activated their inferior-parietal cortex (also
part of the hMNS). These findings provide the first
indication that the hMNS is also involved in representing
non-human biological or even non-biological movements, if
the observer is able to anthropomorphize these movements
(cf. De Koning & Tabbers, 2011). So, drawing on the
hMNS by showing learners gestures associated with the
learning content seems an effective instructional strategy to
improve learning about biological movements from
dynamic visualizations.
Based on the notion that learner-generated gestures, as
compared to just observing other’s gestures, have a more
direct and stronger influence on the degree to which the
hMNS is activated (e.g., Montgomery, Isenberg, & Haxby,
2007), asking learners to make gestures related to the
movements depicted in a dynamic visualization themselves
may be a way to further enhance learning (cf. De Koning &
Tabbers, 2011). Additional advantages of self-performed
gestures relate to the manner (e.g., speed, amplitude) in
which the gestures are made and the possibility to draw on
one’s personal experiences (with fish movement) in order to
perform the gestures. By embodying the learning content in
one’s sensory and motor systems based on physical
movements (i.e., gestures), the information is coded in a
distinct, visuospatial representational format that enriches
the way the information is represented, thereby creating a
higher-quality mental representation (Paas & Sweller,
2012). Higher-quality mental representations are associated
with better learning (Goldin-Meadow et al., 2001), yielding
faster and more accurate performance on learning tests. It is
important to note that these anticipated benefits only arise as
long as the act of making gestures is not too demanding,
complex or distracting (De Koning & Tabbers, 2013;
Skulmowski et al., 2014). Together, by focusing on selfperformed gestures whilst learning about biological
movements from dynamic visualizations, we move into a
promising but yet unexplored field of research (for an
exception see De Koning & Tabbers, 2013).

instructions and visualization formats. Higher visuospatial
ability may compensate for “poor” instructions (i.e., in our
case unrelated non-corresponding gestures, cf. Methods
section), whereas learners with lower visuospatial ability
suffer from such instructions (cf. ability-as-compensator
hypothesis; Höffler, 2010). For example, relating this to the
Brucker et al. (2015) study, high-visuospatial-ability
learners likely possess the skills and resources to see when
gestures are in conflict with the depicted content and come
up with an own strategy to elaborate on the relevant
movements, whereas low-visuospatial-ability learners do
not possess these skills and therefore are less able to deal
with situations where gestures are in conflict with the
dynamic visualizations resulting in lower learning
outcomes. Thus, taking into account learners’ visuospatial
ability is relevant when studying the value of gestures in
learning about movements from dynamic visualizations.

Present Study
This study addresses the question to what extent learning
about biological movements from dynamic visualizations
can be enhanced by adding information in the form of
gestures. We implemented gesture-information in two ways:
By making gestures of the learners themselves and by
observing gestures displayed on a video. We investigated
making gestures (by the learner) by contrasting (1) studying
the dynamic visualizations whilst making gestures to (2)
studying the visualizations without making gestures.
Moreover, we examined observing gestures (that do or do
not correspond to the depicted non-human biological
movements) by contrasting studying the dynamic
visualization whilst (1) observing corresponding versus (2)
observing non-corresponding versus (3) not observing
additional gestures. Furthermore, functional near-infrared
spectroscopy (fNIRS), which is a non-intrusive
neurophysiological method to gather data about cortical
activation of humans, is used to investigate whether the
hMNS is activated during viewing gestures and learning
about biological movements from dynamic visualizations.
We hypothesize that studying the dynamic visualization
with additionally making gestures yields higher learning
outcomes than studying without making gestures.
Additionally, we hypothesize that studying the dynamic
visualizations with additionally observing gestures yields
higher learning outcomes than studying without observing
gestures. In accordance with Brucker et al. (2015), this
pattern is expected to vary as a function of level of gesture
correspondence and learner’s visuospatial ability: lowvisuospatial-ability learners are expected to show higher
learning outcomes only on corresponding gestures, whereas
high-visuospatial-ability learners are expected to show
improved learning outcomes for corresponding and noncorresponding gestures. Furthermore, we hypothesize that
the hMNS is more strongly activated with self-performed
gestures than with observed gestures, which in turn is more
strongly activated than studying without gestures.
Moreover, we hypothesize that higher hMNS activation is

Visuospatial Ability, Gestures, and Learning
As processing continuous changes requires visuospatial
ability (cf. Hegarty, 1992), it is likely that learners’
visuospatial ability will determine how much they benefit
from dynamic visualizations and additional gestures (cf.
Hegarty & Waller, 2005). According to previous research
(e.g., Höffler, 2010) learners with higher visuospatial ability
outperform learners with lower visuospatial ability during
learning with visualizations, and visuospatial ability may
moderate the effectiveness of learning with different

169

associated with higher learning outcomes. This is expected
to be particularly true for low-visuospatial learners.

dimensional space (i.e. different paddle-like or wave-like
movements). The four different movement patterns were: 1.
oscillation of the pectoral fins; 2. undulation of the body; 3.
undulation of the dorsal and anal fins; and 4. oscillation of
the dorsal and anal fins (and undulation of the pectoral fins).
During identifying these movement patterns it is very
challenging that fish may deploy other movements in
addition (e.g., to navigate) and these additional movements
can easily be mistaken for movements used for propulsion
in another movement pattern. We used the fish animations
and gesture videos from Brucker et al. (2015). The
movement cycles of the movement patterns were presented
in loops in the animations (30 s per movement pattern, 25
fps, size: 480 x 360 pixels). The gestures were presented in
the respective conditions in loops in the videos (30 s per
movement pattern, 25 frames per s, size: 480 x 360 pixels).
The presentation of all visualizations was system-controlled.

Methods
Participants and Design
One hundred and eighteen university students (M = 24.37
years, SD = 3.99; 84 females; 109 right handed) were
recruited via an online system (http://www.orsee.org/) and
compensated with 10 Euro. They had to learn to
discriminate different fish according to their movements
based on dynamic visualizations. There were four different
to-be-learned movement patterns of fish. The participants
saw each movement pattern twice: Firstly, they saw an
animation of the specific movement pattern. Secondly, they
saw the animation of the specific movement pattern again.
But this time depending on the experimental condition, the
animation could have been complemented with two
additional sources: either a written instruction to self-gesture
(making gestures) and/or a video of a person performing
gestures with his hands and arms (observing gestures).
Depending on this 2-by-3-between subjects design of the
study with the two independent factors making gesture and
observing gesture there were six conditions in total. Making
gesture was varied in two variants: Participants either did or
did not get the instruction “Please make your own gestures,
that help you to better understand the movement.”
Observing gesture was varied in three variants: Participants
either saw gestures that did correspond or that did not
correspond (i.e., were unrelated) to the fish movement
patterns or they saw no gesture at all (see Figure 1).
For the observing gestures conditions we used the
gestures from Brucker et al. (2015). For the corresponding
gestures, an expert regarding fish movements displayed with
his hands and arms representations of the respective
movements as clearly as possible, whereas for the noncorresponding gestures the (same) expert performed
gestures with his hands and arms that were unrelated to the
fish movement patterns (i.e., waving, circulating the
forearms around each other, drumming, and pointing.
Participants saw the animation of the first fish movement
for 30 s. Then a pause of 30 s (black screen) followed before
they saw the animation of the first fish movement with its
additions (depending on the experimental condition) for 30 s
again. Then again a pause of 30 s (black screen) followed
before the presentation of the next fish movement started in
the same manner. The learners were instructed to relax in
the pauses with the intention that the activations of the brain
areas of interest were supposed to return to baseline level
before the next visualization was displayed.

Figure 1: Six conditions in the 2-by-3-design of the study.

Measures
Learning Outcomes To assess learning outcomes, we
administered a movement pattern classification test
comprising 45 dynamic multiple-choice items. These items
consisted of underwater videos of real fish performing one
of the four to-be-learned movement patterns or a distractor
movement pattern. Learners had to identify the body parts
relevant for propulsion and their way of moving to choose
for each item the kind of movement pattern that was
depicted. Each item was visible for 7 s and immediately
afterwards participants had 3 s time to choose the correct
answer by pressing a corresponding button. Each item was
awarded one point for the correct answer (0 to max. 45
points). The test items were presented in blocks of 30 s so
that 3 items were grouped together. Pauses of 30 s (black
screen) followed each block.
Learners’ Visuospatial Ability To assess learners’
visuospatial ability we used a short version of the paper
folding test (PFT, Ekstrom et al., 1976; ten multiple-choice
items; total processing time: three minutes). In this task,
participants see five options from which they have to choose
the correct answer. The stimuli are depictions of papers that
are folded stepwise and then were punched in the folded
state. The answer options depict unfolded papers with
punches being either in the correct or incorrect positions.
Each correct answer is worth one point (max. 10 points).

Materials
Participants were asked to learn to classify four different
fish movement patterns. These fish movement patterns
differ in terms of the parts of the body that generate
propulsion (i.e., several fins or the body itself) and also in
the manner of how these body parts move in the three-

170

Cortical Activation During viewing the fish animation for
the second time in the learning phase, cortical activation
was assessed via fNIRS measurements with an ETG-4000
(Hitachi Medical Co.). We used a 2x22 channel array as
probe set that was placed over fronto-temporo-parietal
regions and was centered at the T3-T4 and C3-C4 positions
(not exactly terminating on these positions because of the
fixed interoptode distances) according to the standard
locations of the 10-20 system for electrode placement
(Jasper, 1958). The fNIRS system measures the change in
the product of hemoglobin (Hb) concentration and effective
optical path length in human brain tissue. The unit of Hb
change is molar concentration (mM = mmol/l) multiplied by
optical path length (mm). Local increases of Hb are
indicators of cortical activity (Obrig & Villringer, 2003).

gestures and learners’ visuospatial ability on learning
outcomes (F(2, 106) = 7.93, MSE = 119.63, p = .001, η2p =
.13; see means and standard errors in Figure 2). There were
no other significant interactions or three-way-interactions
(all ps > .35, ns). The significant interaction between
observing gestures and learners’ visuospatial ability on
learning outcomes showed that for participants with high
visuospatial ability (defined as one standard deviation above
the sample mean) the non-corresponding gesture led to
better learning outcomes than the corresponding gesture (p
= .001) and no gesture (p = .02). For participants with low
visuospatial ability (defined as one standard deviation below
the sample mean) non-corresponding gestures were worse
for learning than no gesture (p < .01), whereas there was no
significant difference between the corresponding gesture
condition and the no gesture condition (p = .23, ns). Thus,
the non-corresponding gestures are beneficial for high-, but
detrimental for low-visuospatial-ability learners.

Procedure
Participants were tested individually. After reading a printed
overview with information about the procedure of the study,
they had to answer the demographics and the PFT. Then, the
experimenter placed and adjusted the fNIRS probe set on
the scalp of the participants. Subsequently, the computerbased learning materials were presented (learning phase).
For each of the four to-be-learned movement patterns,
learners were presented with the two presentations of the
fish animations (1. fish animation and 2. fish animation plus
additional gesture video and/or self-gesturing instruction
depending on the experimental condition). Following the
learning phase (8 min) learners performed a filler task
(about 8 min), in which they answered some questions on
object positions of depicted objects. Subsequently, learners
completed the movement classification test (15 min).
Participants were instructed to put both their forefingers and
both their middle fingers on predefined keys as well as one
of their thumbs on the space bar to answer the test items.
The predefined keys were labeled on the screen with static
screenshots from the learning animations of the four
movement patterns and the spacebar was labeled with a grey
bar indicating movements that were not part of the learning
phase (i.e. distractor items). In total, one experimental
session lasted approximately 50 minutes.

Figure 2. Interaction between learners’ visuospatial ability
and observing gestures on learning outcomes.

Cortical Activation
To analyze the cortical activation, we defined two regions of
interest (ROIs) on the left hemisphere for the hMNS among
the respective channels (cf. Rizzolatti & Craighero, 2004).
The two ROIs were the left inferior-frontal cortex (IFC) and
the left inferior-parietal cortex (IPC, cf. Figure 3). Cortical
activation in these areas was analyzed with two ANCOVAs
with the factors making gestures, observing gestures, and
learners’ visuospatial ability as a covariate. We had to
exclude five participants from these analyses because of
poor data quality resulting in a total number of 113
participants in these analyses. Even though making gestures
did not influence results on learning outcomes, analyses on
cortical activation showed tendencies for an interaction
between making gestures and observing gestures for both
IFC activation (F(2, 100) = 2.94, MSE = .001, p = .06, η2p =
.06) and IPC activation (F(2, 100) = 2.42, MSE = .001, p =
.06, η2p = .05). There were no other significant main effects
or interactions in these analyses (all ps > .104, ns). Pairwise
comparisons revealed that participants observing
corresponding gestures showed higher IFC activation if they
self-gestured than when they did not self-gesture (p = .005).
However, participants observing non-corresponding
gestures showed higher IPC activation if they self-gestured
than when they did not self-gesture (p = .02). This might be
an indicator that during watching corresponding gestures the

Results
Learning Outcomes
To analyze learning outcomes, we conducted an ANCOVA
(univariate analysis of covariance) with the factors making
gesture, observing gesture, and the continuous factor
learners’ visuospatial ability as a covariate. We inserted all
interaction terms in the analysis to investigate the possible
interactions. For learning outcomes, results showed no main
effect of making gestures (F < 1, ns), no main effect of
observing gestures (F(2, 106) = 1.65, MSE = 119.63, p =
.20, η2p = .03, ns), but there was a significant main effect for
learners’ visuospatial ability (F(1, 106) = 11.58, MSE =
119.63, p = .001, η2p = .10). This effect has to be interpreted
in terms of the significant interaction between observing

171

IFC is more important, whereas during processing noncorresponding gestures the IPC becomes more important –
at least when the participants were instructed to self-gesture.

they properly understand the depicted movement. In
contrast, low-visuospatial-ability learners presumably are
insufficiently equipped for managing such a situation of
conflicting information (e.g., they do not have the resources
to identify the mismatch or do not know how to cope with
that), and are not able to accurately process the movements
and to avoid reduced performance.
In this study, IFC activation tended to predict better
learning outcomes. However, compared to the Brucker et al.
(2015) study, we did not find the result pattern that IPC
activation compensates for missing support of visuospatial
ability or non-conflicting gestures. This might be explained
by the fact that in the present study participants who neither
have visuospatial ability nor non-conflicting gestures at their
disposal (i.e. the group of low-visuospatial-ability learners
who saw non-corresponding gestures) still could focus on
the fish animation. This was possible because in this study
the gestures were presented at the same time as the fish,
whereas in our prior study the gestures were presented
separated in time from the fish animations. However, further
research should investigate direct comparisons of sequential
and simultaneous presentations of additional gestures.
Another interesting result of this study is that, in contrast
to our hypothesis, self-performed gestures did not improve
learning outcomes. In line with this, several recent attempts
to augment learning about non-human movement (e.g.,
lightning formation, grammar rules) by instructing learners
to make gestures while studying an animation also failed to
improve learning performance (e.g. De Koning & Tabbers,
2013; Post et al., 2013). Collectively, the conclusion from
this and other studies is that independent from timing of
gestures (during or after learning from dynamic
visualizations) and instructional approach (instruct specific
ways to perform gestures or let learners decide how to
perform gestures) making gestures does not seem to benefit
learning from dynamic visualizations involving non-human
movement. Importantly, however, making gestures did
activate the hMNS. Participants who were instructed to selfgesture activated different parts of the hMNS depending on
which gesture they simultaneously observed: with the
corresponding gestures there was higher IFC activation,
whereas with the non-corresponding gestures there was
higher IPC activation. This can be brought in line with our
previous findings (Brucker et al., 2015), in which we also
found evidence that the IFC plays a role during watching
corresponding gestures, whereas the IPC comes into the
picture when (conflicting) non-corresponding gestures have
to be processed. The IPC is associated with processes of
motion analysis and motor imagery, which may both be
helpful in the context of identifying the mismatch between
the to-be-learned movements and the non-corresponding
gestures. However, future research is needed to explore
these processes in more detail. Future research should also
address one limitation of this study – namely the lack of
insight into learners’ strategies – by replicating it with
think-aloud protocols so that it is possible to discover the
strategies learners use when observing and making (non-

Figure 3. Spatial arrangement of the left probe set.

Effects of Cortical Activation on Learning
To address the question whether higher hMNS activation is
directly associated with better learning outcomes, we
conducted two ANCOVAs with the factors making gestures,
observing gestures, learners’ visuospatial ability and
cortical activation in terms of IFC activation or IPC
activation, respectively. There was a tendency that higher
IFC activation lead to higher learning outcomes (F(1, 88) =
3.22, MSE = 124.85, p = .08, η2p = .04). This analysis on
IFC activation did also show the main effect for visuospatial
ability (F(1, 88) = 7.58, MSE = 124.85, p < .01, η2p = .08) as
well as the interaction between observing gesture and
visuospatial ability (F(2, 88) = 3.93, MSE = 124.85, p = .02,
η2p = .08; both effects reported for learning outcomes, see
Figure 2). For IFC activation there were no other significant
main effects or interactions (all ps > .27, ns). The analysis
on IPC activation did also show the main effect for
visuospatial ability (F(1, 88) = 7.18, MSE = 128.56, p < .01,
η2p = .08) and the interaction between observing gesture and
visuospatial ability (F(2, 88) = 5.18, MSE = 128.56, p < .01,
η2p = .11; both effects reported for learning outcomes, see
Figure 2). For IPC activation there were no other significant
main effects or interactions (all ps > .189, ns).

Discussion
This study investigated whether making and observing
additional gestures improves learning about biological
movements from dynamic visualizations and to what extent
this is related with the cortical activation in areas associated
with the hMNS. Regarding learning outcomes, our results
indicate that the observation of gestures has different effects
for high- and low-visuospatial-ability learners, particularly
when dealing with non-corresponding gestures. For highvisuospatial-ability learners, non-corresponding gestures
improved learning (even beyond corresponding gestures),
whereas for low-visuospatial-ability learners the observation
of non-corresponding gestures had detrimental effects on
learning. These findings are largely in line with those
reported by Brucker et al. (2015) and indicate that
particularly when high-visuospatial-ability learners are
challenged by a desirable difficulty (cf. Schüler, 2017), in
this case by creating a conflict between the visualized fish
movements and the (mismatching) gestures, they are
stimulated to put more effort in reducing the conflict and
come up with a strategy to more elaborately process the
relevant movements. This in turn increases the chance that

172

corresponding) gestures in learning from dynamic
visualizations. Furthermore, it is important to further
identify potential neural correlates of (gesture-supported)
learning with dynamic visualizations and to further unravel
the relations between activation in different parts of the
brain and learning outcomes. The present study provides a
starting point from which future research endeavors within
this emerging field of research can be explored with the goal
to incorporate (observing and making) gestures in a way that
learning about non-human movements from dynamic
visualizations is enhanced. In conclusion, this study shows
that observing additional gestures is helpful for learning
about movements, but learners need different types of
gestures depending on their amount of visuospatial ability.
Thus, different types of gestures should be applied: Highvisuospatial-ability learners should be challenged with noncorresponding gestures, whereas low-visuospatial-ability
learners might be supported with corresponding gestures.

Hegarty, M. (1992). Mental animation: Inferring motion
from static diagrams of mechanical systems. Journal of
Experimental Psychology: Learning, Memory and
Cognition, 18, 1084-1102.
Hegarty, M., & Waller, D. (2005). Individual differences in
spatial ability. In P. Shah, & A. Miyake (Eds.), Handbook
of Visuospatial Thinking. Cambridge University Press.
Höffler, T.N. (2010). Spatial ability: Its influence on
learning with visualizations—a meta-analytic review.
Educational Psychological Review, 22, 245-269.
Höffler, T.N., & Leutner, D. (2007). Instructional animation
versus static pictures: A meta-analysis. Learning and
Instruction, 17, 722-738.
Jasper, H. H. (1958). The ten-twenty electrode system of the
International Federation. Electroencephalography and
Clinical Neurophysiology, 10, 370-375.
Marcus, N., Cleary, B., Wong, A., & Ayres, P. (2013).
Should hand actions be observed when learning hand
motor skills from instructional animations? Computers in
Human Behavior, 29, 2172-2178.
Mayer, R. E., Hegarty, M., Mayer, S., & Campbell, J.
(2005). When static media promote active learning:
Annotated illustrations versus narrated animations in
multimedia instruction. Journal of Experimental
Psychology: Applied, 11, 256–265.
Montgomery, K.J., Isenberg, N., & Haxby, J.V. (2007).
Communicative hand gestures and object-directed hand
movements activated the mirror neuron system. Social
Cognitive and Affective Neuroscience, 2, 114–122.
Obrig, H., & Villringer, A. (2003). Beyond the visible –
Imaging the human brain with light. Journal of Cerebral
Blood Flow & Metabolism, 23, 1-18.
Paas, F., & Sweller, J. (2012). An evolutionary upgrade of
cognitive load theory: Using the human motor system and
collaboration to support the learning of complex cognitive
tasks. Educational Psychology Review, 24, 27-45.
Post, L.S., van Gog, T., Paas, F., & Zwaan, R. A. (2013).
Effects of simultaneously observing and making gestures
while studying grammar animations on cognitive load and
learning. Computers in Human Behavior, 29, 1450-1455.
Rizzolatti, G., & Craighero, L. (2004). The mirror-neuron
system. Annual Review of Neuroscience, 27, 169–192.
Schüler, A. (2017). Investigating gaze behavior during
processing of inconsistent text-picture information:
Evidence for text-picture integration. Learning and
Instruction, 49, 218-231.
Skulmowski, A., Bunge, A., Kaspar, K., & Pipa, G. (2014).
Forced-choice decision-making in modified trolley
dilemma situations: a virtual reality and eye tracking
study. Frontiers in Behavioral Neuroscience, 8, 426.
Van Gog, T., Paas, F., Marcus, N., Ayres, P., & Sweller, J.
(2009). The mirror-neuron system and observational
learning: Implications for the effectiveness of dynamic
visualizations. Educational Psychology Review, 21, 2130.

References
Ayres, P., Marcus, N., Chan, C., & Qian, N. (2009).
Learning hand manipulative tasks: When instructional
animations
are
superior
to
equivalent
static
representations. Computers in Human Behavior, 25, 348353.
Brucker, B., Ehlis, A.-C., Häußinger, F.B., Fallgatter, A.J.,
& Gerjets, P. (2015). Watching corresponding gestures
facilitates learning with animations by activating human
mirror-neurons: An fNIRS study. Learning and
Instruction, 36, 27-37.
Castro-Alonso, J.C., Ayres, P., & Paas, F. (2016).
Comparing apples and oranges? A critical look at research
on learning from statics versus animations. Computers &
Education, 102, 234-243.
Chu, M., & Kita, S. (2011).The Nature of Gestures’
Beneficial Role in Spatial Problem Solving. Journal of
Experimental Psychology: General, 140, 102-116.
Cook, S.M., & Goldin-Meadow, S. (2006). The role of
gesture in learning: Do children use their hands to change
their minds? Journal of Cognition and Development, 7,
211 - 232.
De Koning, B.B., & Tabbers, H.K. (2011). Facilitating
understanding of movements in dynamic visualizations:
An embodied perspective. Educational Psychology
Review, 23, 501-521.
De Koning, B.B., & Tabbers, H.K. (2013). Gestures in
instructional animations: a helping hand to understanding
non-human movements? Applied Cognitive Psychology,
27, 683-689.
Ekstrom, R., French, J., Harman, H., & Dermen, D. (1976).
Manual for Kit of Factor-Referenced Cognitive Tests.
Princeton: Educational Testing Service.
Fogassi, L., & Ferrari, P.F. (2011). Mirror systems. Wiley
Interdisciplinary Reviews: Cognitive Science, 2, 22-38.
Goldin-Meadow, S., Nusbaum, H., Kelly, S.D., & Wagner,
S. (2001). Explaining math: gesturing lightens the load.
Psychological Science, 12, 516–22.

173

