A Bayesian model of knowledge and metacognitive control:
Applications to opt-in tasks
Stephen T. Bennett (stbennet@uci.edu)
Department of Cognitive Science
University of California, Irvine

Aaron S. Benjamin (asbenjam@illinois.edu)
Department of Psychology and Beckman Institute for Advanced Science and Technology
University of Illinois at Urbana-Champaign

Mark Steyvers (mark.steyvers@uci.edu)
Department of Cognitive Science
University of California, Irvine
Abstract
In many ecologically situated cognitive tasks, participants engage in self-selection of the particular stimuli they choose to
evaluate or test themselves on. This contrasts with a traditional
experimental approach in which an experimenter has complete
control over the participant’s experience. Considering these
two situations jointly provides an opportunity to understand
why participants opt in to some stimuli or tasks but not to
others. We present here a Bayesian model of cognitive and
metacognitive processes that uses latent contextual knowledge
to model how learners use knowledge to make opt-in decisions.
We leverage the model to describe how performance on selfselected stimuli relates to performance on true experimental
tasks that deny learners the opportunity for self-selection. We
illustrate the utility of the approach with an application to a
general-knowledge answering task.
Keywords: metacognitive control; Bayesian cognitive model;
wisdom of the crowd; opt-in; missing not at random

Background
In traditional approaches to experimental psychology, an experimenter has unilateral control over which stimuli a participant experiences and the tasks that they complete. Yet
in many real-world situations, such as providing ratings to
videos on the Internet, the participant has some or even total
control over the specific stimuli and tasks that they experience. The choice behavior underlying such self-selection is
an important domain of study called metacognition (Nelson
& Narens, 1990), and the self-selection of activities or stimuli is specifically called metacognitive control (Fiechter, Benjamin, & Unsworth, 2016; Finley, Tullis, & Benjamin, 2010).
Some work on monitoring and control processes in memory tasks focused on confidence judgments as an indicator
of self-selection questions (Kelley & Sahakyan, 2003; Koriat & Goldsmith, 1996). It is unclear precisely how this
self-selection is generated, however. To better understand
metacognitive control behavior, a model is needed that accounts for performance on the task of interest as well as the
choice behavior that leads participants to select only some
stimuli for exposure, evaluation, or testing.
The major difficulty of such an endeavor is that participants
select tasks according to their interests and expertise, and so

the data is missing in a nonrandom fashion (see Little & Rubin, 2014, for a description of other missing data scenarios).
Consequently, participants can only be compared and their
performance fairly evaluated if a model is specified for the
opt-in process. If a participant does not opt in to a particular question, then we simply do not see that participant’s
response to that question.
A starting point in explaining opt-in behavior is that participants have some meta-knowledge of what it is they already
know, and use that knowledge effectively in service of ongoing learning. People provide higher assessments of their ability to answer inference questions in domains in which they
have greater expertise (Bradley, 1981), and learners often
choose to engage more effective study techniques for material
that is more difficult for them (A. S. Benjamin & Bird, 2006).
Memory reports are also considerably more accurate when
respondents have the option of withholding answers that they
are unsure of or of titrating the grain size of their answers to
their perceived accuracy (Goldsmith & Koriat, 2007).
Self-regulated learning often has substantial benefits in
educational contexts (Mezirow, 1981; Zimmerman, 1989;
Boekaerts & Minnaert, 1999; Paris & Paris, 2001). Learners use meta-knowledge to allocate time, resources, and activities to an array of learning goals, and this application increases overall performance compared to learners who have
their learning activities dictated by an instructor (Winne &
Hadwin, 1998; Finley et al., 2010).
The benefits of self-control extend beyond these constrained tasks, however. In causal reasoning experiments, participants can more quickly understand the causal structure of
a network if they intervene in the learning process and design their own “experiments” (Steyvers, Tenenbaum, Wagenmakers, & Blum, 2003; Sobel & Kushnir, 2006; Lagnado &
Sloman, 2004). Human strategy selection can be explained
in terms of rational metareasoning, wherein humans flexibly choose strategies in accordance with their environment
(Lieder & Griffiths, 2015; Lieder et al., 2014).
The core claim across each of these examples is that selfselection within a task aimed at measuring performance is

1623

spond to):

driven by metacognitive knowledge, which leads to a higher
rate of success, expertise, or interest for the selected items.
This process makes it difficult to evaluate the stimuli and
the participants in an unbiased way. One test-taker may, for
example, outperform another not because they have greater
knowledge but rather because they make more a judicious selection of problems.

p(θ|c, d, x, y) ∝ p(x|θ, c)p(y|θ, d)p(d|θ)p(θ)

Figure 1: Outline of our modeling approach. Latent knowledge and design both explain the performance on the task.
In the case of a subject-chosen design, latent knowledge also
explains the design.
The aim of this project is to develop a cognitive model
of the metacognitive aspect of item selection. In doing so,
it also provides a framework to relate performance on selfselected materials with performance on an unconstrained set
of items or stimuli. Here we apply this model to data collected from participants answering general knowledge questions, but the model is considerably more general: the same
principles could apply in other metacognitive control tasks,
such as study time allocation or selection of items for restudy.
We are aware of one current model of metacognitive control, which takes as a given the state of the world, which then
causes the observed behaviors (Fleming & Daw, 2017). We
take a different approach, which starts with the latent knowledge that the participant is coming to the experiment with
and uses that in both the selection process behind opting-in
and the observed responses to questions, as illustrated in Figure 1. Performance on the task is explained by both the design—that is, the particular experience of the participant in
the task—and the latent knowledge of the participant. In the
case of a participant who can opt in to certain questions but
avoid others, the design is also partially informed by the latent
knowledge. We are interested in estimating the latent knowledge of each participant and evaluating how it relates both
to performance on the task and to opt-in behavior. In order
to infer latent knowledge from the observed data, we apply
Bayes’ rule in the equation below, where θ is the latent knowledge, c is the experimenter design, d is the subject-chosen
design, x are the performance data from a true experimental
design (where subjects respond to all or to a random subject
of probes), and y are the performance data from a subjectchosen design (in which subjects choose which probes to re-

(1)

In a traditional cognitive model, the important part of the
model is the specification of p(x|θ, c) and p(y|θ, d), termed
the likelihood functions. These functions directly explain the
empirical effect of interest by relating latent knowledge to
performance on the task given the experimental design. The
novel part of the model relates to the specification of the
metacognitive control process p(d|θ), which explains how
the participant self-designs on the basis of their latent knowledge. If we would ignore this model component, we would
likely, and incorrectly, conclude that participants who selfdesigned were more knowledgeable than participants subject
to the experimenter’s design because they outperformed their
experimenter-designed counterparts. Such an error could be
catastrophic if we were trying to compare across individuals
or across tests. Because subjects are randomly assigned to
conditions, it is highly unlikely that they differ widely. The
process by which the participants who self-designed outperformed those who could not lies in the opportunity to selfdesign. Here we see the importance of jointly modeling the
selection process and the task at hand in order to understand
the interplay between latent knowledge, opt-in behavior, and
performance.
Since this is a task in which many participants give judgments to many questions, we also expect to find that averaging across participants leads to higher accuracy–an effect
termed the wisdom of the crowd (Surowiecki, 2004; Steyvers,
Miller, Hemmer, & Lee, 2009). Here we have the opportunity to evaluate whether the opportunity to opt in to a selfselected portion of the questions will enhance or attenuate
such benefits associated with averaging. Certainly, many participants will gravitate towards the same questions when they
can opt in, which would potentially decrease the benefits of
averaging across a crowd by virtue of reducing input to the
more difficult questions. However, based on what is known
about metacognition, we expect that participants will opt in
to questions for which they have relevant knowledge, which
could lead to a more informed set of responses to average with
the remaining crowd. Crowd behavior provides an additional
benchmark against which we can evaluate the performance of
the metacognitive model.

Experiment
Stimuli The question set consisted of 100 general-knowledge
binary choice questions. The questions were drawn from 12
topics: World Facts, World History, Sports, Earth Sciences,
Physical Sciences, Life Sciences, Psychology, Space & Universe, Math & Logic, Climate Change, Physical Geography,
and Vocabulary. The question set was created by collecting
from multiple sources. Two example questions are shown in
Table 1. Based on the empirically observed accuracy levels,
the first is difficult and the second is easy.
Participants A total of 83 participants were recruited
through Amazon Mechanical Turk (AMT). Each participant

1624

Table 1: Example questions.
Difficulty Example
Hard
The Sun and the planets in our Solar system all rotate in the same direction because: (a) they were all formed from
the same spinning nebular cloud, or (b) of the way the gravitational forces of the Sun and the planets interact
Easy
Greenhouse effect refers to: (a) gases in the atmosphere that trap heat, or (b) impact to the Earth’s ozone layer

was compensated $1 for the 30 minutes the experiment was
expected to take, and assigned to one condition.
Design Participants could view the survey description on
AMT. If they selected the survey they were redirected to another website. They were first directed to a study information sheet which provided details of the survey and compensation. If they agreed to continue, they were instructed to
answer some demographic questions. Participants were randomly assigned to either a random condition (N = 44) or a
self-selection condition (N = 39), determining the subject’s
role in selecting which questions to answer. Participants were
not aware of the existence of other conditions. Each participant saw the questions in 5 blocks of 20 questions each. In
each block, they were instructed to rate the difficulty of each
question and then, if they were assigned to the opt-in condition, instructed to choose 5 of those 20 questions to answer.
The participants in the random assignment condition were
randomly assigned 5 questions from that block to answer. After rating the difficulty of all 100 questions and answering 25
of them, participants were thanked for their time and given
instructions on how to receive payment.

For the self-selection condition, we assume that participants have a preference to select questions for which they
believe they have knowledge. Let c represent the observed
question selections with ci, j = 1 if question j was selected
by participant i. For each participant and question block, we
model question selection in the opt-in condition by a sampling process:
ci ∼ SampleWR((δi,1 + κ, ..., δi,K + κ), M)

where K is the total number of questions available for selection in each block (K=20 in our experiment), M is the number
of questions that need to be selected (M=5 in the experiment),
SampleWR(δ,M) represents a sampling without replacement
distribution where M items are sampled with probability proportional to δ, and κ is a fixed parameter that controls the
randomness in the selection process. Higher κ values make
it more likely that questions are selected for which the participant has no subjective knowledge. For participants in the
random condition, we assume that the questions are randomly
sampled by a process that is under control of the experimenter
(where M out of K questions are randomly allocated).
Let xi, j represent the observed accuracy for participant i on
question j. We do not assume a fixed relationship between
belief of knowledge and accuracy. For each question, we introduce guessing rate parameters ρ j and λ j that control the
probability of correct responding if the participant does or
does not have subjective knowledge about a question:

Model
The model utilizes an IRT model to generate subjective latent knowledge (the belief of a participant that she can answer a question), which informs all aspects of participants’
responses including the observed accuracy and difficulty ratings, as well as the metacognitive process of question selection. We describe participants as opting-in to questions for
which they believe they have knowledge, answering with accuracy dependent on whether or not they believe they have
knowledge, and giving lower difficulty ratings when they believe they have knowledge.
We use an IRT model to generate the subjective latent
knowledge, δi, j , for each participant i (across both the optin and random condition) and question j,
δi, j ∼ Bernoulli(logit−1 (θi + η j ))

(3)

xi, j ∼ Bernoulli(δi, j ρ j + (1 − δi, j )λ j )

(2)

where θi is the self-perceived skill of participant i, η j is the
ex
perceived familiarity of question j, and logit−1 (x) = 1+e
x.
This latent knowledge is represented as a 0 or 1, indicating
whether or not that participant believes that she has knowledge for that question. We place a Normal prior on the selfperceived skill, θi ∼ Normal(0, σ), such that participants are
expected to have the same skill (on average) for both the selfselection and random conditions.

(4)

For example, with ρ = 0.8 and λ = 0.4, the probability of a
correct response is 0.8 if a participant has subjective knowledge, but 0.4 if the participant does not. The guessing parameters are given Beta priors, ρ j ∼ Beta(α, β), λ j ∼ Beta(α, β)
where α and β are hyperparameters that control the variability
in guessing rates across questions.
To model the difficulty ratings, we use an ordered logit
model (Williams et al., 2006). We assume that subjective latent knowledge informs the perceived difficulty of questions.
Questions for which the participant believes they have knowledge are perceived as easier. Let φi, j represent the perceived
difficulty for participant i on question j. We determine the
perceived difficulty by:
φi, j = −δi, j − η j ξ + ωi − β j + σi, j

(5)

where β j and ωi capture participant and item level effects
(e.g. some participants might find all items easy, some items

1625

might be judged as easy) independent of subjective knowledge. In addition, we also allow the perceived familiarity of
a question η j to affect the perceived difficulty weighted by a
fixed scaling parameter ξ. Finally, σi, j represent small perturbations centered around 0 to explain the random variability
in difficulty ratings unrelated to any of the previous factors
mentioned. These perceived difficulties feed into the ordered
logit model to generate the difficulty ratings ri, j ,
ri, j ∼ OrderedLogit(φi, j , τi )

(6)

where τi is the set of criteria cutoffs for participant i.
We used JAGS to perform parameter inference. All parameters were inferred jointly from the opt-in and random
condition. All model predictions were derived from posterior
predictives where we simulate new participants from the distribution and assess how they self-select from a new set of
questions.

Results
We examine several empirical effects within the data and observe that the model captures the appropriate trend in most
cases.
Item selection and latent knowledge. The model captures the expected relationship between opting-in behavior
and knowledge (see Figure 2). Participants were more likely
to select questions for which they had pre-existing knowledge. Each question was randomly assigned to at least four
participants in the random assignment condition. However,
in the opt-in condition, there were seven questions that no
participant chose to answer. Question selection strongly corresponded with the inferred latent knowledge (δi, j ) for the
participant-question pair, with participants choosing questions for which they had latent knowledge. Across conditions,
latent knowledge is distributed in a similar manner: most participants have knowledge for popular questions, few participants have knowledge for unpopular questions, and some
participants are more knowledgeable than others. However,
the model has substantially more certainty about the localization of this knowledge in the opt-in condition compared to
the random condition because it can leverage the opt-in behavior. In Figure 2, this certainty is expressed as black or
white squares, while uncertainty is represented in gray. We
see the uncertainty about which participants have knowledge
for which question as a “blurring” of the latent knowledge
space.
Effect of opting-in on participant performance. Average
performance across questions was higher in the self-selection
condition (86.05%) than in the random condition (67.27%).
We computed a Bayes Factor (BF) given a binomial distribution with a shared or different rate of correct responding and
find a Log10 BF of 21.12 in favor of a higher rate of correct
responding in the opt-in condition. This corresponds to decisive evidence that average accuracy is higher in the opt-in
condition than the random assignment condition. This occurs
even when taking into account the fact that people tend to opt

Figure 2: Latent knowledge is similar between conditions and
corresponds to opting-in behavior. Plotted are the opt-in behaviors and average δi, j values across conditions, all sorted by
the popularity of the question in the opt-in condition. White
corresponds to questions that the participant opted in to or the
inferred presence of knowledge.

in to easier questions. In order to perform this analysis, we
took the product of the evidence that performance is higher
in the opt-in condition than the random assignment condition
for each question and find a Log10 BF of 9.02. So, even when
comparing on an item-by-item basis, opting-in provides an
advantage.
Effect of opt-in on model performance. For the model,
the average accuracy for posterior predictive samples in the
self-selection condition (mean = 79.03%) is also significantly
higher than in the random condition (mean = 67.07%), both
across all questions (99.86 % of samples) and even within
questions (68.93 % of sample-question pairs). We observe
this benefit in accuracy despite the average inferred ability of individual subjects (θi ) being equivalent across conditions: θi = 0.00, SD = 0.99 in the opt-in condition versus
θi = −0.09, SD = 2.04 in the random assignment condition.
This means that the benefit to accuracy that the model predicts is due to downstream consequences of the metacognitive selection process and not an (inaccurate) inference that
participants in one condition were more skillful than in the
other.
Difficulty Ratings. Participants tended to give lower average difficulty ratings to questions that they opted in to (Log10
BF = 91.89) and higher average difficulty ratings to questions
that they did not opt in to (Log10 BF = 64.09), relative to the
random condition. The model captures, but understates, this
trend (see Figure 3).
Wisdom of the crowd. The left panel of Figure 4 shows
the relationship between crowd size and crowd accuracy for
the two conditions in the experiment, as well as a hybrid condition in which the two groups are combined. The right side
of the Figure shows that the model captures this effect qualitatively. Crowd responses were determined by taking the most

1626

Figure 4: Crowd performance when varying the number of
participants (measured by the total number of judgments)
Figure 3: Distribution of difficulty ratings for participants and
model for questions that were selected or not selected in the
opt-in condition and the random condition. Lower ratings indicate lower perceived difficulty.

common response across the participants in the crowd. Since
seven questions went unanswered in the opt-in condition, we
had to consider how unanswered questions impacted crowd
performance. To treat the self-selection condition maximally
conservatively, we graded any question that went unanswered
as incorrect for that crowd. Even with this penalty, the crowd
composed of the participants from the self-selection condition
(79%) outperformed the crowd of subjects from the random
condition (73%).
We also considered the impact of crowd size on performance. To do this, we evaluated the average performance of
crowds composed of random samples of participants from a
condition and varied the number of participants drawn to form
the sample. We plot average crowd performance as a function
of the total number of judgments, where a judgment is a person’s response to a question. The hybrid condition provides a
means of improving upon both conditions. To create a hybrid
crowd, we first sampled participants that answered the question from the opt-in condition. If a question had no responses,
we added the answer from one participant in the random condition in order to guarantee that all questions received at least
one answer. This hybrid crowd has high performance across
all questions. The model captures the general trends in the
data in that larger crowds result in higher crowd accuracy,
opt-in crowds outperform random-assignment crowds, and
the hybrid crowds perform well across all questions.
Additional simulations. Given our model, we investigated
which circumstances would likely lead to changes in the relative performance of the self-selection and random conditions
in terms of both average overall accuracy and crowd performance. We varied the heterogeneity of perceived question
difficulty (η j ) and latent ability (θi ). We did this by simulating experiments in which we varied the underlying hyperparameter corresponding to the variability of θi and η j by factors of 0.25, 1, and 4 while keeping other parameters constant

(see Figure 5). We find that increasing the heterogeneity of
perceived question difficulty increases self-selection accuracy
overall, but decreases it at the crowd level since participants
tend to avoid answering the same difficult questions. Heterogeneity in question difficulty does not have an appreciable impact on performance in the random condition. In both
conditions, higher heterogeneity of participant skill leads to
higher crowd performance and gives resilience to heterogeneously difficult questions in the opt-in condition. However,
it detracts from overall accuracy in the self-selection condition.

Figure 5: Simulated performance depending on variability of
question difficulty (η j ) and participant skill (θi ).

Conclusions
A comprehensive model of cognition must make allowance
for the fact that cognitive behavior is driven by motivations.
We choose what we attend to and attempt to encode, and
what we attempt to remember. Metacognitive behavior is at
the heart of most learning outside the laboratory, and a fair
amount within it as well (A. Benjamin & Ross, 2008). The
joint modeling of metacognitive behavior–like self-selection

1627

of items–along with cognitive performance has the potential
to address a wider and more representative range of realworld learning and testing behaviors, and can serve as the
basis for drawing comparisons across individuals or tests that
would otherwise be hopelessly confounded. Additionally, the
model could be extended to explain various incentives given
to the participant, which would impact how latent knowledge interacts with the task to generate opt-in behaviors. The
model presented here provides a starting point for such an
enterprise. It leads to a relatively good description of performance across a variety of metrics. A single latent knowledge state for each participant-question pair permits an explicit representation of the metacognitive process that governs the relationship between opt-in, accuracy, and difficulty
behaviors. The model is successful in describing the nonrandom missing nature of the data that we observed by relying on
principled psychological theories about why someone might
choose one question over another.
An additional lesson of the current research can be seen
in the crowd data. Opting in is generally beneficial to crowd
accuracy in both the observed data and our model. This result indicates that the metacognitive skill of the individuals
in self-selection can be leveraged in order to create a smarter
crowd. This effect is sufficiently robust that it appears to outweigh the cost associated with small crowd sizes for some
questions or no volunteered responses at all for a small number of questions. Such a result is particularly important when
considering the widespread availability of datasets in which
responses are self-selected.

References
Benjamin, A., & Ross, B. H. (2008). The psychology of
learning and motivation: Skill and strategy in memory use
(Vol. 48). Academic Press.
Benjamin, A. S., & Bird, R. D. (2006). Metacognitive control
of the spacing of study repetitions. Journal of Memory and
Language, 55(1), 126–137.
Boekaerts, M., & Minnaert, A. (1999). Self-regulation with
respect to informal learning. International journal of educational research, 31(6), 533–544.
Bradley, J. V. (1981). Overconfidence in ignorant experts.
Bulletin of the Psychonomic Society, 17(2), 82–84.
Fiechter, J. L., Benjamin, A. S., & Unsworth, N. (2016).
16 the metacognitive foundations of effective remembering. The Oxford Handbook of Metamemory, 307.
Finley, J. R., Tullis, J. G., & Benjamin, A. S. (2010).
Metacognitive control of learning and remembering. In
New science of learning (pp. 109–131). Springer.
Fleming, S. M., & Daw, N. D. (2017). Self-evaluation
of decision-making: A general bayesian framework for
metacognitive computation. Psychological Review, 124(1),
91.
Goldsmith, M., & Koriat, A. (2007). The strategic regulation
of memory accuracy and informativeness. Psychology of
learning and motivation, 48, 1–60.

Kelley, C. M., & Sahakyan, L. (2003). Memory, monitoring,
and control in the attainment of memory accuracy. Journal
of Memory and Language, 48(4).
Koriat, A., & Goldsmith, M. (1996). Monitoring and control
processes in the strategic regulation of memory accuracy.
Psychological review, 103(3), 490.
Lagnado, D. A., & Sloman, S. (2004). The advantage of
timely intervention. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 30(4), 856.
Lieder, F., & Griffiths, T. L. (2015). When to use which
heuristic: A rational solution to the strategy selection problem. In Cogsci.
Lieder, F., Plunkett, D., Hamrick, J. B., Russell, S. J., Hay,
N., & Griffiths, T. (2014). Algorithm selection by rational metareasoning as a model of human strategy selection.
In Advances in neural information processing systems (pp.
2870–2878).
Little, R. J., & Rubin, D. B. (2014). Statistical analysis with
missing data. John Wiley & Sons.
Mezirow, J. (1981). A critical theory of adult learning and
education. Adult education quarterly, 32(1), 3–24.
Nelson, T. O., & Narens, L. (1990). The psychology of learning and motivation. Metamemory: A theoretical framework
and new findings.
Paris, S. G., & Paris, A. H. (2001). Classroom applications of
research on self-regulated learning. Educational psychologist, 36(2), 89–101.
Sobel, D. M., & Kushnir, T. (2006). The importance of decision making in causal learning from interventions. Memory
& Cognition, 34(2), 411–419.
Steyvers, M., Miller, B., Hemmer, P., & Lee, M. D. (2009).
The wisdom of crowds in the recollection of order information. In Advances in neural information processing systems
(pp. 1785–1793).
Steyvers, M., Tenenbaum, J. B., Wagenmakers, E.-J., &
Blum, B. (2003). Inferring causal networks from observations and interventions. Cognitive science, 27(3), 453–
489.
Surowiecki, J. (2004). The wisdom of crowds: why the many
are smarter than the few and how collective wisdom shapes
business, economics, society and nations. Little, Brown.
Williams, R., et al. (2006). Generalized ordered logit/partial
proportional odds models for ordinal dependent variables.
Stata Journal, 6(1), 58.
Winne, P. H., & Hadwin, A. F. (1998). Studying as selfregulated learning. Metacognition in educational theory
and practice, 93, 27–30.
Zimmerman, B. J. (1989). A social cognitive view of selfregulated academic learning. Journal of educational psychology, 81(3), 329.

1628

