PACKER: An Exemplar Model of Category Generation
Nolan Conaway (nconaway@wisc.edu)
Joseph L. Austerweil (austerweil@wisc.edu)
Department of Psychology, 1202 W. Johnson Street
Madison, WI 53706 USA
Abstract
Generating new concepts is an intriguing yet understudied
topic in cognitive science. In this paper, we present a novel
exemplar model of category generation: PACKER (Producing
Alike and Contrasting Knowledge using Exemplar Representations). PACKER’s core design assumptions are (1) categories
are represented as exemplars in a multidimensional psychological space, (2) generated items should be similar to exemplars
of the same category, and (3) generated categories should be
dissimilar to existing categories. A behavioral study reveals
strong effects of contrast- and target-class similarity. These
effects are novel empirical phenomena, which are directly predicted by the PACKER model but are not explained by existing
formal approaches.
Keywords: Categorization, exemplar models, category generation, creative cognition, computational modeling.

Introduction
The creation of new concepts and ideas is among the most
interesting – yet infrequently studied – capabilities of human
cognition. This paper focuses on one topic within the broader
field of creative cognition: category generation. Foundational
work on this topic (e.g., Smith, Ward, & Schumacher, 1993;
Ward, 1994, 1995; Ward, Patterson, Sifonis, Dodds, & Saunders, 2002) has focused on the role of prior knowledge in generating novel concepts. A core phenomenon is that people
generate categories with similar distributional properties as
existing categories. For example, Ward (1994) asked participants to generate species of plants and animals that might exist on other planets. Generation was strongly constrained by
prior knowledge of Earth species: People generated species
with the same features as those found on Earth (e.g., eyes,
legs, wings) and possessing the same feature correlations observed on Earth (e.g., feathers co-occur with wings).
Recent work has proposed and tested formal models to explain these observations. Jern and Kemp (2013) trained participants on experimenter-defined categories composed of exemplars within an artificial three-dimensional domain. After a short training phase, participants were asked to generate
exemplars from a new category. Participants were provided
with a set of scales to adjust the feature values of each generated stimulus, and were given unlimited time to create each
example. As in the classic Ward (1994) experiment, Jern and
Kemp (2013) found that generated categories possessed the
same feature variance and correlations as the experimenterdefined categories in the domain.
Jern and Kemp (2013) tested several different computational models on their data. Most relevant to the present investigation, they tested a ‘copy-and-tweak’ model that generates items by copying and changing previous observations,
and a hierarchical Bayesian model that uses the structure of

known categories to infer the structure of new ones. They
found that the hierarchical Bayesian model provided the
strongest account of the behavioral results.
In this paper, we introduce a novel exemplar-based approach to category generation, PACKER (Producing Alike
and Contrasting Knowledge using Exemplar Representations), which creates categories by balancing two constraints:
(1) new categories should be different from known categories (minimizing between-class similarity), and (2) new
categories should be internally coherent (maximizing withinclass similarity). As such, PACKER is a significant departure
from previous accounts of generation – rather than proposing that people create categories by abstracting and re-using
knowledge of related categories, PACKER first considers how
the generated category should differ from related categories.
Further, it does so using the well-studied mechanics of exemplar representations and therefore possesses a rich connection
to the wider body of research on category learning.
In the sections below, we formally describe the PACKER
model and explore its predictions in a behavioral experiment.
We compare its performance to copy-and-tweak and hierarchical Bayesian models by examining their fits to aggregate
results and individual differences.

PACKER: An Exemplar Model
The PACKER model is an extension of the Generalized Context Model of category learning (GCM; Nosofsky, 1984). It
assumes that each category is encoded by a set of exemplars
within a k-dimensional psychological space, and that generation is constrained by both similarity to members of the target
category (the category in which a stimulus is being generated)
as well as similarity to members of other categories.
As in the GCM, the similarity between two examples,
s (xi , x j ), is an inverse-exponential function of distance:
(

)

s (xi , x j ) = exp −c ∑ xik − x jk wk

(1)

k

where wk is the attention weighting of dimension k (wk ≥ 0
and ∑k wk = 1), accounting for the relative importance of each
dimension in similarity calculations, and c (c > 0) is a specificity parameter controlling the spread of exemplar generalization. For simplicity, our simulations will use uniform attention weights, except when otherwise noted.
To generate a new example, the model considers both the
similarity to examples from contrast categories as well as the
similarity to examples (if any exist) in the target category.

1812

(a) Contrast Influence

(b) Target Influence

(c) Combination
Greatest
Probability

B

B

A

B

A

{c = 1, γ = 0, θ = 3}

A

{c = 1, γ = 1, θ = 3}

{c = 1, γ = 0.5, θ = 3}

Lowest
Probability

Figure 1: PACKER generation of a category ‘B’ example, following exposure to one member of category ‘A’ and category ‘B’.
The panels differ in how the trade-off between within- and between-category similarity is managed (via the γ parameter).

The aggregated similarity a between generation candidate y
and stored exemplars x is:
a(y, x) = ∑ f (x j )s(y, x j )

(2)

j

where f (x j ) is a function specifying the extent to which each
exemplar contributes to the generation. A negative value for
f (x j ) produces a ‘repelling’ effect (items are less likely to be
generated nearby x j ), and a positive value produces an ‘attracting’ effect (items are more likely to be generated nearby
x j ). When f (x j ) = 0, the exemplar does not contribute to
generation.
PACKER sets f (x j ) depending on exemplar j’s category
membership: f (x j ) = γ if x j is a member of the target category, and f (x j ) = γ − 1 if x j is a member of a contrast
category. γ is thus a free parameter (0 ≤ γ ≤ 1) controlling
the trade-off between within- and between-category similarity. PACKER’s core proposal is that new categories should
be different from existing categories, and same-category exemplars should be similar to one another. This is realized
when γ assumes an intermediate value: For example, when
γ = 0.5, f (x j ) = 0.5 for members of the target category and
f (x j ) = −0.5 for members of other categories; thus, the
model is likely to generate items that are similar to members
of the target category but are not similar to members of other
categories. However, more extreme values can be used to
produce different behavior, see Figure 1.
The probability that a given candidate y will be generated
is evaluated using an Exponentiated Luce (1977) choice rule.
Candidates with greater values of a are more likely to be generated than candidates with smaller values:
p(y) =

exp(θ · a(y, x))
∑i exp(θ · a(yi , x))

(3)

where θ (θ ≥ 0) controls response determinism.

Summary
The proposed PACKER model suggests people generate categories by minimizing between-category similarity and maximizing within-category similarity. The underlying processes
assumed by PACKER are highly similar to those in the GCM.

The main difference is that PACKER aggregates positiveand negative-valued similarities, rather than only aggregating positive-valued similarities. In later sections, we will explore the unique predictions yielded by these design principles. First, however, we contrast PACKER with other category generation models.

Previous Accounts of Category Generation
Previous models of category generation focus on capturing
the tendency for people to produce new categories that have
similar distributional properties to existing categories. To the
best of our knowledge, Jern and Kemp (2013) were the first
to evaluate computational models of generation. Based on
their work, we describe two alternative models: a formalization of the Path of Least Resistance hypothesis (later termed
copy-and-tweak, see Jern & Kemp, 2013), and the hierarchical sampling hypothesis (Jern & Kemp, 2013).

Copy-and-Tweak
The copy-and-tweak model, based broadly on the earlier Path
of Least Resistance view (Ward, 1994, 1995), proposes that
participants generate categories by retrieving an observation
of the target class from memory, and then tweaking it to make
something new. Jern and Kemp (2013) interpreted this proposal in terms of an exemplar model using the GCM (Nosofsky, 1984). Formally, their model is equivalent to PACKER
with γ = 1 (see Figure 1). In this case, f (x j ) = 1 for members
of the target category and f (x j ) = 0 for members of other
categories; thus, the model considers only target-class similarity, and when no members of the target class are known,
the model generates items at random.
In our work we provide simulations from the copy-andtweak account, realized as a variant of PACKER with a fixed
γ parameter. Formalizing a model family where PACKER
and copy-and-tweak are different parameterizations within
the same framework is useful because comparison between
the models provides a test of the explanatory value of the contrast mechanism: The account provided by copy-and-tweak
will only equal that of PACKER if the contrast mechanism
does not offer an advantage (i.e., if γ < 1 significantly improves model fits).

1813

Bottom

Hierarchical Sampling
Based on several results inconsistent with the copy-and-tweak
account, Jern and Kemp (2013) advocated a hierarchical
Bayesian model. Exemplars of each category were generated from a multivariate Normal distribution over the dimensions of stimulus space. The mean of each category was
independently generated, but the covariance matrix (encoding feature variances and correlations) was generated from a
common prior distribution. New categories are produced by
generating a new mean (uniform over stimulus space) and covariance matrix from the common prior distribution. Because
the shared prior distribution’s parameters were unobserved, a
hierarchical Bayesian model uses information from the previous categories (their feature variances and correlations) to
generate the covariance matrix of the new category.
Each category’s exemplars are assumed to be a multivariate Normal distribution with parameters (µ, Σ). Each category’s covariance matrix is assumed to be inverse-Wishart
distributed with parameters (v, κ, and ΣD ).1 ΣD is the covariance matrix shared between categories. We assume the
shared covariance matrix ΣD is generated from a Wishart distribution (for conjugacy) with parameters v0 , κ0 , and Σ0 . We
set ν0 = 4, and Σ0 = λI, where λ is a free parameter controlling the expected variance of dimensions (dimensions of the
shared covariance matrix are expected to be uncorrelated) and
I is the identity matrix.
To simplify the model predictions, we used maximum a
posteriori (MAP) estimates for the hidden parameters and
then generated new categories based on those estimates. Due
to conjugacy, the MAP estimate for the shared covariance matrix ΣD = Σ0 + ∑c Cc , where Cc is the empirical covariance
matrix of category c. The MAP estimate of the covariance
matrix for the target category B is


κnB
(x¯B − µB )(x¯B − µB )T (ν + nB )−1
ΣB = ΣD ν +CB +
κ + nB
(4)
where ν (ν > k − 1) is an additional free parameter (from the
Inverse-Wishart prior on ΣB ) weighting the importance of ΣD .
When the target category has no members (i.e., nB = 0), items
are generated at random.
Generated exemplars are drawn from a multivariate Normal distribution specified by (µB , ΣB ). Thus, p(y) is
p(y) =

exp(θ · Normal(y; µB , ΣB ))
∑i exp(θ · Normal(yi ; µB , ΣB ))

(5)

where θ is a response determinism parameter and
Normal(y; µ, Σ) denotes a multivariate Normal density
evaluated at y.

Behavioral Experiment
The copy-and-tweak and hierarchical sampling models were
designed to explain effects of prior knowledge on the struc1 Note that Jern and Kemp (2013)’s model is slightly different, as
they used a non-conjugate model. Their model acts very similar to
our version of it and receives comparable fits.

A A
A A

Middle

A A
A A

Figure 2: Conditions tested in the behavioral experiment.

ture of categories, but they do not make any assumptions
about the role of between-category contrast. Indeed, when
there are no known examples of the target category, both
models assume that generation is random. PACKER is thus
unique in its prediction that contrast categories should influence both the structure and location of generated categories.
The behavioral experiment described below was designed to
test this key prediction.
The experiment follows the paradigm developed by Jern
and Kemp (2013): first, participants learn members of a
known category (‘Alpha’, or ‘A’), and are then asked to generate exemplars belonging to a new category (‘Beta’, or ‘B’).
We developed two Alpha categories (see Figure 2): the ‘Bottom’ Alpha category is a tight cluster in the bottom-center of
the space, and the ‘Middle’ Alpha category is identical except
that it lies in the center of stimulus space.
Although our manipulation is minimal, the PACKER
model predicts strong between-condition differences. According to PACKER, the nature of the space not occupied by
the Alpha category should determine where members of the
Beta category are likely to be generated. Thus, the lower areas of the stimulus space should be less frequently used for
generation in the Bottom condition compared to the Middle
(as these areas possess greater similarity to the Bottom Alpha
category). Conversely, the upper areas of the stimulus space
should be used for generation more frequently in the Bottom
condition compared to Middle.
More generally, PACKER proposes that the probability a
stimulus y will be generated is a function of its similarity to
contrast categories and to members of the target category.
Two more general predictions (not specific to either condition) follow from this proposal: (1) the location of Beta examples should be positively related to distance from the Alpha
category, and (2) Beta examples should be more similar to
one another than they are to members of the Alpha category.
Participants & Materials We recruited 122 participants
from Amazon Mechanical Turk from the US equally assigned
to each condition. Stimuli were squares varying in color
(grayscale 9.8%-90.2%) and size (3.0–5.8cm). The assignment of perceptual features (color, size) to axes of the domain
space (x, y) was counterbalanced across participants.
Procedure Participants began the experiment with a short
training phase (3 blocks of 4 trials), where they observed ex-

1814

B

B

B
B

A A
A A

A A
A A

B

B

B

B

B

B
B

B BB

B
B

Table 1: Behavioral results.

A A
A A

B
B

B

B
BB

B

A A
A A

B

B

A A
A A

B
B

Used top row
28
11

No top row
18
4

Bottom
Used bottom row
No bottom row

Used top row
16
31

No top row
8
6

BB B
B
B

A A
A A

Middle
Used bottom row
No bottom row

A A
A A

A A
A A

Figure 3: Sample generated categories.
number of exemplars produced at different distances to the
center of the Alpha category. These data (Figure 4 left) reveal
a strong preference for stimuli that are dissimilar to the Alpha
category members: maximally distant items were by far the
most frequently generated.
Finally, we computed for each participant the average distance between exemplars belonging to the same and opposite
categories. These data (Figure 4 right) show that, as observed
by Ward (1994), most people generated Beta categories in
which members are closer to one another than they are to
members of the Alpha category (i.e., more between- than
within-category distance). We did however, observe a notable subset of individuals with greater within-class distance.
These individuals tended to adopt a ‘corners’ approach, in
which Beta examples were placed almost exclusively in the
corners of the space.

emplars belonging to the ‘Alpha’ category. Participants were
instructed to learn as much as they can about the Alpha category, and that they would answer a series of test questions
afterwards. On each trial, a single Alpha category exemplar
was presented, and participants were given as much time as
they desired before moving on. Exemplars were randomly ordered within each block. Participants were shown the range
of possible colors and sizes prior to training.
Following the training phase, participants were asked to
generate four examples belonging to another category called
‘Beta’. Participants were instructed that members of the Beta
category could be quite similar or different depending on
what they think makes the most sense for the category, but
that they were not allowed to make the same example twice.
As in Jern and Kemp (2013), generation was completed using
a sliding-scale interface. Two scales controlled the features of
the generated example. An on-screen preview of the example
updated whenever one of the features was changed. Participants could generate any example along an evenly-spaced
9x9 grid, except for any previously generated Beta exemplars.
Neither the members of the Alpha category nor the previously
generated Beta examples were visible during generation.

Summary

Results Several sample Beta categories are depicted in Figure 3. Because the conditions differ only in their location
along the y-axis, we first focus on how Beta exemplars are
generated above and below the contrast category. As is evident in Figure 3, we observed broad individual differences
in generation strategy: Whereas some participants generated
all four Beta examples within a narrow y-axis range, others
generated Beta examples along a wide range.
To evaluate the key predictions of PACKER, we determined the number of participants in each condition who
placed at least one Beta exemplar on the top and bottom
‘rows’ of the space (the maximum and minimum possible yaxis value, respectively). The resulting contingencies data are
shown in Table 1. Fisher’s Exact Tests reveal that more Middle participants generated a Beta exemplar in the bottom row
, p < 0.001, but the conditions did not differ in use of the top
of the space, p = 0.16. More Middle participants placed Beta
exemplars in the top and bottom rows, p = 0.038.
To evaluate PACKER’s other predictions, we computed the

Our results support PACKER’s predictions: People tend to
generate items that are dissimilar from the contrast category
and similar to the target category. We observed considerable
differences in generation between the Middle and Bottom
conditions: Participants in the Bottom condition were less
likely to use the bottom row of the stimulus space for generation, and participants in the Middle condition were more
likely to create categories spanning the entire y-axis (utilizing the top and bottom row of the space). This latter result
is especially interesting as it conflicts with previous results:
Qualitatively different types of categories were generated, depending only on the location of the Alphas.
Some aspects of the results described above are somewhat
commonsense: They demonstrate that the location of existing
categories imposes constraints on generation because people
tend to generate examples in areas not occupied by existing
categories. This principle, however, is novel and not predicted by existing models of generation – these models were
designed to explain distributional correspondences between
generated and existing categories, not effects of contrast.

Model Evaluation
To obtain an overall sense of each model’s ability to explain our results, we fit each model by maximizing the loglikelihood of the model’s predictions of the human results.
The c, γ, and θ parameters were fitted for PACKER; c, and θ

1815

Between-Class Distance

Generations Per Stimulus

30
Bottom

25

Middle
20
15
10
5
0
Min

Distance

Max

Table 2: Model-fitting results.

Within-Class Distance

Figure 4: Behavioral results. Left: Frequency of exemplar
generation as a function of distance from the Alpha category normalized by the maximum possible distance. Right:
Within- vs. between-category distance for every participant.

were fitted for the copy-and-tweak model (γ was fixed at 1),
and κ, λ, ν, and θ were fitted for the hierarchical sampling
model. Note that each model possess a θ parameter fulfilling
the same role (response determinism). Attention in PACKER
and copy-and-tweak was set uniformly. Parameters were not
allowed to vary between participants or conditions – the goal
was to obtain the best-fitting values to our entire dataset.
Each model’s best-fitting parameterization is shown in Table 2. Overall, PACKER outperformed copy-and-tweak and
the hierarchical sampling model by a considerable margin
(∼ 11% improvement in log-likelihood). The parameter settings associated with PACKER’s best fit are exactly as expected: a strong preference for items that are similar to members of the target category but are dissimilar to members of
the contrast category. A similar pattern of results was obtained when we only considered the second to fourth exemplars generated by each participant.
Our model-fitting results make sense given the assumptions
made by each model. As the copy-and-tweak and hierarchical
sampling models are not influenced by the location of contrast
categories within the space, they do not capture the broad tendency for generated items to be dissimilar to existing classes.

Relation Between Category Structure & Location
Generally, our behavioral results showed that members of
generated categories are dissimilar to opposite categories, and
similar to members of their own category. However, we also
observed a great deal of individual differences in generation
style. Manually inspecting the data reveals four typical patterns (see Figure 3): ‘corners’ categories with one Beta example in each corner of the space, tight clusters, ‘column’-like
categories, and ‘row’-like categories. This informal inspection also reveals that each of these category types tended to
be generated into distinct regions of the domain, suggesting a
link between category location and distributional structure.
To more systematically evaluate this possibility, we computed, for each stimulus in the domain, the difference in range
between the features (range(size) − range(color)) across every generated category that had the stimulus as member. Ag-

PACKER

Copy & Tweak

AIC = 3474
c = 0.565
γ = 0.469
θ = 6.632

AIC = 3914
c = 4.894
γ = 1 (fixed)
θ = 3.712

Hierarchical
Sampling
AIC = 3972
κ < 0.001
ν = 4.660
λ = 0.423
θ = 2.771

gregating over these range differences yields a gradient describing how categories tended to distributed for each stimulus. These data (Figure 5) reveal a systematic relationship between category structure and location. Whereas column-like
categories more often include stimuli to the left or right of
the Alpha class, row-like categories appear above and below
the Alpha class. Thus, participants modify the distributional
structure of new categories to the maximize distance from the
contrast category.
To simulate this finding, we set the attention weight parameters in PACKER and copy-and-tweak per participant. The
other free parameters were set as in Table 2. While there exist methods to find the optimal attention weights for a given
classification (see Vanpaemel & Lee, 2012), for simplicity we
approximated the weights using proportionally to inverse of
each feature’s range: Thus, the Alpha and Beta categories
are assumed to be distinct along dimensions that the Betas do
not vary on. To simulate the hierarchical sampling model we
set the domain covariance prior, Σ0 , proportional to the range
(not inverted) of each feature: Thus new categories were distributed more widely along the features that each participant
used more widely. We then simulated 50 Beta categories with
each participant’s weighting scheme to obtain a sense of how
the relative importance of each dimension affects what types
of categories are generated and where they are generated. The
results of these simulations are depicted in Figure 5.
When the x-axis is weighted more, PACKER creates column categories to the sides of the Alphas. Conversely, when
the y-axis is weighted more, PACKER creates row categories
above and below the Alphas. This behavior falls out from
the nature of selective attention: Dimensions weighted more
have a sharper similarity gradient. For example, when the xaxis is weighted more, PACKER favors Beta categories with
more within-class similarity (less range), and less betweenclass similarity along the x-axis, resulting in column-like categories that differ from the Alphas along the x-axis.
Although differentially weighting the features results in
different types of categories from the hierarchical sampling
and copy-and-tweak models, the location of the Alpha category does not affect where items will be generated by these
models. Thus, row- and column-like categories are not systematically generated in different areas of the stimulus space,
resulting in the uniform predictions shown in Figure 5.

1816

Middle
Bottom

Behavioral

PACKER

Copy and Tweak Hierarchical Sampling

A A

A A

A A

A A

A A

A A

A A

A A

A A

A A

A A

A A

A A

A A

A A

A A

Horizontally
Aligned
Category

Vertically
Aligned
Category

Figure 5: Generated category structure as a function of location. Orange areas in each gradient correspond to stimuli that were
commonly generated into category possessing greater y-axis range (columns). Purple areas correspond to categories possessing
greater x-axis range. White areas correspond to equal range along both features (or infrequent generation).

Discussion
The creative use of conceptual knowledge is a fascinating yet
understudied topic in categorization. In this paper, we presented a novel exemplar-based approach to explaining category generation. The PACKER model proposes that categories are represented as a collection of exemplars stored in
memory, and that members of generated categories should be
similar to one another, yet dissimilar to members of opposing
categories. Exemplar models can be viewed as ImportanceSampling approximations of Bayesian models (Shi, Griffiths,
Feldman, & Sanborn, 2010). So, PACKER can be viewed as
a rational process model, approximating the expected density
of a new category based on a contrast category.
In a behavioral study and subsequent formal modeling, we
found broad support for the PACKER model. Participants
in our study more frequently generated items that are distant
from members of contrast categories, and they tended to generate categories with more within-class than between-class
similarity. Likewise, we found that the location of contrast
categories (as opposed to their structure) shapes generation by
imposing constraints on the areas of space that remain available for a new category. Formal simulations reveal that existing models (see Jern & Kemp, 2013), making no assumptions
about category-contrast, do not account for these effects.
The PACKER model is, in general, highly expressive in its
performance. Under different parameter settings it is capable of generating tightly clustered or highly distributed categories, and adjusting the distribution of categories along each
feature. Future work will focus on exploring the broad degree of individual differences we observed in generation, and
whether PACKER can explain previous results in the field
(Jern & Kemp, 2013; Ward, 1994).

Acknowledgments
Support for this research was provided by the Office of the
VCRGE at the UW - Madison with funding from the WARF.
We thank Kenneth Kurtz for helpful comments, and Alan Jern
and Charles Kemp for providing code and data.

References
Jern, A., & Kemp, C. (2013). A probabilistic account of exemplar and category generation. Cognitive Psychology,
66(1), 85–125.
Luce, R. D. (1977). The choice axiom after twenty years.
Journal of mathematical psychology, 15(3), 215–233.
Nosofsky, R. M. (1984). Choice, similarity, and the context
theory of classification. Journal of Experimental Psychology: Learning, Memory, & Cognition, 10(1), 104.
Shi, L., Griffiths, T. L., Feldman, N. H., & Sanborn, A. N.
(2010). Exemplar models as a mechanism for performing Bayesian inference. Psychonomic Bulletin & Review, 17(4), 443-464.
Smith, S. M., Ward, T. B., & Schumacher, J. S. (1993). Constraining effects of examples in a creative generation
task. Memory & Cognition, 21(6), 837–845.
Vanpaemel, W., & Lee, M. D. (2012). Using priors to formalize theory: Optimal attention and the generalized
context model. Psychonomic Bulletin & Review, 19(6),
1047–1056.
Ward, T. B. (1994). Structured imagination: The role of
category structure in exemplar generation. Cognitive
Psychology, 27(1), 1–40.
Ward, T. B. (1995). Whats old about new ideas. The creative
cognition approach, 157–178.
Ward, T. B., Patterson, M. J., Sifonis, C. M., Dodds, R. A., &
Saunders, K. N. (2002). The role of graded category
structure in imaginative thought. Memory & Cognition,
30(2), 199–216.

1817

