Decoding Virtual Agent’s Emotion and Strategy from Brain Patterns
Eunkyung Kim, Sarah I. Gimbel, Aleksandra Litvinova, Jonas T. Kaplan and Morteza Dehghani
{eunkyung; sgimbel; alitvino; jtkaplan; mdehghan}@usc.edu
University of Southern California, Los Angeles, CA 90089 USA
Abstract
Recent advances in technology have paved the way for humanagent interactions to become ubiquitous in our daily lives, and
decades worth of research on virtual agents have enhanced
these interactions. However, for the most part, the effect of different types of agents on the human brain is unknown, and the
neuroscience of human-agent interactions is rarely studied. In
this study, we examine the underlying neural systems involved
in processing and responding to different types of negotiating
agents. More specifically, we show that different brain patterns
are observed for various types of virtual agents; consequently,
we can decode the strategy and emotional display of the agent
based on the counterpart’s brain activity. Using fMRI data, we
analyzed participants’ brain activity during negotiations with
agents who show three different emotional expressions and use
two different types of negotiation strategies. We demonstrate
that, using Multi-Voxel Pattern Analysis, we can reliably decode agents’ emotional expressions based on the activity in the
left dorsal anterior insula, and also agents’ strategies based on
the activity in the frontal pole.
Keywords: Human-Agent Interaction; Negotiation; Emotion;
Decision-Making; fMRI

Introduction
Virtual agents have become a part of our daily lives. From
commercial websites that make use of chat agents for answering users’ questions in personalized settings to educational and training software that incorporate virtual agents to
provide better learning experiences, our numbers of interactions with virtual agents have dramatically increased in the
past couple of years.
Parallel to this increase, various lines of research have studied the interactions between humans and virtual agents. For
example, research has examined the contributing factors to
user engagement (Bickmore, Schulman, & Yin, 2010; Castellano, Pereira, Leite, Paiva, & McOwan, 2009) and establishment of bonds with virtual agents (Cassell & Thorisson,
1999; Wang & Gratch, 2009). Bickmore et al. (2010) showed
that people engage more with life-like virtual agents, such
as a relational agent who remembers past history and relates
to that history when communicating. Moreover, Wang and
Gratch (2009) demonstrated that virtual agents who give nonverbal immediacy feedback, such as eye contact and gestures,
are found to establish rapport with human partners.
Related to our research, the effects of emotion and strategies are among the most widely explored topics in humanagent interaction research (Maldonado et al., 2005; Kim,
Dehghani, Kim, Carnevale, & Gratch, 2014; Van Kleef,
De Dreu, & Manstead, 2004). These factors are particularly important because they are central in providing clues
about the internal states and the intentions of the counterpart
in any type of interactions (Jurafsky, Ranganath, & McFarland, 2009; Rafaeli & Sutton, 1987). Previous studies on

the effect of emotion include the work of Maldonado et al.
(2005) where they demonstrated that people who interact with
an emotional agent perform better on a test than those who
interact with an emotionless agent in a web-based learning
environment. Van Kleef et al. (2004) argue that automated
agents who express emotions, such as anger or happiness,
elicit different levels of concessions based on the type of expressed emotions. Also, several researchers have examined
the effects of different types of negotiation strategies during human-agent interactions. For example, Das, Hanson,
Kephart, and Tesauro (2001) demonstrated that the agreed
trade prices from human-agent interactions are different for
two types of agent strategies; one strategy is to maximize its
expected surplus using trade history and the other strategy is
to make small random adjustments to the trade price continuously. Similarly, Grosz, Kraus, Talman, Stossel, and Havlin
(2004) demonstrated that there are particular agent strategies
that elicit more concessions during negotiations.
These, along with numerous other studies, have helped the
field establish sets of features that influence the quality of
human-agent interaction, resulting in a more enhanced and
realistic experience for the human user. However, with a few
exceptions (e.g., Sanfey, Rilling, Aronson, Nystrom, and Cohen (2003)), the majority of these studies have for the most
part treated the process and the mechanism through which
these features affect the human counterpart as a black box;
they demonstrate that a particular type of agent, with particular emotion and strategy, enhances a user’s experience (e.g.
performance on a test). However, the question of how these
features affect the underlying neural mechanisms of the user
that result in such enhancement still remains unanswered. For
instance, even though it is established that interacting with
an emotional agent results in better performance on a test
than interacting with an unemotional agent (Maldonado et
al., 2005; Karacora, Dehghani, Krämer-Mertens, & Gratch,
2012), the neuroscience of these interactions are not fully understood.
In this paper, we investigate the underlying neural systems
that are activated when participants interact with agents who
show different emotional expressions and apply different negotiation strategies. As various emotions and strategies have
been related to diverse reactions in previous studies, we assumed that we could find differences between the neural processes that are activated when these factors are manipulated.
To examine these differences, we studied people’s neural activation while they were interacting with a virtual agent. We
hypothesize that distinct patterns of brain activity would be
observed for each agent type.
To study brain activity during human-agent interaction,

2407

we used functional magnetic resonance imaging (fMRI). We
used a human-agent negotiation platform for the experiment
in order to capture active interaction between a participant
and a virtual agent while they are trying to reach an agreement. The virtual agent was designed to display three different emotional expressions and apply two fixed negotiation
strategies, for six combinations of emotional expression and
negotiation strategy. During the experiment, we had participants engage in several rounds of negotiations with the virtual
agent inside the fMRI scanner. We then analyzed their brain
patterns during the decision-making period using Multi-Voxel
Pattern Analysis (Norman, Polyn, Detre, & Haxby, 2006).
We focused our analyses on the anterior insula and the
frontal pole. Anterior insula is a well-known emotion-related
brain region that is consistently activated when processing
basic emotions such as anger and sadness, as well as social
emotions such as empathy and vicarious emotions (Kober et
al., 2008; Lamm & Singer, 2010). It has been repeatedly
reported that both observing the emotional facial expression
and feeling the emotion activate the anterior insula (Zaki,
Davis, & Ochsner, 2012). On the other hand, the frontal pole
is one of well-known decision-making-related brain regions.
It has been shown that frontal pole plays a significant role
in thinking about the future (Okuda et al., 2003), and people with frontal pole impairment make disadvantageous decisions (Anderson, Bechara, Damasio, Tranel, & Damasio,
1999).
We hypothesize that an agent’s negotiation strategy can be
predicted based on the activity in the frontal pole, and an
agent’s emotional expression can be predicted based on the
activity in the anterior insula. To validate these hypotheses,
we compared participants’ brain activities in these regions
during negotiations in terms of an agent’s emotional expression (angry, neutral, and sad), and an agent’s negotiation strategy (conceding and non-conceding).
Our research contributes to a fast growing field of humanagent interaction, and is one of the first lines of work that investigates the underlying neural systems involved in the process of human-agent negotiation. This paper is organized as
follows. First, we introduce our negotiation platform, the Objects Negotiation Task. Next, we explain our experimental
settings and the design of our virtual agent. Then, we describe the parameters used to record fMRI data and how this
data were analyzed. Finally, we show our results and discuss
our findings.

Objects Negotiation Task
The Objects Negotiation Task is a multi-round human-agent
negotiation platform (Dehghani, Carnevale, & Gratch, 2014)
where a human negotiator and a virtual agent can negotiate
diverse items over multiple rounds. We used a modified version of this task tailored for use in the fMRI. Common fruits
were used as negotiation items, and the payoff for each item
for both players were explicitly specified on the screen. In
order to make sure all participants had the same goal during

negotiations, they were asked to focus on maximizing their
total payoffs. To ease the calculation of total payoffs, the system automatically calculated the player’s total payoff as well
as the agent’s total payoff, and displayed whenever the items
are redistributed.
When the negotiation starts, items are placed in the middle
row indicating that they do not belong to anyone. After the
participant distributes all the items, the ‘Go!’ button on the
right bottom corner is enabled and the participant can propose his/her offer by clicking the button. The participant is
then asked to wait until the virtual agent accepts or rejects
the offer. If accepted, both parties get the proposed items and
the negotiation ends. If rejected, the participant is asked to
wait for the virtual agent to propose a counteroffer. Next, the
virtual agent’s offer is shown and the participant reviews the
offer for five seconds. During this time, the participant simply
observes the counteroffer and cannot relocate the items. For
the fMRI version of the task, this review time was introduced
to make sure that we can separate brain activity between the
offer-making period and the non-offer-making periods. After the review, the participant decides whether to accept or
reject the virtual agent’s offer. If he/she accepts the virtual
agent’s offer, the negotiation ends. If rejected, the participant
is again asked to wait for five seconds and then is redirected
to the first step. Figure 1 shows an example of the timeline of
the Objects Negotiation Task.
In our study, we used six types of agents characterized by
the three types of emotions they expressed and the two types
of offer sets representing their negotiation strategies. More
details about these features are described in the following two
sections.

Agent’s Emotional Expressions
The role of emotional displays in negotiation has been extensively documented (Lerner, Li, Valdesolo, & Kassam, 2015).
To find the neural mechanisms involved in processing different emotional displays in human-agent interactions, we used
three types of facial expressions to express agents’ emotions;
angry, neutral (no emotion) and sad. Figure 2 shows agents’
emotional expressions that were used in the experiment. In
angry and sad conditions, the virtual agent’s face starts as
neutral and changes to the emotional expression for five seconds on the first, third, and fifth rounds of negotiation. In the
neutral condition, the agent’s face starts as neutral and does
not change.

Agent’s Negotiation Strategies
We used two sets of pre-programmed agent offer strategies:
non-conceder and conceder. In the non-conceder strategy, the
agent starts with no concession and continues with gradually increased concession. In the conceder strategy, the agent
starts with some concessions and keeps conceding further in
the next rounds.
When the virtual agent decides whether to accept or reject
a participant’s offer, the agent calculates the summed payoffs and compares it with its next offer. If the summed pay-

2408

Figure 1: Timeline of the Objects Negotiation Task used in the fMRI experiment. During a negotiation, a participant and a
virtual agent take turns in making a proposal. If the proposed offer is accepted, the negotiation ends. If rejected, the player who
rejected the offer makes a new proposal.

Procedure
Upon arrival, participants completed the informed consent
and each participant was asked to read the following hypothetical scenario:
You are a restaurant owner in a small town. There
has been a major fire in the market providing the necessary fruits for your restaurant and as a result only a limited number of fruits are available. Because of this you
have to split the available fruits with another restaurant
owner. You and the other owner value each fruit differently. In order to run your restaurant you need to get as
many fruits as possible.
In the task that follows, you will negotiate about how
to distribute the fruits between you and the other restaurant owner.

Figure 2: Agent’s emotional expressions. Angry (left), neutral (middle), and sad (right).

offs are larger than the summed payoffs of the next offer, the
agent accepts the offer. Otherwise, the agent rejects the offer
and proposes a new offer. The same payoff matrix was used
across all six negotiation tasks so that we could control for the
potential effect of varying payoff values. However, we randomized the order of items shown on the screen in each task
to give participants the impression that they were playing a
new negotiation task every time.

Experiment

After reading the scenario, participants were first invited
to play a trial negotiation so that they got used to the interface of the task. Then participants performed six negotiation
tasks inside an fMRI scanner. The total scan time for each
participant was approximately 45 minutes.

Data Analysis

Participant
We recruited ten participants through an online bulletin board
at the University of Southern California. Prior to the study,
all participants completed a checklist to make sure they were
eligible to take part in an MRI study. All procedures were
approved by the USC Institutional Review Board and participants were provided with a written informed consent for
the study. One participant’s data was later excluded from the
analyses because of technical problems with the obtained images.

To study brain patterns during negotiations with various
agents, we analyzed participants’ brain activity using general
linear models (GLM) analysis and multi-voxel pattern analysis (MVPA).

General Linear Model Analysis
To extract brain activity of the offer-making period from
the whole negotiation period, we ran a general linear model
(GLM) analysis using tools from the FMRIB’s Software Library (FSL) (Smith et al., 2004). First, we pre-processed

2409

our fMRI data using FSL to reduce the noise of our dataset.
Data pre-processing included the following mostly standard
steps: (1) Motion-correction with MCFLIRT to fix head motion artifacts during scans, (2) Slice timing correction for interleaved acquisitions to compensate for timing difference between slices of functional images, (3) Non-brain structures
removal with Brain Extraction Tool to remove non-brain regions, such as the scalp, (4) Spatial smoothing with a Gaussian kernel of full width at half maximum 5mm to increase
statistical power by improving the signal to noise ratio, (5)
High-pass temporal filtering to let high frequencies, such as
activities relevant to experimental conditions, pass and to remove low frequencies such as signal drifts.
For each negotiation for each participant, changes in the
blood-oxygen-level dependent signal were modeled with regressors for the offer-making period. Then the regressors
were convolved with a double-gamma hemodynamic response function. The non-offer-making periods were modeled as baseline.

Multi-Voxel Pattern Analysis
When analyzing fMRI data, it is important to take into account the activity of the voxels, as well as the interactions between them because the activity in neighboring voxels is interdependent. However, given the univariate nature of GLM,
the model fits to each voxel’s time-course separately. To
overcome this drawback, we used multi-voxel pattern analysis (MVPA) (Norman et al., 2006) which uses patternclassification techniques to extract the pattern of response
across multiple voxels.
We preprocessed the GLM analysis results and used them
as inputs for the MVPA. The preprocessing included linear
de-trending which removes any bias resulting from scanner
drift over the acquisition time, and z-scoring which normalizes the range of each voxel. We used leave-one-participantout cross-validation for MVPA, in which a classifier is trained
on eight participants’ data and then tested with the last participant’s data.
Previous studies have shown that the anterior insula (AI)
is activated when processing emotions (Kober et al., 2008;
Lamm & Singer, 2010), and the frontal pole (FP) is activated
when making a decision that affects the future (Okuda et al.,
2003). We therefore hypothesized that the agent’s negotiation
strategy can be predicted based on the participant’s brain activity in the FP, and the agent’s emotional expression can be
predicted based on the participant’s brain activity in the AI.
To test our hypothesis, we chose the AI and the FP as our regions of interest (ROIs), and performed ROI MVPA. In the
following section, we explain the ROI MVPA approach.
Region of Interest Multi-Voxel Pattern Analysis To find
the relationship between an agent’s expressed emotion and
brain activity as well as an agent’s negotiation strategy and
brain activity, we performed region of interest (ROI) MVPA
with both the AI and the FP. The AI on each side of the brain
can be divided into two subregions with distinct patterns of

connectivity: dorsal anterior insula (dAI), connected to the
dorsal anterior cingulate cortex; and the ventral anterior insula (vAI), connected to the pregenual anterior cingulate cortex (Deen, Pitskel, & Pelphrey, 2011). We ran ROI analyses for all four AI regions. On the contrary, the FP does not
have widely accepted subregions (Moayedi, Salomons, Dunlop, Downar, & Davis, 2014). Thus, we ran ROI analysis for
the whole FP labeled by the Harvard Center for Morphometric Analysis (Desikan et al., 2006).
To make sure brain activity in the AI or the FP is responsible either for agent’s emotional expressions or negotiation strategies, we ran ROI analyses for both conditions,
i.e., we calculated the prediction accuracy of agent’s negotiation strategies using both the AI and FP as ROIs. We assumed
that the prediction accuracy with their expected ROI would be
significantly higher than the chance level, but the prediction
accuracy with their unexpected ROI would be indistinguishable from chance.
We trained a linear Support Vector Machine (SVM) classifier using voxels from each of our ROIs separately using
feature selection. Feature selection is a common approach
used to reduce the number of features (voxels) by selecting
only relevant features as input to a classifier. Classification
performance improves with feature selection as it only picks
features that vary significantly between categories (Guyon &
Elisseeff, 2003). In our analyses, we used the GLM analysis
results to compute the F-score for each voxel, and then used
an analysis of variance (ANOVA) measure to select the top
10% of features with the highest F-scores.
Each participant’s brain was transformed into standard
MNI space (Evans et al., 1993) to have a brain that is more
representative of the population. After performing this process for all participants, individual-level analyses were combined for a group-level analysis.

Results
As discussed previously, the AI is a brain region known to
respond to emotional expressions, and it can be divided into
two subregions with distinct patterns of connectivity. Therefore, we first ran ROI MVPA for all four (left/right × ventral/dorsal) AI regions separately. The prediction accuracies
of emotional expression using our ROI MVPA with four AI
regions indicate that ROI MVPA with the L-dAI has the best
prediction accuracy (38.40%), while the prediction accuracy
of other AI regions are indistinguishable from the chance
level of 33%. A binomial test revealed that the prediction
accuracy of the ROI MVPA with the L-dAI is significantly
above chance (p = 0.0566). Therefore, we focus our analysis
on the results of ROI MVPA with the L-dAI.
The prediction accuracy for decoding an agent’s emotional
expressions is higher for the L-dAI decoder (38.40%, compared to 33.87% for the FP (p = 0.0960) or chance (p =
0.0566). The prediction accuracy using the FP ROI MVPA
was not different from chance (p = 0.4266). This supports
our hypothesis that an agent’s emotional expression can be

2410

Table 1: Prediction accuracy on an agent’s emotional expression and an agent’s strategy from ROI MVPA. One-tail binomial tests were performed for each condition compared to the
chance level, and significant results (p < 0.05) were marked
with (*).
Region of
Prediction Chance
Condition
Interest (ROI)
Accuracy
Level
All Emotions 38.40% (*)
Left Dorsal
Angry
45.07% (*)
Anterior Insula
33.33%
Neutral
29.60%
(L-dAI)
Sad
40.53% (*)
All Strategies 58.96% (*)
Frontal Pole
Conceder
47.50%
50.00%
(FP)
Non-conceder 70.42% (*)

reliably predicted using brain activity in the AI which is an
emotion-related brain region, but not with information in the
FP.
Similarly, the accuracies for predicting an agent’s negotiation strategy using the ROI MVPA with the FP was 58.96%.
A binomial test again confirmed that this performance is significantly higher than the chance level of 50% (p = 0.0085).
The prediction performance was almost at chance (52.71%)
for the L-dAI (p = 0.2581). This result validates our hypothesis that counterpart’s negotiation strategy can be predicted
based on brain activity in the FP, which is activated when people do active decision-making, but not with the insula, which
is involved in emotion processing.
In order to examine these results in more detail, we broke
down the predictions. Specifically, we analyzed the prediction accuracy for each agent’s emotional expression and negotiation strategy from ROI MVPAs with anterior insular regions and frontal lobe (Table 1). Our results indicate that
brain patterns in the L-dAI can predict angry and sad conditions but not neutral agent facial expressions. Also, brain
patterns in the FP can predict the non-conceder negotiation
strategy but not the conceder strategy.
Overall, our results confirm that negotiating with different
types of agents results in activity in different brain regions,
and these activity patterns can be used to further decode the
specific type of interaction agent.

Discussion
The neuroscience of human-agent interactions is a rarely
studied topic and the majority of studies treat the processes
by which various agent features affect human participants as a
black box. To answer the question of how these features interact with underlying neural mechanisms, we investigated brain
activity during human-agent interactions. More specifically,
participants engaged with virtual agents who showed three
different emotional expressions (angry, neutral and sad) and
used two different types of negotiation strategies (conceding
and non-conceding). Using a human-agent negotiation platform, participants interacted with virtual agents in an fMRI

scanner, and their brain activity during the interaction was
recorded. We hypothesized that an agent’s emotional expression could be predicted based on patterns in emotion-related
brain regions, and an agent’s negotiation strategy could be
predicted based on patterns in decision-making-related brain
regions. Therefore, we focused our analyses on the AI and
the FP, as previous studies have shown that AI is activated
when people engage in emotional tasks, and the FP is activated when people perform active decision-making tasks.
Our ROI MVPA results support our hypothesis; prediction
accuracy of an agent’s emotional expression based on brain
patterns in the L-dAI, and that of agent’s negotiation strategy
based on brain patterns in the FP are well above the chance
level. These results indicate that different features are likely
processed in different brain regions. Finding which information is processed in certain brain regions would allow us to
reliably decode the feature of the agent from users’ brain activity. More detailed analyses revealed that brain patterns in
the L-dAI could be used to predict angry (45.07%) and sad
(40.53%) conditions, but not the neutral condition (29.60%).
This indicates that there are clear differences in brain patterns
in the L-dAI between angry and sad conditions. We hypothesize that the reason why the patterns in this region failed to
predict the neutral condition is that the neutral facial expression is the default expression throughout the experiment. The
facial expression of the agent only changes when it morphs
into sad or angry. We plan to tackle this problem by only
showing the agent’s face during the decision making phase.
With regard to agent’s negotiation strategies, predictions
using brain patterns from the FP showed significantly higher
accuracy compared to the chance level (50%) for the nonconceding condition (70.42%), but not for the conceding condition (47.50%). We assume that this is because participants
expected to deal with a counterpart that acted like a conceding and fair agent, i.e. an agent who might start with a slightly
unfair offer but over time it makes adjustments toward a fair
offer. It is possible that the distinct patterns in the FP witnessed during negotiations with the non-conceding agent is
because this agent acts in a very greedy and tough way that is
not typical in social interactions. This could result in unique
patterns of activity in the FP.
While our sample size could be considered small, we
would like to note that sample size tends to be small in fMRI
studies. Also, it is worth mentioning that the probability of
finding the same effect as one found in the original experiment is not dependent on sample size, but dependent on p
value (Killeen, 2005). This is because large effect sizes produce significant results, even with small sample size.
In conclusion, our results indicate that there is a link between an agent’s emotional expression and brain activity in
the L-dAI, and also between an agent’s negotiation strategy
and brain activity in the FP. Even though the results are preliminary, our work sheds light on the links between certain
brain regions and different agent features. In future studies, we plan to continue investigating these links with other

2411

features such as voice tone and gestures, and hopefully over
time construct a map of the brain regions activated by various
agent features and compare these regions to human-human
interactions.

Acknowledgments
This research is supported by AFOSR FA9550-14-1-0364.

References
Anderson, S. W., Bechara, A., Damasio, H., Tranel, D., &
Damasio, A. R. (1999). Impairment of social and moral
behavior related to early damage in human prefrontal cortex. Nature neuroscience, 2(11).
Bickmore, T., Schulman, D., & Yin, L. (2010). Maintaining engagement in long-term interventions with relational
agents. Applied Artificial Intelligence, 24(6).
Cassell, J., & Thorisson, K. R. (1999). The power of a nod
and a glance: Envelope vs. emotional feedback in animated
conversational agents. Applied Artificial Intelligence, 13.
Castellano, G., Pereira, A., Leite, I., Paiva, A., & McOwan,
P. W. (2009). Detecting user engagement with a robot
companion using task and social interaction-based features.
In 2009 international conference on multimodal interfaces.
Das, R., Hanson, J. E., Kephart, J. O., & Tesauro, G. (2001).
Agent-human interactions in the continuous double auction. In International joint conference on artificial intelligence (Vol. 17).
Deen, B., Pitskel, N. B., & Pelphrey, K. A. (2011). Three
systems of insular functional connectivity identified with
cluster analysis. Cerebral Cortex, 21(7).
Dehghani, M., Carnevale, P. J., & Gratch, J. (2014). Interpersonal effects of expressed anger and sorrow in morally
charged negotiation. Judgment and Decision Making, 9(2).
Desikan, R. S., Ségonne, F., Fischl, B., Quinn, B. T., Dickerson, B. C., Blacker, D., . . . others (2006). An automated
labeling system for subdividing the human cerebral cortex
on mri scans into gyral based regions of interest. Neuroimage, 31(3).
Evans, A. C., Collins, D. L., Mills, S., Brown, E., Kelly, R., &
Peters, T. M. (1993). 3d statistical neuroanatomical models
from 305 mri volumes. In Nuclear science symposium and
medical imaging conference, 1993.
Grosz, B. J., Kraus, S., Talman, S., Stossel, B., & Havlin, M.
(2004). The influence of social dependencies on decisionmaking: Initial investigations with a new game. In Proceedings of the third international joint conference on autonomous agents and multiagent systems-volume 2.
Guyon, I., & Elisseeff, A. (2003). An introduction to variable
and feature selection. The Journal of Machine Learning
Research, 3.
Jurafsky, D., Ranganath, R., & McFarland, D. (2009). Extracting social meaning: Identifying interactional style in
spoken conversation. In Proceedings of human language
technologies: The 2009 annual conference of the north
american chapter of the association for computational linguistics.

Karacora, B., Dehghani, M., Krämer-Mertens, N., & Gratch,
J. (2012). The influence of virtual agents? gender and
rapport on enhancing math performance. In Proceedings of
the 34th annual meeting of the cognitive science society.
Killeen, P. R. (2005). An alternative to null-hypothesis significance tests. Psychological science, 16(5).
Kim, E., Dehghani, M., Kim, Y. K., Carnevale, P. J., &
Gratch, J. (2014). Effects of moral concerns on negotiations. Proceedings of the 36th Annual Meeting of the Cognitive Science Society.
Kober, H., Barrett, L. F., Joseph, J., Bliss-Moreau, E.,
Lindquist, K., & Wager, T. D. (2008). Functional grouping
and cortical–subcortical interactions in emotion: a metaanalysis of neuroimaging studies. Neuroimage, 42(2).
Lamm, C., & Singer, T. (2010). The role of anterior insular
cortex in social emotions. Brain Structure and Function,
214(5-6).
Lerner, J. S., Li, Y., Valdesolo, P., & Kassam, K. S. (2015).
Emotion and decision making. Psychology, 66.
Maldonado, H., Lee, J.-E. R., Brave, S., Nass, C., Nakajima,
H., Yamada, R., . . . Morishima, Y. (2005). We learn better
together: enhancing elearning with emotional characters.
In Proceedings of th 2005 conference on computer support
for collaborative learning.
Moayedi, M., Salomons, T. V., Dunlop, K. A., Downar, J., &
Davis, K. D. (2014). Connectivity-based parcellation of the
human frontal polar cortex. Brain Structure and Function.
Norman, K. A., Polyn, S. M., Detre, G. J., & Haxby, J. V.
(2006). Beyond mind-reading: multi-voxel pattern analysis
of fmri data. Trends in cognitive sciences, 10(9).
Okuda, J., Fujii, T., Ohtake, H., Tsukiura, T., Tanji, K.,
Suzuki, K., . . . Yamadori, A. (2003). Thinking of the future and past: The roles of the frontal pole and the medial
temporal lobes. Neuroimage, 19(4).
Rafaeli, A., & Sutton, R. I. (1987). Expression of emotion
as part of the work role. Academy of management review,
12(1).
Sanfey, A. G., Rilling, J. K., Aronson, J. A., Nystrom,
L. E., & Cohen, J. D. (2003). The neural basis of economic decision-making in the ultimatum game. Science,
300(5626).
Smith, S. M., Jenkinson, M., Woolrich, M. W., Beckmann,
C. F., Behrens, T. E., Johansen-Berg, H., . . . others (2004).
Advances in functional and structural mr image analysis
and implementation as fsl. Neuroimage, 23.
Van Kleef, G. A., De Dreu, C. K., & Manstead, A. S. (2004).
The interpersonal effects of anger and happiness in negotiations. Journal of personality and social psychology, 86(1).
Wang, N., & Gratch, J. (2009). Can virtual human build
rapport and promote learning? In Artificial intelligence in
education conference.
Zaki, J., Davis, J. I., & Ochsner, K. N. (2012). Overlapping
activity in anterior insula during interoception and emotional experience. Neuroimage, 62(1).

2412

