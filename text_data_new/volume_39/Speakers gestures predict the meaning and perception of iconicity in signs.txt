Speakers’ gestures predict the meaning and perception of iconicity in signs
Gerardo Ortega1,2 (gerardo.ortega@mpi.nl), Annika Schiefner1 (a.schiefner@student.ru.nl), &
Aslı Özyürek1,2,3 (asli.ozyurek@mpi.nl)
1

Centre for Language Studies, Radboud University,
Max Planck Institute for Psycholinguistics, and 3Donders Centre for Cognition
Wundtlaan 1, 6525XD Nijmegen, The Netherlands

2

Abstract
Sign languages stand out in that there is high prevalence of
conventionalised linguistic forms that map directly to their
referent (i.e., iconic). Hearing adults show low performance
when asked to guess the meaning of iconic signs suggesting
that their iconic features are largely inaccessible to them.
However, it has not been investigated whether speakers’
gestures, which also share the property of iconicity, may
assist non-signers in guessing the meaning of signs. Results
from a pantomime generation task (Study 1) show that
speakers’ gestures exhibit a high degree of systematicity, and
share different degrees of form overlap with signs (full,
partial, and no overlap). Study 2 shows that signs with full
and partial overlap are more accurately guessed and are
assigned higher iconicity ratings than signs with no overlap.
Deaf and hearing adults converge in their iconic depictions
for some concepts due to the shared conceptual knowledge
and manual-visual modality.
Keywords: iconicity; gesture; sign language; embodied
cognition

Introduction
A question that has puzzled psychologists and linguists for
decades is to what extent sign iconicity is accessible to
individuals with no knowledge of a sign language. Iconicity,
defined as the direct relationship between a linguistic form
and its referent, is a ubiquitous property of sign languages
observable at many of their linguistic levels of organisation
(Cuxac, 1999; Perniss, Thompson, & Vigliocco, 2010;
Pietrandrea, 2002). Sign-naïve adults can accurately guess
the meaning of only a small proportion of signs (Griffith,
Robinson, & Panagos, 1981; Grosso, 1993; Klima &
Bellugi, 1979; Pizzuto & Volterra, 2000), but it has been
hard to establish what factors allow them to map certain
features of a sign to its correct referent. In an attempt to
shed light on this question, we look at the iconic gestures
produced by hearing non-signers. Given that iconic gestures
are expressed through the same (manual-visual) modality,
and importantly, they also share the property of iconicity
(Kendon, 2004; McNeill, 1992), we entertain the hypothesis
that non-signers may rely on their own gestural repertoire to
make form-meaning judgements about signs.

Iconicity in gesture and sign
Gestures are a fundamental aspect of human communication
and are present in all ages and cultures (Kendon, 2004;
McNeill, 1992). Gestures are holistic units highly integrated
with speech that together convey unified semantic
information of a multimodal utterance (Kelly, Creigh, &

Bartolotti, 2010; McNeill, 1992). Sign languages, in
contrast, occur independently from speech, and critically,
they have the same levels of linguistic organisation as those
reported in spoken languages (Sandler & Lillo-Martin,
2006).
One point of intersection between sign and gesture is
iconicity. Speakers can depict through iconic gestures the
visual form of a concept and integrate them with speech as
part of a multimodal message. For instance, when a speaker
says ‘I’ll be outside’ while producing the gesture of smoking
it is clear to the interlocutor that she is going for a cigarette.
On the other hand, a large proportion of a signed lexicon has
iconic motivation (Pietrandrea, 2002), and crucially, signs
may have overlapping structures as gestures (e.g., the sign
TO-SMOKE depicts a person smoking a cigarette).
The similarities between sign and gesture were
overlooked for many decades, but in recent years scholars
have begun systematically comparing both modes of manual
communication to shed light on their differences and
similarities (e.g., Cormier, Schembri, & Woll, 2013;
Goldin-Meadow & Brentari, 2015; Perniss, Özyürek, &
Morgan, 2015; Quinto-Pozos & Parrill, 2015). Given the
growing body of evidence showing that gestures and signs
share more forms and functions than previously assumed
(arguably due to the shared manual-visual modality)
(Perniss et al., 2015), we investigate whether non-signing
adults fall back on their own gestural repertoire to make
judgements about conventionalised signs. The aim of the
present study is therefore to investigate whether the overlap
in form between signs (i.e., linguistic structure) and gestures
(i.e., iconic depictions) predicts non-signers’ ability to guess
the meaning of signs and assign iconicity ratings.

Perception of sign iconicity
Iconicity and the extent to which sign-naïve adults can
understand the meaning of iconic signs has been a central
focus of attention in sign research. The first investigations
on the topic demonstrated that iconicity is not easily
accessible to non-signers and that the meaning of signs is
very difficult to access. In their seminal study, Klima and
Bellugi (1979) asked hearing adults without any knowledge
of a sign language to guess the meaning of a set of signs.
When signs were presented in isolation and when they had
to select the correct meaning out of five plausible
candidates, participants showed a very low success rate (less
than 10%). They showed significant improvement, however,
when they were presented the sign along with its English
translation, and were asked to explain the iconic relationship

889

between the sign and its meaning. Participants showed
overall agreement in that they were able to accurately
describe the iconic motivation of more than 50% of the
signs (e.g., most participants agreed that the sign VOTE
depicted a person putting a ballot in a box). This study set a
benchmark in sign language research and convincingly
argued that iconicity is difficult to access by hearing nonsigners and that the notion of iconicity is better understood
as a property that lies in a continuum with the meaning of
some signs being more transparent than others.
Another study highlights the possibility that similarities
between signs and the gestures used by the hearing
community may assist sign-naïve participants in guessing
the meaning of signs. Grosso (1993) showed a set of iconic
and arbitrary signs in Italian Sign Language (LIS) to hearing
non-signing adults and asked them to guess their meaning.
Participants could not provide a correct response for a large
proportion of signs (76%) but they were very accurate for a
considerable number of items on the list (24%). A detailed
analysis of the correctly guessed items revealed that these
signs resemble the emblems commonly used by Italian
speakers (e.g., the sign GOOD has the same form and
meaning as the emblem used by hearing Italians). Emblems
have a conventionalised, culture-specific form and meaning
(Kendon, 1995, 2004) so when non-signing adults are
confronted by signs that overlap in structures, they rightly
assume that they also share the same meaning. This study is
one of the first to suggest that non-signers’ ability to guess
the meaning of signs is based on the structural similarities
between conventionalised (linguistic) signs and the gestures
produced by the surrounding speaking community.
A limitation of this study is that it presupposes that only
emblems facilitate the accurate guessing of the meaning of
signs but does not say how other types of gestures may also
be recruited. Emblems have highly conventionalised hand
configurations, are used for specific pragmatic purposes
(Kendon, 1995, 2004), and have mental representations akin
to those of abstract words (Gunter & Bach, 2004), so they
are retrievable gestural entities that can be compared with
convetionsalised signs. However, other types of iconic
gestures may also be used as a basis to make judgments
about the meaning of signs. In this study, we turn to the
systematic iconic gestures shared in a community of
speakers to investigate how overlap in form with
conventionalised
signs
influences
meaning-based
judgements about signs.

For instance, it has been found that the iconic co-speech
gestures used in object descriptions are highly systematic
and their form depends on the physical properties of the
referent (Masson-Carro, Goudbeek, & Krahmer, 2015).
Objects that can be manipulated with the hands (e.g., a pen)
are represented with gestures mimicking how the object is
held; while objects with low manipulability affordances
(e.g., a sink) are represented through gestures outlining their
shape. A striking degree of systematicity has also been
reported in elicited silent gestures (i.e., pantomimes). When
asked to express concepts in pantomime, participants tend to
systematically differentiate actions from tools through
distinct gestural forms (i.e., re-enactment of bodily
movements for verbs and handshapes representing the form
of objects for nouns) (Padden et al., 2013; Padden, Hwang,
Lepic, & Seegers, 2015). More recently, high degree of
systematicity in the structure of pantomimes has also been
found across different semantic domains and for
geographically unrelated cultures. Ortega and Özyürek
(2016) elicited pantomimes from Dutch and Mexican adults
and found that both groups employ remarkably similar
strategies to depict referents. Through the implementation of
specific types of iconic representations and their
combinations, participants systematically represent concepts
across different semantic domains. These pantomimes bare
strong resemblance with the structures of recently
discovered sign languages (Safar & Petatillo, in
preparation), so it has been argued that pantomimes reveal
some of the cognitive dispositions that give rise to a signed
lexicon in emerging sign languages.
The relevance of these studies is two-fold: first, they
demonstrate that iconic gestural depictions are not as
variable as previously assumed, but rather are deployed
systematically to represent concrete concepts within specific
semantic domains. Second, such systematicity results in
shared knowledge about some manual forms across a
community of speakers. As a consequence, individuals are
likely to have expectations of how a concept should be
represented in the manual-visual modality – at least for a set
of referents. This has important implications for the
perception of sign iconicity by non-signers. Non-signing
adults confronted by conventionalised signs for the first
time will not make judgements about their meaning in a
vacuum. Rather, they are likely to fall back on their gestural
knowledge to make judgments about the meaning of iconic
signs.

Systematicity in iconic gestures

The Present Study

The form of iconic gestures has been assumed to be
variable, with their structure depending on the context in
which they are used, the interlocutor, and the
communicative intent of the speaker. It has been assumed
that individuals tailor their gestures to the main focus of a
conversation and as a result they vary in form and meaning
from one conversation to the next (Müller, 2013). However,
recent studies have found that contrary to this received
knowledge, the iconic gestures produced by hearing adults
exhibit a high degree of systematicity, and tend to represent
very similar forms across individuals.

Based on evidence that many iconic gestures are highly
systematic across individuals (Masson-Carro et al., 2015;
Ortega & Özyürek, 2016; Padden et al., 2013, 2015; van
Nispen, van de Sandt-Koenderman, Mol, & Krahmer, 2014)
it is possible to assume that non-signing adults have at their
disposal a cohort of shared gestures with specific forms and
meanings on which they may base their judgment about
signs. In order to test this hypothesis, we carried out two
studies. In Study 1 we elicited pantomimes from nonsigning adults to determine which gestures were the most
systematic across participants. Once these pantomimes were

890

selected, we compared them to signs from Sign Language of
the Netherlands (NGT) and looked for signs that overlapped
in form to different degrees (full, partial, or no overlap).
These signs served as stimulus materials for Study 2. In this
study, a different group of participants were presented with
the signs and were asked, first, to guess their meaning. After
they gave their response, they were given the correct
translation, and then were asked to give iconicity ratings.
The prediction is that when signs map directly to their
gesture non-signing adults will be more accurate at guessing
their meaning and will assign higher iconicity ratings (e.g.,
the gesture and the NGT sign TO-SMOKE represent a
person smoking a cigarette so participants are likely to be
very accurate and give high iconicity ratings). The expected
results will lend credence to the hypothesis that sign-naïve
adults base their responses not only on their emblems
(Grosso, 1993), but also on other types of (iconic) gestures
that are systematic within a community.

Methodology
Study 1: Pantomime generation task
Participants

one parameter differed from the gesture (Figure 1B). 3) No
overlap (N=54): signs in which two or more parameters
differed. This category consisted of 27 signs that did not
overlap at all with the elicited gesture, plus an additional 27
signs for which no default gesture could be established
(Figure 1C). These three groups of NGT signs (N=146)
were the stimuli for Study 2.

Study 2: Open-cloze and iconicity rating
Participants
The participants of this study were a different group of 20
hearing native speakers of Dutch (14 female, mean age =
21.8 years) with no knowledge of NGT or any other sign
language. None of them took part in the pantomime
generation task.

Stimuli
The stimuli consisted of videos of the 146 NGT selected
from Study 1 (i.e., signs with full, partial, and no overlap
with gesture). Videos were produced by a deaf signer with
neutral face and without mouthings to avoid giving away
cues about the meanings of the signs.

Twenty native speakers of Dutch (10 females, age range:
21-46, mean: 27 years) living in the area of Nijmegen, the
Netherlands, took part in the study.

Procedure
Participants were seated in front of a computer and were
asked to produce a gesture that conveyed exactly the same
meaning as the word on the screen. They were explicitly
told that it was not allowed to speak or to point to any object
in the room and that they could say ‘pass’ if they were
unable to generate a pantomime. Two cameras were
positioned on each side of the participant to record their
gestural productions. Trials started with a fixation cross
(500 ms) followed by the target word (4000 ms) time during
which they had to produce their gesture. After the 4000 ms
ended, the next trial began. The motivation behind this strict
timing was to elicit participants’ most intuitive response.
Participants were presented a total of 273 words.
Pantomimes were coded according to the description
parameters proposed by Bressem (2013), which are based
on the phonological parameters handshape, location, and
movement of sign languages. Based on these features, we
looked at the gestures that exhibited the same structure
across a large number of participants. If the same gesture
was produced by at least 12 out of 20 participants, it was
considered the default gesture for that concept. These
resulted in a total of 119 pantomimes that were consistent
across a large proportion of the pool of participants (mean
number of participants producing the same pantomime:
15.14).
These default gestures were compared to their NGT
translation on each phonological parameter (i.e., handshape,
location and movement) to select items with different
degrees of form overlap. This comparison resulted in three
categories of signs. 1) Full overlap (N=36): gesture-sign
pairs did not differ in any parameter (Figure 1A). 2) Partial
overlap (N=56): this category includes signs in which only

Figure 1: Examples of sign-gesture pairs with different
degrees of overlap. A) TO-CUT shares all the components
(handshape, location, movement) between sign and gesture.
B) TO-SAW differs in only one parameter (handshape). C)
In LAPTOP, sign and gesture have no overlap.

Procedure
At the beginning of each trial, an NGT sign in citation form
was presented. After the video had played in full and
disappeared from the screen, a new screen was presented
instructing participants to guess the meaning of the sign and
write its meaning in one word (typed). Participants were
required to type in an answer for every item but they were
also allowed to skip items if they could not come up with a

891

meaning. After participants had entered an answer, a new
screen of instructions came up. Here participants were given
the actual meaning of the sign and were asked to judge how
well the sign represented its meaning. The sentence read:
‘The meaning of the sign is [translation equivalent]. How
much does the sign look like its meaning?’ The screen
displayed a 7-point Likert and participants were required to
type in their rating (1 representing the lowest similarity and
7 the highest).

Analysis
Participants gave a response for a large proportion of the
signs with passes representing only 6.5% of responses.
Despite being instructed to write only one word, many
responses were phrases, but they were still included in the
analysis. Based on the Dutch version of the Boston Naming
Task (Roomer, Hoogerwerf, & Linn, 2011), answers were
coded as correct and incorrect. Answers were coded as
correct if they matched exactly the expected answer (e.g.,
sign: TO-PULL; response: to pull) or if they were synonyms
of each other (e.g., sign: TO-PHONE; response: to ring).
This category also included answers that were not the same
part of speech as the target sign, but where the answer was
specific to the target concept (e.g., sign: TO-PHONE;
response: telephone)1. We also included phrases containing
a verb and the correct argument depicted in the sign (e.g.,
sign: BANANA; response: to peel a banana). Responses
that did not fit into any of these categories were classed as
incorrect answers.
Incorrect answers were subdivided into responses that
were semantically related and unrelated to the sign.
Semantically related answers included responses that
belonged to the same semantic domain (e.g., sign: DUCK;
response: penguin); as well as answers that were lacking the
appropriate abstraction to the target concept (e.g., the sign
MONKEY, which re-enacts how a primate scratches the
sides of its torso, was often labelled as scratching).
The semantically unrelated category included responses
that were plainly wrong, or answers derived from visual
information of the sign, but that had no relationship with the
concept (e.g., the sign MOUNTAIN describes the outline of
two horizontal bumps, but it was often interpreted as a
camel).
For the open cloze, the proportions of correct,
semantically related, and semantically unrelated answers
were calculated for every item, thereby collapsing across
participants' answers. Missing answers were discarded for
this analysis and did not contribute to the proportions. For
the iconicity ratings, all values were averaged across
participants to obtain the mean ratings for each of the 146
signs.

1
In Dutch, verb/noun distinctions are differentiated through
affixes to the root. For example, telefoneren (to phone) is a verb
and telefoon (telephone) is a noun. The English translations do not
reflect that participants responded with a single word.

Results
Performance on the open cloze was highly variable across
participants and items. While only nine items (6.2%) were
correctly identified by all participants, half of the signs (73
signs) were correctly identified by at least 25% of
participants. For 26 items (17.8%), all answers were
semantically related to the target meaning, suggesting that
participants were able to correctly identify some aspect of
the sign but did not make the full abstraction to the target
meaning (e.g., sign: TO-FLY; response: bird). Regarding
the iconicity ratings, participants were able to give a
response for all items. In order to establish to what extent
sign-gesture overlap contributes to guessing the meaning of
a sign and assign iconicity ratings, we considered the
following variables in the statistical analysis.
Independent variable: Degree of overlap (full, partial, and
no overlap)
Dependent variables:
i. Proportion of correct answers (open cloze)
ii. Proportion of semantically related answers (open cloze)
iii. Proportion of semantically unrelated answers (open
cloze)
iv. Mean iconicity rating
A multivariate ANOVA was run to determine the
relationship between type of gestural overlap (full, partial
and, no overlap) and the dependent variables of the open
cloze and the iconicity ratings. Using Pillai's Trace we
found a significant overall effect of the degree of overlap, V
= 0.541, F(6,230)=14.205, η2= .27, p < .001. The following
sections will describe the between-subjects effects for each
dependent variable.
i) Turning to the proportion of correct answers in the
open cloze, tests of between-subjects revealed a significant
effect of degree of overlap, F(2,116)=24.168, η2= .194, p <
.001. Planned contrasts revealed an increase of correct
answers from no overlap items (M = 0.12, SE = .03) to
partial overlap (M = 0.46, SE = .05, Δ = -0.31, SEΔ = .06, p
<.001, BCa 95% CI [-0.45, -0.18]), but no significant
difference between partial and full overlap (M = 0.61, SE =
.06, p = .209). The proportion of correctly identified items
was thus higher for items with full and partial overlap than
for those with no overlap (Figure 2).
ii) Regarding the proportion of incorrect answers that
were semantically related to the sign, a test of betweensubjects effects revealed no significant effect of the degree
of overlap between gestures and signs, p = .305. That is,
wrong answers in the open cloze were equally distributed
across the three types of signs (full, partial, and no overlap).
iii) Turning to the proportion of incorrect answers that
were semantically unrelated to the target concept, tests of
between-subjects effects revealed a significant effect of the
degree of overlap, F(2,116)=26.909, η2= .317, p < .001.
Signs with no overlap were significantly less likely to be
guessed correctly (M =0.75, SE = .05) than those with
partial overlap (M = 0.41, SE = .05, Δ = 0.34, BCa 95% CI
[0.21, 0.47], p < .001). Signs with full overlap were

892

significantly more likely to be guessed accurately than signs
with partial overlap (M = 0.21, SE = .04, Δ = 0.192, BCa
95% CI [0.05, 0.33], p =.009). In other words, the less
similar a sign is from a gesture, the more likely it is to be
guessed inaccurately.
iv) When we look at iconicity ratings, we found an
association with the degree of overlap between sign and
gesture F(2,111.836)=54.13, η2=.483, p < .001. Planned
contrasts revealed a significant increase of mean iconicity
ratings from no overlap (M = 3.18, SE = 0.22) to partial
overlap (M = 5.34, SE = .17, Δ = -2.13, BCa 95% CI [2.617, -1.642], p < .001) but not from partial to full overlap
(M = 5.92, SE = .15, p = .07). These results suggest that
when signs have greater overlap in form with their gestures
they perceive signs as more iconic (see Figure 3).

Figure 2: Mean proportion of correctly guessed answers
as a function of gesture overlap with the target sign

partial overlap. This suggests that despite their slight
structural differences, these two types of signs bear enough
resemblance to participants’ gestures to make an association
between them.
Signs and gestures share the same physical constraints to
express a concept in the manual modality, with the referent
shaping to some extent the features than can be expressed
with the hands (Masson-Carro et al., 2015). It is therefore
not surprising that signs and gestures converge in the
strategies to depict the visual characteristics of many
concepts. If signs and gestures have similar structures for
some concepts, it means that deaf and hearing adults share
conceptual knowledge about these concepts (i.e., visual,
semantic, perceptual, sensorimotor representations). When
there is sufficient overlap between signs and gesture, nonsigning adults may tap into these schemas to make
judgements about the meaning of signs. These findings also
relate to research showing that humans – as well as other
primates – understand and evaluate the correctness of
others’ actions through the activation of brain regions
engaged when they perform the same actions themselves
(Koelewijn, van Schie, Bekkering, Oostenveld, & Jensen,
2008; Rizzolatti, Fadiga, Gallese, & Fogassi, 1996).
The errors produced by participants, however, clearly
show that if gesture and sign mismatch, or if the meaning of
signs departs slightly from the features they depict,
participants are unable to estimate accurately the meaning of
a sign. As a result, they will also rate the sign as less iconic.
Non-signers have a very limited scope to assign meanings to
signs and seem to be inclined to describe only what is
directly encoded in them. While they are capable of
extracting some visual information from the signs they often
fail to respond with the correct metonymic associate (e.g.,
they respond scratch instead of monkey). This goes to show
that despite their similarities, sign languages have
established linguistic conventions not shared with gestures
and thus are inaccessible to non-signing adults.
This study adds to the body of research investigating how
modality shapes linguistic/communicative structures
(Perniss et al., 2015).

Acknowledgments

Figure 3: Mean iconicity ratings as a function of gesture
overlap with the target sign

This work was supported by a Veni grant by the
Netherlands Organisation for Scientific Research (NWO)
awarded to the first author. We would like to thank the
members of the Multimodal Language and Cognition lab at
Radboud University for their helpful comments, in
particular to Swen Schreiter, Linda Drijvers, and James
Trujillo for their support.

Discussion
These data expands on previous research by showing that
the gestural repertoire of non-signing adults is recruited to
make judgments about the meaning of lexical signs. We
showed that signs that overlap in form with their gestures
are guessed more accurately and are judged as more iconic.
The proportion of correct answers and iconicity ratings were
higher for signs that overlapped in form with gestures, but
there was no additional improvement between full and

References
Cormier, K., Schembri, A., & Woll, B. (2013). Pronouns
and pointing in sign languages. Lingua, 137, 230–247.
http://doi.org/10.1016/j.lingua.2013.09.010
Cuxac, C. (1999). French sign language: proposition of a
structural explanation by iconicity. Gesture-Based
Communication in Human-Computer, 1739(1), 165–
184.
Goldin-Meadow, S., & Brentari, D. (2015). Gesture, sign

893

and language: The coming of age of sign language
and gesture studies. Behavioral and Brain Sciences,
8(4),
1–82.
http://doi.org/10.1017/S0140525X15001247.
Griffith, P. L., Robinson, J. H., & Panagos, J. M. (1981).
Perception of iconicity in American sign language by
hearing and deaf subjects. The Journal of Speech and
Hearing
Disorders,
46(4),
388–97.
http://doi.org/dx.doi.org/10.1044/jshd.4604.388
Grosso, B. (1993). Iconicity and Arbitrariness in Italian
Sign Language: An Experimental Study. University of
Padua, Italy.
Gunter, T. C., & Bach, P. (2004). Communicating hands:
ERPs elicited by meaningful symbolic hand postures.
Neuroscience Letters, 372(1), 52–6.
Kelly, S. D., Creigh, P., & Bartolotti, J. (2010). Integrating
speech and iconic gestures in a Stroop-like task:
evidence for automatic processing. Journal of
Cognitive
Neuroscience,
22(4),
683–694.
http://doi.org/10.1162/jocn.2009.21254
Kendon, A. (1995). Gestures as illocutionary and discourse
structure markers in Southern Italian conversation.
Journal
of
Pragmatics,
23(3),
247–279.
http://doi.org/dx.doi.org/10.1016/03782166(94)00037-f
Kendon, A. (2004). Gesture: Visible action as utterance.
Cambridge: Cambridge University Press.
Klima, E., & Bellugi, U. (1979). The Signs of Language.
Harvard: Harvard University Press.
Koelewijn, T., van Schie, H. T., Bekkering, H., Oostenveld,
R., & Jensen, O. (2008). Motor-cortical beta
oscillations are modulated by correctness of observed
action.
NeuroImage,
40(2),
767–775.
http://doi.org/10.1016/j.neuroimage.2007.12.018
Masson-Carro, I., Goudbeek, M., & Krahmer, E. (2015).
Can you handle this? The impact of object affordances
on how co-speech gestures are produced. Language,
Cognition
and
Neuroscience,
1–11.
http://doi.org/10.1080/23273798.2015.1108448
McNeill, D. (1992). Hand and mind: What gestures reveal
about thought. Chicago: University of Chicago Press.
Müller, C. (2013). Gestural modes of representation as
techniques of depcition. In C. Müller, A. Cienki, S.
Ladewig, D. McNeill, & J. Bressem (Eds.), Body Language - Communication: An International
Handbook on Multimodality in Human Interaction
(pp. 1687–1701). Berlin: De Gruyter Mouton.
Ortega, G., & Ozyürek, A. (2016). Generalisable patterns of
gesture
distinguish
semantic
categories
in
communication without language. In A. Papafragou,
D. Grodner, J. Mirman, & J. Trueswell (Eds.),
Proceedings of the 38th Annual Meeting of the
Cognitive Science Society (pp. 1182–1187). Austin,
TX: Cognitive Science Society, Inc.
Ortega, G., & Özyürek, A. (2016). Generalisable patterns of
gesture
distinguish
semantic
categories
in
communication without language. In A. Papafragou,
D. Grodner, D. Mirman, & J. Trueswell (Eds.),
Proceedings of the 38th Annual Meeting of the

Cognitive Science Society (CogSci 2016) (pp. 1182–
1187). Austin, TX.: Cognitive Science Society, Inc.
Padden, C., Hwang, S.-O., Lepic, R., & Seegers, S. (2015).
Tools for Language: Patterned Iconicity in Sign
Language Nouns and Verbs. Topics in Cognitive
Science,
7(1),
81–94.
http://doi.org/10.1111/tops.12121
Padden, C., Meir, I., Hwang, S.-O., Lepic, R., Seegers, S., &
Sampson, T. (2013). Patterned iconicity in sign
language lexicons. Gesture, 13(3), 287–305.
Perniss, P., Özyürek, A., & Morgan, G. (2015). The
influence of the visual modality on language structure
and conventionalization: Insights from sign language
and gesture. Topics in Cognitive Science, 7(Special
Issue), 2–11.
Perniss, P., Thompson, R. L., & Vigliocco, G. (2010).
Iconicity as a general property of language: evidence
from spoken and signed languages. Frontiers in
Psychology,
1(227),
1664–1678.
http://doi.org/dx.doi.org/10.3389/fpsyg.2010.00227
Pietrandrea, P. (2002). Iconicity and arbitrariness in Italian
Sign Language. Sign Language Studies, 2(3), 296–
321. http://doi.org/dx.doi.org/10.1353/sls.2002.0012
Pizzuto, E., & Volterra, V. (2000). Iconicity and
transparency in Sign Languages: A cross-linguistic
cross-cultural view. In K. Emmorey & H. L. Lane
(Eds.), The signs of language revisited: An anthology
to Honor Ursula Bellugi and Edward Klima (pp. 229–
250). Mahwah, N. J.: Lawrence Erlbaum Associates.
Quinto-Pozos, D., & Parrill, F. (2015). Signers and Cospeech Gesturers Adopt Similar Strategies for
Portraying Viewpoint in Narratives. Topics in
Cognitive
Science,
7(Special
Issue),
1–23.
http://doi.org/10.1111/tops.12120
Rizzolatti, G., Fadiga, L., Gallese, V., & Fogassi, L. (1996).
Premotor cortex and the recognition of motor actions.
Cognitive
Brain
Research,
3(2),
131–141.
http://doi.org/10.1016/0926-6410(95)00038-0
Roomer, E. K., Hoogerwerf, A. C., & Linn, D. E. (2011).
Boston benoem taak 2011. Utrecht.
Safar, J., & Petatillo, R. (n.d.). Strategies of noun-verb
distinction in Yucatec Maya Sign Languages. In O. Le
Guen, J. Safar, & M. Coppola (Eds.), Emerging Sign
Languages of the Americas. Sign Language Typology
Series. Berlin: Mouton de Gruyter.
Sandler, W., & Lillo-Martin, D. (2006). Sign Language and
Linguistic Universals. Cambridge: Cambridge
University Press.
van Nispen, K., van de Sandt-Koenderman, M., Mol, L., &
Krahmer, E. (2014). Pantomime Strategies: On
Regularities in How People Translate Mental
Representations into the Gesture Modality. In
Proceedings of the 36th Annual Conference of the
Cognitive Science Society (CogSci 2014) (pp. 3020–
3026). Austin, TX: Cognitive Science Society, Inc.

894

