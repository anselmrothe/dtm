Adaptability and Neural Reuse in Minimally Cognitive Agents
Matthew Setzler (msetzler@indiana.edu)
Cognitive Science Program, Indiana University
Bloomington, IN 47408 USA

Eduardo J. Izquierdo (edizquie@indiana.edu)
Cognitive Science Program, School of Informatics and Computing, Indiana University
Bloomington, IN 47408 USA
Abstract
Cognitive agents are continuously faced with new problems.
To facilitate adaptation, emerging theories of neural reuse propose that evolution might often favor re-purposing existing
brain structures for new functions. This paper presents a novel
approach to the study of neural reuse based on the evolution
of simulated agents in an object-categorization task. We artificially evolve populations of dynamic neural networks to perform two variants of a categorization task that alternate over
evolutionary time. We find that populations become increasingly adaptive over repeated exposures to the tasks. Analysis
of evolved networks reveals two types of equally-fit solutions:
one that is specialized to a given task variant and does not adapt
to changes easily; and another that is more general, in that it
can adapt to the other task with minimal change to its structure.
Interestingly, we find that populations exposed to alternating
tasks spontaneously locate the latter type of structures.
Keywords: neural networks; minimally-cognitive behaviors;
neural reuse; artificial evolution; evolvability

Introduction
A central goal of cognition is adaptation. Cognitive agents
are continuously faced with new problems and it is in their
best interest to reuse pre-existing solutions from prior problems wherever possible. Many lines of research in cognitive science are motivated by this broad notion of adaptation. An emerging class of theories suggest neural reuse
is a fundamental organizing principle of adaptation in the
brain (Anderson, 2007; Gallese, 2008; Dehaene, 2005; Hurley, 2008). According to these theories, neural circuits established for one purpose are often put to different use without
losing entirely their original functions. Different versions of
this theory have been proposed, some that operate over the developmental timescale (Dehaene, 2005) and others over evolutionary timescales (Anderson, 2007). These theories are
supported by a wealth of empirical evidence (see Anderson,
2010, for more details). However, a grounded account of how
reuse might occur remains poorly understood. Computational
models in this area have tended to focus on disembodied cognitive architectures (Braylan, Hollenbeck, Meyerson, & Miikkulainen, 2016), largely detached from the increasingly accepted viewpoint that cognition is distributed over a dynamically coupled brain-body-environment system (Chiel & Beer,
1997).
The work presented here takes a different approach to
studying adaptation and neural reuse over the evolutionary
timescale. We study adaptation in a computational model of
dynamic embodied agents evolved to perform two variants

of a visual discrimination task. Specifically, we artificially
evolve populations of dynamic neural networks to perform
two closely-related variants of a visual discrimination task
that alternate over evolutionary time. To the extent that adaptability is achieved, we should observe populations adapting
quicker to changes in the task requirement over the course of
an evolutionary run. The degree to which neural reuse occurs will be evident in the evolved neural structures, which
are readily analyzable after evolution.
Our approach extends beyond the scope of other work in
two important ways. First, studying reuse in a deliberately
simple computational model gives us a tractable and fully
transparent system which can be analyzed to yield a grounded
account of how adaptability and neural reuse play out in
an embodied and situated context. Secondly, unlike other
computational approaches which employ evolutionary frameworks specifically designed to facilitate neural reuse (Stanley
& Miikulainen, 2002), we use a standard genetic algorithm,
thus minimizing assumptions about the form of neural reuse
by seeing how it might occur spontaneously as a result of implicit pressures imposed by changing task requirements.
In this paper, we aim to show adaption via neural reuse
over the evolutionary timescale in an idealized computational
model of dynamic, embodied agents performing two variants of a previously studied visual discrimination task (Beer,
1996, 2003). Insofar as model-agents exhibit hallmarks of
cognition, this is of interest in and of itself. Additionally, insight gained in this idealized computational setting will provide a proof of principle for how reuse might occur in naturalistic (embodied/situated) settings, which will help inform
the conceptual and analytical tools used to study this phenomenon in living systems.

Methods
Agent, Environment, and Task
In previous work (Beer, 1996, 2003), model agents were
evolved that could “visually” discriminate between objects
of different shapes, catching some while avoiding others. All
details of the agent, environment, and task have been adapted
from these previous studies.
The agent lives on the floor of a 275 x 400 rectangle (Figure 1A). It has a circular body with a diameter of 30 and an
“eye” which consists of 7 vision rays evenly distributed over
an angle of π/6. These rays extend out from the agents body
with a maximum range of 220. There are two kinds of objects

3119

A

B

to produce evolved examples of categorical perception (Beer,
2003). For the purpose of our study, there are two variants
of the object discrimination task. In Task A, agents must distinguish between circles and line objects by reliably moving
towards the former and away from the latter. In Task B, the
agents must do the opposite: move towards line objects and
avoid circles. The tasks were chosen because of how closely
related they are to one another.

Evolutionary Swapping
Figure 1: Agent and environment, adapted from (Beer, 1996).
[A] Agent moves horizontally while an object falls towards
it from above. Object can be one of two shapes: circle or
line segment. Agent’s sensory apparatus consists of an array
of seven distance sensors (dashed lines). [B] Neural architecture. Distance sensors (black) project to a layer of fully
interconnected interneurons (gray, recurrent connections not
shown), which in turn project to the two motor neurons (light
gray).
in the world: circles (with a radius of 15) and horizontal line
segments (of length 30). These objects fall from a height of
275 at some initial horizontal offset with respect to the agent.
Objects fall with a constant vertical velocity of -3 and no horizontal motion. If an object intersects a ray within this range,
an external input is fed to a corresponding sensory neuron.
The value of the input is inversely proportional to the distance at which the intersection occurs, normalized from 0 to
10.
As depicted in Figure 1B, the agent’s nervous system is a
3-layer continuous time recurrent neural network (CTRNN)
with the following state equation (Beer, 1996):
τi

N
dyi
= −yi + ∑ w ji σ(y j + θ j ) + Ii
dt
j=1

(1)

where y is the state of each neuron, τ is its time constant,
w ji is the strength of the connection from the jth to the ith
neuron, θ is a bias term, σ(x) = 1/(1 + e−x ) is the standard
logistic activation function, and I represents an external input (e.g., from a sensor). The top layer consists of 7 sensory
neurons which are stimulated by the agent’s vision rays as
described above. These project down to a middle layer of
5 fully interconnected neurons, which in turn feed into two
motor neurons. The difference in output between the motor
neurons results in an instantaneous horizontal velocity which
moves the agent in one direction or the other. The network is
bilaterally symmetric in terms of connection weights, gains
and biases, with the additional stipulations that all sensory
neurons shared the same gain and biases. This makes for a
total of 47 parameters. States were initialized to 0 and circuits were integrated using the forward Euler method with an
integration step size of 0.1.
The task of the agent is to visually discriminate between
objects of different shapes. These experiments were designed

A real-valued genetic algorithm was used to evolve CTRNN
parameters: connection weights, biases, time constants, and
gain. Agents were encoded as 47-dimensional vectors of
real numbers varying from [-1, 1]. Each vector element
linearly mapped to a parameter of the CTRNN: interneuron and motorneuron biases ∈ [−5, 5], sensory neuron biases
∈ [−4, −2], time-constants ranged ∈ [1, 2], and connection
weights ∈ [−5, 5]. Parents were selected with a rank based
mechanism, with an enforced elitist fraction of 0.04. Offspring were generated from uniform crossover of two parents
(probability 0.5). A Gaussian distributed mutation vector was
applied to each parent (µ = 0, σ2 = 0.01).
Fitness was calculated across 24 trials with objects dropped
at horizontal offsets uniformly distributed over the range of
+/-50. The performance measure to be maximized was:
∑24
i=1 pi /24, where pi = 1 − |di | for the objects that need to be
caught and pi = |di | for the objects that need to be avoided,
and di is the horizontal distance between the centers of the
object and the agent when their vertical separation goes to
zero on the ith trial. The distance di is clipped to a maximum
value of 45 and normalized to run between 0 and 1.
The two tasks have inverted fitness rewards. In Task A,
agents are required to catch circles and avoid line segments;
in Task B agents are required to catch line segments and avoid
circles. Therefore, high fitness with respect to one guarantees
proportionately low fitness on the other.
We performed evolutionary runs under changing conditions. In this case, a population was initially evolved for a
given task variant until the best agent reached a fitness threshold of 95%. Once the fitness threshold was reached, the task
was changed to the opposite task. Populations were continuously evolved in this alternating manner for 2500 generations.
In addition to these, runs were performed for Task A and Task
B in isolation under the same evolutionary conditions.

Results
The following sections report results obtained from 40 evolutionary swapping runs. We focus first on understanding one
exemplary evolutionary run where adaptation occurs via neural reuse. That is, we show that over the course of an evolutionary run the population locates a general-purpose neural
structure that enables it to quickly respond to changing task
requirements. We then analyze this increase in adaptability in
terms of population dynamics in parameter space and reuse
of neural structure in the agents. In the last section, we look

3120

at how well these findings generalize to the full ensemble of
evolutionary runs.

A1

A

B1

Increased adaptation over time
As most work on evolving minimally cognitive behaviors focuses on one task, we set out to test whether evidence for
adaptation could be observed in populations under changing
conditions. That is, does learning one task help a population
adapt to new tasks over time?
Figure 2 depicts the fitness trajectory of the best agent in
the population in each generation for one of the evolutionary runs. Altogether, this population achieved a total of 98
swaps in 2500 generations. The first 4 are labeled for the
sake of discussion; A1 indicates the first time the population achieves fitness threshold for Task A, B1 indicates the
first time population reaches threshold for Task B, etc. This
trajectory shows evidence of adaptation. As can be seen,
the population starts off close to random behavior (0.5) and
rapidly improves performance at the first task Task A: circlecatching, line-avoidance). As soon as the best individual in
the population surpasses the established fitness threshold, the
task is swapped (A1), at which point fitness of the population suddenly drops. Recall that fitness measures of each task
are inversions of one another, so we should expect a population with high fitness with respect to one task to have a
sudden drop in fitness when tasks are swapped. The population quickly begins to make improvements in performance
at the new task (task B: circle-avoidance, line-catching). As
soon as the threshold is met for this task (B1), the task is
swapped back to Task A, and the fitness drops again. Interestingly enough, the fitness does not drop anywhere as low
as what we would expect from an agent that performs the opposite behavior, or even a random behavior. Thereafter, the
population evolved to very quickly re-adapt to the new task
after each new swap (Figure 2B).
The fitness trajectory of this evolutionary run shows evidence of adaptation. The trajectory is characterized by periods of ascent leading to a peak and sudden drop offs that
occur due to task swaps. The time-to-adapt corresponds to
the number of generations taken to reach the fitness threshold for the new task after a swap. In this study, we consider
adaptability as the extent to which populations improve their
time-to-adapt to changes in the fitness landscape over time.
Therefore, a population shows evidence of adaptability when
the time-to-adapt decreases over the course of an evolutionary
run. In this example, we observe that the first time the population evolves for Task A and Task B it took around 500 generations each. After that, the time-to-adapt dropped sharply
to between 1 and 30 generations. In other words, the population improved substantially its time-to-adapt by the third
task swap, and maintained this ability for the duration of the
evolutionary run.

Spontaneous meta-fitness selection
How did such a dramatic increase in adaptation efficiency occur? We hypothesize that the population increases its adap-

B

B1
A2
B2

Figure 2: Increased adaptation over time in an evolutionary
run. [A] Fitness trajectory of the best individual in the population over time. Dashed vertical lines mark task swaps. Blackdotted horizontal line represents the 95% fitness threshold.
The first 4 task swaps are labeled A1, B1, A2 and B2 respectively. Panel [B] shows the inset in panel [A].

tation efficiency by locating a region of meta-fitness, which
we define as a region in parameter space where individuals
with high fitness on both tasks exist in close proximity to one
another. In other words, the proposal is that adaptability over
evolutionary timescales involves a spontaneous selection for
regions of parameter space that support multiple tasks over
regions of parameter space that are specialized for only one
of the tasks. In order to test this hypothesis, a principal component analysis (PCA) was performed on the 47-dimensional
set of all genotypes in populations at points A1, B1, A2 and
B2. Figure 3 shows these populations in a reduced 2D space,
where the first dimension captures 73.1% of the overall variance and the second captures 1.7%.
The movement of the population in parameter space during evolution shows evidence of meta-fitness selection. Figure 3 shows the structure of the evolutionary dynamics over
the course of the first four task swaps. The population starts
off at random in parameter space and they find a region that
is adapted to Task A for the first time (A1). The task swaps to
Task B, and the population moves from A1 to B1, where they
find a region that is adapted to Task B, also for the first time.
When Task A is presented once again, the population finds
a region of parameter space that is relatively nearby (A2).
The same occurs when the task swaps back to B (B2). The
number of generations to adapt to each task, therefore, corresponds to the relative distances that the population moved in

3121

A2

A
B2
B1
A1
A2

B2

B

A1

A2

Figure 3: Spontaneous meta-fitness selection. [A] Projection
of populations at A1, B1, A2 and B2 in reduced 2D space obtained through PCA. [B] Fitness distributions of populations
seeded around the best individual at A1 (blue) and the best
individual at A2 (green). Populations were obtained by applying Gaussian perturbations within the range of [-0.2, 0.2]
to the seeded values in each dimension.

parameter space. This suggests that the fitness landscape contained two qualitatively different kinds of adaptive regions.
Although individuals in populations A1 and A2 were both
equally adapted to Task A, individuals in A1 were isolated in
parameter space, while individuals in A2 were in close proximity to regions of high fitness for the opposite task. What is
interesting about this result is that we did not include direct
selection pressure towards these regions; instead populations
were guided towards them spontaneously over repeated exposure to different tasks.
What is the fundamental difference between these two
types of adaptive regions? To address this question we sampled random populations in each region and evaluated their
fitness distributions; results are presented in Figure 3B. Recall that each agent achieved equally high fitness with respect
to Task A, but the best individual at A2 was found to be located in a region of meta-fitness whereas the best individual
at A1 was not. As expected, both populations contain equally
high-performing circuits. However, the fitness distribution of
agents sampled around A1 (blue) is highly skewed towards
high fitness values, as would be expected for a typical fitness
peak. In contrast, the fitness distribution of agents sampled
around A2 (green) is significantly flatter, indicating that it
encompasses a greater diversity of phenotypes. This result
reveals a crucial difference between meta-fitness regions and
otherwise equally-fit fitness peaks.

Figure 4: Similar behavioral strategy and neural structure for
opposite tasks. Behavior (left) and structure (right) of best
individuals from populations A2 and B2. Left panels: Horizontal position of agents over time as object falls for all 24
trials (circle trials purple, line trials cyan). Right panels:
Neurons represented by disks (opacity depicts bias parameter, where a large negative bias is white and a large positive
bias is black) and connections represented by lines (excitatory
grey, inhibitory black; thickness proportional to strength).

Neural reuse in meta-fitness regions
Meta-fitness regions in parameter space have interesting implications for neural reuse. Due to linearity in the genotypephenotype map, close proximity of genomes in parameter
space translates to a high degree of similarity between neural
structures in corresponding phenotypes. As a result, we observe that best-agents from the same meta-fitness region share
the same essential structure despite being evolved to perform
different tasks. This is illustrated in Figure 4, which presents
graphic depictions of the neural structures of the best agents
taken from populations A2 and B2. The two structures are
nearly identical, despite the fact that they ultimately support
different task variants. In other words, structure evolved to
perform Task A in A2 was largely reused in the subsequent
evolution for Task B.
In addition to neural reuse, we observe that these two individuals utilize the same behavioral strategy for performing
each task (Figure 4). In each case, the agent scans back and
forth over the mid-line several times before either centering
or heading away. Although not shown, this strategy is qualitatively different from the behaviors of best agents performed
in isolated runs.

Ensemble Analysis
How reliably does evolutionary swapping result in increased
adaptation of populations? Of the 40 runs, 17 never reached
the fitness threshold in the allotted 2500 generations. We
consider these to be null cases. Of the remaining 23 runs, 9
showed evidence of increased adaptation over time. It is unclear whether or not the runs that did not demonstrate signs

3122

Euclidean Distance

Euclidean Distance

Figure 5: Increased adaptability via meta-fitness. Distances
between best-genomes from swapping runs (black) versus
distances between best-genomes produced by successful isolated runs (grey).

of increased adaptability would have had they been allowed
to run past the arbitrary 2500 generation cutoff. Regardless,
the remainder of the section is concerned with the 9 runs that
showed evidence of increased adaptability.
In the case study, increased adaptation was due to the
evolving population locating an area of meta-fitness. Is this
also the case for the rest of the populations that showed increased adaptability? We computed the set of Euclidean distances separating best genomes in successive epochs for all
swapping runs and compared this with the set of distances
separating best genomes coming from successful isolated
runs. As depicted in Figure 5, best-agents produced in the
same swapping run are usually quite closer to one another
in parameter space than random opposite-task best-genomes
sampled from isolated runs. This suggests that in general,
adaptability is achieved via meta-fitness regions.
Given that populations in general became more adaptive by
locating meta-fitness regions, it is of interest to ask whether
each run located the same region or different meta-fitness regions were located by different runs. Figure 6 displays distributions of within-run distances (Euclidean distances separating the best agents of all target populations occurring within
given runs) and inter-run distances (Euclidean distances separating best agents of all target populations occurring in different runs). If all swapping runs located the same meta-fitness
region we would expect these two distributions to be equivalent. That within-run distances are substantially less than
inter-run distances suggests that different swapping runs indeed located different meta-fitness regions.
As we observed in the case study, a region of meta-fitness
in parameter space corresponds to a general-purpose neural
structure. In the case study, we also observed that best-agents
from the same meta-fitness region can perform opposite task
variants with the same behavioral strategy. Figure 7 illustrates
the generalization of these ideas to multiple meta-fitness regions. It depicts three pairs of best-agents from separate evolutionary runs. In each pair both agents have nearly-identical
neural structures and qualitatively similar behavioral strategies despite performing opposite tasks. If we contrast pair

Figure 6: Different runs locate different meta-fitness regions.
Distances between best-genomes of the same evolutionary
run (black) versus distances between best-genomes of different evolutionary runs (grey).
A with pair B we see that they each perform the tasks with
the same qualitative behavioral strategy, despite the fact that
they have different neural structures. This demonstrates how
a given behavioral strategy can be supported by qualitatively
different neural structures. In contrast, pair C has a qualitatively different behavioral strategy from both A and B.

Discussion
We set out to test whether evidence for adaptation and neural
reuse could be observed in populations of embodied agents
required to perform two variants of an object discrimination
task that changed over evolutionary time. Evidence of adaptation was indeed observed: over the course of an evolutionary
run populations gained the ability to more quickly adapt to
changes in task requirement. In general, populations achieved
this increase in evolutionary efficiency by evolving generalpurpose neural structures, which could be reused to support
either task variant with minimal structural modifications. Below we discuss these results in light of continued work in
neural reuse and evolvability.
Despite a growing body of empirical and theoretical support for neural reuse (Anderson, 2010), a grounded understanding of how neural reuse works in organisms remains
elusive. There have been some computational efforts to this
effect (Hurley, 2008; Anderson, 2010), but the present work
contributes in two important ways. First, whereas other modeling work incorporates elements of reuse into the model,
we see reuse spontaneously arising in our system as a result of the evolutionary pressures imposed by the varying
task requirements. Secondly, in studying dynamic embodied
agents, we examine neural reuse in light of the increasingly
accepted viewpoint that cognition is distributed over a brainbody-environment system (Chiel & Beer, 1997). In previous work, it was shown that the model-agents studied here
offload some cognitive load to their bodies and the environment (Beer, 2003). It is likely that successful neural reuse
observed in these simulations relies on features of embodiment. Ongoing work on a deeper, more thorough analysis of
agents exhibiting reuse promises to bring some of these issues
to light.

3123

A(i)

(ii)

B(i)

(ii)

(ii)

C(i)

Figure 7: Different behavioral strategies and general-purpose neural structures are located in different evolutionary runs. Example solutions from three different evolutionary runs are shown in panels [A], [B] and [C]. For each evolutionary run, two
solutions are shown: the best agent for line-catching and circle avoidance (i); the best agent for the opposite task obtained
successively (ii). Graphics follow same coloring conventions as Fig. 4.
Work on lifetime learning and adaptation in cognitive systems parallels ongoing research on evolvability in evolutionary systems. Evolvability refers to the ability of a population to adapt to changes in its fitness landscape, and has been
a hot topic in theoretical biology and evolutionary computation (Pigliucci, 2008). In order for an evolutionary system to
support evolvability there must be redundancy in the fitness
landscape; there must be many genomes which have high fitness in a given environment, and some of these must be more
adaptable to future environmental changes than others. Typically investigations of evolvability examine the role of developmental processes indirectly linking genotype to phenotype
as a mechanism for such redundancy (Pigliucci, 2008). In the
present work, however, we observe signatures of evolvability
in a system without any developmental scenario. While our
system has a direct genotype to phenotype map, the complexity of agents introduces sufficient redundancy in the phenotype to behavior map (qualitatively different neural structures
can produce the same basic behavior) and behavior to fitness
map (qualitatively different behaviors can yield the same fitness) to allow for increased adaptation.
The work presented above demonstrates that evolving
embodied agents for time-varying cognitive tasks is fertile
ground for exploration of cognitive adaptation in general, and
neural reuse more specifically. There are many ways in which
efforts here can be built upon. First, given that neural reuse
is most concerned with brain circuits supporting vastly different cognitive functions (Anderson, 2010), we would like
to expand the repertoire of tasks to include some that are
qualitatively different. As our understanding of neural reuse
advances, we can extend our approach to increasingly complex interesting tasks. Secondly, we would like to analyze
in more depth the operation of agents that exhibit reuse. Past
work has shown that the mathematical tools of dynamical systems theory and information theory can be used to provide
rich accounts of cognition exhibited in these systems (Beer
& Williams, 2015). Ultimately, we would like to provide a
grounded understanding of neural reuse in embodied, situated
agents capable of diverse cognitive phenomena.

References
Anderson, M. (2007). Evolution of cognitive function via
redeployment of brain areas. The Neuroscientist, 13, 13–
21.
Anderson, M. (2010). Neural reuse: a fundamental organizational principle of the brain. Behav Brain Sci., 11(4),
245–266.
Beer, R. (1996). Toward the evolution of dynamical neural
networks for minimally cognitive behavior. In From animals to animats 4 (pp. 421–429). MIT Press.
Beer, R. (2003). The dynamics of active categorical perception in an evolved model agent. Adaptive Behavior, 33(4),
209–243.
Beer, R., & Williams, P. (2015). Information processing and
dynamics in minimally cognitive agents. Cognitive Science, 39(1), 1–38.
Braylan, A., Hollenbeck, M., Meyerson, E., & Miikkulainen,
R. (2016). Reuse of neural modules for general video game
playing. AAAI.
Chiel, H., & Beer, R. (1997). The brain has a body: adaptive behavior emerges from interactions of nervous system,
body and environment. Trends in Neuroscience, 20(12),
553–557.
Dehaene, S. (2005). Evolution of human cortical circuits for
reading and arithmetic: The “neuronal recycling” hypothesis. In From monkey brain to human brain. MIT Press.
Gallese, V. (2008). Mirror neurons and the social nature
of language: The neural exploitation hypothesis. Social
Neuroscience, 3(3–4), 317–333.
Hurley, S. (2008). The shared circuits model (SCM): How
control, mirroring, and simulation can enable imitation, deliberation, and mindreading. Behavioral and Brain Sciences, 31(1), 1–58.
Pigliucci, M. (2008). Is evolvability evolvable? Nature
Reviews Genetics, 9, 75–82.
Stanley, K., & Miikulainen, R. (2002). Evolving neural
networks through augmenting topologies. Evol. Comput,
10(2), 99–127.

3124

