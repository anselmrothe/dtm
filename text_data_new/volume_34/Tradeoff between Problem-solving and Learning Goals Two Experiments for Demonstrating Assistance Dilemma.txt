UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Tradeoff between Problem-solving and Learning Goals: Two Experiments for
Demonstrating Assistance Dilemma

Permalink
https://escholarship.org/uc/item/95r4z60z

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)

Authors
Miwa, Kazuhisa
Terai, HItoshi
Nakaike, Ryuichi

Publication Date
2012-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Tradeoff between Problem-solving and Learning Goals:
Two Experiments for Demonstrating Assistance Dilemma
Kazuhisa Miwa (miwa@is.nagoya-u.ac.jp)
Hitoshi Terai (terai@is.nagoya-u.ac.jp)
Graduate School of Information Science, Nagoya University

Ryuichi Nakaike (nakaike@educ.kyoto-u.ac.jp)
Graduate School of Education, Kyoto University
Abstract
Recent intelligent tutoring systems give participants various
types of supports. We hypothesize that a high level of support
activates participants’ orientation to problem-solving goals but
reduces the priority of attaining learning goals; as a result,
higher problem-solving performance is attained, but the learning effect is reduced. We tested this hypothesis by using two
relatively largely different experimental tasks: Tower of Hanoi
puzzle as a simple problem solving task and Natural Deduction
learning as a more complex learning task. Overall results supported our hypothesis and were discussed from the viewpoint
of the assistance dilemma.
Keywords: Problem solving goal; Learning goal; Assistance
dilemma.

Introduction
Recently, highly interactive intelligent tutoring systems have
been developed whose design principles come from cognitive science theories. A series of cognitive tutors has been
constructed based on the ACT-R theory (Anderson, Corbeett,
Koedinger, & Pelletier, 1995). Intelligent tutoring systems
give participants various feedback such as verification, correct response, try again encouragement, error flagging, and
elaboration messages (Shute, 2008). In the interaction design
between a tutoring system and learners, feedback to learners
is a central issue.
In this context, the assistance dilemma has been recognized. Koedinger and Aleven pointed out a crucial question
(Koedinger & Aleven, 2007): How should learning environments balance assistance giving and withholding to achieve
optimal learning? High assistance sometimes provides successful scaffolding and improves learning, but at other times
it elicits superficial responses without consideration from students. On the other hand, low assistance sometimes encourages students to make a large effort, but other times results in
enormous errors and interferes with effective learning.
We reformulate the assistance dilemma as a tradeoff of selecting either the problem-solving goal or the learning goal.
In a representative situation, participants learn while solving
instance problems given by a tutoring system. Attaining the
problem-solving goal means solving such instance problems
as accurately and rapidly as possible. However, the learning
goal requires another attainment that is usually more essential. A primary objective is not to solve instance problems
but to learn by solving instances. Dweck classified two types
of goals: learning and performance (Dweck, 1986; Ames,
1992). Highly motivated children tend to set learning goals to

increase their competence to understand or master something
new rather than just solving problems. Comparing the learning and problem-solving goals in our current study corresponds to Dweck’s learning and performance goals. Achieving a problem-solving goal is measured by the solution time
and the error ratio for solving problems in the learning phase.
The learning goal is usually measured by a posttest after the
learning phase.
Another important difference between the two goals is that
the problem-solving goal can be achieved with the support
of a tutoring system, but the learning goal should be reached
without supports of a tutoring system. Achieving the learning goal is usually measured in a setting without tutoring system support because learners should solve problems by themselves without external support from a tutoring system. The
need for support means that participants do not complete the
learning.
Participant goal setting may be influenced by the feedback
information from a tutoring system. One perspective for characterizing the feedback is directive and facilitative (Black &
William, 1998). Directive feedback tells participants what
needs to be fixed in the next step. Such feedback tends to
be more specific than facilitative feedback, which provides
participants with comments and suggestions directly relating to the problem-solving. When participants are solving a
problem, directive feedback may guide them to focus on the
problem-solving goal.
Another perspective for characterizing feedback is its timing. Researchers have addressed whether feedback should
be delivered immediately or delayed. Delayed means that
it occurs minutes, hours, or even weeks later. Mathan and
Koedinger reviewed various studies and concluded that timing effects emerge interactively with other factors such as
task difficulty and individual student needs or characteristics
(Mathan & Koedinger, 2002). Immediate feedback may facilitate problem-solving goals because participants are repeatedly given indications for determining what to do next when
solving a problem.
In the context of the investigation of the assistant dilemma
issue, we control the levels of support (LOS) in the following experiments. A high level of support means that more
direct and immediate feedback is given. Our hypothesis is
that a high level of support activates participants’ orientation
to problem-solving goals and reduces the priority of attaining

2008

learning goals. This hypothesis predicts that in the high level
support condition, the problem-solving performance is higher
than in the low level support condition, but the learning effect
was reduced; therefore in the posttest where no supports are
given, participants who learned in the high level support condition score lower than those in the low level support condition.
We tested this hypothesis using two experimental tasks. In
Experiment 1 we used the Tower of Hanoi (TOH) puzzle,
which is one representative experimental task widely used in
problems-solving studies. In Experiment 2, the participants
engaged in a natural deduction (ND) task.
TOH is a simpler task. The problem space is systematically
organized and is not so large. Problem solving is achieved by
only one operator that corresponds to disk movement. The
knowledge and strategies for the solution are represented by
less than ten production rules. ND is a more complex task.
Its problem space is much larger than that of TOH. To solve
problems, since participants must acquire many kinds of inference rules and solution strategies, a complete model for
solving ND problems consists of around a hundred production rules. In addition, TOH is basically a problem-solving
task, but ND is a learning task. The participants in Experiment 1 joined the experiment in a laboratory setting; those in
Experiment 2 engaged in it in a learning context. We confirmed our hypothesis using two relatively largely different
experimental tasks.

Experiment 1
Task
The six disks TOH puzzle was used as an experimental task.

Experimental system
The participants individually engaged in the task using an experimental environment established on a personal computer.
Figure 1 shows an example screenshot of the experimental
system. The participants selected one of the possible disk
movements by clicking a button with a mouse. A production
system model was mounted on the system to solve TOH by
the perceptual strategy. The model infers the next step, the
next five steps, and the next nine steps for reaching the goal
state through the minimum steps and presents the participants
the best next state, the best state after five, and nine steps as a
hint.
The LOS was manipulated by the presented hints. In the
highest LOS condition, the participants were presented the
next step at every problem-solving trial. In other conditions
where the best step after five or nine steps was presented,
the participants were given such hints at every five or nine
problem-solving trials. Higher supports mean direct and immediate feedback; therefore, the participants in these two
conditions were given lower levels of support than those in
the next step condition. Additionally, in the lowest LOS condition, the participants were given no hint information.

Figure 1: Example screen shot of experimental system for
TOH. The upper and lower windows show the goal state and
the current state. The middle window presents hint information; in this case the next one step is presented.

Participants and Procedure
Seventy-one participants joined our experiment. 17, 19, 17,
and 18 participants were assigned to the one step, five steps,
nine steps, and no hints conditions, respectively. The experiment lasted 90 minutes. The participants were instructed to
learn strategies for solving TOH and informed that after the
learning session, a posttest would be performed to test their
degree of skill acquisition.
In the initial stage of the experiment, the participants
learned the constraints of the disk movements and how to use
the experimental system. In the learning phase, they solved
various types of six-disk-TOH problems in 40 minutes in one
of the four experimental conditions. When one problem was
completed, the next was given. After the learning phase, a
posttest was performed in which the participants solved a test
problem by themselves without hint information.

Result
As a problem-solving performance measure, we used normalized steps for the solution in the learning phase. Figure
2 1 shows the average steps for the solution where the index indicated in the vertical axis was normalized by dividing
the solution steps that the participants actually needed to follow by the minimum steps for reaching the goal state from
the initial state in each problem. The value, 1.0, means the
completed solution, and larger values indicate a poorer solution. The normalized steps needed for the solution were
fewer in the one step, five, and nine step conditions where
1 Note

that in figures 2, 4, and 6, the value of the vertical axis is
reversed to compare those with Figure 9 in conclusion.

2009

Figure 2: Average steps for solving problems in learning
phase (TOH). The value of the vertical axis is reversed to represent higher values as high problem-solving performances,
i.e., value 1.0 means the optimal problem solving. An
ANOVA indicates that the main effect of Conditions factor
was significant (F(3, 67) = 12.5, p < 0.01). Fisher’s LSD
analysis shows significant differences between one step and
no hints (p < 0.01), five steps and no hints (p < 0.01), and
nine steps and no hints (p < 0.01).

hint information was presented than those in the no hint condition. Our prediction was confirmed because the result of the
problem-solving performance was worst in the lowest LOS
condition. However, we did not detect statistically significant
differences among the three hint conditions. The normalized
steps in the three conditions almost reached 1.0. This means
that the hint information was sufficient for reducing the trial
and error behavior of the participants, even in the nine step
condition.
Next, to investigate to what degree each participant thoroughly considered rational actions in each problem-solving
step, we calculated the average time to decide each disk
movement. We assumed that the priority of the problemsolving goal over the learning goal reduces this consideration
time. Figure 3 shows the time that passed between one disk
movement and the next. The time in the one step condition
was shorter than in the five and nine step conditions, confirming our prediction. However, in the no hit condition, the
time was also shorter than the five and nine step conditions,
contradicting our prediction.
Next, as a learning performance measure, we used the normalized steps for the solution in the posttest. Figure 4 shows
the result. The average steps in the nine steps condition were
fewer than those in the one step and no hint conditions. The
graph shows that in the three conditions (one, five, and nine
step conditions) where hint information was presented, as
lower LOSs were given, the learning effect increased, confirming our prediction. However, in the no hint condition,
the performance was poorer than that in the higher LOS (nine
steps) condition, contradicting our prediction.
Contradictory to our prediction, in the no hint condition,
the time for deciding the next disk movement was shorter, and
the learning effect was poorer. Perhaps in the learning phase,

Figure 3: Average time for deciding disk movement (TOH).
An ANOVA indicates that the main effect of Conditions factor was significant (F(3, 67) = 18.0, p < 0.01). Fisher’s LSD
analysis shows significant differences between one and five
steps (p < 0.01), one and nine steps (p < 0.01), five steps and
no hints (p < 0.01), and nine steps and no hints (p < 0.01).

Figure 4: Average steps for solving problems in posttest
(TOH). The value of the vertical axis is reversed to represent
higher values as high problem-solving performances, i.e.,
value 1.0 means the optimal problem solving. An ANOVA
indicates that the main effect of Conditions was significant
(F(3, 67) = 3.25, p < 0.05). Fisher’s LSD analysis shows
significant differences between one and nine steps (p < 0.01)
and nine steps and no hints (p < 0.05).
it was difficult for the participants to learn strategies without
hint presentation. This point will be mentioned below in the
discussion and conclusion.

Experiment 2
Task
Natural deduction (ND) is a kind of proof calculus: e. g.,
inducing a proposition ¬Q → ¬P from a premise P → Q.
Logical reasoning is expressed by inference rules closely related to a natural way of reasoning. They learned nine basic
rules and five formal strategies, all of which are fundamental
knowledge in ND. Most problems can be solved using this
knowledge.

Experimental system
The experimental system used in Experiment 2 was developed as a tutoring system for teaching ND to university un-

2010

In the second week, the first half of the learning phase was
performed where the participants learned the four basic inference rules. In this class session, all participants learned in the
high LOS condition.
In the third week, the latter half of the learning phase followed where LOS was manipulated. The participants solved
relatively complex problems for which a sub-derivation process with sub-goal setting was needed. The instructor demonstrated the solutions of two problems, and then the participants solved Problems 1 to 4 with the tutoring system. In
the class, the participants were divided into two groups: high
LOS and low LOS. After the learning phase, two posttests
were performed. Posttest 1 was identical to Problem 2, which
they solved in the learning phase, and Posttest 2 as a transfer
problem was a new challenge for the participants.

Result

Figure 5: Example screen shot of ND tutoring system. The
left side window shows a current status of inference processes
where the red items are propositions to which the selected inference rule should be applied. The upper and lower center
windows show a strategy list and an inference rule list where
the red items are applicable candidates. The right upper window shows the LOS selector where the levels of support are
controlled. In the current experiment, LOS was fixed based
on the experimental manipulation.
dergraduates. Figure 5 shows an example screenshot of the
tutoring system, which provides participants with lists of the
inference rules and strategies. After users select one from
the lists, the system automatically runs the rules and presents
the inference result. The system scaffolds the participants by
giving help information about the selection of the rules and
strategies and presents the candidates of the rules that should
be applied in a given situation.
The LOS was controlled by the presented hint information.
In the high LOS condition, the system presented both applicable candidates (rules and strategies) and propositions to which
the rules should be applied. In the low LOS condition, the
system only presented a set of inference rules; no candidates
were presented. The participants had to select an adequate
rule from the list by themselves without system support. A
high LOS means that the participants were given direct help
that guides them to a determined behavior.

Participants and Procedure
Twenty-nine participants joined our experiment. 13 and 16
were assigned to the high and low LOS conditions, respectively. The experiment was performed over three weeks in an
introductory cognitive science class.
In the first week, the participants learned the basics of formal inference systems and ND as an example of the systems.

The optimal steps for a solution are determined in TOH. However, in the solutions of some ND problems, various reasoning paths are rational; therefore we used the average time for
solving each problem in the second half of the learning phase
as a problem-solving performance measure. Figure 6 shows
the result. The solution time was shorter in the high LOS
group than in the low LOS group when solving Problems 3
and 4. This result is consistent with our prediction.
Figure 7 shows the time for deciding and implementing an
inference rule to forward reasoning. The decision time was
shorter in the high LOS group than in the low LOS group
when solving Problems 3 and 4. This result is consistent with
our prediction.
Next, we used the scores, i.e., the ratio of successfully
solved tests, in the posttest as a learning performance measure. Figure 8 shows the result, indicating that when solving
Posttest 2, more participants in the low LOS group reached
the solution than in the high LOS group, confirming our prediction. This effect was only observed in solving the transfer
problems, but not in the repeated problems. This result is
consistent with earlier experimental studies, confirming that
delayed and lowering supports make positive effects, especially in solving transfer problems (Schroth, 1992, 1997).

Discussion and Conclusions
The assistance dilemma hypothesizes an optimum point of
learning effects as a function of cognitive load. Koedinger
et al. (2008) indicated two dimensions of assistance: the
practice spacing dimension and the example-problem dimension (Koedinger, Pavlik, Mclaren, & Aleven, 2008). They
demonstrated a reverse U-shape learning curve on the two dimensions. We conceptualized such a learning effect curve
with a problem-solving performance curve (Figure 9). As
the support level increases, the problem-solving performance
is gradually promoted; however the learning effect reaches
maximum at a specific support level and decreases form the
point.
When comparing this framework with the results of our
two experiments, note that in Experiment 1, the results indi-

2011

Figure 6: Average solution times for solving problems in learning phase (ND). The value of vertical axis is reversed to represent
higher values as high problem-solving performances. T-tests show a marginal significant difference between high and low
conditions in Problem 3 (t(22) = 1.92, p = 0.07) and a significant difference in Problem 4 (t(15) = 2.70, p < 0.05), but no
differences in Problems 1 and 2 (t(25) < 1, n.s.; t(24) = 1.14, n.s.).

Figure 7: Average time for deciding inference rule (ND). T-tests show a marginal significant difference between high and low
conditions in Problems 3 (t(22) = 2.55, p < 0.05) and 4 (t(25) = 2.98, p < 0.01), but no differences in Problems 1 and 2 (t(25)
< 1, n.s.; t(24) < 1, n.s.).

Figure 8: Ratio of successful participants (ND) . Chi square
tests show a significant difference between high and low conditions in Posttest 2 (χ2 (1) = 7.49, p < 0.01), but no difference
in Posttest (χ2 (1) < 1, n.s.).

cated in Figures 2 and 4 demonstrated this pattern, controlling the support level from the highest to the lowest on the
diagram. But in Experiment 2, the results, as indicated in Figures 6 and 8, only demonstrated the left side of the pattern:
control from the highest to the mid level. Our hypothesis was
that a high level of support activates participant orientation to

the problem-solving goal and promotes the problem-solving
performance, but reduces the priority of attaining the learning goal and decreases the learning effects. This hypothesis
is consistent with the left part of Figure 9. On the right side,
meaning no or a very low LOS, both the problem-solving performance and the learning effect decreased. This pattern suggests two interpretations. One is that in the right side situation, even if the participants set their goal to learning in the
learning phase, they might not be able to decide what to do
next and may make enormous errors, resulting in low learning effects. The other interpretation is that the participants
give up the attainment of the learning goal because they face
difficulties in learning without support. In the current study,
we could not decide which explanation is better. Future work
will address this issue.
We discussed a tradeoff between problem-solving and
learning goals with the degree of support in the learning
phase. This tradeoff issue appears in various research fields.
For example, the effect of goal specificity has been investigated (Burns & Vollmeyer, 2002; Sweller, 1988). Participants often neglected to consider the theories or rules behind phenomena when they aimed for a specific goal. That is,

2012

emerges when automation breaks down can be understood
based on the tradeoff issue of the two goals.

References

Figure 9: Conceptualized diagram of assistance dilemma and
relation of problem-solving and learning goals.

they tended not to search in a hypothesis space, because they
concentrated on a search in an instance space to achieve the
goal. Consequently, they could not find any rules or underlying mechanisms of the task. Goal specificity studies have
indicated that this tendency can be controlled by manipulating goal specificity. Specific goals are direct and immediate;
therefore the degree of goal specificity is considered consistent with the levels of support in the current study. The results
of our two experiments are consistent with the findings observed in many experiments conducted in the context of goal
specificity studies.
Similar to our definition of the levels of support, in studies on automation usage, Sheridan & Verplank indicated ten
grades of automation levels (Sheridan & Verplank, 1978).
The highest level, Level 10, means that the computer acts
entirely autonomously, and the lowest level, Level 1, means
that a human does everything. Lin, et al. conducted a microworld experiment where the participants controlled an atomic
power plant under various levels of automation (Lin, Yenn, &
Yang, 2010). Medium levels of automation maximized the
participants’ operations. The most important objective in the
use of automation systems is stable manipulation while using the systems; therefore, the tradeoff issue of the two goals
might not appear. However, researchers have pointed out the
possibility that the continuous usage of automation systems
may sometimes cause serious damage when the automation
support is removed. For example, Parasuraman et al. argued
that stable automation systems produce automation-induced
complacency (Parasuraman, Molloy, & Singh, 1993; Molloy & Parasuraman, 1996). They demonstrated in a laboratory setting that operator detection of automation failures was
substantially worse for constant-reliability than for variablereliability (unstable) automation. As we mentioned, in the
learning context, the problem-solving performance was measured with learning system support; on the other hand, the
learning effect was usually tested without external support
in posttests. In this sense, the complacency problem that

Ames, C. (1992). Classrooms: Goals, in structure, and student motivation. Journal of Educational psychology, 84,
261-271.
Anderson, J. R., Corbeett, A. T., Koedinger, K. R., & Pelletier, R. (1995). Cognitive tutors: Lessons learned. Journal of the Learning Sciences, 4, 167-207.
Black, P., & William, D. (1998). Assessment and classroom
learning. Assessment in Education: Principles, Policy &
Practice, 5, 7-74.
Burns, B. D., & Vollmeyer, R. (2002). Goal specificity effects
on hypothesis testing in problem solving. The Quarterly
Journal of Experimental Psychology, 55.
Dweck, C. S. (1986). Motivational processes affecting learning. American Psychologist, 41, 1040-1048.
Koedinger, K. R., & Aleven, V. (2007). Exploring the assistance dilemma in experiments with cognitive tutors. Educational Psychology Review, 19, 239-264.
Koedinger, K. R., Pavlik, P., Mclaren, B., & Aleven, V.
(2008). Is it better to give than to receive? the assistance
dilemma as a fundamental unsolved problems in the cognitive science of learning and instruction. In Proceedings of
the 30th annual conference of the cognitive science society
(p. 2155-2160).
Lin, C. J., Yenn, T., & Yang, C. (2010). Automation design in
advanced control rooms of the modernized nuclear power
plants. Safety Science, 48, 63-71.
Mathan, S. A., & Koedinger, K. R. (2002). An empirical assessment of comprehension fostering features in an intelligent tutoring system. In Lecture notes in computer science
(proceedings of 6th international conference on intelligent
tutoring systems), 2363 (p. 330-343).
Molloy, R., & Parasuraman, R. (1996). Monitoring an automated system for a single failure: Vigilance and task complexity effects. Human Factors, 38, 311-322.
Parasuraman, R., Molloy, R., & Singh, I. L. (1993). Performance consequences of automation-induced ’complacency’. The International Journal of Aviation Psychology,
3, 1-23.
Schroth, M. L. (1992). The effects of delay of feedback on
a delayed concept formation transfer task. Contemporary
Educational Psychology, 17, 78-82.
Schroth, M. L. (1997). The effect of different training conditions on transfer in concept formation. Journal of General
Psychology, 124, 157.
Sheridan, T. B., & Verplank, W. L. (1978). Human and
computer control of undersea teleoperators. Cambridge,
MA: MIT Man-Machine Laboratory.
Shute, V. J. (2008). Focus on formative feedback. Review of
Educational Research, 78, 153-189.
Sweller, J. (1988). Cognitive load during problem solving:
Effects on learning. Cognitive Science, 12, 257-285.

2013

