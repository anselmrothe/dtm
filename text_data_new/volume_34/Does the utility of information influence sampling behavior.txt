UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Does the utility of information influence sampling behavior?

Permalink
https://escholarship.org/uc/item/0fz3d3mw

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)

Authors
Markant, Doug
Gureckis, Todd

Publication Date
2012-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Does the utility of information in uence sampling behavior?
Doug Markant (doug.markant@nyu.edu)
Todd M. Gureckis (todd.gureckis@nyu.edu)
New York University
Department of Psychology
6 Washington Place, New York, NY 10003 USA
Abstract

reasoning task where the predictions of these measures could
be readily distinguished. Learners who could query diﬀerent
stimulus features before making classi cation decisions were
found to prefer to learn about features that maximized probability gain, a measure of how well a potential observation is
expected to improve classi cation accuracy.
In these studies, however, costs were not explicitly manipulated or controlled. Taking costs into account can alter the
optimal strategy in a given task, but it is unclear whether people adjust their behavior in a similar way. e goal of the
present paper is to explore the impact of costs on sampling decisions. We begin by evaluating two alternative objectives that
people may adopt when deciding what information to gather.
Like the models reviewed by Nelson (2005), the rst ignores
the implications of task-speci c costs and casts information
sampling strictly in terms of uncertainty reduction (i.e., information gain). e second approach balances the costs and
expected bene ts of information in the context of the task.
We then describe the results of two experiments that manipulate the concordance between these two approaches, in one
case creating an environment where the goals of minimizing
uncertainty and maximizing utility predict diﬀerent patterns
of information sampling. Our results show that people tend
to value information (in terms of the number of hypotheses
ruled out by a new observation) over situation-speci c costs
and bene ts. e implications of these results for theories of
information sampling are discussed.

A critical aspect of human cognition is the ability to actively
query the environment for information. One important (but
oen overlooked) factor in the decision to gather information is
the cost associated with accessing diﬀerent sources of information. Using a simple sequential information search task, we explore the degree to which human learners are sensitive to variations in the amount of utility related to diﬀerent potential observations. Across two experiments we nd greater support for
the idea that people gather information to reduce their uncertainty about the current state of the environment (a “disinterested”, or cost-insenstive, sampling strategy). Implications for
theories of rational information collection are discussed.
Keywords: information sampling, active learning, information
access costs

Introduction
From controlling the movement of our eyes to determining
which sources of news to consult, judging the quality of alternative sources of information is a critical part of adaptive
behavior. Research exploring how people make information
gathering (or “sampling”) decisions has shown that people can
discern subtle diﬀerences in the potential information value
of various aspects of the environment. For example, measurements of eye movements during object categorization show
that people preferentially allocate attention to object features
that are most useful for making subsequent classi cation decisions (c.f., Nelson & Cottrell, 2007; Rehder & Hoﬀman, 2005).
One aspect that typically complicates the analysis of information sampling behavior is that information rarely comes for
free. All natural tasks involve information access costs, even
if the only cost is the time required to gather information (Fu,
2011). In addition, diﬀerent pieces of information may be
more useful depending on how one will be tested in the future. Optimal search behavior must weigh the costs of collecting particular bits of information against the bene t it is expected to convey (Edwards, 1965; Juni, Gureckis, & Maloney,
2011; Tversky & Edwards, 1966), a point frequently made in
research on animal foraging (Stephens & Krebs, 1986).
Despite its importance, previous work on information sampling has oen failed to test whether people take into account
costs related to diﬀerent sources of information. For example, Nelson (2005) provides a comprehensive review of various
ways an optimal Bayesian agent might value potential information sources in the absence of task-speci c costs (see also
Nelson et al., 2010). One conclusion from this line of work is
that people make information search decisions that are consistent with normative measures of information value (many
of which oen make similar predictions). For example, Nelson et al. (2010) studied information sampling in a diagnostic

e rational analysis of information sampling:
comparing “interested” and “disinterested” search
How should a rational agent make information sampling decisions? Existing proposals fall into two broad categories which,
borrowing from Chater, Crocker, and Pickering (1998), we
call “interested” and “disinterested.” Unlike the distinctions
explored by Nelson (2005), these two proposals diﬀer signi cantly in terms of the overall goal of information sampling.
Interested (or cost sensitive) sampling e rst approach
represents a decision-theoretic approach to information sampling. In particular, the agent considers the cost for collecting
a piece of evidence and weighs this against the expected bene t
it should convey with respect to the goals of the task. For example, a car shopper might decide if the possible savings available from obtaining information contained in a vehicle history
report is worth the cost of the report. Similarly, preferentially
xating the features of an object that are diagnostic of its category membership may be a cost-sensitive strategy under the

719

Experiment 1: Rectangle Search

assumption that additional xations cost time and the number of xations needed to reach a correct decision should be
minimized. Sampling in this case is “interested” in that information acquisition is focused on some purpose or goal beside
acquiring the information itself. In many ways, “interested”
sampling is a fully rational strategy and this formulation is often adopted in economics research.

Hidden Gameboard

Possible Shapes

random samples

Disinterested (or cost insensitive) sampling e second approach values information to the degree that it reduces our
uncertainty about the world. Chater et al. liken this to basic research where the goal is learning without regard for
the ultimate utility of this knowledge for society. In their
words, “inquiry is valuable for its own sake, because it leads
to knowledge” (Chater et al., p.4). Disinterested inquiry can
be conveniently expressed as actions which have a high probability of reducing the Shannon entropy over the agent’s beliefs (Lindley, 1956; Mackay, 1992). Critically, disinterested
inquiry doesn’t depend on the costs associated with collecting information or using it to make subsequent decisions.
e basic premise of the experiments reported in this paper
is that these two strategies or ways of valuing information can
be dissociated on the basis of observed choice behavior. We
gave participants a simple, intuitive information search task
where they were asked to make sequences of decisions to reduce their uncertainty about a hidden target. Mathematical
models instantiating each of the two theories just described
are then t to the choice patterns of individual subjects. is
tting procedure (common in the reinforcement learning literature) allows us to evaluate which of the approaches we have
described gives the best account of participants’ choices.

Figure 1: e generative process underlying the shape search game.
A xed set of possible shapes is speci ed. A hidden gameboard is
created by sampling from this set of shapes and randomly arranging
the targets in the grid. During the sampling phase the participants
clicks on grid locations to reveal their contents. In the painting phase,
the subject draws in the remaining squares using the mouse and is
rewarded for accuracy.

than observations. e goal of the game is to nish with the
lowest score possible, which is achieved by learning the most
about the hidden shapes in the fewest number of observations.
Based on past work with this task (Gureckis & Markant,
2009), we have found that the overall objective of the game
is easily understood by the participants. A critical feature of
the task (which we exploit in our second experiment) is that
it allows for arbitrarily de ned targets (i.e., the target shapes
may be composed of any con guration of squares) that can be
manipulated to vary the complexity of the task.

Formal task analysis
In order to evaluate both “interested” and “disinterested” information search in the task, we compare the search behavior of subjects to that of a rational learner who updates their
beliefs about the gameboard in an optimal way. Formally,
players make a sequence of observations in order to learn the
hidden gameboard, gh ∈ G, where G is the universe of legal
gameboards. Each individual gameboard is de ned by an arrangement of N non-overlapping shapes {r1 , r2 , ..., rN } with
unique labels {l1 , l2 , ..., lN }, and each shape consists of a set
of squares such that oij ∈ rn (where i and j index the x- and
y-coordinates of the square).
On each trial the player makes an observation oij and receives feedback in the form of a label ln , where l0 indicates the
observed square is empty, l1 means it contains part of shape
r1 , and so on. Since each square in the grid is deterministically assigned to either one or zero shapes, we assume that the
likelihood of a particular observation oij belonging to one of
the shapes (i.e., oij = ln for n > 0) for a particular gameboard
g is deterministic (i.e., p(oij = ln ∣g) = 1 if the location falls
within a target and 0 otherwise).
e prior belief about the likelihood of each individual
gameboard is represented by p(g). In our experiments, participants were instructed that the shapes were chosen at random and that all legal gameboards were equally likely (i.e.,
p(g) = 1/∣G∣ for all g, a uniform prior).
Bayes rule can be used to compute the posterior belief

e Experimental Task: e Shape Search Game
Participants in our task are presented with a  by  grid that
contains non-overlapping hidden shapes made up of individual grid cells. e hidden targets are randomly drawn from
a set that is known to the participant. ere are two phases
in each game: a sampling phase and a painting phase. In the
sampling phase, the player learns about the form and location
of the hidden shapes by choosing squares in the grid to uncover. On each trial, they make an observation at one location,
revealing either part of a hidden shape or an empty square.
When they think they know the location and form of all the
shapes they can stop sampling and enter the painting phase.
In the painting phase, the player is tested for their knowledge
of the shapes by “painting” any remaining squares they believe
belong to one of the shapes in the appropriate color.
e player is penalized one point for every observation
made in the sampling phase and two points for every error
committed in the painting phase (e.g., failing to ll in a square
that belongs to a shape). ese costs promote eﬃcient information search in two ways. First, the observation cost discourages sampling in locations whose contents can be inferred
from evidence that has already been uncovered. Second, it encourages continued sampling while there is still uncertainty
about the hidden shapes, since painting errors are more costly

720

j

0.6

0.6
0.4

Information Gain
Expected Savings

0.4

0.2

Random

0.2

0

20 40 60 80 100
Rank

0.0
0

5 10 15 20 25 30 35
Rank

Figure 2:

Cumulative frequency of ranks assigned to participants’ samples. A: In Experiment 1, the average rank of participants’
choices is higher than expected from a random sampling strategy, but
there is no diﬀerence between rankings assigned by the two models.
B: In Experiment 2, participants’ samples are more highly ranked according to EIG than ES.

observation and its observed outcome (oij = ln ) we can calculate information gain as:

n

(1)
For a new observation and its observed outcome (oij = ln )
we then calculate the resulting cost savings, or reduction in
expected costs:
S(B, oij = ln ) = EC(B) − EC(B∣oij = ln )

Experiment 2

0.8

0.8

0.0

EC(B) = Cerror ⋅ ∑ ∑ ∑ p(oij = ln ∣B)⋅[1−p(oij = ln ∣B)]
i

B 1.0

Experiment 1

1.0

Cumulative Frequency

Interested (cost-sensitive) Sampling e objective of the
game is to minimize the number of points accumulated, where
each individual observation costs Cobs points and each error
during painting costs Cerror points. Given these constraints,
we can quantify the value of observations with respect to the
overall goal of minimizing total costs. We assume that the likelihood of labeling a point oij with label ln during the painting phase is simply the marginal probability p(oij = ln ∣B),
and the cost associated with that action is tied to the uncertainty about its label when the sampling phase ends (e.g., if
p(oij = ln ∣B) = 1, the true label is known with certainty and
there is no chance of committing an error during painting)1 .
On each trial, the total expected cost EC(B) of ending the
sampling phase and entering the painting phase is de ned as:

A
Cumulative Frequency

about the identity of the hidden gameboard and to predict the
marginal probability of any point in the grid having any particular label ln (this is a very straightforward Bayesian approach
to the problem, see Gureckis and Markant, 2009).

I(B, oij = ln ) = H(B) − H(B∣oij = ln )

(4)

To account for uncertainty about the true outcome of an
observation, information gain for each possible outcome is
weighted by the predicted probability of that outcome occurring, giving the expected information gain for an observation
oij :

(2)

e savings achieved from feedback is oﬀset by the cost of
making the observation (Cobs ). To account for uncertainty
about the true outcome we nd the expected savings by weighting the savings for each outcome by its likelihood of occurring:

EIG(B, oij ) = ∑ p(oij = ln ∣B) I(B, oij = ln )

(5)

n

ES(B, oij ) = −Cobs + ∑ p(oij = ln ∣B) S(B, oij = ln ) (3)

As above, on each trial we compute EIG(B, oij ) for all locations in the grid, and assume that the optimal model chooses
the location with the highest value on each trial2 .
In applying each model to human choice data, the model is
“yoked” to the decisions of the player. On each trial, the models assign a value (either EIG or ES) to each point in the
grid. ese utilities can be used to compute choice probability of various grid locations. Aer revealing what the subject
actually chose on a given trial, the model updates its posterior beliefs about the current gameboard con guration. ese
new beliefs then feed into new predictions about the utility of
choosing each grid location. e process ends when the participant ends the game.

n

For each trial, ES(B, oij ) is calculated for all oij , giving a distribution of the expected saving for remaining observations in
the grid. An ideal learner maximizes this value by choosing
the location oij with the highest ES(oij ) on each trial.
Disinterested (cost insensitive) Sampling A “disinterested”
sampling norm values observations according to their eﬀect
on the learner’s beliefs without account for task-speci c costs
and bene ts. is captures the intuition that observations that
produce a large change in the agent’s beliefs tend to be more
useful than observations that have little or no eﬀect (i.e., nothing new is learned). In our approach this was modeled using
information gain, which values an observations according to
the expected reduction of uncertainty about the hidden gameboard. is uncertainty can be quanti ed by the Shannon entropy measured over the current belief distribution (H(B)).
Entropy is maximized when all hypothesized gameboards
are equally likely (as with our initial uniform prior), and minimized when there is only one possible hypothesis. For a given

Experiment 1: Rectangle Search
e rst experiment re-analyses a previously published result which introduced explicit task-speci c costs (Gureckis &
2
It is important to note that this represents a “greedy” policy that
chooses the best observation available on any given trial, but this
may not re ect the globally optimal solution. e current framework
could be extended to account for how participants might estimate the
value of sequences of observations. However, due to the computational complexity of nding this solution given the large number of
potential observations on any trial, for the present studies we focus
our analysis on the greedy model.

1
For shorthand, B represents a vector of probabilities p(g∣oij =
ln ), for all g ∈ G, that represents the full posterior distribution over
the entire space of gameboards. is is the agents current “belief distribution” about which gameboard is the current target.

721

ese modi cations resulted in a much less complex hypothesis space (e.g., only  hypotheses were possible at the beginning of an L/D game, and  for C/U games). Importantly,
because the two shapes involved in each type of game diﬀered
in area (for example, the ‘D’ shape contained a greater number
of lled squares than the ‘L’), the predictions of information
gain and expected savings diverged in the task.

Markant, 2009). Six participants played a series of games in
which they searched for three rectangular shapes, randomly
drawn from the set shown in Figure 1. e set of shapes
was displayed on screen throughout the game. Participants
were instructed that the three shapes in each gameboard were
non-overlapping and were shown a large number of examples
gameboard con gurations prior to the experiment.
Each observation made by a participant during the sampling phase was ranked according to the predictions of both
models (the median rank was used when multiple observations had equal value). Overall, the results show that people
consistently sampled points that were assigned a high value by
both models, with approximately 50% of their samples falling
within the top 10 ranked observations available to them (see
Figure 2A). In this experiment, however, the hypothesis set
that was used (rectangular shapes of varying shape and size)
led to highly similar predictions for both information gain and
expected savings, precluding a test of whether people were
sensitive to the costs in the task.
It is important to consider why the predictions of the two
models converged in this case. As discussed previously, a costsensitive learner should value observations that have higher
utility—that is, those that will reduce the likelihood of committing errors in the painting phase by the greatest amount.
Intuitively, this implies that learning about bigger shapes is
especially useful, since it will allow one to correctly label a
greater number of squares. is idea is illustrated in Figure 3A
for a simple hypothesis set made up of three rectangles in different locations. While observing a “hit” on any shape determines the true hypothesis (middle column), observing a
“miss” (righthand column) has diﬀerent utilities depending
on the size of the shape it rules out. For example, ruling out
the smallest shape (top row) leaves uncertainty about how to
label eight other squares, whereas ruling out the largest shape
(bottom row) leaves uncertainty about only four.
While the shape set used in Experiment 1 contained a range
of sizes, the fact that the hypotheses were “nested” (i.e., the
largest shapes overlapped with a set of progressively smaller
ones) meant that learning about larger shapes also tended to
rule out many hypotheses. As a result, the predicted choice
values according to both models were highly similar. For our
second experiment we created an alternative hypothesis space
in which there were clearer diﬀerences in both the size of alternative hypotheses and the choices that were related to shapes
of diﬀerent size, leading to a greater number of potential observations where the predictions of the two models diverged.

Methods
Participants Sixteen NYU undergraduates completed the study
for course credit or for a $ payment. e experiment was presented
on a standard Macintosh computer.

Materials A gameboard consisted of one of the four letter shapes

seen in Figure 3 placed in any location on the grid. All possible gameboards were generated, resulting in  gameboards for each letter,
or  for each game combination (L/D or C/U). For L/D games,
 gameboards were randomly selected from the pool of L and D
gameboards. is was repeated for C/U games. e resulting total of
 unique games were used for all participants. e order of games
played was randomized for each person.

Procedure Many aspects of the design were identical to Experi-

ment 1 (described in Gureckis and Markant, 2009), so we highlight
diﬀerences below. In Experiment 2 we sought to reduce any reliance
on the visual display for recalling the speci c shapes being used. Participants began the experiment with two training phases to memorize
the four shapes. In the rst, a letter cue (e.g., the character ‘L’ in a
standard computer font) was presented at the top of the screen along
with its corresponding shape, which appeared inside a x grid. e
participant was asked to copy the same shape onto an empty x grid
below. is was done twice for each letter (L, D, C, U) in randomized
order. In the second training phase, they were presented only with
the letter cue and an empty x grid, and asked to ll in the correct
letter shape from memory. is was repeated three times for each
letter in random order. In order to progress from one training trial
to the next, the participant was required to successfully reproduce
the correct shape. Training was followed by on-screen instructions
which were modi ed to re ect the new hypothesis spaces.
Before the sampling phase began, a -second cue was displayed
on the screen that indicated the type of game about to be played (the
characters “L D” or “C U”). is cue was also displayed on the right
side of the display during the game. is ensured that participants
were aware of the shapes that were possible but that they had to use
their memory of the actual shapes to guide their observations.
Sampling and painting phases proceeded in the same way as Experiment 1. e nal score was then displayed, including how many
points were the result of sampling and how many were the result of
painting errors. e lowest score obtained by the participant in any
game so far was shown to provide motivation and a means to evaluate their performance over time. Each participant played 30 games
at their own pace, resulting in a total of 480 games collected.

Results
Sample rank On each sampling trial, the ideal models were
used to compute the expected information gain and expected
savings for all remaining observations available. A participant’s decisions were ranked according to EIG and ES (if multiple observations had the same value, the median rank was
used). e relative frequency of each sample rank was computed for each participant across all trials, and averaged across
participants (see Figure 2B). e rank frequency shows that
participants’ choices were more highly ranked on average according to EIG than ES. Participants’ samples were ranked
within the top 5 observations according to EIG on approximately 57% of trials, whereas according to ES only 35% of
samples fell in the same range.

Experiment 2: Letter Search
In Experiment 2, we simpli ed the task to involve searching
for a single target in the grid. For the hypothesis space we created a set of simple “letter” shapes (see Figure 3B). ere were
two types of games: L/D games, where the hidden letter could
be an L or D, and C/U games, where the hidden letter could
be a C or U. In each game a single point belonging to the hidden shape was revealed before the participant began sampling.

722

A

HITS

MISSES
x

x

? ? ?

B

Experiment 2: Letter Search
D/L shape set
random sample

? ? ? ? ?

HYPOTHESES

x

h2

x

?

h1

x

x

? ? ? ? ?

h3

Hidden target

EIG

ES

C/U shape set
random sample

?

x

x

? ? ?
x

x

Figure 3: A: Illustration of the divergence between information gain and expected savings. Le: A hypothesis space is comprised of three

possible rectangles, h1 , h2 , and h3 . Middle: A hit on any one rectangle leads to the same number of hypotheses being ruled out, and no uncertainty about the label of any square in the grid. Right: A miss in any of the three locations rules out a single hypothesis, but the predictive utility
of the sample diﬀers based on its location. e labels for 8 squares are uncertain following a miss that rules out rectangle h1 , 6 squares following
a miss ruling out h2 , and 4 squares following a miss ruling out h3 . B: Gameboard design and example model predictions in Experiment 2. Two
types of games were possible: L/D games, where the hidden shape could be an L or D, and C/U games, where the hidden shape could be a C
or U. Predicted value distributions for EIG and ES are shown for the rst sampling trial in each kind of game, with a darker value indicating a
higher value according to the model.

Discussion

Model ts We next computed the likelihood of participants’
decisions under the two alternative models. For each trial, the
value of available observations was transformed into choice
probabilities using the somax function:
P (oij ) =

eβ⋅V (oij )
∑x,y eβ⋅V (oxy )

e results of our second experiment show that people performed well compared to the ideal searcher model, frequently
choosing highly ranked observations and consistently performing better than expected by a random search strategy. In
addition, participants rarely gathered more information than
necessary, which is consistent with prior work showing that
people are sensitive to costs incurred by oversampling (Fu &
Gray, 2006). Most importantly, we found that participants’
sampling choices were better described by information gain
than a cost-sensitive utility measure (expected savings).
Prior studies of human information collection have focused
to a large extent on “disinterested” accounts of sampling decisions, showing that people are sensitive to the amount of information conveyed by diﬀerent sources of information (Nelson,
McKenzie, Cottrell, & Sejnowski, 2010). However, this line
of work has oen failed to consider whether people account
for variations in task-speci c utility when making sampling
decisions. In our task, we introduced penalties for information access and uncertainty that altered the optimal sampling
strategy. By manipulating the set of hypotheses in Exp. 2, we
showed how sampling based on an information-maximizing
strategy can be distinguished from cost-sensitive sampling.
With respect to the distinction we began this paper with, our
results suggest that people (at least in this task) prefer to gather
information according to a “disinterested” measure of value.
Notably, diﬀerences in value between choices were not directly observable by the participant (as opposed to when some
observations are more costly or diﬃcult to make than others).
Whether a given observation was valued diﬀerently according
to EIG and ES depended upon the set of hypotheses remaining, and this divergence could change from trial to trial in response to new data. As a result, establishing which model pro-

(6)

e parameter β was t on an individual basis for each model
by maximizing the log-likelihood summed across all observations made by a participant. In all cases, EIG provided a better
t to participants’ data than ES (Table 1).
Stopping decisions Our nal analysis focused on participant’s decisions to stop sampling. While EIG and ES make
the same prediction as to when sampling should stop3 , we
were interested in whether people showed any sensitivity to
the cost of collecting information. If people uncovered more
squares than necessary it would suggest a failure to account
for the cost of new observations (either in terms of ES or
EIG). We classi ed each game according to whether the person decided to stop sampling before the trial predicted by the
model (“early”), on the same trail (“optimal”), or aer that
trial (“late”). On average, participants ended sampling early
(M = 0.46, SD = 0.14) or on the optimal trial (M = 0.50,
SD = 0.14) on a similar proportion of games. In contrast,
participants oversampled very rarely (M = 0.04, SD = 0.01).
3
is convergence was due to the cost structure we used, in which
the penalty for stopping before the hidden target was known was
greater than the cost of an additional sample. Increasing the cost of
sampling relative to the cost of errors would lead to ES predicting
earlier stopping decisions than EIG.

723

Acknowledgments

vided a better account required tting them to participants’
decisions across a variety of choice contexts. is highlights
a feature of our approach in that we could evaluate diﬀerent
sampling strategies using a set of highly variable choice sequences. rough the use of a well-de ned hypothesis set and
explicit cost structure, the “shape-search” task provides a useful framework for studying how information search decisions
and task demands interact over the course of learning.
Of course, one potential caveat of the current study is that
(due to computational demands) we evaluated a greedy decision policy such that the predicted value of a new observation
does not take into account the consequence or utility of subsequent actions. It is possible that fully accounting for sequential dependencies in the search problem may alter the optimal
utilities computed by the model. However, one might reasonably question if the computational demands of such a multistep planning process are within reach of human reasoners.
In addition, it is unclear that accounting for multi-step planning strategies would alter the choice utilities in a way that
would bias for or against the results we report. Also note that
comparing Exp. 1 and 2 illustrates that expected saving is not
always at an advantage (i.e., in some choice environments the
models become less distinguishable).

e authors would like to thank Dan Navarro, Jonathan Nelson, and the other reviewers for their helpful comments. is
work was supported by the Intelligence Advanced Research Projects
Activity (IARPA) via Department of the Interior (DOI) contract
D10PC20023. e U.S. Government is authorized to reproduce and
distribute reprints for Governmental purposes notwithstanding any
copyright annotation thereon. e views and conclusions contained
herein are those of the authors and should not be interpreted as necessarily representing the oﬃcial policies or endorsements, either expressed or implied, of IARPA, DOI, or the U.S. Government.

References
Chater, N., Crocker, M., & Pickering, M. (1998). e rational analysis of inquiry: the case of parsing. In M. Oaskford & N. Chater
(Eds.), Rational models of cognition (pp. 441–468). Oxford
University Press.
Edwards, W. (1965). Optimal strategies for seeking information:
models for statistics, choice reation times, and human information processing. Journal of Mathematical Psychology, 2,
312–329.
Fu, W, & Gray, W. (2006). Suboptimal tradeoﬀs in information seeking. Cognitive Psychology, 52(3), 195–242.
Fu, W. (2011). A dynamic context model of interactive behavior. Cognitive Science, 35(5), 874–904.
Gureckis, T., & Markant, D. (2009). Active learning strategies in
a spatial concept learning game. In N. Taatgen & H. van
Rijn (Eds.), Proc of the 31st annual conference of the cognitive science society (pp. 3145–3150). Cognitive Science Society. Austin, TX.
Juni, M., Gureckis, T., & Maloney, L. (2011). Don’t stop ’till you get
enough: adaptive information sampling in a visuomotor estimation task. In L. Carlson, C. Hölscher & T. Shipley (Eds.),
Proceedings of the 33rd annual conference of the cognitive science society. Cognitive Science Society. Austin, TX.
Lindley, D. (1956). On a measure of the information provided by an
experiment. Annals of Mathematical Statistics, 986–1005.
Mackay, D. (1992). Information-based objective functions for active
data selection. Neural Computation, 4, 590–604.
Nelson, J. (2005). Finding useful questions: on bayesian diagnosticity, probability, impact, and information gain. Psychological
Review, 114(3), 677.
Nelson, J, & Cottrell, G. (2007). A probabilistic model of eye
movements in concept formation. Neurocomputing, 70(1315), 2256–2272.
Nelson, J., McKenzie, C., Cottrell, G., & Sejnowski, T. (2010). Experience matters: information acquisition optimizing probability
gain. Psychological Science, 21(7), 960–969.
Rehder, B., & Hoﬀman, A. (2005). Eyetracking and selective attention
in category learning. Cognitive Psychology, 51, 1–41.
Stephens, D., & Krebs, J. (1986). Foraging theory. Princeton, NJ:
Princeton University Press.
Tversky, A., & Edwards, W. (1966). Information versus reward in binary choices. Journal of Experimental Psychology, 71(5), 680–
683.

Conclusion
So, why might human reasoners preferentially adopt “disinterested” sampling over “interested” sampling? One possibility is that sampling based on information gain (or other “disinterested” norms) may re ect a general purpose strategy that
is useful in a variety of contexts. In particular, information
gain can still be computed even when the cost of uncertainty
(i.e., not knowing which hypothesis is true at the end of sampling) is diﬃcult to predict. In addition, in many natural environments it may be consistent with the predictions of a costsensitive utility function, as illustrated by our rst experiment.
At the very least, our results highlight the need to understand
the kinds of problems that lead people to adapt to task-speci c
costs in lieu of a general-purpose, “disinterested” approach to
information search.

Subj
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

βEIG
17.544
5.18
5.93
6.96
10.15
12.34
9.64
7.63
5.36
7.79
5.85
6.39
6.97
7.02
8.94
10.0

Table 1: Model ts
βES
0.95
0.88
0.77
0.75
0.89
1.06
1.20
0.98
0.90
0.91
0.85
0.92
1.03
0.86
0.99
0.76

-LLH(EIG)
215.05
391.77
326.45
325.81
241.04
250.17
276.05
308.63
354.49
315.92
340.22
328.24
319.96
283.32
293.38
293.65

-LLH(ES)
334.65
426.18
388.79
403.61
324.30
329.72
339.62
373.03
387.43
387.56
388.05
373.22
362.27
341.79
371.22
403.22

724

