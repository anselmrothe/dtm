UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Flexible sequence learning in a SOM model of the mirror system

Permalink
https://escholarship.org/uc/item/39r423bn

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)

Authors
Thill, Serge
Behr, Josef
Ziemke, Tom

Publication Date
2012-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Flexible sequence learning in a SOM model of the mirror system
Serge Thill (serge.thill@his.se)
Interaction Lab, School of Humanities and Informatics
University of Skövde
54128 Skövde, Sweden

Josef Behr (jbehr@uos.de)
Institut für Kognitionswissenschaft, Neurokybernetik
University of Osnabrück
49069 Osnabrück, Germany

Tom Ziemke (tom.ziemke@his.se)
Interaction Lab, School of Humanities and Informatics
University of Skövde
54128 Skövde, Sweden

Sequencing via ordinal nodes and conditions of
satisfaction

Abstract
We present initial work on a biologically and cognitively inspired model that may allow embodied agents to autonomously
learn sequences of action primitives (forming an overall behaviour). Specifically, we combine a flexible model of sequence generation with a model of parietal mirror neuron activity. The main purpose is to illustrate that the approach is
viable. Although further work is needed to improve the results sketched out here, the concept is sound and relevant both
to efforts in modelling mirror neuron activity and enabling artificial embodied agents to autonomously learn sequences of
action primitives.
Keywords: Behavioural sequence learning; Ordinal node
model; Self-organising maps; Mirror neurons

Introduction
We are concerned with the problem of generating sequences
of action primitives which are flexible with respect to the precise time it takes to execute the different components (primitives) of the same sequence at different times. A thorough discussion of the issue is given, for instance, by Sandamirskaya
& Schöner (2010). In a nutshell, part of the problem is
that one cannot simply chain together the different primitives through, for example, simple Hebbian learning. Rather,
mechanisms must exist for keeping track of the current location in the sequence, including ways of verifying that the current action has successfully completed or failed to complete.
Sandamirskaya & Schöner (2010) describe a general framework which can address these issues and we briefly sketch the
main points in the next section.
Overall, the aim of the work in the present paper is to combine said framework with a model of parietal mirror neuron
activity (Thill et al., 2011) and to illustrate that such an approach is, in principle, viable. Importantly, since the mirror
neuron model used here autonomously organises itself, the
work proposed here may be relevant and helpful in designing
artificial embodied agents that should autonomously learn sequences of actions and use them to predict actions of others.

The gist of the framework by Sandamirskaya & Schöner
(2010) is the existence of ordinal nodes which essentially
count through the sequence. These nodes are implemented
via coupled dynamical systems (see Methods), designed so
that only one node can be active at a time. Upon completion of the element of the sequence represented by the active node, activation is passed onto the next node in the sequence. In their work (e.g. Sandamirskaya & Schöner, 2010;
Sandamirskaya et al., 2011), the action primitives forming the
sequence exist in the sensorimotor representation of an embodied agent, implemented using techniques from Dynamic
Field Theory (Schöner, 2009; Spencer et al., 2009). This has
the advantage that the sensorimotor representations of these
primitives are stable (since they are essentially stable fixedpoint attractors), which makes it particularly simple to link
specific locations in the dynamic fields representing the sensorimotor space of the agent to specific ordinal nodes. Part
of the challenge of the work presented in the present paper
is to illustrate that the ordinal node system could also be attached to a representation with more noise and less stability
than dynamic fields.
The decision that a given action primitive has completed
is implemented a separate system (also exploiting dynamic
fields) that checks for a Condition of Satisfaction (CoS). One
of the open challenges here is the question of how to best
learn the CoS for specific primitives (including identifying
that the primitive has, for whatever reason, failed). It is not
the purpose of the present work to address the open issues
regarding the CoS - rather, we focus on combining the ordinal
node model with a model of mirror neuron activity discussed
in the next section.

Mirror system sequences
One example of sequencing in biology is given by the hypothesised functioning of the mirror system. Without entering the debate on what higher-level cognitive abilities mirror

2423

Combining models

neurons may or may not be useful/essential for (see for instance Hickok, 2008; Rizzolatti & Sinigaglia, 2010, for such
a debate), it appears that parietal mirror neurons in macaque
monkeys organise into pools of neurons responding to specific motion primitives (e.g. a reach or a grasp but not both;
Fogassi et al., 2005). It has then been hypothesised (e.g.
Chersi et al., 2006) that these pools of neurons can be chained
together to form sequences of simple, often-encountered actions (such as reach-grasp-bring to mouth for eating). Models on the basis of this hypothesis have proven useful, for instance, in putting forward theories unifying apparently conflicting results on interference and facilitation in action language processing (Chersi et al., 2010).
A particular model that specifically addresses the development of parietal mirror neurons has been previously presented by some of us (Thill et al., 2011). This model uses a
self-organising map (SOM) to illustrate how a “blank” structure, through the organisational principles of SOMs can autonomously form an organisation whose activity resembles
that of parietal mirror neurons.
The inputs to the model represent an arbitrary encoding
of observed (or executed) motion primitives (e.g. based on
changes in position per time step) and contextual information (including, for instance, affordances in the perceived
scenery). These two components are sampled from two distinct spaces (of arbitrary dimensionality) and concatenated
into a single input vector as required by standard SOM implementations (Kohonen, 1997). The model is trained on repeated presentations of all combination of motion primitives
and contexts. After training, the model can be run on-line by
continuously feeding it input vectors and some plasticity (allowing, for instance, the learning of new primitives) can be
retained by not reducing the learning rate to 0 (albeit keeping
it at a low level, see Thill & Ziemke, 2010).
The trained maps organise in a fashion remarkably similar to that of parietal mirror neurons (Fogassi et al., 2005):
Within the map, different areas encode different action primitives (which could represent motions such as reaching, grasping or bring-to-mouth, similar to e.g. Chersi et al., 2006).
Within the area encoding one such primitive, some nodes are
active whenever the model input encodes that primitive. Others are active only if the action input additionally encodes a
specific context in which the primitive is observed (usually
sufficient to specify the most likely goal of the action, see
Thill et al., 2011). The proportion of context-independent
nodes is a direct consequence of the way inputs are represented (specifically, of the ratio between the maximal variability in encoding the primitives and contextual information
respectively, called β in the model). Exploring how β (for
which values between 1 and 5 cover most aspects of interest)
affects the organisation of the maps revealed that, for β ≈ 3.5,
the proportion of context-independent nodes is similar to the
corresponding neurophysiological data observed in the parietal mirror area of macaques (Fogassi et al., 2005).

Previous models of parietal mirror neuron activation tend not
to address the timing aspect of the chains in much detail, focussing instead on merely linking the different pools forming
a chain through hard-coding (Chersi et al., 2006, 2010) or, for
instance, Hebbian learning (Erlhagen et al., 2007). With the
exception of Chersi et al. (2010), these models do not take
into account that the pools encoding the same primitive under
different goals are not entirely distinct (Fogassi et al., 2005).
Thill et al. (2011), whose main focus is the exact nature of
this overlap between populations, do not specifically address
chain formation at all.
The present paper therefore presents an augmented version
of the model from Thill et al. (2011). Specifically, we now
implement the learning of chains of primitives, using the approach of Sandamirskaya & Schöner (2010). This new model
then allows us to address a number of open issues: to what extent is the ability to activate the correct (and only the correct)
sequence of events (given the first element) affected by the
overlap between neural populations? When observing an action primitive in an unknown context, is it possible to predict
all possible chains this action could be part of?
These issues are relevant, both for our understanding of
(in particular) sequences in mirror neuron activity and for the
ability to endow artificial agents with similar abilities. If one
subscribes to the hypothesis that mirror neuron activity helps
us understand the actions of others (see Rizzolatti & Sinigaglia, 2010, for a thorough review and discussion), then the
ability to predict the likely outcome of an action given the initial movement based on the resulting mirror neuron activity is
a desirable ability. This includes the ability to autonomously
learn sequences of actions as well as the ability to both correctly identify a sequence if the context is clear and predict all
possible sequences if the context is ambiguous (for instance,
a familiar gesture observed in a completely new context).

Methods
Overall model design
The model (Fig. 1) is composed of a self-organising map
which is meant to represent parietal mirror neuron activation
(Thill et al., 2011) and an ordinal node model for sequence
learning (Sandamirskaya & Schöner, 2010). The activity over
time in the SOM is used (1) to train the sequence learning
model, (2) to activate learned sequences and (3) to provide
the input necessary to move from one sequence element to
the next. It therefore combines the idea of chaining pools of
neurons (e.g. Chersi et al., 2006) with the flexible execution
of sequences provided by the ordinal nodes model of Sandamirskaya & Schöner (2010).

Self-organising maps as a mirror system
The self-organising maps used in this paper are in essence
identical to those used by Thill et al. (2011) and are trained
in the same manner. The only difference is that the previous
maps explicitly dedicated part of their space to the theoretical

2424

Memory nodes
Ordinal nodes

IC

CoS fulﬁlled

SOM

Figure 1: Overall model architecture. Activation in the
SOM feeds into the ordinal nodes, both to activate the sequence (green) and to move between sequence elements (red)
if the CoS is fulfilled (blue). Connections between the SOM
and the nodes are bidirectional; node activity can thus also be
used to activate regions in the SOM (omitted in the figure for
clarity).

Figure 2: Sequences in the SOM. Shown are the activation of
three primitives (columns) seen under two different contexts
(rows) for a map with β = 3. Dark regions indicate most activity and are clearly in different locations for different primitives. For the same primitives in different contexts, similar
regions are activated but the overlap between the most active
neurons in each context is limited (see Thill et al., 2011, for a
thorough discussion).

possibility of learning motion primitives from a second limb
(see Thill & Ziemke, 2010, for details). Since that is irrelevant here, the present maps are trained assuming the need to
represent just one limb. The trained maps therefore represent,
as before, five motion primitives observed under two different
contexts. They behave as described in the introduction: input
vectors consisting of a concatenation of observed/executed
motion encoding and contextual information are continuously
fed to the map. Depending on the previously discussed ratio
β, some nodes of the map will be active regardless of the
contextual information whereas others will be sensitive to the
latter (see Thill et al., 2011, for a complete discussion of the
definition of activity).

Ordinal node model
The ordinal node model used here largely follows Sandamirskaya & Schöner (2010) and is described by the following equations:
τḋi (t)

= −di (t) + hd + c0 f (di (t))

m
−c1 ∑ f (di′ (t)) + c2 f di−1
(t)

(1)

i′ 6=i

−c3 f (dim (t)) − cCoS IC (t) + cin I
τd˙im (t)

Primitive 3

Context 2

I

Primitive 2

Context 1

Primitive 1

= −dim (t) + hm + c4 f (dim (t))
−c5 ∑ f (di′ (t)) + c6 f (di (t))

(2)

i′ 6=i

where di refers to the activation of the ith ordinal node
(and dim is the associated memory node needed for proper

functioning, see Sandamirskaya & Schöner (2010) for details), f (·) is a sigmoidal nonlinearity and the constants in
the present implementation are chosen as: c0 = 7.2, c1 = 3.6,
c2 = 4.8, c3 = 0.8, c4 = 4, c5 = 2, c6 = 2.6, cin = 0.1,
cCoS = 0.2, hd = −5 and hm = −2. A detailed discussion
of the functioning of the model is given by Sandamirskaya &
Schöner (2010). We deviate in two minor aspects: (1) The
term cin I is added and provides an external input (obtained
from the activity in the SOM described above). This is only
used at the beginning of a sequence to activate the first ordinal
node. (2) We simplify the Condition-of-Satisfaction (CoS)
aspect. In the original model, this is given by an additional
dynamic field which is able to “perceive” that the CoS has
been reached. Here, the inhibitory activation is obtained from
the same SOM that would provide I for the activation of the
first node, which simplifies the design of the model. Since
the model is not actually implemented in an agent, there is
also no point in devising a sophisticated ”perception” of the
CoS here. Rather, the CoS is presumed fulfilled after a randomly chosen number of time-steps and the inhibitory activation released to the ordinal model, thus moving the model
onto the next element of the sequence. This is acceptable for
the present purposes since the point here is to illustrate the
learning of sequences, not the ability to autonomously detect that an element of a sequence has completed (or failed
to complete). An implementation of this model in an agent
would of course need to address this aspect in more detail.

2425

Task and learning

Results

1
0.9
0.8
0.7
Proportion

For each β value between 1 and 5 (in increments of .5), 100
maps have been generated. Each map is activated manually
with a series of input vectors which simulate a sequence of 3
motion primitives being executed first in one context and then
in another (see Fig. 2 for an example of two sequences). Two
sets of ordinal nodes are used to learn these two sequences.
Learning is achieved during manual activation of the map by
clamping the relevant ordinal node to an active state and then
using simple Hebbian learning to train weights between this
node and all neurons in the map (with normalised activation).
After training, any weights below a threshold of 0.5 are set to
0 to allow only the SOM nodes with the strongest activation
to connect with the relevant ordinal nodes.
Of particular interest are the following questions: Will both
sets of ordinal nodes correctly activate if the SOM activity is
that of the first element of their respective sequences? Also,
will a set of ordinal nodes trained on the first sequence remain inactive if the SOM activity represents the first element
of the second sequence (and vice versa)? Illustrating these
behaviours would confirm good performance of the model
given that sequences are correctly activated if and only if the
map activity corresponds to their first element. It should be
remembered at this point that map activity is noisy and fluctuates over time - the task is therefore not trivial.
An additional interest is the behaviour of the model in case
of ambiguous contextual information. As discussed in the
introduction, this could correspond to observing a familiar
primitive in an unfamiliar context and predicting what the
likely outcome of the action could be. It is of course a matter
of debate what the exact behaviour of the model should be
in this case; one could for instance argue that it should depend on how similar the unfamiliar context is to previously
encountered ones. Here, we simply investigate the behaviour
if the vector encoding contextual information is truly ambiguous, namely by corresponding to the point in the input
space whose coordinates are equidistant from the subspaces
encoding all known contexts. In other words, the ambiguous context encoding vector cannot be uniquely assigned to
any previously encountered case. We simply postulate that,
in the absence of any information that could favour either of
the chains, the desirable behaviour of the model is to activate
both, essentially predicting that both behaviours are equally
likely.

0.6
0.5
0.4
0.3
0.2
Correct activation
Correct non−activation

0.1
0

1

2

3
β

4

5

Figure 3: Correct activation/non-activation. Dark bars indicate the proportion of cases in which presenting the first element of a sequence correctly triggered the sequence. Light
bars indicate the proportion of the cases that correctly trigger
which correctly remain silent if the first element presented
has the same motion primitive but different contextual information.
ond sequence) becomes harder as β increases. This is indeed
what we find (see Fig. 3). Specifically, it is possible, in most
cases, to correctly activate a sequence by presenting its first
element in map activation (although it does fail on occasion,
likely due to the noisy map activity). Importantly, this is independent of β, which is expected. The light bars in Fig. 3
then show how many (proportionally) of the sequences correctly activated by their own first element also remain silent
when the first element of the second possible sequence is presented instead. As expected, this number decreases over time
but remains over .5 in all cases.
However, this measure iterates over sequences that are correctly activated (or not); it does not measure the number of
maps for which both sequences are correctly activated (or
not). The evolution of this proportion is shown in Fig. 4
(black bars) and is decreasing more dramatically as β increases. At the same time, it should be noted that for e.g.
β = 4, ≈ 60% of nodes in the SOM encoding a given primitive
are active independent of context (leaving only 20% capable
of uniquely identifying each of the contexts).

Correct activation/non-activation

Correct behaviour under ambiguous context

For each value of β, 100 sets of 2 sequences have been
learned. Per set, the sequences differ only in the context in
which they have been executed. As β increases, the proportion of neurons active in one but not both of the contexts decreases (Thill et al., 2011). It can therefore be expected that
the basic task of correctly activating a sequence if the map
activity corresponds to its first element (and not activating
said sequence if the contextual information is that of the sec-

The second interesting question was whether both sequences
would be activated by the first motion primitive shown in a
perfectly ambiguous context. Considered independently of
the performance on the previous task, we find that a large
number of models indeed activate both sequences given an
ambiguous context In particular, we find that this proportion
increases with β (from0.4 to > 0.9), likely due to the increasing number of neurons which are active irrespective of con-

2426

Discussion

1
Fully correct on trained
Correct prediction

0.9

Insights from the model

0.8

Proportion

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

1

2

3
β

4

5

Figure 4: “Perfect” models. Black bars indicate proportion
of cases in which the same model activates the correct sequence for each of the two learned sequences if presented
with the first element of each sequence and does not activate
the wrong sequence. Light bars indicate the proportion of
cases fulfilling the first condition which also correctly activate
both sequences if the contextual information is ambiguous
.

text.
This illustrates the expected effect of β: as the distinction
between contexts diminishes, activation of both sequences is
facilitated. However, this measure can again be seen as being a bit too general since there is not necessarily anything
special about activating both sequences if the same model
failed to not activate a sequence when primed with the first
element (including the contextual information) of the second sequence. The more interesting question is therefore
simply how many of the models that correctly behave given
the learned sequences (black bars in Fig. 4 also behave as
expected given the first primitive under an ambiguous context (within the context of this work, we can call these models “perfect”, since they fulfil all the expectations set out to
them). Surprisingly, this proportion appears to be independent of β (light bars in Fig. 4), although it has to be kept in
mind that for β ≥ 3, the number of models which fulfill the
first condition is rather low.
In other words, if a model is capable of correctly activating
the relevant sequence (and only that sequence) given a full
first element of that sequence, it is likely to also activate both
sequences if given the first primitive under an ambiguous context. This is the most significant result in the present paper:
although it is increasingly difficult to find a model which will
correctly activate its sequences given the first element as β increases, it is then much easier to find a model which can also
activate both sequences in the case of perfectly ambiguous
contextual information.

Most of the results shown have their main purpose in illustrating that the model works as expected, including the increasing difficulty in obtaining “perfect” models as the β values
of the underlying maps increase. The exact extent of this increase in difficulty is hard to judge from the work presented
here as several aspects can be improved. First and foremost,
the parameters for the ordinal nodes model are set independently of β, even though β has a rather significant effect on
the input into the ordinal nodes and therefore the behaviour
of the sequences. The fact that it was possible at all to create
successful models across the entire range of β values underlines the potential of the approach. In future work, however,
the focus would have to be on β values around 3 − 3.5, since
these are the values for which the activity in the SOM most
resembles that observed by Fogassi et al. (2005) in parietal
mirror neurons (Thill et al., 2011).
The self-organising maps themselves are randomly generated; nonetheless it seems that some are more suited for a
combination with an ordinal nodes model than others (since
some “perfect” models were found even for β = 5, although
the number was very low) and more work would be needed
to investigate what features of these maps, if any, facilitate
the task. Insights into this question could prove very valuable in more general future work combining the ordinal node
model with sequences that are generated in systems which do
not offer the “nice and clean” activation patterns of dynamic
fields.
The connections between the activity in the map and the ordinal nodes are learned with a simple general Hebbian learning approach and the only transformation of the map activity
consisted of a simple normalisation. Again, this is about the
simplest approach imaginable and it is likely that improvements, including possible non-linear transformations of map
activity, can lead to a higher proportion of “perfect” models
for larger values of β (in particular of course for β ∈ [3, 3.5]).
The most interesting result in the present paper was that the
largest difficulty resided in finding models which perform as
expected when started with a first element from either learned
sequence and not, as one might have expected, in finding such
a model that also perform correctly on the prediction task.
This is encouraging as it illustrates that the concepts of using
a combination of our previous SOM models of mirror neurons
and the ordinal node model has potential, not just for generating the sequences one wishes to generate but also for predicting what sequences observed actions can be part of; this both
in the case where the contextual information strongly favours
one of the learned sequences and when the contextual information is perfectly ambiguous.
Again, there is a need for future work in this aspect. It
seems reasonable (for the purposes of predicting likely sequences an observed primitive could belong to) to expect that
a perfectly ambiguous context should activate all candidates
but it is less clear - and beyond the scope of what can be

2427

achieved in this space - what should happen if the context is
merely ambiguous but closer in input space to some known
contexts than others. Should the model simply activate the
most likely sequence or would one prefer a mechanism that
could attach a confidence value - indicated for instance by the
time it takes the first ordinal nodes of all candidate sequences
to activate - to indicate most and least likely sequences?

Overall relevance
The work presented here is relevant for at least two areas.
First is the modelling of mirror neurons as it is one of the
first attempts to explicitly include the idea that executing the
same action primitive at different points in time can lead to
different durations, thus going beyond simple Hebbian-type
associations directly between the primitives forming an overall action sequence (e.g. Chersi et al., 2010). Second, by
modelling the specific organisation of parietal mirror neurons
(which can develop autonomously, see Thill et al., 2011) and
using that as an input to the ordinal node system, the model
may provide a way for an artificial agent to learn sequences
of primitives online and autonomously, which is still an open
challenge (Sandamirskaya & Schöner, 2010).
The practical future applications are thus primarily in the
design of future artificial cognitive systems; however all aspects of the model are inspired by biology; any implementation of the model could thus also be relevant to improve our
understanding of the analogous biological systems.

Conclusion
We presented an initial implementation of a mirror system activity model augmented with a framework for generating sequences. The main purpose was that it is in principle feasible
to use the ordinal node framework to this effect. Although
further work is needed to improve the quality, it was possible to show that the model can learn sequences based on the
noisy SOM activity as well as correctly predict the likely sequence an observed initial primitive can belong to (including
predicting both if both are equally likely). Since the SOM
autonomously organises, the model presented here may be a
viable candidate for autonomous sequence learning using the
ordinal node framework (Sandamirskaya & Schöner, 2010).

Acknowledgments

motor chains.
Frontiers in Neurorobotics, 4(4),
DOI:10.3389/fnbot.2010.00004.
Erlhagen, W., Mukovskiy, A., Chersi, F., & Bicho, E. (2007).
On the development of intention understanding for joint
action tasks. In Proceedings of the 6th ieee international
conference on development and learning (p. 140-145). Imperial College London.
Fogassi, L., Ferrari, P. F., Gesierich, B., Rozzi, S., Chersi,
F., & Rizzolatti, G. (2005). Parietal lobe: from action
organization to intention understanding. Science, 308, 662667.
Hickok, G. (2008). Eight problems for the mirror neuron
theory of action understanding in monkeys and humans.
Journal of Cognitive Neuroscience, 21(7), 1229-1243.
Kohonen, T. (1997). Self-organizing maps. Heidelberg:
Springer.
Rizzolatti, G., & Sinigaglia, C. (2010). The functional role of
the parieto-frontal mirror circuit: interpretations and misinterpretations. Nature Reviews Neuroscience, 11(4), 264274.
Sandamirskaya, Y., Richter, M., & Schöner, G. (2011). A
neural-dynamic architecture for behavioral organization of
an embodied agent. In Ieee 10th international conference
on development and learning (icdl), frankfurt.
Sandamirskaya, Y., & Schöner, G. (2010). An embodied
account of serial order: how instabilities drive sequence
generation. Neural Networks, 23, 1164-179.
Schöner, G. (2009). Toward a unified theory of development. In J. P. Spencer, M. S. C. Thomas, & J. L. McClelland (Eds.), (p. 25-48). Oxford.
Spencer, J. P., Perone, S., & Johnson, J. S. (2009). Toward a unified theory of development. In J. P. Spencer,
M. S. C. Thomas, & J. L. McClelland (Eds.), (p. 86-118).
Oxford.
Thill, S., Svensson, H., & Ziemke, T. (2011). Modeling the
development of goal-specificity in mirror neurons. Cognitive Computation, 3(4), 525-538.
Thill, S., & Ziemke, T. (2010). Learning new motion primitives in the mirror neuron system: A self-organising computational model. In S Doncieux et al (Ed.), Sab 2010, lnai
6226 (p. 413-423). Heidelberg: Springer.

This work was supported by the European Commission
FP7 project NeuralDynamics, (A neuro-dynamic framework
for cognitive robotics: scene representations, behavioral sequences, and learning), Grant agreement no. 270247.

References
Chersi, F., Mukovskiy, A., Fogassi, L., Ferrari, P. F., & Erlhagen, W. (2006). A model of intention understanding based
on learned chains of motor acts in the parietal lobe. In Proceedings of the 15th annual computational neuroscience
meeting. Edinburgh, UK.
Chersi, F., Thill, S., Ziemke, T., & Borghi, A. M.
(2010).
Sentence processing: linking language to

2428

