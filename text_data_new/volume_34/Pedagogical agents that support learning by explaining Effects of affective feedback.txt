UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Pedagogical agents that support learning by explaining: Effects of affective feedback

Permalink
https://escholarship.org/uc/item/3q92z8bz

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)

Authors
Hayashi, Yugo
Matsumoto, Mariko
Ogawa, Hitsohi

Publication Date
2012-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Pedagogical agents that support learning by explaining:
Effects of affective feedback
Yugo Hayashi (yhayashi@fc.ritsumei.ac.jp)
Mariko Matsumoto (is039081@ed.ritsumei.ac.jp)
Hitoshi Ogawa (ogawa@airlab.ics.ritsumei.ac.jp)
College of Information Science and Engineering, Ritsumeikan University,
1-1-1 Nojihigashi, Kusatsu, Shiga, 525--8577, Japan

Abstract
The present study investigates how a conversational agent can
facilitate explanation activity. An experiment was conducted
where pairs of participants, who were enrolled in a
psychology course, engaged in a task of explaining to their
partners the meanings of concepts of technical terms taught in
the course. During the task, they interacted with a
conversational agent, which was programmed to provide
back-channel feedbacks and metacognitive suggestions to
encourage and facilitate conversational interaction between
the participants. Results of an experiment suggested that
affective positive feedbacks from conversational agent
facilitate explanation and learning performance. It is
discussed that a conversational agent can play a role for
pedagogical tutoring and triggers a deeper understanding of a
concept during an explanation.
Keywords: pedagogical
affective learning.

agents;

explanation

activities;

Introduction
The ever-evolving information and communication
technology has made it possible to support human cognition
by using systems which aids human interaction. Many
researchers in computer science are tackling on the theme of
developing embodied conversational agents to support
education. Recent studies on cognitive science and learning
science show that collaborative learning facilitates
understanding or acquisition of new concepts depends
greatly on how explanations are provided. In this study a
collaborative activity of making explanation is
experimentally investigated by using a conversational agent
that serves as a teaching assistant. The goal of the
experiment is to find out what kind of feedback from the
agents is most conducive to successful learning performance.

Related work
Explanations during collaborative activities
Number of studies on collaborative problem solving in
cognitive science revealed how concepts are understood or
learned. Researchers have shown that asking reflective
questions for clarification to conversational partners is an
effective interactional strategy to gain a deeper
understanding of a problem or a concept (e.g. Chi, Bassok,
Lewis, Reimann, & Glaser, 1989; Miyake, 1986; Salomon,

2001; Okada & Simon, 1997). It has also been demonstrated
that the use of strategic utterances such as asking for
explanation or providing suggestions can stimulate
reflective thinking and meta cognition involved in
understanding a concept. Playing different roles during
explanation is also said to help problem solvers reconstruct
external representation and concepts (Shirouzu, Miyake, &
Masukawa; 2002).
Studies that are discussed above suggest that how well one
can explain is the key to understanding and learning of a
concept. However, explanation becomes successful if
people have difficulties in retrieving and associating
relevant knowledge required for explanation activity.
Researches on collaborative learning have reported that
these difficulties rise among novice problem solvers
(Coleman, 1998; King, 1994). Also, it may not help learn a
concept if people cannot communicate with each other as in
when, for example, they use technical terms or phrases
unknown to others (Hayashi & Miwa, 2009).
It is assumed that one of the ways to help collaborative
problem solvers is to introduce a third-person or a mentor
who can facilitate the task by using prompts such as
suggestions and back-channels. However, it is often difficult
for one teacher to monitor several groups of collaborators
and to supervise their interaction during explanation in
actual pedagogical situations. Recently there are studies
which demonstrate that the use of conversational agents that
act as educational companions or tutors can facilitate
learning process (Holmes, 2007; Baylor & Kim, 2005).
Unfortunately, it has not been fully understood if and what
kinds of support by such agents would be more helpful for
collaborative learners. In this paper, the author will further
investigate this question through the use affective
expressions.

Pedagogical conversational agents as learning
advisers
Researchers in the field of human computer interaction have
conducted a number of experimental studies which involve
the use of pedagogical agents (e.g. Kim, Baylor & Shen,
2007; Reeves & Nass, 1996; Graesser & McNamara, 2010).
One point to be taken into consideration in studies of human
performance is the affective factor. This factor influences
people's performance in either negative or positive ways and

1650

several studies reported that such factors are especially
important in learning activities (Baylor & Kim, 2005). For
example, Bower & Forgas (2001) revealed that positive
moods can increase memory performance. Mayer & Turner
(2002) also demonstrated that positive state of mind can
improve text comprehension.
Moods may affect the performance of human activities
both verbally and non-verbally. In a study by Kim, Baylor,
& Shen (2007), which examined how positive and negative
comments from conversational agents affect learning
performance, a pictorial image of an agent was programmed
to project a textual message to the participant; in the
positive condition, a visual avatar produced a short
comment like "this task looks fun", while in the negative
condition, it produced a short comment like "I don't feel like
doing this, but we have to do it anyway". The results
showed that the conversational agents that provided the
participants with comments in a positive mood furnished
them with a higher motivation of learning.
The studies discussed above suggest that the performance
of explanation would also be enhanced if suggestions are
given in positive mood either verbally or through visual
feedbacks.

rest of the two words to his/her partner. This was repeated
but the words they explained the second time were different
from those in the first time. All participants received the
same prompts of suggestions from the agent on how
explanations should be given and how questions should be
asked about the concepts. After this pre-test, they took the
same test in the post-test. The descriptions of the concepts
they provided in the post-test were compared with those of
the pre-test to analyze if the participants gained a deeper
understanding of the concepts after the collaborative activity.
The whole process of the experiment took approximately 80
minutes (see Figure 1).

START
Pre-test
(20 minutes)
Explanation
(10 minutes)
No

For trial >= 4
trial ++

Yes

Research Goal
This study investigates how conversational agents can
facilitate understanding and learning of concepts. This paper
will focus on an agent which has a role that assists paired
participants to explain concepts to their partners during the
collaborative peer-explanation activity. A natural language
processing agent monitored the interaction between the
participants and provided prompts to them which were
generated by pre-defined rules. The research goal of this
study is to understand if the use of positive expressions
provided by a conversational agent facilitates collaborative
learners' understanding of concepts.

Method

Post-test
(20 minutes)

END

Figure 1: Experiment flow.

Experimental system
In the experiments, a computer-mediated chat system was
set up through computer terminals connected via a local
network and the interactions of the participants during the
activity were monitored. The system used in the
experiments was programmed in Java (see Figure2).

Experimental task and procedure
The experiment was conducted in a room where the
computers were all connected by a local area network.
Participants were given four technical terms presented on
the screen. They were: 'schema', 'short-term / long-term
memory', 'figure-ground reversal', and 'principle of
linguistic relativity', which had been introduced in a
psychology class. Along with the keyterms, a brief
explanation of the concept was described by a few sentences.
They were asked to describe the concepts of these words.
After this pre-test, they logged in the computer and used the
program installed in a USB flash drive (see the next section
for detail). The pairs of participants were communicated
through the chat program and one of the paired participants
was instructed to explain to their partner the meanings of the
words presented on their computer screen one by one. When
two of the four concepts were explained to their partner,
they switched the roles and the other partner explained the

1651

Server

Client program 1
(Student A)

Pedagogical
Conversational Agent

Client program 2
(Student B)

Figure 2: Experimental Setting.

Concepts for
explanation
Brief explanation
(normative description)
Dialogue history
(inputs and outputs
from participants)

‘Schema’
Schemata influence our attention, as we are more likely to
notice things that fit into our schema. If something
contradicts our schema, it may be encoded or interpreted
as an exception or as unique. Thus, schemata are prone to
distortion. They influence what we look for in a situation.
They have a tendency to remain unchanged, even in the
face of contradictory information. We are inclined to place
people who do not fit our schema in a "special" or
"different" category, rather than to consider the possibility
that our schema may be faulty.

Explanation input

Figure 3: Screenshot of the chat system.

The system consists of three program modules of Server,
Chat Clients, and Agent, all of which are simultaneously
activated. Multi-threads are used so that the server program
can send all messages to the clients' chat system and the
agent simultaneously.
The pedagogical agent used in this study is a simple rulebased production system typical of artificial intelligence. It
is capable of meaningfully responding to input sentences
from users and consists of three main modules: Semantic
Analyzer, Generator, and Motion Handler (see Figure 4).

Semantic
Analyzer
Module

input

Table 1: Types of output messages from the agent.

Type of messages
Input messages (Detected
key words are in Bold)

Generator Module

Working
Memory

are being used in the explanation task (e.g. "I think that a
schema is some kind of knowledge that is used based on
one's own experience." (detected key words are shown in
bold italic). Next, the extracted keywords are sent to the
working memory in the generator and processed by the rule
base, where various types of rule-based statements such as
'if X then Y' are stored to generate prompt messages (if
there are several candidates of matching statements for the
input keywords, a simple conflict-resolution strategy is
utilized). When the matching process is completed, prompt
messages are selected and sent back to the working memory
in the generator. The messages generated by the rule base
are also sent to the motion handler module to activate an
embodied conversation agent, a computer-generated virtual
character which can produce human-like behaviors such as
blinking and head-shaking. Each output message is textually
presented in a text filed on the computer display (See next
sections for details).
Several types of output messages are presented by the
agent depending on the content of input text from the
participants (see Table 1 below for examples). Only short
back-channels are sent when there are several related key
words in a text (Type 1 output); Messages of
encouragement are given when the agent detects some
keywords related to the target concept (Type 2 output, Type
3 output, Type 4 output).

Rule
Base

Motion
Handler
Module

output

Output: type 0
Back-channels
Output: type 1
Positive Suggestion (Used
in Positive condition)
Output: type 2
Negative Suggestion(Used
in Negative condition)

Output: type 3
Normal Suggestion (Used
in Neutral condition)

Examples
"I think that a schema is some
kind of knowledge that is
used based on one’s own
experience."
"That's the way", "Keep
going! ", "Um-hum"
"Wow! You used a few very
good keywords. That's great!
It is better if you explain it
from a different perspective!"
"Well,
you
used
few
keywords. That is not enough.
It is not satisfactory unless
you explain it from a different
perspective."
"You used few important
keywords. Try to explain from
a different perspective."

Participants and conditions
Figure 4: Architecture of message production.

Textual input of all conversational exchanges produced by
paired participants is sent to the semantic analyzer of the
conversation agent. The semantic analyzer then scans the
text and detects keywords relevant to the concepts if they

In this study, 90 participants participated in the experiment.
The participants were all undergraduate students who were
taking a psychology course and participated in them as part
of the course work. They were randomly assigned to three
conditions, which varied with respect to how prompts of

1652

conditions (see Figure 5). The vertical axis in Figure 5
represents the average scores of the tests for the three
groups at the times of pre- and post- tests. A statistical
analysis was performed using a 2 x 3 mix factor ANOVA
with the two evaluation period (the pre-test vs. the post-test)
and the three conditions with different feedback (Positive vs.
Negative vs. Neutral) as independent factors.
5

positive condition

negative condition

4.5

Average score of test

suggestions were presented and how conversational agents
were used (see the sections below for details).
To find out how affective factors influence the task of
explanation, three types of avatars were created: one is the
positive agent with friendly facial expression which was
used for the "positive condition", and the negative agent
with unfriendly facial expression which was used for the
"negative condition", and finally the neutral agent with no
facial expression which was used for "neutral condition". In
the positive condition (n = 31), the participants were given
positive suggestions, which were synchronized with the
facial expressions of the positive agent. In the negative
condition (n = 28), the participants were given negative
suggestions, which were synchronized with the facial
expressions of the negative agent. In the nutural condition (n
= 31), the participants were given suggestions without
emotional expressions.
The messages were given through chat dialogue and the
virtual character moved its head gestures while the
participants chat on the computer (For examples of
suggestion for the conversational agent see Table 1). Since
there was odd number of participants in positive and neutral
condition, one group was composed by three.

4

nutral condition

3.5
3
2.5
2
1.5
1

pre-test

Dependant variables

post-test

Figure 5: Results of the quality of the performance of learning.

To evaluate the outcome of (1) quality of the performance
of learning, and (2) interaction process, two types of
measures were used.
First, for the learning performance, the results of the preand post- tests were compared to find out how the
explanation task with different conditions facilitated their
understanding or learning of the concepts. For the
comparison, their descriptions were scored in the following
way: 1 point for a wrong description or no description, 2
points for a nearly-correct description, 3 points for a fairlycorrect description, 4 points for an excellent description, and
5 points for an excellent description with concrete examples.
It was judged that the greater the difference in scores
between the two tests the higher the degree of the effect of
explanation.
Second, for the analysis of explanation process, all the
dialogs during the task were analyzed. The main focus of
the analysis was to investigate what kind of explanations
were used during their interaction. Each dialog sentences
that included explanations were coded by the following two
categories: (1) explanations that were made by using terms
and phrases presented by the system (see Figure 3 for an
example of the description), and (2) explanations that were
generated based on subjective inference. The former is
called "normative explanations". On the other hand, the
utterance in the latter is called "subjective explanations".

Results
Quality of performance
The results showed that the participants' understanding of
the concepts improved after the explanation task in all

There was significant interaction between the two factors
(F(2, 87) = 3.388, p < .05). First, an analysis of the simple
main effect was done on each level of the feedback factor.
In the Positive, Negative, and Neutral condition, the average
scores in post-test was higher than pre-test respectively
(F(1,87) = 254.397, p < .01; F(1,87) = 172.796, p < .01;
F(1,87) = 155.812, p < .01). Next, an analysis of the simple
main effect was done on each level of the period factor. In
the pre-test, there was no differences between conditions
(F(2,174) = 0.202, p = .82). Although in the post-test there
were differences between conditions (F(2,174) = 9.094, p
< .01). Further analysis on the post-test was conducted using
the Ryan's method. Results indicate that the average score of
Positive condition was higher than Negative condition and
the average score of Positive condition was higher than
Neutral condition respectively (p < .01; p < .01). There were
no differences between Negative condition and Neutral
condition (p = .51).
The overall result suggests that the collaborative activities
facilitated the participants' understanding or learning of the
concepts more when the positive suggestions were
presented to the participants.

Interaction process
Figure 6 indicates the relationships between the usage of
normative explanations and subjective explanations. The
vertical axis represents the average ratio of each
participant’s explanation type. The horizontal axis shows
each of the three conditions.
The analysis of ANOVA with the factor of explanation
type (normative explanations vs. subjective explanations)

1653

was conducted on each condition. The results show that
participants in the Neutral condition and Negative
conditions used more subjective explanations than
normative explanations, respectively (F(1, 27) = 7.326, p
< .05; F(1, 30) = 25.116, p < .01). On the other hand, there
were no statistical differences between the two conditions in
the Positive condition (F(1, 30) = 0.46, p = .50). These
results indicate that participants in the Negative and Neutral
conditions made explanations mostly based on subjective
explanations.
100%
Average ratio of explanation type

90%

80%
70%
60%
50%

normative

40%

subjective

30%
20%

10%
0%
Positive

Negative

Neutral

Figure 6: Results of the type of explanation activities.

Discussion
Affective expressions of the conversational agent
The results of the experiment suggested that the greater the
positive affective expressions from the conversational agent
the more it can facilitate explanation activities which leads
to a deeper understanding of concepts (i.e., Positive
condition > Negative condition, Positive condition > Neutral
condition). The results of the dialogue analysis somewhat
support these result. That is, participants in the Negative and
Neutral condition used more subjective inferences and
interpretations about the key concept instead of using
normative phrases, which might leaded to construction of
misunderstandings on the concepts. On the other hand,
participants in the Positive condition used normative
expressions that were on track. It is assumed that the
affective expressions generated by the agents facilitated the
participants’ motivation to keep their attention to the
computer system which provided important information.
These results provide more reliable findings than those
compared with experiments conducted by the authors’
previous work (Hayashi, 2012). In those experiments, the
influences of affective feedbacks were examined during
collaborative activities. Unfortunately, there was no neutral
condition and dialogue analyses were not further conducted.
The present study makes it clear that positive emotions
expressed by a pedagogical agent facilitated explanation
activities at the interaction level. This suggests that the
participants might have paid more attention during the
interaction process and worked harder when they received
positive comments than they received neutral and negative

comments. One of the interesting finding is that, the
learning performance of the participants in the Negative
condition and Neutral condition were the same in this
experiment. It is assumed that negative affective feedbacks
were not able to trigger such motivation and enhance
performance as much as the Positive condition. This may be
affected by the lack of attention to the agent. This point will
be further investigated elsewhere.

Awareness towards the conversational agent
Studies in social psychology have suggested that work
efficiency is improved when a person is being watched by
someone, or, that the presence of an audience facilitates the
performance of a task. This impact that an audience has on a
task-performing participant is called the "audience effect".
Another relevant concept on task efficiency, but from a
slightly different perspective, is what is called "social
facilitation theory". The theory claims that people tend to do
better on a task when they are doing it in the presence of
other people in a social situation; it implies that person
factors can make people more aware of social evaluation.
Zajonc (1965), who reviewed social facilitation studies
concluded that the presence of others have positive
motivational affects.
Holmes (2007) is one of the experimental studies which
investigated the effects of a tutoring agent. In this
experiment, an agent, which played the role of an assistant,
was brought in to help a participant who explained a
concept. In the experiment, three different environments
were set up for the 'explaining activity'. They were: (1) two
participants working with a text-based prompt, (2) two
participants working with a visual image of pedagogical
agent which produced a text-based prompt (3) one
participant working with a visual image of pedagogical
agent which produced a text-based prompt (in this setup,
participants did not have a human co-learner and directly
interacted with the agent). The result showed that the
participants in the last two conditions did better than the
first where only textual prompts were presented. It also
showed that the participants in the second condition did not
engage in the explanation activity as much as those in the
third. The first finding of Holmes (2007) is that the
participants in the last two conditions, who worked with the
agent, performed better may be attributed to the fact that
their task of explanation was being watched or monitored by
the agent. Also, the second finding that the effect of the
agent for the participants in a pair was not as high as for
those directly interacting with it alone may be because the
level of attention of the participants in the second condition
was not as great as that in the third condition; it may be that
the participants in the second were less conscious of the
presence of the monitoring agent than those in the third
group.
These results of the present study suggest that participants
would do better in the task of explanation if they are more
conscious of the presence of the agents or if they are given
an explicit direction to pay attention to the agent. The

1654

results of the present experiment provide new evidence that
the positive feed backs made by the agents can facilitate
such "audience effect".

Conclusion and future work
The present study investigated the effectiveness of the use
of a conversational agent in a collaborative activity, where
paired participants explained each other the meaning of
technical terms taught in a psychology class for a better
understanding. Conversational agents were used to
encourage and facilitate the students' interaction through
both verbal and visual input. The experimental results
suggested that the presence of a conversational agent with
positive expressions can trigger a deeper understanding of a
concept during an explanation.
Pedagogical agent can play several different roles for
collaborative learning activities and several studies have
looked into the effectiveness of the use of a pedagogical
agent with different roles. For example, Baylor & Kim
(2007) investigated the effectiveness of the use of a
pedagogical agent which plays the roles of an expert teacher,
a motivator, and a mentor (both an expert and motivator).
However, not much is known yet about what roles it can
play effectively. Another issue to be further investigated is
the effect of the personality of the agent upon these roles.
These and other related topics need to be further studied in
future.

Acknowledgments
I appreciate all students who participated in this experiment.
I also want to thank my student advisee Shin Takii
(Ritsumeikan University) and Rina Nakae (Ritsumeikan
University), Yuichi Mizuno (Ritsumeikan University) for
helping me to conduct the experiment.

7. Hayashi, Y., Miwa, K. (2009). Prior experience and
communication media in establishing common ground during
collaboration. In Proceedings of the 31th annual conference of
the cognitive science society, 528-531.
8. Hayashi, Y. (2012), On pedagogical effects of learner-support
agents in collaborative interaction. In S.A. Cerri and B. Clancey
(Eds.): Proceeding of the 11th International Conference on
Intelligent Tutoring Systems (ITS2011), Lecture Notes in
Computer Science, Springer-Verlag, Vol 7315, pp. 22-32.
9. Holmes, J. (2007). Designing agents to support learning by
explaining. Computers & Education, 48, 523-547.
10. Kim, Y., Baylor, A. L., & Shen, E. (2007). Pedagogical agents
as learning companions: The impact of agent emotion and
gender. Journal of Computer Assisted Learning, 23, 220-234.
11. King, A. (1994). Guiding knowledge construction in the
classroom: Effects of teaching children how to question and
how to explain. American Educational Research Journal. 30,
338-368.
12. Mayer, D, K., Turner, J,C. (2002). Discovering emotion in
classroom motivation research, Educational Psychologist. 37,
107-114.
13. Miyake, N. (1986). Constructive interaction and the interactive
process of understanding. Cognitive Science. 10, 151-177.
14. Okada, T., & Simon, H. (1997). Collaborative discovery in a
scientific domain. Cognitive Science. 21, 109-146.
15. Reeves, B., Nass, C. (1996). The Media Equation: How People
Treat Computers, Television, and New Media Like Real People
and Places. New York: Cambridge University Press
16. Salomon, G. (2001). Distributed cognition: Psychological and
educational considerations. New York: Cambridge University
Press
17. Shirouzu, H., Miyake, N., & Masukawa, H. (2002). Cognitively
active externalization for situated reflection. Cognitive Science.
26, 469-501.
18. Zajonc, R. B. (1965). Social facilitation. Science. 149, 271274.

References
1. Baylor, A. L., & Kim, Y. (2005). Simulating instructional roles
through pedagogical agents. International Journal of Artificial
Intelligence in Education, 15, 95-115.
2. Bower, H, G., Forgas, P, J. (2001). Mood and social memory.
In J. P. Forgas (Ed). Handbook of affect and social cognition,
NJ, LEA, 95-120
3. Chi, M. T. H., Bassok, M., Lewis, M. W., Reimann, P., &
Glaser, R. (1989). Self-explanations: How students study and
use examples in learning to solve problems. Cognitive Science,
13, 145-182.
4. Coleman, E. B.(1998). Using explanatory knowledge during
collaborative problem solving in science. The Journal of
Learning Sciences, 7, 387-427.
5. Graesser, A., McNamara, D. (2010). Self-regulated learning in
learning environments with pedagogical agents that interact in
natural language. Educational Psychologist, 45, 234-244.
6. Gulz, A., Haake, M. (2006). Design of animated pedagogical
agents – A look at their look, International journal of HumanComputer Studies, 64, 322-339.

1655

