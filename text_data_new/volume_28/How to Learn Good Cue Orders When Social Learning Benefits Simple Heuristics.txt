UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
How to Learn Good Cue Orders: When Social Learning Benefits Simple Heuristics

Permalink
https://escholarship.org/uc/item/7s51w12b

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)

Authors
Garcia-Marques, Rocio
Gigerenzer, Gerd
Takezawa, Masanori

Publication Date
2006-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

How to Learn Good Cue Orders:
When Social Learning Benefits Simple Heuristics
Rocio Garcia-Retamero (rretamer@mpib-berlin.mpg.de)
Center for Adaptive Behavior and Cognition, Max Plank Institute for Human Development, Lentzeallee 94,
14195 Berlin, Germany

Masanori Takezawa (take@mpib-berlin.mpg.de)
Center for Adaptive Behavior and Cognition, Max Plank Institute for Human Development, Lentzeallee 94,
14195 Berlin, Germany

Gerd Gigerenzer (gigerenzer@mpib-berlin.mpg.de)
Center for Adaptive Behavior and Cognition, Max Plank Institute for Human Development, Lentzeallee 94,
14195 Berlin, Germany
Abstract
Take The Best (TTB) is a simple one-reason decisionmaking strategy that searches through cues in the order
of cue validities. Interestingly, this heuristic performs
comparably to, or even better than, more complex
information-demanding strategies such as multiple
regression. The question of how a cue ordering is
learned, however, has been only recently addressed by
Dieckmann and Todd (2004). Surprisingly, these
authors showed that learning cue orders through
feedback––by updating cue validities––leads to a slow
convergence to the ecological cue validities. Various
other simple learning algorithms do not provide good
results either. In the present paper, we provide a
solution to this problem. Specifically, in a series of
computer simulations, we show that simple social rules
such as “imitate the successful” help to overcome the
limitations of individual learning reported by
Dieckmann and Todd (2004). Thus, the dilemma of
individual learning can be collectively solved. In line
with the spirit of bounded rationality, we found that
several simple social rules performed comparably to, or
better than computationally demanding social rules. We
relate our results to previous findings on bounded
rationality in the social context.

Fast and Frugal Heuristics
In our everyday lives, we make decisions frequently.
However, the information we need for that might not always
be available, and it might not be possible to consider all
alternatives as fully as we would wish because of our
limitations in time and cognitive processing power, and the
complexity of the environment. How do we make such
decisions? One recent approach, promoted by the Center for
Adaptive Behavior and Cognition (ABC; Gigerenzer, Todd,
& the ABC Research Group, 1999; Todd & Gigerenzer,
2000) suggests that in these situations, people use fast and
frugal heuristics, that is, simple but nevertheless fairly
accurate strategies that use a minimum of information.

These heuristics enable organisms to make smart choices
quickly under limitations of time and cognitive processing
by exploiting the way information is structured in the
environment (Martignon & Hoffrage, 2002). The ecological
rationality view of decision making as promoted by
Gigerenzer et al. (1999; see also Simon, 1990) thus brings
the two elements—mind and environment—together,
focusing on how minds with limited capacities are adapted
to their environments and how the environments in which
we make decisions shape our strategies, a concept known as
bounded rationality.
One fast and frugal heuristic is Take The Best (TTB;
Gigerenzer & Goldstein, 1996). This heuristic is designed
for forced-choice paired comparisons. That is, it can be used
to infer which of two alternatives, described on several
dichotomous cues, has a higher value on a quantitative
criterion, such as which of two cities has a higher
population based on cues such as whether they have a
university.
As a step-by-step algorithm, TTB is constructed from
building blocks of information gathering and processing to
generate a decision. More specifically, it has a search rule,
which prescribes the order in which to search for
information (TTB looks up cues sequentially in the order of
the cue validities⎯the probability that a cue will lead to the
correct decision given that it discriminates between the
alternatives; Martignon & Hoffrage, 2002); a stopping rule,
describing when the search is to be stopped (TTB stops after
the first discriminating cue); and a decision rule for how to
use the available information to make a decision (TTB
chooses the alternative favored by the first discriminating
cue and ignores the rest of the cues. TTB is thus called onereason decision making).
The efficiency of TTB consists in its surprising
performance relative to its extreme simplicity. For instance,
Gigerenzer and Goldstein (1996) tested the performance of
TTB with more savvy strategies such as multiple regression
using the German cities data set, which consists of the 83
German cities that had more than 100,000 inhabitants at the
time. These cities were described on nine cues, such as

1352

whether a city has a university or whether it is on an
intercity train line. Gigerenzer and Goldstein (1996) found
that the performance of TTB came close to or even
outperformed multiple regression in cross-validation. 1
Czerlinski, Gigerenzer, and Goldstein (1999) further
demonstrated the superiority of TTB across a wide range of
real-world environments.

each cue: a count of all discriminations made by a cue up to
a certain trial, and a count of all the correct decisions. The
validity of each cue was then computed by dividing the
number of current correct decisions by the number of
current discriminations.2 The tally algorithm simply counts
the number of correct decisions made by a cue up to a
certain trial minus the number of incorrect decisions
(=tally). The swap algorithm moves a cue up one position in
the cue order after a correct decision, and down if the
decision was wrong. Finally, the tally swap algorithm makes
swapping contingent on whether or not a cue has a higher
tally than a neighboring cue.
Dieckmann and Todd (2004) tested the offline accuracy
and frugality of the learned cue orderings at each trial, for
the total set of pair comparisons (i.e., 3,403 pairs of 83
cities). Their results showed that the performance of these
algorithms soon rose above that achieved by Minimalist,
which searches through cues in a random order (see Figure
1).

The Problem: Individual Learning of Cue
Orders Is Very Slow

75

Percentage of correct decisions

The accuracy of a strategy depends critically on the order in
which cues are searched in the environment. Note that in the
German cities data set, there are 9! (i.e., 362,880) possible
cue orders. Therefore, finding the best cue order is
computationally intractable (Martignon & Hoffrage, 2002).
In this data set, the best cue order achieves an accuracy of
75.8%. TTB, which searches through cues in the order of
the cue validities calculated using the 83 cities in the
German cities data set, achieves an accuracy of 74.2%.
Surprisingly, only 1.8% (i.e., 7,421) of the cue orders
achieve a higher performance than TTB. The mean of the
distribution of the accuracy of all possible cue orderings in
the German cities data set (i.e., 70%) corresponds to the
expected performance of the Minimalist, a one-reason
decision-making heuristic that searches through cues in a
random order (Gigerenzer & Goldstein, 1996). This
indicates that TTB achieves an impressive performance.
How can an individual learn a good cue order if cue
validities are not available beforehand? One could assume
that people could use TTB and update a cue order by using
only the cues they searched. That is, one could assume that
a cue ordering by validity can be acquired by learningwhile-doing. This question has been only recently addressed
by Dieckmann and Todd (2004) in a series of computer
simulations. These authors evaluated the performance of a
variety of simple learning algorithms for ordering cues in
the forced-choice paired comparison task for which TTB
was designed. These algorithms update cue orderings on a
trial-by-trial basis (Bentley & McGeoch, 1985).
The accuracy of the cue orderings resulting from the
application of simple learning algorithms was tested using
the German cities data set. Specifically, each algorithm
started with a random cue ordering and searched one cue at
a time until it found a cue that discriminated between the
alternatives, which was then used to make the decision (i.e.,
the algorithm chose the alternative favored by the first
discriminating cue). After each decision, feedback was
provided and the cue ordering was updated. Decisions were
made repeatedly through one hundred pair comparisons.
Which simple learning algorithms did Dieckmann and
Todd (2004) consider in their simulations? They evaluated
the validity, tally, swap, and tally swap learning algorithms.
The validity algorithm keeps two pieces of information for

74
73
72
71
70
69

1

10

19

28

37

46

55

64

73

82

91

100

Trials

Tally

Validity

Swap

Tally Swap

Figure 1. Mean performance of the four learning algorithms
in Dieckmann and Todd’s (2004) simulations. The bottom
straight line shows the performance of Minimalist (random
cue orders). The top straight line shows the performance of
TTB (ecological cue validities).
Surprisingly, none of these learning algorithms reached
the benchmark performance of TTB based on the ecological
cue validities (upper line in Figure 1). Even after obtaining
feedback for 100 pair comparisons, they all fell well behind.
These simulations showed that updating cue validities
through feedback leads to a slow convergence to the
ecological cue validities, at least after the first 100 pair

1

2

Cross-validation refers to the accuracy of a decision strategy that
is fitted to one half of a data set (training set) and tested on the
other half (test set).
1353

Note that the difference between TTB and a one-reason decisionmaking strategy that uses the validity learning algorithm is that the
first computes cue validities beforehand using all objects and cues
in the environment, whereas the second acquires knowledge about
cue validity on a trial-by-trial basis using only information about
the first discriminating cue that is looked up. Therefore, these
strategies do not necessarily have to arrive at the same cue
ordering.

comparisons. Various other learning algorithms reported by
Dieckmann and Todd (2004) did not provide better results
either.

Simulation Study
We conducted a series of computer simulations in which we
evaluated the success of several social rules (see Hastie &
Kameda, 2005, for a review) when they were implemented
in two of the simple learning algorithms for ordering cues
considered by Dieckmann and Todd (2004). The two
algorithms chosen were validity and tally swap. The former
defines cue orderings of TTB and performed best in Figure
1. The latter was reported in an experiment by Dieckmann
and Todd (2006) to capture the participants’ behavior best.
We tested the simple social rules using the German cities
data set (Gigerenzer & Goldstein, 1996). As we mentioned
above, the German cities data set consisted of the 83
German cities that had more than 100,000 inhabitants. In
our simulation study, the task was to infer which of two
cities has a higher population. For this inference, several
cues could be considered (e.g., whether the city is the
national capital).

Can Social Learning Resolve the Limitations of
Individual Learning?
The simple learning algorithms we mentioned above suffer
from a serious and frequent problem in the literature of
reinforcement learning (e.g., Sutton & Barto, 1998; see also
Einhorn, 1980): the trade-off between exploration and
exploitation of the information in the environment.
Specifically, to be fast and frugal, the learning algorithms
considered by Dieckmann and Todd (2004) do not search
for all the available cues in the environment but instead stop
search after the first discriminating cue. Exploitation of the
environment, therefore, is an important constraint that
significantly reduces performance (i.e., if all the cues were
looked up, these algorithms would soon achieve the
performance of the ecological cue validities). Yet, an
exhaustive exploration of all the cues in a real world
environment not only decreases speed and frugality but is
often impossible. Thus, the trade-off between exploration
and exploitation seems to be an essential problem for these
learning algorithms for ordering cues as long as they
simultaneously pursue speed, frugality, and accuracy in
performance.
In contrast to laboratory tasks, in real-world environments
we often exchange reports of our experiences with other
individuals. Consider once again the question of which of
two cities has a higher population; rather than collecting
information individually, we might discuss with other
people what the relevant cues are. Thus, people could learn
to order cues not only individually but also socially by
exchanging information. Our argument is that the exchange
of information can help boundedly rational individuals to
solve the problem of learning good cue orders without
impairing speed and frugality.
From a theoretical point of view, collective information
sharing can be modeled as rules that define how people
update cue orders on the basis of those of others. These
rules might differ both in simplicity and accuracy. For
instance, a complex social rule (the average rule) implies
exchanging knowledge about subjective cue validities with
other individuals, then computing the mean value across all
individuals for each cue to arrive at a new cue order. In
contrast, the majority rule is an example of a less demanding
way to come up with new cue orders: Individuals simply
vote for the cue that they consider best, second best, and so
on.
Along these lines, Hastie and Kameda (2005) recently
compared the performance accuracy of several social rules
in group decision making. Interestingly, they found that
very simple social rules performed comparably to much
more demanding rules. Bearing this in mind, in the
following we investigate, by means of a simulation study,
whether social rules can help to overcome the slow progress
in individual learning of a good cue order.

Basic Setting of the Simulations
The key feature of our simulations was that a group of
individuals exchanged information about the cue orders that
they learned independently. Specifically, they started from
random cue orderings and went through a subset of pair
comparisons in which they derived and updated such
random cue orders with feedback. In the basic simulation,
groups of ten individuals went through a set of five pair
comparisons. They all used the same learning algorithm but
received a different set of five pair comparisons.
Consequently, after this individual learning experience, each
group member came up with a different cue order.
All group members then exchanged information about
their cue orders and used a social rule to arrive at a single
social cue order. In a further subset of five trials, group
members started looking cues up in the social cue order
instead of the random one. This social cue order was
subsequently updated through individual learning. The
process of social exchange and individual learning took
place repeatedly every five trials and for up to one hundred
pair comparisons.

Social Rules
In the simulations, we investigated the following five social
rules:
1. The average rule: Each group member estimates the
validity (or tally) of each cue, and the group computes the
mean value across all members for that cue.
2. The Borda rule: Each group member ranks all cues
according to their validities (or tallies). The value assigned
to each cue is the sum of the members’ rank order for that
cue.
3. The majority rule: Each member assigns one vote to
the cue with the highest validity (or tally), and the cue that
receives the most votes is selected. This process is repeated
for all the cues.

1354

less demanding, performed comparably, placing behind the
average rule. The Borda rule fell behind the rest of the
social rules. It did not achieve the accuracy that the
individual learning algorithm did, but its performance was
better than that of random order. Interestingly, the “imitate
the most successful” rule beat the rest of the social rules we
considered and even TTB.
75
Percentage of correct decisions

4. The Condorcet rule: All comparisons between two cues
are kept. The cue that wins all comparisons in each cue
order position is the Condorcet winner.
5. The imitate the most successful rule (also known as the
best member rule): The cue order of the group member who
achieved the highest accuracy in the last five trials is used
by all members of the group. Note that imitate the most
successful is the simplest rule. In contrast to the rest of the
social rules, it does not involve the aggregation of social
information. Individuals using such a rule just have to find
out who performed best in the last five trials and imitate that
group member.
During a set of five trials, all the group members kept
count of the number of correct decisions and the total
number of discriminations made by each cue. These
accounts were subsequently updated when individuals
arrived at a social cue order. Specifically, those individuals
who used the average rule replaced the accounts of each cue
by the corresponding values averaged across all group
members. Individuals using the rest of the social rules
estimated both accounts drawing random values for a
uniform distribution with the following constraints: (1) the
range of the distribution was determined by the maximum
and minimum values stored in the previous trial, and (2) the
final values had to conform to the social cue order.

73
72
71
70
69
1

10

19 28 37 46 55
Average
Trials
Majority
Imitate the successful

64 73 82
Borda
Condorcet

91 100

Figure 3. Mean performance of the five social rules when
implemented in the tally swap learning algorithm.

Results of the Basic Simulation
Figure 2 shows the performance of the five social rules
averaged across 1000 runs when they were implemented in
the validity learning algorithm. For comparison, we further
added the performance of individual learning according to
validity (solid jagged line), TTB (upper straight line), and
Minimalist (lower straight line).

74

How frugal were these rules? Table 1 shows the mean
number of cues looked up in 100 trials for all the social
rules. Note that all social rules, except “average,” were
comparable to and more frugal than TTB, which looked up
4.23 cues on average. However, all these rules were less
frugal than Minimalist, which looked up 3.34 cues on
average.

Percentage of correct decisions

75

Table 1. Averaged number of cues looked up in 100 trials
for the five social rules.

74
73
72

Individual
Average
Borda
Majority
Condorcet
Imitate the
successful

71
70
69
1

10

19 28 37 46 55
Average
Trials
Majority
Imitate the successful

64 73 82
Borda
Condorcet

91 100

Figure 2. Mean performance of the five social rules when
implemented in the validity learning algorithm.
Results showed that the complex average rule matched
the performance of TTB after 100 pair comparisons. The
majority and Condorcet rules, which are computationally

Validity
algorithm
3.2
3.94
3.35
3.50
3.35
3.53

Tally swap
algorithm
3.13
2.96
3.19
2.99
3.03
3.26

What is the consequence when individual learning by
validity is replaced by the tally swap algorithm? Some of
the previous findings were replicated (see Figure 3). For
comparison, we also added the performance of individual
learning according to tally swap (solid jagged line), TTB
(upper straight line), and Minimalist (lower straight line).
The average, majority, and Condorcet rules outperformed
individual learning using tally swap. However, they still
1355

lagged behind the performance of the ecological cue validity
order. Again, the Borda rule performed worse than
individual learning, but still better than the random order.
The accuracy of the “imitate the most successful” rule when
implemented in the tally swap learning algorithm matched
that of TTB. Yet one characteristic distinguished it from
validity learning: After each social exchange, there was a
decline in performance, and the average performance was
not as high as when individuals updated cue orders by
validity. Furthermore, all these social rules were comparable
in frugality (see Table 1). In fact, they all were more frugal
than the TTB and Minimalist.
Why did the cue orders resulting from the application of
some of these social rules perform so well? Why did some
of them perform better than others? To answer these
questions, we analyzed the accuracy of the social cue orders
obtained by each social rule for both the validity and tally
swap learning algorithms. We gathered 20,000 cue orders
that each social rule produced until the 100th trial. Then, we
classified them depending on whether they achieved a better
performance than (1) the ecological cue validities order (i.e.,
TTB), (2) individual learning according to the
corresponding learning algorithm, (3) the random order (i.e.,
Minimalist), or (4) whether their performance was even
worse than the random order (see Figure 4).

validities order. Finally, most of the cue orders produced by
the Borda rule achieved a performance that was worse than
individual learning.

Sensitivity Analyses

100%

In an extended set of simulations, we modified several
parameters of the basic simulation. More specifically, we
focused on the number of individuals who exchange
information to arrive at a social cue order (i.e., the group
size), and the number of trials in which individuals learned
independently before exchanging information (i.e., the
frequency of social information exchange). In these
extended simulations, we focused on three of the social
rules (i.e., average, majority, and imitate the most
successful). We examined the performance and frugality of
these rules for four different group sizes (2, 10, 25, and 100
individuals), and three different frequencies of social
information exchange (5, 25, and 50 trials).
The general conclusion from these extended simulations
is that the more individuals are included in the group, the
higher the group’s accuracy, regardless of the social rule
and the individual learning algorithm they used. The
increase in accuracy from 5 to 25 individuals was larger
than from 25 to 100. Furthermore, the lower the frequency
of exchanging social information, the lower the performance
was in the long run. Again, the combination of “imitate the
most successful” and the validity learning algorithm proved
to be an effective way to resolve the problem of slow
individual learning. For instance, when 10 individuals
exchanged information on just one occasion (after 50 trials),
this was sufficient to achieve the accuracy of the ecological
cue validity order.
To achieve the performance of TTB, it is not necessary to
exchange information with a large group of individuals.
Frequent exchange, or the aggregation of social information
(as would be required, for instance, in the average rule), is
not necessary either. In summary, the superiority of the
“imitate the most successful” rule, observed in the basic
simulation, remained stable with an increasing number of
individuals in the group, even when the number of social
exchange opportunities was reduced. Briefly, the results of
the basic simulation were strengthened.

Percentage of trials

75%

50%

25%

> Individual Learning
< Random

TS. Imitation

TS.
Condorcet

TS. Borda

TS. Majority

TS. Average

> Ecological Validity
> Random

Val. Imitation

Val.
Condorcet

Val. Borda

Val. Majority

Val. Average

0%

Figure 4. Performance of the five social rules relative to
three criterions: the ecological cue validities order,
individual learning, and the random order.

Conclusions

Interestingly, results showed that the cue orders produced
by the “imitate the most successful” rule were more accurate
than the ecological cue validities order in more than 50% of
occasions. This result was found for both validity and tally
swap. Note that, as we mentioned above, in the German
cities data set, there are 9! (i.e., 362,880) possible cue
orders, and only 1.8% (i.e., 7,421) of them perform better
than the ecological cue validity order does. Furthermore, the
rest of the social rules apart from Borda generally came up
with cue orders that achieved a better performance than
individual learning but worse than the ecological cue

The results of our simulations support the hypothesis that
very simple social rules are able to improve the performance
of simple algorithms for ordering cues. Specifically, all
social rules we considered (except the Borda rule)
outperformed individual learning. Even more interestingly,
the “imitate the most successful” rule beat more
computationally demanding social rules, such as the average
or majority. Furthermore, it not only solved the problem of
slow individual learning, but also performed even better
than TTB using the ecological cue validity order did. In
everyday life, aggregation of information can be difficult,
requiring demanding computations; cognitively simple
strategies are a viable alternative.

1356

As mentioned above, finding the best cue order becomes
computationally intractable as the number of available cues
in the environment increases. One solution to this problem
is to rely on simple learning algorithms for ordering cues.
However, as Dieckmann and Todd (2004) showed, updating
a cue order in a trial-by-trial basis leads to a slow
convergence to the ecological cue validities. We showed
that the limitation of the individual learning algorithms
considered by Dieckmann and Todd (2004) could be
resolved at a group level. That is, higher performance could
be achieved by very simple social rules. These results
suggest a promising avenue for future research on bounded
social rationality (see Gigerenzer et al., 1999).
For the sake of simplicity, in the current computer
simulations we mimicked a situation of group discussion.
However, this might be troublesome in real-world group
discussion because communication and coordination require
a huge effort, especially when aggregating complex
information such as cue orders. The discussions of a group
of people who report cue orders sequentially and aggregate
them with a certain protocol, such as majority rule, could
also be cumbersome. Rather, in the real world, a single
person could easily come up with a single cue order (i.e., a
social cue order) when listening to other people’s opinions.
In such a situation, we believe that the “imitate the most
successful” rule would also work well.
Our basic results are in line with findings by Hastie and
Kameda (2005). By means of computer simulations and
using forced-choice tasks, they investigated how the
members of a group who made decisions individually
arrived at a collective decision. In contrast, we focused on
how individuals exchanging information about individually
learned cue orders arrive at a social cue order. The results of
Hastie and Kameda (2005) are focused on the social
learning of “decisions.” In contrast, our results are focused
on the social learning of the “information” individuals use
to make decisions. Despite theses differences, both studies
found that cognitively simple social rules perform
comparably to cognitively more demanding rules, such as
the average rule. However, the impressive performance of
the “imitate the most successful” rule is unique in our study.
The performance of this rule in Hastie and Kameda’s (2005)
study was worse than those of the majority and average
rules in most cases. In future research, we will investigate

the reasons for this seemingly contradictory finding, and
also study the generalizability of our results to other
learning situations.

References
Bentley, J. L., & McGeoch, C. C. (1985). Amortized
analyses of self-organizing sequential search heuristics.
Communications of the ACM, 28, 404−411.
Czerlinski, J., Gigerenzer, G., & Goldstein, D. G. (1999).
How good are simple heuristics? In G. Gigerenzer, P. M.
Todd & the ABC Research Group. Simple heuristics that
make us smart. New York: Oxford University Press.
Dieckmann, A., & Todd, P. M. (2004). Simple ways to
construct search orders. In K. Forbus, D. Gentner & R.
Regier (Eds.), Proceedings of the 26th Annual Conference
of the Cognitive Science Society (pp. 309–314). Mahwah,
NJ: Erlbaum.
Dieckmann, A., & Todd, P. (2006). Simple rules for
ordering cues in one-reason decision making. Manuscript
submitted for publication.
Einhorn, H. J. (1980). Learning from experience and
suboptimal rules in decision making. In T. S. Wallsten
(Ed.), Cognitive processes in choice and decision
behavior. Hillsdale, NJ: Erlbaum.
Gigerenzer, G., & Goldstein, D. G. (1996). Reasoning the
fast and frugal way: Models of bounded rationality.
Psychological Review, 103, 650–669.
Gigerenzer, G., Todd, P. M., & the ABC Research Group.
(1999). Simple heuristics that make us smart. New York:
Oxford University Press.
Hastie, R., & Kameda, T. (2005). The robust beauty of
majority rules in group decisions. Psychological Review,
112, 494−508.
Martignon, L., & Hoffrage, U. (2002). Fast, frugal, and fit:
Simple heuristics for paired comparison. Theory &
Decision, 52, 29–71.
Simon, H. A. (1990). Invariants of human behavior. Annual
Review of Psychology, 41, 1−19.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement
learning: An introduction. Cambridge: MIT Press.
Todd, P. M., & Gigerenzer, G. (2000). Précis of simple
heuristics that make us smart. Behavioral & Brain
Sciences, 23, 727–780.

1357

