Table 1: Ten basic situations in the microworld.
situation
B OUT
J OUT
SOCCER
HIDE
B COMP
J COMP

B DOG
J DOG
B WIN
J WIN

Table 2: Rewrite rules of the microlanguage. The probabilities of possible productions are indicated if they are unequal.
S = sentence; NP = noun phrase; VP = verb phrase; APP = adverbial/prepositional phrase. Variable n denotes verb number
(singular or plural).

meaning
Bob is outside
Jilly is outside
Bob and Jilly play soccer
Bob and Jilly play hide-and-seek
Bob plays a computer game
Jilly plays a computer game
Bob plays with the dog
Jilly plays with the dog
Bob wins
Jilly wins

Head
S
Splay

→
→

Swin

→

NPsing

→

NPplu

→

VPplay,n

→

VPwin,intrans
VPwin,trans

→
→

APP

→

Place (.3) | at Games (.3) |
Place at Games (.2) |
at Games Place (.2)

Vplay,sing
Vplay,plu
Vwin,intrans
Vwin,trans

→
→
→
→

plays
play
wins | loses
beats | loses to

Place

→

inside | outside

Activities

→

Activity

→

game (.15) | Activity (.45) |
Activityi or Activity j (i 6= j) (.4)
Game (.75) | with dog (.25)

Games

→

Game

→

Microworld and Microlanguage
Since we are dealing with semantic (as opposed to syntactic) systematicity, arising from situational representations, it
is necessary to define a world in which situations occur. Obviously, the real world (or even a substantial subset thereof)
is far too complex to implement in a computational model.
For this reason, we make use of the microworld developed by
Frank, Koppen, Noordman, and Vonk (2003) for their Distributed Situation Space (DSS) model of knowledge-based
inference in story comprehension. The microworld has only
two characters, called Bob and Jilly, who can perform different actions and be in several places and states. Anything that
happens to them can be expressed as a boolean combination
of a small number of so-called ‘basic situations’. Only ten of
these, listed in Table 1, are relevant here.
Of course, not just anything is possible in the microworld.
For instance, soccer and hide-and-seek are always played by
Bob and Jilly together. This is why there are no basic situations in which only Bob or only Jilly plays one of these
games. Also, someone can win only when SOCCER or HIDE
is the case, or when both Bob and Jilly play a computer game.
Furthermore, soccer can only be played when Bob and Jilly
are outside, while computer games are only played inside.
Naturally, Bob and Jilly cannot win simultaneously.
Any situation in the microworld can be described by a
sentence from a microlanguage. Sentence comprehension is
taken to be the reconstruction of the situation from the sentence that describes it. In the Frank (2005) model, there were
only 15 different words, which could be combined to form
328 different sentences. Moreover, word order was not relevant to these sentences. Here, a more complex microlanguage is used: It has 20 words that can form 3558 different
sentences, some of which contain a transitive verb that makes
word order relevant to the sentence’s meaning.
The language’s lexicon contains two proper names (bob,
jilly), one pronoun (someone), five nouns (dog, soccer, hideand-seek, computer-game, game), five verbs (wins, loses,
beats, plays, play), two adverbs (inside, outside), three prepositions (with, to, at), and two connectives (and, or). These
words form sentences according to the grammar in Table 2.
A few examples of sentences, and the way the corresponding
microworld situations are constructed from basic situations,
are shown in Table 3.

Production
Splay | Swin
NPsing VPplay,sing (.7) |
NPplu VPplay,plu (.3)
NPsing VPwin,intrans | NPsing VPwin,trans
someone (.2) | bob (.3) | jilly (.3) |
bob or jilly (.1) | jilly or bob (.1)
bob and jilly | jilly and bob
Vplay,n (.36) | Vplay,n Place (.24) |
Vplay,n Activities (.24) |
Vplay,n Activities Place (.16)
Vwin,intrans (.3) | Vwin,intrans APP (.7)
Vwin,trans NPsing (.3) |
Vwin,trans NPsing APP(.7)

game (.15) | Game (.45) |
Gamei or Game j (i 6= j) (.4)
soccer | hide-and-seek | computer-game

Note that the microlanguage (as any natural language) can
describe situations that are impossible to occur, such as Jilly
losing to herself or playing soccer inside. To prevent the
occurrence of a very large number of such sentences, plural noun phrases and the phrase with dog are not allowed in
sentences describing winning or losing. As a result, there are
no sentences stating that both Bob and Jilly win or lose, nor
any sentence stating that someone wins or loses at playing
with the dog. Both these situations are impossible in the microworld.

The Model
Representing microworld situations
According to Hadley (2004, p. 150), for a sentencecomprehension model to display semantic systematicity, it is
necessary that ‘the resultant meaning representation for the
entire sentence possesses properties which would enable us to
justly claim that the entire representation could, in principle,
227

be estimated by:

Table 3: Examples of microlanguage sentences and the construction of the corresponding situations from the basic situations in Table 1.
Sentence
jilly plays

Situation

bob plays game
bob and jilly play with dog
someone wins
bob or jilly wins at
computer-game
bob loses
jilly loses to jilly inside

SOCCER

SOCCER ∨ HIDE
∨ J DOG

τ(p|X) =

τ(p ∧ X) ∑i µi (p)xi
=
.
τ(X)
∑i xi

(3)

These τ-values are called belief values because they indicate
the extent to which the DSS model may ‘believe’ particular
(basic) situations to be the case given a situation vector. As
shown empirically by Frank et al. (2003), belief values are
accurate estimates of (un)conditional probabilities in the microworld. This proves that relations among microworld situations are indeed (implicitly) encoded in the organization of
situation space.

∨ J COMP

∨ HIDE ∨ B COMP
B DOG ∧ J DOG
B WIN ∨ J WIN
(J WIN ∧ J COMP )
∨ (B WIN ∧ B COMP )
J WIN
B WIN ∧ J WIN ∧¬J OUT

The Network
A new approach to dynamical computation by neural networks was recently developed independently by Maass,
Natschläger, and Markram (2002) and by Jaeger (2003).
Their so-called Liquid State Machines (LSM; Maass et al.)
and Echo State Networks (ESN; Jaeger)2 are both based on
the insight that training all connections of a recurrent network is not needed. Instead, the weights in the recurrent part
of the network may remain fixed, greatly increasing training efficiency. These recurrent connections contribute to the
network’s computations by forming a ‘dynamical reservoir’
(DR) that stores the input sequence in an unstructured manner
(in much the same way that the pattern of waves in a bucket
of water contains information about what has recently fallen
in). A separate ‘readout’ network is trained to convert the
activation patterns in the DR into target outputs.
The network used here (drawn schematically in Figure 1) is
in fact an extension of the ESN in that the readout network is a
two-layer feedforward network, that is, it has a hidden layer.
Frank (in press) reports that this additional layer improves
generalization in a sentence-processing task. The complete
network consists of four layers:

constrain the set of situations which could render the sentence
true.’ The representations used by the DSS model (which are
also used here) have exactly these properties. Any situation
that can occur in the microworld is represented distributively
by a vector in a high-dimensional ‘situation space’. As formalized below, relations among these ‘situation vectors’ correspond to probabilistic relations among the represented microworld situations.
Situation vectors are developed by training a SelfOrganizing Map (SOM; Kohonen, 1995) on descriptions of
situations occurring in the microworld (see Frank et al., 2003,
for details). These descriptions take the form of binary vectors containing a 1 for each basic situation that occurs at a certain moment in time, and a 0 for each basic situation that does
not. As a result of training, a membership value µi (p) ∈ [0, 1]
is associated to each SOM-cell i and basic situation p. This
value indicates the extent to which cell i forms part of the
representation of p. The SOM has 150 cells, so the representation of p can also be viewed as a 150-element situation
vector of membership values µ(p) = (µ1 (p), . . . , µ150 (p)).
Representations of negations, conjunctions, and disjunctions are constructed as is common in fuzzy logic:
µi (¬p) = 1 − µi (p)
µi (p ∧ q) = µi (p)µi (q)
µi (p ∨ q) = µi (p) + µi (q) − µi (p)µi (q),

• The input layer, with one unit for each of the 20 words in
the microlanguage.
• The dynamical reservoir, serving as a short-term memory
for retaining the input sequence. The number of DR-units
was varied from 80 to 250.

(1)

where p and q can themselves be combinations of (basic) situations.
Let vector (x1 , . . . , x150 ) represent some microworld situation X.1 The a priori probability that this situation occurs can
be estimated by
1
xi .
(2)
τ(X) =
150 ∑
i

• The hidden layer, varied in size from 10 to 40 units.
• The output layer, with one unit for each of 150 dimensions
of situation space.
Words enter the network one at a time by activating only
the corresponding unit of the input layer. This activation is
sent to the DR, which, like the recurrent layer of a SRN, also
receives its own activation that resulted from processing the
previous word. The main difference with a SRN is that the

The content of situation X can be extracted by comparing its representation (x1 , . . . , x150 ) to several known situation
vectors µ(p). From Equations 1 and 2 it follows that the conditional probability that some p is the case in situation X, can

2 Although

LSMs and ESNs are conceptually similar, there are
some differences in their technical descriptions and in the way they
are commonly applied. The network we use here is most like an
ESN, so we shall refer to it as such.

1 This can be any vector in situation space, that is, it does not
need to have been constructed using Equation 1.

228

The network is trained on situations in which Bob loses or
Jilly wins. However, these situations are only described using
the intransitive verbs wins and loses, not the transitive beats
and loses to.
The microworld situation referred to by each training sentence was the target output during training on that sentence. After each word of each training sentence, connection weights Whid and Wout were updated using the standard
backpropagation algorithm.
After training, the network was tested on three sets of test
sentences:

150 output units
Wout
hidden units

readout
network

Whid
Dynamical Reservoir

Wdr

Win

• The ‘win’-set, consisting of all 88 sentences that describe a
possible situation in the microworld, do not specify a place,
and have either bob as object of beats or jilly as object of
loses to.

20 input units

Figure 1: Extended Echo State Network. Solid arrows indicate connections with trainable weight, dashed arrows are
connections with fixed weight. Connection weight matrices
are denoted W.

• The ‘game/place’-set, consisting of all 14 sentences that
describe a possible situation in the microworld, and end
in either play(s) with dog inside or play(s) hide-and-seek
outside.

ESN’s input and DR-weight matrices Win and Wdr remain
fixed at their initial values.3 If the two weight matrices of
the readout network, Whid and Wout , are trained correctly, the
network’s output activation vector represents the microworld
situation referred to by the input sentence.
Although Wdr , the matrix of DR connection weights, has
random values, not just any matrix will do. First, it is important that only a small fraction of possible DR-connections
is present. In the network used here, 85% of DR-connection
weights are set to zero. Second, the optimal overall scaling
of weights depends on the network’s task. This scaling is
expressed by the spectral radius of Wdr ,4 which was varied
from .4 to .95. The larger this value, the longer information
remains in the DR.

• The ‘random’-set, consisting of 100 random sentences.
The sentences in the ‘random’-set may or may not
have been training sentences. In contrast, the ‘win’ and
‘game/place’-sets contain only novel sentences. Moreover,
the situations described by the ‘game/place’-sentences were
not presented to the network during training. To successfully
process these sentences, the network must have learned how
novel combinations of known words refer to novel combinations of known situations. That is, it must display semantic
systematicity.

Rating performance
Comprehension is not an all or none type of phenomenon.
Millis, King, and Kim (2000), for instance, showed experimentally that the extent to which readers develop a situational
representation of a text varies between subjects and experimental tasks. The model, too, may only construct the correct
situation vector to a degree. To ascertain the network’s performance, the comprehension measure defined by Frank (2005)
is used. It is based on the belief values from Equations 2 and
3.
Suppose the network processes a sentence that states p, and
the resulting output vector represents some situation X. The
associated comprehension score is then defined as

Training and Testing
The network was trained on a set of 10 000 sentence tokens,
randomly generated by the production rules of Table 2. The
following sentences were excluded from the training set:
• Those with bob, bob or jilly, or jilly or bob as object of
beats.
• Those with jilly, bob or jilly, or jilly or bob as object of
loses to.
• Those containing both dog and inside.

τ(p|X) − τ(p)
.
τ(p|p) − τ(p)

• Those containing both hide-and-seek and outside.

If the probability of p in situation X is larger than its a priori probability (i.e., if τ(p|X) > τ(p)), processing the sentence has increased belief in p. This means that the sentence
was understood to some extent, as indicated by a positive
comprehension score. A perfect network will result in X = p,
so τ(p|X) = τ(p|p), corresponding to a comprehension score
of 1. In contrast, if processing the sentence decreases belief

This means that the network does not learn to construct any
situation in which Bob and/or Jilly plays with the dog inside,
nor any situation in which they play hide-and-seek outside.
3 Furthermore, in our network, DR-units are linear and have no
bias weights, while all units of a SRN usually have sigmoidal activation functions and bias weights.
4 The spectral radius of a matrix is its largest absolute eigenvalue.

229

in p (i.e., if τ(p|X) < τ(p)), the sentence was misunderstood
and the network made an error. The resulting comprehension
score is negative.
Most sentences describe more than one basic situation. For
instance, jilly wins at hide-and-seek outside describes a game
(Jilly plays hide-and-seek), a place (Jilly is outside) and an
outcome (Jilly wins). To investigate whether the network understood all of these basic situations, comprehension scores
are computed not only for the complete situation HIDE ∧
J OUT ∧ J WIN, but also separately for the three basic situations. In general, each sentence may give rise to up to four
separate comprehension scores: for the game, the place, the
outcome, and the conjunction of the three.

bob or bob beats jilly, yet it could correctly construct representations of these situations. Furthermore, it was never
trained to represent situations in which Bob and/or Jilly play
with the dog inside, nor situations in which hide-and-seek is
played outside. Nevertheless, it did comprehend sentences
referring to such situations.
These results are quite robust in the sense that they did not
crucially depend on a delicate setting of parameters, nor required an extensive and complex training procedure. Ten networks were trained, starting with different weight settings,
and each of these managed to comprehend the test sentences.
Also, we demonstrated that many novel sentences were comprehended correctly, again indicating that the network’s abilities are not just accidental.
As stated in the Introduction, two more criteria are often applied to evaluate systematicity in connectionist models:
scalability and usability in inference. It is only fair to admit that the size and complexity of the simulations need to
be expanded considerably in order to be psychologically realistic. Although the microlanguage was 10 times the size of
Frank’s (2005b), other models of language processing have
used more extensive languages. However, it should be kept
in mind that our focus on microworld situations requires the
time-consuming and effortful construction of a microworld
and corresponding microlanguage, making large-scale simulations laborious to set up. As for the criterion of usability
in inference, be reminded that the situational representations
were developed for the DSS model of inferencing in story
comprehension (Frank et al., 2003). Correct predictions of
experimental data by the DSS model indicate that these representations not only allow for simulations of inferencing, but
do so in a realistic manner.

Results
To investigate the robustness of the training process and the
model’s results, three parameters were varied: the number of
DR-units (80 – 250), the number of hidden units (10 – 40),
and the spectral radius of Wdr (.4 – .95).
Apart from the smallest networks, all combinations of parameter settings resulted in positive comprehension scores for
all test sets (averaged over test sentences). In general, the
larger the network, the better its performance. For the largest
network, the optimal spectral radius was around .6.
Applying this optimal parameter setting, ten networks were
trained, differing only in their initial random weights. The resulting comprehension scores, averaged over the ten networks
and over test sentences, are listed in Table 4. Clearly, all
scores are significantly positive, indicating that the network
understood each part of the sentences (as well as complete
sentences) of each test set. The table also shows the percentage of errors. Most of these are very low, meaning that only
a few sentences were misunderstood.

Acknowledgments
We would like to thank Leo Noordman and Michael Klein for
their useful remarks. The research presented here was supported by grant 451-04-043 of the Netherlands Organization
for Scientific Research (NWO).

Conclusions
The sentence comprehension model clearly behaves systematically: It was not trained on sentences stating that jilly beats

References
Table 4: Average comprehension scores with 95% confidence
intervals, and percentage of errors for all three test sets.
set
win

part
game
outcome
complete

comprehension
.80 ± .02
.30 ± .01
.28 ± .01

% errors
0.0
0.0
0.0

game/place

game
place
complete

.29 ± .03
.76 ± .03
.27 ± .02

7.1
0.0
0.0

random

game
outcome
place
complete

.77 ± .02
.49 ± .03
.70 ± .04
.60 ± .02

2.9
0.4
7.4
2.2

Aizawa, K. (2003). The systematicity arguments. Dordrecht,
The Netherlands: Kluwer Academic Publishers.
Bodén, M., & Niklasson, L. (2000). Semantic systematicity
and context in connectionist networks. Connection Science,
12, 111-142.
Butler, K. (1993). Connectionism, classical cognitivism and
the relation between cognitive and implementational levels
of analysis. Philosophical Psychology, 6, 321–333.
Chalmers, D. J. (1990). Syntactic transformations on distributed representations. Connection Science, 2, 53–62.
Christiansen, M. H., & Chater, N. (1994). Generalization
and connectionist language learning. Mind & Language, 9,
273–287.
230

Dennett, D. C. (1991). Mother nature versus the walking
encyclopedia: a western drama. In W. Ramsey, S. Stich, &
D. Rumelhart (Eds.), Philosophy and connectionist theory.
Hillsdale, NJ: Erlbaum.

Hadley, R. F., & Cardei, V. C. (1999). Language acquisition from sparse input without error feedback. Neural Networks, 12, 217–235.
Hadley, R. F., Rotaru-Varga, A., Arnold, D. V., & Cardei,
V. C. (2001). Syntactic systematicity arising from semantic
predictions in a Hebbian-competitive network. Connection
Science, 13(1), 73–94.

Elman, J. L. (1990). Finding structure in time. Cognitive
Science, 14, 179–211.
Fodor, J. A., & McLaughlin, B. (1990). Connectionism and
the problem of systematicity: Why Smolensky’s solution
does not work. Cognition, 35, 183–204.

Haselager, W. F. G., & Van Rappard, J. F. H. (1998). Connectionism, systematicity, and the frame problem. Minds
and Machines, 8, 161–179.

Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and
cognitive architecture: a critical analysis. Cognition, 28,
3–71.

Jaeger, H. (2003). Adaptive nonlinear system identification with echo state networks. In S. Becker, S. Thrun, &
K. Obermayer (Eds.), Advances in neural information processing systems (Vol. 15). Cambridge, MA: MIT Press.

Frank, S. L. (2005). Sentence comprehension as the construction of a situational representation: a connectionist model.
In A. Russell, T. Honkela, K. Lagus, & M. Pöllä (Eds.),
Proceedings of AMKLC’05. Espoo, Finland: Helsinki University of Technology.

Kohonen, T. (1995). Self-organizing maps. Berlin: Springer.
Maass, W., Natschläger, T., & Markram, H. (2002). Realtime computing without stable states: a new framework for
neural computation based on perturbations. Neural Computation, 14, 2531–2560.

Frank, S. L. (in press). Learn more by training less: systematicity in sentence processing by recurrent networks. Connection Science.

Millis, K. K., King, A., & Kim, H. J. (2000). Updating situation models from descriptive texts: a test of the situational
operator model. Discourse Processes, 30, 201–236.

Frank, S. L., Koppen, M., Noordman, L. G. M., & Vonk,
W. (2003). Modeling knowledge-based inferences in story
comprehension. Cognitive Science, 27, 875–910.

Niklasson, L. F., & Van Gelder, T. (1994). On being systematically connectionist. Mind & Language, 9, 288–302.

Hadley, R. F. (1994a). Systematicity in connectionist language learning. Mind & Language, 9(3), 247–272.

Pollack, J. B. (1990). Recursive distributed representations.
Artificial Intelligence, 46, 77–105.

Hadley, R. F. (1994b). Systematicity revisited: reply to Christiansen and Chater and Niklasson and van Gelder. Mind &
Language, 9, 431–444.

Wilks, Y. (1990). Some comments on Smolensky. In D. Patridge & Y. Wilks (Eds.), The foundations of AI. Cambridge: Cambridge University Press.

Hadley, R. F. (2004). On the proper treatment of semantic
systematicity. Minds and Machines, 14, 145–172.

Zwaan, R. A., & Radvansky, G. A. (1998). Situation model
in language comprehension and memory. Psychological
Bulletin, 123, 162–185.

231

