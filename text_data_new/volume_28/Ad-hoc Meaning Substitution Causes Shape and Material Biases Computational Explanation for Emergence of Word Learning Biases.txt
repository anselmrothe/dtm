get stimuli and it supports possibility of their common
mechanism.
Linda Smith and colleagues led LBA studies (Smith,
1995; Samuelson & Smith, 1999; Smith, Jones, Landau, Gershkoﬀ-Stowe, & Samuelson, 2002; Samuelson,
2002; Colunga & Smith, 2005). They explained that
when experiencing word learning, a “nonlinear attentional system (NAS)” organized attention to particular
dimensions (e.g. shape dimensions), and the selective
attention resulted in word learning bias (Smith, 1995).
Recently, they advanced their hypothesis as “higherorder abstraction (HOA)” (Smith et al., 2002). It explains that from learned noun meanings, children abstract higher-order knowledge which represents meaning nouns generally have; children use it as a template
to estimate meanings of novel nouns, which accelerates
vocabulary acquisition; when children with this mechanism experience early vocabulary that is dominated by
nouns organized by similarity in shape (Samuelson &
Smith, 1999), learned higher-order knowledge becomes
shape-based, which subsequently encourages acquisition
of shape-based meanings.
Their hypothesis is meaningful because it intended
to illustrate mechanism that had been compressed into
NAS. But their hypothesis contains two points of debate:
the age at which cognitive functions become available
and the concreteness of their hypothesis. First, Smith
(1989) demonstrated that children begin to pay selective
attention to particular sensory dimensions in nonnaming
categorization tasks after ﬁve years old. So we consider it
diﬃcult to explain shape and material bias at 24 months
by NAS, although experiencing novel nouns may encourage the organization of selective attention (Smith, 1995).
And they also didn’t provide evidence that HOA is possible at 24 months1 . The second issue about concreteness
means that their hypothesis still retains some unclear
points. Therefore though they veriﬁed NAS or HOA by
simulations with neural networks (Smith, 1995; Samuelson, 2002; Colunga & Smith, 2005), it remains unclear
whether those studies could have veriﬁed their hypothesis because they didn’t describe its essence, that is, how
to acquire the attention or abstraction.
So, in this study we manifest requirements for an alternative hypothesis of shape and material biases as follows:
(1.3a) concrete description of computational process of
the biases; (1.3b) explanation for why their emergence
depends on children’s vocabulary size; (1.3c) explanation
for the existence of interference between them; (1.3d)
clariﬁcation of essential factors that enable shape bias
to appear at 24 months. Based on them, we hypothesize
about these biases as below. Shape and material biases
arise from simple learning and consist of two common
primitive abilities available from infancy: (1) ability to
learn the meanings of words by induction, and (2) ability
to instantly estimate novel noun meaning based on al-

ready learned words meanings and given input stimulus
with the noun. We call the second ability “ad-hoc meaning substitution (AMS).” Both have no speciﬁc mechanism to generate biased behavior. But when exposed to
the statistically biased vocabulary of toddlers, learner
with the two abilities shows shape and material biases.
This triad of the word meaning induction, AMS, and the
early biased vocabulary is essential for the emergence of
biases. But this triad can cause stable biases from very
early stages of development, while young children actually show ineﬃcient word learning and no bias (1.1.1a;
1.1.2a). The delay of bias emergence is caused not by
the triad but by a secondary factor: maturity in learning word meanings.

Proposed Hypothesis
Input and Word Representation
Following Samuelson (2002), input consisted of three attributes: SOLIDITY, SYNTAX, and FEATURE. Their
attributes are represented as 3, 3, and 30 dimensions,
respectively. SOLIDITY, which denotes solidness of objects/substances presented to learners, has three discrete
attributes, SOLID, NONSOLID, and AMBIGUOUS2 .
The attributes were represented as vectors of (0.95, 0,
0), (0, 0, 0.95), and (0, 0.95, 0), respectively. SYNTAX,
which represents a contextual attribute in a sentence
given in parallel with the named object3 , has three discrete attributes: COUNT, MASS, and AMBIGUOUS4 .
They are represented as vectors of (0.95, 0, 0), (0, 0,
0.95), and (0, 0.95, 0), respectively. FEATURE is an
attribute that denotes other information and consists of
three attributes: SHAPE, MATERIAL, and OTHER5 .
They are represented as separate 10-dimentional vectors,
respectively.
We assumed children constructed category knowledge
for nouns. Based on the deﬁnition of FEATURE, we
deﬁned three categories: SHAPE-BASED, MATERIALBASED, and OTHER-BASED, organized on the basis of
similarity in SHAPE, MATERIAL, and OTHER, respectively. These categories can overlap for each noun. Inputs belonging to a SHAPE-BASED category had arbitrary but constant values of SHAPE dimensions and random values of MATERIAL and OTHER dimensions. To
the ﬁxed SHAPE dimensions, random noises between [0.05, 0.05] was added. Inputs belonging to MATERIALand OTHER-BASED categories were made in the same
way as SHAPE-BASED category, respectively.

Word Distributional Prototype (WDP)
We introduced explicit representation of word meaning,
which enable us to describe process of AMS concretely.
We assume that a word meaning is deﬁned as an extended prototype (Rosch & Mervis, 1975) that consists of
distribution formed by inputs co-occurring with the word
2
AMBIGUOUS denotes borderline cases in which SOLIDITY is neutral or both SOLID and NONSOLID are possible.
3
Broad sense of syntax such as articles and determiners.
4
SYNTAX AMBIGUOUS also denotes borderline cases.
5
OTHER includes miscellaneous information other than
SHAPE and MATERIAL: color, taste, temperature, fun, etc.

1

Children begin to use words of superordinate-level categories at about four years.
If the knowledge of a
superordinate-level category word is abstracted from the
knowledge of basic-level category words, the abstraction process should resemble HOA and both available ages can hardly
be expected to be so diﬀerent.

1641

(Kurosaki & Omori, 2005a,b). For example, banana is
generally used for crescent-shaped yellow fruits, but not
for red or globular things. It’s so sensitive to shape and
usage information that even slight discrepancy in them
is unacceptable. Meanwhile, such information as emotion isn’t crucial for the recognition of banana, though
it’s also presented simultaneously with the name. Such
characteristics of word meaning can be expressed by two
factors: mean value in each input dimension and allowable deviation from the mean value. Children must learn
them based on experience.
So we used a multidimensional normal distribution as
one of the simplest possible representations for a word’s
meaning to fulﬁll the above requirements. We called it
word distributional prototype (WDP). Its mean vector
denoted the word’s standard appearance in input space.
Its variance matrix denoted the allowable range of ﬂuctuation from the mean vector. We used diagonal variance
matrices assuming that input dimensions had no correlation each other. Though the simpliﬁcation might have
some problems, we thought WDP was suﬃcient for word
meaning representation of young children.
WDP described each word meaning as below. ~x ∈ <M ,
~x = (x1 x2 · · · xM )T is an input vector in M -dimensional input space, j is an ID number for WDPj ,
µ
~ j ∈ <M , µ
~ j = (µj 1 µj 2 · · · µj M )T is a mean vector
of WDPj , µj i ∈ < is a mean value of input unit i of
WDPj , Σj ∈ <M × <M is a diagonal variance matrix of
WDPj , and σj i ∈ < is a standard deviation of input unit
i of WDPj . Then, likelihood of WDPj to an input ~x is
calculated as:
pj (~x) =

( 1
)
exp − (~x −~
µj )T Σ−1
x −~
µj ) . (1)
j (~
2
(2 π) |Σj |
1

M
2

1
2

WDPs were trained by following algorithm: (2a) Initially, each word is paired with an individual WDP; (2b)
Given an input vector ~x and a corresponding word, all
WDPs calculate likelihood pj (~x) for the input, and the
winning WDPc that outputs the highest likelihood is
chosen; (2c) If the WDPc is the correct paired one with
the given word, then it learns to increase its likelihood for
the input (Eq. (3) and (4)), and the others don’t; (2c’)
If WDPc is incorrect WDP for the given word, then it
learns to decrease its likelihood pj (~x) for the input. Its
update rules correspond to those of the opposite direction of (2c). The correct WDP, which is paired with
a given word and should give the highest likelihood, simultaneously learns by the update rule (2c); (2d) Repeat (2b), (2c), and (2c’) depending on word inputs.
The learning corresponds to extension of “learning vector quantization (LVQ)” (Kohonen, 1995). To maintain
stable learning, we set lower limit of σj i to 0.1 and range
of µj i to [−1, 1]. The initial value of every σj i is set to
a suﬃciently large value so that all WDPs don’t output
higher likelihood to particular inputs in the initial state.
Loss function εc (~x) and update rules for each parameter
are deﬁned as:
εc (~x) = − log (pc (~x))

(2)
1642

p0 (x) p c (x)
Known WDPs
( = Meanings)
Nouns

...

...

Copying Variance
Matrix. (NNH)
Novel WDP

c
0
Dog Cloud

Salt

Novel Noun
"Yith"

Copying Input Vector.
MATERIAL OTHER

SHAPE
...

...

SOLIDITY SYNTAX

...

FEATURE

Figure 1: Ad-hoc meaning substitution (AMS).
µc i − xi
∂εc (~x)
= −α
∂µc i
σc i 2
)
(
∂εc (~x)
1
(µc i − xi )2
.
= −β
= −β
−
∂σc i
σc i
σc i 3
∆µc i = −α

∆σc i

(3)
(4)

Ad-hoc Meaning Substitution (AMS)
When a child is given a novel objects/substances and a
corresponding noun, the single experience may indicate
the mean vector of the noun, but not the standard deviation. Nevertheless, the existence of word learning bias
demonstrates that children do complement the missing
information of standard deviations in some way. Children must do it by use of mechanisms nonspeciﬁc to vocabulary learning because as discussed in introduction,
the biases seems to be learned.
We explain a process of novel noun generalization as
below. Children determine the noun’s meaning based
on meaning of a known noun: The variance matrix of
a known noun WDP, which output the highest likelihood to the input information (Eq. (5)), is copied as
the variance matrix of the novel noun WDP (Eq. (6)).
We call the copying process “nearest neighbor hypothesis (NNH)” (Fig. 1), which denotes that WDP which has
the nearest Mahalanobis distance to the input vector was
chosen. Meanwhile, the input vector presented with the
novel noun was copied as the mean vector of the novel
noun WDP (Eq. (7)). Though they may not work always correctly, algorithm which always complements the
missing information correctly doesn’t exist. We suggest
that one of probable strategies for the complements is to
substitute the nearest known word and input vector for
the information. We call the ad-hoc construction of novel
noun meaning ad-hoc meaning substitution (AMS).
c = arg max pj (~x)

(5)

Σnew = Σc

(6)

µ
~ new = ~x

(7)

j

Table 1: Structural ratio in early vocabulary evaluated in this study. Upper table shows ratio of each attribute value
in SOLIDITY, SYNTAX, and FEATURE. Lower tables show conditional ratio of each attribute value.

SOLID
.63

SOLIDITY
NONAMBIGSOLID
UOUS
.04
.32

SOLID
NONSOLID
AMBIGUOUS

SYNTAX
COUNT MASS
AMBIGUOUS
.74
.10
.16
.88
.03
.09
.07
.43
.50
.56
.19
.25

Simulation 1
Inductive Learning Phase
We divided the word learning process into two phases.
In “inductive learning phase,” learners received sets of
words and corresponding input and constructed WDPs
with Eq. (3) and (4). Then in “generalizing phase,” they
were conducted novel noun generalization tasks on with
AMS and the learned WDPs. We prepared six groups
whose learners were given 18, 50, 102, 213, 281, and 312
words, respectively (see Samuelson & Smith, 1999). We
called them groups 1, 2, 3, 4, 5, and 6, respectively. Each
learner in a group learned diﬀerent words each other.
Vocabulary used in this phase should contain words
generally heard and produced by young children. Hence,
we used MCDI (Fenson et al., 1994), which include a
typical vocabulary for 16- to 30-month-olds. Based on
previous studies (Samuelson & Smith, 1999; Samuelson,
2002), we used 312 words from nine categories6 of MCDI.
We evaluated the words ourselves in terms of SOLIDITY, SYNTAX, and FEATURE (Table 1) so that the
evaluated structural ratio in the vocabulary was consistent with previous study (Samuelson & Smith, 1999) and
made the 36-dimensional input data.
We set learning parameters α and β to 0.001, the initial values of µj i to [0.4999, 0.501] and σj i to 1.0. We
prepared 50 instances for a word, and a corresponding
WDP learned the instances. In an epoch , a learner experienced all instances of all words prepared for the learner.
The learning iterated the epoch 30 times. We conﬁrmed
that the learning of all groups almost converged at 30th
epoch, and that the learned parameters in each WDP
were correctly estimated to form distribution of corresponding noun.

Generalizing Phase
We prepared solid and nonsolid sets and conducted novel
noun generalization tasks separately with them. Each
set had 21 stimulus sets, and a stimulus set consisted
of a target stimulus and 20 pairs of shape-match and
material-match stimuli. Shape-match stimuli were made
as input vectors that had same SOLIDITY and SHAPE
values as the target stimulus, random MATERIAL and

SHAPE
.48
.71
.07
.24

FEATURE
MATERIAL
.16
.09
.86
.36

OTHER
.39
.19
.14
.45

OTHER values, and no SYNTAX7 value. Random
noises between [−0.05, 0.05] were added to the SHAPE
values. The material-match stimuli were made in the
same way. Both sets were initially prepared and shared
by all learners in all groups.
In our model, shape choice probabilities for each group
were calculated as below. Given a novel target stimulus
and a novel noun, Learnerg i of Groupg made a novel
noun WDP applying AMS. Then we gave Learnerg i
20 pairs of shape-match and material-match stimuli
and compared which evoked higher likelihood for each
pair. Shape choice probability to Target stimulusj by
Learnerg i was calculated by p(g, i, j) = (winning number
of shape-match stimuli) / (total number of pairs). Shape
choice probability to all target
(∑ stimuli)by Learnerg i was
calculated by pL (g, i) =
j p(g, i, j) / (total number
of target stimuli). Mean probability
(∑of shape )choice in
Groupg was calculated by p(g) =
i pL (g, i) / (total
number of learners).
For the solid set, t-test conﬁrmed that the shape choice
probability for each group was signiﬁcantly larger than
chance: They showed shape bias; t(20)=12.95, p≺.001;
t(20) =23.02, p≺.001; t(20) =26.77, p≺.001; t(20)=
27.98, p≺.001; t(20)=27.89, p≺.001; and t(20)=32.45,
p≺.001, respectively. For the nonsolid set, the probabilities of shape choice for groups 1, 2, and 3 were signiﬁcantly larger than chance even for the nonsolid set:
They showed overgeneralized shape bias; t(20)=2.71, p≺
.05; t(20)=3.27, p≺.01; and t(20)=4.68, p≺.001, respectively. But those for groups 4, 5, and 6 were signiﬁcantly
smaller than chance: They showed material bias; t(20)
=−3.54, p≺.01; t(20)=−3.57, p≺.01; and t(20)=−4.82,
p≺.001, respectively (Fig. 2).
Results of the stable shape bias during somewhat
larger vocabulary (1.1.1b), the overgeneralized shape
bias during small vocabulary (1.1.2b), and the material
bias during large vocabulary (1.1.2c) is explained as follows. WDPs having same SOLIDITY as a target stimulus, which were SOLID WDPs for solid set and NONSOLID WDPs for nonsolid set, were likely to be chosen
as the nearest WDP by NNH because they output higher
likelihood. Since SOLID WDPs tended to be SHAPE-

6
animals, vehicles, toys, food and drink, clothing, body
parts, small household items, furniture and rooms, outside
things

7
SYNTAX information was withheld because we intended
to compare our results to previous experimental results without syntax (see Samuelson & Smith, 1999; Samuelson, 2002).

1643

0.8
0.6

1

***

***

* **

***

**

0.2

*** ***

***

0.4

0

***

** ***

Solid Set
Nonsolid Set
18

Probability of Shape Choice

Probability of Shape Choice

1

0.8

0.4

*** ***

**

*** ***

***

0.6

***
***

0.2
0

50
102
213
281 312
Number of Known Words in Each Group

***

***

***
Solid Set
Nonsolid Set

18

50
102
213
281 312
Number of Known Words in Each Group

Figure 2: Probabilities of shape choices in generalizing phase of simulation 1 (left) and 2 (right). Vertical lines depict
standard errors. Horizontal broken line depicts chance level (=.5). *p ≺ .05. **p ≺ .01. ***p ≺ .001.

BASED (see Table 1), novel WDP was inclined to copy
the SHAPE-BASED variance matrix by NNH and it led
to shape bias. Meanwhile, NONSOLID WDPs couldn’t
be the nearest to nonsolid set during small vocabulary.
NONSOLID WDPs were far less than SOLID WDPs and
the NONSOLID ﬁeld was too sparse (see Table 1) for
them to be chosen as the nearest. Therefore SOLID
WDPs could be chosen alternatively, though they had
a disadvantage of having diﬀerent SOLIDITY from the
target stimulus. Then as with the case of solid set, it
led to overgeneralized shape bias. When the sparseness
disappeared, NONSOLID WDPs could be the nearest to
nonsolid stimuli and it caused material bias.

Simulation 2
Inductive Learning Phase
We introduced maturity to produce a situation in which
convergence of word meaning learning was more insuﬃcient for learners who had small vocabularies than large
vocabularies. We realized the situation by reducing the
number of instances for each word depending on the vocabulary size of each group: 5, 10, 20, 40, 50, and 55,
respectively, in ascending order of group number. Other
conditions were identical to simulation 1.
Learning hadn’t converged even at 30th epochs in
groups 1 and 2. Their parameters hadn’t been estimated correctly, compared with simulation 1. Though
the learning in group 3 apparently converged better than
groups 1 and 2, their parameters hadn’t been estimated
as well as simulation 1. After group 4, their learning
had almost converged and their parameters had been estimated well enough. From these results, the expected
eﬀects by introducing the maturity were realized.

Generalizing Phase
The generalizing phase procedure was identical to simulation 1. For the solid set, the probability of shape choice
for group 1 was signiﬁcantly lower than chance even

for the solid set, demonstrating material bias: t(20)=
−20.39, p≺.001. But in group 2, there was no signiﬁcant
diﬀerences to chance, that is, it showed no bias: t(20)=
0.081, pÂ.05. After that, the probabilities for groups 3,
4, 5, and 6 were signiﬁcantly higher than chance, showing
shape bias: t(20)=15.01, p≺.001; t(20)=33.59, p≺.001;
t(20)=26.63, p≺.001; and t(20)=37.49, p≺.001, respectively. For the nonsolid set, the shape choice probabilities for groups 1, 2, 4, 5, and 6 were signiﬁcantly smaller
than chance, showing material bias; t(20)=−24.54, p≺
.001; t(20)=−5.98, p≺.001; t(20)=−3.77, p≺.01; t(20)=
−3.86, p≺.001; and t(20)=−4.01, p≺.001, respectively.
But in group 3, we observed a signiﬁcantly larger shape
choice than chance, demonstrating shape bias even for
the nonsolid set; t(20)=5.48, p≺.001 (Fig. 2).
Only the results during small vocabulary were different from simulation 1. So, we discuss the disappearance of robust shape bias during small vocabulary
(1.1.1a, 1.1.2a) and the alternative appearance of material bias. WDPs in the very early groups were closer
to hyperspheres because learning of their variance matrices hadn’t progressed at all due to the eﬀect of maturity. Novel WDPs that copied the variance matrix
output low likelihood to every input equally. It led to no
shape and material bias. Actually the robust shape bias
during small vocabulary in simulation 1 disappeared and
p(g = 1) and p(g = 2) became almost close to chance.
But statistically, early groups showed signiﬁcant material choice. The problem seemed to be due to our simpliﬁed setting in each group. Exactly the same setting of
the maturity and the number of words for learners in a
group caused almost same pL (g, i) and their small standard deviation. In that case, just slight diﬀerence than
chance lead to shape/material bias. But the problem
could be resolved by modifying the current simpliﬁcation. In sum, we could replicate some previous ﬁndings
in simulation 1 (1.1.1b; 1.1.2b; 1.1.2c) and the others in
simulation 2 (1.1.1a; 1.1.2a) and also explain the process
computationally.

1644

Position of AMS
HOA and AMS were proposed as LBA instead of accounts based on innateness of the biases. It’s well known
that children reason about animals and plants by analogy with humans familiar to them (Inagaki & Hatano,
1987). Since AMS means that children construct knowledge of unfamiliar nouns based on nouns familiar to
them, AMS resembles in it. Therefore AMS is natural for
them as a method to estimate novel noun meaning. And
AMS has simpler process, but it can illustrate wider phenomena of shape bias, material bias, and overgeneralized
shape bias than other accounts. Besides, since applicability of HOA to them all is unclear and it has the age
problem, we argue that AMS is more appropriate explanation for the biases. But we shouldn’t exclude HOA
because it can apparently explain acquisition of words
belonging to superordinate categories or more abstract
concepts well. And it is quite plausible that HOA and
AMS cooperatively engage in bias emergence with other
functions (e.g. theory of mind ). We consider AMS to be
the simplest function within them, and thus it takes an
important role as the foundation of bias emergence from
the early stage of development.

Conclusion
In this paper, we presented an integrated explanation of
ad-hoc meaning substitution (AMS) for behaviors that
had been described separately as shape and material biases and veriﬁed it by computer simulations. Besides, to
describe AMS’s processing, we introduced word distributional prototype (WDP) with the inductive learning
function. We consider the explicit meaning representation is valid methodology for illustrating the computational mechanism of word learning bias, which is deeply
committed to word meaning.
Simulation 1 revealed that when learners that possess
(1) WDP and (2) AMS were exposed to (3) early biased
vocabulary, they showed shape, material, and overgeneralized shape bias. This result suggested that the triad
is essential for emergence of the biases. Simulation 2
revealed that when maturity was introduced, learners
showed neither shape nor material bias during the early
small vocabulary. This result indicated that the period
of bias emergence is decided not by the triad but by
maturity. So we can reply to the requirements in introduction: (1.3a) AMS (especially, NNH); (1.3b) bias
emergence is inﬂuenced by maturity and sparseness of
SOLID and NONSOLID WDPs; (1.3c) shape and material biases derive from common mechanisms; (1.3d) the
triad and maturity. Our results suggest that phenomena
concerning shape and material biases, which have been
explained by meta learning (HOA) or built-in languagespeciﬁc mechanism, are explicable with a simple ad-hoc
learning mechanism.

References
Carey, S., & Bartlett, E. (1978). Acquiring a single new
word. Papers and Reports on Child Language Development, 15 , 17–29.

Colunga, E., & Smith, L. B. (2005). From the lexicon
to expectations about kinds: A role for associative
learning. Psychological Review , 112 (2), 347–382.
Dickinson, D. K. (1988). Learning names for materials:
Factors constraining and limiting hypotheses about
word meaning. Cognitive Development, 3 (1), 15–35.
Fenson, L., Dale, P. S., Reznick, J. S., Bates, E., Thal, D.
J., & Pethick, S. J. (1994). Variability in early communicative development. Monographs of the Society
for Research in Child Development, 5 , 242.
Imai, M., & Gentner, D. (1997). A cross-linguistic study
of early word meaning: universal ontology and linguistic inﬂuence. Cognition, 62 (2), 169–200.
Inagaki, K., & Hatano, G. (1987). Young children’s
spontaneous personiﬁcation as analogy. Child Development, 58 , 1013–1020.
Kohonen, T. (1995). Self-organizing maps. New York:
Springer-Verlag.
Kurosaki, K., & Omori, T. (2005a). Computational modeling of word learning biases by using known word
meanings. Proceedings of the 9th IASTED International Conference on Artiﬁcial Intelligence & Soft
Computing (pp. 201–206), Calgary: ACTA Press.
Kurosaki, K., & Omori, T. (2005b). Children construct
novel word meaning ad-hoc based on known words:
Computational model of shape and material biases.
Manuscript submitted for publication.
Landau, B., Smith, L. B., & Jones, S. S. (1988). The
importance of shape in early lexical learning. Cognitive
Development, 3 (3), 299–321.
Quine, W. V. (1960). Word and object. Cambridge: MIT
Press.
Rosch, E., & Mervis, C. B. (1975). Family resemblances:
Studies in the internal structure of categories. Cognitive Psychology, 7 , 573–605.
Samuelson, L. K. (2002). Statistical regularities in vocabulary guide language acquisition in connectionist models and 15-20-month-olds. Developmental Psychology, 38 (6), 1016–1037.
Samuelson, L. K., & Smith, L. B. (1999). Early noun vocabularies: do ontology, category structure and syntax
correspond? Cognition, 35 (3), 1–33.
Smith, L. B. (1989). A model of perceptual classiﬁcation
in children and adults. Psychological Review , 98 , 125–
144.
Smith, L. B. (1995). Self-organizing processes in learning
to learn words: Development is not induction. In C.
A. Nelson (Ed.), The Minnesota Symposia on Child
Psychology, Vol. 28 . Mahwah, NJ: Erlbaum.
Smith, L. B., Jones, S. S., Landau, B., Gershkoﬀ-Stowe,
L., & Samuelson, L. K. (2002). Object name learning
provides on-the-Job training for attention. Psychological Science, 13 (1), 13–19.
Soja, N. N., Carey, S., & Spelke, E. S. (1991). Ontological categories guide young children’s inductions of
word meaning: Object terms and substance terms.
Cognition, 38 (2), 179–211.

1645

