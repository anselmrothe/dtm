UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Teaching Structural Knowledge in the Control of Dynamic Systems: Direction of Causality
makes a Difference

Permalink
https://escholarship.org/uc/item/2044r0jb

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)

Author
Schoppek, Wolfgang

Publication Date
2004-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Teaching Structural Knowledge in the Control of Dynamic Systems:
Direction of Causality makes a Difference
Wolfgang Schoppek (wolfgang.schoppek@uni-bayreuth.de)
Department of Psychology, University of Bayreuth
D-95440 Bayreuth, Germany

Abstract
Recent publications about humans controlling dynamic
systems have emphasized the role of specific rules or
exemplar knowledge. Although it has been shown that
small systems can be controlled with these types of
knowledge, there is evidence that general knowledge about
the structure of a system plays an important role, too,
particularly when dealing with systems of higher complexity. However, teaching structural knowledge has often
failed the expected positive effect. The present work
investigates details of acquisition and use of structural
knowledge. It is hypothesized that guiding subjects to focus
on dependencies rather than effects supports them in applying structural knowledge, especially when the application is
practiced in a strategy training. An experiment with N=95
subjects supported the hypothesis of the usefulness of the
dependency perspective, but revealed an adverse effect of
the strategy training. Differences between subgroups
studying different majors have been found that give rise to
questions about the relation between prior knowledge and
instruction. The results have interesting implications for
models of how structural knowledge is represented as well
as for methods of teaching system control efficiently.

Humans have to deal with dynamic systems throughout
their lives. Especially in industrial environments, people
are confronted with new systems such as production lines
frequently. Therefore it is worthwhile to study how
humans learn to control dynamic systems, and how
instruction can support the learning process.
In cognitive psychology, a common paradigm for
studying the control of dynamic systems can be characterized by the following features: The systems simulate
some fictitious device or environment that most people
have no specific experience with (e.g. a tank with sea
animals in a biology lab, used by Vollmeyer, Burns, &
Holyoak, 1996). This is to ensure an equally low level of
prior knowledge. Discrete linear additive equations are
used for simulation, one equation per output variable.
There is the opportunity to assign values to the input
variables in each simulation step, which is referred to as
“trial”. A number of trials, e.g. six simulated hours, make
up a “round”. The objective for participants is to attain a

1219

specific goal state either at the end of a round, or as soon
as possible and to maintain the state. A prominent, yet
simple example is the “Sugar Factory” (Berry & Broadbent, 1984) that has been used to investigate questions
about implicit vs. explicit knowledge and about rule vs.
exemplar learning (e.g. Dienes & Fahey, 1995; Fum &
Stocco, 2003; Lebiere, Wallach, & Taatgen, 1998)
Research with this paradigm has shown that subjects
largely prefer acquiring and using exemplar knowledge
rather than structural knowledge, i.e. subjects memorize
specific actions taken in specific situations together with
their outcomes. This strategy can be successful under
certain conditions: First, when the system has a small
problem space like, for example, the Sugar Factory (144
states); second, when the same goal state has to be attained repeatedly (Vollmeyer et al., 1996), which means that
only a small fraction of a possibly large problem space is
relevant. Simulation studies with the Sugar Factory have
shown that it can be successfully controlled by using
either declarative representations of specific actions
(Lebiere et al., 1998), or learned production rules that also
represent specific interventions (Fum & Stocco, 2003). In
conditions, however, where subjects have to deal with
huge problem spaces (e.g. because the system is more
complex and subjects have to attain a number of different
goal states), the exemplar strategy is no longer useful1.
Instead, it is more reasonable to use general knowledge
about the causal structure of the system to navigate
through the problem space. I will refer to this type of
knowledge as “structural knowledge”.
In principle, complete structural knowledge is sufficient to control a system even without specific experience.
Although correlations between structural knowledge and
performance have been reported (Funke, 1993), experiments where structural knowledge was taught, usually
failed to demonstrate its superiority (Putz-Osterloh, 1993;
Schoppek, 2002). One reason for this is that deriving
1

The inclination to use exemplar knowledge even when it is
inappropriate may explain why subjects generally perform at
very low levels when they are asked to control complex
dynamic systems that are new to them.

specific actions from structural knowledge is a skill that
has to be practiced in addition to learning the structure.
This view is corroborated by results from studies where
the application of structural knowledge has been practiced
extensively (Preussler, 1998). A second reason for the
difficulties of applying structural knowledge is that knowledge about causal relations is acquired under a different
perspective than it is applied when controlling a system.
This issue is elaborated in the following paragraphs.
Verbal protocols of successful system controllers and
simulation studies (Schoppek, 2002) have helped identify
efficient strategies for acquisition and application of
structural knowledge. A good strategy for exploring the
causal structure of a system is to vary input variables one
at a time to identify the immediate effects of the input
variables and the momentum of the system, which is
produced by effects of output variables onto each other.
For example, a subject could put some lime into the
animal tank to observe the effect onto the oxygen content
of the water, then set lime input back to zero and observe
how the oxygen content changes on its own.
A common application strategy starts with (1) predicting the next state of the system under the assumption of
no interventions, continues with (2) calculating the differences between the predicted and the desired state, (3)
selecting a free input variable, (4) calculating the input
value, and ends with (5) applying the intervention. In the
course of this strategy, for each output variable all their
dependencies are considered in turn. This consideration of
dependencies is a marked difference compared with the
focus on effects that is prevalent during acquisition of
structural knowledge.
Thus we can distinguish two perspectives on causal
relations: One looking for effects of a given cause, the
other looking for possible causes of a given effect. The
first perspective is prevalent during exploration of a new
system, the second is more adaptive during system
control. In the following, I will use the word “effects” to
characterize constructs related to the first perspective, and
“dependencies” to characterize the second perspective.
The distinction of perspectives on causal relations has
a number of implications. The first has to do with the
question what given information cues the retrieval of
what other information. During exploration, when input
variables are manipulated and effects are observed,
associations from cause to effect are learned, resulting in
a structure where representations of input manipulations
act as cues for representations of changes in output
variables. When the task is to control a system and the
dependencies of output variables are considered, output
variables should be learned as cues for input variables.

1220

A second implication concerns the mechanism of
chunking, which plays an important role in successful
problem solving (Newell, 1990, Gobet & Simon, 1996).
The effect perspective suggests chunking together single
effects of a variable (which can be an input or an output
variable), whereas the dependencies perspective suggests
chunking together all dependencies of an output variable.
Again, the second possibility seems to be more adaptive
in system control, because having all dependencies in one
chunk relieves the problem solver from extensive memory
search, a process that consumes much time, poses high
demands on working memory, and is thus error prone.
A second issue in the context of helping humans to use
structural knowledge has to do with strategy instruction.
Undoubtedly, extensive practice under supervision of
experienced operators is effective, but also very costly.
Thus it is important to find ways of leveraging structural
knowledge efficiently. The way followed here was to base
a training program on a strategy that has proven successful in a computer simulated cognitive model of controlling a system similar to the present one (Schoppek, 2002).
To summarize, the aim of the present work is to
investigate ways of teaching structural knowledge about
dynamic systems, either indirectly by manipulating the
perspective on causal relations, or directly by practicing
the application of structural knowledge. Specifically, I
tested the hypothesis that guiding subjects to focus on
dependencies rather than effects enhances performance.
By measuring access to causal knowledge with a speeded
judgment task I investigated if the different perspectives
are also reflected in the representation of structural
knowledge. The results may show new ways of teaching
structural knowledge and extend our understanding of the
use of this type of knowledge.

Experiment
The system I used in this experiment is a simulation of the
influences of three fictitious medicines onto the levels of
three fictitious peptides in the blood. The medicines are
called MedA, MedB, and MedC; the peptides are called
Muron, Fontin, and Sugon. The effects of the substances
onto each other are simulated with the following discrete
linear equations:
(1) Muron t = 0.1 Muron t-1 + 2 MedA t
(2) Fontin t =
Fontin t-1 + 0.5 Muron t-1 – 0.2 Sugon t-1 + MedB t
(3) Sugon t = 0.9 Sugon t-1 + MedC t
In a neutral state with Muron = Sugon = 0 and Fontin = x,
the system is stable. Once some of the medicines are
administered, the system gains momentum. Note that the

amount of Fontin in the blood can only be reduced
through Sugon, which depends on MedC. Since Sugon
decomposes slowly, large time delays of changes in
medication have to be dealt with. Subjects interacted with
the system through an interface consisting of two tables
showing the states of the variables in all trials, and input
boxes where they could enter values for the medicines.
One round comprised six trials, introduced to the subjects
as “simulated hours”.
Structural knowledge was tested with a speeded causal
relation judgment task. All names of input and output
variables were shown on a screen in a spatial arrangement
that matched that of the simulation interface. This was
done to assure that variables could be identified by both,
their names and their locations. Then the name of an
output variable was highlighted on the right side of the
screen, followed by the highlighting of another variable
name on the left side with an ISI of 500 ms. The subject
was asked to respond with pressing one of two keys as
quickly and accurately as possible to indicate her judgment if there was a causal relation between the highlighted variables or not. All 18 possible input-output and
output-output relations were shown in one test. Eight of
these relations had to be answered with yes, 10 with no.
The procedure was arranged such that knowledge of
dependencies should result in faster judgments compared
with pure knowledge of effects. This is expected because
the variable that was highlighted first (the effect) is
assumed to act as a prime for the variable highlighted
second (the cause) only when causal relations have been
memorized under the perspective of dependencies.
Subjects and Design
N=95 subjects, studying different majors at the University
of Bayreuth, participated in the experiment. Subjects were
paid 10 € for their participation.
The factor “type of knowledge” with the levels “knowledge of effects” (Eff) and “knowledge of dependencies”
(Dep), and the factor “strategy training” with the levels
“no training” and “training” were varied between subjects. A third, quasi-experimental factor “field of study”
with the three levels arts/humanities, law/economy, and
science was also analyzed. In principle, subjects were
randomly assigned to one of the four conditions. A few
exceptions from complete randomization were due to the
objective to have approximately equal distributions of
field of study in each condition.
Procedure
The experiment began with a general instruction about the
system. All subjects went through a standardized exploration phase guided by the experimenter. The exploration

1221

was designed to demonstrate all causal relations between
the variables of the system. Subjects were guided to
analyze the observed effects and asked to enter them in
cards provided by the experimenter. The procedure in this
phase was different for the two knowledge conditions: In
the Dep condition, the experimenter consistently asked
for dependencies, and the cards were sorted by the
“dependent” variables Muron, Fontin, and Sugon. In the
Eff condition, the experimenter consistently asked for
effects, and the cards were sorted by the “independent”
variables MedA, MedB, and MedC. At the end of this
phase, the experimenter examined the knowledge of the
subject orally, again consistently asking either for dependencies or for effects. Subjects had to recall all possible
relations with the respective numeric weights before
moving on to the next phase (all subjects achieved that).
Subjects in the “no strategy training” condition could
then explore the system for one round (six simulated
hours) on their own. Subjects in the “strategy training”
condition went through a number of exercises where they
practiced a method of predicting future states of the
system. As mentioned above, this was the first part of a
strategy tested earlier in a cognitive model. Only a part of
the complete strategy was selected to keep the training
short. Nevertheless, all effects (condition Eff) or dependencies (condition Dep) were needed and rehearsed in
these exercises.
Next, all subjects were given the control problems. All
problems comprised six simulated hours and were given
with the objective that the goal states had to be reached as
soon as possible, and to be maintained. Table 1 shows the
initial states and the goal states for the four control
problems. Initially, all variables except Fontin were zero.
In order for the subjects to familiarize themselves with the
control task, they were given two rounds for Problem 1.
Table 1: The four control problems given to the subjects
Problem 1: Fontin = 50
Problem 2: Fontin = 900
Problem 3: Fontin = 2000
Problem 4: Fontin = 50

Æ
Æ
Æ
Æ

Muron = 200 , Fontin = 1000
Muron = 100
Fontin = 1000
Muron = 400 , Fontin = 900

Results
To measure control performance, the solution error was
calculated by summing the natural logs of the absolute
differences between the goal values and the actual values
for each time step of a round (Müller, 1993). A perfect
solution is indicated by a solution error of zero. Since the
results of Problem 2 were close to ceiling, they were
excluded from the analysis. I analyzed the mean solution
error of the remaining problems as dependent variable in

Table 2: Solution error of system control in the various
conditions of the experiment

Strategy
training

Eff

Dep

yes

2.9 (1.5)
n = 24

2.4 (1.5)
n = 26

2.6 (1.5)
n = 50

no

2.4 (1.5)
n = 22

1.7 (0.9)
n = 23

2.0 (1.3)
n = 45

2.6 (1.5)
n = 46

2.1 (1.3)
n = 49

2.4 (1.4)
n = 95

4,5
4
3,5
Solution Error

an ANOVA with the factors “type of knowledge”
(knowledge about effects, “Eff” vs. knowledge about
dependencies, “Dep”), “strategy training” (with vs.
without training), and the quasi-experimental factor “field
of study” of the participant (arts/humanities,
law/economy, science). The means aggregated across all
fields of study are listed in Table 2.
The ANOVA yielded significant main effects of all
three factors, “type of knowledge” (F = 3.94, df = 1,
MSE = 5.57, p = .05), “strategy training“ (F = 5.97,
df = 1, MSE = 8.45, p < .05), and “field of study”
(F = 13.24, df = 2, MSE = 18.75, p < .01). As expected,
subjects who were guided to acquire knowledge of
dependencies were more successful in controlling the
system (mean solution error = 2.1, SD = 1.3) than
subjects who were guided to acquire knowledge of effects
(M = 2.6, SD = 1.5). Contrary to expectation, subjects
who underwent the strategy training performed lower
(M = 2.6, SD = 1.5) than those without strategy training
(M = 2.0, SD = 1.3). Subjects studying arts or humanities
performed worst (M = 3.2, SD = 1.4, n = 33), followed by
subjects studying law or economy (M = 2.3, SD = 1.2,
n = 30). Most successful in controlling the system were
science students (M = 1.6, SD = 1.1, n = 32).
There is a significant interaction between “field of
study” and “type of knowledge“ (F = 3.29, df = 2,
MSE = 4.65, p < .05). Detailed analyses revealed that a
strong effect of “type of knowledge” was only present in
the group of subjects who studied arts/humanities (see
Figure 1). No other effects reached statistical significance
(all p > .05).

3
2,5

Eff
Dep

2
1,5
1
0,5
0
Arts/Hum

Econ/Law

Science

Figure 1: Means and standard errors of solution
error of controlling the system (smaller values
indicating better performance)
“type of knowledge” was confirmed by the analysis
(F = 7.83, df = 1, p < .01), (1559 ms vs. 1237 ms, Dep
faster). However unexpectedly, there was also a main
effect of “strategy training” (F = 11.24, df = 1, p < .01),
(1576 ms vs. 1236 ms, with training faster). No other
effects were significant at the level of α = .05. The results
of the second structural knowledge test were analogous to
the first test.
Similar analyses with the discrimination index (an
index of how well subjects can discriminate between
relations and no relations, cf. Snodgrass & Corvin, 1988)
as dependent variable yielded no significant effects.
Discrimination indices were relatively high in all
conditions (di = 0.89).

Discussion

To test the expectation that knowledge of dependencies
results in faster response times in the speeded structural
knowledge test, I calculated an ANOVA with the same
factors as described above and the mean response times
for hits in the first test as dependent variable. (Three
subjects with mean response times of greater than 3800
ms were excluded from the analysis. Raw values were lntransformed for the ANOVA). The expected effect of

1222

The experiment has confirmed the hypothesis that guiding
subjects to focus on dependencies of output variables
rather than on effects of input variables can enhance performance in controlling a complex dynamic system.
Although there is an effect in the complete sample, the
major contribution came from the subjects studying arts/
humanities. Presumably, this group has the least experience with abstract representations of dynamic systems
and thus learned something new when focusing on
dependencies instead of effects. If the other groups did
not benefit from the manipulation because they take the
dependencies perspective on their own, or because of
some other strategy cannot be told with the present data.
The results of the speeded causal judgment task indicate that focusing on dependencies vs. effects affects the

mental representation of causal relations. The task was
arranged to enable priming from output to input variables,
but not the other way round. Subjects in the Dep condition were significantly faster in judging the relations,
supporting the assumption that they have established
stronger associations between output to input variables
than subjects in the Eff condition.
The two findings are raising the question about their
relation. Are these stronger associations a cause for better
performance or are they just a side effect of the experimental manipulation? If the relation was causal, there
should be a substantial (negative) correlation between
response time in the causal judgment task and solution
error in the control problems. The respective correlation is
r=.05 in the whole sample. Hence, the faster reaction
times in the Dep condition are probably a side effect of
the manipulation. This, in turn, supports the hypothesis
that the positive effect of knowledge of dependencies on
performance is based on the chunking aspect, i.e. the integration of single effect representations according to
output variables. It is possible that especially science
students have built such chunks on their own, even in the
Eff condition. (Note that subjects in the Eff condition
were not prevented from gaining knowledge of dependencies). Figure 2 shows a sketch of the hypothetical
structure of a dependency chunk “Dep01” (the causal
weights are omitted for clarity). The shaded substructure
“Eff01” is a chunk that represents the single causal
relation between MedC and Sripon. The structure, whose
construction in a learning process appears straightforward, mirrors the equations defining the behavior of the
system remarkably. The solid lines indicate slot-value

Figure 2: Hypothetical structure of a dependency
chunk; solid lines indicate slot-value relations, dotted
lines indicate associations.

1223

relations. Dotted lines indicate the associations between
the name of the dependent variable and names of
influencing variables, which may have been learned under
the Dep condition. These associations can explain the
effects in the speeded judgment task, but are not necessary for the usefulness of dependency chunks in control
tasks. This interpretation is in line with the assumption of
Boucher & Dienes (2003) that there are two ways of
learning associations, one resulting in activating relations,
the other resulting in chunks that combine the associated
information. Baker, Murphy and Vallée-Tourangeau
(1996) suppose that these two ways may be attributed to
different modules of the mind. Research on causal reasoning has discovered many other cases where conceptdriven symbolic processing must be assumed in addition
to pure associative learning to explain the phenomena
(Waldmann, 1996).
Unexpectedly, the strategy training had an effect on
answering speed in the causal judgment task (with training faster). According to the above interpretation subjects
must have rehearsed relations between each output variable y and the variables affecting y during the training. In
the Dep condition, this is obvious. Since in the Eff
condition subjects were asked for all variables that had an
effect on the output variable in question, they had to
search memory for names of input variables while the
name of the output variable was present in working
memory. Thus, subjects have learned associations from
output to input in that condition, too.
The adverse effect of the strategy training was also
unexpected. The training had been inspired by results
from cognitive tutoring that subskills can effectively be
trained based on single production rules (Anderson,
1993), and thus, practicing only the most difficult part of
a larger strategy appeared reasonable. However, the success of this kind of training depends on the compatibility
of the practiced subskills with the subjects’ own strategies. This condition seemed to be hurt in the present case.
Subjects might have applied the practiced method of
predicting the next state, and after successful completion
were unclear about what to do next and how to use the
result. An alternative explanation is that the practiced
strategy has interfered with the subjects’ own strategies,
resulting in mixtures of incompatible strategy fragments.
(see e.g. Vosniadou, 1997 for the difficulties of integrating new knowledge with prior knowledge).
In future efforts to train the application of structural
knowledge it should be assured that subjects have at least
an idea of the whole strategy. This could be achieved by
introducing abstract labels for all subgoals and practicing
the whole strategy at least once before possibly focusing
on the most difficult part of it (Catrambone, 1998).

In general, the results of the experiment show that
variations of structural knowledge do affect performance
in the control of dynamic systems. This extends the view
that mainly exemplar knowledge or very specific rules are
used for controlling systems (Dienes & Fahey, 1995; Fum
& Stocco, 2003; Lebiere et al., 1998). It is important to
note that not knowledge about single causal relations as
measured by the discrimination index of the causal
judgment task makes the difference (there were no effects
of the experimental factors on di), but rather the way of
using it, obviously depending on prior knowledge, and the
way of chunking it into larger units.

Acknowledgments
The research reported here was supported by the University of Bayreuth. I would like to thank the anonymous reviewers for valuable hints and Lucie Necasova, Tereza
Kvetnova, and Nha-Yong Au for collecting the data.

References
Anderson J.R. (1993). Rules of the mind. Hillsdale, NJ:
Lawrence Erlbaum Associates.
Baker, A.G., Murphy, A., & Vallée-Tourangeau, F.
(1996). Associative and normative models of causal
induction: Reacting to versus understanding cause. In
D. R. Shanks, K. J. Holyoak, & D. L. Medin (Eds.),
The Psychology of Learning and Motivation 34 (pp. 3 46). San Diego: Academic Press.
Berry D.C., & Broadbent D.E. (1984). On the relationship
between task performance and associated verbalisable
knowledge. The Quarterly Journal of Experimental
Psychology, 36A, 209 - 231.
Boucher, L., & Dienes, Z. (2003). Two ways of learning
associations. Cognitive Science, 27, 807-842.
Catrambone R. (1998). The subgoal learning model:
Creating better examples so that students can solve
novel problems. Journal of Experimental Psychology:
General, 127, 355 - 376.
Dienes Z., & Fahey R. (1995). Role of specific instances
in controlling a dynamic system. Journal of Experimental Psychology: Learning, Memory, and Cognition,
21, 848 - 862.
Fum, D., & Stocco, A. (2003). Outcome evaluation and
procedural knowledge in implicit learning. In R.
Alterman & D. Kirsh (Eds.), Proceedings of the 25th
Annual Conference of the Cognitive Science Society.
Funke, J. (1993). Microworlds based on linear equation
systems: A new approach to complex problem solving
and experimental results. In G. Strube & K.-F. Wender
(Eds.), The cognitive psychology of knowledge. (pp.
313-330). Amsterdam: Elsevier (North-Holland).

1224

Gobet F., & Simon H.A. (1996). Recall of Random and
Distorted Chess Positions: Implications for the Theory
of Expertise. Memory & Cognition, 24, 493-503.
Lebiere C., Wallach D., & Taatgen N. (1998). Implicit
and explicit learning in ACT-R. In F.E. Ritter, & R. M.
Young (Eds.), Proceedings of the Second European
Conference on Cognitive Modelling (ECCM-98) (pp.
183 - 189). Nottingham, UK: Nottingham University
Press.
Müller, H. (1993). Complex problem solving: Knowledge
and reliability. Bonn: Holos.
Newell A. (1990). Unified Theories of Cognition: The
1987 William James Lectures. Cambridge, MA:
Harvard University Press.
Preußler W. (1998). Strukturwissen als Voraussetzung für
die Steuerung komplexer dynamischer Systeme.
[Structural knowledge as precondition for the control of
complex systems] Zeitschrift für Experimentelle
Psychologie, 45, 218 - 240.
Putz-Osterloh W. (1993). Strategies for knowledge acquisition and transfer of knowledge in dynamic tasks. In G.
Strube, & K. F. Wender (Eds.), The cognitive psychology of knowledge (pp. 331 - 350). Amsterdam:
Elsevier.
Taatgen N.A. (2001). A model of individual differences
in learning air traffic control. In E. M. Altmann, A.
Cleeremans, C. D. Schunn, & W. D. Gray (Eds.),
Fourth international conference on cognitive modeling
(pp. 211 - 216). Mahwah, NJ: Lawrence Erlbaum
Associates.
Schoppek W. (2001). The influence of causal interpretation on memory for system states. In J. D. Moore,
& K. Stenning (Eds.), Proceedings of the 23rd Annual
Conference of the Cognitive Science Society (pp. 904 909). Mahwah, NJ: Erlbaum.
Schoppek W. (2002). Examples, rules, and strategies in
the control of dynamic systems. Cognitive Science
Quarterly, 2, 63 - 92.
Snodgrass J.G., & Corvin J. (1988). Pragmatics of measuring recognition memory: applications to dementia
and amnesia. Journal of Experimental Psychology:
General, 117, 34 - 50.
Vollmeyer R., Burns B.D., & Holyoak K.J. (1996). The
impact of goal specificity on strategy use and the acquisition of problem structure. Cognitive Science, 20, 75 100.
Vosniadou, S. (1997) The development of the understanding of abstract ideas, in K. Harnqvist & A. Burgen (Eds.)
Growing up with Science, Jessica Kingsley Publishers
Waldmann M.R. (1996). Knowledge-based causal induction. In D. R. Shanks, K. J. Holyoak, & D. L. Medin
(Eds.), The Psychology of Learning and Motivation 34
(pp. 47 - 88). San Diego: Academic Press.

