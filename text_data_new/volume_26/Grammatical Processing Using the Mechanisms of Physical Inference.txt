UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Grammatical Processing Using the Mechanisms of Physical Inference

Permalink
https://escholarship.org/uc/item/4cw7504r

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)

Author
Cassimatis, Nicholas L.

Publication Date
2004-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Grammatical Processing Using the Mechanisms of Physical Inference
Nicholas L. Cassimatis (cassimatis@itd.nrl.navy.mil)
Naval Research Laboratory, Code 5513
4555 Overlook Ave. SW
Washington, DC 20375

and the anaphoric binding principles that occur in some
form in most mature formal theories of human syntax. Until
these apparently peculiar formal properties are accounted
for using general cognitive mechanisms, the thesis that
humans use different mechanisms for syntactic and
nonlinguistic processing remain plausible.
This paper outlines a mapping of structures in formal
grammar to cognitive structures used to represent physical
events, shows how to use this mapping to construct a model
of human syntactic parsing that uses only the mechanisms
of a model of infant physical reasoning and demonstrates
how this model explains a universal, putatively innate and
language-specific grammatical constraint in terms of
domain-general cognitive mechanisms.

Abstract
Although there is considerable evidence that humans use the
same mechanisms for linguistic and nonlinguistic cognition,
the thesis of linguistic modularity will remain plausible so
long as well-established formal properties of syntax remain
unexplained in terms of domain-general cognitive
mechanisms. This paper presents several dualities between
the formal structure of syntax and cognitive structures used to
represent the physical world. These dualities are used to
construct a cognitive model of syntactic parsing that uses only
the mechanisms required for infant physical reasoning. The
model demonstrates how a formal syntactic constraint, the ccommand condition on binding, can be explained by a
cognitive process used in physical reasoning. Several
consequences for language development and the doctrine of
linguistic modularity are considered.

Grammatical Structure Cognitive structure
Word, phrase, sentence
Event
Constituency
Meronomy
Phrase structure
Constraints among (parts of)
constraints
events
Word/phrase category
Event category
Word/phrase order
Temporal order
Phrase attachment
Event identity
Coreference/binding
Object identity
Traces
Object permanence
Short- and long-distance Apparent motion and long
dependencies
paths.
Table 1. Dualities between elements
of physical and grammatical structure.

Introduction
Although there is extensive evidence that humans use the
same or similar mechanisms for linguistic and nonlinguistic
cognition, the precise manner in which nonlinguistic
cognitive processes are related to the formal properties of
human grammar have yet to be determined.
In the field of linguistic semantics, several researchers
have noticed extensive parallels between physical and
abstract semantic fields. For example, Jackendoff (1990)
has formalized the semantics of many verbs with primitive
conceptual structures such as GO, TO, FROM, PATH, etc.
Leonard Talmy (1985) has shown that semantic fields for
psychological, social, argumentative and many other
domains involve notions of force dynamics that underlie
semantic fields for physical domains.
Cognitive
psychologists (e.g, Boroditsky, 2001; Spelke & Tsivkin,
2001) have found that the way in which language represents
a concept can influence cognition using that concept.
Clark’s (1996) work culminates a long tradition beginning
in the philosophy of language that analyzes language use as
a species of social interaction. Bloom (2000) presents
evidence that children use cognitive abilities that exist for
nonlinguistic purposes to learn the meaning of words.
Some researchers (e.g., Langacker, 1999) have explored
the interaction of grammar and nonlinguistic cognition by
advancing a “cognitive grammar” research program that
views the grammatical structure of sentences as the result of
the process which maps linear sequences of words into
nonlinear cognitive structures. Although the research has
explained many linguistic phenomena, it has not shown in
detail how this transformation explains specific syntactic
constraints such as the empty-category principle, subjacency

Structural Dualities
The structures of grammar and of naïve physics appear more
similar when a verbal utterance is conceived as an event that
is composed of a sequence of word utterance subevents.
Like physical events, verbal events belong to categories,
combine to form larger verbal events and are ordered in
relation to other verbal events according to lawful
regularities. This section examines these dualities in detail,
and shows that many grammatical structures have analogues
to nonlinguistic cognitive structures. These dualities are
summarized in Table 1.

Notation
In order to explain the mapping between syntactic structure
and cognitive structures used to represent the physical
world, it will be helpful to use a formal notation for
representing physical events. This paper uses a notion
based on the notation Cassimatis (2002) uses to present

192

Thus, both perceiving physical scenarios and perceiving
spoken utterances involve a linear sequence of foci that each
integrate multiple dimensions of information.

problems to his model of physical reasoning. Although
there is no claim that the notation resembles the mind’s
representations for syntactic or physical structure, the next
section will show how to use this formalism to present
sentences to a model physical reasoning so that the model
can use its own representations and processes to infer the
syntactic structure of sentences.
In this formalism, events, objects and places have names.
Predicates describe attributes on and relations among named
entities. For example, an event in which an object, x,
moves from p1 to p2 during the temporal interval, t, is
indicated with the following propositions: Category(e,
MotionEvent), Agent(e, x), Origin(e, p1),
Destination(e, p2), Occurs(e, t). Intervals are
ordered using Allan’s (1983) temporal relations. For
example, Before(t1,t2) indicates that t1 finishes before
t2 begins and Meets(t1, t2) indicates that t1 ends
precisely when t2 begins.
Category hierarchies are
described
using
subcategory
relationships,
e.g.,
Subcateogry(Fly, MotionEvent). PartOf(e1,e2)
indicates that event e1 is part of event e2. That two names
for events, objects or places refer to the same object is
indicated using an identity relationship. For example,
Same(o1,o2) indicates that “o1” and “o2” name the same
object. Finally, regularities between physical events can be
expressed using material implication. For example, that an
unsupported object falls is indicated:

Utterances are events.
The philosophical tradition of “speech act theory”, (which is
psycholinguistically implemented by Clark (1996)), holds
that linguistic utterances are actions used to achieve goals.
In this way, words are similar to other nonlinguistic actions
such as gesturing or tool use. Other people’s actions are
events we must perceive in order to interpret their intent.
Both verbal and nonverbal events occur over temporal
intervals. Like nonverbal events, verbal utterances can be
executed with various manners (hastily, carefully, loudly,
softly).
Thus, the same concepts used to describe physical events
can be used to describe verbal utterances. For example,
using the present notation, the utterance of the word “dog”
at time, t, may be represented, Category(e, dogutterance), Occurs(e, t).

Word order is temporal order.
The temporal order of a set of physical events has important
consequences for their ultimate result. For example, pulling
a gun’s trigger before loading it results in a much different
event from the pulling its trigger after loading it. This is
also a fundamental feature of grammar: the result (in terms
of its effect on the listener) of uttering “The dog”, uttering
“bit” and then uttering “John” is much different from the
result of uttering “John”, “bit” and then “the dog”. In our
notation, “John bit the dog” is represented as sequence of
utterance events:

Location(o,p1,t1) + Below(p2,p1) +
Empty(p2,t1)
Æ
Category(e,MotionEvent) + Origin(e,p1)
+ Destination(e,p2) + Occurs(e,t2) +
Meets(t1,t2).

1. Category(e1, JohnUtterance)
Occurs(e1, t1)
2. Category(e2, BitUtterance)
Occurs(e2, t2)
Meets(t1, t2)

With this background, it is now possible to describe
several dualities between syntactic and physical structure.

Physical and verbal event perception both have a
linear order.

Etc.

Physical and linguistic events both belong to
categories, which exist in hierarchies.

Although human vision has two-dimensional access to a
three-dimensional physical world, there is a linear order to
human perception. People can attend to only one region of
space at a time. Large, complicated and/or spatially
distributed visual scenes must be perceived through a series
of attentional foci. For example, a person standing between
two houses, A and B, can perceive that A is to the left of B
by turning to the left, focusing on house A, turning to the
right and focusing on house B. Likewise we perceive verbal
utterances as a linear sequence of word utterance events.
Further, such multidimensionality as there is in the visual
system is not unique to it. Spoken phonemes have a
multidimensional character. In most phonological theories
phonemes are points in a multi-dimensional vector space
with dimensions such as “voiced” or “nasal”.

Word and phrase categories are an important component of
almost every serious syntactic theory and especially
important in some (e.g., Pollard & Sag, 1994). Categories
are also an essential part of most every other domain of
cognition. The previous subsection demonstrated that the
same Category predicate that represents the category of a
physical event can represent the category of a word or
phrase utterance. Likewise, just as physical categories exist
in hierarchies (e.g., Subcategory( RunningEvent,
MotionEvent), so do verbal and phrasal categories (e.g.,
Subcategory(CommonNoun, Noun) and Subcategory(
TransitiveVerbPhrase, VerbPhrase)).

193

Just as the category of a physical event determines which
other events it occurs with (e.g., a gun-firing event tends to
be preceded by a trigger-pulling event), so does the category
of a word or phrase determine the distribution of words and
phrases (e.g., transitive verbs are often followed by noun
phrases).

1987) or category persistence (Xu & Carey, 1999)) by
testing whether infants are surprised by identities that
violate those constraints.

Phrase attachment is an event identity
relationship.
The occurrence of a physical event often implies the
occurrence of another physical event. For example, when
an object resting on shelf falls to the floor (event f), there
must have been an event (p) which pushed the object off the
shelf. One can infer the pushing event from the falling
event even if the pushing event is not visible. Later, after
observing marks left by a cat’s claws on the shelf, we can
infer a cat walking event (w). If this cat walking event
occurred near the original location of the object that fell,
then the cat walking event might be identical to the pushing
event, i.e., Same(p,w).
Event identity is an important feature of grammar as well.
For example, the existence of a prepositional phrase
utterance within a sentence utterance implies the existence
of a noun or verb utterance that the prepositional phrase is
an argument or adjunct of. For example, in the sentence
“John saw the man with the telescope”, the “with the
telescope” utterance event implies the existence of an
utterance event, pp-head, which takes “with the telescope”
as an argument or adjunct. In this case, pp-head might be
the “John” or “man” utterance event. More formally, either
one of the following propositions might be true:
Same(“John”,pp-head) or Same(“the man”, pphead). Thus phrase attachment and attachment ambiguity
are instances of event identity and uncertainty about event
identity.

Constituency is a meronomic relationship.
Physical events combine into larger events, which
themselves can combine into even larger events. Word
utterance events combine into phrase utterance events which
combine into larger phrase utterance events. Meronomy1 is
thus a feature of both physical and verbal events. Predicates
for representing physical event meronomy can capture
phrasal constituency. For example, the noun phrase “the
dog” can be represented thus:, Category(e,
CommonNounPhrase), Category(e1, Determiner),
Occurs(e1,
t1), Category(e2,
CommonNoun),
Occurs(e2,
t2), PartOf(e1,e), PartOf(e2,e),
Meets(e1,e2).
The same notation for expressing physical regularities can
be used to represent phrase structure rules and constraints.
For example, a rule for a transitive verb’s arguments can be
expressed thus:
Category(verb, TransitiveVerb) +
Occurs(verb, t-verb)
Æ
Exists(object) + Category(object,
NounPhrase) + Occurs(object, t-object)
+ Before(t-verb, t-object).

Coreference and binding are object-identity
relationships.

Traces and Object Permanence

Coreference and binding are perhaps the most obvious
identity relationships in language. Consider the following
sentence, where “the dog” refers to an object, d, “the cat”
refers to an object, c, and “it” refers to an object, i:

In many clauses, the arguments of a word are spoken. For
example, in (1), the subject and object phrases of “eats” are
spoken directly before and after it:
(1) [The man] eats [steak].

The dog chased the cat through the park where it lives.

In some cases, however, the argument of a word is not
spoken near it. For example, in (2), the subject noun phrase
of “eats” is not adjacent to it. Sentence (3) show this
distance is not an obstacle to “eats” requiring its subject to
be third-person singular.

“it”’s reference is ambiguous. It can refer to the dog
(Same(i,d)), to the cat (Same(i,c)) or to some other
object in the conversation or the environment (Same(i,?)).
In each case, the coreference is just a special kind of identity
relationship.
Identity is an extremely widespread and important
relationship in everyday physical reasoning. When we lose
visual contact with an object because we turn our gaze or
because it is occluded and then see a similar object, we must
decide whether the sightings are of the same object. Many
infant reasoning experiments test for sensitivity to a
physical constraint (e.g., continuity (Kestenbaum et al.,

(2) The man John said [_ eats [steak]] was wearing a hat.
(3) *The men John said [_ eats [steak]] was wearing a hat.
Thus, even though the subject of “eats” is a “long
distance” from it, that subject’s character is constrained by
or “depends” on “eats”. Such relationships are often called
“long-distance dependencies”. Some grammatical theories
posit the existence of a “trace” that is the subject of “eats”

1
Meronomy is the study of the relationships of an entity with its
parts.

194

structure of an utterance. This section presents a model of
syntactic understanding that is based on Cassimatis’ (2002)
model of infant physical reasoning. The model accounts
for a wide variety of syntactic phenomena, including
gapped constructions, long-distance dependencies and
binding principles, using only the mechanisms included in
the physical reasoning model.
Since the central argument of this paper is that human
physical reasoning mechanisms, whatever they are
ultimately found to be, are sufficient to parse syntactic
structure, this paper has only discussed how to formulate
parsing problems as physical reasoning problems and does
not discuss in any detail the mechanisms of the physical
reasoning model.
Polyscheme contains several modules, called specialists,
for representing aspects of the physical world.
Grammatical knowledge was added to Polyscheme using
the representations of these specialists. For example, the
category hierarchy (strictly speaking, a multi-tree) of
Polyscheme’s category specialist was used to represent
lexical and phrasal category relationships; its temporal
specialist was used to represent word order; its meronomic
specialist was used to represent phrasal constituency and
Polyscheme’s physical constraint specialist was used to
represent phrase structure constraints. No modifications of
Polyscheme representations were needed to represent
grammatical knowledge.
Physical problems are presented to Polyscheme using the
formal language outlined in the last section. The structural
dualities described in that section enable sentences to be
presented to Polyscheme so that it can use its physical
reasoning mechanisms to parse them. For example, the
sentence, “The dog John bought bit him”, is represented by
a series of propositions, Category(w1, TheUtterance)

and is left when “the man” is “moved” out of that subject
position to another part of the sentence.
Invisible events such as traces and long-distance
dependencies are common features of physical inference.
For example, if a ball is rolled behind a screen on a flat table
and fails to roll out from the other end of the screen, one can
posit the existence of a second object behind the screen
blocking the first object and make inferences about it (for
example, that it is large and massive enough to stop the
rolling ball) without ever perceiving the obstacle itself.
Thus just as understanding language (depending on one’s
favorite syntactic theory) requires reasoning about
phonetically
unrealized
phrases,
physical
event
understanding often requires one to reason about events that
are not “visually realized”, i.e., perceived.

Long-distance dependencies and apparent motion
It was just noted that in (2), the number of the phrase “The
man” is constrained by “eats”, which is grammatically
distant from it. Characterizing and inferring these longdistance dependencies has been a difficult problem for
linguistic theorists and designers of sentence parsers for
most of modern linguistic history. This contrasts with
“short-distance” dependencies which are much simpler.
Notice that the subject of “eats” is much closer to and more
obvious in (4) than it is in (2).
(2) The man John said [_ eats [steak]] was wearing a hat.
(4) The steak John said [the man eats _] was tender.
A rough way of characterizing the difference between the
two sentences is that the immediate proximity of “eats” to
its subject makes their relationship much more obvious.
In the case of physical inference, the phenomenon of
object permanence is an example of proximity (in space and
time) making an identity relationship much more obvious
than the identity of two objects perceived over a longdistance. When people see an object at a particular place
and time and then in a fraction of a second see an object
with a similar appearance, the two object sightings are
perceived as the “apparent motion” of a single object.
When the distance between the two objects in space and
time is much larger, the identity is no longer obvious. For
example, when a red Toyota drives into a crowded parking
deck and a red Toyota emerges an hour later, the two car
sightings might or might not be of the same car. The
identity is not so obvious. Thus, long-distance dependencies
and the problems they pose are a common feature of
linguistic as well as physical events.

Occurs(w1, t1), Category(w2, DogUtterance)
Occurs(w2, t2), Meets(t1,t2), etc.

Upon receiving sentences in this format Polyscheme
(using its mechanisms for resolving uncertainties) infers
that the event w2 is a NounUtteranceEvent and that it
should be preceded by an event, dogDeterminer, that is a
DeterminerUtterance event. Polyscheme’s inference
that “the” is the determiner of “dog” is represented by the
proposition: Same(w1,dogDeterminer), which indicates
that the determiner event implied by “dog” is “the”.
Polyscheme’s parse of an utterance is represented by a set
of such propositions. They represent the identity of heads,
arguments and adjuncts implied by words and phrases in
the utterance (e.g., dogDeterminer) to the actual spoken
words or phrases perceived (e.g., “the”).
These
propositions constitute a complete description of the
syntactic structure of a sentence. Figure 1 illustrates the
parse of the sentence, “The dog John bought bit him”.
The crucial point is that once a sentence is represented in
Polyscheme’s input format, only the mechanisms needed
for physical inference are necessary to infer the
grammatical structure of the sentence.

Infant physical reasoning mechanisms are
sufficient to infer grammatical structure.
The dualities between physical and grammatical structure
suggest that mechanisms for inferring the physical structure
of the world might be useful for inferring the grammatical

195

PartOf
(NP1-utterance,
sentence-utterance)

sentence-utterance
Occurs(NP1-utterance,t1-4)
Occurs(“bit”-utterance,t5)
Before(t1-4, t5)

NP1- utterance
Same(NP1-utterance,
“bit-subject-utterance”)
PartOf
(“the”utterance,
NP1-utterance)

Same(NP1-utterance,
“bit-subject-utterance”)

“bought”-subject-utterance

“bit”-object-utterance

relative-clause-utterance

“the”-utterance
t1

“dog”-utterance
t2

“john”-utterance

“bought”-utterance

“bit”-utterance

t4

t5

t3

“him”-utterance
t6

Occurs(“the”utterance”,t1)
Occurs(“dog”-

“bought”-subject-utterance

“bought”-object-utterance
(i.e., “trace” or “gap”)

Same(“john”-utterance,
“bought-subject-utterance”)

Same(NP1-utterance,
“bought-object-utterance”)

Figure 1. The syntactic structure of a sentence represented using concepts from infant physical reasoning.

PartOf(X,Y) and PartOf(Y,X) are both false
(“neither dominates the other”).

Domain-general cognitive processes can
enforce grammatical constraints.

Having thus reformulated c-command as a meronomic
relationship, it is possible explain how a cognitive process
called part inhibition, which Polyscheme uses for physical
reasoning, forces Polyscheme to observe the c-command
condition on binding when parsing sentences.
In most physical interactions, a moving object “stays
together”. The parts of the object move together with the
rest of the whole object. Spelke (1990) has termed this the
“Cohesion Principle”. This principle implies that people
tracking the motion of an object composed of many smaller
objects need only track the compound object.
Its
component objects will be wherever the whole object is.
This makes, for example, the task of tracking one object
composed of seven smaller objects generally much less than
seven times more difficult than tracking one simple object.
Thus, the Cohesion Principle supports the practice of paying
more attention to a whole objects than to its parts.
Markman (1989) found evidence that children do this when
learning words. In Polyscheme, this attention preference
can be implemented with “part inhibition”:

The existence of several language universal constraints on
syntactic structure is perhaps the most apparently unique
feature of syntactic theory. Since they appear so peculiar to
language, these constraints lend creditability to the thesis of
linguistic modularity. This section argues that a supposed
language-specific constraint, the c-command condition on
binding, can be represented using the same cognitive
structures used to represent physical events and that the
cognitive processes used in the Polyscheme physical
reasoning model from the last section explain how parsing
obeys such constraints.
Radford (1997) formulates the c-command condition on
binding thus: A bound constituent must be c-commanded by
an appropriate antecedent. He defines c-command by
stating that A node X c-commands Y if the mother of X
dominates Y, X ≠ Y and neither dominates the other.
C-command is a constituency relationship and can
therefore be reformulated using the notation of section 2:
X c-commands Y if PartOf(X,Z) and there is no
X’ such that PartOf(X,X’) and PartOf(X’,Z)
(i.e.,“Z is the mother of X”); PartOf(Y,Z) (“Z
dominates Y”); Same(X,Y) is false (“X≠Y”); and

When entities e1, …, en, are learned to be part of a
larger entity, E, inhibit each of the ei.

196

physical experience is so much richer than their early
linguistic experience, this and many issues surrounding a
putatively innate language faculty cannot therefore be
resolved through a priori learnability arguments alone.
Finally, this work raises two methodological
opportunities.
First, since many other arguments in
developmental psychology are also of the form, “behavior B
implies knowledge or mechanism X”, cognitive models
which display B without X can potentially falsify those
claims. Second, if the same mechanisms underlie physical
reasoning and syntactic parsing, then corresponding to each
language universal syntactic constraint should be a
cognitive mechanism that supports the observance of this
constraint in the same way that supports binding constraints.
This suggests the potential for the theoretical posits of
syntactic theory to be used as clues for the discovery of
cognitive processes and visa versa.

When active during syntactic parsing in Polyscheme, part
inhibition suppresses the activation of phrases that are
forbidden antecedents under the c-command condition of
binding. This is illustrated for the sentence, “The doctor
Mary met at Bill’s house likes herself”. Notice that by the
time processing reaches “herself”, the model will have
inferred that several noun phrases are constituents (directly
or indirectly) of other noun phrases. In particular:
•
•
•

PartOf(“house”, “Bill’s house”).
PartOf(“Bill’s house”, “The doctor Mary
met at Bill’s house”).
PartOf(“Mary”, “The doctor Mary met at
Bill’s house”).

Part inhibition will therefore inhibit (and hence make
them less likely binding targets) “house”, “Bill’s house” and
“Mary” because they are each part of at least one larger
utterance event. The following rewrite of the sentence
shows these inhibited noun phrases in light gray:

References
Allen, J. F. (1983). Maintaining knowledge about temporal
intervals. Communications of the ACM, 26:832--843.
Boroditsky, L. (2001). Does language shape thought?
English and Mandarin speakers' conceptions of time.
Cognitive Psychology, 43(1), 1-22.
Cassimatis, N. (2002).
Polyscheme:
A Cognitive
Architecture for Integrating Multiple Representation and
Inference Schemes. Ph.D. Dissertation. MIT Media
Laboratory.
Chomsky, N. (1975). Reflections on language. New York:
Pantheon.
Clark, H. (1996). Using Language. Cambridge University
Press, Cambridge, England.
Jackendoff, R. (1990). Semantic Structures. Cambridge,
MA: MIT Press.
Kestenbaum, R., Termine, N., & Spelke, E. S., (1987).
Perception of objects and object boundaries by 3-monthold infants. British Journal of Developmental Psychology,
5, 367-383.
Langacker, Ronald W. (1999).
Grammar and
Conceptualization. Berlin: Mouton de Gruyter, 1999.
Markman, Ellen M. (1989). Categorization and Naming in
Children: Problems of Induction. Cambridge, MA: MIT
Press, 1989.
Pollard, C. & Sag., I. (1994). Head-driven Phrase Structure
Grammar. CSLI Publications, Stanford, 1994.
Radford, Andrew (1997). Syntactic theory and the structure
of English: A minimalist approach. Cambridge:
Cambridge University Press.
Spelke, E. S. (1990). Principles of Object Perception.
Cognitive Science 14: 29-56.
Spelke, E. S., & Tsivkin, S. (2001). Language and
number: A bilingual training study. Cognition , 78, 45–88.
Talmy, L. (1985). Force Dynamics in Language and
Thought. Papers from the Parasession on Causatives and
Agentivity.
Xu, F., Carey, S. & Welch, J. (1999) Infants ability to use
object kind information for object individuation.
Cognition, 70, 137-166.

[The doctor [Mary] met at [[Bill]’s house]]] likes herself.
This example demonstrates that a single cognitive process
(meronomic inhibition) can help syntactic inference
conform to a grammatical constraint (on anaphoric binding)
and on a physical constraint (on object motion). Although
this is only one of many language-universal syntactic
constraints, it raises the possibility that other constraints can
be so treated.

Conclusions and future directions
Considerable work remains to establish that the mechanisms
underlying physical reasoning also support syntactic parsing
and that the model this paper presents is on the right track.
The model must be extended to account for more languages
and more grammatical phenomena, especially accounts of
other universal syntactic constraints. The influence of
mechanisms such as part inhibition on the observance of
syntactic constraints must also be empirically confirmed.
To the extent that the proposed dualities between cognitive
structures and processes involved in inferring the structure
of physical events and the syntactic structure of sentences
are real, several important consequences follow.
First, the ability of a cognitive process such as part
inhibition on help inference conform both to syntactic
binding conditions and to the cohesion constraint on object
motion is relevant to arguments for the existence of innate
linguistic knowledge. These arguments (e.g., Chomsky,
1975) assert that children’s early linguistic experience is too
poor for them to learn these grammatical constraints and
conclude that the constraints must therefore be part of some
innate linguistic knowledge.
This paper raises the
possibility that these constraints are the linguistic
manifestations of cognitive processes involved in cognition
generally. These processes themselves may be innate or
children may develop them as they learn to interact with
their physical environment. In either case, since children’s

197

