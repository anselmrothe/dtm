UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
On the Tip of the Mind: Gesture as a Key to Conceptualization

Permalink
https://escholarship.org/uc/item/0bq3923m

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)

Authors
Hostetter, Autumn B.
Alibali, Martha W.

Publication Date
2004-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

On the Tip of the Mind: Gesture as a Key to Conceptualization
Autumn B. Hostetter (abhostetter@wisc.edu)
Department of Psychology, University of Wisconsin–Madison
1202 W. Johnson Street, Madison, WI 53706 USA

Martha W. Alibali (mwalibali@wisc.edu)
Department of Psychology, University of Wisconsin–Madison
1202 W. Johnson Street, Madison, WI 53706 USA

Abstract

model proposed by Levelt (1989), which divides the speech
production process into three broad stages:
conceptualization, formulation, and articulation. During
conceptualization, the prelinguistic thoughts of a speaker are
generated and combined into propositional form. During
formulation, these thoughts are translated into the
appropriate linguistic units by searching through the mental
lexicon and identifying the proper lemmas and lexical
entries. During articulation, the motor plan for pronouncing
the phonemes corresponding to the lexemes is created and
executed. It seems unlikely that the production of
representational gestures influences motor aspects of
articulation, and, indeed, research has typically focused on
the earlier stages of speech production (conceptualization
and formulation) as the possible beneficiaries of gesture.
Work by Krauss and colleagues (Krauss, Chen, &
Chawla, 1996) places the influence of gesture on speech as
occurring primarily during the formulation stage. According
to their view, referred to hereafter as the Lexical Access
Hypothesis, gestures serve as a cross-modal prime to help
speakers access specific items in the lexicon. In support of
this view, a number of studies have shown that speakers
produce more iconic gestures when the words of an
utterance are more elusive (e.g. Hostetter & Hopkins, 2002;
Morrel-Samuels & Krauss, 1992). For example, when
speakers have time to verbally rehearse an utterance, they
gesture less than when speaking completely
extemporaneously (Chawla & Krauss, 1994). Similarly,
speakers gesture more when describing ideas or shapes that
are not readily named than when describing ideas or shapes
that are easily named (Graham & Argyle, 1975; Morsella &
Krauss, in press). Research with aphasic patients also
suggests that gesture is involved in the formulation stage.
Aphasic patients whose problems are primarily ones of
lexical access use more gestures than age-matched controls
(Hadar, Burstein, Krauss, & Soroker, 1998). Those whose
problems are primarily not ones of lexical access produce
fewer gestures than other types of aphasic patients (Hadar,
Wenkert-Olenik, Krauss, & Soroker, 1998). Finally,
prohibiting speakers from gesturing has been shown to
negatively affect speech fluency, especially for speech that
is spatial in nature (Rauscher et al, 1996).
Studies that induce tip-of-the-tongue states have yielded
slightly less compelling and more contradictory findings
about the facilitative effects of gesture on formulation.

Why do people gesture when they speak? The reasons are not
entirely clear. This paper tests two hypotheses about the role
of gesture in speech production: the Lexical Access
Hypothesis, which holds that gesturing aids in lexical access,
and the Information Packaging Hypothesis, which holds that
gesturing aids in conceptualization. Participants were asked to
describe dot patterns that were either easy or difficult to
conceptualize in terms of geometric shapes. Patterns that were
more difficult to conceptualize elicited more gesture than the
patterns that were easier to conceptualize. This result supports
the Information Packaging Hypothesis.

Introduction
It is often said that a picture is worth a thousand words. In
the case of speech production, it sometimes seems that
creating pictures with our hands can help our audience
understand what we are saying. However, despite the
intuitive feeling that we gesture primarily to help our
audience, some research suggests that gestures contribute
little to an audience’s understanding of a gesturer’s speech
(Krauss, Morrel-Samuels, & Colasante, 1991; Krauss,
Dushay, Chen, & Rauscher, 1995; but see Kendon, 1994 for
an alternative perspective). Speakers often produce
representational gestures even when they know that their
audience cannot see them, making it unlikely that their
intended purpose is solely to help the audience (Alibali,
Heath, & Myers, 2001).
This evidence that gesture does not help comprehension
has led some investigators to propose that gesture has a
more direct role in the speech production process, by
facilitating the planning of speech. Specifically, gesture may
play a role in speaking about ideas that are highly spatial or
motoric in nature (Kita, 2002; Krauss & Hadar, 2001). It has
been shown, for example, that gestures are more likely to
coincide with words that are spatial and concrete (e.g., spin,
under, or cube) than with words that are non-spatial and
abstract (such as evil) (Krauss, 1998; Morsella & Krauss, in
press; Rauscher, Krauss, & Chen, 1996). By actively
engaging spatial-motoric ideas through gesture, it may
become easier to speak about them.
Although gesture may be an overt manifestation of spatiomotoric thought, exactly how gesture may facilitate speech
production is still the subject of some debate. The majority
of research in this area has followed the speech production

589

Frick-Horbury and Guttentag (1998) found that preventing
participants from gesturing increased retrieval failures,
whereas Beattie and Coughlan (1998) found exactly the
opposite pattern. Participants who were restricted from
gesturing actually retrieved more words than those who
were allowed to gesture in Beattie and Coughlan’s study. In
both studies, when participants actually produced gestures,
they did not resolve their tip-of-the-tongue states more often
than when they did not gesture. Thus, although gestures do
tend to co-occur with speech that is spatial and with words
that are difficult to find, the claim that gestures actually help
the speaker to find the right words remains somewhat
unwarranted at this time.
Because Levelt’s (1989) speech production model is a
stage model, each stage of the production process partly
depends on input coming from the previous stage.
Articulation cannot begin without at least a minimal amount
of characteristic input from the formulator; a word cannot be
uttered until it has been decided what word should be
uttered. Likewise, formulation and lexical access depend on
the output from the conceptualizer. The ideas and
propositions that are to be expressed must be available
before the correct lemma and lexical affiliate can be
searched for and accessed. It would seem therefore that
facilitation in the conceptualization process would translate
into some facilitation at the lexical level as well. A concept
that is clear in the speaker’s mind is more readily lexicalized
than a concept that is unclear and vague. Thus, the fact that
gestures tend to co-occur with words that are spatial and
somewhat elusive could also be explained as facilitation at
the conceptual level. Gesture may help speakers to clarify or
organize their ideas, and this may make the output of the
conceptualizer more readily accessible to the formulator.
Such a view of gesture is referred to as the Information
Packaging Hypothesis (Kita, 2000). According to this
hypothesis, gestures help speakers organize knowledge that
is spatio-motoric in nature and put it into a verbalizable
form. Gesture is thus a mode of thinking, an aid in
translating spatio-motoric knowledge into linguistic output.
By activating the appropriate bodily representations of
spatio-motoric ideas, the ideas can be more fully realized.1
Evidence for the Information Packaging Hypothesis
comes from studies that have attempted to manipulate the
difficulty of conceptualization while holding constant the
difficulty of lexical access. Alibali, Kita, and Young (2000)
did this with children using a conservation task. They found
that children used more representational gestures when they
were asked to explain why two items (e.g. two balls of play
dough) were different amounts than when they were asked
simply to describe how the two items looked different. The
words used by the children were highly similar across the

two tasks; however, the explanation task required more
complex conceptualization than did the description task.
The authors argued that children used representational
gestures more frequently in the explanation task because of
the increased demands on the conceptualizer. Melinger and
Kita (2001) found that adults were more likely to gesture in
instances where there was a greater choice of what to say,
despite the fact that the actual words being spoken in both
situations were nearly identical. Again, the authors argued
that gesture arises as a result of taxing the conceptualizer
rather than the formulator.
Although these studies provide suggestive evidence for
the Information Packaging Hypothesis, their conclusions are
far from definitive. Although Alibali et al. (2000) found a
significant difference in gesture production based on the
difficulty of conceptualization, they did not find an
especially strong effect. Also, because the study investigated
gesture production in children, it may not be appropriate to
generalize the findings to adult gesturers. Melinger and
Kita’s (2001) results suggest an effect in adults. However,
they did not report statistical analysis of their data, so it is
not clear whether the differences they describe are reliable.
The present experiment was designed to further
distinguish between the Information Packaging Hypothesis
and the Lexical Access Hypothesis. If gestures do indeed
help speakers to conceptualize a spatial situation rather than
just helping them to find the right words to describe that
situation, then speakers should produce more gestures in a
task where there are multiple conceptual options. Similarly,
when the task provides only one conceptual option, speakers
should produce fewer gestures. However, as long as the
words that are ultimately used to describe the situation are
the same, the Lexical Access Hypothesis would predict no
difference in gesture production regardless of conceptual
difficulty.
In order to manipulate conceptual difficulty without
affecting difficulty of lexical access, it was necessary to find
a task that would result in similar verbal output regardless of
the level of conceptual difficulty. For this purpose, we
designed a dot description task in which participants were
asked to describe patterns of dots to a listener. Patterns of
dots were created that could be conceptualized in a number
of different ways; that is, a number of different geometric
patterns could be imagined to be drawn through each dot
pattern. For example, the pattern in Figure 1 could be
conceptualized as two triangles, one rectangle with a
triangle on top, a five-pointed star, three straight lines, or
two parallelograms. This scenario (the dots-only condition)
should be more conceptually difficult than a scenario in
which the same dot patterns are displayed with lines drawn
through them to guide conceptualization (the dots-plusshapes condition). In both conditions, however, the goal of
the participants is to describe only the dots; thus, the words
ultimately used by participants should be similar regardless
of the conceptual condition in which the dot pattern is
presented.

1

It should be noted that when gestures aid the conceptualizer, they
may or may not also aid the formulator. A speaker who is having
difficulty finding a particular word may use gesture as a way of
clarifying the idea in his or her mind; this may or may not add
enough additional information for the formulator to successfully
access the lexical affiliate.

590

ability to remember dot patterns that are presented to them
for a very short duration and to describe these patterns
effectively to another participant. They were told that their
descriptions would be audio-taped and played later for
another participant who would try to recreate the pattern
based on their descriptions. A hidden video camera was
focused on the participant throughout the experiment
providing a head-on view of the participant from the waist
up, and their descriptions were never heard or seen by
anyone other than the experimenters. Following their
participation, each participant was debriefed regarding the
true nature of the experiment and given the opportunity to
have his or her videotape destroyed. All declined.
Each participant was brought individually into the testing
room, which was divided by a wooden screen. On one side
of the screen, a chair was placed in front of a small table
(58.5 cm H x 71 cm W x 71 cm L) where the laptop
computer was situated. The participant was told to sit in this
chair and the experimenter knelt next to the computer and
participant to give the instructions and practice trial for the
experiment. During instruction, each participant was shown
a sample dot pattern and told that patterns similar to it
would appear on the computer screen for a very short
duration. The participants were told that their task was to
describe the pattern as clearly as possible so that the
participant who would hear the description via audiotape
would be able to successfully reproduce it. Furthermore,
they were told that while the task was to get the listener to
reconstruct the dots only, they should imagine the dot
patterns in terms of geometric shapes and figures. Rather
than saying, for example, that there is a dot at the top of the
page and another dot about 2 cm below it with another dot
directly to the left of that, they should describe the pattern as
being a right triangle with dots on each corner with the right
angle of the triangle facing toward the left.
Participants in both conditions received two sample
patterns that were appropriate to their experimental
condition (i.e., patterns shown to dots-only participants
contained only dots while patterns shown to dots-plusshapes participants showed dots with lines drawn to aid
conceptualization). During the instructions and example
presentations in both conditions, the experimenter produced
some small, scripted gestures that were identical across
conditions.
Following these instructions, participants in both
conditions were asked to complete a practice trial while the
experimenter was still present. The experimenter provided
feedback as needed based on this practice trial to
reemphasize the need to describe the pattern in terms of
geometric shapes and to adequately describe the location of
the dots within these shapes.
Because the purpose of this study was to manipulate
conceptual difficulty without affecting lexical difficulty,
measures were taken to assure that lexical access was as
easy as possible for participants in both conditions.
Following the examples and practice trial, all participants
were presented with an alphabetical list of 16 words that

Figure 1: Sample Pattern (top) in Dots-Only Condition and
4 Possible Conceptualizations (bottom)

Method
Participants
Undergraduate students were recruited from the Psychology
participant pool at the University of Wisconsin, Madison. A
total of 63 individuals (32 males, 31 females) were screened
for participation, and those who had not learned English in
infancy were excluded. Additionally, one individual was not
included because a wrist injury made it difficult for him to
gesture. These eliminations resulted in a final sample of 48
individuals (24 males, 24 females) with a mean age of 19.4
yrs (SD = 1.75). Participants did not know that gesture was
the focus of the study.

Stimuli
The stimuli were six dot patterns, each of which included 6
to 9 black dots on a white background. Each pattern was
designed so that it did not represent the outline of any single
geometric shape, but instead afforded a variety of different
geometric shapes (see Figure 1). Patterns were created in
AppleWorks 6 and loaded into PsyScope for experimental
presentation in the conceptually difficult (dots-only)
condition.
From each of these six dot patterns, the participants’
natural responses in the dots-only condition were used to
create patterns for the dots-plus-shapes condition. Each
conceptualization provided by the participants in the dotsonly condition was transposed onto the appropriate dot
pattern by making lines through the dots to indicate the
conceptualization pattern. These patterns were also made in
AppleWorks 6 and loaded into PsyScope for experimental
presentation in the conceptually easy (dots-plus-shapes)
condition. All stimulus patterns in both conditions were
presented on a Macintosh Powerbook G3 laptop with a 35cm color screen.

Procedure
Participants were told that the focus of the study was their

591

seemed likely to occur in descriptions of the patterns. This
list included names of geometric shapes as well as spatial
and relational prepositions. Each word was displayed in the
center of the computer screen for 1200 ms, and participants
were asked to pronounce each word out loud as it appeared
on the screen. The goal was to have these 16 words primed
and readily accessible to participants in both conditions.
After making sure that the participant understood all of
the instructions, the experimenter pressed ‘record’ on the
audio-recorder and went to the other side of the wooden
screen, where she pretended to prepare for the next part of
the experiment. On the experimenter’s side of the screen, a
table and chair were set up to face away from the participant
so that in addition to vision being blocked by the wooden
screen, the experimenter was not looking in the same
general direction as the participant as he or she described
the patterns. While it is difficult to ever definitively rule out
the possibility that some gestures produced by the
participants were intended for communicative illustration,
the presence of the wooden screen, the relative positions of
the experimenter and participant, and the participants’
naivete regarding the hidden video camera make it highly
unlikely that the participants perceived any direct visual
audience for their descriptions. Thus the gestures produced
by the speakers were most likely for purposes other than
direct communicative illustration.
When the participant was ready to begin the first trial, he
or she pressed a key on the laptop keyboard. At the
beginning of each of the six trials in the experiment, a single
black dot was displayed in the center of the computer screen
for 2 s as a signal that the stimulus pattern was about to
appear. The single dot was then replaced by one of the six
dot patterns, which were presented to all participants in the
same fixed order. The pattern remained on the screen for 3 s
and was followed by a 1 s pause. After this brief pause, a
short beep was heard which cued the participant that it was
time to begin describing the pattern. When the participant
was ready to proceed with the next pattern, pressing any key
on the laptop keyboard prompted the beginning of the next
trial.
Because it is crucial to the design of this study that the
words used by participants in the dots-only condition
closely match those used by participants in the dots-plusshapes condition, each participant in the dots-plus-shapes
condition was matched to a participant in the dots-only
condition. The responses of the participant in the dots-only
condition were used to create the stimuli shown to the
matched participant in the dots-plus-shapes condition. Lines
and shapes were drawn through each dot pattern to produce
a replica of the conceptualization that was described by the
dots-only participant. This was done in order to encourage
the participants in the dots-plus-shapes condition to
conceptualize the pattern in the same way as their dots-only
counterpart, and consequently to use similar words to
describe the pattern. Because this process necessitated that
the stimuli be redesigned and the computer reprogrammed
for each dots-plus-shapes participant, 8 participants were

completed in the dots-only condition before the patterns
were recreated for the dots-plus-shapes condition.
Participants were then randomly matched to one of the dotsonly participants with gender as the only criterion for
pairing; males were always matched to males and females
were always matched to females. This pairing procedure
was then repeated for blocks of 8 individuals until the final
sample of 24 gender-matched pairs (12 male and 12 female)
was obtained.

Coding
The descriptions given by each participant were transcribed
verbatim, and all iconic gestures were identified. Individual
gestures were distinguished from one another by a change in
hand shape or motion. For example, a motion straight across
from left to right accompanying the words “the bottom line”
was coded as one iconic gesture. If a similar movement
occurred as the first motion of a sequence in which the hand
moved diagonally upward and then diagonally downward
without changing hand shape (to imply triangle), this entire
sequence was coded as one iconic gesture.
The total number of gestures produced by each participant
for each pattern was divided by the total number of words
uttered during the participant’s description of that pattern.
This quotient was then multiplied by 100 to yield the iconic
gesture rate per 100 words. Thus, each participant’s gesture
rate per 100 words was calculated for each of the six pattern
descriptions.
Table 1: Frequency of Spatial Terms Used in Each
Condition
Dots-Only
121
116
110
100
94
90
72
58
58
54

Spatial Word
Line
Triangle
Top
Right
Bottom
Point
Middle
Parallelogram
Down
Left

Dots-Plus-Shapes
148
172
122
67
124
106
78
89
45
40

Results
Analysis of Speech Produced
To address the question of whether conceptual difficulty
affects gesture production separately from the effects of
lexical difficulty, it is important that the lexical items being
retrieved are similar across conditions. The 10 most
commonly used spatial terms used by participants in the
dots-only condition are shown in Table 1. Although there is
some variation in the exact rank-order of the words, the
correlation between the frequency of occurrence of each
word in the two conditions is high and significant, r(8) =

592

0.833, p < .001, suggesting that the spatial words used by
participants were similar in the two conditions.

The dots-only condition included the four participants with
the highest gesture rates, but also included four participants
who did not gesture at all. The dots-plus-shapes condition
also included participants at both extremes, with six who
gestured more than 10 times per 100 words and seven who
did not gesture at all.

Analysis of Gesture Production

Iconic Gestures / 100
Words

The central question of interest in this experiment is whether
or not frequency of gesture production is affected by
difficulty of conceptualization. The Information Packaging
Hypothesis predicts that gestures aid in conceptualization
and should thus be used more frequently when
conceptualization is more difficult. Alternatively, the
Lexical Access Hypothesis predicts that so long as the
accessibility of the lexical items being produced remains
constant, gesture production should not be affected.
An inherent problem in analyses of gesture production
between participants, however, is the fact that there is a
large amount of individual variation in the amount of
gesture produced by different individuals. Some individuals
gesture a lot, while others gesture rarely or not at all. For
example, in the current data set, two participants produced
an average of more than 16 gestures per 100 words, while
eleven others did not gesture at all. This large variation in
gesture production between participants necessitates the
presence of a very large difference between condition means
before a significant effect can be detected. Because of this
issue, we analyzed the data using items (i.e., dot patterns)
rather than individuals as the unit of analysis. That is, rather
than collapsing across patterns and comparing individuals in
the two different conditions, data were collapsed across
participants and gesture rates were compared for each of the
six patterns.
A paired t-test revealed a significant effect of condition
on iconic gesture rate (t(5) = 2.84, p < .05). Patterns in the
dots-only condition elicited an average of 5.69 iconic
gestures per 100 words (SD = .81) whereas the same
patterns in the dots-plus-shapes condition elicited an
average of 4.79 iconic gestures per 100 words (SD = 0.26)
(see Figure 2). Thus, gesture rate varied as a function of
difficulty of conceptualization, with more gestures produced
in the condition with more difficult conceptualization. The
distribution of high and low gesturers in each condition
suggests that this effect was not driven solely by the
presence of a few high gesturers in the dots-only condition.

Discussion
The Information Packaging Hypothesis (Kita, 2000) holds
that gesture helps refine and organize spatio-motoric
concepts so that they can be readily translated into
verbalizable units. Alternatively, the Lexical Access
Hypothesis holds that gesture primarily aids speech
production by priming the appropriate items in the lexicon.
The present experiment sought to distinguish between these
two hypotheses by varying the conceptual difficulty of a
task and observing the extent to which gestures are
produced under the two levels of conceptual difficulty.
The difference in gesture rates in the two conditions
supports the Information Packaging Hypothesis. Although
the difference was small, participants gestured at a higher
rate in the dots-only condition, in which they had to produce
their own conceptualizations of the stimuli, than in the dotsplus-shapes condition, in which they had the
conceptualizations given to them. This finding suggests that
gesture does indeed occur not only when lexical access is
more difficult, but also when the situation is more
conceptually difficult. The present findings do not disprove
the Lexical Access Hypothesis, but they do suggest that
gesture can serve an earlier stage in the speech production
process, above and beyond any benefits it may have at the
lexical access stage.
This finding is consistent with current views about the
embodied nature of cognition (e.g., Glenberg, 1997). Briefly
stated, the central claim of embodied accounts of cognition
is that the ways in which we are able to interact bodily with
the world profoundly affect the way we think. From an
embodied perspective, symbolic representations such as
language are grounded or assigned meaning via their links
to bodily experiences and actions. It has been shown, for
example, that sentence comprehension is affected by how
easy it is for the comprehender to mentally simulate him or
herself actually performing the actions implied by the
sentence (Chambers, Tanenhaus, Eberhard, Filip, &
Carlson, 2002; Glenberg & Kaschak, 2002).
If we understand language in terms of how we can
interact bodily with the world, it seems likely that this same
embodied knowledge may also be integral to our ability to
produce language. Indeed, the present work regarding the
role of gesture in conceptualizing and formulating speech
seems to point to a role for embodiment in language
production. We suggest that the spontaneous gestures
produced in the act of speaking are a manifestation of
embodied knowledge. Borrowing a phrase from Schwartz
(1998), who argued that gestures reflect “physically
instantiated mental models”, we suggest that gestures reflect
bodily instantiated mental models. According to the

6.5
6
5.5
5
4.5
4
3.5
3
Dots-Only

Dots-Plus-Shapes

Figure 2: Average Rate of Iconic Gestures/100 Words
Produced in Each of the Two Conditions

593

Information Packaging Hypothesis, such gestures enhance
speakers’ abilities to think and speak about those concepts.
Thus, when speakers activate their embodied knowledge
through gestures, they are better able to express that
knowledge in the linear, symbolic system of language.
In conclusion, then, it may very well be that a picture is
worth a thousand words; however, the pictures we make
with our hands are not only worthwhile for our listeners, but
also for ourselves.

Neuropsychological evidence. Brain & Language, 62,
107-126.
Hostetter, A. B., & Hopkins, W. D. (2002). The effect of
thought structure on the production of lexical movements.
Brain & Language, 82, 22-29.
Kita, S. (2001). How representational gestures help
speaking. In D. McNeill (Ed.), Language and Gesture.
Cambridge, UK: Cambridge University Press.
Krauss, R. M. (1998). Why do we gesture when we speak?
Current Directions in Psychological Science, 7, 54-60
Krauss, R. M., Chen, Y., & Chawla, P. (1996) Nonverbal
behavior and nonverbal communication: What do
conversational hand gestures tell us? Advances in
Experimental Social Psychology, 28, 389-450.
Krauss, R. M., Dushay, R. A., Chen, Y., & Rauscher, F.
(1995). The communicative value of conversational hand
gestures. Journal of Experimental Social Psychology, 31,
533-552.
Krauss, R. M. & Hadar, U. (2001). The role of speechrelated arm/hand gestures in word retrieval. In R.
Campbell & L. Messing (Eds.), Gesture, Speech and Sign.
Oxford: Oxford University Press.
Krauss, R. M., Morrel-Samuels, P., & Colasante, C. (1991).
Do conversational hand gestures communicate? Journal
of Personality and Social Psychology, 61, 743-754.
Levelt, W. J. M. (1989). Speaking: From intention to
articulation. Cambridge, MA: MIT Press.
Levelt, W. J. M., Roelofs, A., & Meyer, A. S. (1999). A
theory of lexical access in speech production. Behavioral
and Brain Sciences, 22, 1-38.
McNeill, D. (1992). Hand and mind: What gestures reveal
about thought. Chicago: University of Chicago Press.
Melinger, A. & Kita, S. (2001?). Does gesture help
processes of speech production? Evidence for conceptual
level facilitation. Proceedings of the 27th Berkeley
Linguistics Society Meeting.
Morrel-Samuels, P., & Krauss, R. M. (1992). Word
familiarity predicts temporal asynchrony of hand gestures
and speech. Journal of Experimental Psychology:
Learning, Memory, & Cognition, 18, 615-622.
Morsella, E., & Krauss, R. M. (in press). Movement
facilitates speech production: A gestural feedback model.
Rauscher, F. H., Krauss, R. M., & Chen, Y. (1996). Gesture,
speech, and lexical access: The role of lexical movements
in speech production. Psychological Science, 7, 226-231.
Schwartz, D., & Black, J. B. (1996). Shuttling between
depictive models and abstract rules: Induction and
fallback. Cognitive Science, 20, 457-497.

Acknowledgments
We thank Arthur Glenberg, Charles Snowdon, and
Maryellen MacDonald for their insightful comments on the
design of this project.

References
Alibali, M. W., Kita, S., & Young, A. J. (2000). Gesture and
the process of speech production: We think, therefore we
gesture. Language and Cognitive Processes, 15, 593-613.
Alibali, M. W., Heath, D. C., & Meyers, H. J. (2001).
Effects of visibility between speaker and listener on
gesture production: Some gestures are meant to be seen.
Journal of Memory and Language, 44, 169-188.
Beattie, G., & Coughlan, J. (1998). An experimental
investigation of the role of iconic gestures in lexical
access using the tip-of-the-tongue phenomenon. British
Journal of Psychology, 90, 35-56.
Chambers, C. G., Tanenhaus, M. K., Eberhard, K. M., Filip,
H., & Carlson, G. N. (2002). Circumscribing referential
domains during real-time language comprehension.
Journal of Memory and Language, 47, 30-49.
Chawla, P., & Krauss, R. M. (1994). Gesture and speech in
spontaneous and rehearsed narratives. Journal of
Experimental Social Psychology, 30, 580-601.
Frick-Horbury, D., & Guttentag, R. E. (1998). The effects of
restricting hand gesture production on lexical retrieval
and free recall. American Journal of Psychology, 111, 4363.
Glenberg, A. M. (1997). What memory is for. Behavioral
and Brain Sciences, 20, 1-55.
Glenberg, A. M. & Kaschak, M. P. (2002). Grounding
language in action. Psychonomic Bulletin, & Review, 9,
558-565.
Graham, J. A., & Argyle, M. (1975). A cross-cultural study
of the communication of extra-verbal meaning by
gestures. International Journal of Psychology, 10, 57-67.
Hadar, U., Burstein, A., Krauss, R. M., & Soroker, N.
(1998). Ideational gestures and speech: A neurolinguistic
investigation. Language and Cognitive Processes, 13, 5976.
Hadar, U., Dar, R., & Teitelman, A. (2001). Gesture during
speech in first and second language: Implications for
lexical retrieval. Gesture, 1, 151-165.
Hadar, U., Wenkert-Olenik, D., Kuass, R. M., & Soroker,
N. (1998). Gesture and the processing of speech:

594

