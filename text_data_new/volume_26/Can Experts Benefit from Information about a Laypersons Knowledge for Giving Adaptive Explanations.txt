UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Can Experts Benefit from Information about a Layperson’s Knowledge for Giving Adaptive
Explanations?

Permalink
https://escholarship.org/uc/item/6mz4r13b

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)

Authors
Wittwer, Jorg
Nuckles, Matthias
Renkl, Alexander

Publication Date
2004-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Can Experts Benefit from Information about a Layperson’s
Knowledge for Giving Adaptive Explanations?
Jörg Wittwer (wittwer@psychologie.uni-freiburg.de)
Matthias Nückles (nueckles@psychologie.uni-freiburg.de)
Alexander Renkl (renkl@psychologie.uni-freiburg.de)
University of Freiburg, Educational Psychology, Engelbergerstr. 41
79085 Freiburg, Germany

Abstract
E-consulting services such as asynchronous helpdesks for
hardware and software are a common and comfortable way to
get expert advice. However, the constraints of asynchronous
communication and the experts’ inclination to forget about
the exclusiveness of their specialist knowledge may impair
the advisory success. Against this background, an assessment
tool has been developed which aids helpdesk experts in
evaluating an inquirer’s background knowledge. In a previous
study, it could be demonstrated that the assessment tool increased the effectiveness and efficiency of asynchronous
communication. In order to test the mechanisms that make the
assessment tool effective, another dialogue experiment was
conducted that varied the validity of the information displayed in the assessment tool. The results showed that the information presented to the experts did not only sensitize them
for the inquirers’ needs but also allowed for specific adaptation to their individual knowledge state. Hence, the validity of
the information provided by the assessment tool is crucial.

Introduction
Inasmuch as knowledge becomes ever more specialized and
complex, individuals often lack the expertise necessary for
making a decision or solving a problem on their own (Nückles & Bromme, 2002). Thus, in many situations, laypersons
are reliant on expert advice. The proliferation of the Internet
offers new possibilities for laypersons to enlist the assistance of experts. Not only can laypersons retrieve expert
information publicly available from the World Wide Web
but they can also obtain personal advice from experts in a
one-to-one fashion. Helpdesks for hardware and software
are a prominent example of e-consulting services that enjoy
increasing popularity (Moncarz, 2001). Virtually every
large computer company and university computer centre
offers helpdesk support, often in a text-based, asynchronous
way via electronic mail. The aim of computer consulting is
to convey knowledge, which enables the inquirers to solve
their problem by themselves, for example, when new and
complex software has to be learned or an unexpected technical problem with the computer suddenly occurs. The advisory success heavily depends on the experts’ ability to provide intelligible and informative explanations for inquirers
with differing levels of experience, ranging from very inexpert to more advanced users (Chin, 2000; Kiesler, Zdaniuk,
Lundmark, & Kraut, 2000). Thus, in order to give effective
and satisfactory advice, experts should adapt their commu-

nication to the knowledge prerequisites of the client (Clark
& Murphy, 1982). Both from an educational (e.g., Renkl,
2002) and psycholinguistic perspective (e.g., Clark, 1996),
adaptation to a communication partner’s prior knowledge is
regarded as fundamental for comprehension and learning.
Research on expertise has shown that experts, as compared to novices, possess an extensive and highly differentiated knowledge base that facilitates a rapid categorization of
problem situations and the activation of routine problem
solving strategies (Chi, Glaser, & Farr, 1988). However,
these very characteristics of expert knowledge might interfere with the task of taking into account the limited domain
knowledge of a layperson. Hinds (1999) called this phenomenon the ‘curse of expertise’. She reported two experimental studies in which experts systematically underestimated the difficulties laypersons faced when performing a
complex task. Alty and Coombs (1981) analyzed face-toface advisory dialogues between computer experts and clients. They found that the computer experts rarely attempted
to ascertain the clients’ prior knowledge and rarely monitored the clients’ comprehension of their explanations. As a
result, the clients often did not understand the advice given.
From these studies it can be concluded that in order to assure effective advice, experts should be supported in taking
into account the knowledge prerequisites and comprehension of the client.
In face-to-face communication, the communication partners can use a variety of situational and interactional cues to
monitor their interlocutor’s comprehension moment by
moment and thereby refine and update their mental model
of what the other person knows or does not know (Clark,
1996; Nickerson, 1999). In Internet-based counselling,
however, the evaluation of an interlocutor’s knowledge and
the continuous construction of a mutual understanding are
considerably more difficult when compared with face-toface communication (Clark & Brennan, 1991). First, in
asynchronous communication, nonverbal feedback is virtually impossible because the interlocutors cannot see nor
hear one another. Second, the costs of message production
are higher than in verbal communication because every
message has to be typed on a keyboard. Third, there is no
set sequentiality between a message and its reply, because
the interlocutors’ turn taking may be interrupted by messages from third parties, which can impair comprehension
(Clark & Brennan, 1991). Given these constraints, the pos-

1464

sibilities to establish a mutual understanding are clearly
more restricted as compared with face-to-face communication. On the other hand, asynchronous communication also
offers affordances that can facilitate adaptation to a communication partner. It allows for a careful planning and revision of a message before it is sent. There is time to reflect
about a communication partner’s background knowledge
and communicational needs.

Computer knowledge
My knowledge about
computer is

low

rather low

moderate rather high high

low

rather low

moderate rather high high

Internet knowledge
My knowledge about
the Internet is

Knowledge about concepts
Trusted Zone

low

rather low

moderate rather high high

Applet

low

rather low

moderate rather high high

Dialog with client

The Assessment Tool - A Measure to Support
Asynchronous Communication
From the preceding discussion it can be concluded that it
would be useful to provide helpdesk experts with a support
procedure that compensates for the constraints of asynchronous communication on the one hand, and takes advantage
of the affordances on the other hand. When computer experts communicate with clients via an Internet-based helpdesk, they are in an anonymous communication situation
with only little information available about the client.
Therefore, the procedure should enable the expert to achieve
a relatively concise and veridical evaluation of a client’s
knowledge state right from the start, because the lack of
nonverbal feedback, the raised production costs and the limited sequentiality impede the continuous construction of a
mutual understanding considerably. With regard to experts’
inclination to forget about the exclusiveness of their knowledge, the procedure should encourage them to carefully reflect about a client’s knowledge prerequisites in order to
facilitate adaptation to the client’s communicational needs.
The better the computer experts’ model of the client’s
knowledge is, the better the experts can adapt their explanations to the client’s knowledge (Clark & Murphy, 1982).
In this paper, an assessment tool will be empirically tested
that supports computer experts in constructing a mental
model of the client’s knowledge state in asynchronous communication (see also Nückles, Wittwer, & Renkl, 2003).
The tool consists of a small Internet-based questionnaire by
which users who place a technical support inquiry are asked
to provide the expert with several self-assessments of their
computer expertise (cf. Figure 1). For example, the clients
are asked to rate their general level of computer knowledge
as well as their knowledge of concrete specialist terms semantically relevant to the topic addressed by their inquiry.
The assessment tool can be especially useful to the expert if
it enables them to form a picture of the client’s knowledge
level based on a small number of highly relevant information items. The assessment tool provides the expert with
information about the client right from the start, which normally can only be collected during the course of the interaction process. Consequently, it should facilitate the collaborative effort of communication (Clark, 1996). However, the
assessment tool can only be effective if the medium of
communication allows for careful planning and the revision
of one’s communicational contributions. Therefore, the assessment tool seems to be especially suitable for asynchronous, written communication because there is time for reflection and revision before a message is sent.

Sometimes when I visit websites I get
the following message: „Your current
security settings prohibit running
Active X controls on the page. As a
result, the page may not display
correctly.” I would like to understand
why this happens and how can I get
rid of the problem.

Field for your answer
The Internet Explorer divides internet
addresses into four zones the most
important of which is the "Internet
Zone". As almost every single site
you ever visit will be in this zone, you
should pay particular attention to
what its security settings are.
Therefore, the default security level

SEND

DELETE

Figure 1: Screenshot of the assessment tool
available to the computer expert.
The assessment tool has already been successfully tested
in a web-based dialogue experiment between computer experts and clients (Nückles & Stürz, in press). With the assessment tool, the clients acquired significantly more
knowledge than the control group without the assessment
tool (increased communicative effectiveness). At the same
time, they wrote back only half as often in response to the
experts’ explanations (increased communicative efficiency).
Although the study demonstrated that the assessment tool
approach was successful, it is unclear which mechanisms
led to the increase in communicative effectiveness and efficiency. There are two main theoretical explanations that
may account for these findings.

Theoretical Explanations of the Assessment Tool
Effect
In Nickerson’s theory (1999), the construction of a mental
model of another person’s knowledge is conceptualized as
an anchoring and adjustment process (Tversky & Kahneman, 1974), where one’s model of one’s own knowledge
serves as a default model of what a random other person
knows. This default model is transformed, as individuating
information is acquired, into models of specific other individuals. Accordingly, one could argue that the assessment
tool presented individuating information about the client’s
knowledge level that provided the computer expert with a
relatively specific anchor right from the start of the advisory
dialogue. This enabled the expert to calibrate their mental
model of the client’s knowledge more quickly and accurately than would have been possible without the assessment tool, that is, only on the basis of the client’s written
questions and comments. According to this explanation,
communicative effectiveness was raised because the assessment tool provided the expert with specific information
that helped them to adapt to the client’s individual knowledge level.
On the other hand, it may be argued that communicative
effectiveness was raised not because of the information pre-

1465

sented, but simply because the assessment tool increased the
expert’s awareness of the client and counteracted the tendency of de-individuation in Internet-based communication
(Gunawardena, 1995). The experts were sensitized to reflect
about the client’s knowledge, for example which computer
concepts are typically known by laypersons and which are
not. This may have helped them to produce explanations
that were more intelligible or informative for the typical
layperson, irrespective of the specific knowledge level of an
individual client. According to this explanation, the assessment tool had a more or less non-specific sensitizing effect
on the expert. Against this background, the goal of the present experiment was to test whether the availability of specific information about the client’s knowledge would make
a difference at all, that is, support the experts’ adaptation
and thereby enhance communicative effectiveness and efficiency.
To this purpose, we modified the experimental design
employed by Nückles and Stürz (in press). First, instead of
using self-assessments, the assessment tool in this experiment provided the expert with objective information about
the client’s knowledge. Although self-assessments have
proven to be good predictors of computer expertise (cf.
Richter, Naumann, & Groeben, 2000; Vu, Hanley, Strybel,
& Proctor, 2000), they still are not completely valid. Therefore, by using objective data about the client’s computer
knowledge, we increased the power for detecting a potential
effect of specific adaptation. Secondly, a third experimental
condition was included, in addition to a communication
condition with the assessment tool and a condition without
assessment tool. The information displayed in this additional condition was randomly drawn from the pool of
knowledge data of clients who had previously participated
in the experiment. The random data condition checked to
see whether a distortion of the information about the client’s
knowledge level would impair the communication process.
Consequently, the inclusion of this experimental condition
would enable us to evaluate whether the specific information displayed by the assessment tool would influence the
adaptivity of the experts’ explanations.

Predictions
Sensitization hypothesis. If the assessment tool mainly had
a sensitizing effect on the computer expert, that is, the information about the client was of little surplus value, it
should make no difference whether the displayed information was valid or distorted. Accordingly, the mere presence
of an assessment tool is supposed to increase the experts’
awareness of the client and this alone should help them to
improve their explanations. Consequently, in the conditions
with the assessment tool the clients should acquire substantially more knowledge compared with clients in the condition without the assessment tool. Moreover, if the clients
received explanations that were more intelligible and more
informative compared with the condition without the assessment tool, they should experience less comprehension
problems and should be more satisfied with the explana-

tions. Hence, this should lessen their need of writing back in
response to an expert’s explanation. Consequently, the frequency of questions, and more specifically, the frequency of
comprehension questions should be reduced in both conditions with the assessment tool.
Specific adaptation hypothesis. If the information provided by the assessment tool facilitates the adaptation to a
specific client’s knowledge, both the increase in communicative efficiency and effectiveness should be substantially
larger in the condition presenting valid data about the client
as compared with the other conditions. In contrast, communicative effectiveness and efficiency should be the lowest in
the random data condition, because the distorted information should result in a biased mental model of the client’s
knowledge and this should impair the expert’s adaptation to
the client’s actual knowledge state to some degree.

Method
The assessment tool. The assessment tool provided the
computer experts both with ratings of the client’s general
computer knowledge and their Internet knowledge (see Figure 1). Apart from these global evaluations, it was also displayed to what extent the client already knew the meaning
of two specialist concepts semantically relevant to the understanding of the problem addressed by an inquiry. Thus,
the experts had the possibility to adapt their explanations
both to the client’s general knowledge background and, on a
more concrete level, to their prior knowledge regarding a
specific inquiry. The values displayed in the assessment tool
were determined through an objective and standardized assessment procedure. To this purpose, an updated version of
the computer and Internet knowledge test developed by
Richter et al. (2000) was constructed and pre-tested on 40
humanities students. In the experiment, the number of items
that a client had solved correctly in the general computer
knowledge subtest (10 items) and in the Internet knowledge
subtest (10 items) was translated into values on the corresponding five-point scales in the assessment tool (cf. Figure
1). For example, if a client had solved only one or two items
out of the ten items of the Internet knowledge subtest, this
was indicated as a low Internet knowledge level. In contrast,
if the client had solved nine or ten items of a subtest, this
would be represented in the assessment tool as a high
knowledge level. To assess the client’s knowledgeability
regarding the specialist concepts they were asked to describe the meaning of each of the concepts. Two raters independently scored the written descriptions for correctness
by using the five-point rating scale displayed in the assessment tool (see Figure 1). Inter-rater reliability was .92.
Participants. 60 computer experts and 60 clients participated in the experiment. Computer experts were recruited
among advanced students of computer science. The laypersons serving as clients were recruited among students of
psychology and the humanities. The results of the knowledge tests showed that the clients covered a wide range of

1466

different knowledge levels. In the general computer knowledge test, a mean of 5.33 correctly solved items was obtained with a standard deviation of 2.50 and a range of 10
items. In the Internet knowledge test, the clients were able
to solve 5.80 items on average with a standard deviation of
2.33 and a range of 8 items. Thus, there was ample opportunity for the experts to adapt their explanations to clients
with different prior knowledge levels.
Design. Computer experts and clients were combined into
dyads that were randomly assigned to the experimental conditions. A one-factorial between-subjects design was used
comprising three different conditions: (a) communication
with an assessment tool displaying valid information about
the client’s knowledge (in the following labeled ‘valid AT’),
(b) communication without assessment tool (‘no AT’), and
(c) communication with an assessment tool displaying random information about the client’s knowledge (‘random
AT’). Dependent variables encompassed measures of communicative effectiveness (i.e., the client’s increase in
knowledge) and communicative efficiency (i.e., the number
of questions asked by the client in response to an expert’s
explanation).
Materials. A pool of 20 inquiries was constructed that demanded explanations of relevant Internet topics and problems. Based on expert ratings regarding the familiarity and
relevance of the inquiries, six of them were selected for the
experiment. Three inquiries required the computer expert to
explain a technical concept. The other three were more
complex. They asked the expert to instruct the client how to
solve a problem and, additionally, to provide an explanation
why the problem occurred in order to help the client understand the nature of the problem (e.g., “I’m running Internet
Explorer 6. Whenever I try to print a website consisting of
several frames, my printer only prints out one frame. I
would like to understand why this happens and what I can
do so that the frames are printed out all at once?”).
Procedure. In the beginning of the experiment, the students
serving as clients were administered the general computer
knowledge test, the Internet knowledge test, and the concept
description task. In addition, their prior knowledge about
the six inquiries to be discussed in the communication phase
was determined. The students were encouraged to try to
answer each of the inquiries if possible. They were informed that they were participating in a study on students’
knowledge about computers and the Internet. Thus, it was
made certain that the students had no reason to assume that
their test results would later be relevant to the communication phase of the experiment. This was important because
otherwise the students’ self-perceptions of their test performance might have influenced their behavior during the
advisory exchange with the computer expert. In the communication phase, the expert and client sat in different
rooms and communicated through a text-based interface.
The client’s task was to sequentially direct each one of the

prepared six inquiries verbatim to the expert by typing the
prepared wording of the inquiry into the text form of the
interface. The expert was asked to answer each inquiry as
well as possible. The clients were encouraged to write back
and ask as many questions as needed. In the experimental
conditions with the assessment tool, the completed form
was visible to the expert during the entire course of the exchange. When the client asked a new inquiry, the assessment tool was automatically updated with regard to the client’s knowledge about the specialist concepts relevant to the
current inquiry (see Figure 1). After the communication
phase, the clients were again asked to write down their
knowledge about each of the six inquiries. In this way, it
was possible to calculate the individual increase in knowledge for each client (cf. Table 1).

Results
Before the client’s individual increase in knowledge was
computed, it was made sure that the clients had no substantial prior knowledge about the inquiries. The mean scores of
the clients’ answers collected before the communication
phase clearly ranged below one (4-point rating scale, cf.
Table 1) indicating that, on average, the clients did not
know the correct answer to the inquiries prior to the exchange with the computer expert. There were no differences
between the experimental conditions, F < 1.
Communicative effectiveness. In order to compute the
clients’ individual increase in knowledge, the mean scores
of the clients’ answers to the six inquiries prior to the communication phase were subtracted from the corresponding
mean scores after the communication phase (cf. Table 1).
The maximum score to be attained was three points. An
ANOVA performed on the individual difference scores revealed an overall effect of experimental condition, F(2, 57)
= 5.37, p < .01, η² = .16 (strong effect). Following the sensitization hypothesis, a substantial increase in knowledge
should be observed in the conditions with an assessment
tool but not in the condition without an assessment tool. The
validity of the displayed information should make no difference. This prediction was represented by the following contrast: valid data: 1, random data: 1, no assessment tool: –2.
Following the specific adaptation hypothesis, the information displayed by the assessment tool should indeed
make a difference: The client’s increase in knowledge
should be larger in the valid data condition compared with
the condition without the assessment tool and the random
data condition. The smallest knowledge increase would be
expected in the random data condition, because the distorted
information should impair the expert’s adaptation to the
client’s knowledge level. This linear trend hypothesis was
represented by the following contrast weights: valid data: 1,
no assessment tool: 0, random data: –1.
The results of the contrast analysis clearly contradicted
the sensitization hypothesis and supported the specific adaptation hypothesis. The planned contrast representing the
sensitization hypothesis failed to reach statistical signifi-

1467

cance, F < 1, whereas the contrast testing the specific adaptation hypothesis was highly significant, F(1, 57) = 9.99, p
< .01, η² = .15 (strong effect). Table 1 shows that the mean
values of the clients’ increase in knowledge evidently displayed the predicted linear trend with the largest increase in
knowledge occurring in the valid data condition and the
smallest in the random data condition.
Table 1: Means and standard deviations (in parentheses) of
the dependent variables of the experiment.

also when the analysis was restricted to the comprehension
questions, F(1, 57) = 9.76, p < .01, η² = .15 (strong effect).
With valid data in the assessment tool, the laypersons wrote
back only about half as often in response to an expert’s explanation as compared to the other experimental conditions
(cf. Table 1, last two rows). Thus, only the provision of
valid information reduced the frequency of questions by
which the client explicitly articulated a comprehension
problem or asked for further information. On the other hand,
most of the questions occurred in the condition that presented distorted information about the client’s knowledge.

Experimental Condition

Valid
AT

No
AT

Random
AT

0.46
(0.38)

0.68
(0.73)

0.58
(0.50)

1.97
(0.71)

1.66
(0.55)

1.37
(0.59)

1.52
(0.81)

0.99
(0.78)

0.80
(0.54)

Total number of questions
per expert-client exchange

2.15
(1.73)

4.35
(2.98)

4.70
(2.54)

Number of comprehension
questions per expert-client
exchange

1.75
(1.74)

3.80
(2.44)

3.85
(2.13)

Dependent Variable

Mean scores of the clients’
answers before the
communication phase*
Mean scores of the clients’
answers after the
communication phase*
Mean differences of the
clients’ increase in knowledge

Discussion

Note. *For each answer up to three points could be assigned (0 =
no or wrong answer, 1 = predominantly wrong answer, 2 = roughly
correct answer, 3 = completely correct answer).

Communicative efficiency. To obtain a measure of communicative efficiency, we counted the total number of questions the client produced in response to the expert’s explanations during the whole exchange, that is, throughout the
six inquiries. An ANOVA performed on the total number of
questions revealed a significant overall effect of experimental condition, F(2, 57) = 6.27, p < .01, η² = .18 (strong effect). When the analysis was restricted to the frequency of
comprehension questions, that is, to those questions by
which the client explicitly articulated a comprehension
problem, a similar result was obtained, F(2, 57) = 6.36, p <
.01, η² = .18 (strong effect). To test the sensitization hypothesis and the specific adaptation hypothesis, planned
contrasts were computed with the contrast weights reported
above. As before, the data analyses yielded no support for
the sensitization hypothesis, regardless of whether the total
number of questions or the number of comprehension questions was used as the dependent variable, F(2, 57) = 1.87,
ns, and F(2, 57) = 2.95, ns, respectively. On the other hand,
the specific adaptation hypothesis was also confirmed with
regard to communicative efficiency. The linear contrast was
significant when the total number of questions was considered, F(1, 57) = 10.67, p < .01, η² = .16 (strong effect), and

The dialogue experiment presented in this paper replicates
the results found in a previous study (Nückles & Stürz, in
press). The approach to support asynchronous communication between computer experts and laypersons by means of
an assessment tool has indeed proven to be successful. More
importantly, the present findings also allow for conclusions
about the mechanisms that led to the increase in communicative effectiveness and efficiency. The clients acquired the
most knowledge and asked the fewest questions when the
computer expert was presented valid data about the client’s
knowledge. When the information was distorted, the client’s
knowledge acquisition was impaired. The clients in the random data condition profited the least from the experts’ explanations and asked the most questions. These results
clearly contradicted the sensitization hypothesis and supported the specific adaptation hypothesis. Thus, it can be
concluded that it was in fact the individuating information
about the client’s knowledge that led to the increase in
communicative efficiency and effectiveness. From the perspective of Nickerson’s anchoring and adjustment model
(Nickerson, 1999) the assessment tool improved the communication between expert and client because the information about the client’s knowledge provided the computer
expert with a specific anchor right from the start of the
counselling process. This enabled the expert to calibrate
their mental model about the client’s knowledge more
quickly and accurately than would have been possible without such individuating information or with distorted information.
The present findings show that the assessment tool fostered specific adaptation to the clients’ knowledge. It is still
unclear how the experts exactly used the information about
the client to produce adaptive explanations. It is plausible to
assume that the experts used a ‘linear strategy’. For example, they might have reasoned that ‘the lower the client’s
knowledge level, the more extensive my explanations
should be’ in order to provide the client sufficient context
for comprehension (cf. Clark, 1996). Indeed, we found such
a correlation between the extensiveness of the experts’ explanations and the client’s displayed knowledge level (r = –
.32, p < .05). Still, the correlation was rather low. It cannot
help to fully understand the cognitive heuristics the computer experts used to adjust their explanations to the client’s
knowledge. Thus, beyond the tendency to link explanatory

1468

extensiveness to the client’s knowledge level, the experts
apparently used the information displayed by the assessment
tool to adjust their explanations in more sophisticated and
individualized ways. One possibility is that the experts referred to the information in the assessment tool to make
decisions during the planning phase of an explanation, for
example, whether a technical term they intended to use in
their answer would already be known by the client, or
would have to be introduced in case it was not known (socalled ‘pruning’, see Chin, 2000). To explore such possibilities, we are currently running a ‘think-aloud’ study in which
we are investigating how the experts developed a qualitative
representation of the client’s knowledge from the quantitative information provided by the assessment tool and how
this qualitative representation was used to generate instructional explanations for the client. In addition, ‘thinking
aloud’ protocols of the client’s comprehension processes
could help to identify features of the expert’s explanations
that hinder or enhance the clients’ understanding. The identification of features that make an expert’s explanation well
adapted to a specific knowledge level could be interesting
both for the design of advice-giving systems (e.g., Chin,
2000) and intelligent tutoring systems (e.g., Dede, 1986).
The finding that information about a client’s knowledge
fostered the provision of adaptive instructional explanations
might also be suggestive of ways in which Internet-based
collaborative settings other than helpdesk communication
could be supported. In the realm of distance learning, many
universities offer online courses where students of diverse
educational backgrounds and with a wide range of different
knowledge participate. As the tutors in these courses have to
provide instructional explanations for people they – at least
initially – do not know, an assessment tool could provide
valuable information that may help the tutors to adapt their
explanations to the learners’ knowledge level. Hence, an
assessment tool might also be an appropriate method to
support online tutoring (Siler & VanLehn, in press).

Acknowledgments
This project is funded by the German Research Foundation
(DFG; NU 129/1-1).

References
Alty, J. L., & Coombs, M. J. (1981). Communicating with
university computer users: A case study. In M. J. Coombs
& J. L. Alty (Eds.), Computing skills and the user interface (pp. 7-71). London: Academic Press.
Chi, M., Glaser, R., & Farr, M. (Eds.) (1988). The Nature of
Expertise. Hillsdale, NJ: Erlbaum.
Chin, D. N. (2000). Strategies for Expressing Concise,
Helpful Answers. Artificial Intelligence Review, 14 (4-5),
333-350.
Clark, H. H. (1996). Using language. Cambridge: University Press.
Clark, H. H., & Brennan, S. E. (1991). Grounding in communication. In L. B. Resnick, J. M. Levine, & S. D.
Teasley (Eds.), Perspectives on socially shared cognition

(pp. 127-149). Washington, DC: American Psychological
Association.
Clark, H. H., & Murphy, G. L. (1982). Audience design in
meaning and reference. In J. F. LeNy & W. Kintsch
(Eds.), Language and comprehension (pp. 287-299). Amsterdam: North-Holland Publishing Company.
Dede, C. (1986). A Review and Synthesis of Recent Research in Intelligent ComputerAssisted Instruction. International Journal of Man-Machine Studies, 24, 329-353.
Gunawardena, C. N. (1995). Social Presence Theory and
Implications for Interaction and Collaborative Learning in
Computer Conferences. International Journal of Educational Telecommunications, 1 (2/3), 147-166.
Hinds, P. J. (1999). The curse of expertise: The effects of
expertise and debiasing methods on predictions of novice
performance. Journal of Experimental Psychology: Applied, 5 (2), 205-221.
Kiesler, S., Zdaniuk, B., Lundmark, V., & Kraut, R. (2000).
Troubles with the Internet: The dynamics of help at home.
Human-Computer Interaction, 15, 323-351.
Moncarz, R. (2001). Computer support specialists. Occupational Outlook Quarterly, 45, 16-19.
Nickerson, R. S. (1999). How we know - and sometimes
misjudge - what others know: Imputing one’s own
knowledge to others. Psychological Bulletin, 125 (6),
737-759.
Nückles, M., & Bromme, R. (2002). Internet experts’ planning of explanations for laypersons: A Web experimental
approach in the Internet domain. Experimental Psychology, 49 (4), 292-304.
Nückles, M., & Stürz, A. (in press). The assessment tool. A
method to support asynchronous communication between
computer experts and laypersons. Computers in Human
Behavior.
Nückles, M., Wittwer, J., & Renkl, A. (2003). Supporting
computer experts’ adaptation to the client’s knowledge in
asynchronous communication: The assessment tool. In F.
Schmalhofer, R. Young, & G. Katz (Eds.), Proceedings of
the European Conference of the Cognitive Science Society
2003 (pp. 247-252). Mahwah, NJ: Erlbaum.
Renkl, A. (2002). Learning from worked-out examples:
Instructional explanations support self-explanations.
Learning & Instruction, 12, 529-556.
Richter, T., Naumann, J., & Groeben, N. (2000). Attitudes
toward the computer: Construct validation of an instrument with scales differentiated by content. Computers in
Human Behavior, 16, 473-491.
Siler, S. A., & VanLehn, K. (in press). Accuracy of Tutors’
Assessments of their Students by Tutoring Context. In R.
Alterman & D. Kirsch (Eds.), Proceedings of the 25th
Annual Conference of the Cognitive Science Society.
Mahwah, NJ: Erlbaum.
Tversky, A., & Kahneman, D. (1974). Judgement under
uncertainty: heuristics and biases. Science, 185, 11241131.
Vu, K. L., Hanley, G. L., Strybel, T. Z., & Proctor, R. W.
(2000). Metacognitive processes in human-computer interaction: Self-assessments of knowledge as predictors of
computer expertise. International Journal of HumanComputer Interaction, 12, 43-71.

1469

