UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Metacognitive Judgments, Study-Time Allocation and Inferences: The Effect of Multimedia
Discrepancies

Permalink
https://escholarship.org/uc/item/2r32v5b8

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)

Authors
Burkett, Candice
Azevedo, Roger

Publication Date
2011-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Metacognitive Judgments, Study-Time Allocation and Inferences: The Effect of
Multimedia Discrepancies
Candice Burkett (cburkett@memphis.edu)
Department of Psychology, Institute for Intelligent Systems, University of Memphis
400 Innovation Drive, Memphis, TN 38152 USA

Roger Azevedo (roger.azevedo@mcgill.ca)
Department of Educational and Counselling Psychology,
Laboratory for the Study of Metacognition and Advanced Learning Technologies, McGill University
3700 McTavish, Montreal, QC H3A 1Y2 Canada

Abstract
This
study
investigated
undergraduate
students’
metacognitive judgments while learning about complex
science topics using multimedia material (text and graph). A
within-subjects design was used to examine the effect of
discrepancies on study-time allocation, metacognative
judgments and inference generation. There were three types
of discrepancies: none, text (between two ideas in the text)
and text and graph (between the text and graph). Forty
(N=40) participants completed 12 trials where they were
asked to provide 6 judgments: Ease of Learning judgments
(EOLs), immediate and delayed Judgments of Learning
(JOLs) for both text and graph and Retrospective
Confidence Judgments (RCJs). Participants provided
significantly lower JOLs for content that contained
discrepancies but RCJs remained high across conditions.
Discrepancies did not influence study-time allocation, but
did significantly influence inference scores. Overall, results
suggest that participants may be aware of discrepancies, but
lack the control strategies needed to overcome them.
Keywords: metacognitive judgments; self-regulated learning;
metacognitive monitoring; control strategies; discrepancy
detection; multimedia

Introduction
In comparison to other developed countries, the United
States has been consistently outperformed in science
international assessments (PISA, 2006; TIMSS, 2007). With
projected university undergraduate enrollment in the United
States on the rise (Planty et al., 2009), it is important that
students begin to master the skills necessary to succeed in
school at an early age. It is, therefore, becoming
increasingly important that students become proficient in the
most effective ways to increase, maintain, and demonstrate
knowledge in a variety of difficult domains including
Biology, Physics, Chemistry, and Physical Science. One
way for students to become more proficient in these
domains is to effectively use certain metacognitive
processes in order to better self-regulate their learning.
Research has shown that the key to successful regulation
of learning depends on the use of certain metacognitive
processes related to planning (e.g., activation of prior

knowledge), monitoring (e.g., judgments of learning), and
the selection of appropriate learning strategies (e.g.,
coordination of informational sources) (Azevedo &
Witherspoon, 2009; Azevedo, Witherspoon, Chauncey, &
Burkett, 2010; Winne & Hadwin, 2008; Zimmerman &
Schunk, 2011). Research has further indicated that
metacognitive monitoring and judgments are extremely
important in determining students’ metacognitive control
strategies such as study time allocation. Specifically, it is
important that students are able to accurately judge to what
degree they understand the information they are studying in
order to effectively control their learning (Dunlosky,
Hertzog, Kennedy, & Thiede, 2005; Dunlosky & Metcalfe,
2009; Metcalfe, 2009; Metcalfe & Finn, 2008).

Metacognitive Monitoring and Control
Metacognitive monitoring research traditionally emphasizes
the importance of making judgments about one’s own
learning. This field of research often focuses on three types
of judgments: Ease of Learning judgments (EOLs),
Judgments of Learning (JOLs) and Retrospective
Confidence Judgments (RCJs).
Ease of Learning (EOL) judgments call for learners to
evaluate how easy or difficult information will be to learn.
These judgments typically take place prior to study or
learning. EOLs can be used as a measure of predicted task
difficulty and are associated with the planning phase of selfregulated learning (e.g., determining how much time to
allocate to studying) (Pintrich, 2000). Furthermore, there
has been evidence to indicate that EOLs are related to
metacognitive control, specifically to study choice (Thiede,
Anderson & Therriault, 2003).
Judgments of Learning (JOLs) are one of the most
frequently studied metacognitive judgments in current
literature. These judgments are made by asking participants
to rate their level of understanding about a specific set of
material. JOLs can be made at any point during learning, but
are often made at strategic points (e.g., after reading a
specific amount of material or at the end of a learning
session). Typically, research in this field has been geared
toward understanding the processes behind these judgments,
methods for improving the judgment accuracy, and the
relationship between these judgments and the control of

1013

learning processes. For example, in an attempt to determine
a better method to increase judgment accuracy, Nelson and
Dunlosky (1991) introduced the idea of delayed JOLs.
Results from this study indicate that the relative accuracy
was considerably higher for delayed JOLs and that this
increase could be achieved within a relatively small amount
of time (30 seconds) between the stimuli presentation and
subsequent judgment. The results of this study have been
replicated numerous times in a variety of settings with
different groups of participants and, therefore, have become
increasingly well known (see Dunlosky & Metcalfe, 2009).
Another judgment frequently studied in metacognitive
research is the Retrospective Confidence Judgment (RCJ).
When asked to provide a RCJ, participants are typically
asked how confident they are that they answered items
correctly on a learning measure, usually a test. As with other
monitoring processes, learners’ judgments are often
inaccurate when compared with their level of performance
(Dunlosky & Metcalfe, 2009).
The extensive literature base that supports the strong
relationship between metacognitive monitoring and control
has begun to answer some questions about self-regulated
learning (Azevedo, Moos, Johnson, & Chauncey, 2010;
Winne & Hadwin, 2008). Specifically, in an attempt to
further explain the relationship between the two processes,
Metcalfe and Kornell (2005) have proposed the region of
proximal learning framework (Metcalfe, 2009). This
framework suggests that, when studying material of varying
difficulty, learners’ perseverance is based on their perceived
rate of learning. According to this model, when learners
perceive that their rate of learning has reached zero
(indicating that they are no longer actively learning) they
will cease their study of the material. Therefore, according
to this framework, learners will spend the most time
studying items with judgments indicating a moderate level
of difficulty.

Inferences and Comprehension Regulation
Frequently, learners make an inference by combining
information that is present in learning materials with
information that is found elsewhere in the text or with their
prior knowledge in order to better understand the target
concept. Inference generation can often be particularly
difficult when reading texts for which the learner has little
prior knowledge (Graesser et al., 2007). Furthermore,
combining information can be particularly difficult when the
information is contradictory (Otero & Kintsch, 1992).
Research
geared
toward
understanding
learners’
comprehension regulation has often utilized texts containing
such contradictions or discrepancies. This line of research
indicates that learners’ are often unable to detect even
simple discrepancies in text and are frequently unsure of
how to deal with the discrepancies when they do detect
them (Britton & Eisenhart, 1993; Otero & Campanario,
1990; Otero & Kintsch, 1992).

Current Study
The current study focused on multimedia content delivered
through multiple representations (i.e., text and graph), and
utilized a within-subjects design to examine how type of
discrepancy (no discrepancy, text discrepancy, and text and
graph discrepancy) affects metacognitive judgments,
allocation of study-time, and the generation of inferences.
For this study, a text discrepancy is defined as discrepant
information about a particular concept addressed in two
separate sentences in a single text about a particular topic
(e.g., Microorganisms). A text and graph discrepancy, on
the other hand, is defined as discrepant information about a
particular concept addressed both in the text and the graph
about a particular topic (e.g., Transpiration). Each
discrepancy, whether in the text only or between the text
and the graph, was related to the content related to the
inference question for that topic.

Method
Participants
Forty (N=40) undergraduate students from a public
university in the mid-south region of the United States took
part in this study and were paid $20 for their participation.

Materials
The researchers developed 12 content slides about 12
different science topics (one topic per slide) across four
science domains (Chemistry, Physics, Biology and Physical
Science). Each slide contained text and a corresponding
graph that illustrated a particular concept discussed in the
text. For each slide, the researchers included content from
college text books from four science domains (Chemistry,
Physics, Biology and Physical Science) (Getis, Getis, &
Fellmann, 2009; Halliday, Resnick, & Walker, 2008;
Hoefnagels, 2009; Masterson & Hurley, 2006; Tillery,
2007).
For text containing no discrepancies or a discrepancy
between the text and graph, the text was taken directly from
the textbook with no modifications. For text containing a
discrepancy in the text, the text was altered slightly in order
to include discrepant material. In many cases, the original
text was only altered by changing one word in a sentence to
make information found within the text contradictory. Each
graph that accompanied the text was created by the
researchers in order to ensure that all graphs had consistent
features (e.g., font, line color and background color). Slides
were counterbalanced and equally divided among the three
discrepancy types. The content was presented using the
computer-based Automated Testing System (ATS)
(Lehman, D’Mello, & Person, 2008) (See Figure 1).

Experimental Procedure
The experimental procedure for the study included 3 phases:
1) collection (participants filled out informed consent and
demographic information), 2) testing (participants were
given two tests to assess different aspects of their prior

1014

knowledge) and 3) experimental session in which the
participants provided several metacognitive judgments (i.e.,
EOL, JOL and RCJ), read text and inspected graphs, and
provided answers to 12 science inference questions.
The participants were given 20 minutes total to complete
both a test of science knowledge basic graph comprehension
test (10 minutes per test). To begin the experimental session,
participants were shown a video that provided experimental
instructions as well as demonstrated how to navigate the
ATS system. The session consisted of 12 trials, for each of
which participants were presented with an open-ended
inference question based on the specific content they were
about to view. Participants were asked to make an Ease of
Learning (EOL) judgment by selecting the appropriate
multiple-choice option in the ATS system that corresponded
to a 0-100% scale that increased in increments of 20 percent
(i.e., 0%, 20%, 40%, 60%, 80%, 100%). A judgment of 0%
indicated that participants predicted the question would be
very difficult to answer, whereas a judgment of 100%
indicated that participants predicted the question would be
very easy to answer.
Participants were then presented with a content slide
about a single topic that contained three paragraphs of
textual material and a corresponding graph. For each topic
(e.g., Atoms) all three paragraphs and graph were presented
simultaneously. Once participants read the paragraphs and
inspected the diagram, they were asked to make an
immediate Judgment of Learning about textual material (IT
JOL) using the ATS system as previously described. For
this and for all subsequent JOLs, a response of 0% indicated
that the participant judged they did not understand the
material at all, whereas a response of 100% indicated a
judgment that they completely understood the material.
Participants were also asked to make an immediate
Judgment of Learning about the graph that was presented
with the text (IG JOL) using the ATS system as previously
described. Once participants provided their immediate
judgments, the system then displayed a screen with an
image of a stop sign for 30 seconds. After the delay,
participants were asked to make delayed judgments about
both the text (DT JOL) and graph (DG JOL) that was
previously presented. These judgments followed the same
format as the judgments of learning described above.
At the end of each of the 12 trials, participants were also
asked to provide a response to the inference question
presented initially and were given as much time as needed
to respond to the question. They were then asked to make a
Retrospective Confidence Judgment (RCJ) by selecting the
appropriate multiple-choice option in the ATS system that
corresponded to a 50-100 percentage scale increasing in
increments of 10 percent. A judgment of 50% confidence
indicated that a participant simply guessed at the answer
(indicating that participants believed they have a 50/50 shot
at getting their answer correct), whereas a judgment of
100% indicated that a participant was completely confident
in their response. This procedure was repeated for the
remaining number of slides that were presented (a total of

12 trials). Following the completion of the experimental
session, the participants were debriefed and paid $20 for
their participation in the study.

Scoring Responses
Open-ended responses were scored using a thematic coding
method in which the researchers indentified specific units of
analysis found within target material (Jackson & Trochim,
2002; Ryan & Bernard, 2000). Each participant’s responses
to the 12 inference questions were graded by calculating a
percentage of the identified units of analyses that were
present in their response. The primary coder scored all 480
answers and a secondary coder scored a random selection of
25% (i.e., 120) responses. The inter-rater reliability for the
raters was found to be substantial at Kappa=0.77 (p<0.001).
Any disagreements between the researchers were settled
through discussion.

Results
Metacognitive Judgments
In order to determine if there were differences among
participants metacognitive judgments a series of one-way
repeated measures ANOVAs were conducted on each of the
recorded judgments for each type of discrepancy (none, text,
and text and graph). Results indicated that participants’ EOL
judgments were not significantly different F (1.60, 38) =
0.07, p = 0.89 (See Table 1). Results further indicated that
participants’ RCJ judgments were not significantly different
F (2, 38) = 2.50, p = 0.09
Table 1. Means, standard deviations and ANOVA results for
six metacognitive judgments, by condition.
None

Text

Graph

ANOVA

M(SD)

M(SD)

M(SD)

F

η2

EOL

0.68
(0.18)

0.67
(0.20)

0.67
(0.19)

0.07

0.01

IT
JOL

0.76
(0.14)

0.67
(0.19)

0.71
(0.17)

10.58**

0.33

IG
JOL

0.72
(0.16)

0.66
(0.20)

0.63
(0.19)

9.86**

0.35

DT
JOL

0.75
(0.14)

0.68
(0.19)

0.72
(0.17)

6.37**

0.20

DG
JOL

0.70
(0.17)

0.66
(0.21)

0.62
(0.19)

8.07**

0.29

RCJ

0.86
(0.08)

0.84
(0.10)

0.85
(0.08)

2.50

0.13

**p < .01

1015

Mean Judgment of Learning (JOL)

80

Mean Judgment of Learning (JOL)

With regard to Judgments of Learning, results indicated
participants’ immediate text JOLs were significantly
different F (2, 38) = 10.58, p < .001, η2=0.33. A Bonferroni
pairwise comparison revealed immediate text JOLs were
significantly higher for content that contained no
discrepancies (M=0.76, SD=0.14) than either text (M=0.67,
SD=0.19) or text and graph discrepancies (M=0.71,
SD=0.17) (See Table 1 and Figure 1).
Results indicated that participants’ immediate graph JOLs
were also significantly different F (2, 38) = 9.86, p < .001,
η2=0.35. A Bonferroni pairwise comparison revealed that
immediate graph JOLs were significantly higher for content
that contained no discrepancies (M=0.72, SD=0.16) than
either text (M=0.66, SD=0.20) or text and graph
discrepancies (M=0.63, SD =0.19) (See Table 1 and Figure
1).

70
65
60
None

Text

Text and Graph

Type of Discrepancy

Figure 1. Mean immediate text and graph JOL judgments by
type of discrepancy (none, text and text and graph).
Judgments were made on a 0-100 percentage scale.
Additionally, results indicated that participants’ delayed
text JOLs were significantly different F (2, 38) = 6.37, p <
.01, η2=0.20. A Bonferroni pairwise comparison revealed
delayed text JOLs were significantly higher for content that
contained no discrepancies (M=0.75, SD=0.14) than text
discrepancies (M=0.68, SD =0.19). Delayed text JOLs were
not significantly different for the no discrepancy condition
and the text and graph condition (M =0.72, SD=0.17) (See
Table 1 and Figure 2). Delayed text JOLs were also not
significantly different between the text discrepancy content
and the text and graph discrepancy content.
Results further indicated that participants’ delayed graph
JOLs were significantly different F (2, 38) = 8.07, p < .001,
η2=0.29. A Bonferroni pairwise comparison revealed
delayed graph JOLs were significantly higher for content
that contained no discrepancies (M=0.70, SD=0.17) than
text and graph discrepancies (M=0.62, SD =0.19). Results
indicate no significant difference for delayed graph JOLs
between the no discrepancy content and the text discrepancy
content (M=0.66, SD=0.21) (See Table 1 and Figure 2).

Text JOL
Graph JOL

75
70
65
60
None

Text

Text and Graph

Type of Discrepancy

Figure 2. Mean delayed text and graph JOL judgments by
type of discrepancy (none, text and text and graph).
Judgments were made on a 0-100 percentage scale.

Text JOL
Graph JOL

75

80

Overall, the results indicate that neither EOL judgments
nor RCJ judgments were affected by discrepancy type.
However, participant JOLS (i.e., immediate text and graph
and delayed text and graph JOLs) were affected by
discrepancy type. Both immediate JOLs (text and graph)
were significantly higher for material containing no
discrepancies than for material containing either type of
discrepancy (text or text and graph). However, results were
different for delayed JOLs. Delayed text JOLs were
significantly higher for material that contained no
discrepancy than material that contained discrepancies in the
text only. They were not, however, significantly different
when compared to either material with no discrepancies or
discrepancies between the text and the graph. The opposite
was found for delayed graph JOLs. Delayed graph JOLs
were significantly higher for material that contained no
discrepancy than material that contained discrepancies
between the text and graph. However, delayed graph JOLs
were not significantly different when compared to either
material with no discrepancies or discrepancies only in the
text.

Study-Time Allocation
In order to determine if type of discrepancy (none, text, and
text and graph) affected participants’ study-time during
multimedia learning a one-way repeated measures ANOVA
was conducted. Results indicated that there was no
significant difference, F (2, 38) = 0.45, p = 0.64, among
participants’ study time for material with no discrepancy
(M=128.34, SD=63.76), text discrepancy (M=129.08,
SD=53.61), or text and graph discrepancy (M=123.29,
SD=50.90).

Responses to Inference Questions
In order to determine if type of discrepancy (none, text, and
text and graph) impacted participants’ answers to the
inference questions a one-way repeated measures ANOVA
was conducted. Results indicated that there was a significant

1016

Mean Response Score

difference among participants’ response scores F (2, 38) =
4.11, p < .05, η2=0.10. A Bonferroni pairwise comparison
revealed response scores were significantly higher for
content that contained no discrepancies (M=0.46, SD=0.20)
than text discrepancies (M=0.37, SD =0.23). Response
scores were not significantly different between the no
discrepancy content and the text and graph content (M=0.41,
SD=0.19). Response scores were also not significantly
different between the text discrepancy content and the text
and graph discrepancy content (See Figure 3).
50
40
30
20
10
0
None

Text

Text and
Graph

Type of Discrepancy
Figure 3. Mean response score by type of discrepancy
(none, text and text and graph). Participant responses were
scored on a 0-100 percentage scale.
Overall, these results suggest that when there is any type
of discrepancy in the multimedia content participants
immediately judge their understanding to be lower, but seem
to have difficulties judging what specific aspect (i.e., text or
graph) of the content is related to their lack of
understanding. However, after a delay, participants seem to
have made a judgment about what aspect of the content they
believe is responsible for their understanding deficit.
However, despite discrepancies in the material and
participants’ difference in judgments, there was no
significant difference in their allocation of study-time. This,
combined with low overall scores on inference responses
(M=0.41) indicates that participants may lack the prior
knowledge or the control strategies (in this case allocation
of study-time) to overcome discrepancies in the material.
Furthermore, because discrepancies in the text (as opposed
to discrepancies between the text and the graph) have a
stronger influence on participants’ generation of inferences
results indicate that participants’ have more difficulty
overcoming discrepancies found within the text.

Conclusion
This study sought to explore the relationships between
metacognitive monitoring and control during multimedia
learning. This research has generated numerous unanswered
questions about the accuracy of metacognitive judgments
during multimedia learning and how they are related to
study-time allocation and accuracy to inference questions.
Most of the literature has examined metacognitive

judgments in word-pair recall tasks. This study extends
current research by examining several metacognitive
judgments, using multimedia science materials, and also
including both immediate and delayed JOLs for both text
and graphs, all within one experimental session. Overall, the
results of this study have far reaching implications for both
existing models of metacognition and metacomprehension
(e.g., Metcalfe, 2009) and theories of multimedia learning
(e.g., Mayer, 2009). Specifically, this study has begun to
examine a neglected area of metacognitive judgments with
complex multimedia science materials.
Some results of the current study, however, require
further investigation. Specifically, future studies should
investigate whether this phenomenon is primarily due to a
lack of prior knowledge, a lack of control strategies or a
combination of these factors by including a measure of
strategy monitoring. The use of on-line trace methodologies
such as concurrent think-alouds could potentially provide
additional evidence regarding the role and nature of the
underlying cognitive and metacognitive processes (e.g.,
Azevedo et al., 2010). In addition, future studies should take
into account the impact of participants’ epistemological
beliefs (i.e., benefit of texts and diagrams to learning) and
should investigate effects on other control strategies in
addition to study-time allocation.
Our current and future research focuses on using multimethod approaches, using eye-tracking and other on-line
trace methodologies and learning outcomes (e.g., Azevedo
et al., 2011). In particular, we will examine how gaze
behaviors, fixations, and regressions, are related to and
indicative of the role of attention, number of fixations on
relevant and irrelevant parts of the text and diagrams and
discrepancies, and how discrepancies are resolved based on
gaze behavior.

Acknowledgments
This study was supported by funding from the National
Science Foundation awarded to the second author.

References
Azevedo, R., &Witherspoon, A. M. (2009). Self-regulated
use of hypermedia. In D. J. Hacker, J. Dunlosky, & A. C.
Graesser (Eds.), Handbook of metacognition in education
(pp. 319–339). Mahwah, NJ: Erlbaum.
Azevedo, R., Johnson, A., & Chauncey, A. & Graesser, A.
(2011). Use of hypermedia to convey and assess selfregulated learning. In B. Zimmerman & D. Schunk (Eds.),
Handbook of self-regulation of learning and performance
(pp. 102-121). New York: Routledge.
Azevedo, R., Moos, D., Johnson, A. M., & Chauncey, A.
(2010). Measuring cognitive and metacognitive processes
during hypermedia learning: Issues and challenges.
Educational Psychologist, 45(4), 210-223.
Azevedo, R., Witherspoon, A., Chauncey, A., & Burkett, C.
(2010). Self-regulated learning with MetaTutor:

1017

Advancing the science of learning with metacognitive
tools. In M.S. Khine & I.M. Saleh (Eds.) New science of
learning: computers, cognition, and collaboration in
education (pp. 252-247). Amsterdam: Springer.
Britton, B. K., & Eisenhart, F. J., (1993). Expertise, text
coherence and constraint satisfaction: Effects on harmony
and setting rate. Proceedings of the Fifteenth Annual
Conference of the Cognitive Science Society (pp. 266271). Hillsdale, NJ: Earlbaum.
Dunlosky, J., Hertzog, C., Kennedy, M., & Thiede, K.
(2005). The self-monitoring approach for effective
learning. Cognitive Technology, 10, 4-11.
Dunlosky, J., & Metcalfe, J. (2009). Metacognition: A
textbook for cognitive, educational, life span and applied
psychology. Newbury Park, CA: Sage.
Getis, A. Getis, J., & Fellman, J. (2009). Introduction to
Geography. Boston, MA: McGraw Hill.
Gonzales, P., Williams, T., Jocelyn, L., Roey, S., Kastberg,
D., & Brenwald, S. (2008). Highlights from TIMSS
2007:Mathematics and science achievement of U.S.
fourth- and eighth-grade students in an international
context (NCES 2009–001Revised). National Center for
Education Statistics, Institute of Education Sciences, U.S.
Department of Education. Washington, DC.
Graesser, A., Louwerse, M., McNamara, D.S., Olney, A.,
Cai, Z., & Mitchell, H. (2007). Inference generation and
cohesion in the construction of situation models: Some
connections with computational linguistics. In F.
Schmalhofer & C.A. Perfetti (Eds.), Higher level
language processes in the brain: Inference and
comprehension processes (pp. 289-310). Mahwah, NJ:
Erlbaum.
Halliday, D., Resnick, R., & Walker, J. (2008).
Fundamentals of Physics. US: Wiley.
Hoefnagels, M. (2009). Biology: Concepts and
Investigations. US: McGraw Hill.
Jackson, K., & Trochim, W. (2002). Concept mapping as an
alternative approach for the analysis of open-ended
survey responses. Organizational Research Methods,
5(4), 307-336.
Lehman, B., D’Mello, S.K., & Person, N. (2008). All alone
with your emotions: An analysis of student emotions
during effortful problem solving activities. Workshop on
emotional and cognitive issues in ITS held in conjunction
with ninth international conference on intelligent tutoring
systems, Montreal, Canada.
Masterson, W., & Hurley, C. (2008). Chemistry: Principles
and reactions. US: Wadsworth.
Metcalfe, J. (2009). Metacognitive judgments and control of
study. Current Directions in Psychological Science, 18,
159-163.
Metcalfe, J., & Finn, B. (2008). Familiarity and retrieval
processes in delayed judgments of learning. Journal of
Experimental Psychology: Learning, Memory, and
Cognition, 34, 1084-1097.

Metcalfe, J., & Kornell, N. (2005). A region of proximal
learning model of study time allocation. Journal of
Memory and Language. 52, 463-477.
Nelson, T. O., & Dunlosky, J. (1991). When people’s
judgments of learning (JOLs) are extremely accurate at
predicting subsequent recall: The “delayed-JOL effect.”
Psychological Science, 2, 267-270.
Otero, J. C., & Companario, J. M., (1990). Comprehension
evaluation and regulation on learning from science texts.
Journal of Research in Science Teaching, 27, 447-460.
Otero, J., & Kintsch, W. (1992). Failures to detect
contradictions in a text: What readers believe versus what
they read. Psychological Science, 3, 229-235.
Pintrich, P. (2000). The role of goal orientation in selfregulated learning. In M. Boekaerts, P. Pintrich, & M.
Zeidner (Eds.), Handbook of self-regulation (pp. 451502). San Diego, CA: Academic Press.
Planty, M., Hussar, W., Snyder, T., Kena, G.,
KewalRamani, A., Kemp, J., Bianco, K., Dinkes, R.
(2009). The condition of education 2009 (NCES 2009081). National Center for Education Statistics, Institute of
Education Sciences, U.S. Department of Education.
Washington, DC.
Programme for International Student Assessment (PISA).
(2006). Oraganisation for economic co-operation and
development, PISA 2006 Technical Report, OECD, Paris.
Ryan, G.W., & Bernard, H.R. (2000). Data management and
analysis methods. In N. Denzin & Y. Lincoln (Eds.),
Handbook of qualitative research (2nd ed.) (pp. 769-802).
Thousand Oaks, CA: Sage.
Thiede, K. W., Anderson, M.C.M., & Therriault, D. (2003).
Accuracy of metacognitive monitoring affects learning of
texts. Journal of Educational Psychology, 95, 66-73.
Tillery, B. (2007). Physical science. US: McGraw Hill.
Trends in International Mathematics and Science Study
(TIMSS). (2003). Released set eighth grade science items.
International Association for the Evaluation of
Educational Achievement (IEA).
Winne, P. & Hadwin, A. (2008). The weave of motivation
and self-regulated learning. In D. Schunk & B.
Zimmerman (Eds.), Motivation and self-regulated
learning: Theory, research and applications (pp. 297314).
Zimmerman, B., & Schunk, D. (2011). Handbook of selfregulation of learning and performance. New York:
Routledge.

1018

