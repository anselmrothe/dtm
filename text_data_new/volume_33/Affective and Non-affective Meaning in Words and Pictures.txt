UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Affective and Non-affective Meaning in Words and Pictures

Permalink
https://escholarship.org/uc/item/8ss0g8x7

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)

Authors
Lai, Vicky Tzuyin
Hagoort, Peter
Casasanto, Daniel

Publication Date
2011-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Affective and Non-affective Meaning in Words and Pictures
Vicky Tzuyin Lai1
(vicky.lai@mpi.nl)

Peter Hagoort1,2
(peter.hagoort@mpi.nl)

Daniel Casasanto1,2,3
(casasanto@alum.mit.edu)

1

Max Planck Institute for Psycholinguistics, Nijmegen, NL
Donders Center for Brain, Cognition, and Behaviour, Nijmegen, NL
3
Department of Psychology, The New School for Social Research, New York, USA
2

affective information associated with the animals they see
(e.g., safe or dangerous) might be very salient. If a person is
taking a timed test for a biology class, however, and
classifying animals according to their ontological categories,
they might not even notice if some animals are more
dangerous than the others.
We propose a context-dependent view of affective and
non-affective information processing. Rather than arguing
for the primacy one type of information over the other, we
suggest that the relative speed with which affective and nonaffective information gets activated in response to words
and pictures should depend upon the contexts in which the
stimuli are processed.

Abstract
When people see a snake, they are likely to activate both
affective information (e.g., dangerous) and non-affective
information (e.g., animal). According to the Affective
Primacy Hypothesis, the affective information has priority,
and its activation can precede identification of the ontological
category of a stimulus. Alternatively, according to the
Cognitive Primacy Hypothesis, perceivers must know what
they are looking at before they can make an affective
judgment about it. We propose that neither hypothesis holds
at all times. In two experiments, we show that the relative
speed with which affective and non-affective information gets
activated by words and pictures depends upon the contexts in
which the stimuli are processed. These data support a view
according to which words and pictures do not “have”
meanings; rather, they are cues to activate patterns of stored
knowledge, the specifics of which are co-determined by the
item itself and the context in which it occurs.

Affective Primacy
Three lines of empirical evidence have been used to
support the Affective Primacy Hypothesis, namely,
subliminal priming (Murphy & Zajonc, 1993), affective
priming (Klauer & Musch, 2003), and neuropsychological
evidence (LeDoux, 1996). However, as can be seen in the
following, these data seem to support an affective-early
theory, as opposed to an affect-precedes-non-affect theory.
Subliminal priming studies in general show that briefly
presented affect-laden primes (e.g., smiling and angry faces)
can influence the affective evaluation of the subsequent
unseen targets (e.g., Chinese ideographs). In contrast,
briefly presented non-affective primes (e.g., big and small
geometric shapes) cannot influence the non-affective
processing of the unseen targets (e.g., the size of the object
the ideographs might represent). In the latter case, though, if
the presentation duration is adjusted to allow for optimal
viewing, the classic semantic priming (e.g., doctor-nurse)
effects emerge. It is argued that affect can be activated first
with minimal exposure, prior to the activation of nonaffective information.
Affective priming refers to the phenomenon that
positively- or negatively- valenced targets (e.g., sunshine)
can be primed when preceded by primes with congruent
valence (e.g., love) compared to primes with incongruent
valence (e.g., death). Affective priming occurs even when
attention is focused on another, concurrent verbal task
(Calvo & Nummenmaa, 2007), suggesting the automaticity
of affective evaluation (Fazio, 2001; Bargh, Chen, &
Burrows, 1996). These data support the part of the claim in
the Affective Primacy Hypothesis that affect can be elicited
with virtually no non-affective processing.

Keywords: ad hoc cognition; affective primacy; cognitive
primacy; affective priming; context; emotion; task set inertia.

Introduction
When people see a snake, they are likely to activate both
affective information (e.g., snakes are dangerous) and nonaffective information (e.g., snakes are animals). Which kind
of information is activated first? For decades, researchers
have debated the temporal priority of affective and nonaffective processing. According to the Affective Primacy
Hypothesis (Zajonc, 1980, 2000; Murphy & Zajonc, 1993;
LeDoux, 1996), information relevant for affective responses
can be activated quickly and automatically, before
information about ontological kinds. Alternatively, the
Cognitive Primacy Hypothesis (Lazarus, 1984; Storbeck,
Robinson, & McCourt, 2006; Calvo & Nummenmaa, 2007)
posits that perceivers must know what they are looking at
before they can make an affective judgment about it.
The present study investigated whether one kind of
information is activated faster than the other, in general, or
whether the speed with which affective and non-affective1
information gets activated varies with context. Imagine the
following scenarios: If a person is hiking around in a
tropical jungle and is constantly reminded of possible
poisonous animals and plants in the surroundings, the
1

We use “non-affective” instead of “cognitive” to allow the
possibility that both affective and non-affective meaning be
considered aspects of cognitive processing.

390

Neuropsychological data also upport the immediateness
of affective evaluation (LeDoux, 1996; Whalen, Rauch,
Etcoff, McInerney, Lee, & Jenike, 1998). It has been shown
that when it comes to processing emotional stimuli, a neural
system that learns the emotional significance of these
stimuli is activated. This system is a shortcut for ensuring
fast reaction to potentially life-threatening stimuli. The
shortcut can by-pass the neural system that allows us to
identify objects, people, events, etc.
These data indeed point to a fast and early processing of
affect. However, it is not clear if these data strongly support
the temporal primacy of affective information. For example,
the null results for non-affective dimension in the subliminal
priming literature do not rule out the possibility that such
information (e.g., size) can be elicited early in
circumstances where the information is made salient (e.g.,
finding the right size lego for building something in a lego
contest). Even if one non-affective dimension (e.g., size) is
indeed activated late, other non-affective dimensions (e.g.,
color) may not be perceived late. Moreover, in those
subliminal priming experiments, the affective and the nonaffective processing are not put directly in competition with
each other. As for evidence coming from the affective
priming effect, the automaticity of affect seems similar to
the automaticity of lexical-semantic (non-affective)
information observed in classic Stroop tasks. So, people
cannot suppress affective evaluation while doing a verbal
task. People also cannot suppress lexical access of “red”
even when their task is to verbally state the blue ink color of
the printed word “red”. If these two kinds of information are
compatible and can be both viewed as a feature in a
semantic-network, then there is no need to ask which feature
must be always activated first. Lastly, the shortcut for the
fearful stimuli in the neural system seems valid, but such a
neural circuit does not prevent parallel processing of the
non-affective aspects of the emotional stimuli. Therefore,
we can agree that affective processing is early, but cannot be
certain about whether affective processing precedes nonaffective processing.

and targets, people rely on that dimension, even if the
dimension is affective.
The problem with the lack of affective priming effect is
similar to the lack of non-affective processing in subliminal
priming. The null effects cannot strongly rule out the
existence of either kind of priming. In addition, the fact that
Storbeck & Robinson (2004) actually found affective
priming when the semantic categorization was restricted to
one category suggests that people are able to make use of
affective information when the situation (context) requires
them to.
Recently, an eye-tracking study provided strong support
for the Cognitive Primacy Hypothesis. Nummenmaa,
Hyönä, & Calvo (2010) presented their participants with
paired pictures of emotional and neutral scenes involving
humans and animals. They eye-tracked the participants’
saccades when they did an affective categorization (pleasant
or unpleasant) task and a non-affective categorization
(animal or human) task. They found that while the saccades
for both kinds of tasks were fast, within ~220 msec, the
ontological categorization always preceded the affective
categorization, by ~40 msec in all their experiments.
Interestingly, the item-wise affective and semantic
recognition times were positively correlated and additive.
They suggested that this means that affective processing is
an additional stage that occurs after object identification and
recognition, consistent with models in which a serial
processing is assumed.
Nummenmaa et al. (2010) show that, across 7
experiments, non-affective information is consistently
activated faster than affective information when participants
judge complex scenes. They interpret this finding as strong
evidence for the Cognitive Primacy Hypothesis, and suggest
that, “[non-affective] processing of visual scenes is faster
than their affective processing and…semantic categorization
precedes affective evaluation” (pg. 243). We suggest that
this robust result may have had much to do with the
particular stimuli they used. For example, judging from the
example stimuli the authors provide, the photographs might
have biased participants toward processing the nonaffective, ontological information first because the
information relevant for determining whether a stimulus
was an animal or a human was detectable from coarsergrained visual information (i.e., information with a lower
spatial frequency) than the information relevant to
determining the pictures’ affective content (e.g., an
emotional facial expression). It is no surprise if making
judgments based on fine-grained information takes longer
than making judgments based on coarse-grained
information. If low-level visual factors were responsible for
their results, it should be possible to observe a different
pattern simply by performing a similar task with different
stimuli.

Cognitive Primacy
Evidence supporting the Cognitive Primacy Hypothesis
primarily comes from studies showing the lack of affective
priming (e.g., Storbeck & Robinson, 2004). Storbeck and
colleagues used prime-target pairs of positive/negative
words orthogonally involving semantic dimensions of
religion, animal, and texture. In one experiment, they had
the participants do a lexical decision (word/nonword) task
on the target words. In another experiment, they had the
participants do an affective (good/bad) task. In both of the
experiments, they found significant facilitation when the
prime and target words were congruent along the semantic
dimension, but not when the words were congruent along
the affective dimension. Interestingly, the affective priming
emerged when they restricted the prime-target pairs to one
semantic category (e.g., animal). They concluded that when
people are left with only one dimension for relating primes

Context-Dependent Processing
What determines which kind of information gets activated
first? We suggest that neither Affective Primacy nor

391

To test this proposal, we had participants process the same
stimuli in different contexts.
To create the different contexts, we used a "Task-Set
Inertia" paradigm (Allport & Wylie, 2000). In this
paradigm, there are target trials and filler trials. The target
trials contain stimuli (e.g., words) with characteristics
varying in two orthogonal dimensions (e.g., affective, nonaffective). The filler trials contain a different kind of stimuli
(e.g., scenes) that vary along the same dimensions as the
target stimuli (e.g., affective, non-affective). The idea is that
what the participants do for the filler trials will persist and
facilitate or interfere with the execution of the response for
the target trials. In other words, the filler trials serve as a
context that orients the participants toward a specific
dimension of the stimuli during the target trials.
In Experiment 1, we tested whether a context-dependent
account holds in word meaning. We used affective
(positive/negative) and non-affective (animal/human) words
as the target trials, and affective (pleasant/unpleasant) and
non-affective (indoor/outdoor) scenes as the filler trials. We
predicted a context-congruent facilitation for the target word
trials.
In Experiment 2, we tested the context-dependent
processing using pictures. We swapped the target and the
filler trials in Experiment 1, so that the scenes became the
targets, and the words became the fillers. We also predicted
a context-congruent facilitation for the target scene trials.

Cognitive Primacy holds at all times. Furthermore, the
stimuli themselves should not fully determine the relative
primacy with which affective and non-affective information
gets activated, nor should the judgments that people make
on the stimuli. Instead, the context in which processing
occurs should be able to modulate the relative speed with
which affective and non-affective information gets
activated, even when the stimuli themselves and the
judgments people make on them are held constant. This
claim is motivated by the Ad Hoc Cognition framework,
according to which the role that words, pictures, and other
cues play in activating neurocognitive representations is
inseparable from the role played by the context in which
they are experienced (Casasanto & Lupyan, 2011).
Examination of the past studies supports our proposal that
the processing priority of affective and non-affective
information should be determined by context. First, the
literature reviewed so far suggests that neither affective nor
non-affective information must always be activated first.
Second, corroborative evidence from electrophysiological
data suggests that both affective and non-affective
information can be processed at an early, overlapping time
window. Some studies showed that affective processing can
be early. For example, one study demonstrated that the
emotional content of the visual cues can facilitate the
sensory encoding of these stimuli, as revealed by a P100
component starting at ~100 msec (e.g., Schupp, Junghöfer,
Weike, & Hamm, 2003). Another study using word stimuli
showed that the emotional tone of words can be identified at
~80-120 msec, and can lead to differential processing
(Scott, O’Donnella, Leutholda, & Sereno, 2009). Other
studies showed that non-affective processing too can be
early. One study showed that rapid semantic analysis of
visual scenes can occur in less than 120 msec (Kirchner &
Thorpe, 2006). In terms of word processing, it has been
demonstrated that some lexical-semantic analysis can take
place at ~100 msec (Sereno, Brewer, & O'Donnell, 2003;
Hauk, Davis, Ford, Pulvermüller, & Marslen-Wilson, 2006).
In addition, affective priming literature indicates that such
priming may be goal dependent. While many studies found
goal-independent affective evaluation (Bargh, Chaiken,
Raymond, & Hymes, 1996), one study found goaldependent affective evaluation (Klauer & Musch, 2002).
Klauer & Musch (2002) used prime-target pairs with words
that can be categorized by an affective dimension and a nonaffective dimension. They had one group of participants do
an affective (positive/negative) task, and had another group
do a non-affective task (e.g., upper-/lower- letter case, the
stimulus locations on the screen, color, etc.) They found
priming effects only when the priming dimension was taskrelevant. Although the design of their study is not ideal due
to a between-group comparison, these data are consistent
with a context-dependent processing account.

Experiment 1
Experiment 1 tested the context-dependence of affective and
non-affective information cued by words (targets), in the
context of visual scenes (distractors). We predicted a
context-congruity effect: the relative speed with which
affective and non-affective information could be activated in
response to the target words should vary according to the
type of processing (affective or non-affective) participants
were required to perform on the distractor pictures.

Method
Participants Native Dutch-speaking undergraduates (N=27;
mean age=22.6) at the Raboud University Nijmegen
participated in this experiment for payment. Of these
participants, 13 were assigned to the affective context group
and 14 to the non-affective context group.
Materials and Design The stimuli consisted of 96 nouns,
24 each of 4 types: positive-valence animals (e.g., konijntje
‘bunny’, panda ‘panda’, etc.), negative-valence animals
(e.g., parasite, ‘parasite’, kakkerlak, ‘cockroach’, etc.),
positive-valence humans (e.g., prinses ‘princess’,
grootvader ‘grandfather’ etc.), and negative-valence humans
(moordenaar ‘murderer’, pedofiel ‘pedophile’, etc.).
A norming pretest was carried out to ensure the valence of
the target words. 18 native Dutch speakers participated in
the pretest for payment. Each participant was given 145
nouns, one word at a time, and was to rate the valence of
each noun on a 9-point Self-Assessment Manikins scale

The Present Study
The present paper aims at testing a context-dependent
account of affective and non-affective meaning processing.

392

(Lang, 1980), ranged from a smiling figure at the positive
end of the scale to a frowning one at the negative end. Based
on the rating results, we chose 96 nouns that were clearly
valenced out of the original 145, for the purpose of
matching the clear-cut animal vs. human distinction. The
mean valence ratings were 6.78 (SD=0.59) for the positive
nouns and 2.79 (SD=0.82) for the negative nouns. The
valence for the two types differed significantly, as
confirmed by a two-tailed t-test (t=27.29, p=.0001).2
The 96 selected nouns were then divided into 2 blocks.
For each session, 12 of each of the 4 types of nouns were
randomly selected to be included in the first block, while the
remaining 12 of each type were presented in the second
block. The participants made affective judgments
(Positive/Negative) for one of these blocks, and nonaffective judgments (Animal/Human) for the other block.
The order of the blocks was counterbalanced between
participants.
To create a biasing context, we adapted the Task-Set
Inertia paradigm. Randomly intermixed with the target word
judgments were an equal number of filler trials. The fillers
consisted of 96 photos of complex scenes, 24 each of 4
types: pleasant indoor, unpleasant indoor, pleasant outdoor,
unpleasant outdoor scenes. The valence of the pictures
(positive or negative) was rated by two independent coders.
Inter-coder agreement was 100%. In the affective context
group, the participants made affective judgments
(pleasant/unpleasant) for all photos. In the non-affective
context group, the participants made non-affective
(indoor/outdoor) judgments for all photos. Therefore, for
each participant, the biasing context was congruent with the
target judgments for one block and incongruent for the other
block.

Results and Discussion
Accuracy The accuracy was the number of correct
responses divided by the overall number in the target trials.
3 participants were excluded due to their low accuracies
(<80%). 24 participants (Mean accuracy=89%, ±0.75%,
range 81-93%) were included in the following analysis.
Performance on the filler (scene) trials was not analyzed.

Figure 1. Reaction times for the (word) targets when participants
made affective judgments (dark bars) and non-affective judgments
(light bars) in the affective context group (left bars) and the nonaffective context group (right bars) in Experiment 1. The error bars
indicate standard errors.

Reaction Times Extreme reaction times (>5,000 msec)
were excluded (0.06% of the data). The averaged reaction
times by item are summarized in Figure 1.
To test the predicted effect of contextual modulation on
the reaction times, we carried out a linear mixed-effects
regression model of context types (affective, non-affective)
x 2 judgment types (affective, non-affective). A significant
interaction was found between the judgment type and the
context type [F(1,2069)=21.42, p=.0001]. Within the
affective context group (Figure 1, left bars), affective targets
were judged faster than the non-affective targets
[F(1,1059)=11.26, p=.001]. Within the non-affective context
group (Figure 1, right bars), non-affective targets were
judged faster than the affective targets [F(1,1039)=9.95,
p=.002]. Within the affective judgments (Figure 1, black
bars), the judgments were not significantly faster in the
affective context than in the non-affective context
[F(1,24)=0.64, p=.43]. Within the non-affective judgments
(Figure 1, grey bars), the judgments were not significantly
faster in the non-affective context than in the affective
context [F(1,24)=1.25, p=.27].
As predicted, context mattered. When participants
attended to the affective dimension, their affective
judgments about the word targets were facilitated. When
participants were oriented toward the non-affective
dimension, their non-affective judgments about the word
targets were facilitated. The effect of context cannot be
attributed to superficial similarities between the responses

Procedure
Participants sat in a comfortable chair about 90 cm from a
monitor in a soundproof, dimly-lit experimental booth.
Stimuli were presented on a computer monitor (resolution =
1024 x 768 pixels). In a target (word) trial, the word was
presented for 300 msec, followed by a dark screen until an
(affective or non-affective) judgment was made. In a filler
(scene) trial, the scene was presented for 500 msec, also
followed by a dark screen until an (affective or nonaffective) judgment was made. Participants were instructed
to press the response keys (e.g., pleasant and unpleasant) as
quickly and accurately as possible. The order of the key
assignments (left to right vs. right to left) was
counterbalanced for both the affective and the non-affective
judgments across participants. Participants responded with
the index fingers of both hands. A brief practice was given
at the beginning of the session, and a brief break was given
between the two blocks during the session. Each session
lasted approximately 15 minutes.

2

While the present design does not require the length, the log
frequency, or the arousal of words in different categories to be
matched, we still matched these factors.

393

during filler and target trials, since the effect was found
even when responses were dissimilar (e.g., indoor - human).

affective context than in the non-affective context
[F(1,24)=1.07, p=.31]. Within the non-affective judgments
(Figure 2, grey bars), the judgments were significantly faster
in the non-affective context than in the affective context
[F(1,24)=8.19, p=.009]. Therefore, the RT difference
between affective and non-affective target judgments varied
depending on the context.

Experiment 2
Experiment 2 tested whether processing pictorial stimuli is
also context-dependent, using a ‘mirror’ version of
Experiment 1. The scenes were now used as target trials,
and the words, filler trials. The context-dependent account
predicts an effect of congruity between the type of context
and the type of target judgments, regardless of the format of
the target stimuli (pictorial or verbal). Affective target
judgments should be faster in the context affective filler
judgments than in the context of non-affective filler
judgments, and vice versa for non-affective target
judgments.

Method
Participants 26 Native Dutch-speaking undergraduates
(mean age=21.4) at the Raboud University Nijmegen
participated payment. None of them previously took part in
Experiment 1. Among these participants, 13 were assigned
to the affective context group and 13 to the non-affective
context group.
Figure 2. Reaction times for the (picture) targets when
participants made affective judgments (dark bars) and nonaffective judgments (light bars) in the affective context
group (left bars) and the non-affective context group (right
bars) in Experiment 2. The error bars indicate standard
errors.

Materials, Design, and Procedure The materials and the
procedure were the same as in Experiment 1. The design for
the words and scenes was reversed, so that the scenes
became the targets (judgments manipulated within-subject)
and the words became the context (judgments manipulated
between-subjects).

To test whether the results of the two experiments
differed, we carried out a linear mixed effects regression of
2 experiments (word, picture) X 2 context types (affective,
non-affective) X 2 judgment types (affective, non-affective).
There was no 3-way interaction [F(1,4155)=.15, p=.70],
suggesting no difference between when the target stimuli
were words and when the target stimuli were pictures. Yet,
the 2-way interactions observed in the previous analyses
still hold [F(1, 4156)=36.41, p=.0001].This means that there
was little difference between the significant effects of
context types on judgment types between experiments.

Results and Discussion
Accuracy We excluded 2 participants due to low accuracy
(<80%) on the target (picture) trials. 24 participants (Mean
accuracy=89%, ±0.67%, range 82-94%) were included in
the analysis. Performance on the filler (word) trials was not
analyzed.
Reaction Times Extreme reaction times (>5,000 msec)
were excluded (0.04% of the data). The averaged reaction
times by item are summarized in Figure 2. To test the
predicted effect of contextual modulation on reaction times,
we carried out a linear mixed-effects regression model of 2
context types (affective, non-affective) X 2 judgment types
(affective, non-affective). There was a significant
interaction between the judgment type and the context type
[F(1,2086)=15.43, p=.0001], replicating Experiment 1.
There was a main effect between the context types
[F(1,23)=4.30, p=0.05], and a main effect between the
judgment types [F(1,2089)=48.83, p=0.0001]. Within the
affective context group (Figure 2, left bars), affective
judgments were made faster than the non-affective
judgments [F(1,1062)= 51.26, p=.0001]. Within the nonaffective context group, affective targets were still judged
faster than the non-affective targets [F(1,1031)=5.85,
p=.02]. Within the affective judgments (Figure 2, black
bars), the judgments were not significantly faster in the

General Discussion
Changes in the context can determine the relative speed
with which people make affective and non-affective
judgments on words and pictures. These findings challenge
both the Affective Primacy and the Cognitive Primacy
hypotheses. Our results support the Ad Hoc Cognition
framework, according to which words and pictures activate
different neurocognitive representations every time they are
processed, the specifics of which are co-determined by the
stimuli themselves and the contexts in which they occur
(Casasanto & Lupyan, 2011; see also Elman, 2004).
Although we obtained interactions of nearly identical
sizes for scenes and for words, which did not differ
statistically across experiments 1 and 2, the details of the
data were different descriptively. For the scene targets, the

394

Keller, F., Gunasekharan, S., Mayo, N., & Corley, M. (2009).
Timing accuracy of Web experiments: A case study using the
WebExp software package. Behavior Research Methods, 41(1),
1-12.
Klauer, K. C., & Musch, J. (2003). Affective priming: Findings
and theories. In J. Musch & K. C. Klauer (Eds.), The
psychology of evaluation: Affective processes in cognition and
emotion (pp. 7-50). Mahwah, NJ: Erlbaum.
Scott, G. G., O'Donnell, P. J., Leuthold, H., & Sereno, S. C.
(2009). Early emotion word processing: Evidence from eventrelated potentials. Biological Psychology, 80(1), 95-104.
Storbeck, J., & Robinson, M. D. (2004). Preferences and
inferences in encoding visual objects: A systematic comparison
of semantic and affective priming. Personality and Social
Psychology Bulletin, 30, 81-93.
Storbeck, J., Robinson, M. D., & McCourt, M. (2006). Semantic
processing precedes affect retrieval: The neurological case for
cognitive primacy in visual processing. Review of General
Psychology, 10, 41-55
Whalen PJ, Rauch SL, Etcoff NL, McInerney SC, Lee M, Jenike
MA. (1998). Masked presentations of emotional facial
expressions modulate amygdala activity without explicit
knowledge. Journal of Neuroscience, 1998; 18:411-418.
Zajonc, R. (1980). Feeling and thinking: Preferences need no
inferences. American Psychologist, 35, 151-175.
Zajonc, R. (2000). Feeling and thinking: Closing the debate over
the independence of affect. In J. P. Forgas (Ed.), Feeling and
thinking: The role of affect in social cognition. Studies in
emotion and social interaction (Vol. 2, pp. 31-58). New York:
Cambridge University Press.

affective judgments were made faster than the non-affective
judgments, no matter what the context. On one possible
explanation, the representations activated in response to
scenes (in this case, detailed color photographs) may be
more constrained by the stimuli themselves than is the case
for words. Whereas words name generic types (e.g.,
“puppy” can refer to any puppy) pictures depict a specific
instance of a type (e.g., a photo must be of a specific
puppy). Therefore, representations activated in response to
photographs may be more constrained than representations
activated in response to words (c.f., De Houwer & Hermans,
1994). Yet, importantly,
in both experiments,
representations varied as a function of cues-in-context,
belying any broad generalizations about the primacy of once
kind of information (affective or non-affective) over the
other.

Acknowledgments
We thank the research assistants in the Neurobiology of Language
Group at the Max Planck Institute of Psycholinguistics, Nijmegen.
Research supported in part by a grant from the Junta de Andalucía
(P09-SEJ-4772) and a McDonnell Scholar Award to DC.

References
Allport, A. & Wylie, G. (2000). ‘Task-switching’, stimulusresponse bindings, and negative priming. In S. Monsell & J. S.
Driver (Eds.), Control of cognitive processes: Attention and
Performance XVIII. Cambridge: MIT press.
Bargh, J. A., Chen, M., & Burrows, L. (1996). Automaticity of
Social Behavior: Direct Effects of Trait Construct and
Stereotype Activation on Action. Journal of Personality and
Social Psychology, 71(2), 230-244.
Calvo, M. G., & Nummenmaa, L. (2007). Processing of unattended
emotional visual scences. Journal of Experimental Psychology:
General, 136, 347-369.
Casasanto, D. & Lupyan, G. (2011). Ad Hoc Cognition.
Manuscript submitted for publication.
De Houwer, J., & Hermans, D. (1994). Differences in the affective
processing of words and pictures. Cognition & Emotion, 8(1), 120.
Elman, J. L. (2004). An alternative view of the mental lexicon.
Trends in Cognitive Sciences, 8(7), 301-306.
Fazio, R. H. (2001). On the automatic activation of associated
evaluations: An overview. Cognition and Emotion, 15, 115-141.
Glaser, W., & Glaser, M. O. (1989). Context effects in Stroop-like
word and picture processing. Journal of Experimental
Psychology: General, 118(1), 13-42.
LeDoux, J. E. (1996). The emotional brain. New York, NY: Simon
& Schuster.
Lang, P. J. (1980). Behavioral treatment and bio-behavioral
assessment: computer applications. In J. B. Sidowski, J. H.
Johnson, & T. A. Williams (Eds.), Technology in mental health
care delivery systems (pp. 119-l37). Norwood, NJ: Ablex.
Lazarus, R. (1984). On the primacy of cognition. American
Psychologist, 39, 124–129.
Murphy, S. T., & Zajonc, R. B. (1993). Affect, cognition, and
awareness: affective priming with optimal and suboptimal
stimulus exposures. Journal of Personality and Social
Psychology, 64(5), 723-739.
Nummenmaa, L., Hyönä, J., & Calvo, M. G. (2010). Semantic
Categorization Precedes Affective Evaluation of Visual Scenes.
Journal of Experimental Psychology: General, 139(2), 222-246.

395

