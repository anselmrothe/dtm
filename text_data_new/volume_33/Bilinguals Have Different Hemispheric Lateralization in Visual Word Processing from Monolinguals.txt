UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Bilinguals Have Different Hemispheric Lateralization in Visual Word Processing from
Monolinguals

Permalink
https://escholarship.org/uc/item/5350n1mm

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)

Authors
Lam, Sze-Man
Hsiao, Janet Hui-wen

Publication Date
2011-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Bilinguals Have Different Hemispheric Lateralization in Visual Word Processing
from Monolinguals
Sze-Man Lam (fannylam@hku.hk)
Janet Hui-wen Hsiao (jhsiao@hku.hk)
Department of Psychology, University of Hong Kong
Pokfulam Road, Hong Kong
Abstract
Previous bilingual studies showed reduced hemispheric
asymmetry in visual tasks such as face perception in bilinguals
compared with monolinguals, which suggested that
hemispheric asymmetry in visual tasks could be modulated by
experience in reading one or two languages. Here we examined
whether difference in hemispheric asymmetry in visual tasks
can also be observed in bilinguals who have different language
backgrounds. We compared the behavior of three language
groups in a tachistoscopic English word sequential matching
task: English monolinguals (or alphabetic monolinguals,
A-Ms), bilinguals with an alphabetic language L1 and English
L2 (alphabetic-alphabetic bilinguals, AA-Bs), and bilinguals
with a logographic language (Chinese) L1 and English L2
(logographic-alphabetic bilinguals, LA-Bs). The results
showed that AA-Bs had a stronger right visual field/ left
hemispheric (LH) advantage than A-Ms and LA-Bs,
suggesting that different language learning experiences can
influence how visual words are processed in the brain. In
addition, we showed that this effect could be accounted for by
a computational model that implements a theory of
hemispheric asymmetry in perception (i.e. the Double Filtering
by Frequency theory, Ivry & Robertson, 1998); the modeling
data suggested that this difference may be due to both the
difference in participants’ vocabulary size and the difference in
word-to-sound mapping between alphabetic and logographic
languages.
Keywords: Hemispheric asymmetry; bilingualism; visual
word recognition; computational modeling.

Introduction
Researchers have found different functional dominance
between the two hemispheres. One of the most salient
functional differences is the superiority of the left and right
hemisphere (LH and RH) in language processing, especially
in phonology processing (Corina, Vaid, & Bellugi, 1992),
and in visuospatial processing and face processing
(Kanwisher, McDermott, & Chun, 1997) respectively.
Despite the converging evidences showing the RH
superiority in specific tasks such as face recognition and
visuospatial tasks, there have been studies showing reduced
lateralization in these well-known RH tasks in bilinguals
compared with monolinguals. Back in the 1980s, Sewell and
Panou (1983) observed the typical right visual field (RVF)/
LH advantage in accuracy in an English word naming task in
both bilinguals and monolinguals; in contrast, the typical left
visual field (LVF)/ RH advantage in a spatial dot localization
task was only found in monolinguals but not in bilinguals. In
this task, a 4x5 grid with a dot in one of the boxes was shown
unilaterally, and participants were required to report the

location of the dot. Therefore, Sewell and Panou’s results
(1983) suggested that the processing of some visual tasks
such as spatial dot localization may be influenced by
participants’ language experiences. About 20 years later,
Hausmann et al. (2004) examined performance of bilinguals
and monolinguals in visual tasks and found consistent results.
They showed that in the accuracy data of both groups, a
typical RVF/LH advantage was found in a sequential
word-matching task whereas a typical LVF/RH advantage
was found in a face detection task; however, the respond time
data revealed a significant LVF/RH advantage in the face
detection task only in monolinguals but not bilinguals. This
result suggested that the RH visual processing abilities may
be affected by language experience.
The above results seemed to suggest that hemispheric
asymmetry in RH dominant visual tasks such as face
perception and spatial localization could be affected by
language experience, but not for LH dominant visual tasks
such as visual word recognition. However, some difference
between the bilinguals and monolinguals was observed in
Sewell and Panou’s study (1983). In their word naming task,
words were presented unilaterally and the participants were
required to report the word they perceived; the display time
was 20ms and 40ms for monolinguals and bilinguals
respectively. The authors selected these display times where
the two groups made approximately the same number of
errors. This suggested that bilinguals might process the
words differently compared with monolinguals. In addition,
in the word sequential matching task in which Hausmann, et
al. (2004) did not find performance difference between
bilinguals and monolinguals, a centrally presented word was
followed by a unilaterally displayed word; the exposure
time was 175ms for both groups. As the same display time
was used for both groups and performance level between
groups was not controlled, the results from the word
sequential matching task of Hausmann, et al. (2004) might
not completely reflect the difference between bilinguals and
monolinguals in visual word processing. Therefore, in this
study, we aim to control for the performance level in the
sequential word matching task employed by Hausmann et al.
(2004) for investigating the impact of language experience
on hemispheric asymmetry in visual word recognition.
Moreover, as all the previous studies investigated only the
population of alphabetic language users, here we aim to
investigate hemispheric asymmetry in visual word
recognition in the following three groups of people with
different language experiences: (1) alphabetic monolinguals
(A-Ms), who know only one alphabetic language; (2)

3409

alphabetic-alphabetic bilinguals (AA-Bs), who are
proficient in two alphabetic languages; and (3)
logographic-alphabetic bilinguals (LA-Bs), who acquire a
logographic language (e.g. Chinese) and an alphabetic
language with high proficiency in both. We believe that an
investigation on the behavioral difference between AA-Bs
and LA-Bs will provide a broader view on how different
language experiences modulate hemispheric asymmetry in
visual word recognition. We describe the differences
between alphabetic and logographic languages below.
In alphabetic language processing, functional MRI studies
revealed a specific region in the LH (i.e. the visual word
form area) that responds to words selectively (McCandliss,
Cohen, & Dehaene, 2003); some researchers (Maurer &
McCandliss, 2007) suggested that the observed LH
lateralization in alphabetic language processing is due to the
application of grapheme-phoneme conversion (GPC) rules
during learning to read. Behavioral studies also found a
RVF/LH advantage in reading words in alphabetic
languages in tachistoscopic recognition (Bryden & Rainey,
1963). In short, the superiority of the LH in processing
alphabetic languages has been consistently reported.
In contrast to alphabetic languages, the relationship
between written and spoken logographic languages, such as
Chinese, is more opaque due to its morphosyllabic features.
Moreover, stroke patterns in Chinese characters do not map
to phonemes in the pronunciation, so GPC rules in
alphabetic languages do not apply to Chinese reading.
Functional MRI studies (Tan et al., 2001; Tan et al., 2000)
showed more activation in the visual areas in the RH than
the LH in reading Chinese characters, and this effect has
been argued to be due to elaborated visual analysis required
for processing spatial information and locations of strokes.
In behavioral studies, a LVF/RH advantage was observed in
tachistoscopic recognition of Chinese characters (Tzeng,
Hung, Cotton, & Wang, 1979); in a recent study, Hsiao and
Cottrell (2009) showed a left side bias effect in Chinese
readers but not in non-Chinese readers in a Chinese
character perception task, suggesting more RH involvement
in Chinese characters recognition. In sum, the superiority of
the RH in processing the orthography of logographic
Chinese, a logographic language, has been consistently
reported.
Due to the dramatic differences in orthographic
processing and hemispheric lateralization between
alphabetic and logographic languages, we predict that in
visual word recognition, (1) as alphabetic reading involves
more LH processing, and AA-Bs have acquired one more
alphabetic language than A-Ms, AA-Bs may have a stronger
LH lateralization than A-Ms; and (2) although both AA-Bs
and LA-Bs acquired two languages, logographic reading
involves more RH processing, and thus AA-Bs may show a
stronger LH lateralization than LA-Bs.

Behavioral Study
We examined hemispheric asymmetry in visual word
recognition in three groups of participants with different

language backgrounds, namely, A-Ms, AA-Bs, and LA-Bs,
using a divided visual field word sequential matching task
modified from Hausmann, et al. (2004).

Participants
66 participants were recruited; all were right-handed
according to the Edinburgh handedness inventory (Oldfield,
1971), and had normal or corrected to normal vision.
Participants were undergraduate or postgraduate students at
the University of Hong Kong and were divided into three
groups of equal size (n=22) according to their language
background: English monolinguals (A-Ms), bilinguals with
an alphabetic language L1 and English L2 (AA-Bs), and
bilinguals with Chinese L1 and English L2 (LA-Bs). The
A-Ms spoke English as their L1 and could not fluently use
any other languages. The AA-Bs learnt a non-English, west
European alphabetic language as their L1 (i.e., French,
Spanish, Dutch, German or Italian), and English as their L2
during schooling; they were proficient in both their L1 and
English. Both A-Ms and AA-Bs had none or very limited
knowledge about logographic scripts such as Chinese
characters. The LA-Bs were local Hong Kong students who
learnt Chinese as their L1 and English as an L2 since
kindergarten in formal education; they were proficient in
both Chinese and English. Average age of acquisition of
English was 3.3 for LA-Bs and 7.4 for AA-Bs.

Stimuli & Procedures
We used an English word sequential matching task to
measure hemispheric lateralization in English word
processing in the three groups. A hundred pairs of English
words were selected as the test stimuli from the SUBTLEXUS
corpus (Brysbaert & New, 2009). In each pair, the two words
had the same number of letters and the same initial and final
letters, and were matched in word frequency. The length of
the word stimuli ranged from four to seven and the average
frequency of the word stimuli was 407.57 per million words
in the SUBTLEXUS corpus.
The task consisted of a pre-test and a test. In the pre-test,
the staircase method was employed to determine a perceptual
threshold for each participant in the word matching task, in
which the participant achieved reliably 80% accuracy. A
1-up 3-down staircase rule was applied (Hartmann, 2004).
That is, for every three consecutive correct responds, the
display time was decreased by one refresh rate, and every
single incorrect response made the display time increased by
one refresh rate. Three staircases were run in each pre-test,
and each run proceeded until eight turnarounds had occurred.
Only the third to the eighth turnarounds were averaged and
used as the estimate of the threshold. The display time for the
English words in the subsequent sequential matching task
was then calculated by averaging the estimated thresholds of
the three runs 1. The pre-test followed a similar procedure as
the test except all the stimuli were presented at the center of
1
Note that average threshold for A-Ms (53ms) were slightly
lower than LA-Bs (59ms) and AA-Bs (62ms).

3410

the screen. The stimuli used in the pre-test were not used
again in the test.
There were 100 trials in the test. In each trial, after a
1000ms central fixation, the first stimulus was presented
either in the LVF or RVF, at about 1.5o to 5o of visual angle
away from the centre (thus the size of the stimulus was about
3.5o), for the display time obtained in the pre-test. The second
stimulus was then presented at the center of the screen after
another 1000ms central fixation. There were equal numbers
of stimuli presented in the two visual fields. The presentation
order and condition (LVF or RVF) was randomized.
Participants were asked to judge whether the two stimuli
were the same by pressing corresponding keys on the
keyboard.

Results
Here we define the variable hemisphere lateralization as the
performance difference between the LVF/RH and the
RVF/LH conditions in terms of accuracy; therefore, positive
and negative indices reflect RH and LH lateralization
respectively. One-sample t-test against zero and ANalysis
Of VAriance (ANOVA) were used for the analysis.
Hemispheric lateralization was the dependent variable and
language background was the independent variable.
Hemispheric Lateralization

RH
0%
-2%
-4%
-6%
-8%
-10%
-12%

LH

*
Alphabetic
Monolingual

Computational Modeling
Here we aimed to account for the behavioral results through
computational modeling. We hypothesized that the
hemispheric lateralization difference in English word
processing among the three groups may be due to two
factors: (1) bilinguals have a larger vocabulary size, and (2)
reading in alphabetic and logographic languages involve
different word-to-sound mappings. We applied the
intermediate convergence model proposed by Hsiao, Shieh,
and Cottrell (2008) to model bilingual visual word
recognition. Hsiao et al. (2008) showed that this model was
able to account for the left-side bias effect in face perception
observed in human data (Brady, Campbell, & Flaherty,
2005). The model incorporates several known observations
about visual anatomy and neural computation and
implements a theory of hemispheric asymmetry in
perception, Double Filtering by Frequency (DFF, Ivry &
Robertson, 1998), but does not assume a LH localized
language center. The DFF theory posits that visual
information is captured by frequency-based representation
at multiple scales, and the frequency information is filtered
at two stages; in the first stage, a task-relevant frequency
range is selected through attention processes; and at the
second stage, asymmetric filtering processing is applied to
the two hemispheres: The LH amplifies high spatial
frequency (HSF) information, while the RH amplifies low
spatial frequency (LSF) information. We describe our
modeling details below.

*
**
AlphabeticAlphabetic
Bilingual

LogographicAlphabetic
Bilingual

Figure 1: Results from the behavioral experiment. (* p
< .05, ** p < .01). Error bars show one standard error.
The results from a one-sample t-test against zero showed
a significant LH lateralization among all the participants
(t(65) = -3.538, p = .001). For individual groups, a
significant LH lateralization was found only in AA-Bs (t(21)
= -3.598, p = .002), while A-Ms showed a tendency of a LH
lateralization (t(21) = -1.830, p = .082) and LA-Bs did not
exhibit any significant LH lateralization (t(21) = -.478, n.s.).
ANOVA also showed a significant effect of language
background on hemispheric lateralization (F(2, 63) = 4.625,
p = .013); post hoc analysis showed that the LH
lateralization was significantly stronger in AA-Bs than the
other two groups (independent t-test, A-Ms: t(42) = -2.030,
p = .049; LA-Bs: t(42) = -2.717, p = .01).
These results are consistent with our predictions that in
the English word sequential matching task, AA-Bs have
more LH lateralization than both A-Ms and LA-Bs. Thus, it
suggests that hemispheric lateralization in visual word
recognition may be affected by language experience and
also the orthographic processing of the languages.

Figure 2: Hsiao et al.’s hemispheric processing model
(2008)
In the model, each input image (35x60 pixels) was first
filtered with a rigid grid (5x10) of overlapping 2-D Gabor
filters (Daugman, 1986) at five scales and eight orientations.
Gabor filters were used to simulate neural responses of
complex cells in the early visual system (Lades et al.,
1993), and the frequency range represented the task-relevant
frequency range in DFF theory, as the five scales
corresponded to 2 to 32 (i.e., 21 to 25) cycles per word
whereas our image height was 35 pixels. After the Gabor
filters, each input image was transformed into a vector of
size 2000 (5x10 sample points x 5 scales x 8 orientations).
A base-line condition and a biased condition were then
created. The second stage of the DFF theory was only
applied to the biased condition, in which the Gabor
responses of the left and right half of the word were biased

3411

to low and high spatial frequencies respectively by applying
a sigmoidal weighting function. In contrast, in the base-line
condition, equal weights were given to the Gabor responses
of different scales. The Principal Component Analysis
(PCA), a biological plausible linear compression technique
(Sanger, 1989), was then applied to the Gabor
representations of the left and right half-words separately to
compress each representation into a 50-element
representation (i.e., 100 elements in total). This PCA
representation was then used as the input to a two layer
neural network (See Hsiao et al., 2008, for more simulation
details).
Our model was trained to recognize the input images until
the performance on the training set reached 100% accuracy.
The training algorithm used was gradient descent with an
adaptive learning rate. To test hemispheric asymmetry
effects, we used left or right half damaged inputs, which
were generated by setting one half of the PCA
representation to zero. When mapping damaged inputs to
their corresponding outputs, only the representation from
one of the visual fields was informative in recognition. Thus,
in the biased condition, a right-damaged word carried only
LSF/RH information and a left-damaged word carried only
HSF/LH information. The RH (LSF) lateralization effect
was then measured as the accuracy difference between
recognizing a right-damaged word and a left-damaged word
as the original word. The model was run 40 times in each
condition in the analysis.
We created artificial lexicons for the current examination.
Each word consisted of three letters. The task of the model
was to map each word input to its pronunciation with a
consonant-vowel-consonant (CVC) structure. Each lexicon
consisted of an alphabet of size 13; eight letters were
randomly assigned as consonants and the rest five letters as
vowels in the pronunciation. Eight different fonts of the
words were used as input images; four of them were used as
the training set and the other four as the testing set. The
output layer was divided into three parts; each part
corresponded to a position in the CVC structure, and each
node corresponded to a phoneme in that position. To
counterbalance the information available in the left and right
side of the input images, in each lexicon, the frequency of
each letter in the first and third position was kept equal;
mirror images were used in half of the simulation runs.
In order to compare with the behavioral data, we built
three models of visual word recognition with different
vocabulary sizes and different orthography-to-phonology
mappings to capture the behavioral differences among our
three groups of participants, as describe below.
Alphabetic Reading Model (A-model) We simulated
alphabetic reading by mapping each letter in a word
systematically to each phoneme in the pronunciation; in
addition, we examined the effect of vocabulary size by
varying the number of words in the artificial lexicons from
16 to 40 (while keeping the alphabet size 13). As both our
A-Ms and AA-Bs were experts in alphabetic reading, and

the two languages acquired by our AA-Bs have similar
alphabets (i.e. one was English and the other was a
west-European language), we assumed that the behavioral
difference between the two groups was mainly due to a
larger vocabulary size in AA-Bs compared with A-Ms.
Logographic Reading Model (L-model) We simulated
logographic reading by randomizing the mapping between
each word and its pronunciation (i.e. no systematic
letter-to-phoneme mapping). We also varied the vocabulary
size from 16 to 40 and compared the results with the
A-model to examine the difference between logographic and
alphabetic reading.
Logographic-Alphabetic Model (LA-model) This model
was trained to perform both alphabetic and logographic
reading, so that its behavior could be compared with the
LA-Bs in the behavioral data. Two alphabets were used in
each simulation run, in which letters in one of the alphabet
were systematically mapped to phonemes in the
pronunciation, whereas in the other alphabet there was no
systematic mapping. The assignment of mapping method to
the two alphabets was counterbalanced among the runs. The
range of the vocabulary size (half from each lexicon) also
ranged from 16 to 40.

Our Hypotheses
In a recent study adopting also the intermediate convergence
model (Hsiao, et al., 2008), Cheung and Hsiao (2010)
demonstrated two factors that lead to more LH bias in visual
word recognition: (a) visual similarity among word stimuli
in the lexicon: more HSF information is required when the
visual stimuli look more alike; (b) the task requirement to
decompose visual stimuli into smaller parts for performing
grapheme-phoneme conversion.
Here we hypothesize that (1) The LH (HSF) lateralization
of the A-model will increase with vocabulary size, since the
similarity of words increase with vocabulary size; this
prediction is consistent with our behavioral data showing
that AA-Bs exhibited a stronger LH lateralization compared
with A-Ms; (2) the A-model (alphabetic reading) will show
more LH (HSF) lateralization than the L-model
(logographic reading), since decomposition of words is not
necessary in the L-model; (3) When performing alphabetic
reading, the LA-model will show less LH (HSF)
lateralization than the A-model, since during learning,
decomposition of words is required in only half of the times.

Results
In Figures 3 and 4, hemispheric lateralization represents the
performance difference between correctly recognizing a
right-damage word and a left-damaged word as the original
word, in the biased condition over the base-line condition.
Vocabulary Size Both the A-model and the L-model
showed an increase in LH (HSF) lateralization with
increasing vocabulary size (Figure 3), and the LA-model

3412

exhibited a similar pattern as the L-model. For all three
models, significant but weak positive correlations were
observed between LH (HSF) lateralization and vocabulary
size (A-model: R2 = .054, p < .001; L-model: R2 = .070, p
< .001; LA-model: R2 = .083, p < .001). These results
showed that in all three models, LH (HSF) lateralization
increased with vocabulary size 2.
Mapping Method Results from Figure 3 showed that the
L-model (logographic reading) had a weaker LH (HSF)
lateralization than the A-model (alphabetic reading) (p < .01,
except for vocabulary size of 16).
A-model

Hemispheric Lateralization

RH
25%

*** ***

L-model

*** *** *** ***

20%

** *** ** ***
*** ***

15%
10%
5%

Discussion & Conclusion

0%
LH

16 18 20 22 24 26 28 30 32 34 36 38 40

Vocabulary Size

Figure 3: Results from computational modeling:
hemispheric lateralization in alphabetic (A-model) and
logographic reading (L-model). (* p < .05; ** p < .01;
*** p < .001).
Model Comparison In performing alphabetic reading, the
behavior of the LA-model was more similar to logographic
reading in the L-model than alphabetic reading in the
A-model; the LA-model showed a significantly weaker LH
(HSF) lateralization than A-model (p < .01, except for
vocabulary size of 16), but no significant differences from
the L-model.
A-M

RH
Hemispheric Lateralization

Comparison with Behavioral Data The modeling data and
behavioral data are compared in Figure 4. We assumed that
the vocabulary size of AA-Bs’ was twice of the A-Ms’. In the
three comparisons, A-M data was derived from the A-model
of vocabulary size of 16, 18 and 20; data for AA-Bs and
LA-Bs were obtained from the A-model and the LA-model of
vocabulary size of 32, 36 and 40 respectively; for the
LA-model, the presented data consisted of the behavior of
alphabetic reading only, to match the behavioral study.
The modeling data fit with the behavioral data well. All
three comparisons exhibited a significant group difference
(1: F(2, 117) = 3.721, p = .027; 2: F(2, 117) = 6.397, p
= .002; 3: F(2,117) = 5.822, p = .004); post hoc showed a
stronger LH (HSF) lateralization in AA-Bs than LA-Bs (1:
t(78) = -2.980, p = .004; 2: t(78) = -3.574, p = .001; 3: t(78)
= -2.070, p = .042). AA-Bs also showed a stronger LH (HSF)
lateralization than A-Ms in comparison 2 and 3 (2: t(78) =
-2.620, p = .011; 3: t(78) = -3.531, p = .001).

**

20%

AA-B
*

LA-B
***

***

*

15%
10%
5%
0%
LH

1

2

Comparison

3

Figure 4: Results from computational modeling:
hemispheric lateralization in alphabetic reading. (* p
< .05; ** p < .01; *** p < .001). Error bars show one
standard error.

2
Note that the RH lateralization of the model, in contrast to the
LH in the human data, was because the stimuli used (3-letter words)
were simpler and the vocabulary sizes used were much smaller
than the English lexicon (see Cheung & Hsiao, 2010). Here we
examined relative changes in lateralization between different
models rather than the absolute lateralization.

In this study, we examined how hemispheric asymmetry in
visual tasks can be modulated by language experience.
Previous studies found that compared with monolinguals,
bilinguals have a reduced hemispheric lateralization in RH
dominant visual tasks such as face perception, but not in LH
dominant visual tasks such as word naming or word
matching. However, we suspected that the lateralization
difference between the two groups in the LH visual tasks did
not emerge because the performance level between the two
groups was not matched.
Therefore, in the behavioral study, we used a perceptual
threshold match in an English word sequential matching task
and investigated lateralization difference among three groups
of people with different language experiences: A-Ms, AA-Bs,
and LA-Bs. We found a stronger LH lateralization in AA-Bs
over both LA-Bs and A-Ms. We hypothesized that this effect
may be due to at least two factors: (1) vocabulary size: in the
study the languages acquired by A-Ms and AA-Bs used a
similar alphabet, but AA-Bs learned more words overall than
A-Ms; and (2) the application of GPC rules in alphabetic
reading but not in logographic reading: alphabetic reading
required decomposing a word into letters in order to map
them to phonemes, and thus involved more LH (HSF)
processing.
To verify our hypothesis, we applied the hemispheric
processing model (Hsiao et al., 2008) on visual word
recognition; the model implements the DFF theory (Ivry &
Robertson, 1998) and does not assume any influence from
the LH-lateralized language processing. The modeling data
fit well with the behavioral data, explaining the above two
factors: (1) vocabulary size: when the vocabulary size
increases, the words in the lexicon look more similar to each
other, thus more HSF information is required to distinguish
words; (2) the application of GPC rules in alphabetic
reading but not in logographic reading: since half of the
words in LA-Bs’ lexicon involve logographic mapping and
thus do not require the application of GPC rules, compared

3413

with AA-Bs, LA-Bs’ behavioral might be influenced by
their logographic mapping experience and thus exhibited
less LH (HSF) lateralization even in alphabetic reading.
Thus, our results showed that differences in hemispheric
lateralization between bilinguals and monolinguals can also
be observed in LH dominant visual tasks; in addition, this
effect can further be modulated by different bilingual
experiences. This result suggests that our expertise domains
(e.g. expertise in different languages) can influence each
other, and is consistent with recent research on perceptual
expertise, showing that similar brain areas are recruited for
different expertise domains, and thus these domains may
influence each other (e.g. Gauthier, et al., 2000).
In addition to the two factors we examined in the
modeling, there are some other factors that may also
account for the observed difference among the three
language groups, such as the difference in word/character
features between different languages, as well as the age of
acquisition of the second language in the bilinguals; thus,
further investigations are required to examine these factors.
In summary, here we show that hemispheric asymmetry
in English word sequential matching can be modulated by
bilingual language experience, and our modeling data
suggested that at least two factors may account for this
effect: (1) larger vocabulary size in bilinguals, and (2) the
difference in word to sound mapping between alphabetic
and logographic languages.

Acknowledgments
We are grateful to the HKU Seed Funding Program for
Basic Research (project #10400471 to J.H. Hsiao) and the
Research Grant Council of Hong Kong (project code: HKU
744509H and 745210H to J. H. Hsiao).

References
Brady, N., Campbell, M., & Flaherty, M. (2005). Perceptual
asymmetries are preserved in memory for highly familiar
faces of self and friend. Brain and Cognition, 58, 334-342.
Bryden, M. P., & Rainey, C. A. (1963). Left-right differences
in tachistoscopic recognition. Journal of Experimental
Psychology, 66, 568-571.
Brysbaert, M., & New, B. (2009). Moving beyond Kučera
and Francis: A critical evaluation of current word
frequency norms and the introduction of a new and
improved word frequency measure for American English.
Behavior Research Methods, 41(4), 977-990.
Cheung, K. C. F., & Hsiao, J. H.-w. (2010). Visual and Task
characteristics may explain hemispheric asymmetry in
visual word recognition. Proc. of the 32rd Annual Meeting
of the Cognitive Science Society, Portland.
Corina, D. P., Vaid, J., & Bellugi, U. (1992). The linguistic
basis of left hemisphere specialization. Science, 255,
1258-1260.
Daugman, J. G. (1986). uncertainty relation for resolution in
space, spatial frequency, and orientation optimized by
two-dimensional visual cortical filters. Journal of Optical
Society of America A, 2(7), 1160-1169.

Gauthier, I., Skudlarski, P., Gore, J. C., & Anderson, A. W.
(2000). Expertise for cars and birds recruits brain areas
involved in face recognition. Nat. neurosci., 3(2), 191-197.
Hartmann, W. M. (Ed.). (2004). Signals, sound and sensation
(5th ed.). NY: American Institute of Physics.
Hausmann, M., Durmusoglu, G., Yazgan, Y., & Gunturkun,
O. (2004). Evidence for reduced hemispheric asymmetries
in non-verbal functions in bilinguals. Journal of
Neurolinguistics, 17, 285-299.
Hsiao, J. H.-w., & Cottrell, G. W. (2009). Not all visual
expertise is holistic, but it may be leftist: The case of
Chinese character recognition. Psychological Science,
20(4), 455-463.
Hsiao, J. H.-w., Shieh, D. X., & Cottrell, G. W. (2008).
Convergence of the visual field split: Hemispheric
modeling of face and object recognition. Journal of
Cognitive Neuroscience, 20(12), 2298-2307.
Ivry, R. B., & Robertson, L. C. (1998). The Two Sides of
Perception. Cambridge: MIT Press.
Kanwisher, N., McDermott, J., & Chun, M. M. (1997). The
fusiform face area: a module in human extrastriate cortex
specialized for face perception. Journal of Neuroscience,
17(11), 4302-4311.
Lades, M., Vorbruggen, J. C., Buhmann, J., Lange, J.,
Malsburg, C. v. d., Wurtz, R. P., & Konen, W. (1993).
Distortion Invariant Object Recognition in the Dhynamic
Link Architecture. IEEE T. Comput., 42(3), 300-311.
Maurer, U., & McCandliss, B. D. (2007). The development
of visual expertise for words: The contribution of
eletrophysiology. In E. L. Grigorenko & A. Naples (Eds.),
Single-word reading: Cognitive, behavioral and biological
perspectives (pp. 43-64). NJ: Erlbaum.
McCandliss, B. D., Cohen, L., & Dehaene, S. (2003). The
visual word form area: expertise for reading in the fusiform
gyrus. Trends Cogn. Sci., 7(7), 293-299.
Nicholls, M. E. R., Wood, A. G., & Hayes, L. (2001).
Cerebral asymmetries in the level of attention required for
word recognition. Laterality, 6(2), 97-110.
Oldfield, R. C. (1971). The assessment and analysis of
handedness: The Edinburgh inventory. Neuropsychologia,
9(1), 97-113.
Sanger, T. D. (1989). An optimality principle for
unsupervised learning. In D. Touretzky (Ed.), Adv. Neur.
In. (Vol. 1). San Mateo: Morgan Kaufmann.
Sewell, D. F., & Panou, L. (1983). Visual field asymmetries
for verbal and dot localization tasks in monolingual and
bilingual subjects. Brain and Language, 18, 28-34.
Tan, L. H., Liu, H.-L., Perfetti, C. A., Spinks, J. A., Fox, P. T.,
& Gao, J.-H. (2001). The neural system underlying
Chinese logograph reading. NeuroImage, 13, 836-846.
Tan, L. H., Spinks, J. A., Gao, J.-H., Liu, H.-L., Perfetti, C.
A., Xiong, J., et al. (2000). Brain activation in the
processing of Chinese characters and words: a functional
MRI study. Human Brain Mapping, 10, 16-27.
Tzeng, O. J. L., Hung, D. L., Cotton, B., & Wang, W. S.-Y.
(1979). Visual lateralisation effect in reading Chinese
characters. Nature, 282, 499-501.

3414

