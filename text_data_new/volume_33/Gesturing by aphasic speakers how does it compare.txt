UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Gesturing by aphasic speakers, how does it compare?

Permalink
https://escholarship.org/uc/item/5hb0b2hc

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)

Authors
Mol, Lisette
Krahmer, Emiel
Van de Sandt-Koenderman, Mieke

Publication Date
2011-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Gesturing by aphasic speakers, how does it compare?
Lisette Mol (l.mol@uvt.nl)1
Emiel Krahmer (e.j.krahmer@uvt.nl) 1
Mieke van de Sandt-Koenderman (m.sandt@rijndam.nl)2
1

Tilburg Center for Cognition and Communication (TiCC), School of Humanities, Tilburg University
P.O. Box 90135, NL-5000 LE Tilburg, The Netherlands
2
Rotterdam Neurorehabilitation Research (RoNeRes) Rijndam Rehabilitation Centre, and Erasmus MC, dept.
of Rehabilitation Medicine, P.O. Box 23181, NL-3001 KD, Rotterdam, The Netherlands
Abstract

gesture and speech production are complementary and can
compensate one another, which also underlies the Tradeoff
Hypothesis. This hypothesis states that “when speaking gets
harder, speakers will rely relatively more on gestures”, and
vice versa (De Ruiter, Bangerter, & Dings, in press). Yet De
Ruiter et al. found only little evidence that people gesture
more when speech is harder. Rather, they found that gesture
and speech tended to express similar types of information,
consistent with the idea that gesture and speech are two
sides of a coin.

We compared gesturing by aphasic speakers to that of healthy
controls, to see if gesture degrades with speech, or can be
compensatory. We found that gestures by aphasics were less
informative than those of controls, and that gestures by people
with severe aphasia were less informative than those by people
with mild aphasia. We also found that aphasics tended to use
fewer representation techniques in gesture than healthy controls
who were asked to use gesture instead of speech. These results
suggest that in aphasia, gesture tends to degrade with speech,
rather than it being compensatory. This implies that the
processes underlying speech and gesture production may be
tightly linked or shared.

Gesture Production and Aphasia

Keywords: Aphasia, Gesture.

Introduction
Gesture and Speech Production
When speaking, people oftentimes produce hand gestures,
which are closely linked to their speech temporally (Chui,
2005), structurally (Kita & Özyürek, 2003), and
semantically (e.g. McNeill, 2005). For example, when
asking a sales clerk for a sweater, gestures may indicate that
we prefer a V-neck, a large front pocket, or one just like the
one we are wearing. Both the production of speech and the
production of gestures seem to be part of a speaker’s
communicative effort (Kendon, 2004). Although different
functions of gesture have also been recognized, such as
facilitating speech production (Krauss, 1998) and supporting
cognition (Melinger & Kita, 2007), much empirical
evidence has been gathered for the idea that gestures are
communicative and are intended as such (e.g. Alibali,
Heath, & Myers, 2001; Beattie & Shovelton, 1999).
McNeill (2005) argued that speech and gesture co-express
idea units, which develop themselves into utterances. That
is, that they are two sides of the same coin. In support of this
idea, So, Kita, and Goldin-Meadow (2009) found that if
information was lacking in speech, it tended to be missing in
gesture as well. However, Melinger and Levelt (2004) found
that speakers sometimes divide the content of their message
across gesture and speech. They found that if critical spatial
information was expressed in gesture, it was more likely to
be omitted in speech. This goes well with the idea that

In light of the question of whether gesture and speech
compensate for one another, it is interesting to study what
happens to gesture when speech breaks down, such as in
aphasia. Aphasia is an acquired language disorder caused by
brain damage. It not only affects verbal expression, but has
an impact on all language modalities. In our current study
we focus on aphasic people who have severe to mild
problems expressing themselves verbally.
Numerous studies have shown that aphasic people still
gesture spontaneously and frequently (Rose, 2006). People
with fluent aphasia may even gesture more informatively
than non-aphasic speakers (Carlomagno, Pandolfi, Martini,
Di Iasi, & Cristilli, 2005). Case studies and clinical
experience confirm that some aphasic speakers use gesture
effectively to communicate (e.g. Goodwin, 2002). This
suggests that they may be able to partly compensate for their
speech impairment with gesture. Yet does this mean their
gesturing is unimpaired?
Studies that looked at gesturing by people with aphasia
have mostly used the gesture coding scheme developed by
McNeill (2005). For example, Carlomagno et al. looked at
the informativeness of iconic gestures, which are gestures
that mostly depict entities or movements. Yet when
producing an iconic gesture, there are still different ways in
which we can depict (Cienki & Müller, 2008). For example,
if we want to depict a sweater, we can outline the shape of
it, or we can pretend to put it on. And if we are talking about
a car, we can move our hands as though steering it, or we
can let our hand represent the car, depicting its path with our
hand movement. So there is more to say about a gesture’s

1454

form than just that it is an iconic gesture. And being able to
produce a meaningful iconic gesture does not mean that all
these different representation techniques are intact.
Therefore, to know whether gesture is impaired in aphasia,
we need to study both its meaning and its form, and we need
to compare aphasic speakers to non-aphasic controls.
Cocks, Dipper, Middleton and Morgan (2010) drew a
detailed comparison between gestures produced by a
speaker (LT) with conduction aphasia and those of nonaphasic speakers. They found that LT’s gestures during
word finding problems differed from those accompanied
with fluent speech by herself and the control speaker. For
example, most of those gestures outlined shapes. They also
found that the differences in LT’s gesturing paralleled the
differences in her speech, suggesting that although LT could
still use gesture effectively, her gesture production was
impaired, much like her speech production. Cocks et al. call
for a study in which iconic gestures of a larger number of
aphasic and non-aphasic speakers are compared. This is
what we do in our current study.

Present Study
To assess whether or not gesture tends to be impaired in
aphasic speakers, we compare gestures by 26 people with
milder or more severe aphasia to those of 17 non-aphasic
controls. New to our approach is the combination of a
detailed gesture analysis with a larger number of aphasic
speakers. In addition, we not only compare gestures of
aphasic speakers to those of control speakers, but also to
gestures produced by controls when they were asked to
communicate by gesture alone. This gives us insight into
how people with an unimpaired gesture production system
would compensate for speech with gesture.
First, we look at the intelligibility of gestures. If aphasic
speakers compensate for speech with gesture, we expect
their gestures will be more informative than those of nonaphasic speakers, who can rely on speech more. Also, the
more impaired speech, the more informative gesture will be.
Alternatively, if speech and gesture are two sides of a coin,
and therefore also break down together, the opposite is
expected. We test this by means of three perception
experiments, in which we separately assess the
informativeness of verbal and nonverbal communication of
people with milder and more severe aphasia and healthy
controls, on an easier and harder communication task.
Second, we present a detailed analysis of the iconic and
deictic gestures produced by aphasics and controls, zooming
in on their representation techniques. If their gesturing is
unimpaired, the techniques used by aphasic speakers may
resemble the techniques used by non-aphasic speakers. If
aphasic speakers compensate for speech with gesture, the
techniques they employ may be similar to those of nonaphasics who are asked to communicate without speech. On
the other hand, if their gesturing is impaired, this may affect
some techniques more than others, and therefore aphasic

speakers may prefer different techniques than non-aphasic
speakers or gesturers, and there may be differences in the
techniques used by people with milder and more severe
aphasia.

Perception Experiments
Material
We used video clips of 26 native Dutch stroke patients with
aphasia (17 male). Types of aphasia included: Global (8),
Broca (2), Wernicke (3), Anomic (1), Conduction (1), and
non-classifiable (7). For 4 patients the type of aphasia was
not known. The mean age was 56 years, range 37 – 70. The
mean time post-onset was 24 months, range 1 – 152. All
patients gave their informed consent for the use of their data
for research purposes.
The patients were performing an experimental version of
the Scenario Test (Van der Meulen, Van de SandtKoenderman, Duivenvoorden, & Ribbers, 2009). This test
measures a person’s ability to functionally communicate, in
a dialogue setting. The clinician takes part in the
communication process and actively suggests the use of
alternative means of communication, such as gesture. We
used data from two subtasks. In the sweater task, the patient
is explained a scenario in which they are in a store and want
to buy a sweater. The clinician talks about a sales clerk
approaching and asking: “How may I help you?”. The
patient is then to communicate as though addressing the
sales clerk, for example by saying: “I would like to buy a
sweater”. In the accident task, the information to be
conveyed is more complex. The clinician explains a
scenario in which the patient witnessed an accident, in
which a car hit a biker. A police officer then approaches the
patient asking: “What happened?”. The patient is then to
explain what took place, as though addressing the officer.
Apart from the videos of aphasic speakers, we also used
video data of non-aphasic controls, who were matched for
age and educational level, and did the same test items with a
trained tester. They were allowed to speak on one subtask
(verbal control) and were asked to communicate using
gesture exclusively on the other (nonverbal control).
We cut out fragments of the videos of all people
performing the two subtasks, starting right after the final
question posed by the clinician, and stopping right before
the next change of turn. Out of these fragments, we made
three stimulus movies for our perception studies: one
containing all fragments of aphasic speakers, one containing
all fragments of the verbal controls, and one with all
fragments of the nonverbal controls. For the aphasic
speakers and the verbal controls, we created three versions
of these stimulus movies: one with just the video image and
no sound, one with sound and blank video, and one with
both image and sound. The clips of the nonverbal controls
were video image only, that is, without sound.

1455

Raters and Task

Table 1: Means and standard deviations of the ratio of
correct answers.

Raters were native Dutch students from Tilburg University.
They performed a forced choice task, in which they were
asked to judge whether the person in each clip of the
stimulus movie was communicating that they wanted to buy
a sweater, or that they had witnessed a car accident.
We did three separate perception studies, with different
raters. In the first study, we used the stimulus movies of the
aphasic speakers only. Raters saw the video clips without
sound, heard the audio clips without video, or saw and heard
the video clips with sound. The second perception study was
similar, but with the stimulus movies of the verbal controls
instead. Finally, we also did a perception test with the
stimulus movie of the nonverbal controls.

Analysis
Based on their score on the ANTAT test (Blomert, Koster,
& Kean, 1995), which is similar to the Scenario test but in
which only verbal communication attributes to a patient’s
score, the aphasic speakers were divided into two groups.
Speakers with a score below 30 (out of 10 – 50) were
labeled as speakers with severe aphasia, and speakers with a
score above 30 were labeled as speakers with mild aphasia.
Clearly, this division serves our statistical analysis rather
than it being meaningful at the level of an individual
speaker. There were 11 speakers in the mild aphasia group
and 15 in the severe aphasia group. Since we ran our
perception experiments separately, we present three separate
analyses of variance. For pairwise comparisons we used the
LSD method, with a significance threshold of .05. Our
dependent variable in each analysis is the ratio of correct
answers to all answers, averaged over raters.

Results
Table 1 shows the means and standard deviation of the ratio
of correct answers, for clips from each group of ‘speakers’,
for either task, and for each modality in which they were
shown to the raters. Performance at chance level would
render a score of .5. We first present an analysis of the study
with clips from the two groups of aphasic speakers. We
performed an ANOVA with Group (Severe aphasia, Mild
aphasia) and Task (Sweater, Accident) as within factors and
Modality (Visual, Audio, Audiovisual) as a between factor.
There were 15 raters in each cell, 45 in total.
All factors showed a main effect. The ratio of correct
answers was higher when judging speakers with mild
aphasia (M = .89) compared to speakers with severe aphasia
(M = .70), F(1, 42) = 205.70, p < .001. It was also higher
when judging clips from the accident task (M = .82) than of
the sweater task (M = .77), F(1, 42) = 10.16, p < .01.
Performance was worse with the visual presentation (M =
.66), compared to the audio-visual (M = .88) and audio
presentation (M = .85), F(2, 42) = 68.78, p < .001. The
difference between the latter two showed a trend towards
significance, p = .07.

Group
Severe
Aphasia

Task

Ratio Correct per Modality
Visual
Audio
AV

Sweater
Accident

.51 (.16)
.66 (.10)

.74 (.10)
.75 (.10)

.73 (.11)
.84 (.10)

Mild
Aphasia

Sweater

.79 (.13)

.90 (.05)

.96 (.07)

Accident

.70 (.14)

1.0 (.00)

1.0 (.00)

Verbal
Control

Sweater

.78 (.14)

1.0 (.00)

1.0 (.00)

Accident

.74 (.11)

.99 (.03)

1.0 (.00)

Nonverbal
Control

Sweater

.95 (.06)

-

-

Accident

.90 (.06)

-

-

The interaction between Group and Modality was not
significant, F < 1. There was a three-way interaction
between Group, Task and Modality, F(2, 42) = 14.01, p <
.001. Gestures of speakers with mild aphasia were
particularly more informative than those of people with
severe aphasia on the sweater task, whereas the difference in
informativeness of speech was larger on the accident task.
Our next analysis compares the judgment of clips from
speakers with mild aphasia to that of clips from the controls
when they were allowed to speak (verbal controls). We used
an ANOVA with Task as a within factor and Group and
Modality as between factors. For clips from aphasic
speakers, there were 15 raters per cell, and for clips from
non-aphasic speakers there were 16 raters per cell, summing
up to 93 raters in total.
There was a main effect of Group, F(1, 87) = 4.05, p <
.05. The ratio of correct answers was higher when judging
clips from the verbal controls (M = .92) compared to those
of speakers with mild aphasia (M = .89). There also was a
main effect of Modality, F(2, 87) = 115.78, p < .001.
Performance was worse in the visual modality (M = .75),
compared to the audio (M = .97) and audiovisual modality
(M = .99). The interaction between Group and Modality was
not significant, F < 1.
There was a two-way interaction between Modality and
Task, F(2, 87) = 15.75, p < .001. In the visual modality,
performance was slightly better on the sweater task, whereas
in the audio modality it was slightly better on the accident
task. There was a three-way interaction between Group,
Modality, and Task, F(2, 87) = 7.09, p < .001. When
judging aphasic speakers, raters experienced a benefit from
access to visual information on top of audio information for
the sweater task. There was no such benefit for the accident
task, or when judging verbal controls, because performance
on the audio only clips was already at ceiling.
Lastly, we present an analysis comparing the judgment of
visually presented clips of the controls when they could
speak and when they could not speak (nonverbal controls).
There were 16 raters in each cell, 32 in total. Task was again
the only within factor. There was a main effect of Group,

1456

F(1, 30) = 24.84, p < .001. The ratio of correct answers was
higher for clips of nonverbal controls (M = .93) compared to
clips of verbal controls (M = .77). We did not find a main
effect of Task, F < 1, but there was an interaction between
Group and Task, F(1, 30) = 8.85, p < .01. For verbal
controls, performance was better on clips of the sweater task
whereas for nonverbal controls performance was better for
clips of the accident task.

Discussion
Clips from speakers in the mild aphasia group were judged
more accurately than clips from speakers in the severe
aphasia group for audio, video, and audiovisual clips. This
indicates that nonverbal communication may break down
with verbal communication, rather than it taking on the role
of verbal communication. This is confirmed by the fact that
clips from verbal controls were in turn judged better than
those of the mild aphasia group, independent of whether
they were presented visually, auditory, or audiovisually.
The almost perfect scores on the clips of nonverbal
controls show that, in principal, gesture can largely
compensate for speech on this simple judgment task. It
therefore seems that people with (severe) aphasia cannot use
gesture as freely as healthy controls to compensate for
speech. Yet although generally the audio information was
more informative than the visual information, the
audiovisual presentation sometimes rendered still higher
scores. This shows that information in gesture and speech
was not fully redundant either. For some aphasic speakers,
seeing them too was apparently more informative than just
hearing them. This may mean that gesture did take on some
of the communicative burden.
Seeing a speaker of course provides more information than
just gestures. We think however that gesture was the most
important nonverbal cue in our clips. Since many people
hardly spoke intelligibly, lip movements for instance were
not very informative.

Gesture Analysis
We coded the (co-speech) gestures in each of the clips used
in our perception studies, starting with the scheme by
McNeill (2005). We coded all movements of the hands that
seemed relevant to the communication task and that cooccurred with speech. Since the gestures, or rather
pantomimes, of the nonverbal controls always occurred
without speech, these were coded despite the absence of
speech. We currently focus on representational gestures,
that is, gestures referring to the content of the message being
conveyed. In our current sample these consisted of iconic
and deictic gestures. Deictic gestures for example include
locating objects in the gesture space and pointing gestures.
Based on work by Müller (2008), we further coded all
iconic gestures into three categories, based on the
representation technique used to depict. Gestures that
outlined something in the gestures space, either by showing

its contour (2D) or molding its shape (3D) were labeled as
outlining/molding, for example drawing the outline of a
sweater in the air. Gestures that depicted the handling of a
virtual object, such as holding the hands up as if using a
steering wheel to depict a car, were labeled as handling.
Gestures in which the hands represented an object, or in
which the entire body depicted the body of another person
were labeled as object/enact. Examples are moving an
upright hand forward and then flipping it horizontally, to
depict that a biker fell, or shifting the upper body from a
vertical to a horizontal position, depicting the same event.
Although theoretically possible, we found it too opaque to
code deictic gestures into these categories. Therefore, such
gestures were only labeled as deictic.

Analysis
We conducted 4x2 ANOVAs with Group (levels: Severe
aphasia, Mild aphasia, Verbal control, Nonverbal control)
and Task (levels: Sweater, Accident) as fixed factors.
Pairwise comparisons were done using the LSD method,
with a significance threshold of .05. Our dependent
variables are the mean proportion of gestures of a certain
category that ‘speakers’ in a certain group produced. This is
because we are interested in the extent to which the different
representation techniques are used by each group, rather
than in overall differences in gesture frequency.

Results
Table 2 provides an overview of the proportion of gestures
produced of each type, by each group of participants on
either task. Table 2 also shows the mean number of gestures
produced of these types combined. Overall, more gestures
were produced on the accident than on the sweater task, F(1,
78) = 13.09, p < .001. There also was a main effect of group
F(3, 78) = 8.42, p < .001. The two groups of aphasic
speakers did not differ significantly in the mean number of
representational gestures produced. They produced more
gestures than the verbal controls and fewer than the
nonverbal controls.
Outlining/molding gestures were produced more with the
sweater task than with the accident task F(1, 65) = 4.18, p <
.05. Although there was no main effect of Group, post hoc
analysis showed that people with severe aphasia produced a
larger proportion of outlining/molding gestures than people
with mild aphasia. There was no significant interaction
between Group and Task, yet on the accident task, the
severe aphasics also produced significantly more outlining/
molding gestures than the verbal and nonverbal controls.
Handling gestures were produced more on the sweater
than on the accident task, F(1, 65) = 9.92, p < .01. This is
because the nonverbal controls were the only group who
made considerable use of these gestures on the accident
task, significantly more so than any other group. For the
sweater task, there were no significant differences in the
proportion of handling gestures between the groups.

1457

Table 2: Means and standard deviations of the proportion of each gesture type, for each group and either task.
Proportion of gestures per Group and Task
Severe Aphasia
Sw
N=15

Acc
N=15

Mild Aphasia
Sw
N=11

Acc
N=11

Verbal Control
Sw
N=8

Acc
N=9

NonVerbal Control
Sw
N=9

Acc
N=8

Outlining/Molding .33 (.36) .30 (.42)

.11 (.20) .11 (.16)

.33 (.47) .00 (.00)

.29 (.27) .06 (.12)

Handling

.09 (.22) .03 (.06)

.13 (.35) .00 (.00)

.33 (.47) .00 (.00)

.23 (.22) .12 (.15)

Object/Enact

.00 (.00) .05 (.16)

.00 (.00) .09 (.14)

.00 (.00) .00 (.00)

.05 (.10) .48 (.15)

Deictic

.58 (.40) .61 (.41)

.77 (.37) .80 (.24)

.33 (.47) 1.0 (.00)

.43 (.17) .34 (.16)

Mean N Gestures

2.3 (2.6) 4.6 (4.1)

3.0 (3.4) 5.7 (3.8)

.88 (1.1) 2.1 (1.8)

4.9 (1.5)

Object/enact gestures were produced more on the accident
task than on the sweater task, F(1, 65) = 29.08, p < .001.
There also was a main effect of Group, F(3, 65) = 21.09, p <
.001, and significant interaction between Group and Task,
F(3, 65) = 13.25, p < .001. The nonverbal controls produced
a larger proportion of object/enact gestures than all other
groups. This difference was larger on the accident task.
Deictic gestures were produced more on the accident task,
F(1, 65) = 4.34, p < .05. There also was a main effect of
Group, F(3, 65) = 4.86, p < .01 and a significant interaction,
F(3, 65) = 3.52, p < .02. Post hoc analysis showed that on
the accident task, the nonverbal controls produced smaller
proportions of deictic gestures than any other group, and the
verbal controls produced more deictics than the severe
aphasics. On the sweater task, there were no significant
differences between the groups, though the moderate
aphasics tended to produce more deictics than the verbal and
nonverbal controls, p values < .06.

Discussion
Clearly, the aphasic speakers were not using the same
techniques to depict in gesture as the healthy controls who
were not allowed to speak. This was most apparent on the
accident task. While the nonverbal controls made frequent
use of object/enact and handling gestures, the aphasics
hardly used these techniques when trying to describe the car
accident. For example, the nonverbal controls used their
hand to represent a biker that first drove and then fell (the
hand changing orientation), or they pretended to be the biker
that fell, moving their upper body sideways. Aphasic
speakers did not tend to use these object/enact techniques.
Also, the nonverbal controls held their hands as though
steering a car or a bike (handling). In our data sample, the
aphasic speakers never did this. So the aphasic speakers did
not make use of the techniques of object/enact and handling
to compensate for their speech impairment, despite these
techniques being very suitable to replace speech.
Both the verbal and the nonverbal controls produced a
considerable proportion of outlining/molding gestures on
the sweater task, indicating that on this task, this technique
is suitable for producing co-speech gestures as well as to

11 (8.9)

replace speech. Many people were outlining features of a
sweater, such as a V-neck or sleeve length, with respect to
their own body. Both groups of aphasics also used this
technique on the sweater task, showing some similarity with
the controls in the representation techniques used.
However, neither control group used outlining/molding
much on the accident task. The nonverbal controls hardly
used molding gestures to depict vehicles like cars, or bikes.
Yet the aphasics did sometimes do this, instead of using
techniques like object/enact or handling, like the nonverbal
controls did. This may indicate that outlining/ molding was
the only way of depicting in gesture that was available to
most aphasics. The severe aphasics made more use of
outlining/molding gestures than the mild aphasics, which
may indicate a greater need to depict in gesture, possibly
due to more word finding problems.
It thus may be the case that most aphasic speakers were
unable to use the techniques of handling and object/enact to
depict on the accident task. However, the verbal controls did
not use these techniques on the accident task either.
Therefore, given the task, these techniques may be more
common for gestures replacing speech (pantomimes) than
for co-speech gestures. It would be interesting to test
whether aphasics can make use of these techniques when
asked to pantomime. Although many aphasics were
unsuccessful in explaining the accident scenario verbally,
their attempts at speaking may have caused them to produce
co-speech gestures rather than pantomimes. Our current data
do not reveal whether aphasics would use different
techniques when using pantomime.

General Discussion
Our perception studies showed that gestures produced by
speakers with aphasia were less informative than gestures
by non-aphasic speakers and by non-aphasics who used
gesture instead of speech. Moreover, gestures by people
with more severely impaired speech were less informative
than those of people with milder speech impairment. It
therefore seems that aphasic speakers could not compensate
for their impaired expressivity in speech by gesturing.

1458

Our analysis of gesture form showed that most people with
aphasia may not be able to use all possible techniques for
depicting in gesture freely. It seems that especially
techniques which require access to conceptual knowledge of
the thing depicted (object/enact and handling), were used
relatively little by people with aphasia, while techniques
using perceptual features (outlining/molding) were still
available. There may thus be a problem translating
conceptual knowledge into uttered speech and gesture (see
McNeill & Duncan, 2010).
The finding that people with severe aphasia predominantly
use outlining/molding to depict in gesture is consistent with
the case study by Cocks et al. (2010), who found that LT
used this type of gesturing frequently with difficulties in
speech. This finding could be of use in clinical settings. For
example, such gestures may be particularly suitable for
training purposes. Also, it may facilitate understanding
when others are aware that aphasic speakers use these
gestures more widely than non-aphasic speakers.
Our studies into the informativeness of gesture, and our
analysis of gestural representation techniques both suggest
that like speech, gesture is impaired in most people with
aphasia. It therefore seems that gesture and speech
production are likely to break down together. This makes it
likely, though not necessary, that the processes of speech
and gesture production draw on many of the same resources,
and share an underlying process (McNeill, 2005). Although
further research is needed to study the links between gesture
and speech production, our study contributes to the
accumulating evidence that these links are tight, rather than
gesture and speech production largely being separate
processes. This unfortunately limits aphasic speakers’
ability to communicate through co-speech gestures. Despite
these limitations, some of the gestures they produce are
informative, and add information on top of speech.

Acknowledgements
We gratefully acknowledge all speakers for allowing us to
analyze their data, Renske Hoedemaker for collecting the
data of our control group, and Hans Westerbeek, Hanneke
Schoormans, and Manon Yassa for their help in the
perception studies.

References
Alibali, M. W., Heath, D. C., & Myers, H. J. (2001). Effects
of visibility between speaker and listener on gesture
production: Some gestures are meant to be seen. Journal
of Memory and Language, 44, 169-188.
Beattie, G., & Shovelton, H. (1999). Mapping the range of
information contained in the iconic hand gestures that
accompany spontaneous speech. Journal of Language and
Social Psychology, 18, 438-462.
Blomert, L., Koster, C., & Kean, M. L. (1995). AmsterdamNijmegen Test voor Alledaagse Taalvaardigheden
(ANTAT). Lisse: Swets & Zeitlinger.

Carlomagno, S., Pandolfi, M., Martini, A., Di Iasi, G., &
Cristilli, C. (2005). Coverbal gestures in Alzheimer's type
dementia. Cortex, 41(324-329).
Chui, K. (2005). Temporal patterning of speech and iconic
gestures in conversational discourse. Journal of
Pragmatics, 37(6), 871-887.
Cienki, A., & Müller, C. (2008). Metaphor, gesture, and
thought. In R. W. Gibbs (Ed.), The Cambridge Handbook
of Metaphor and Thought. Cambridge: Cambridge
University Press.
Cocks, N., Dipper, L., Middleton, R., & Morgan, G. (2010).
What can iconic gestures tell us about the language
system? A case of conduction aphasia. International
Journal of Language & Communication Disorders, Early
Online Article, 1-14.
De Ruiter, J. P., Bangerter, A., & Dings, P. (in press). The
interplay between gesture and speech in the production of
referring expressions: Investigating the tradeoff
hypothesis. Topics in Cognitive Science
Goodwin, C. (Ed.). (2002). Conversation and Brain
Damage. Oxford: Oxford University Press.
Kendon, A. (2004). Gesture: Visible action as utterance.
Cambridge: Cambridge University Press.
Kita, S., & Özyürek, A. (2003). What does cross-linguistic
variation in semantic coordination of speech and gesture
reveal?: Evidence for an interface representation of spatial
thinking and speaking. Journal of Memory and Language,
47, 16-32.
Krauss, R. M. (1998). Why do we gesture when we speak?
Current Directions in Psychological Science, 7, 54-60.
McNeill, D. (2005). Gesture and Thought. Chicago and
London: University of Chicago Press.
McNeill, D., & Duncan, S. (2010). Gesture and growth
points in language disorders. In J. Guendouzi, F. Loncke
& M. J. Williams (Eds.), The handbook of psycholinguistic
and cognitive processes. New York, London: Psychology
Press.
Melinger, A., & Kita, S. (2007). Conceptualisation load
triggers gesture production. Language and Cognitive
Processes, 22(4), 473-500.
Melinger, A., & Levelt, W. J. M. (2004). Gesture and the
communicative intention of the speaker. Gesture, 4(2),
119-141.
Rose, M. L. (2006). The utility of arm and hand gestures in
the treatment of aphasia. Advances in Speech-Language
Pathology, 8(2), 92-109.
So, W. C., Kita, S., & Goldin-Meadow, S. (2009). Using the
hands to indentify who does what to whom: Gesture and
speech go hand-in-hand. Cognitive Science, 33, 115-125.
Van der Meulen, I., Van de Sandt-Koenderman, W. M. E.,
Duivenvoorden, H. j., & Ribbers, G. M. (2009).
Measuring verbal and non-verbal communication in
aphasia: reliability, validity, and sensitivity to change of
the Scenario Test. International Journal of Language &
Communication Disorders, 1-12.

1459

