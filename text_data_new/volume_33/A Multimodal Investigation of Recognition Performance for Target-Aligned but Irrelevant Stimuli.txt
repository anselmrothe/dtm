UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Multimodal Investigation of Recognition Performance for Target-Aligned but Irrelevant
Stimuli

Permalink
https://escholarship.org/uc/item/4ws7s3tz

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)

Authors
Dewald, Andrew
Sinnett, Scott

Publication Date
2011-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

A Multimodal Investigation of Recognition Performance for Target-Aligned but
Irrelevant Stimuli
Andrew D. Dewald (adewald@hawaii.edu)
Department of Psychology, University of Hawaii at Manoa
2530 Dole Street, Honolulu, HI 96822
Scott Sinnett (ssinnett@hawaii.edu)
Department of Psychology, University of Hawaii at Manoa
2530 Dole Street, Honolulu, HI 96822
Abstract
Overtly presented, but ignored visual and auditory stimuli
presented within the same sensory modality are inhibited in a
later recognition task if previously presented synchronously
with an attended visual target (Tsushima, Sasaki & Watanabe,
2006; Tsushima, Seitz & Watanabe, 2008; Dewald, Doumas
& Sinnett, 2010; Dewald & Sinnett, 2011). We extend these
findings to conditions in which task irrelevant stimuli (written
or spoken words) were presented in a separate sensory
modality than task-relevant targets (picture or sound
repetitions). A subsequent recognition task was given for the
previously presented irrelevant stimuli (words). Words that
had been simultaneously presented with a target in the
previous repetition detection task were later recognized at
chance levels, demonstrating a bolstered recognition of taskirrelevant items (e.g. target-aligned words) when compared
with performance under unimodal presentation.
Key words: Attention, Multimodal Presentation, Recognition

Introduction
It has been demonstrated throughout cognitive psychology’s
history that attention is a limited resource (Broadbent, 1954;
Cherry, 1953; James, 1890; Mack & Rock, 1998; Rees,
Russell, Frith, & Driver, 1999; Sinnett, Costa & SotoFaraco, 2006; Triesman, 1960). Interestingly, the capacity of
the attentional system seems to be modulated if a difficult
unisensory task is divided across multiple sensory
modalities (i.e., a multiple resources theory, see Wickens,
1984). For instance, Sinnett et al. (2006) showed that under
multimodal presentations, inattentional blindness for words
was ameliorated (i.e., perception improved) when compared
with unimodal conditions, regardless of the modality of
word presentation (see also Toro, Soto-Faraco, & Sinnett,
2005 for a similar example involving statistical learning).
These findings seem to provide support for an attentional
system that is segregated, such that each sensory modality
has access to individualized attentional resources (see
Wickens, 1984).
Providing support for a multiple resource view, Duncan,
Martens, and Ward, (1997) demonstrated that participants
had difficulty identifying target items presented 300 ms or
less after previous targets under unimodal presentations
(auditorily or visually), due to the well-documented

phenomenon termed the attentional blink (AB; see Shapiro,
1992). However, when targets were presented across
sensory modalities (that is, when first identifying a visual
target and then an auditory target, or vice versa), typical AB
effects were not observed. That is, participants were able to
detect an auditory or visual target even when it was
immediately preceded by a target occurring in the other
modality (i.e., visually or auditorily respectively), whereas
they were unable to do so under unimodal presentations.
This finding suggests that attentional costs are reduced if a
difficult task is divided across sensory modalities.
Further investigating this issue, Sinnett et al. (2006)
explored a multiple resource view of attention by means of
an inattentional blindness (IB) paradigm. IB is a wellstudied phenomenon in attention research that illustrates a
situation in which an individual fails to detect an explicitly
presented event due to attention being directed elsewhere
(Mack & Rock, 1998; Simons & Chabris, 1999). The
overwhelming conclusion of IB research is that if attention
is deployed to a difficult or demanding primary task,
information not relevant to the primary task goes
unprocessed, at least not to levels of explicit awareness. A
curious finding by Sinnett et al. (2006), however, was that
when irrelevant information was presented in a sensory
modality separate from the target information, levels of IB
was reduced for irrelevant items (i.e., performance
improved when compared to unimodal presentations).
Sinnett et al. (2006) utilized an IB paradigm (see also
Rees et al., 1999) incorporating multisensory presentations
in which participants detected immediate repetitions in a
stream of rapid serially presented items in either visual,
auditory, or bimodal conditions. In the unimodal conditions
(visual only or auditory only streams) the primary task was
to monitor either pictures (or sounds) presented
simultaneously with written words (or spoken, respectively)
and detect immediate repetitions in either the word or the
distractor stream. Immediately following this task, a
surprise word recognition test was administered for
unattended words (after having attended to pictures or
sounds). In the crossmodal condition words were presented
either visually or auditorily, with a distracting stream of
either overlaid/superimposed sounds or pictures in the
opposite modality. Unimodal performance (both auditory

1164

and visual) yielded high and comparable levels of IB.
However, when attention was divided across modalities
(that is, attending to visual pictures while ignoring spoken
words, or attending to sounds while ignoring written words),
participants performed significantly better in subsequent
word recognition tests for the unattended words. Thus,
despite the accepted notion that attentional capacity is
limited, recognition performance for irrelevant stimuli can
often be improved as long as it had been presented in a
separate sensory modality (e.g., Duncan et al., 1997; Sinnett
et al., 2006; Wickens, 1984).
Although a plethora of scientific evidence suggests that
stimuli that receive attention are more efficiently processed
than stimuli that go unattended (Ahissar & Hochstein, 1993;
Mack & Rock, 1998; Sinnett et al, 2006; Spence & Squire,
2003) a number of investigations have demonstrated that
unattended information can nevertheless be processed and
affect behavior. However, the findings from these
investigations fail to yield a clear picture as to the degree to
which unattended stimuli can influence behavioral
processing. That is, the nature of these effects has ranged
from facilitation to inhibition depending on whether the
unattended stimuli were presented above or below
threshold. Moreover, a critical relationship between whether
or not the irrelevant stimuli occur synchronously with a
relevant target has been recently uncovered.
Watanabe, Náñez, and Sasaki (2001, see also Seitz &
Watanabe, 2003; 2005) demonstrated significant perceptual
learning enhancements in the absence of focused attention
for stimuli that were presented below the threshold from
visual awareness (i.e. implicitly presented). However, when
using explicit stimuli, Tsushima and colleagues (Tsushima,
Sasaki & Watanabe, 2006; Tsushima, Seitz & Watanabe,
2008) demonstrated a later inhibition. Accordingly,
behavioral facilitation or inhibition appears to be partly
dependent on whether or not stimulus presentation is sub- or
superthreshold. Critically, the facilitatory and inhibitory
effects in both of these examples appear to be contingent on
the temporal relationship between the irrelevant stimulus
and an attended target in a separate task. That is,
performance changes for irrelevant stimuli were observed
only when temporally aligned with relevant stimuli (i.e.,
task targets). For instance, Tsushima and colleagues
(Tsushima et al., 2006, 2008) demonstrated that the
detection of ignored, but explicitly presented coherent
motion displays, is inhibited when the motion display is
temporally aligned with the presence of an attended taskrelevant target in a simultaneously presented task. The same
inhibition was not observed for non-aligned presentations,
and was further supported by functional magnetic resonance
imaging (fMRI) data showing an inhibition in brain activity
in brain areas associated with processing motion direction
(Tsushima et al., 2006).
We have recently published behavioral data also showing
an inhibition for temporally aligned, but irrelevant stimuli
(Dewald et al., 2010). In this example we modified the
paradigm utilized by Sinnett et al (2006) investigating

inattentional blindness to include an additional analysis for
items that had appeared simultaneously with targets in the
separate task. When doing so, an inhibition for visually
presented words (explicitly presented) was observed. In an
ensuing investigation, the same IB paradigm was adapted to
auditory presentations in which spoken words were overtly
presented at the same time as common everyday sounds,
with the primary task to detect target repetitions in the
sound stream, and the secondary task to later recognize the
previously ignored words (Dewald & Sinnett, submitted).
Again, the findings demonstrated that akin to visually
presented words, word recognition was inhibited for spoken
words that had previously been temporally aligned with
sound repetitions.
In the present investigation, we extend these unimodal
examples of inhibited performance for task irrelevant but
target-aligned stimuli to multimodal presentations. As
increased performance has been observed for such
presentations (see Duncan et al., 1997; Sinnett et al., 2006),
we would expect that previously documented inhibition
might disappear, or perhaps even lead to enhanced
recognition performance for task-irrelevant words, as long
as they had previously been presented with a target
repetition. To address this, we presented participants with
multisensory visual and auditory streams (adapted from
those used in the unimodal conditions in Dewald et al., 2010
and Dewald & Sinnett, 2011). Here, one of the streams
included spoken words with distracting pictures, and the
other had written words with distracting sounds. The task
was to respond to repetitions in the target stream (i.e.,
sounds or pictures) and then to subsequently recognize as
many words that had been previously presented (i.e.,
ignored) in the repetition detection task.

Method
Participants. Sixty participants (n=60) were recruited from
the University of Hawai’i at Manoa in exchange for course
credit. A total of 30 participants were used for each
condition (visual words and sounds or auditory words and
pictures). Participants were naïve to the experiment and had
normal or corrected to normal vision and hearing. Written
informed consent was obtained before participation in the
experiment occurred.
Materials. The multimodal streams were concatenated using
the same stimuli as used in the visual (Dewald et al., 2010)
and auditory (Dewald & Sinnett, 2011) experiments that
previously showed inhibitory results. A total of 150 one to
two syllable, high-frequency English words (average length
of 5 letters) were selected from the MRC psycholinguistic
database (Wilson, 1988). The overall average frequency of
the 150 selected words was 120 per million, ranging
between 28 and 686. The words were presented either
visually (Arial font at a size of 24 points) or auditorily. For
the auditory presentation, a native English speaker’s voice
was recorded reading the list of selected words three times,

1165

after which three blind listeners chose the best exemplar of
each spoken word. In the event that the three exemplars of a
specific word were chosen by the listener, a fourth listener
was asked to decide which one was best. The selected
recordings were edited using sound editing software so as to
all contain the same length of presentation length (350 ms)
and average amplitude.
A total of 100 pictures were selected from the Snodgrass
and Vanderwart (1980) picture database. The pictures (on
average 5 to 10 cm’s) were randomly rotated +/-30 degrees
from upright so as to ensure task difficulty (see also Rees et
al., 1999). A database of 100 familiar sounds were edited to
350 ms and for average amplitude and served as the
auditory analog of the visual pictures in the visual stream.
The exact same stimuli and design to create streams were
used here as in Dewald et al. (2010, for visual stimuli) and
Dewald & Sinnett (2011, for auditory stimuli). The 100
sounds/pictures were randomly separated into two equal
groups, while the 150 words (both visual and auditory) were
randomly divided into three equal groups (similar average
frequency). In each group of sounds/pictures, half (25) were
pre-selected and duplicated. These repeated sounds/pictures
acted as targets as each pair occurred in the auditory/visual
presentation as an immediate repetition. The remaining 25
sounds/pictures were also duplicated, but their positioning
in the stream never allowed for an immediate repetition.
One hundred of the 150 words were overlaid/superimposed
on each of the sounds/pictures, creating a block size of 100
sound-word/picture-word items. Across two blocks of
presentation, half of these words (i.e., 50) were targetaligned with a sound/picture repetition while the other half
were non-aligned. Each block of 100 items was created in
which the 25 sounds/pictures not immediately repeated in
the first block now served as the sounds/pictures that were
immediately repeated, with the same 100 randomized words
superimposed as in the first block (note, an
overlaid/superimposed word was never repeated within a
block). Therefore, across both blocks in each experiment,
each sound/picture was played/displayed a total of four
times (once as a repeat and then two other times as nonrepeats in the complementary block). The words were
presented a total of two times throughout the experiment,
once in each block respectively.
The same principle was used when making streams of
items when the words (both written and spoken) were
repeated (attending to words condition). As there were 150
words and 100 sounds/pictures six different versions of the
sound-word superimposed stimuli were created for use in
the attending to sounds condition as well as the attending to
words condition.
Participants were administered a surprise recognition test
immediately following the repetition detection task. This
task consisted of 100 words from both the previously
heard/viewed stream (50) as well as never heard or seen
before foil words (50). These words were used in a different
version of the experiment (fully randomized). The 50 nonfoil words (i.e., words that had been presented) in the

surprise recognition test were words that had either been
temporally aligned with the task-relevant target, (i.e., targetaligned), or had not been temporally aligned with the taskrelevant target (i.e., non-aligned) in the previous repetition
detection task. The surprise word recognition tasks were
randomly presented by DMDX software, one at a time,
written in bold, capitalized letters in Arial font at a size of
24 points (see also Dewald et al., 2010; Dewald & Sinnett,
2011; Sinnett et al., 2006 for a similar design). An
analogous version of the experiment was created where the
repeated targets were words rather than sounds. All word
repetitions followed this design. Care was taken to ensure
that sound-word combinations did not have any semantic
relationship.
Power analysis. An a priori power analysis indicated that a
minimum of 10 subjects in each condition would yield a
95% confidence for detecting a medium sized effect when
employing the traditional .05 criterion of statistical
significance. As we predict a possible amelioration of the
inhibition witnessed in Dewald et al (2010) and Dewald &
Sinnett (2011), it is important that there is sufficient power.

Procedure
Participants were randomly assigned and completed only
one of the repetition detection tasks. That is, half of the
participants were given the stream of visual words and
auditory sounds, while the other half were given the stream
of visual pictures and auditory words. Importantly,
superimposed/overlaid irrelevant stimuli (visual or auditory
words), were presented in a different sensory modality from
the targets in the repetition detection task. The primary task
of detecting immediate repetitions in either the sound or
picture stream was presented as follows, respectively: a
visual–auditory condition with a visual word stream and an
auditory sound stream; and an auditory-visual condition
with spoken words and visual pictures. Participants were
required to detect immediate repetitions in either the sound
(or picture) or the word stream.
Participants were randomly assigned to either condition,
and then again randomly assigned to one of two attention
conditions. One group was required to attend and respond to
repetitions in the sound/picture stream (i.e., ignore the
words), while the other group was required to respond to
immediate repetitions in the spoken/written word stream and
ignore the sounds/pictures. Participants responded to
repetitions by pressing the ‘G’ key on the keyboard.
Each item in the sound/picture-word presentation was
presented for 350 ms with a 150 ms inter-stimulus interval
(ISI; silence/blank screen) between each item for a stimulus
onset asynchrony (SOA) of 500 ms. A repeatable training
block of eight trials was given before the experiment started.
Immediately after the repetition detection task the surprise
word recognition test was administered. Participants were
instructed to press the “V” key if they recognized the word
from the repetition detection task or instead the “B” key if
they did not see/hear the word before.

1166

Results
Target detection accuracy in the primary task.
An analysis of the overall accuracy (for both experiments
and across all conditions) of the primary task of immediate
target repetition detection revealed that participants were
accurate at detecting target repetitions in the primary task,
(78% hit rate vs. 22% miss rate, t(59)= 21.09, p<.001).

Visual words and auditory sounds
Overall visual surprise recognition performance. The
results of the surprise recognition test were compared
between conditions (attending sounds vs. attending written
words), and also against chance levels. Overall recognition
performance was significantly better after attending to the
written words when compared with after attending to the
sounds (63.1% SE=2.57 vs. 53.7%, SE=1.40, t(29)=2.25,
p<0.05). Additionally, recognition performance after
attending to the words was significantly better than chance
(t(29)=5.08 , p<0.001) while performance after attending to
the picture stream was not (t(29)=1.14, p= 0.26).
Target-aligned and non-aligned words. When attending to
written words in the repetition task (rather than sounds),
subsequent recognition for target-aligned as well as nonaligned words were both significantly better than chance
performance (target-aligned: 66.0%, SE=2.68, t(14)= 5.97,
p<.001; non-aligned: 61.3%, SE=4.45, t(14)= 2.55, p<.005).
Despite the 4.7% trend in the data for improved
performance with target-aligned words, there was no
significant difference between target-aligned and nonaligned word recognition (t(14)= .804, p=.44; see Figure 1).
Most importantly, the analysis of recognition
performance after attending to the sound stream confirmed
that participants were not better than chance at recognizing
non-aligned words (55.4%, SE=4.69, t(14)= 1.26, p=.228).
And critically, recognition performance was not
significantly different from chance for target-aligned words
(51.0%, SE=4.18, t(14)= .197, p=.847; see Figure 1).
Furthermore, when compared to each other, recognition for
non-aligned words was not significantly different from
target-aligned words (t(14)= -.762, p=.459) .

Figure 1. Recognition percentages for Target-Aligned (black bars)
and Non-Aligned (grey bars) words in the surprise word
recognition test after attending to either the visual word stream
(left) or the sound stream (right).

Visual pictures and auditory words
Overall visual surprise recognition performance. The
results of the surprise recognition test were analyzed in the
same manner as above. Overall recognition performance
was significantly better after attending to the spoken words
when compared with after attending to the pictures (68.4%,
SE=2.08 vs. 52.6%, SE=3.47, t(29)=-4.97, p<0.01).
Additionally, recognition performance after attending to the
spoken words was significantly better than chance
(t(29)=8.85, p<0.001) while performance after attending to
the picture stream failed to be significantly better than
chance (t(29)=.754 p= 0.457).
Target-aligned and non-aligned words. When attending to
spoken words in the repetition task, ensuing recognition for
target-aligned as well as non-aligned words was
significantly better than chance performance (targetaligned: 70.7%, SE=2.31, t(14)= 4.31, p<.001; non-aligned:
65.5%, SE=3.54, t(14)= 8.95, p<.001). Again, despite the
5.2% difference, there was no significant difference between
target-aligned and non-aligned word recognition
performance after attending to the words (t(14)= 1.04,
p=.316; see Figure 2).
Recognition performance after attending to the picture
stream showed that recognition of non-aligned words was
not better than chance (52.8%, SE=5.39, t(14)= .539,
p=.599). Moreover, analogous to the other condition (visual
words and sounds), recognition for target-aligned words
was not significantly different from chance (51.0%,
SE=4.81, t(14)= .206, p=.840; see Figure 2). There were no
significant differences when compared to each other (t(14)=
.272, p=.790).

1167

Figure 2. Recognition percentages for Target-Aligned (black bars)
and Non-Aligned (grey bars) words in the surprise word
recognition test after attending to either the spoken word stream
(left) or the picture stream (right).

Discussion
There are a number of key findings that merit discussion.
First, we have replicated previous findings on inattentional
blindness under multimodal presentations, showing that
performance after attending the distracting stream (either
sounds or pictures) lead to the inability of participants to
recognize previously presented words above chance levels.
This is a particularly relevant finding given that, in a
virtually identical task, Sinnett et al. (2006) did find above
chance results for both of these multimodal conditions. It is
difficult to speculate as to why we found contradicting
results, although, it should be noted that under certain
conditions equivalent levels of IB were observed when
comparing unimodal to crossmodal conditions in Sinnett et
al. (2006; see Experiment 2). Regardless of this difference,
it is apparent that attending to the words resulted in
enhanced word recognition levels. Furthermore, the present
experiment expands on Sinnett et al. by directly measuring
the fate of irrelevant stimuli presented simultaneously with
or without a task-relevant target.
Recall that Dewald et al (2010), showed support for a
possible inhibitory mechanism for overtly presented but
irrelevant visual information that appeared simultaneously
with an attended target within the same sensory modality
(recognition performance was 36%). Furthermore, Dewald
and Sinnett (submitted) extended this finding to auditory
presentations (recognition performance was 40%). In the
present study, the same inhibition was not observed.
Instead, performance for irrelevant words simultaneously
presented with an attended target in a separate modality
remained at chance levels (51% in both conditions). While a
confirmatory analysis across experiments was done, indeed
showing improved performance for multimodal conditions
when compared to unimodal conditions (both collapsed or
not across modalities), it should be acknowledged that the
unimodal experiments were conducted and reported
separately making a direct comparison challenging.

The present findings are of particular interest considering
the recent documented inhibition observed for targetaligned, explicitly presented superthreshold stimuli in
numerous investigations (Dewald et al., 2010; Dewald &
Sinnett, submitted; Tsushima et al., 2006, 2008). Recall that
Tsushima et al (2006) demonstrated that performance for
superthreshold motions that were simultaneously presented
with a task-target were later inhibited when compared to
motions not presented with a task-target. This finding was
further supported by word recognition performance in an
inattentional blindness paradigm showing inhibition for
previously aligned words in a repetition detection task; for
both visual (Dewald et al., 2010) and auditory (Dewald &
Sinnett, submitted) presentations. However, here, targetaligned irrelevant stimuli were recognized no differently
than non-aligned irrelevant stimuli in both experimental
conditions (Visual words: 55% Non-Aligned vs. 51%
Target-Aligned; Auditory Words: 52% Non-Aligned vs.
51% Target-Aligned), suggesting that the previously
observed inhibition seemingly disappears when attention is
divided across sensory modalities. This could arise due
possibly to the existence of individualized attentional
reservoirs for each sensory modality (see Wickens, 1984).
Interestingly, recent research by Swallow and Jiang
(2010) suggests an “attentional boost” (i.e., facilitation) for
simultaneously presented information in a dual-task
paradigm (see also Lin et al., 2010). Although in their
experiment, participants were required to divide their
attention across both streams, rather than only pay attention
to one of the streams, our results are somewhat analogous to
this notion. It is perhaps possible that by dividing the task
across modalities, an analogous boost emerges.
To conclude, the findings presented here provide an
outcome that aligns with most literature regarding divided
attention across sensory modalities. That is, while
attentional capacity is limited for stimulus processing within
the same sensory modality, tasks presented to different
sensory modalities may in fact increase that capacity by
enabling access to individualized reservoirs (e.g., Duncan et
al., 1997; Lavie, 2005; Sinnett et al., 2006; Wickens, 1984).
It seems to be that the additional attentional resources were
directed towards irrelevant stimuli and therefore,
recognition of the irrelevant stimuli was enhanced, or at the
very least, not inhibited.

References
Ahissar, M., & Hochstein, S. (1993). Attentional control of
early perceptual learning. Proceedings of the
National Academy of Science U.S.A, 90, 5718–
5722.
Bright, P., Moss, H., & Tyler, L. K. (2004). Unitary vs
multiple semantics: PET studies of word and
picture processing. Brain and Language, 89, 417432.
Broadbent, D.E. (1958). Perception and communication.
London: Pergamon Press.

1168

Britten, K.H., Shalden, M.N., Newsome, W.T., & Movshon,
J.A. (1992). The analysis of visual motion: A
comparison of neuronal and psychophysical
performance. Journal of Neuroscience, 12, 47454765.
Cherry, E. C. (1953). Some experiments on the recognition
of speech, with one and two ears. Journal of the
Acoustical Society of America, 25, 975–979.
Dewald, A.D., Sinnett, S., & Doumas, L.A.A. (2010). The
inhibition and facilitation of stimuli can be
modulated by the focus of direct attention.
Proceedings of the Twenty-Eight Annual
Conference of the Cognitive Psychology Society
Dewald, A.D., & Sinnett, S. (submitted). An inhibited
recognition performance for explicitly presented
target-aligned irrelevant stimuli in the auditory
modality. Proceedings of the Twenty-Ninth Annual
Conference of the Cognitive Psychology Society
Driver, J., & Spence, C. (2004). Crossmodal spatial
attention: Evidence from human performance.
In C. Spence & J. Driver (Eds.), Crossmodal space
and crossmodal attention. Oxford, UK: Oxford
University Press.
Fahle, M., & Poggio, T. (2002). Perceptual learning, The
MIT Press.
Lin, J.Y., Pype, A.D., Murray, & Boynton, G.M. (2010).
Enhanced memory for scenes presented at
relevant points in time. PLoS Biol, 8(3), E1000337.
Lupker, S. J. (1984). Semantic priming without association:
A second look. Journal of Verbal Learning and
Verbal Behavior, 23, 709–733.
Peterson, S.E., Fox, P.T., Posner, M.I., Mintun, M., &
Raichle, M.E. (1989). Positron emission
tomographic studies of the processing of single
words. Journal of Cognitive
Neuroscience,1(2), 153-170.
Rees, G., Russell, C., Frith, C. D., & Driver, J. (1999).
Inattentional blindness versus inattentional
amnesia for fixated but ignored words. Science,
286, 2504-2507.
Roelfsema, P. R., van Ooyen, A., & Watanabe, T. (2009).
Perceptual learning rules based on reinforces
and attention. Trends in Cognitive Sciences, 14(2),
64-71.
Ruz, M. A., Worden, M. S., Tudela, P.O. & McCandliss, B.
D. (2005). Innattentional amnesia to words in a
high attentional load task. Journal of Cognitive
Neuroscience,
17, 768-776.
Seitz, A. R., Kim, R., & Shams, L. (2006). Sound facilitates
visual learning. Current Biology, 16, 1422-1427.
Seitz, A. R. & Watanabe, T. (2003). Psychophysics: Is
subliminal learning really passive? Nature, 422,
36.
Seitz, A. R. & Watanabe, T. (2005). A unified model for
perceptual learning. Trends in Cognitive
Science, 9 (7), 329-334.

Seitz, A. R. & Watanabe, T. (2008). Is task-irrelevant
learning really task-irrelevant? PLoS ONE 3(11):
e3792
Sinnett, S., Costa, A., & Soto-Faraco, S. (2006).
Manipulating inattentional blindness within and
across sensory modalities. Quarterly Journal of
experimental Psychology, 59(8), 1425-1442
Snodgrass, J. G., & Vanderwart, M. (1980). A standardized
set of 260 pictures: Norms
for name
agreement, image agreement, familiarity, and
visual complexity.
Journal of Experimental
Psychology: Human Learning and Memory, 6, 1
74–215.
Spence, C., & Squire, S. (2003). Multisensory integration:
Maintaining the perception of synchrony. Current
Biology, 13, R519-R521.
Spence, C., & Driver, J. (1998). Cross-modal links in spatial
attention. Pilosophican Transactions of the Royal
Society :Biological Sciences, 352(1373), 13191331.
Stroop, J. R. (1935). Studies of interference in seriail verbal
reactions. Journal of Experimental Psychology,
18,643-662.
Swallow K. M., & Jiang, Y. V. (2010). The attentional
boost effect: Transient increases in attention to
one task enhance performance in a second task.
Cognition, 115, 118-132.
Tshushima, Y., Sasaki, Y., & Watanabe, T. (2006). Greater
disruption due to failure of inhibitory
control on an ambiguous distractor. Science, 314,
1786-1788.
Tsushima, Y., Seitz, A. R., & Watanabe, T. (2008). Taskirrelevant learning occurs only
when the
irrelevant feature is weak. Current Biology,18(12),
516-517.
Watanabe, T., Náñez,Y., & Sasak, S. (2001). Perceptual
learning without perception. Nature, 413, 844–
848.
Wilson, M. D. (1988). The MRC psycholinguistic database:
Machine readable dictionary, version 2.
Behavioural Research Methods, Instruments and
Computers, 20, 6-11.

1169

