UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Analyzing Conservative and Liberal Blogs Related to the Construction of the ‘Ground Zero
Mosque’

Permalink
https://escholarship.org/uc/item/64x320w7

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)

Authors
Dehghani, Morteza
Gratch, Jonathan
Sachdeva, Sonya
et al.

Publication Date
2011-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Analyzing Conservative and Liberal Blogs Related to the
Construction of the ‘Ground Zero Mosque’
Morteza Dehghani1 (morteza@ict.usc.edu), Jonathan Gratch1 (gratch@ict.usc.edu),
Sonya Sachdeva2 (sachdeva@northwestern.edu), Kenji Sagae1 (sagae@ict.usc.edu)
1

Institute for Creative Technologies, University of Southern California,
1205 Waterfront Dr., Playa Vista, CA 90094-2536, USA
2

Department of Psychology, Northwestern University
2029 Sheridan Road, Evanston, IL 60208-2710, USA

Abstract
The issue of the „Ground Zero Mosque‟ has been one of the
most controversial political issues in US politics in the last
several years. Using two different statistical text-analysis
techniques, we analyze conservative and liberal blog posts,
related to the construction of this Muslim community center
and the debates surrounding the issue. In the first experiment,
we use a machine learning technique to automatically classify
the blogs according to which group wrote them. We also
examine the distinctive features that make these blogs liberal
or conservative. In the second experiment, by examining
posts in consecutive time blocks, we show that there was a
significant increase over time in affective processing, and in
anger, especially for conservatives. Overall, our results show
that there are significant differences in the use of various
linguistic features between liberals and conservatives,
highlighting the differences between the ideologies and the
moral frameworks of the two groups.
Keywords: blog analysis; text classification; sacred rhetoric;
LIWC; machine learning; SVM; Ground Zero mosque

Introduction
There is evidence that striking differences in the very
definitions of morality are at the root of many socialideological differences within a country. Haidt and Graham
(2007) propose that liberals and conservatives in the US
have very different ways of seeing the social environment
around them, and rely on distinct moral structures and
ideologies. Consequently, several important differences
have been noted in the political rhetoric employed by these
groups (Lakoff 2002, 2008; Marietta 2008, 2009). Lakoff
(2008) argues that the type of language used in political
discussions is of utmost importance because it “is far more
than a means of expression and communication… It
organizes and provides access to the system of concepts
used in thinking” (p. 231). In other words, the language in
these discussions often conveys the value systems adhered
to by liberal and conservative groups. A linguistic study of
presidential debates from 1976-2007 (Marietta, 2009)
reveals that Republicans employed sacred rhetoric, which is
grounded in “transcendent authority and moral outrage”,
more frequently and on a broader range of issues, while
Democrats relied more on quantitative facts such as plans
and projected numbers.

The issue of the Cordoba Muslim Community Center,
known as Park51 or „the Ground Zero Mosque‟ as it came to
be called, has been one of the most contentious political
issues in the United States in the past five years or so. It
served to highlight the ideological differences between
liberal and conservative moral frameworks and to a certain
extent, exposed the deep prejudices that still remain toward
the Muslim community. This paper focuses on the
differences in the use of linguistic features in over 3000
conservative and liberal blog posts related to the
construction of Park51.
We first explore whether the differences in the choice of
words used by conservative and liberal bloggers with regard
to this issue is significant enough that classifiers can be
trained to automatically categorize posts as conservative or
liberal. If such classifiers can be trained, we can use feature
analysis to explore the most indicative features of the
groups, exploring what makes the posts liberal or
conservative, and gaining insight into the ideologies of the
groups. Examining further differences in language use, we
use the Linguistic Inquiry and Word Count tool (Tausczik &
Pennebaker, 2010) to track linguistic changes associated
with affect, religiosity and sociality in the two groups over a
period of seven months. Our hypothesis is that if there is a
greater use of sacred rhetoric by conservatives, it should be
accompanied by an increase in the use of religious and
affective words, especially words related to anger.
We begin by discussing the timeline of events that took
place in response to the construction of Park51. Next, we
describe our data collection method. Then, we discuss
experiments, and close with conclusions and implications.

Timeline of Events
On December 8, 2009, the New York Times published an
article on plans to build an Islamic cultural center at a
building two blocks from Ground Zero (Blumenthal &
Mowjood, 2009). In response to this article, a conservative
blogger criticized the project dubbing it as the “Ground Zero
mosque” (Geller, 2009) and started a national controversy
about the issue that lasted about six months (Elliott, 2010).
The story did not receive much media attention until May 6,
2010, when the building of the mosque was approved by a

1853

committee, and in response, some of the 9/11 victims‟
families expressed anger that the center was being built so
close to where their relatives were killed (Salazar, 2010).
Following this news, the story was brought back to public
attention by a series of posts by conservative bloggers which
relied heavily on the use of sacred rhetoric, framing the
issue as a religious/historical event, threatening the
American identity (e.g. “This is Islamic domination and
expansionism. The location is no accident. Just as Al-Aqsa
was built on top of the Temple in Jerusalem”, Geller 2010a).
Rallies were organized by bloggers on May 29 (marking
"May 29, 1453, [when] the Ottoman forces led by the Sultan
Mehmet II broke through the Byzantine defenses against the
Muslim siege of Constantinople.") (Geller, 2010b) and June
6 (marking D-Day, June 6 1944) (Peyser, 2010). By this
time the controversy was not only widespread through
conservative blogs, but mainstream media and liberal blogs
were also giving considerable attention to the issue (Elliott
2010). On July 21, in response to plans to build Park51, a
Florida Church announced “plans to host an „International
Burn A Quran Day,‟ on the ninth anniversary of the Sept. 11
attacks this year” (Hyde, 2010), which lead to much
expressed anger from both sides. On August 13, President
Obama gave a speech in which he defended freedom of
religion and stated that Muslims are entitled to build the
center. This resulted in a heated response from the right
(“Right-wing media blast President”, 2010). Another rally
was held on August 22 in NYC by the opponents and
supporters of the mosque in which there was prevalent use
of sacred rhetoric by both sides, clashing the sacred
American value of religious freedom against the moral
decadence of contamination of the “hallowed ground”
(Davis & Dover, 2010) of Ground Zero. In September, in
conjunction to other events related to Park51, much media
attention was given to the issue of burning Qurans in
Florida, where two different American sacred values
clashed again: “the constitutional right” to burn a Quran
versus the sacred value of religious tolerance. Beginning in
October, the coverage of the issue started disappearing from
the media as quickly as it had initially started.
One of the most interesting aspects of the controversy
regarding Park51 is the fact that it initially started on the
blogosphere by a single blogger (Elliott, 2010), and most of
the discussions regarding this issue took place on various
different political blogs. This provided us the ability to track
responses to events as they naturally unfolded, allowing
longitudinal analysis of changes in different linguistic and
psychological factors.

Data Collection
In order to compile a representative sample of the blog posts
of each group, we first identified five top popular
conservative and liberal news blogs, each rated by the
website blogs.com 1. Next, we performed a Google search to
1

The conservative blogs we chose for this experiment are:
hotair.com, reason.com, redstate.com, rightwingnews.com and

find all posts within each of these blogs that include the
word “mosque” and were posted between March 1, 2010
and October 6, 2010. We then automatically downloaded
the HTML files for all the links returned by the search
queries. This included a total of 3140 blog posts, consisting
of 1473 posts from the conservative blogs and 1667 from
liberal blogs. Finally, we used customized scripts for each
blog to remove HTML tags, headers, tables, etc. and
extracted only the blog post itself and the comments on the
post, ignoring all the other fields such as advertisement,
blogrolls, name of the authors, dates, etc.

Experiment 1
As we argued in the introduction, one of the important
differences between liberals and conservatives is the type of
language and rhetoric they employ in political discussions -reflecting disparities in the ideologies and value systems of
the two groups. The aim of the first experiment is to see if
the differences in language use and choice of words are
great enough that blog posts can be automatically classified
as conservative or liberal using a machine learning
technique. If we are able to classify these blog posts, then
we will be able to determine the indicative features of each
group using feature analysis and gain insight into what
makes the blogs conservative or liberal.
In a similar line of work, using machine learning to
examine political differences, Diermeier and colleagues (in
press) classify transcribed Senate speeches by first training a
classifier on the speeches of the 25 most liberal and 25 most
conservative senators from the 101st through 107th
Congresses. Then, their classifier is tested on the speeches
of the 25 most liberal and 25 most conservative senators of
the 108th Congress, achieving an accuracy of 92%. Also,
they use a similar technique to classify Senate speeches by
training on the House speeches of the same year (Yu,
Diermeier & Kaufmann, 2008). Performing a feature
analysis, they report that the most important features for
Democrats included company names and words related to
environmental and economic interests (e.g. Enron, ethanol,
hydrogen, lakes), and for conservatives included words with
cultural significance (e.g. cloning, unborn, abortion,
marriage and homosexual).
Our approach was to use supervised machine learning, in
which training data for each predefined category is needed
to build a classifier. This classifier is then used to predict for
each new data point which category it belongs to. Support
Vector Machines (SVMs), first introduced by Vapnik
(1995), is a general learning algorithm used for binary
classification. SVMs represent features, or data points, as
points in space and try to find a hyperplane that is
maximally distant from nearest training data points of each
of the categories. Also, in SVMs, words with the highest
absolute coefficients (i.e. most positive for one group, and
townhall.com, and the liberal blogs are: crooksandliars.com,
dailykos.com, huffingtonpost.com, thinkprogress.com and
wonkette.com.

1854

Conservative: obama, leftist, rino, islam, obamacar, koran, pelosi, allah, suicid, illeg, jihadi, infidel, socialist, shariah,
bloomberg, hussein, hamas, islamist, saddam, communist
Liberal: center, republican, gingrich, teabagg, beck, religi, corpor, muslin, filibust, wingnut, fear, jeer, hate, glenn, cheer,
stewart, right-w, anti-muslim, sarah, geller
Figure 1: 20 of the top 50 feature words with the highest weight for each group within a classifier which achieved 86%
accuracy. Words are listed in decreasing weight order. All words were converted into lower case, and in order to reduce
vocabulary size, word stems were used in classification.
most negative for the other group) are considered the most
informative features, and are the most indicative, or
discriminative, of each category.
We used SVMlight (Joachims, 1999) with its default
settings in this experiment. Prior to generating features
vectors for classification, the documents were subjected to
several pre-processing procedures. We first used a
tokenizer2 to separate text into individual words. Next, in
order to reduce vocabulary size, word stems were derived 3
and different forms of each word were mapped in to the
same word stem. Finally, we removed stop words, which are
common words not useful for classification (e.g. “the”, “a”,
“is”), and several other categories of words such as name of
the blogs and names of frequently referred to websites such
as twitter.com and youtube.com. For training, we used
“term frequency–inverse document frequency” (tf*idf) word
weighting scheme to convert documents and words in the
documents to numerical document vectors. However, in the
prediction step, given that the total number of documents is
assumed to be unknown to the classifier, only word
frequencies were used to represent test documents.
We also examined blog posts according to the date that
they were posted. In order to get consistent number of posts
per time period for both groups, we grouped blog posts into
seven consecutive time blocks (3/01-7/16, 7/17-8/09, 8/108/17, 8/18-8/24, 8/25-9/04, 9/05-9/13, 9/14-10/06)4. The
time blocks were chosen so that there would be at least 200
blog posts for each of the groups per time block. The large
time blocks were necessary in order to compensate for the
amount of noise existing in the files retrieved from the
websites, especially the noise in the comments sections.

Results
Classification accuracy was calculated using a 10-fold cross
validation, where in each run our program randomly chose a
subset of the blogs from each group as the training set, and
25 other blog posts from each group as the testing sample.
This process was repeated ten times and the overall
2

For tokenization, we used the Word Splitter tool, available at
http://cogcomp.cs.illinois.edu/page/tools_view/8
3
To derive word stems, we used the lisp implementation of the
Porter
stemmer,
available
at
http://tartarus.org/~martin/PorterStemmer/
4
There was no significant difference in word count of blog posts
between the liberal and conservative groups and in any of the time
periods.

accuracy of the classification was obtained by averaging
over the accuracy of each of the tests. Overall, with a
training set consisting of 750 blogs per group, our system
achieved average prediction accuracy of 85.6% (p < 0.001).
We coded the top 100 feature words with the highest
absolute coefficients for each group within a classifier that
achieved an accuracy of 86% for in-group and out-group
membership. This coding was done relative to each
subculture, for example “pelosi”, “leftist” and “socialist”
were coded as out-group for conservatives, and
“republican”, “right-w” and “beck” as out-group for liberals
(Figure 1). The results show that for both conservative and
liberals, the most important words for distinguishing them
were words which referenced out-group members
(conservatives: 31% out-group, 14% in-group, χ2 = 7.3405,
p = 0.0067; liberals: 25% out-group, 3% in-group, χ2 =
18.314, p < 0.001) (Figure 2).
We performed the same analysis for each of the time
blocks. Specifically, for each time block, our program
randomly chose 175 blog posts from each group for training
and another 25 posts per group for testing. Similar to the
previous analysis, this process was repeated 10 times for
each of the time blocks and the overall accuracy of the
classifier was calculated by averaging over the 10 tests. The
classification results, averaged over the 7 time periods, was
75.54% (p < 0.001). The accuracy of the classifier did not
significantly differ between any of the time blocks. Coding
the words with the highest feature weights, in classifiers
which achieved accuracy most close to the mean accuracy
rate of each block for in-group and out-group membership,
resulted in a similar pattern as above. That is, within each
time block, the most indicative words for each group, were
references to out-group members and negative portrayals of
out-group members (all p‟s < 0.05).

Discussion
Choice of words used by these two ideological groups were
distinct enough that our system was able to classify their
blog posts as conservative or liberal with an accuracy of
85.6%. Even though we expected that this difference would
diminish for posts within each time block, as the topics of
discussion would be more similar, we were able to classify
blog posts within each block with an average accuracy of
75.54%. Also, feature analysis revealed that the most
distinctive aspect of either liberal or conservative blogs is
not the description, or the ideology, of the in-group, but

1855

Figure 2: Changes in different psychological processes captured by LIWC
rather the use of words related to the negative portrayal of
the out-group.

Experiment 2
In Experiment 1, we demonstrated that the use of language,
and choice of words, between liberal and conservative are
different enough that classifiers can be trained to predict
with high accuracy which group wrote a particular post. In
Experiment 2, we use the Linguistic Inquiry and Word
Count (LIWC) tool (Tausczik & Pennebaker, 2010) to both
further investigate differences in language use between the
two groups and to track linguistic changes associated with
affective and social processes within a seven months time
period. LIWC is one of the most widely used tools for
automatic text analysis in psychology, and has provided
evidence for the psychological and social implications of
word use in various studies (Pennebaker, Mehl &
Niederhoffer, 2003). LIWC has also been used as a tool for
tracking changes in linguistic features over time. For
instance, Back, Küfner and Egloff (2010) examine the
immediate negative emotional reactions on September 11,
2001 expressed in messages sent to text pagers within the
US using LIWC. In a similar study, Cohn, Mehl and
Pennebaker (2004) track psychological changes in response
to the 9/11 attacks using daily writings of 1,084 bloggers for
two months prior to and after the attacks using LIWC.

LIWC performs word counts and catalogs words in to
psychologically meaningful categories (Tausczik &
Pennebaker, 2010). The default LIWC2007 dictionary
includes 4,500 words and word stems which define its 76
different language categories. LIWC assigns each word to
specific linguistic categories, and it reports the total number
of words in each category normalized by the total number of
words in the document. The LIWC categories examined in
this study are: social processes (e.g. talk, share, friends),
affective processes (e.g. happy, cried, abandon), anger (a
subcategory of affective processes) (e.g. hate, kill, annoyed)
and religion (e.g. Altar, church, mosque). We also created a
custom Islam category which included all words in the
religion category related to Islam.

Results
First, we examined how linguistic features for affective
processes changed over time for the two groups by
correlating the percentage of affective words reported by
LIWC for each of the groups with time. There was a
positive correlation between time and use affective words
for conservatives (r = 0.85, p = 0.014), however this
correlation did not reach significant for liberals (r = 0.49, p
= 0.26)5. For conservatives, there was a significant increase
5

Examining this correlation within the first 6 time periods,
revealed the same trend (conservatives: r = 0.86; liberals r = 0.15).

1856

in affective words between the first time block and the 9/059/13 time block (t(8) = 3.6949, p = 0.006). At the 9/05-9/13
block, the differences in this category between the two
groups approached significance (t(8)=2.2023, p = 0.0587).
Overall, the amount of affective words used by conservative
websites was higher than liberal websites (t(68) = 1.9342, p
= 0.0573).
A repeated measures ANOVA with a Greenhouse-Geisser
correction, where the first factor was time and the second
factor group, determined an overall main effect of time for
anger (F(2.470,48) = 5.894, p = 0.007). The same test
revealed that the interaction between time and groups
approached significance for anger (F(2.470,48) = 2.988, p =
0.064). Even though the overall difference in the use of
words related to anger between the two groups did not reach
significance (t(68) = 0.9329, p = 0.35), this difference
became significant at the 9/05-9/13 time block (t(8) =
5.3128, p < 0.001).
We also ran LIWC on the top 5000 words with the
highest absolute coefficients for each of the groups in each
time block, from SVM classifiers that achieved average
predication accuracy. The results show that in words that are
most indicative of conservative blogs, there was an increase
in the use of words related to anger within the first six time
periods (r = 0.7843, p = 0.064).
As shown in the graphs, there was a sharp decrease in the
use of affective words and anger in the last time block,
especially for conservatives, which is an indication of these
processes returning to baseline rates (there is no significant
difference between the first and last time blocks in any of
the emotion categories mentioned above for either of the
two groups).
Another repeated measures ANOVA was ran for the
religion category. There was a main effect of time (F(6,48)
= 4.333, p = 0.001), and the interaction between time and
group approached significance (F(6,48) = 1.959, p = 0.090).
There was a positive correlation between anger and religion
(r = 0.7447, p = 0.0548). Correlating the Islam sub-category
with anger indicated that the correlation between anger and
religion was not due to use of words related to Islam (r =
0.0171, p = 0.9708).
For both groups, there was increase in social orientation
over time (conservatives: r = 0.92, p = 0.004; liberals: r =
0.86, p = 0.0133) which unlike other factors did not return
to baseline. Also the use of words related to social processes
was higher for conservatives than for liberals (t(68) =
3.9122, p < 0.001).

Discussion
Overall, there were significant differences in the use of
words related to affective and social processing between
conservatives and liberals. As our results show, for
conservatives there was a significant increase in the use of
words related to affect, and anger, in periods leading to the
anniversary of 9/11. These changes in the choice of words
used in the posts reflect underlying differences in the type of

rhetoric employed, and subsequent changes in emotional
responses.
Also, for conservatives the rise in the use of words
related to anger was positively correlated with the use of
religious words, which is an indication of an increase in
reliance on sacred rhetoric. The use of sacred rhetoric has
been linked to the emergence of sacred values (Marietta,
2008; Dehghani et al., 2009; Dehghani et al., 2010), as
values that get tied to religion more easily achieve a sacred
status (Marietta, 2009). As previous work shows violations
of sacred values result in anger and moral outrage (e.g.
Tetlock, 2003; Ginges et al., 2007).
The number of words related to anger in feature words
(words with the highest absolute coefficients in SVM
classifiers) increased in the first six time blocks for
conservatives. In other words, echoing the increase in anger
words, the proportion of anger words, useful in
distinguishing conservative posts, increased with time.
Traumatic and upsetting events are generally followed by
an increase in social processes such as seeking of social
support, increase in collective orientation and social sharing
(Mehl & Pennebaker, 2003). Our results indicate that there
were increases in the use of words related to social
processing by both conservatives and liberals over time,
which may be due to attempts to validate their threatened
cultural worldview (Pyszczynski et al., 2004), and to
facilitate social sharing (Rimé et al., 1998).

Conclusions
In this paper, we analyzed conservative and liberal blogs
posts, and their corresponding comments, related to the
construction of a Muslim community center close to Ground
Zero. Most of the controversy and debates surrounding the
issue took place online and thus this methodology seemed
quite apropos. Using two different statistical text analysis
techniques, we showed that there are significant differences
in the use of various linguistic features, and in choice of
words, between liberals and conservatives.
In the first experiment, we used a machine learning
technique to both automatically classify the blogs based on
the group they were written by, and to examine the
indicative features which make these blogs liberal or
conservative. Our results indicate that words which
reference out-group members and are used for out-group
derogation are most characteristic of the ideology of a group
(whether liberal or conservative). Similar to Haidt and
Graham (2007), Lakoff (2002) argues that the ideologies of
conservatives and liberals embody their value systems and
personal conceptions of morality. Instead, our results show
that at least in political debates, the ideas that make these
groups liberal or conservative, are stereotypes of the outgroup.
In the second experiment, by examining posts in
different time blocks, we showed that there was an increase
in words related to affective processes and anger over time,
especially for conservatives. We argued that this increase is
potentially related to the use of sacred rhetoric, as there was

1857

a significant correlation between anger and the use of
religious words.
In conclusion, by analyzing over 3000 conservative and
liberal blog posts related to the constructions of Park51, our
results confirm significant differences in the use of
language, and its resultant emotions, between the two
groups. Language use in these blogs reflects ideological
differences between liberals and conservatives. We believe
the ability to perform this type of mass text analysis and to
track changes of different psychological processes over
different periods of time, as they naturally unfold among
diverse cultural groups, can provide new insights which
arguably cannot be achieved in an experimental setting
inside the lab.

Acknowledgments
This research was supported by AFOSR FA9550-09-10507.

References
Back, M. D., Küfner, A. C. P., and Egloff, B. (2010). The
emotional timeline of September 11, 2001. Psychological
Science 21: 1417-1419.
Blumenthal, R. and Mowjood, S. (December 8, 2009).
Muslim prayers and renewal near Ground Zero. New York
Times.
Cohn, M. A., Mehl, M. R., & Pennebaker, J. W. (2004).
Linguistic markers of psychological change after
September 11, 2001. Psychological Science, 15, 687-693
Davis, L. and Dover, E. (Aug 22, 2010). Ground Zero
Mosque opponents, supporters turn out to demonstrate.
ABC News.
Dehghani, M., Iliev, R., Sachdeva, S., Atran, S., Ginges, J.
& Medin, D. (2009). Emerging sacred values: Iran‟s
nuclear program. Judgment and Decision Making, 4, 7,
930-933.
Dehghani, M., Atran, S., Iliev, R., Sachdeva, S., Medin, D.
& Ginges, J. (2010). Sacred values and conﬂict over
Iran‟s nuclear program. Judgment and Decision Making,
5, 7, 540-546.
Diermeier, D., Godbout, J. F., Yu B., & Kaufmann, S. (in
press). Language and ideology in Congress. British
Journal of Political Science.
Elliott, J. (Aug 16, 2010). How the "ground zero mosque"
fear mongering began. Salon.com.
Geller, P. (Dec 8, 2009). Giving thanks.
Geller, P. (May 6, 2010a) Monster mosque pushes ahead in
shadow of World Trade Center. Islamic death and
destruction.
Geller, P. (May 8, 2010b) SIOA campaign offensive: Stop
the 911 Mosque Protest. Update: date change June 6th D
Day.
Ginges, J., Atran, S., Medin, D. & Shikaki, K. (2007).
Sacred bounds on rational resolution of violent political
conﬂict. Proceedings of the National Academy of
Sciences, 104, 7357-7360.

Haidt, J., & Graham, J. (2007). When morality opposes
justice: Conservatives have moral intuitions that Liberals
may not recognize. Social Justice Research, 20, 98-116
Hyde, M. (July 21, 2010). Fla. church plans to burn Qurans
on 9/11 anniversary. Religion News Service.
Joachims, T. (1999). Making Large-Scale SVM learning
practical. In: Advances in Kernel methods - Support
Vector Learning, B. Schölkopf, C. Burges, and A. Smola
(ed.). MIT Press, Cambridge, MA.
Lakoff, G. (2002). Moral politics: How Liberals and
Conservatives think. University of Chicago Press,
Chicago, IL.
Lakoff, G. (2008). The political mind: Why you can’t
understand 21st-century politics with an 18th-century
brain. Viking, New York, NY.
Marietta, M. (2008) "From my cold, dead hands”:
Democratic consequences of sacred rhetoric. Journal of
Politics. 70, 3:767-779.
Marietta, M. (2009). The absolutist advantage: sacred
rhetoric in contemporary presidential debate. Political
Communication. 26, 4:388-411
Mehl, M. R. & Pennebaker, J. W. (2003). The social
dynamics of a cultural upheaval: Social interactions
surrounding September 11, 2001. Psychological Science,
14, 579-585.
Pennebaker, J. W., Mehl, M.R., & Niederhoffer, K. (2003).
Psychological aspects of natural language use: Our words,
our selves. Annual Review of Psychology, 54, 547-577.
Peyser, A. (May 13, 2010) Mosque madness at Ground
Zero. New York Post.
“Right-wing media blast President” (Aug 14, 2010).
MediaMatters.com.
Pyszczynski, T., Greenberg, J., Solomon, S., Arndt, J., &
Schimel, J. (2004). Why do people need self-esteem? A
theoretical and empirical review. Psychological Bulletin,
130, 435-468.
Rimé, B., Finkenauer, C., Luminet, O., Zech, E., &
Philippot, P. (1998). Social sharing of emotion: New
evidence and new questions. In W. Stroebe & M.
Hewstone (Eds.), European Review of Social Psychology
(Vol. 9, pp.145-189). Wile, Chichester.
Salazar, C. (June 7, 2010) Building damaged in 9/11 to be
mosque for NYC Muslims. The Associated Press.
Tausczik, Y., & Pennebaker, J. W. (2010). The
psychological meaning of words: LIWC and
computerized text analysis methods.
Journal of
Language and Social Psychology, 29, 24-54
Tetlock, P. (2003). Thinking the unthinkable: sacred values
and taboo cognitions. Trends in Cognitive Sciences, 7,
320–24.
“TIMELINE: Nine months of the right's anti-Muslim
bigotry” (Sep 10, 2010) Media Matters for America.
Vapnik, V. N. (1995) The Nature of Statistical Learning
Theory. Springer, New York.
Yu, B., Diermeier, D. & Kaufmann, S. (2008). Classifying
party affiliation from political speech. Journal of
Information Technology in Politics, 5, 33-48.

1858

