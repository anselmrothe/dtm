UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Uncertainty and exploration in a restless bandit task

Permalink
https://escholarship.org/uc/item/5qs1r33b

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)

Authors
Speekenbrink, Maarten
Konstantinidis, Emmanouil

Publication Date
2014-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Uncertainty and exploration in a restless bandit task
Maarten Speekenbrink (m.speekenbrink@ucl.ac.uk)
Emmanouil Konstantinidis (emmanouil.konstantinidis.09@ucl.ac.uk)
Department of Experimental Psychology, University College London
Gower Street, London WC1E 6BT, England
Abstract
Decision-making in noisy and changing environments requires
a fine balance between exploiting knowledge about good
courses of action and exploring the environment in order to
improve upon this knowledge. We present an experiment in
which participants made repeated choices between options for
which the average rewards changed over time. Comparing a
number of computational models of participants’ behaviour in
this task, we find evidence that a substantial number of them
balanced exploration and exploitation by considering the probability that an option offers the maximum reward out of all the
available options.
Keywords:
Dynamic decision making; Explorationexploitation trade-off; Restless multi-armed bandit task

Introduction
In many situations, the expected utility of an action is initially unknown and can only be learned from experience. In
such situations, we can take actions in order to maximise the
utility experienced (exploiting the environment), but also take
actions which might not provide as good outcomes, but which
help us to learn more about the outcomes associated with that
action (exploring the environment). Performing well in these
situations requires a fine balance between exploration and
exploitation. Multi-armed bandit tasks have proven a useful paradigm to study the exploration-exploitation trade-off,
theoretically (e.g., Gittins, 1979; Whittle, 1988) as well as
empirically (e.g., Acuna & Schrater, 2008; Daw et al., 2006;
Knox et al., 2012; Steyvers et al., 2009).
For standard bandit problems, in which the average rewards of unchosen bandits remain unchanged and future
rewards are exponentially discounted, the optimal decision
strategy can be determined by calculating a “Gittins index”
for each arm of the bandit, reflecting the expected total future
rewards associated with the arm at a particular time (Gittins,
1979). Acuna & Schrater (2008) showed that, allowing for
computational constraints, human decisions follow this optimal strategy reasonably well. Although standard bandit tasks
have generated useful results, in real-life situations, the expected rewards of unchosen options do often change. For instance, when choosing a restaurant, we should allow for the
possibility that the quality of the food on offer changes over
time (cf., Knox et al., 2012). For what is now a “restless”
bandit problem, optimal decision strategies have proven elusive, although heuristic strategies have been proposed (Whittle, 1988). Daw et al. (2006) investigated human decision
making in a restless bandit problem, and found that exploration appeared to be unrelated to the uncertainty regarding
the average reward of each arm. Exploration was best described by a heuristic strategy in which arms are chosen probabilistically according to their relative expected rewards (the

“softmax” decision rule). The lack of an effect of uncertainty
on explorative decisions is disappointing, considering that rationally, this should be a driving factor of exploration (Cohen
et al., 2007). Knox et al. (2012) used a restless two-armed
bandit task with a simplified structure, which allowed them to
derive the optimal decision strategy. In their task, the rewards
of the two arms alternated in their superiority, “leapfrogging”
over each other. The results showed that people appeared to
act reflectively, updating their beliefs that the arms switched
in superiority, but that they could not use these beliefs to plan
further ahead in time than for the immediate decision. Moreover, in contrast to Daw et al., Knox et al. found evidence
that exploration was driven by uncertainty regarding the associated rewards. As Knox et al. used a much more constrained
task than Daw et al., it is unclear whether this difference is
due to the task, or to the way in which the effect of uncertainty on decisions was formalized. In the present paper, we
will try to reconcile these conflicting results, by considering
an alternative way to incorporate uncertainty into explorative
decisions than the heuristic strategy used by Daw et al..
To illustrate our model, consider a relatively simple task in
which, on each trial t, the reward R j (t) associated with arm j
is drawn from a Normal distribution, with a mean µ j (t) that
changes over trials according to a random walk
R j (t)=µ j (t) + ε j (t)
µ j (t)=µ j (t − 1) + ζ j (t)

ε j (t)∼N(0, σε )
ζ j (t)∼N(0, σζ )

(1)

An ideal Bayesian learner with knowledge of the properties
of this process would update her belief about the average rewards based on the observed rewards; p(µ j (t)|R1:t ,C1:t ), the
posterior distribution of arm j’s average reward, conditional
upon the observed rewards R1:t and choices C1:t , can be computed by the Kalman filter (cf. Daw et al., 2006). This posterior distribution, together with the structural model, allows
the agent to derive a prior distribution p(µ j (t + 1)|R1:t ,C1:t )
for the next trial, which reflects the current beliefs about each
arm at the moment of choice. How should these beliefs be
used to choose the next arm to play? A “greedy” agent would
always choose the arm with the highest prior mean. While
this is optimal on the last play, if there are more plays left,
it is generally beneficial to sometimes choose a different arm
in order to check that its average reward has not surpassed
that of the currently favoured arm. The longer an arm has
not been played, the higher the (subjective) probability that it
now has a higher average reward. If exploration is based on
this probability, the probability of exploration increases with
the time that an arm has not been played. The difference between this explorative strategy and a greedy one is illustrated

1491

greedy

20

(estimated) average reward

0

−20

explorative

20

0

−20

0

25

50

75

100

trial

Figure 1: Learning and decision making in a restless twoarmed bandit task. Two arms (blue and red) have changing
average rewards (broken lines), generated according to Equation 1. On each trial, an agent chooses an arm (dots at the
bottom of the graphs), observes the associated reward, and
then updates her belief about the average reward for that arm.
These posterior beliefs form the basis of the prior belief on
the next trial (solid lines show the prior means and areas
the 95% highest density intervals of the prior distribution).
The “greedy” agent (top panel) always chooses the arm with
the highest expected reward. After sampling once from both
arms, she always chooses the one with the highest expected
reward until the prior mean falls below the prior mean of the
unchosen arm. A problem with this strategy is that it ignores
the uncertainty in the prior distribution, which increases for
unchosen arms due to the innovations ζ j (t). After not choosing an arm for a prolonged period, the probability that the
mean reward of this arm is higher than the mean reward of the
chosen arm can become substantial. The “explorative” agent
(bottom panel) bases her choices on this probability. On each
trial she chooses an arm randomly according to the probability that this arm will provide the highest reward in the set of
arms. This clearly gives better results than the greedy strategy and the agent mostly chooses the arm with the highest
average reward.

in Figure 1. As shown there, the explorative strategy clearly
outperforms the greedy strategy.
The probability that an arm provides the maximum reward
naturally combines both the expected value (mean) and the
associated uncertainty (variance) of the prior distributions.
While a strategy which bases decisions on this probability is
myopic in the sense that it is solely based on the chance of
obtaining the highest possible reward for the immediate decision, it nevertheless allows for a reasonable balance between
exploitation and exploration. As the probability of maximum
reward increases with uncertainty, the probability of exploring an arm increases the longer it has not been observed. But
the probability of maximum reward is also dependent on the
expected reward, such that this increase is larger for arms that
are closer to the currently favoured arm in expected reward.
While Daw et al. (2006) did not find evidence that exploration
is related to uncertainty, they only considered a heuristic decision strategy with an exploration bonus which increased linearly with the standard deviation of the prior distribution. The
probability of maximum reward strategy offers a more principled way in which to combine expectancy and uncertainty
and results of a simulation study showed that it outperforms
more heuristic strategies. The strategy generalizes the belief
model proposed by Knox et al. (2012) to the more general
situation of a restless multi-armed bandit.
In the present paper, we investigate whether humans performing a restless multi-armed bandit task use this strategy to
make their decisions. We compare the strategy to a number of
heuristic decision strategies proposed for multi-armed bandit
tasks, contrasting also Bayesian “model-based” learning and
two popular “model-free” learning strategies.

Method
We investigated decision-making in a restless four-armed
bandit task similar to that used by Daw et al. (2006). Four
versions of the task were constructed in which (a) the average
rewards of the decks changed either completely unpredictably
or with a small trend, and (b) the volatility of the changes was
either stable or there were periods of relatively high volatility. We expected people who notice an increase in volatility
to make more exploratory decisions, due to the associated increase in uncertainty. People who notice the trends could be
expected to make relatively less exploratory decisions, as the
changes are more predictable.

Participants
Eighty participants (41 female), aged between 18 and 56
(M = 22, SD = 6.72), took part in this study on a voluntary
basis. Participants were randomly assigned to one of the four
experimental conditions: stable volatility with trend (ST), stable volatility without trend (SN), variable volatility with trend
(VT), and variable volatility without trend (VN).

Task
All participants completed a restless four-armed bandit task.
On each of 200 trials, they were presented with four decks

1492

arm

1

2

3

no trend

decks of cards and that their task was to select on each trial
a card from any deck they chose. The only goal of the game
was to win as many points as possible. Participants were informed that some decks may be better than others, but that the
amount they tend to give may vary, so that this can change.
They were not informed of the total number of trials in the
task.
After reading the instructions, participants started the experimental task. On each trial, they were presented with the
four decks of cards and selected a card from one deck via a
mouse click. The number of points won or lost was then displayed for 1.5 seconds, along with either a smiley or a frowning face for wins or losses respectively. Throughout the task,
a counter displayed the total points received thus far.

4
trend

100
stable

0

value

−100
−200
100
variable

0
−100
−200
0

50

100

150

200 0

50

100

150

Behavioural results

200

trial

One participant (age = 21) in the SN condition was excluded
from further analysis as she only sampled from one bandit
throughout the whole task. All other participants sampled
from each bandit at least once.

Figure 2: Example rewards in the four-armed restless bandit
task.

of cards (arms) and asked to draw a card from one of them.
After choosing an arm, they were informed of the reward associated with their choice. For each arm j = 1, . . . , 4, the
reward R j (t) on trial t was randomly drawn from a Normal
distribution with a mean µ j (t) which varied randomly and independently according to a random walk. More precisely, the
rewards were generated according to the following schedule:
R j (t)=µ j (t) + ε j (t)
µ j (t)=λµ j (t − 1) + κ j + ζ j (t)

ε j (t)∼N(0, σε )
(2)
ζ j (t)∼N(0, σζ (t))

The averages of the decks were initialized as µ j (1) =
−60, −20, 20, 60 for decks 1 to 4 respectively. A decay parameter λ = .9836 was used so that values remained closer to
0 than with a pure random walk. In the ST and VT conditions,
the trend parameter had values κ j = 0.5, 0.5, −0.5, −0.5 for
decks 1 to 4 respectively. In the SN and VN conditions, the
values were κ j = 0 for all decks. The reward error variance
was σ2ε = 16 in all conditions. The innovation variance was
σ2ζ (t) = 16 on all trials in the stable volatility conditions (SN
and ST). In the variable volatility conditions, the innovation
variance was the same on half of the trials. On trials 50100 and 150-200, the innovation variance was increased to
σ2ζ (t) = 256. The schedule in Equation 2 was used to generate four sets of reward sequences in each condition, matching
the seed in the random number generated used over conditions. One example of the resulting rewards in the four conditions is provided in Figure 2. A similar structure was used
by Daw et al. (2006), but they didn’t include trends or changes
in volatility and used only posive rewards.

Procedure
Participants completed the 200 trials of the task individually
at their own pace in a single session. At the start of the task,
participants were told that they would be presented with four

Performance
Given the differences between the conditions in obtainable
reward magnitudes (see e.g. Figure 2), total reward obtained
is not an unambiguous measure of performance. We therefore chose to focus on whether, on a given trial, the bandit
with the maximum reward was chosen, which we will refer
to as an advantageous choice. Average proportions of advantageous choices, by block (4 blocks of 50 trials each) and
condition, are given in Figure 3. Choice behaviour was analysed with a generalized linear mixed-effects model, using a
binomial distribution for the number of advantageous choices
in each block. In addition to fixed effects for Block, Volatility,
and Trend, subject-specific random intercepts were included.
Note that this model is structurally similar to a repeatedmeasures ANOVA, but takes into account the non-normal distribution of the number of advantageous choices. This analysis showed a significant main effect of Block, χ2 (3) = 295.97,
p < .001. Averaging over conditions, the proportion of advantageous choices increased from block 1 to block 3, while
there was a small decrease from block 3 to block 4. In addition, there was a significant Volatility by Block interaction,
χ2 (3) = 148.13, p < .001, as well as a significant Trend by
Block interaction, χ2 (3) = 86.38, p < .001. Post-hoc comparisons showed that in block 2, performance in the stable
volatility conditions was significantly better than in the variable volatility conditions (p = .019), and performance in the
no trend conditions was significantly better than in the trend
conditions (p = .004). In block 3, the reverse was true,
with performance better in the variable volatility conditions
(p < .001) and in the trend conditions (p = .029). In the remaining blocks, there was no effect of Volatility or Trend.
The effects of Volatility and Trend on performance are likely
due to their effect on the discriminability between the arms
in terms of their average rewards. For instance, while high
volatility may hinder discrimination between the decks due to

1493

block

1

2

3

no trend

in the variable volatility conditions, χ2 (1) = 35.77, p < .001,
while there is no difference in the stable volatility conditions,
χ2 (1) = 1.89, p = .17.

4
trend
advantageous choice

0.75
0.50

proportion

0.25

Modelling exploration and exploitation
Switching between arms is only a rough measure of exploration, as one can switch arm because one believes it is now
optimal (exploitation) or to gain more information about it
(exploration). We therefore use computational modelling to
gain more insight into explorative decisions. In all models
considered here, u(t), the utility of the reward R(t) received
on trial t, is assumed to be described through the value function of Prospect Theory (cf. Ahn et al., 2008):
(
R(t)α
if R(t) ≥ 0
u(t) =
−λ|R(t)|α if R(t) < 0

0.00
0.4
switch

0.3
0.2
0.1
0.0
stable

variable

stable

variable

where the parameter α > 0 determines the shape of the utility
function: when α < 1, the curve is concave for gains (risk
aversion) and convex for losses (risk seeking). The parameter
λ ≥ 0 can account for loss aversion: when λ > 1, a loss of x
points has a larger negative utility than a win of x points has
a positive utility.
After receiving a reward on trial t, participants are assumed
to update their expectancies E j (t + 1) regarding the utility
they will receive when choosing deck j on trial t + 1. We
consider three possible mechanisms through which these expectancies are updated: Bayesian updating, the delta rule, and
the decay rule.

Figure 3: Proportion of advantageous choices and switches
by block (50 trials each) and condition.

large trial-by-trial variation in average rewards, when volatility reduces again in block 3, the arms are actually more discriminable than in the stable volatility conditions, as the high
volatility has pushed the means further apart.

Switching
Average switching proportions, by block and condition, are
given in Figure 3. Switching behaviour was analysed with
a similar generalized linear mixed-effects model as for the
advantageous choices. This analysis showed a significant
main effect of Block, χ2 (3) = 634.75, p < .001, as well as
a Volatility by Block interaction, χ2 (3) = 26.44, p < .001, a
Trend by Block interaction, χ2 (3) = 44.15, p < .001, and a
three-way interaction between Volatility, Trend, and Block,
χ2 (3) = 16.12, p = .001. No other effects were significant.
Post-hoc analysis did not show any significant differences between pairs of conditions within each block. Comparisons of
consecutive blocks within each condition showed that in the
SN condition, there was a significant decrease in switching
from block 2 to 3. In the VN condition, switching decreased
from block 1 to 2 and from block 2 and 3, while there was
an increase from block 3 to 4. In the ST and VT condition
there was a decrease in switching from block 1 to 2 and from
block 2 to 3. For all these comparisons, p < .001. As for the
number of advantageous choices, this analysis indicates that
there were no general effects of volatility or trend on switching behaviour. However, these manipulations did affect how
switching behaviour developed during the task.
Of particular interest is whether participants in the variable volatility conditions show increased exploration in the
blocks with high volatility. Focussing on switching behaviour
in block 3 and 4, we see an increase from block 3 to block 4

Bayesian updating This model-based learning strategy assumes the utility of arms is determined by a Gaussian process
as in Equation 1. Optimal Bayesian inference regarding mean
utilities is implemented by the Kalman filter:
E j (t) = E j (t − 1) + δ j (t)K j (t)[u j (t) − E j (t − 1)]

(3)

where δ j (t) = 1 if deck j was chosen on trial t, and 0 otherwise. The “Kalman gain” term is computed as
K j (t) =

S j (t − 1) + σ2ζ
S j (t − 1) + σ2ζ + σ2ε

where S j (t) is the variance of the posterior distribution of the
mean utility, computed as
S j (t) = S j (t − 1) + σ2ζ + δ j (t)[1 − K j (t)][S j (t − 1) + σ2ζ ] (4)
Prior means and variances were initialized to E j (0) = 0 and
S j (0) = 1000. For simplicity, we did not consider a model
which learns possible trends or the level of volatility.
Delta rule A popular model-free alternative to Bayesian inference is the delta rule:

1494

E j (t) = E j (t − 1) + δ j (t)η[u j (t) − E j (t − 1)]

Decay rule While the two previous learning rules assume
only the expectancy of the currently chosen deck is updated,
according to the (model-free) decay rule (e.g. Ahn et al.,
2008), expectancies of unchosen decks decay towards 0:

Probability of maximum utility (PMU) The probability
that an arm provides a higher utility than any of the other
arms can be computed as the probability that all pairwise differences between the reward of an arm and the rewards of
the other arms are greater than or equal to 0. For the current
task and generative model in Equation 1, there are three such
pairwise differences scores for each arm which follow a multivariate Normal distribution. Hence, the probability that arm
j is chosen on trial t is

E j (t) = ηEg (t − 1) + δ j (t)u j (t)

P(C(t) = j) = P(∀k : u j (t) ≥ uk (t))

The main difference between this rule and Bayesian updating
(Equation 3) is that the former uses a fixed learning rate 0 ≤
η ≤ 1, while the “learning rate” K j (t) of the latter depends on
the current level of uncertainty.

Z ∞

where the decay parameter 0 ≤ η ≤ 1.

=
0

Choice rules

where Φ is the multivariate Normal density function with
mean vector M j (t) = A j E(t) and covariance matrix H j (t) =
A j diag(S(t) + σ̂2ε )ATj , where diag(S(t) + σ̂2ε ) is a diagonal
matrix with values S j (t) + σ̂2ε and σ̂2ε the error variance assumed by the learner. The matrix A j computes the pairwise
differences between deck j and the other decks. E.g.,


1 −1 0
0
A1 = 1 0 −1 0 
1 0
0 −1

Choice rules describe how the expectancies are used to make
a choice C(t) between the arms. We consider six choice rules,
the “probability of maximum utility” rule described in the
introduction, and three more heuristic rules popular in reinforcement learning (cf. Daw et al., 2006).
ε-greedy This choice rule exploits the arm with the maximum expectancy with probability 1 - ε, and with probability
ε chooses randomly from the remaining bandits:
(
1 − ε if E j (t) > Ek (t), ∀k 6= j
P(C(t) = j) =
ε/3
otherwise

Model estimation and inference

Softmax (SM) The softmax rule can vary gradually between a pure exploitation (maximisation) and pure exploration through an inverse temperature parameter θ(t):
P(C(t) = j) =

exp{θ(t)E j (t)}
4
∑k=1 exp{θ(t)Ek (t)}

The temperature is constant in the fixed softmax (SM f ) models: θ(t) = θ0 , with θ0 ≥ 0. In the dynamic softmax (SMd )
models, the temperature can increase or decrease over trials
according to the schedule θ(t) = [t/10]θ0 . In this case, θ0 can
take values along the whole real line.
Softmax with exploration bonus (SMEB) Exploration
can be increased by adding an “exploration bonus” term,
β j (t), to the Softmax rule:
P(C(t) = j) =

Φ(M j (t), H j (t))

exp{θ0 E j (t) + β j (t)}
4
∑k=1 exp{θ0 Ek (t) + βk (t)}

For each individual participant, model parameters were estimated by maximum likelihood using the Nelder-Mead simplex algorithm implemented in the optim function in R. To
evaluate the fit of the models, we computed the Akaike (AIC)
and Schwartz (BIC) information criteria, reported as difference scores between a null model1 and the model of interest (cf. Ahn et al., 2008). For these difference scores, negative values of ∆(AIC) and ∆(BIC) indicate that the model
fitted worse than the null model, while increasing positive
values indicate better fit. Finally, we computed Akaike and
Schwarz weights, w(AIC) and w(BIC) (e.g., Wagenmakers &
Farrell, 2004). Schwarz (BIC) weights approximate the posterior probability of the models (assuming equal prior probability). Similarly, Akaike (AIC) weights can be interpreted as
reflecting the probability that, given the observed data, a candidate model is the best model in the AIC sense (that it minimizes the Kullback-Leibler discrepancy) in the set of models
under consideration.

Modelling results

The exploration bonus increases with uncertainty. For the
Kalman filter model, we use the standard deviation
of the
q
prior distribution of mean utility: β j (t) = β0 S j (t) + σ̂2ζ ,
with β0 > 0 and S j (t) computed as in Equation 4. As the
Delta and Decay models do not provide measures of uncertainty, we used a simple heuristic according to which the uncertainty increases linearly with the number of trials since a
particular arm was last observed: β j (t) = β0 [t − T j ], where T j
is the last trial before the current trial t in which deck j was
chosen.

Table 1 contains the fit measures for all models. Focussing
first on the AIC and BIC difference scores, we see that on
average the best fitting model is decay learning with fixed
softmax choice (Decay-SM f ). In general, the Decay rule always performs better (on average) than the other two learning
1 The null model used was a simple multinomial model in which,
on each trial, the deck choice is assumed to be an independent random draw from a multinomial distribution. The probabilities of this
distribution were estimated for each participant and the null model
has three parameters (the probability for three bandits; the probability of other bandit deck follows immediately).

1495

Table 1: Modelling results. Values of ∆(·) and w(·) are averages and the standard deviation is given in parentheses. Values of
n(·) are the total number of participants best fit by the corresponding model.
Learning
Bayesian

Delta

Decay

Choice
ε-greedy
SM f
SMd
SMEB
PMU
ε-greedy
SM f
SMd
SMEB
ε-greedy
SM f
SMd
SMEB

∆(AIC)
190.39 (95.72)
237.37 (90.07)
236.22 (90.09)
226.71 (94.8)
223.43 (107.36)
192.81 (94.36)
237.5 (91.7)
236.68 (91.4)
231.39 (93.4)
203.76 (94.01)
244.55 (91.69)
240.59 (95.7)
240.5 (91.99)

w(AIC)
0 (0)
0.04 (0.09)
0.06 (0.17)
0.01 (0.02)
0.25 (0.4)
0 (0)
0.06 (0.13)
0.11 (0.24)
0.02 (0.05)
0.01 (0.11)
0.19 (0.28)
0.2 (0.33)
0.05 (0.08)

rules. This is a common finding which is likely due to the
ability of this rule to capture people’s tendency to repeat previous choices (Ahn et al., 2008). While the delta learning rule
performs on average a little better than Bayesian updating
with the Kalman filter, the differences between these learning rules are less marked. Out of the different choice rules,
the ε-greedy rule clearly performs the worst, while the fixed
Softmax (SM f ) rule performs best for each learning rule.
When we look at the number of participants who are best
fit by each model, a different picture arises: Bayesian updating with the probability of maximum utility choice rule
(Bayesian-PMU) fits more participants best than any of the
other models. This is also the model with the highest average Akaike and Schwarz weights. As such, the evidence for
this model is more marked than the evidence for the other
models. The discrepancy between the results for the AIC and
BIC difference scores on the one hand, and the Akaike and
Schwarz weights on the other, is due to the fact that when the
Bayesian-PMU model fits best, it fits decidedly better than the
other models, resulting in weights close to 1. For participants
with a different best fitting model, the differences between the
AIC and BIC values of the next best-fitting models are generally smaller, resulting in less extreme weights. In conclusion,
there is strong evidence that uncertainty regarding an arm’s
mean reward influenced a substantial number of participants’
decisions. While this contradicts the findings of Daw et al.
(2006), they only considered the heuristic Bayesian-SMEB to
model the influence of uncertainty, for which we also found
little evidence. Clearly, the effect of uncertainty on explorative decisions is more nuanced (cf. Cohen et al., 2007).

Conclusion
We found evidence that a substantial proportion of our participants explored to reduce uncertainty in their beliefs. This
was evident from the finding that participants switched decks
more in periods of increased volatility, and from computational modelling where a model that balances exploration and
exploitation according to the probability that arms provide
the maximum utility fitted the largest number of individual
participants and, on average, had the highest posterior prob-

n(AIC)
0
2
2
0
21
0
6
12
0
1
18
17
0

∆(BIC)
183.79 (95.72)
230.78 (90.07)
229.62 (90.09)
216.81 (94.8)
220.13 (107.36)
189.51 (94.36)
234.2 (91.7)
233.38 (91.4)
224.8 (93.4)
200.46 (94.01)
241.25 (91.69)
237.29 (95.7)
233.9 (91.99)

w(BIC)
0 (0)
0.01 (0.03)
0.03 (0.13)
0 (0)
0.26 (0.41)
0 (0)
0.09 (0.2)
0.15 (0.3)
0.01 (0.01)
0.01 (0.11)
0.22 (0.33)
0.22 (0.34)
0.01 (0.02)

n(BIC)
0
0
2
0
21
0
8
12
0
1
18
17
0

ability in a set of competing models. This supports previous
findings by Knox et al. (2012) in a simpler task, but contrasts
with the findings of Daw et al. (2006) who found no influence
of uncertainty in a task similar to the one used here. Apparently, uncertainty affects decisions in a more refined way than
in the model of Daw et al., through its effect on predicted distributions of the utility associated with the options.

Acknowledgements
We thank James Carragher, Belle Cartwright, Melvin Wan
and Tiffany Yung for their assistance in collecting the data.

References
Acuna, D., & Schrater, P. (2008). Bayesian modeling of human
sequential decision-making on the multi-armed bandit problem.
In B. C. Love, K. McRae, & V. M. Sloutsky (Eds.) Proceedings
of the 30th Annual Conference of the Cognitive Science Society,
(pp. 200–300).
Ahn, W.-Y., Busemeyer, J. R., Wagenmakers, E.-J., & Stout, J. C.
(2008). Comparison of decision learning models using the generalization criterion method. Cognitive Science, 32, 1376–1402.
Cohen, J. D., McClure, S. M., & Yu, A. J. (2007). Should I stay or
should I go? How the human brain manages the trade-off between
exploitation and exploration. Philosophical Transactions of the
Royal Society B, 362, 933–942.
Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B., & Dolan, R. J.
(2006). Cortical substrates for exploratory decisions in humans.
Nature, 441, 876–879.
Gittins, J. C. (1979). Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society. Series B, 41, 148–
177.
Knox, W. B., Otto, A. R., Stone, P., & Love, B. C. (2012). The nature
of belief-directed exploratory choice in human decision-making.
Frontiers in Psychology, 2:398, 1–12.
Steyvers, M., Lee, M. D., & Wagenmakers, E.-J. (2009). A Bayesian
analysis of human decision-making on bandit problems. Journal
of Mathematical Psychology, 53, 168–179.
Wagenmakers, E.-J., & Farrell, S. (2004). AIC model selection using
Akaike weights. Psychonomic Bulletin & Review, 11, 192–196.
Whittle, P. (1988). Restless bandits: Activity allocation in a changing world. Journal of Applied Probability, 25, 287–298.

1496

