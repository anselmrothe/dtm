UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Dual Process Theory of Optimistic Cognition

Permalink
https://escholarship.org/uc/item/23q9v98f

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)

Authors
Sunehag, Peter
Hutter, Marcus

Publication Date
2014-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

A Dual Process Theory of Optimistic Cognition
Peter Sunehag (Peter.Sunehag@anu.edu.au) and Marcus Hutter (Marcus.Hutter@anu.edu.au)
Research School of Computer Science
The Australian National University, Canberra Australia
Abstract
Optimism is a prevalent bias in human cognition including
variations like self-serving beliefs, illusions of control and
overly positive views of one’s own future. Further, optimism
has been linked with both success and happiness. In fact, it has
been described as a part of human mental well-being which has
otherwise been assumed to be about being connected to reality.
In reality, only people suffering from depression are realistic.
Here we study a formalization of optimism within a dual process framework and study its usefulness beyond human needs
in a way that also applies to artificial reinforcement learning
agents. Optimism enables systematic exploration which is essential in an (partially) unknown world. The key property of
an optimistic hypothesis is that if it is not contradicted when
one acts greedily with respect to it, then one is well rewarded
even if it is wrong.
Keywords: Rationality, Optimism, Optimality, Reinforcement
Learning

Introduction
The optimistic bias is (together with the simplicity bias
(Chater and Vitanyi, 2003)) perhaps the most fundamental and prevalent among the human cognitive biases (Taylor
and Brown, 1988; Sharot et al., 2007; Carver et al., 2010;
Kahneman, 2011) It has been found to be correlated with
high achievement, happiness and resilience when challenged
(Carver et al., 2010) but also with dangerous risk-seeking in
e.g. traffic or severely underestimating costs in ruinous public
building projects (Kahneman, 2011). Some of the drawbacks
can be viewed as falling for implausible (cockeyed instead
of cautious (Wallston, 1994)) optimism where one has not
learned from other examples through the base rate for the sort
of situation one is in. Besides resilience in the face of challenges and the effect on others, the optimism bias has uses
beyond the strictly human. In reinforcement learning, artificial agents are often equipped with an optimism bias to enable systematically explorative behavior in an unknown world
(Szita and Lörincz, 2008). Here we mathematically study
plausibly optimistic general reinforcement learning agents as
an alternative (both normative and descriptive) paradigm to
Bayesian models of cognition (Griffiths et al., 2008).
The general reinforcement learning problem in an unknown environment is an extremely challenging problem
(Hutter, 2005). If one has access to a suitable (i.e. probability
mass placed mainly on the actually plausible) a priori environment, e.g. a Bayesian mixture over all environments in a
certain hypothesis class, and the computational resources to
compute the policy that maximizes the desired quality measure, this is the natural choice and optimal by definition. The
quality measure can for example be expected accumulated reward during the life of the agent, or the maximum accumulated reward that is guaranteed with a certain given probability (e.g. 0.95). The two immediate problems are that one

needs such an a priori environment, e.g. through a prior on a
hypothesis class, and that performing the mentioned computation is too hard, even approximately. We argue that the second of these motivates optimism as a rational strategy, since
when optimizing over a too short horizon, realism leads to insufficiently explorative behavior. A property that makes optimism particularly appealing is that as long as the outcome is
as predicted, high rewards are received. This is regardless of
the correctness of the hypothesis. Humans are often trying to
avoid contradiction of their hypothesis and often avoid contradicting each other’s hypothesis (Taylor and Brown, 1988).
Managing to enforce ones hypothesis in a group is primarily
useful if it is optimistic, i.e. self-serving.
We are here going to discuss agents with limited resources
within a dual process agent framework where a limited number of hypotheses are generated by one system and the other
system is making a choice by excluding implausible hypotheses and choosing optimistically/greedily among the rest.
Within such a framework we use normative principles such
as rationality considerations as in the foundations of decision
theory together with the kind of performance guarantees studied in reinforcement learning as a subfield of artificial intelligence. Dual Process theories with an implicit and an explicit part (often called system 1 and system 2) have a strong
position in cognitive science with ample empirical support
(Evans, 2003), though still considered an approximation of a
more complex reality with overlapping functionality.
In our framework, an agent consists of a decision function and a hypothesis generating function. The hypothesis
generating function feeds the decision function a finite class
of environments at every time step and the decision function chooses an action/policy given such a class. We define
agents within this framework by combining optimistic decision functions with hypothesis generating functions defined
by enumerating a countable class and introducing new environments from this list (which could be sampled incrementally with a simplicity bias formalized by Kolmogorov complexity (Sunehag and Hutter, 2013)) when we are within a
given error budget. We present results for generic countable
classes by extending the agents introduced in Sunehag and
Hutter (2012a) from the finite case.
The best bounds for fully general reinforcement learning
have a linear dependence on the number of environments in
the class. Though this is easily seen to be the best one can
do in general (Lattimore et al., 2013), it is bad (exponentially
worse) compared to what we are used to from Markov Decision Processes (MDPs) (Lattimore and Hutter, 2012) where
the linear (up to logarithms) dependence is on the size of the
state space instead. We introduce environment classes that

2949

are much more general than MDPs, while they are finitely
generated (by laws) in a way enabling a good bound.
Outline. We begin by introducing background and notation for general reinforcement learning and then we introduce
our agent framework and provide examples of agents that fit
within it. Within this framework we then provide bounds on
the number of errors that an optimistic agent makes in the
case of an infinite countable class of possible environments
which the agent includes incrementally in its environment
class. Finally we extend (and improve) the results to the case
of environments generated by laws and then conclude.
General Reinforcement Learning. We will consider an
agent (Russell and Norvig, 2010; Hutter, 2005) that interacts
with an environment through performing actions at from a finite set A and receives observations ot from a finite set O and
rewards rt from a finite set R ⊂ [0, 1] resulting in a history
ht := o1 r1 a1 , ..., ot rt . These sets can be allowed to depend on
time or context but we do not write this out explicitly. Let
H := ∪n (O × R × A )n × (O × R ) be the set of histories and
let ε be the empty history. A function ν : H × A → O × R
is called a deterministic environment. A function π : H → A
is called a (deterministic) policy or an agent. We define the
value function V based on geometric discounting (discount
i−t
factor 0 ≤ γ < 1) by Vνπ (ht−1 ) = ∑∞
i=t γ ri where the sequence ri are the rewards achieved by following π from time
step t onwards in the environment ν after having seen ht−1 .
Instead of viewing the environment as a function H × A →
O × R we can equivalently write it as a function ν : H ×
A × O × R → {0, 1} where we also write ν(o, r|h, a) for the
function value ν(h, a, o, r) (which is not the probability of the
four-tuple). It equals zero if in the first formulation (h, a) is
not sent to (o, r) and 1 if it is. In the case of stochastic environments we instead have a function ν : H × A × O × R →
[0, 1] such that ∑o,r ν(o, r|h, a) = 1 ∀h, a. Furthermore, we
define ν(ht |π) := Πti=1 ν(oi ri |ai−1 , hi−1 ) where ai = π(hi ).
ν(·|π) is a probability measure over strings and we define
ν(·|π, ht−1 ) by conditioning ν(·|π) on ht−1 . Vνπ (ht−1 ) :=
i−t
∗
π
Eν(·|π,ht−1 ) ∑∞
i=t γ ri and Vν (ht−1 ) := maxπ Vν (ht−1 ).
Examples of agents: AIXI and Optimist. Given a countable
class of environments M and strictly positive prior weights
wν for all ν ∈ M , we define the a-priori environment ξ by letting ξ(·) = ∑ wν ν(·) and the AIXI agent (in its general form)
/ The
is defined by following the policy π∗ := arg maxπ Vξπ (0).
above agent, and only agents of that form, satisfy the strict
rationality axioms presented in Sunehag and Hutter (2011)
while the slightly looser version from Sunehag and Hutter
(2012b) allows for optimism. The optimist takes the decision

we will call a decision function and a hypothesis generating
function. Within this framework, we will extend the agents
and analysis from the previous section to arbitrary infinitely
countable classes.

Decision Functions
The primary component of our agent framework is a decision
function f : M → A (M is the set of finite sets of environments) only depending on a class of environments M . The
decision function is independent of the history, however, the
class M fed to the decision function introduce an indirect dependence. For example, the environments at time t + 1 can
be the environments at time t, conditioned on the new observation. In this setting we will often write the value function
without an argument due to this independence. Vνπ̃t = Vνπ0 (ht )
if νt = ν0 (·|ht ) where the policy π̃ on the left hand side is the
same as the policy π on the right, just after ht have been seen
so it starts at a later stage, meaning π̃(h) = π(ht h) where ht h
is a concatenation.
Definition 1 (Rational Decision Function). Given alphabets
A , O and R we say that a decision function f : M → A is
a function f (M ) = a that for any class of environments M
based on those alphabets and finite history produces an action a ∈ A . We say that f is strictly rational for the class M
if there are ων ≥ 0, ν ∈ M , ∑ν∈M wν = 1 such that a = π(ε)
for a policy
π ∈ arg max ∑ ωνVνπ .
(1)
π

Agents who are as in Definition 1 are also called admissible
if wν > 0 ∀ν ∈ M since then they are Pareto optimal (Hutter,
2005). Being Pareto optimal means that if another agent (of
this form or not) is strictly better (higher expected value) than
a particular agent of this form in one environment, then it is
strictly worse in another. A special case is when |M | = 1 and
(1) then becomes
π ∈ arg max Vνπ
π

where ν is the environment in M . The more general case connects to this by letting ν̃(·) := ∑ν∈M wν ν(·). The next definition defines optimistic decision functions. They only coincide
with strictly rational ones (as defined above, see Sunehag and
Hutter (2011) for details) for the case |M | = 1. However,
agents based on such decision functions satisfy the looser axioms that define (a weaker form of) rationality in Sunehag
and Hutter (2012b).
Definition 2 (Optimistic Decision Function). We call a decision function f optimistic if f (M ) = a implies that a = π(ε)
for an optimistic policy π, i.e. for

/
π◦ := arg max max Vξπ (0)
π

ν∈M

π ∈ arg max max Vνπ .

ξ∈Ξ

π

for a finite set of beliefs (environments) Ξ.

An agent framework with growing classes
In this section, we introduce an agent framework that we can
fit some existing successful agents into by a choice of what

ν∈M

(2)

Agents Based on Decision Functions
Given a decision function, what remains to create a complete
agent is a hypothesis generating function g(h) = M that for
any history h ∈ H produces a set of environments M . A

2950

special case of a hypothesis generating function is defined
by combining the initial g(ε) = M0 with an update function
ψ(Mt−1 , ht ) = Mt . An agent, i.e. a function from histories
to actions, is defined from a hypothesis generating function g
and a decision function f by choosing action a = f (g(h)) after seeing history h. We discuss a number of examples below
to elucidate the framework and as a basis for the results we
present later.
Example 3. Suppose that ν is a stochastic environment and
g(h) = {ν(·|h)} for all ν and let f be a strictly rational decision function. This agent is a rational agent in the stricter
sense . Also, if g(h) = {ν(·|h) |ν ∈ M } for all h ∈ H (same
M for all h) and there are ων > 0, ν ∈ M , ∑ν∈M wν = 1 such
that a = π(ε) for a policy
π ∈ arg max
π

∑

ωνVνπ ,

(3)

ν∈g(h)

then we say that we have a Bayesian agent, which can be represented more simply in the first way by g(h) = {∑ wν ν(·|h)}.

in a hypothesis generating function. In the choice of hypothesis generating functions we are going to focus on what kind
of performance can be guaranteed in terms of how many suboptimal decisions will be taken. First, however, we want to
restrain ourselves to hypothesis generating functions that are
following Epicurus’ principle that says that one should keep
all consistent hypotheses. In the case of deterministic environments it is clear what it means to have a contradiction between a hypothesis and an observation while in the stochastic
case it is not. One can typically only say that the data make
n hypothesis unlikely as in Example 5. We will consider the
hypothesis generating function to satisfy Epicurus if the update function is such that it might add new environments in
any way while removing environments if a hypothesis is implausible (likely to be false) in light of the observations made.
Aside from satisfying Epicurus’ principle, we will design
hypothesis generating functions based mainly on wanting few
mistakes to be made. For this purpose we first define the term
ε-error. We are going to formulate the rest of the definitions
and results in this section for γ = 0 for simplicity and brevity.

Example 4. Suppose that M is a finite class of deterministic
environments and let g(h) = {ν(·|h) | ν ∈ M consistent with
h}. If we combine g with the optimistic decision function we
have defined the optimistic agents for classes of deterministic environments from Sunehag and Hutter (2012a). We here
extend the analysis to infinite classes by letting g(ht ) contain
new environments that were not in g(ht−1 ).

Definition 8 (ε-error). Given 0 ≤ ε < 1, we define the number
of ε-errors in history h to be

Example 5. Suppose that M is a finite class of stochastic environments and that g(h) = {ν(·|h) | ν ∈ M }. If we combine
g with the optimistic decision function we have defined the
optimistic AIXI agent from Sunehag and Hutter (2012b). If
instead g(h) = {ν(·|h) | ν ∈ M : maxν(h)ν̃(h) ≥ z} for some z > 0
ν̃
we have defined the optimistic agent with stochastic environments from Sunehag and Hutter (2012a).

Since we consider a setting where the true environment is
unknown, an agent cannot know if it has made an ε-error or
not. However, if one assumes that the true environment is in
the class g(ht ), or more generally that the class contains an
environment that is optimistic with respect to the true environment, and if the class is narrow in total variation distance
in the sense that the distance between any pair of environments in the class is small, then one can guarantee that an
ε-error is not made (Sunehag and Hutter, 2012a). Since we
do not know if this extra assumption holds for g(ht ), we will
use the terms ε-confident and ε-inconfident.
If the value functions in the class g(ht ) differ in their predicted value by more than ε > 0, then we cannot be sure not
to make an ε-error even if we knew that the true environment
is in g(ht ). We call such points ε-inconfidence points.

Example 6. The Model Based Interval Estimation (MBIE)
(Strehl et al., 2009) method for Markov Decision Processes
(MDPs) defines g(h) as a set of MDPs (for a given state
space) with transition probabilities in confidence intervals
calculated from h. This is combined with the optimistic decision function.
Example 7. Agents that switch explicitly between exploration
and exploitation are typically not satisfying our (weak) rationality demand. An example is Lattimore et al. (2013) where
the introduced Maximum Exploration Reinforcement Learning (MERL) agent performs certain tests when the remaining
candidate environments are disagreeing sufficiently. This decision function is not satisfying rationality while it is proven
that MERL satisfies near-optimal sample complexity for general reinforcement learning with finite classes. Another example of this kind of agent is BayesExp (Lattimore, 2013).
Properties of hypothesis generating functions. After seeing examples of decision functions and hypothesis generating
functions above, we will discuss what properties are desirable

m(h, ε) = |{i ≤ `(h) | Vµai (hi ) < Vµ∗ (hi ) − ε}|
where µ is the true environment, `(h) is the length of h, ai is
the i:th action and Vµ∗ (h) = arg maxa Vµa (h). Each such timepoint is called an ε-error.

Definition 9 (ε-(in)confidence). Given 0 < ε < 1, we define
the number of ε-inconfidence point in the history h to be
n(h, ε) := |{i ≤ l(h) |

max

ν1 ,ν2 ∈g(hi )

∗

∗

|Vνπ1 −Vνπ2 | > ε}|

where π∗ := arg maxπ maxν∈g(ht ) Vνπ . In the γ = 0 case studied
here, we can equivalently use a∗ := arg maxa maxν∈g(ht ) Vνa
instead of π∗ . The individual time-points are the points
of ε-inconfidence and the other points are the points of εconfidence.
Hypothesis generating functions with budget. We suggest defining a hypothesis generating function from a count-

2951

able enumerated class M based on a budget function for εinconfidence. The idea is simply that when the number of
ε-inconfidence points is below budget we introduce the next
environment in the class. The intuition is that if the current
hypotheses are frequently contradictory, then one should resolve this before adding more. The definition is also mathematically convenient for proving bounds on ε-errors. Besides
the budget function we also require a criterion for excluding
environments.
Definition 10 (Hypothesis generation with budget and exclusion function).
Suppose we have a chosen accuracy ε > 0, an enumerated
countable class of environments M , a finite initial class
M 0 ⊂ M a non-decreasing budget function N : N → N such
that N(t) → ∞ as t → ∞, an exclusion function (criterion)
φ(M̃ , h) = M̂ for M̃ ⊂ M and h ∈ H such that M̂ ⊂ M̃ .
Then the hypothesis generating function g with class M , initial class M 0 , accuracy ε > 0, budget N and exclusion criterion φ is defined recursively as follows:
Let g(ε) = M 0 . If n(ht , ε) ≥ N(t), then
g(ht ) = {ν(·|ht ) | ν ∈ φ({ν ∈ M | ν(·|ht−1 ) ∈ g(ht−1 )}, ht )}
while if n(ht , ε) < N(t), let ν̃ be the environment in M with
the lowest index that is not in ∪t−1
i=1 {ν ∈ M | ν(·|hi ) ∈ g(hi )}
(i.e. the next environment to introduce) and let g(ht ) =
{ν(·|ht ) | ν ∈ {ν̃ ∪ φ({ν ∈ M | ν(·|ht−1 ) ∈ g(ht−1 )}, ht )}}.

Error Analysis
We will now extend the agents described in Example 4 and
Example 5 by removing the demand for the class M being
finite and analyze the effect on the number of ε-errors made.
We will still use the optimistic decision function and apply it
to finite classes but we will keep adding environments from
the full class to the finite working class of environments. The
resulting agents differ from agents such as the one in Example 7 by (among other things) instead of having exploration
phases as part of the decision function, it has a hypothesis generating function that sometimes adds an environment
which may cause new explorative behavior if it becomes the
optimistic hypothesis and it deviates significantly from the
other environments. A nice point about our results is that one
chooses the asymptotic rate, however one gets a worse constant the better rate one chooses. This is due to the fact that if
one includes new environments at a slower rate it takes longer
until the right environment is introduced while the error rate
afterwards is better. If one knew that one had included the
right one, then one would stop introducing more.
We extend the agent for finite classes of deterministic environments in Example 4 to the countable case and we leave the
extension of the stochastic case to a longer report. In the finite case with a fixed class, the proof of the finite error-bound
in Sunehag and Hutter (2012a) builds on the fact that every
ε(1−γ)
time-steps before a contraε-error must be within − log1−γ
diction and the bound followed immediately by only being

able to have at most |M −1| contradictions. In the case where
environments are being added one can have errors either before the truth is added or within that many time-steps before
a contradiction or that many time-steps before the addition of
a new environment. The addition of a new environment can
change the optimistic policy without having encountered a
contradiction, the event temporarily breaks time-consistency.
Hence, every added environment after the truth has been inε(1−γ)
ε-errors. In the γ = 0 case
cluded can add at most 2 − log1−γ
it is only at contradictions and when the truth has not been
added that we can have errors.
Theorem 11. Suppose we have a countable class of deterministic environments M (with a chosen enumeration and
containing the true one). Also suppose we have a hypothesis
generating function g with a finite initial class g(ε) := M 0 ⊂
M , budget function N : N → N, accuracy ε = 0 and suppose
that g excludes contradicted environments. π◦ is defined by
combining g with an optimistic decision function. The number of 0-errors m(ht , 0) is at most n(ht , 0) + C for some constant C > 0 (which is the number of steps before the true environment is introduced and depends on the choice of budget
function N but not on t). Furthermore, ∀i ∈ N there is ti ∈ N
such that ti < ti+1 and n(hti , 0) < N(ti ).
The last claim is the most important saying that we will
always see the number of errors fall within the budget N(t)
again (except for a constant term) even if it can be temporarily above. This means that we will always introduce more
environments and exhaust the class in the limit. If we wanted
the errors to always be within N(t) (except for a constant) we
could forbid the agent from introducing more environments
than N(t) before time t. This is because the number of excluded hypotheses cannot exceed the number of introduced
hypotheses including the ones in g(ε) and we only have an
error when an environment is excluded (and not always then)
or when g(ht ) is empty which it is not again after the true
environment is introduced.
Proof. Suppose that at time t, the true environment µ is in
g(ht ). Then, if we do not have a 0-inconfidence point, it follows from optimism that
◦

Vµπ (ht ) = max Vµa (ht )
a

(4)

since all the environments in g(ht ) agree on the reward for
the optimistic action. Hence m(ht , 0) ≤ n(ht , 0) +C where C
is the time the true environment is introduced. However, we
need to show that it will be introduced by proving that the
class will be exhausted in the limit. If this was not the case,
then there is T such that n(0, ht ) ≥ N(t) ∀t ≥ T . Since we
have 0-inconfidence points exactly when we are guaranteed
to have a contradiction, n(0, ht ) is then bounded by the number of environments that have been introduced up to time t
if we include the number of environments in the initial class.
Hence n(0, ht ) is bounded by a finite number while (by the
definition of budget function) N(t) → ∞ which contradicts
the assumption.

2952

Remark 12 (Extensions: γ > 0, Separable classes). As in the
deterministic case, what makes the γ = 0 case simpler than
the 0 < γ < 1 case is that ε-errors can for γ > 0 also occur
ε(1−γ)
time-steps before a new environment is inwithin − log1−γ
troduced. Further, one can extend our algorithms for countable classes to separable classes since they can be covered by
countable many balls of arbitrarily small radius.

Environments Defined by Laws
In this section we will investigate classes of environments of
a special but generic form combining a number of laws that
partially determine what happens next. Classes of such form
have the property that one can exclude (or merge) laws and
thereby exclude (or merge) whole classes of environments
like when one learns about a state transition when working
with MDPs. Our setting is, however, far more general than
MDPs and shares characteristics with what has been studied
in the Learning Classifier Systems literature (Holland, 1986;
Drugowitsch, 2007).
We consider observations of the form of a feature vector
o = x̄ = (x j )mj=1 ∈ O = ×mj=1 O j (including the reward as one
coefficient) where x j is an element of some finite alphabet Oi .
Definition 13. A law is a function τ : H × A → Õ where Õ
consists of the feature vectors from O but where some elements are replaced by a special letter ⊥ meaning that there
is no prediction for this feature, i.e. Õ = ×mj=1 (O j ∪ {⊥}).
Using a feature vector representation of the observations
and saying that a law predicts some of the features is a convenient special case of saying that the law predicts that the next
observation will belong to a certain subset of the observation
space.
We first consider deterministic laws. Each law τ predicts,
given the history and a new action, some (or none) but not
necessarily all of the features x j at the next time point. We
first define a setting where the set of laws is such that in every
situation and for every feature there is at least one law that
makes a prediction of this feature in the given situation. We
can then directly define a set of environments by combining
such laws.
Definition 14. [Environments from deterministic laws] Given
a finite set of laws T (maps from H × A to Õ) such that O =
× j O j have m features labelled 1, ..., m. We define the class of
environments

M (T ) := {ν |∃T̃ ∈ C (T ) : ν = ν(T̃ )}
where the set of coherent and complete sets of deterministic
laws C (T ) is defined by

C (T ) := {T̃ ⊂ T : ∀h, a∀ j ∈ {1, ..., m}∃τ ∈ T̃ :
ν(h, a)( j) 6= ⊥ ∧ τ̃(h, a)( j) = ⊥∀τ̃ ∈ T̃ \ {τ}}
and for T̃ ∈ C (T ), ν(T̃ ) is the environment ν which is such
that
∀h, a∀ j ∈ {1, ..., m}∃τ ∈ T̃ : ν(h, a)( j) = τ(h, a)( j).

Example 15. Consider an environment with a constant binary feature vector of length m. There are 2m such environments. Every such environment can be defined by combining
m out of a class of 2m laws. Each law says what the value of
one of the features is, one law for 0 and one for 1.
We analyze the optimistic agent from Example 4 in this
new setting. Every contradiction of an environment is a contradiction of at least one law and there are finitely many laws
and this is what is needed for the finite error result from before to hold but with |M | replaced by |T | (see Theorem 16
below) which can be exponentially smaller. Furthermore, the
extension to countable T works the same as in Theorem 11.
Theorem 16 (Finite error-bound when using laws). Suppose that T is a finite class of deterministic laws and let
g(h) = {ν(·|h) | ν ∈ M ({τ| τ ∈ T consistent with h})} . We
define π̄ by combining g with the optimistic decision function.
Following π̄ with a finite class of deterministic laws T in an
environment µ ∈ M (T ),
|Vµπ̄ (ht ) − max Vµπ (ht )| < ε

(5)

π

ε(1−γ)
time steps t where l is the
for all but at most |T − l| − log1−γ
minimum number of laws from T needed to define a complete
environment.

Proof. This is true for the same reason as the finite-error
bound in Sunehag and Hutter (2012a) since there are at most
|T − l| time-steps with a contradiction and errors occur only
ε(1−γ)
at times that are at most − log1−γ
steps before a contradiction. This is due to time-consistency of geometric discounting.
Background Environments. Further improvement in the
rate of errors can be achieved if we have a background environment and the laws are only replacing the prediction of
the background environment for the part they say something
about. Again if one formalizes this notion correctly, one can
prove bounds linear in the number of laws which can be much
fewer in this situation.
Computing the optimistic decision as one planning problem. Finding the optimistic decision with a collection of laws
results in a computation as in an auction-based multi-agent
planning system (as in e.g. Allard and Shekh (2012)), though
many of the laws are making incorrect predictions. The combined choice of laws and action to use forms a larger action
space as in Asmuth et al. (2009), though for a much more
general situation. Monte-Carlo Tree Search methods (Silver
and Veness, 2010), which are also planning methods, could
be applied despite that some of the laws (which are also selected instead of only choosing actions in the search) are incorrect and contradict each other. Value predictions resulting
from function approximation can be very useful for guiding
the tree search (Silver et al., 2012). Human cognition likely
involves both estimation of models as well as direct value estimation (Shteingart and Loewenstein, 2014).

2953

Conclusions
We introduced a dual process framework based on a hypothesis function and a decision function. An optimistic decision function was found to be useful for achieving optimality
guarantees while a simplicity bias can be useful for hypothesis generation (Sunehag and Hutter, 2013). A key point is
that optimism encourages exploration, which is important if
one cannot optimize a strategy for one’s whole life. Further
when acting according to an optimistic hypothesis it is only
important that it is not contradicted while it does not have to
be correct for other circumstances. One will still be highly
rewarded.

References
Allard, T. and Shekh, S. (2012). Hierarchical multi-agent
distribution planning. In AI 2012: Advances in Artificial
Intelligence, volume 7691 of Lecture Notes in Computer
Science, pages 755–766.
Asmuth, J., Li, L., Littman, M., Nouri, A., and Wingate, D.
(2009). A bayesian sampling approach to exploration in
reinforcement learning. In Uncertainty in Artificial Intelligence (UAI), pages 19–26.
Carver, C. S., Scheier, M. F., and Segerstrom, S. C. (2010).
Optimism. Clinical Psychology Review, 30(7):879–889.
Chater, N. and Vitanyi, P. (2003). Simplicity: A unifying
principle in cognitive science? Trends in Cognitive Sciences, 7:19–22.
Drugowitsch, J. (2007). Learning Classifier Systems from
First Principles: A Probabilistic Reformulation of Learning Classifier Systems from the Perspective of Machine
Learning. Technical report (University of Bath. Dept.
of Computer Science). University of Bath, Department of
Computer Science.
Evans, J. S. (2003). In two minds: dual-process accounts of
reasoning. Trends in Cognitive Sciences, 7(10):454 – 459.
Griffiths, T. L., Kemp, C., and Tenenbaum, J. B. (2008).
Bayesian models of cognition. In Sun, R., editor, Cambridge handbook of computational cognitive modeling,
pages 59–100. Cambridge University Press, Cambridge.
Holland, J. (1986). Escaping brittleness: The possibilities
of general purpose learning algorithms applied to parallel
rule-based systems. In Michalski, R., Carbonell, J., and
Mitchell, T., editors, Machine learning: An artificial intelligence approach, volume 2, chapter 20, pages 593–623.
Morgan Kaufmann, Los Altos, CA.
Hutter, M. (2005). Universal Articial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer,
Berlin.
Kahneman, D. (2011). Thinking, fast and slow. Farrar, Straus
and Giroux, New York.
Lattimore, T. (2013). Theory of General Reinforcement
Learning (submitted). PhD thesis, Australian National
University.

Lattimore, T. and Hutter, M. (2012). PAC Bounds for Discounted MDPs. In Bshouty, N. H., Stoltz, G., Vayatis, N.,
and Zeugmann, T., editors, ALT, volume 7568 of Lecture
Notes in Computer Science, pages 320–334. Springer.
Lattimore, T., Hutter, M., and Sunehag, P. (2013). The
sample-complexity of general reinforcement learning.
Journal of Machine Learning Research, W&CP: ICML,
28(3):28–36.
Russell, S. J. and Norvig, P. (2010). Artificial Intelligence:
A Modern Approach. Prentice Hall, Englewood Cliffs, NJ,
3nd edition.
Sharot, T., Riccardi, A. M., Raio, C. M., and Phelps, E. A.
(2007). Neural mechanisms mediating optimism bias. Nature, pages 1–5.
Shteingart, H. and Loewenstein, Y. (2014). Reinforcement
learning and human behavior. Current Opinion in Neurobiology, 25(0):93 – 98.
Silver, D., Sutton, R., and Müller, M. (2012). Temporaldifference search in computer Go. Machine Learning,
87(2):183–219.
Silver, D. and Veness, J. (2010). Monte-Carlo Planning in
Large POMDPs. In Advances in Neural Information Processing Systems 23: 2010., pages 2164–2172.
Strehl, A. L., Li, L., and Littman, M. L. (2009). Reinforcement learning in finite MDPs: PAC analysis. Journal of
Machine Learing Research, 10:2413–2444.
Sunehag, P. and Hutter, M. (2011). Axioms for rational reinforcement learning. In Algorithmic Learning Theory,
(ALT’2011), volume 6925 of Lecture Notes in Computer
Science, pages 338–352. Springer.
Sunehag, P. and Hutter, M. (2012a). Optimistic agents are
asymptotically optimal. In Proc. 25th Australasian Joint
Conference on Artificial Intelligence (AusAI’12), volume
7691 of LNAI, pages 15–26, Sydney, Australia. Springer.
Sunehag, P. and Hutter, M. (2012b). Optimistic AIXI. In
Proc. 5th Conf. on Artificial General Intelligence (AGI’12),
volume 7716 of LNAI, pages 312–321. Springer, Heidelberg.
Sunehag, P. and Hutter, M. (2013). Learning agents with
evolving hypothesis classes. In Proceedings of the 6th International Conference on AGI, volume 7999 of Lecture
Notes in Computer Science, pages 150–159. Springer.
Szita, I. and Lörincz, A. (2008). The many faces of optimism: a unifying approach. In Proceedings of the 20th International Conference on Machine Learning, pages 1048–
1055.
Taylor, S. E. and Brown, J. D. (1988). Illusion and wellbeing: a social psychological perspective on mental health.
Psychological bulletin, 103(2):193.
Wallston, K. A. (1994). Cautious optimism vs. cockeyed optimism. Psychology & Health, 9(3):201–203.

2954

