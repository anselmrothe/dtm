UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Predicting Behavior from the World: Naive Behaviorism in Lay Decision Theory

Permalink
https://escholarship.org/uc/item/1pm9k1j7

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)

Authors
Johnson, Samuel
Rips, Lance

Publication Date
2014-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Predicting Behavior from the World:
Naïve Behaviorism in Lay Decision Theory
Samuel G. B. Johnson (samuel.johnson@yale.edu)
Department of Psychology, Yale University
2 Hillhouse Ave., New Haven, CT 06520 USA

Lance J. Rips (rips@northwestern.edu)
Department of Psychology, Northwestern University
2029 Sheridan Road, Evanston, IL 60208 USA
Abstract

situational constraint), that the Mercury would leave
sufficient space for his Honda (an end-state). Using this
non-mentalistic, behaviorist system only requires seeking
out and representing information about the world—and no
inferences about the mental states of the Mercury’s driver.
Infants can use world-based cues such as efficiency
constraints to reason about behavior before achieving a
representational theory of mind (Gergely & Csibra, 2003),
suggesting that a primitive, behaviorist system is present
in infancy. The behaviorist system therefore seems to
precede the mentalistic system in development (see also
Povinelli & Vonk, 2004 on chimpanzee theory of mind).
However, it is unclear whether the behaviorist system
used by infants is replaced by the mentalistic system that
we use as adults, or whether instead these systems coexist
in adulthood. If these systems coexist, many of our
everyday inferences about behavior may bypass mentalstate inferences altogether, relying instead on directly
observable information about the world, coupled with
more general assumptions such as the efficiency of
actions in achieving optimal end-states.
Here, we test the possibility of a behaviorist system by
studying judgments about agents making decisions under
uncertainty, contrasting inferences about knowledgeable
agents—those who know the efficacies of each option
under consideration—and inferences about ignorant
agents—those who do not know the efficacies of the
options. For example, consider Jill, who wants her hair to
smell like apples and is deciding which of three brands of
shampoo to purchase: one with a high probability of
leading to her goal (“Best”), one with a medium
probability (“Middle”), and one with a low probability
(“Worst”). Which option will Jill choose?
Two principles could potentially be used for predicting
Jill’s choice. First, people might use the Efficiency
Principle (Dennett, 1987), which would lead Jill to
choose Best—the optimal action relative to her goals.
This principle alone would not lead Jill to be any more
likely to choose Middle than to choose Worst, since both
are inefficient relative to Best. Second, people might use a
Preference Principle, which would lead Jill to form
preferences for the options in proportion to their quality,
and be more likely to choose more preferred options—
that is, to be most likely to choose Best, less likely to

Life in our social world depends on predicting and
interpreting other people’s behavior. Do such inferences
always require us to explicitly represent people’s mental
states, or do we sometimes bypass such mentalistic
inferences and rely instead on cues from the environment?
We provide evidence for such behaviorist thinking by
testing judgments about agents’ decision-making under
uncertainty, comparing agents who were knowledgeable
about the quality of each decision option to agents who
were ignorant. Participants believed that even ignorant
agents were most likely to choose optimally, both in
explaining (Experiment 1) and in predicting behavior
(Experiment 2), and assigned them greater responsibility
when acting in an objectively optimal way (Experiment 3).
Keywords: Theory of mind; lay decision theory;
explanation; prediction; rationality.

Introduction
Sunny turned on his Honda’s right blinker as he drove
down Dixwell Avenue. The Mercury to his right slowed
down, and Sunny changed lanes. In changing lanes,
Sunny wagered with his life—gambling that the driver of
the Mercury would leave enough space for his Honda to
enter the right lane—and he won. Indeed, his track record
with such wagers is remarkable. How is Sunny able to
make such successful predictions about others’ behavior?
One strategy that Sunny may have followed in this case
was to infer the driver’s behavior based on his or her
inferred mental-states. That is, Sunny may have reasoned
that the Mercury’s slowing down was a signal of the
driver’s intention to let him change lanes, based on the
driver’s assumed beliefs about road behavior and folk
physics, and the driver’s assumed goals of being a good
road citizen and avoiding a collision. Using this
mentalistic system requires inferring and representing the
agent’s mental states, then predicting and interpreting
actions on the basis of those inferred mental states. This
seems to accord with how we typically experience the
process of making behavior inferences in day-to-day life.
But Sunny could have reached the same conclusion
using a different strategy, inferring the Mercury’s
behavior based on observable states of the world. Sunny
may have inferred from the Mercury’s change in speed
(an action), combined with the geometry of driving (a

695

choose Middle, and least likely to choose Worst.
To see how this task can give evidence for a behaviorist
system, first consider what a normative response pattern
would be if people correctly use mental-state inferences.
If Jill knows the probabilities of all three options (i.e., if
she is a knowledgeable agent), then either the Efficiency
or the Preference Principle potentially apply, and we
would certainly expect her to be more likely to choose a
higher-quality option. On the other hand, if Jill does not
know the efficacies of the options (i.e., she is an ignorant
agent), then we should normatively conclude that she is
equally likely to choose any of the three options, because
she does not have any relevant beliefs. Thus, if people
rate Jill’s likelihood of choosing the three options
differently even when she is ignorant, this inference could
not be produced by veridical mental-state reasoning.
Instead, people might overgeneralize these principles to
ignorant agents for whom they do not apply—using
behaviorist reasoning that bypasses reasoning about the
agents’ beliefs. It is particularly plausible that behaviorist
reasoning could lead to overgeneralization of the
Efficiency Principle, since young infants can use
efficiency to constrain behavior predictions in a
presumably non-mentalistic manner (Gergely & Csibra,
2003).
We compare inferences about knowledgeable and
ignorant agents, using judgments about explanation
(Experiment 1), prediction (Experiment 2), and
responsibility (Experiment 3). We also test whether
people conceptualize suboptimal actions and omissions
differently (Experiment 2), and whether people reinterpret
mental states to rationalize otherwise suboptimal behavior
(Experiment 3), for knowledgeable and ignorant agents.
Throughout these experiments, we gather evidence for a
non-mentalistic, behaviorist system with distinct
signatures from the representational theory of mind that
we are accustomed to using in everyday experience.

also participated in another experiment, with the order of
the experiments counterbalanced. Sixteen participants
were excluded from data analysis because they incorrectly
answered more than 33% of a series of check questions
designed to ensure that participants had attended to the
details of the vignettes (including whether the agent was
knowledgeable or ignorant). However, including all
participants does not qualitatively alter these results.
Participants read three vignettes. The agent’s choice
(Best, Middle, Worst) varied within-subjects across three
cover stories using a Latin square, and the agent’s
knowledge about the options (knowledgeable or ignorant)
varied between-subjects. In the knowledgeable condition,
the agent knew the efficacies of each option. For example:
Jill is shopping for a new shampoo, and wants her hair
to smell like apples. She is considering three brands
of shampoo to use.
She knows that if she uses Variety JLR, there is a 70%
chance that her hair will smell like apples; that if she
uses Variety WYQ, there is a 50% chance that her
hair will smell like apples; and that if she uses
Variety HPN, there is a 30% chance that her hair
will smell like apples.
Jill chooses Variety [JLR/WYQ/HPN], and her hair
smells like apples.
In the ignorant condition, the agent was said to believe
(incorrectly) that all three formulas had a 70% efficacy,
but the actual probabilities were listed for the participant.
The Best, Middle, and Worst versions of each problem
differed only in Jill’s actual choice (JLR, WYQ, or HPN).
Participants then completed the need for explanation
measure (“To what extent do you feel that an explanation
is necessary for Jill’s behavior?”) on a 0-to-10 scale (0:
“explanation definitely not necessary”; 5: “neither
necessary nor unnecessary”; 10: “explanation definitely
necessary”). Vignettes were presented in a random order.

Results and Discussion

Experiment 1

Participants took both the efficacy of the agent’s choice
and the agent’s knowledge into account in determining
whether an explanation was necessary. As Figure 1
shows, in both conditions, participants rated Best choices
as least in need of explanation, but the effect of choice
differed between the knowledgeable and ignorant
conditions. There was no main effect of knowledge,
F(1,82) = 1.82, p = .18, ηp2 = .02, but both the main effect
of choice, F(2,164) = 46.35, p < .001, ηp2 = .36, and the
interaction between knowledge and choice, F(2,164) =
5.90, p = .003, ηp2 = .07, were significant. Knowledgeable
agents’ decisions were rated more in need of explanation
for Middle than for Best, t(45) = 5.22, p < .001, d = 0.77,
and more for Worst than for Middle, t(45) = 4.23, p <
.001, d = 0.62. In contrast, although ignorant agents’
decisions were rated more in need of explanation for
Middle than for Best, t(37) = 3.21, p = .003, d = 0.52, the
difference between Worst and Middle only reached
marginal significance, t(37) = 1.72, p = .095, d = 0.28.

In our first study, we used participants’ ratings of the need
for an explanation to measure expectations about
behavior. Since anomalous events act as triggers for
explanation (e.g., Hilton & Slugoski, 1986), participants
should indicate a higher need for explanation to the
degree that agents’ choices violate their expectations, just
as infants look longer at suboptimal than at optimal
actions (e.g., Gergely & Csibra, 2003). If people use only
normative mentalistic reasoning, one would expect them
to rate optimal decisions less surprising than suboptimal
decisions for agents who are aware of the relative quality
of the choices. But if people supplement mental-state
inferences with behaviorist thinking, then they may also
predict optimal choices even for ignorant agents.

Method
We recruited 100 participants from Amazon Mechanical
Turk in exchange for a small payment. These participants

696

Experiment 2
Our primary goal in Experiment 2 was to replicate these
findings using a different dependent measure—explicit
behavior predictions. In addition, we manipulated whether
the Worst option was framed as an action or as an
omission (e.g., Ritov & Baron, 1992). In contrast to
Experiment 1, where suboptimal choices were seen as
equally surprising for ignorant agents, behaviorist
thinking could potentially lead people to distinguish
between suboptimal actions and suboptimal omissions,
since an option’s being an action or an omission is a
salient feature of the world.

Method
We recruited 100 participants from Amazon Mechanical
Turk in exchange for a small payment. These participants
additionally participated in another experiment that is not
reported here, with the order of the experiments
counterbalanced. Three participants were excluded from
analysis because they incorrectly answered more than
33% of the check questions.
Participants read two vignettes, with worst-option
framing (action or omission) varied within-subjects across
two different cover stories, and knowledge of the
probabilities (knowledgeable or ignorant) varied betweensubjects. In the knowledgeable condition, the agent was
said to know the probabilities of each option leading to
their goal. For example (differences between the action
and omission conditions in brackets):
Angie has a shrub, and wants the shrub’s flowers to
turn red. She is thinking about applying a fertilizer,
and has three options: applying [Formula LPN /
nothing], applying Formula PTY, or applying
Formula NRW.
She does not know anything about the differences
between these options, except that she knows that if
she applies [Formula LPN / nothing] there is a 10%
chance that the flowers will turn red, that if she
applies Formula PTY there is a 50% chance that the
flowers will turn red, and that if she applies Formula
NRW there is a 70% chance that the flowers will turn
red.
In the ignorant condition, the second paragraph instead
stated that the agent did not know the probabilities. In
contrast to Experiment 1 (where this was described as a
false belief), the agent was described as having no
relevant beliefs, to test the generality of our effects.
After reading each vignette, participants were asked to
“Please rate below how likely you think it is that she will
choose each option.” Participants then rated each decision
alternative on a 0-to-10 scale (0: “Very unlikely”; 5:
“Neither likely nor unlikely”; 10: “Very likely”). The
assignment of action or omission framing to the Worst
option was counterbalanced across the two cover stories,
and items were presented in a random order.

Figure 1: Results of Experiment 1. Bars represent ±1 SE.
For knowledgeable agents, people rely on the
Preference Principle, finding knowledgeable agents’
decisions to be increasingly surprising in proportion to
their poorness. More surprisingly, however, people also
take the quality of the options into account in evaluating
the decisions of ignorant agents. Although correct
mentalizing would lead people to predict the agent’s three
possible decisions as equally likely, and hence equally
surprising, they nonetheless found the optimal choice less
surprising than the middle or worst choice. Further, they
did not robustly distinguish between the Middle and
Worst options, suggesting that these inferences about
ignorant agents were made using the Efficiency Principle.
Since both the Middle and Worst options were suboptimal
or inefficient relative to the Best option, the Efficiency
Principle alone would not distinguish between these two
inefficient actions.
We take these results as evidence of behaviorist
thinking in behavior predictions—that is, relying directly
on information about the world rather than on the agents’
mental states. Could these results instead be explained by
participants’ incorrectly attributing knowledge of the
probabilities to the agents, either through inattentiveness
to the vignettes or through a perspective-taking error
(Birch & Bloom, 2007)? Inattentiveness is an unlikely
explanation because participants who failed check
questions (including questions about the agents’
knowledge) were removed from the analysis. In addition,
these explanations would not account for the qualitative
interaction, in which participants used the Preference
Principle for knowledgeable agents (distinguishing
between all three options) but the Efficiency Principle for
ignorant agents (distinguishing between optimal and
suboptimal options, but not among different suboptimal
options). In contrast, use of the Efficiency principle is a
signature of infants’ non-mentalistic behavior predictions
(Gergely & Csibra, 2003) and is therefore quite consistent
with our behaviorist account. We nonetheless sought
converging evidence in Experiments 2 and 3.

697

Results and Discussion

(A) Action Framing of Worst Option

As shown in Figure 2-A, the results for the action framing
condition were similar to the results in Experiment 1.
Under this framing, the Worst option was the choice of a
particular product (e.g., Formula LPN). Here,
knowledgeable agents were thought more likely to choose
Best than Middle, t(49) = 14.56, p < .001, d = 2.06, and
more likely to choose Middle than Worst, t(49) = 8.49, p
< .001, d = 1.20. However, ignorant agents were thought
more likely to choose Best than Middle, t(46) = 2.94, p =
.005, d = 0.43, but equally likely to choose Middle and
Worst, t(46) = 0.95, p = .35, d = 0.14. This difference led
to a significant interaction between choice and knowledge
in the action condition, F(2,190) = 87.30, p < .001, ηp2 =
.48. Mental-state inferences would lead to the prediction
that an ignorant agent is equally likely to choose any of
the options; nonetheless, even ignorant agents were
judged more likely to choose the optimal option—a
further demonstration of behaviorist thinking. Mirroring
Experiment 1, however, people thought Middle more
likely than Worst only for the knowledgeable agents.
However, Figure 2-B shows that the omission framing
produced a quite different pattern of results. Considering
just the omission condition, in which the Worst option
was described as doing nothing, the interaction between
choice and knowledge is again significant, F(2,190) =
43.21, p < .001, ηp2 = .31. But this time, the interaction
occurred because participants made more conservative
predictions about the ignorant agent, rather than because
they failed to differentiate among some of the options.
Participants thought the knowledgeable agents more
likely to choose Best than Middle, t(49) = 12.94, p < .001,
d = 1.83, and more likely to choose Middle than Worst,
t(49) = 8.91, p < .001, d = 1.26. Likewise, ignorant agents
were judged more likely to choose Best than Middle, t(46)
= 3.12, p = .003, d = 0.46, and more likely to choose
Middle than Worst, t(46) = 6.09, p < .001, d = 0.89. That
is, when a suboptimal option is framed as an omission,
people think even ignorant agents are less likely to choose
it. This result too is consistent with behaviorist thinking,
because actions and omissions are qualitatively different
choices not only in the agent’s mind, but in the world,
with actions and omissions tacitly thought to have distinct
affordances (Ritov & Baron, 1992).
As in Experiment 1, it is unlikely that participants were
incorrectly attributing knowledge of the probabilities to
the ignorant agents, because inattentive participants were
removed from the analysis and because such attributions
would lead participants to predict Middle as more likely
than Worst. Furthermore, such explanations could not
straightforwardly account for the difference between the
action and omission conditions, whereas this difference is
a natural consequence of behaviorist thinking.

(B) Omission Framing of Worst Option

Figure 2: Results of Experiment 2. Bars represent ±1 SE.
may have been acting under a different set of beliefs or
goals—a process we can call rationalizing an action
(Baker, Tenenbaum, & Saxe, 2009; Buchsbaum, Gopnik,
Griffiths, & Shafto, 2011). For example, consider Jill’s
shampoo-purchasing decision. The Efficiency Principle
predicts that Jill should prefer a shampoo with a higher
probability of resulting in a luscious apple smell. This
inference, however, relies on the assumption that Jill’s
only goal is making her hair smell like apples, and that
she had accurate information. If Jill chooses an
objectively suboptimal action (from the point of view of
making her hair smell like apples), we can rationalize
Jill’s action by denying either of these assumptions. One
might infer, for example, that Jill had changed her goal or
that Jill did not realize that Variety JLR was superior.
In Experiment 3, we used this idea to provide further
evidence that the inferences about ignorant agents in
previous experiments were due to behaviorist thinking.
We used responsibility judgments as the dependent

Experiment 3
When an agent makes an objectively suboptimal choice,
we can use the Efficiency Principle to infer that the agent

698

measure because previous studies show that people assign
greater responsibility to decision-makers who behave
optimally than to those who behave suboptimally
(Johnson & Rips, 2013). In all cases, the agent was
deciding between options with higher and lower
probabilities of the outcome, and always chose the higher
probability option. To vary the optimality of an action, we
manipulated the agent’s attitude toward that outcome
(desires the outcome, indifferent toward the outcome, or
desires that the outcome not occur). We also varied
whether the agent knew or did not know the efficacies of
the options, as in Experiments 1 and 2. The vignettes in
the knowledgeable condition were of the format:
Jill is shopping for a new shampoo, and is deciding
whether to purchase Variety JLR or Variety WYQ.
[She wants her hair to smell like apples. / It does not
matter to her whether her hair smells like apples. /
She wants her hair not to smell like apples.]
She does not know anything about the differences
between Variety JLR and Variety WYQ, except that
she knows that if she uses Variety JLR, there is a
50% chance that her hair will smell like apples, and
if she uses Variety WYQ, there is a 30% chance that
her hair will smell like apples.
Jill chooses Variety JLR, and her hair smells like
apples.
In the ignorant condition, the third paragraph instead
stated that the agent did not know the probabilities.
When Jill is indifferent to the outcome, there is no
optimal choice, so this condition acts as a baseline in both
the knowledgeable and ignorant conditions. We would
expect choices perceived as optimal (either because their
action is objectively optimal or because their action is
rationalized and made subjectively optimal) to be assigned
increased responsibility. Since people appear to apply the
Efficiency Principle even to ignorant agents, both
knowledgeable and ignorant agents should be assigned
greater responsibility when they act efficiently (i.e., when
Jill desires the outcome that is made likelier by her
choice). However, the agent’s decision is objectively
suboptimal when she desires that the outcome not occur
but nonetheless chooses the action that makes that
outcome more likely. If this suboptimal action is
rationalized, then responsibility judgments should be
higher when she desires the outcome not occur than in the
baseline condition when she is indifferent. This is because
people would attribute mental states to the agent (e.g., an
additional goal such as choosing a less expensive option)
that would make that apparently suboptimal choice
rational. If inferences about ignorant agents are made with
the behaviorist strategy, one might expect people not to
rationalize the actions of ignorant agents, leading to a
difference between the baseline and suboptimal
conditions for knowledgeable but not for ignorant agents.

Turk in exchange for a small payment. These participants
additionally participated in other experiments that are not
reported here, with the order of the experiments
counterbalanced. Forty-nine participants were excluded
from analysis because they incorrectly answered more
than 33% of the check questions.
Participants read three vignettes (similar to that given
above) in a random order. The agent’s goal (desires
outcome, indifferent to outcome, or desires that the
outcome not occur) varied within-subjects across three
cover stories using a Latin square, and the agent’s
knowledge about the probabilities (knowledgeable or
ignorant) varied between-subjects. After reading each
vignette, participants rated their agreement with a
responsibility statement (“Jill is responsible for her hair
smelling like apples”) on a 0-to-10 scale (0: “Disagree”;
5: “Neither Agree nor Disagree”; 10: “Agree”).

Results and Discussion
Responsibility ratings varied both with the agent’s goal,
F(2,416) = 8.76, p < .001, ηp2 = .04, and with the agent’s
knowledge of the probabilities, F(1,208) = 38.63, p <
.001, ηp2 = .16. As Figure 3 illustrates, however, these
effects were qualified by a significant interaction,
F(2,416) = 5.56, p = .004, ηp2 = .03.
We consider first the differences between the optimal
(desires the outcome) and the baseline (indifferent to the
outcome) conditions. Knowledgeable agents were rated
marginally more responsible in the optimal than in the
baseline condition, t(96) = 1.89, p = .062, d = 0.19. This is
consistent with previous work showing that agents who
behave optimally are assigned greater responsibility than
those who do not (Johnson & Rips, 2013). In addition,
ignorant agents were assigned greater responsibility in the
optimal than in the baseline condition, t(112) = 3.91, p <
.001, d = 0.37. This is consistent with Experiments 1 and
2, in that behaviorist thinking leads people to expect
efficient behavior for ignorant agents.

Method

Figure 3: Results of Experiment 3. Bars represent ±1 SE.

We recruited 259 participants from Amazon Mechanical

699

Our main interest, however, was in the differences
between the suboptimal (desires the outcome not occur)
and baseline conditions, which would speak to whether
participants were rationalizing the actions of suboptimal
decision-makers. Knowledgeable agents were rated more
responsible in the suboptimal than in the baseline
condition, t(96) = 4.29, p < .001, d = 0.44, suggesting that
participants rationalized the agent’s suboptimal action.
However, ignorant agents were rated equally responsible
in the suboptimal and in the baseline conditions, t(112) =
0.78, p = .44, d = 0.07, suggesting that suboptimal actions
were not rationalized for ignorant agents.
These results show that rationalizing inferences (e.g.,
attributing an additional goal to the agent to make their
suboptimal actions seem rational from the agent’s point of
view) are made when people follow a mentalizing
strategy, leading to the counterintuitive finding that
knowledgeable agents can be rated more responsible for
an outcome when they desire that it not occur than when
they are indifferent toward it. This finding did not hold
for ignorant agents, consistent with our interpretation of
Experiments 1 and 2—that people used behaviorist
thinking in interpreting the actions of the ignorant agents.
If participants had incorrectly attributed knowledge to the
ignorant agents (either because of a perspective-taking
error or because of inattentiveness), we would not expect
this interactive effect.

made only for knowledgeable but not for ignorant agents
(Experiment 3). These findings all are naturally accounted
for by behaviorist thinking that relies on cues such as
efficiency and direct information about the world.
These results complement other approaches to theory of
mind that involve multiple systems. For example, Apperly
and Butterfill (2009) proposed two systems for reasoning
about beliefs—a flexible system tracking beliefs and a
less flexible system tracking belief-like states such as
perceptual registration. Here, we have provided evidence
for an additional system that does not make mental-state
inferences of any sort but instead makes inferences using
environmental cues together with the Efficiency Principle.
We often seem to infer behavior by pondering mental
states. But to the extent that behavior can be inferred from
efficiency considerations alone (Dennett, 1987), the
behaviorist system may often suffice. It is an open
question how often we unleash the behaviorist within.

Acknowledgments
This research was partially supported by funds awarded to
the first author by the Yale University Department of
Psychology. We thank Laurie Santos and an audience at
Yale University for helpful discussion.

References
Apperly, I.A., & Butterfill, S.A. (2009). Do humans have
two systems to track beliefs and belief-like states?
Psychological Review, 116, 953–970.
Baker, C.L., Saxe, R., & Tenenbaum, J.B. (2009). Action
understanding as inverse planning. Cognition, 113,
329–349.
Birch, S.A.J., & Bloom, P. (2007). The curse of
knowledge in reasoning about false beliefs.
Psychological Science, 18, 382–386.
Buchsbaum, D., Gopnik, A., Griffiths, T.L., & Shafto, P.
(2011). Children’s imitation of causal action sequences
is influenced by statistical and pedagogical evidence.
Cognition, 120, 331–340.
Dennett, D.C. (1987). The intentional stance. Cambridge,
MA: MIT Press.
Gergely, G., & Csibra, G. (2003). Teleological reasoning
in infancy: The naïve theory of rational action. Trends
in Cognitive Sciences, 7, 287–292.
Hilton, D.J., & Slugoski, B.R. (1986). Knowledge-based
causal attribution: The abnormal conditions focus
model. Psychological Review, 93, 75–88.
Johnson, S.G.B., & Rips, L.J. (2013). Good decisions,
good causes: Optimality as a constraint on attribution of
causal responsibility. Proceedings of the 35th Annual
Conference of the Cognitive Science Society (pp. 2662–
2667). Austin, TX: Cognitive Science Society.
Povinelli, D.J., & Vonk, J. (2004). We don’t need a
microscope to explore the chimpanzee’s mind. Mind &
Language, 19, 1–28.
Ritov, I., & Baron, J. (1992). Status-quo and omission
biases. Journal of Risk and Uncertainty, 5, 49–61.

General Discussion
Every day, we successfully predict what others do, and
these successes often seem to be accompanied by
inferences about mental states. In three experiments, we
provided evidence that these inferences are sometimes
made solely on the basis of observable states of the world,
using a behaviorist system for interpreting actions.
Behaviorist thinking manifested in participants’
inferences about agents who were ignorant about the
efficacy of their decision options, but who were
nonetheless expected to choose actions that led to their
goals optimally (Experiments 1 and 2). A mentalistic
strategy would instead lead participants to predict that
ignorant agents are equally likely to choose each option.
Is it possible, however, that people were following a
mentalistic strategy, but incorrectly attributed knowledge
to the ignorant agents? Inattentiveness is not a likely
explanation, since participants failing manipulation
checks were excluded from these analyses. However, a
more plausible possibility is that participants made a
perspective-taking error known as the curse of knowledge
(e.g., Birch & Bloom, 2007), and were unable to separate
their own perspective from that of the agent. This
explanation could account for some inferences about the
ignorant agents, but could not explain why people only
distinguished between Middle and Worst options for
knowledgeable agents (Experiments 1 and 2), why actions
and omissions were treated in qualitatively different ways
(Experiment 2), or why rationalizing inferences were

700

