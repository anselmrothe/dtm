UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Are you hiding something from me? Uncertainty and judgments about the intentions of
others

Permalink
https://escholarship.org/uc/item/9s55q6ft

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)

Authors
Street, Chris N.H.
Richardson, Daniel

Publication Date
2014-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Are You Hiding Something From Me? Uncertainty and Judgments About the
Intentions of Others
Chris N. H. Street (c.street@psych.ubc.ca)
Department of Psychology, University of British Columbia,
2136 West Mall, Vancouver, V6T 1Z4, BC, Canada

Daniel C. Richardson (dcr@eyethink.org)
Cognitive, Perceptual and Brain science, University College London
Gower Street, London WC1E 6BT, UK
Abstract
We are skilled at reading other’s intentions – until they try to
hide them. We are biased towards taking at face value what
others say, but it is not clear why. One possibility is that we
are uncertain, and make the decision by relying on heuristics.
Half of our participants judged whether speakers were lying
or telling the truth. The other half did not have to commit to a
judgment: they were allowed to say they were unsure. We
expected these participants would no longer need to rely on
simplified heuristics and so show a reduced bias compared to
the forced choice condition. Surprisingly, those who could
say they were unsure were more biased towards believing
people. We consider two possible accounts, both highlighting
the importance of examining raters’ uncertainty, which have
so far been undocumented. Allowing raters to abstain from
judgment gives new insights into the judgment-forming
process.
Keywords: social cognition; hidden intentions; heuristics;
uncertainty; deception detection.

Introduction
We all live in our own private worlds, with our own beliefs,
expectations and intentions. Our thoughts and intentions
influence how we act, how we perceive the world, and how
we engage with others (e.g. Kelley & Stahelski, 1970).
Those we engage with come with their own private worlds
too, their own thoughts and intentions. We can never
experience or directly perceive the intentions of others, but
can only guess and infer from behaviour.
Typically we are very skilled at picking up on others
intentions (Clark, Schreuder & Buttrick, 1983) and tracking
the social context (Richardson et al., 2012), but as soon as
interests conflict people may decide to close off their private
worlds and hide their intentions. And when they do, it
becomes near impossible to tell: ability to detect when
someone is or is not concealing something drops to near
chance and people are biased towards believing what others
say (Bond & DePaulo, 2006). Why are we biased towards
taking what other people say at face value, even when we
have good reason to be suspicious of them?
We consider one possible explanation: that people make
use of relatively simple but experientially informed
heuristics such as ‘people usually tell the truth’. They may
rely on these rules because they are unsure whether the

speaker is being honest or is hiding something, but
nonetheless have to commit themselves to a judgment.
There is much room for uncertainty. People are largely
successful at concealing information, giving away very little
in their behaviour (Bond & DePaulo, 2006; Levine, 2010).
And when people hide information from us we tend to be
unsure whether the speaker is being honest or not
(Anderson, DePaulo & Ansfield, 2002; DePaulo, 1992).
When we are unsure, but forced to make a decision, how
do we make a judgment? One way to simplify the task and
reduce the uncertainty is to rely on a heuristic or rule of
thumb (Tversky & Kahneman, 1974). Whether it’s because
the rules of conversation imply that what is communicated
is true (Grice, 1975), because people tell the truth far more
often than they lie (DePaulo, Kashy, Kirkendol, Wyer &
Epstein, 1996), or because social rules encourage politeness
over accusing others of being deceptive (O’Sullivan, 2003),
there are good reasons to make use of a simple ‘people
generally tell the truth’ heuristic.
This paper considers whether raters rely on a truth biased
heuristic precisely because they are forced to judge whether
others when they are unsure. To test this claim, half of hour
participants were allowed to explicitly indicate they were
unsure. We expected that because participants were no
longer forced into making a judgment, and so did not have
to rely on simplified heuristics, they would no longer show
this bias towards taking what others say at face value.

Methods
Materials
In a prior study we developed the Bloomsbury Deception
Set (Street et al., 2011, April). Twenty-two speaker
participants were approached by a junior researcher posing
as a documentary filmmaker’s assistant. He approached
people on the street near a London filming studio and asked
them whether they would like to take part in a documentary.
Those who agreed told the researcher which countries they
had and had not been to. The researcher claimed he was
short of time and, as a favour to him, asked participants
whether they would tell us about one place they had been to
and to make up a story about a place they had not been to.

1545

Those who agreed were taken into a filming studio and
left alone with a senior researcher posing as a film director.
To the participants’ knowledge the director was unaware of
that they had been asked to lie. The senior researcher
explained he was both a director and a researcher interested
in people’s true experiences in other countries, and stressed
it was important that the accounts described were honest.
Participants signed a waiver to this extent, stating they
would be entirely honest in both deliveries.
Participants then delivered either an honest or deceptive
statement in response to the question ‘When you arrived in
[country name], what was your first impression of the
people there?’. They then answered this question a second
time about a second country, this time lying if they told the
truth first time, or vice versa. The junior researcher
counterbalanced the order of the honest and deceptive
statements (the senior researcher was blind to the order).
Two speakers admitted they had been asked to lie, and did
not deliver the statement. We believe this testifies to the
effectiveness of our cover story, and shows that participants
truly intended to mislead the director with their
spontaneously generated deceptions.
Although they were asked to provide statements of
approximately 30 s, statements ranged from 10 s to 91 s.
Truths lasted on average 32.86 s (SD = 10.79). Lies lasted
on average 32.72 s (SD = 24.83).
Two participants were used as a practice set. The
remaining 18 speakers’ lies and truths (total of 36
statements) were split into two video sets, such that a
speaker appeared only once in a video set with each set
containing 50% truths/lies.

Participants
Eighty University College London psychology students
rated the speakers in the above stimulus set. One participant
withdrew consent retrospectively. Of the remaining 79 (54
female), the mean age was 18.87 (SD = 1.31, range 18 to
22). Participants received course credit or £3 compensation.

Procedure
Raters were instructed speakers would either lie or tell the
truth about claiming to have met people in a foreign country
they visited. No information was given about the proportion
of lies to truths.
Raters made a response after viewing each statement.
Raters in the lie-truth (LT) condition (n = 39) made a forced
binary choice. Raters in the lie-truth-unsure (LTU)
condition (n = 40) were given the additional option of
indicating their uncertainty. That is, raters viewed a video
passively, and after each video either made a lie-truth or lietruth-unsure judgment.

Design
The independent variable was the response condition (LT
or LTU). The dependent variables were nonparametric
signal detection measures of accuracy (A’: Rae, 1976) and
bias (B”D: Donaldson, 1992), used as measures of how
likely people were to be biased toward believing others.
The calculation of these measures takes into account the
veracity of the statements. A’ accuracy scores range from 0
to 1, where 0.5 indicates chance accuracy and 1 perfect
accuracy. B”D bias scores range from -1 to +1, where -1
indicates a perfect lie bias, +1 a perfect truth bias and 0
indicates no bias.

Results
Being able to explicitly indicate their uncertainty did
influence how likely people were to believe others. But the
effect was in the opposite direction than predicted: it
increased the bias.
In the LTU condition there were three response options.
Random responding would lead to a lower percentage of
truth judgments (33.3%) compared to the LT condition
(50%), giving rise to artificial bias differences between the
two conditions. To allow for meaningful comparisons, the
unsure responses in the LTU condition were not analysed.
Rather, we were interested in whether raters used the truth
response more often than the lie response, regardless of how
they made use of the unsure option (if that option was
available to them). This allowed us to ask whether the act of
forcing a judgment leads to a reliance on simple heuristics,
which in turn causes the truth bias. On average, 17% (SD =
12, range 0% to 38%) of LTU participants’ responses were
unsure responses.
The proportion of truth to lie judgments was converted
into signal detection measures of accuracy (A’) and bias
(B”D) to examine their independent contributions to the
judgment. People may say truth more often than 50%
because they are particularly accurate at detecting true
statements or because they are biased towards judging all
statements as true. Examining the raw proportion of truth
judgments does not distinguish between those two
possibilities. Separating accuracy from bias by using signal
detection measures avoids this problem.
Two independent samples t-tests found no evidence of a
significant difference in accuracy between the LT (M = .62,
SD = .16, 95% CI [.57, .67]) and LTU conditions (M = .62,
SD = .20, [.56, .68]), t (77) = 0.09, p = .929, d = 0.02
(Figure 1, top). There was a difference of medium effect
between the LT (M = .16, SD = .45, [.01, .30]) and LTU (M
= .38, SD = .50, [.23, .53]) conditions, t (77) = 2.06, p =
.043, d = -0.65 (Figure 1, bottom).

1546

Two independent samples t-tests compared the proportion
of truth judgments and the proportion of lie judgments in the
LT and LTU conditions. The proportion of truth responses
showed no evidence of a statistically significant difference
between the LT (M = .54, SD = .14, 95% CI [.50, .58]) and
the LTU conditions (M = .50, SD = .14, [.46, .54]), t (77) =
1.23, p = .223, d = 0.28, contrary to a veridical truth bias.
The proportion of lie responses did differ showing a large
effect, t (77) = 4.25, p < .001, d = 0.96. There were
significantly fewer lie judgments made in the LTU
condition (M = .33, SD = .14, [.29, .37]) than in the LT
condition (M = .46, SD = .14, [.42, .50]), as would be
!expected of an artefactual truth bias (Figure 2).

0.70

Accuracy (A’)

0.65
0.60
0.55
0.50
0.45

0.6

0.4

Proportion of responses

Response bias (B”D)

0.5

0.3
0.2
0.1
0

LT

LTU

0.5
0.4
0.3
0.2
0.1
0
lie response

Figure 1: Accuracy in the forced choice LT and the
unforced choice LTU conditions. 0.5 indicates change
accuracy (top). Response bias in the LT and LTU
conditions. Zero indicates no bias; positive values indicate a
truth bias (bottom). Error bars denote 95% confidence
intervals.

truth response

LT

unsure response

LTU

!
!
! Figure 2: The proportion of lie, truth and unsure (where
! applicable) responses in the LT and LTU conditions. The
!
! increased truth bias in the LTU unforced choice condition
!
!results from a reduction in the use of the lie response. Error
!
bars denote 95% confidence intervals.
!

Artefactual vs. Veridical Truth Bias
To better understand why the effect was in the reverse
direction to our prediction, we reintroduced the unsure
responses in the LTU condition data. The raw proportion of
truth judgments were used because it does not make sense to
calculate signal detection measures with three responses.
There are two possible explanations of the effect. First,
there is a veridical bias: that is, participants in the LTU
condition made more truth judgments than did participants
in the LT condition. This account predicts a difference in the
proportion of truth judgments made in the two response
conditions. A second possibility is that there is an
artefactual bias: that is, LTU raters made fewer lie
judgments instead electing to use the unsure response. This
would lead to a greater proportion of truth judgments out of
all the lie-truth responses, but as a result of using the lie
response less rather than using the truth response more
often. This account predicts a difference in the proportion of
lie judgments between the two response conditions.

A second analysis explored the use of the unsure response
in the LTU condition in more detail. Prior research suggests
there is greater uncertainty when judging lies than judging
truths (DePaulo et al., 1997). A paired-samples t-test
compared the proportion of unsure responses in the LTU
condition when rating lies versus rating truths. As expected,
there was a significantly greater proportion of unsure
responses when rating lies (M = .39, SD = .19, 95% CI [.33,
.45]) than when rating truths (M = .26, SD = .18, [.20, .32]),
t (38) = 3.40, p = .002, d = 0.15.
In summary, after exploring the data it became clear the
larger truth bias in the unforced choice condition was
because the raters in this condition shifted away from
making lie responses towards making unsure responses. As
such, the proportion of truth to lie judgments increased
because of the smaller number of lie judgments in this
condition. It is also worth noting that deceptive statements
were more likely to elicit unsure responses than were honest
statements, replicating past research (DePaulo, Charlton,
Cooper, Lindsay & Muhlenbruck, 1997).

1547

Discussion
We are able to understand the thoughts and intentions of
others with relative ease when they want to communicate
them. But when they choose to close off their private inner
world to us they do so effectively, to the point where we are
barely able to notice the deception. We are biased towards
taking what people say at face value, even in these
experiments where participants were explicitly aware that
some speakers would be concealing something from them
(Bond & DePaulo, 2006).
We considered one account in this paper. Raters have to
deal with a socially uncertain situation, and to deal with that
uncertainty they may make an informed guess using a
heuristic. We considered whether relieving participants of
the need to make a judgment meant they would no longer
need to rely on these generally useful but somewhat errorprone rules of thumb, and so show a reduced bias toward
believing others. Contrary to predictions, we found raters
were more even more biased towards believing others when
able to abstain from judgment. Further exploration revealed
this was not a case of an increased likelihood to believe
others were telling the truth, but rather that they were less
likely to label any given statement a lie and instead indicate
uncertainty.
There are two possible explanations of our findings. Both
accounts assume raters are more unsure when they are about
to make a lie compared to a truth judgment, as others have
found (Fan, Wagner & Manstead, 1995). That is, we argue
that exploring uncertainty is key to understanding lie-truth
judgments, and that there is an asymmetry in the uncertainty
between judging honest versus deceptive statements.
As a first possibility, raters may become more cautious in
their lie judgments when they can explicitly indicate
uncertainty. They may begin believing the speaker is lying,
but decide to hedge on the side of caution and say they are
unsure. This may be indicative of raters giving heed to the
notion of ‘innocent until proven guilty’.
Alternatively, raters are typically more uncertain when
listening to lies compared to truths (Anderson et al., 2002;
DePaulo, 1992), with lies being harder to detect (Levine,
Park & McCornack, 1999). Raters also have less experience
with listening to lies than truths (DePaulo et al., 1996).
When they are unsure, they may deduce that uncertainty is
typically associated with people being deceptive and so
infer that the speaker is hiding something. This would
require raters can identify their own uncertainty by means of
some meta-cognitive mechanism. This strategy would be
adaptive inasmuch as raters typically are more unsure when
they are listening to a lie, meaning in general this strategy
will boost accuracy rates.
In the first case, the rater in the LTU condition begins
from disbelieving the speaker but changes their response to
an unsure response. In the second case, the rater in the
binary choice LT condition begins feeling unsure but has to
make a response. Aware that uncertainty is usually

associated with deception, raters in that condition make a lie
response. The question is whether raters begin from
disbelieving the speaker or from being uncertain. The
current data are unable to distinguish between these two
possibilities. What is clear though is that there is uncertainty
in the judgment process at some level. Our research is the
first to explicitly explore the effects of allowing raters to
indicate their uncertainty instead of having to make a lietruth judgment. Forcing participants into a response when
they are unsure may lead them into adopting strategies that
have until now gone unnoticed and undocumented.
It seems the bias towards believing what others tell us is
not simply the result of making the best guess when unsure.
When raters can explicitly say they are unsure, they are just
as likely to say raters are telling the truth as when they are
forced into judgment. The truth bias seems to be more
complicated than making the best guess when they are
unsure.
But note that we are discussing uncertainty after raters
have received all the information they can receive. During
the early moments of judgment formation, as the speaker
begins delivering their statement and when there is little
information available to the rater, we might anticipate raters
will make use of context-relevant heuristics like ‘people
usually tell the truth’. With no other information to rely on,
we may expect the early moments of the judgment process
to be biased by these heuristics. A number of decision
making models assume raters begin from a point of
uncertainty, but that they can begin with a preference
towards one judgment as a result of experience and prior
knowledge (e.g. Richter, Schroeder & Wöhrmann, 2009;
Roe, Busemeyer & Townsend, 2001). It is interesting to
explore whether we show an early bias towards believing
others because it begins to ask what biases raters bring with
them to the social situation, which will have at the least
some indirect effect on the final judgment.
In summary, the tendency to believe others does not seem
to be the best guess when unsure but forced to make a
decision. Raters showed a similar degree of bias regardless
of whether they were or were not forced into committing to
a judgment. Rather, raters differed in how likely they were
to rate a statement as deceptive. This may reflect an
‘innocent until proven guilty’ bias, or a meta-cognitive use
of uncertainty to make adaptive and generally successful
judgments. These differences in strategy have so far gone
unnoticed: allowing raters to abstain from judgment gives
new insights into the judgment-forming process.

Acknowledgments
We would like to thank Arndt Bröder, Alan Kingstone and
Nigel Harvey for their comments on earlier versions of this
manuscript. This work was conducted as part of the first
author’s PhD.

1548

References
Anderson, D. E., DePaulo, B. M., & Ansfield, M. E. (2002).
The development of deception detection skill: A
longitudinal study of same-sex friends. Personality and
Social Psychology Bulletin, 28, 536-545.
Bond, C. F., & DePaulo, B. M. (2006). Accuracy of
deception judgments. Personality and Social Psychology
Review, 10(3), 214-234.
Clark, H. H., Schreuder, R., & Buttrick, S. (1983). Common
ground and the understanding of demonstrative reference.
Journal of Verbal Learning and Verbal Behavior, 22,
245-258.
DePaulo, B. M. (1992). Nonverbal behavior and selfpresentation. Psychological Bulletin, 111, 203-243.
DePaulo, B. M., Charlton, K., Cooper, H., Lindsay, J. J., &
Muhlenbruck, L. (1997). The accuracy-confidence
correlation in the detection of deception. Personality and
Social Psychology Review, 1(4), 346-357.
DePaulo, B. M., Kashy, D. A., Kirkendol, S. E., Wyer, M.
M., & Epstein, J. A. (1996). Lying in everyday life.
Journal of Personality and Social Psychology, 70(5), 979995.
DePaulo, B. M., Lindsay, J. J., Malone, B. E., Muhlenbruck,
L., Charlton, K., & Cooper, H. (2003). Cues to deception.
Psychological Bulletin, 129(1), 74-118.
Donaldson, W. (1992). Measuring recognition memory.
Journal of Experimental Psychology: General, 121(3),
275-277.
Fan, R. M., Wagner, H. L., & Manstead, A. S. R. (1995).
Anchoring, familiarity, and confidence in the detection of
deception. Basic and Applied Social Psychology, 17(1 &
2), 83-96.
Kelley, H. H., & Stahelski, A. J. (1970). Social interaction
basis of cooperators’ and competitors’ beliefs about
others. Journal of Personality and Social Psychology,
16(1), 66-91.
Levine, T. R. (2010). A few transparent liars: Explaining
54% accuracy in deception detection experiments. In C.
Salmon (Ed.). Communication Yearbook 34. Thousand
Oaks, CA: Sage.
Levine, T. R., Park, H. S., & McCornack, S. A. (1999).
Accuracy in detecting truths and lies: Documenting the
“veracity effect”. Communication Monographs, 66, 125144.
O'Sullivan, M. (2003). The fundamental attribution error in
detecting deception: The boy-who-cried-wolf effect.
Personality and Social Psychology Bulletin, 29(10), 13161327.
Rae, G. (1976). Table of A’. Perceptual and Motor Skills,
42, 98.
Richardson, D. C., Street, C. N. H., Tan, J. Y. M., Kirkham,
N. Z., Hoover, M. A., & Cavanaugh, A. G. (2012). Joint
perception: Gaze and social context. Frontiers in Human
Neuroscience, 6(194).
Richter, T., Schroeder, S., & Wöhrmann, B. (2009). You
don’t have to believe everything you read: Background

knowledge permits fast and efficient validation of
information. Journal of Personality and Social
Psychology, 96(3), 538-558.
Roe, R. M., Busemeyer, J. R., & Townsend, J. T. (2001).
Multialternative decision field theory: A dynamic
connectionist model of decision making. Psychological
Review, 108(2), 370-392.
Street, C., Tbaily, L. Baron, S., Khalil-Marzouk, P., Wright,
K., Hanby, B., & Richardson, D. C. (2011). Bloomsbury
Deception Set. Paper presented at the BPS Division of
Forensic Psychology Conference, Portsmouth, UK.
Tversky, A., & Kahneman, D. (1974). Judgment under
uncertainty: Heuristics and biases. Science, 185(4157),
1124-1131.

1549

