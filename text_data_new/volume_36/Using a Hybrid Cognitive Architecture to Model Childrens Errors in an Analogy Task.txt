UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Using a Hybrid Cognitive Architecture to Model Children’s Errors in an Analogy Task

Permalink
https://escholarship.org/uc/item/22b5d550

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)

Authors
Licato, John
Sun, Ron
Bringsjord, Selmer

Publication Date
2014-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Using a Hybrid Cognitive Architecture to Model Children’s
Errors in an Analogy Task
John Licato (licatj@rpi.edu)
Department of Computer Science
Rensselaer Polytechnic Institute

Ron Sun (rsun@rpi.edu)
Department of Computer Science
Department of Cognitive Science
Rensselaer Polytechnic Institute

Selmer Bringsjord (selmer@rpi.edu)
Department of Computer Science
Department of Cognitive Science
Rensselaer Polytechnic Institute
Abstract

[W]e can say that the failure to construct relations between relations, or forms of forms, at the more primitive
stages comes about because there are as yet no stable
elementary relations (and we mean two-term relations
here, not four-term relations). Consequently there are
no simple forms that can be expressed in terms of stable
classes (Piaget et al., 2001, p.143).

We model the performance of children on the Goswami and
Brown (1990) analogy task, paying close attention to the distribution of errors children made on the task. This distribution
follows a very particular pattern which, as we show, may be
simulated by assuming a lack of development in the richness
of children’s concepts of physical causation. This modeling is
done using the hybrid cognitive architecture CLARION, and
a method of representing structured knowledge within CLARION’s dual-process system.

Goswami and Brown’s Experiment
Nevertheless, Goswami and Brown (1990) were not convinced that Sternberg and Nigro (1980) and Goldman et al.
(1982) properly controlled for Piaget’s apparent belief that
two-term stable elementary relations did not exist in young
children. For example, one preoperational child brought the
pair cork:bottle together with lid:pot under the justification
“’cause there’s something that comes out of the pot” (Piaget
et al., 2001, p.142). Piaget et al. call this a momentary approximation rather than a stable analogy, since the lid object
could easily be replaced with an oven, with the justification
“sometimes you heat it with the oven.” The relation which
links the two pairs, upon which the determination of an analogical match should be made, is easily abandoned by the
child.
Goswami and Brown (1990) suspected that the results
would be different if two changes were made: First, children
should instead be tested only with very basic two-term causal
relations; and second, the experiment should verify first that
children actually have a stable understanding of those relations. Simple objects with which children were likely to be
familiar were also introduced, such as playdoh:cut playdoh
and apple:cut apple. The relation used here, that of cutting an
object, is typical of the basic, physical transformations used
in the experiment.
The experiment was designed to rule out the possibility of
mere association matches being made over the correct, analogical matches. For each analogy problem, an a, b, and
c term were given as pictures, in the typical a : b :: c :?
proportional-analogy style. Children were asked to choose

Keywords: analogy; analogical reasoning; reasoning; Piaget;
CLARION; cognitive modeling; cognitive architectures

Introduction
Analogical reasoning is a core component of adult human
thought; researchers in cognitive science and artificial intelligence are coming to realize this increasingly (Gentner &
Forbus, 2011; Licato, Bringsjord, & Govindarajulu, 2013).
But the ability to reason analogically was once seen as a
higher cognitive ability that did not emerge fully until Piaget’s
formal-operations stage of cognitive development. While exploring his concept of reflective abstraction, Piaget performed
a set of experiments with children that supported his suspicion that analogy was linked to higher-level cognitive processes (Piaget, Montangero, & Billeter, 2001). Subsequent
work by Sternberg and Nigro (1980) and Goldman et al.
(1982) added to Piaget’s suspicion, eventually transforming
it into the hypothesis that 9- and 12-year-olds solve analogies not using analogical reasoning, but by simple low-level
association (Goswami & Brown, 1990).
But Piaget himself was not necessarily convinced that analogical reasoning was impossible with young children. He
observed cases in which children in the sensorimotor and
preoperational stages displayed basic analogical reasoning,
even if he thought many of them only provided “approximate
analogies from moment to moment.” He offered an explanation for the apparent lack of analogical ability in the children
he worked with:

857

between the following, where the examples we provide are
from the playdoh:cut playdoh::apple:? problem:

seen in Table 1? This question is the focus of our work in this
paper.
The remainder of this paper will first briefly describe
CLARION, the cognitive architecture chosen for this modeling task. We then need to touch on CLARION’s NACS
subsystem, and the style of structured knowledge representation which gives CLARION a unique ability to model many
of the phenomena that are important in this task. We describe
the steps we took to model the error distribution seen in Table
1, and then close with a brief discussion.

• d - Correct choice (cut apple).
• e - Correct transformation, wrong object (cut bread).
• f - Wrong transformation, correct object (bruised apple).
• g - Mere appearance match of the c term (ball, where the
ball’s size and shape are similar to those of the apple).

CLARION

• h - Semantic/category match of the c term (banana).

CLARION (Sun, 2002) is an integrative cognitive architecture that has a dual-process structure; that is, is consists of
two levels: explicit (top level) and implicit (bottom level).
CLARION has been able to model a wide variety of cognitive phenomena while maintaining psychologically plausible
data structures and algorithms (Sun, 2001; Sun & Zhang,
2004, 2006). The dual-process representational system of
CLARION allows for both localist concepts, such as those
presented in the Goswami and Brown (1990) experiment, and
distributed knowledge, which is a natural way of representing
low-level associations between concepts (Sun, 2002). This
makes it an ideal choice for our purposes.
The architecture is further divided into four subsystems,
each with explicit and implicit levels, which specialize in
different aspects of cognition: the Motivational Subsystem (MS), the Metacognitive Subsystem (MCS), the ActionCentered Subsystem (ACS), and the Non-Action-Centered
Subsystem (NACS). Work described in this paper will only
be using the NACS.

The analogy experiment was divided into two phases. In
the first phase (the induction phase), children were simply
asked to choose the correct answer, and were told whether
the choice they made was correct or not. If it was incorrect,
the experimenter would show the child which was the correct
answer, and not explain any further. In the second phase (the
explanation phase), after choosing, the child would be given
an explanation of why the correct answer applies, whether or
not the choice the child made was correct.
Their results were positive: A clear demonstration of analogical ability was shown in children as young as 3 or 4 years
of age, with a significant difference between them, and the
performance by 6-year-olds was nearly perfect. No other age
groups were tested. Perhaps more relevant to our current
work, however, is the distribution of errors made by the 3and 4-year-olds, which can be seen in Table 1.
Table 1: Percentage of children who passed criterion, and
errors made by children on the Goswami and Brown (1990)
analogy task. Errors presented as a percentage of total errors.
Error data from the 6-year-old age group were not provided
in the original paper, as the number of errors was deemed too
small to be significant. Reprinted with permission.

NACS — the Non-Action-Centered Subsystem
The NACS contains general knowledge about the world that
is not contained in the ACS. Whereas the ACS is meant to
capture the knowledge that directly control actions while interacting with the world, the knowledge in the NACS is often more deliberative and used for making inferences. The
top level of the NACS is called the General Knowledge Store
(GKS), and it contains localist chunks that can be linked to
each other using Associative Rules (ARs).
The bottom level of the NACS is called the AMN, or the
Associative Memory Network, and it contains implicit associative knowledge encoded as dimension-value pairs (DV
pairs). Each GKS chunk is connected to a set of DV pairs
in the AMN with weights that can be adjusted over time.
This unique structure gives CLARION the ability to define
a directed similarity measure between two chunks c1 and c2
which is derived from the amount of overlap between the DV
pairs connected to the two chunks (Sun, 1995; Tversky, 1977;
Sun & Zhang, 2004). However, in this paper we will be using a simplified similarity function based on the number of dv
pairs connected to c1 and c2 :

Percentage Passing Criterion (Children)
Age (years)
3
4
6

Induction
28
73
95

Explanation
52
90
100

Errors Made (Children)
Age (years)
3
4

Induce
Explanation
Induce
Explanation

E
21
23
11
30

F
45
39
66
63

G
18
17
10
3

H
16
21
13
3

This distribution of errors is interesting, because it might
be able to provide us some insight into better modeling
of analogical reasoning in cognitive architectures. What
simulation-level details can account for the error distribution

Sc1 →c2 =

858

|c1 ∩ c2 |
|c2 |1.0001

(1)

Note that it is possible for the denominator in Equation 1 to
be zero, in which case the entire equation is given the default
value of 1. It is the measure in Equation 1 which we will
use in this paper to determine low-level associative similarity
between chunks.
The Associative Rules (ARs) link groups of chunks to other
chunks in the GKS, and consist of a set of condition chunks
c1 , c2 , ... and a single conclusion chunk d. For any given AR,
each condition chunk i has a weight Wi such that ∑i Wi = 1.
The chunks in the GKS and DV pairs in the AMN have activation levels which can be set by CLARION’s other subsystems. Activations can also spread through the NACS using
the chunk-DV pair connections and the top-level ARs. The
manner in which this activation spreads can be restricted, as
other subsystems can temporarily disable Rule-Based Reasoning (activation spreading through ARs) or SimilarityBased Reasoning (activation spreading through chunk similarity), or perform activation propagation as some weighted
combination of both of these reasoning types. These abilities
are detailed further in Sun & Zhang (2004, 2006), in which
these mechanisms are shown to be psychologically plausible.
Structural knowledge in CLARION is represented through
combining ARs with Cognitively Distinguished Chunks, or
CDCs. CDCs are depicted as star-shaped in our diagrams
(Figure 1). Associative rules link the CDCs to the chunks in
the structure. For example, the WHOLE CDC links object
nodes to proposition nodes. In Figure 1, which depicts the
proposition CHASES(DOG CAT), the WHOLE CDC is part
of two ARs (depicted in the Figure as an arrow with multiple
tails and one head).
A COMPONENT CDC is also defined, such that for every
rule involving a WHOLE CDC, a complementary rule going
in the other direction is created with a COMPONENT CDC.
Whole chunks are always pictured above component chunks.
Next, we introduce Ordinal CDCs, which are also pictured in
Figure 1 as 1ST, 2ND, etc. Ordinal CDCs simply preserve the
roles objects play within propositions in a general way that
does not name the roles specifically (contrast this with the
LISA model (Hummel & Holyoak, 2003), which has distinct
role units for every type of role.).

Chases
WHOLE

Chases

COMPONENT

Dog
1st

Cat

2nd

Dog

Cat

Figure 1: A knowledge structure representing the proposition
CHASES(DOG,CAT ). On the right is the simplified version,
which omits the CDCs and many of the ARs, though they
are there (just not pictured). Note that in both versions, only
chunks are pictured. The bottom-level DV pairs which each
chunk is connected to are omitted for simplicity.

that tries (in parallel) different transformations of the source
chunks into templates. For the purposes of this paper, a simple version of this algorithm is used that simply transforms
the object-level chunks (objects a and b) to blank chunks.
Actually determining whether the template applies to the
chunks is a nontrivial algorithmic problem. We solve this by
using an Ant Colony Optimization (ACO) algorithm based on
(Sammoud, Solnon, & Ghédira, 2005). The algorithm starts
by collecting the template and target chunks, and drawing eligibility links between pairs of chunks that can potentially be
mapped to each other. It determines this by using the similarity metric defined in Equation 1. If that similarity is above
a certain minimum similarity level ε, then an eligibility link
is drawn. Varying ε allows us to approximate the rigidity of
the matching requirements, so that a lower value encourages
more creative matchings, while a higher value forces more
structural consistency between the structures being matched.
Eligibility links are automatically drawn from blank chunks
to other chunks in the target.
The eligibility links are then selected by the ants in a
bottom-up fashion, starting with the object-level chunks. As
each level is completed, the eligibility links remaining in the
upper levels are re-evaluated. This is because the choices
made at the lower levels may not be consistent with choices
on the upper levels (lest structural consistency between the
template and target structures be violated).
The ACO algorithm which matches templates to targets
can then be used. Our ACO implementation is described in
more detail, along with a more thorough explanation of templates, in (Licato, Sun, & Bringsjord, 2014). The authors are
not unaware of the level of compressed descriptions in this
paper regarding the simulation, but due to space many technical details had to be omitted from this paper.

Analogical Reasoning in CLARION Using
Templates
Analogical and deductive reasoning in CLARION can be
done using the template form. Templates are groups of
chunks that both specify what constitutes an acceptable form
match and how to transform the input when such a match is
found. Chunks can also exist in templates that have zero semantic content. These are called “blank chunks,” and will be
used when matching templates to other structures. In Figure
2, blank chunks are pictured as chunks without any labels.
Given some template and a set of structured chunks, before
performing analogical reasoning using the template system, a
set of source chunks has to first be collected. These chunks
can then be transformed into a template using an algorithm

859

Template

Cut_before_after

same_type

Bruised_before_
after

uncut_cut_
pair

Cut_before_after
uncut_cut_
pair

Target

same_type

uncut_cut_
pair

same_type

unbruised_
bruised_pair

c

d

e

f

g

h

Figure 2: The template and target representation used in our simulation. DV pairs are not pictured here. Units pictured with
dotted lines are only present for the simulation of 6-year-old reasoning, whereas the rest of the units were present for all
simulated age groups.

Matching Children’s Errors

new units as a normal part of the learning process, such as the
neurologically plausible model DORA (Doumas, Hummel, &
Sandhofer, 2008), and those built on cascade-correlation neural networks (Shultz, 2003; Shultz & Sirois, 2008). Such systems increase their representational power dynamically, leading to qualitative changes in the expressivity of the knowledge representation used by the system, and giving them clear
advantages over static models (Shultz, 2012). The cascadecorrelation model has also been used to effectively model Piagetian tasks (Shultz, Mareschal, & Schmidt, 1994; Shultz,
2003).

The period between 3- and 4-years of age is one of rapid cognitive development in many areas, among these the ability to
understand, and reason about, physical causality (Das Gupta
& Bryant, 1989; Frye, Zelazo, & Palfai, 1995). The increase
in this particular ability is reflected in the data from Table 1
(Goswami & Brown, 1990). The question before us, then, is:
Exactly what changes occur at CLARION’s level of abstraction that plausibly explain increased understanding of physical causality? The obvious first answer is quantitative: We
can either increase ε (recall, this is the minimum similarity
required for an eligibility link to be drawn in the matching algorithm), or we can increase the number of ants and iterations
used in the ACO algorithm. Initial testing, however, showed
that a tweaking of these parameters was not enough to explain
the distribution of errors seen in Table 1. In particular, it did
not replicate the fact that f and e errors are by far the most
common, in that order. Something else had to be explored in
addition to the quantitative features. As it turns out, there are
some clues in the psychological literature that hint of a qualitative change as well. That change is a development in the
conceptual representations used by the children.
The idea of qualitative change over time is a fundamental one in Piaget’s constructivist philosophy (Piaget, 1977).
Several systems implement constructivist views by recruiting

It makes sense, then, that a qualitative change in the representations used might offer an explanation for the Table 1
data. Assuming a constructivist (as opposed to a Fodorian
nativist (Fodor, 2008)) theory of cognitive development, concepts such as those used in the Goswami and Brown (1990)
experiments are built and enriched from more primitive constructs. It should not be too controversial to suggest, then,
that perhaps part of what explains the rapid increase in understanding of physical causation after age 4 is a development
in the relevant conceptual constructs themselves. In other
words, perhaps the 3- and 4-year-olds are reasoning using a
primitive version of the concepts being tested, which are not
fully chunked, not fully explicit, and still decomposable into
their constituent concepts; whereas the 6-year-olds, who per-

860

formed at or close to ceiling, have much richer conceptual
representations.1
We will use the playdoh:cut playdoh::apple:? problem as
an example. The relevant relationship being tested is Cut, a
rich concept consisting of: before and after states, a division
in the object in the after state characterized by a clean slice,
which was likely done with a sharp blade, etc. If the Cut
concept is built from observations, it is possible that these
primitive concepts are those from which the Cut concept is
built. If so, does the representational network of concepts
used by children still make use of these primitive concepts?
The abundance of e and f errors might be explained via
the use of primitive conceptual structures. Instead of structuring our simulation such that items c and d are linked
by a single predicate Cut, we link them with two separate concepts: Same Type (since they are both apples), and
Cut Uncut Pair, which simply links two objects such that
one is uncut and the other is. Now c (an uncut apple) and
e (cut bread) can be linked with the Cut Uncut Pair predicate, and c can be linked with f (a bruised apple) by way of
the Same Type predicate. Furthermore, we can link c, f with
an Unbruised Bruised Pair predicate and have that predicate
share DV pairs with Cut Uncut Pair to reflect the semantic
similarity between bruising and cutting.
This representation is pictured in Figure 2. Note that the
source analog (the a and b chunks and all relationships between them) was transformed into a template such that the
object-level chunks were blank. We were then able to execute the model by matching this template to the representation consisting of objects c through h. Most of the time,
the chunks corresponding to c would be mapped to a, and
whichever object’s chunks mapped to b was taken to be the
answer selected by the system. Also note that there are additional chunks which were only present for the representations
used for the 6-year-old group’s simulation, which is meant to
reflect more highly developed Cut and Bruise concepts in the
6-year-olds.
In order to reflect object-level similarities shared between
the c and g objects, we simply created a set of DV pairs that
were uniquely shared by the chunks corresponding to c and
g. A parallel move was made for c and h. A parameter
δ was introduced, which represents how frequently the system resorts to using the sort of low-level associative reasoning children were thought to reason with in place of analogy
(Sternberg & Nigro, 1980). In CLARION, this type of reasoning is called Similarity-Based Reasoning (SBR) (Sun &
Zhang, 2004). Our simulation used SBR to select an answer
whenever either a randomized variable was greater than δ, or
analogical reasoning produced a mapping that was inconsis-

tent or incomprehensible; this is meant to represent instances
where analogical reasoning fails and the subject must resort
to other reasoning methods.
Simulation Recall now that there are four quantitative parameters that were varied in this simulation: the number of
ants and rounds used by the ACO algorithm, the ε value representing the minimum similarity required for an identity link,
and the δ value which set the frequency of SBR.
For the 3-year-old group, a simulation of 20,000 iterations
was done using one ant, one round, and ε was randomly chosen from the [0.15, 0.3] range (one selection was made for
each simulated subject) to introduce variation. The value of
δ used was 0.15. Another simulation of 20,000 iterations was
done to simulate the 4-year-old group, which used five ants,
five rounds, ε ∈ [0.25, 0.4], and δ = 0.05. Finally, for the 6year-old group, the parameters were almost exactly the same
as the 4-year-old group; the only changes were the addition
of relationship chunks as pictured in Figure 2, and the setting
of δ = 0. The results are presented in Table 2.
Table 2: Percentage of correct choices and errors made by
the simulation. Errors are presented as a percentage of total
errors.
Percentage Passing Criterion (Simulation)
Age (simulated years)
3
4
6

Passed
41
74
98

Errors Made (Simulation)
Age (simulated years)
3
4
6

E
23
24
0

F
45
56
99

G
16
10
1

H
16
10
0

Conclusion
Significant features of the human data were preserved in the
simulation. For example, f errors were more frequent than e
errors, and the ratio of correct answers to incorrect answers
lines up nicely with the human data. We feel that this imparts
a certain level of plausibility to our assumptions, but nevertheless, it would be interesting to see if the assumptions made
in this paper hold up in other situations. The hand-chosen
values of the quantitative parameters aside, perhaps more interesting is our simulation’s assumption that part of what explains the difference in performance between 3-, 4-, and 6year-olds is a qualitative difference in the representational
structures used. If toddlers really do use concepts that are
not as “well-chunked” as those of older children, and instead
use concepts composed largely of separable, primitive components, subsequent experimentation might be able to support
or refute this difference. We encourage researchers to exam-

1 This idea was inspired by the quote reprinted in this paper’s introduction, in which Piaget suggests there are “no simple forms that
can be expressed in terms of stable classes” (Piaget et al., 2001). Perhaps the idea that in toddlers, concepts are still in a form which is
predominantly a loose tying-together of primitive conceptual structures, rather than a solid and robust collection of rich concepts, was
what Piaget had in mind.

861

ine this possibility, by designing experiments to detect and
understand the nature of these proto-concepts.
Regardless, the modeling of qualitative increases in representational strength is a promising direction, which our work
will continue to explore.

Shultz, T. R. (2012). A Constructive Neural-Network Approach to Modeling Psychological Development. Cognitive
Development, 27, 383-400.
Shultz, T. R., Mareschal, D., & Schmidt, W. C. (1994). Modeling cognitive development on balance scale phenomena.
Machine Learning, 16, 57-86.
Shultz, T. R., & Sirois, S. (2008). Computational Models
of Developmental Psychology. In R. Sun (Ed.), The cambridge handbook of computational psychology (pp. 451–
476). New York, New York, USA: Cambridge Univ Press.
Sternberg, R. J., & Nigro, G. (1980). Developmental Patterns
in the Solution of Verbal Analogies. Child Development,
51(1).
Sun, R. (1995). Robust Reasoning: Integrating Rule-Based
and Similarity-Based Reasoning. Artificial Intelligence,
75(2).
Sun, R. (2001). From Implicit Skills to Explicit Knowledge:
A Bottom-Up Model of Skill Learning. Cognitive Science,
25(2), 203-244.
Sun, R. (2002). Duality of the Mind: A Bottom Up Approach
Toward Cognition. Lawrence Erlbaum Associates, Mahwah, NJ.
Sun, R., & Zhang, X. (2004). Accounting for SimilarityBased Reasoning within a Cognitive Architecture. In Proceedings of the 26th annual conference of the cognitive science society. Lawrence Erlbaum Associates.
Sun, R., & Zhang, X. (2006). Accounting for a Variety of
Reasoning Data Within a Cognitive Architecture. Journal of Experimental and Theoretical Artificial Intelligence,
18(2).
Tversky, A. (1977). Features of Similarity. Psychological
Review, 84(4), 327-352.

References
Das Gupta, P., & Bryant, P. (1989, October). Young Children’s Causal Inferences. Child Development, 60(5), 11381146.
Doumas, L. A., Hummel, J. E., & Sandhofer, C. (2008). A
Theory of the Discovery and Predication of Relational Concepts. Psychological Review, 115, 1-43.
Fodor, J. A. (2008). LOT 2: The Language of Thought Revisited. Oxford Univ. Press.
Frye, D., Zelazo, P. D., & Palfai, T. (1995). Theory of Mind
and Rule-Based Reasoning. Cognitive Development, 10,
483-527.
Gentner, D., & Forbus, K. (2011). Computational Models
of Analogy. Wiley Interdisciplinary Reviews: Cognitive
Science, 2(3), 266-276.
Goldman, S., Pellegrino, J., Parseghian, P., & Sallis, R.
(1982). Developmental and Individual Differences in Verbal Analogical Reasoning. Child Development, 53, 550559.
Goswami, U., & Brown, A. L. (1990). Melting Chocolate
and Melting Snowmen: Analogical Reasoning and Causal
Relations. Cognition, 35(1), 69-95.
Hummel, J. E., & Holyoak, K. J. (2003). Relational Reasoning in a Neurally-plausible Cognitive Architecture: An
Overview of the LISA Project. Cognitive Studies: Bulletin
of the Japanese Cognitive Science Society, 10, 58-75.
Licato, J., Bringsjord, S., & Govindarajulu, N. S. (2013).
How Models of Creativity and Analogy Need to Answer the Tailorability Concern. In T. R. Besold, K.u. Kühnberger, M. Schorlemmer, & A. Smaill (Eds.), Proceedings of the ijcai 2013 workshop on computational creativity, concept invention, and general intelligence. Beijing, China.
Licato, J., Sun, R., & Bringsjord, S. (2014). Structural Representation and Reasoning in a Hybrid Cognitive Architecture. In Proceedings of the 2014 international joint conference on neural networks (ijcnn).
Piaget, J. (1977). The Essential Piaget (H. E. Gruber &
J. J. Voneche, Eds.). New York, New York, USA: Basic
Books, Inc.
Piaget, J., Montangero, J., & Billeter, J. (2001). The Formation of Analogies. In R. Campbell (Ed.), Studies in reflecting abstraction. Psychology Press.
Sammoud, O., Solnon, C., & Ghédira, K. (2005). An Ant Algorithm for the Graph Matching Problem. In 5th european
conference on evolutionary computation in combinatorial
optimization (evocop 2005). Springer.
Shultz, T. R. (2003). Computational Developmental Psychology. Cambridge, Massachusetts: The MIT Press.

862

