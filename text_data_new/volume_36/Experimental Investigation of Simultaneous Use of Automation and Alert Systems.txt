UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Experimental Investigation of Simultaneous Use of Automation and Alert Systems

Permalink
https://escholarship.org/uc/item/04s1z6t1

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)

Authors
Maehigashi, Akihiro
Miwa, Kazuhisa
Terai, Hitoshi
et al.

Publication Date
2014-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Experimental Investigation of Simultaneous Use of Automation and Alert Systems
Akihiro Maehigashi (mhigashi@cog.human.nagoya-u.ac.jp)
Institute of Innovation for Future Society, Nagoya University, Japan

Kazuhisa Miwa (miwa@is.nagoya-u.ac.jp)
Hitoshi Terai (terai@is.nagoya-u.ac.jp)
Graduate School of Information Science, Nagoya University, Japan

Kazuaki Kojima (kojima@lt-lab.teikyo-u.ac.jp)
Learning Technology Laboratory, Teikyo University, Japan

Junya Morita (j-morita@cmc.ss.is.nagoya-u.ac.jp)
Institute of Innovation for Future Society, Nagoya University, Japan
Abstract
In this study, we experimentally investigated the effects of
the interaction between two individual systems, an automation
system that conducts tasks and an alert system that monitors
automation performance and alerts users to automation failures. The experimental results showed that when users used
automation and alert systems together, when the alert system
missed automation failures, the participants lowered their trust
not only in the alert system but also in the automation system.
It means that the participants confused trust in the automation
system with that in the alert system. Moreover, when the participants highly trusted the automation system, they slowly responded to a true alert from the alert system. These results
were discussed on the basis of the theory shown in previous
studies.
Keywords: Automation system; Alert system; Trust; Miss;
False alarm

Introduction
In recent years, progress in technology provides many opportunities for people to use automation systems such as autodriving systems and autopilot systems. An automation system
is a technology that autonomously conducts a task on behalf
of humans (Parasuraman & Riley, 1997). Users can reduce
workload by using automation systems. The problem with using automation systems is that they are not always perfect and
cause failures that might lead to fatal accidents. Therefore,
when using an automation system, users need to monitor automation performance, and if automation failures occur, they
have to handle the failures (Parasuraman & Manzey, 2010).
An alert system is a technology that monitors the automation performance for possible failures on behalf of users and
alerts users when automation failures occur (e.g., Dixon &
Wickens, 2006; Wickens & Colcombe, 2007). The users can
reduce monitoring loads and suppress missing automation
failures by using an alert system. However, the problem with
using alert systems is that they are not always perfect, and
similar to automation systems, they can cause failures. There
are two types of alert failures: miss and false alarm (FA). Miss
is failing to alert users when an automation failure happens,
and FA is alerting users when no automation failure happens
(Sorkin & Woods, 1985). Therefore, the problem with using
automation and alert systems together is that when one of the
systems or both systems cause failures, users have difficulties understanding what is actually happening and finding the
cause of the failures.
Such a problem that occurs when people use complex artifacts has been pointed out in the field of cognitive science and

cognitive engineering. Rasmussen (1986) indicated that as artifacts became complex according to progress in technology,
the role of the artifacts became large in the interaction between humans and the artifacts, and the connection between
humans and the objects of tasks became indirect. Moreover,
Norman (1988) indicated that in a situation in which such an
indirect connection between humans and the objects of tasks
occurs, there are difficulties in users’ execution of actions and
evaluation of the results of the actions; therefore, the interface
between users and artifacts should be designed to allow users
to appropriately execute actions and evaluate the results of
these actions.
In the previous studies of automation and alert systems, it
has been found that when users use automation and alert systems, which have complex internal processing mechanisms,
they make presumptions as to what is actually happening inside of the systems and to the objects of tasks by the use of
trust as an evaluation parameter. A brief summary of the findings of previous studies is as follows:
• Preceding Finding (PF) I: Automation performance determines users’ trust in the automation system. In particular, automation failures lower users’ trust in the automation
system (Lee & Moray, 1992; Parasuraman & Riley, 1997).
• PF II: In the previous studies about alert systems, because
the main focus was investigating the effects of missed alerts
and FAs on human behavior, those effects on users’ subjective trust in the alert systems were not directly measured.
However, the results of the experiments predict that missed
alerts and FAs lower users’ trust in the systems (e.g., Dixon
& Wickens, 2006; Wickens & Colcombe, 2007).
• PF III: Users’ trust in an alert system influences their response to a true alert (TA), alerting users when an automation failure happens. Particularly, FAs lower users’ trust
in the alert system and cause delayed responses to TAs
(Cry-wolf effect) (e.g., Dixon & Wickens, 2006; Wickens
& Colcombe, 2007).
In the previous studies, automation and alert systems were
treated as two different systems and were individually investigated in terms of the points written above. In contrast with the
previous studies, in this study, we investigated the interaction
between the two different systems. Figure 1 is a conceptual
representation that summarizes the findings of the previous
studies and the research questions in this study. The research
questions in this study are as follows:

2609

• Research question (RQ) I: Does automation performance
influence users’ trust in the alert system?
• RQ II: Does alert performance influence users’ trust in the
automation system?
• RQ III: Does users’ trust in the automation system influence their response to a TA from the alert system?
Alert
performance

PF II

Trust in
alert system

RQ I
RQ II

Automation
performance

PF I

PF III

Response to
true alert

RQ III

Trust in
automation system

Figure 1: Conceptual representation. The solid lines show
the findings of previous studies. The dashed lines show the
research questions in this study.
Regarding RQ I, there is a possibility that when users lower
their trust in an automation system, they might increase vigilance against alert failures because it informs users of automation failures, and as a result, they might also lower their
trust in the alert system. Also, regarding RQ II, the previous studies about alert systems showed that when users experience missed alerts, they lower their trust in and reliance
on the system, and they monitor automation performance by
themselves (e.g., Dixon & Wickens, 2006; Wickens & Colcombe, 2007). Such a phenomenon occurs only when users
experience missed alerts, but not when they experience TAs
or FAs. Therefore, only when users experience missed alerts,
they would lower their trust in the system and increase vigilance against automation failures, and as a result, they might
lower their trust in the automation system. Moreover, regarding RQ III, the previous studies about automation systems
showed that when users overtrust an automation system, they
neglect to monitor automation performance or possibly miss
the typical problems with the automation system (Complacency) (e.g., Parasuraman & Manzey, 2010; Parasuraman &
Wickens, 2008). Therefore, when users highly trust the automation system, they might be slow to respond to a TA from
the alert systemD

Experimental task
We used a dual task consisting of search and line-tracing tasks
(Figure 2). The automation system in Figure 1 indicates the
system that autonomously conducts the line-tracing task. The
alert system in Figure 1 indicates the system that monitors the
automation line-tracing performance. The search task was set
up to experimentally control the participants’ visual attention
as a sub-task. The search task display was set in front of the
participant, and the line-tracing display was set on the right
side of the participant.
In the search task, the participants looked for target stimuli
(mirror L) among distracter stimuli (T and L) that scrolled
downward. When the target was found, the participants
pressed a selector on the keyboard while the target was inside the double line (the detection area) in the middle of the

window. If the target was successfully detected, the color of
the target letter changed to red. When the participants missed
the target or gave a false alarm, the performance score was
reduced as an operational error.
In the line-tracing task, the participants monitored an automation system that operates a circular vehicle to trace a line.
The line scrolled downward past the vehicle. When the vehicle veered off the line, the performance score was reduced as
an operational error. Basically, the automation system perfectly operated the vehicle to trace the line. However, at specific times during the task, the automation system was set up
to cause failures. When the automation failure occurred, no
operational command was given to the vehicle from the automation system for ten seconds; that is, the vehicle suddenly
stopped tracing the line and veered off. When the participants
detected the automation failures, they had to manually operate the vehicle by pressing the left and right arrow keys to
trace the line.
In the experiment, while automation failures were happening, an alert system displayed a flashing red square frame
around the search task window as an alert and informed the
participants of the failures. The automation system restarted
normal operation and recovered from the failure after the ten
seconds. At that time, the search and the line-tracing task
windows showed “Performance Recovered” at the bottom.
The participants were instructed to delegate the operation
back to the automation system after the recovery.

Figure 2: Dual task

Experiment 1
Method
Participants Forty university students participated in this
experiment.
Factorial design The experiment had a two-factor mixed
design. The factors were (1) Automation performance (high
and low) between participants; (2) Alert performance (hit,
miss, and FA) within participants.
Procedure Experiment 1 had three trials of twelve minutes
each. Twenty-one participants were randomly assigned to the
high-automation-performance condition and nineteen to the
low-automation-performance condition. In terms of the automation performance factor, in each trial, the automation
failures occurred four times in the high- and eight times in
the low-automation-performance conditions. In terms of the
alert performance factor, alert performance, hit, miss, and FA
were controlled for each trial, and the order of the alert performance was counterbalanced. In the hit condition, the participants were alerted to all the failures. In the miss condi-

2610

tion, the participants were only alerted to half of all the failures and not alerted to the other half. In the FA condition,
the participants were alerted to all the failures, and FAs occurred the same number of times as the automation failures
occurred. Moreover, the times of the first and last automation
failures were consistently maintained among all the experimental conditions, and the alert system displayed TAs for the
failures. We confirmed homogeneity among the participants
and among the experimental conditions based on the response
to the first TA. On this basis, we investigated RQ III based on
the response to the last TA.
The participants were instructed to acquire as high a score
as possible in the search and line-tracing tasks. They were
also instructed to manually operate the vehicle only when
automation failures occurred in the line-tracing task. After
each trial was completed, we asked the participants to separately rate their trust in the automation and alert systems using a seven point scale (1: Extremely untrustworthy, 2: Very
untrustworthy, 3: Somewhat untrustworthy, 4: Neither trustworthy nor untrustworthy, 5: Somewhat trustworthy, 6: Very
trustworthy, 7: Extremely trustworthy). Based on the results
of the questionnaire, we investigated RQ I and II.

Prediction
First, regarding RQ I, if automation performance influences
users’ trust in the alert system, the following Prediction 1
would be confirmed.
• Prediction 1: The trust rating for the alert system would be
higher in the high-automation-performance condition than
in the low one.
Second, regarding RQ II, if alert performance influences
users’ trust in the automation system, the following Prediction 2 would be confirmed.
• Prediction 2: The trust rating for the automation system
would be lower in the miss condition than in the hit and FA
conditions. Also, there would be no difference between the
hit and FA conditions in the trust rating for the automation
system.
Third, regarding RQ III, if users’ trust in the automation
system influences their response to a TA from the alert system, the following Prediction 3 would be confirmed.
• Prediction 3: The trust rating for the automation system
would be higher, and also, the response time to the last
TA would be slower in the high-automation-performance
condition than in the low one.
Moreover, according to the previous studies, the following
predictions were expected to be confirmed.
• Prediction A: The trust rating for the automation system
would be higher in the high-automation-performance condition than in the low one.
• Prediction B: The trust rating for the alert system would be
lower in the miss and FA conditions than in the hit condition.
• Prediction C: The response time to the last TA would be
slower in the FA condition than in the hit and miss conditions.

Result and discussion
In the analysis of Experiment 1, we conducted 2(Automation
performance: high and low)×3(Alert performance: hit, miss,
and FA) ANOVA on all the dependent variables. First, to investigate RQ I, we conducted analysis of the trust rating for
the alert system. Figure 3a shows the result of the trust rating for the alert system. As a result of the analysis, there was
neither a significant two-way interaction (F(2, 76) = .16, n.s.)
nor a significant main effect on the automation performance
factor (F(1, 38) = .13, n.s.). However, there was a significant main effect on the alert performance factor (F(2, 76) =
58.53, p < .001). As a result of the multiple comparison
(Ryan’s method), the trust rating was lower in the miss and
FA conditions than in the hit condition (ps < .001). There
was no significant difference in the trust rating in the miss
condition versus that in the FA condition (n.s.). From these
results, since there was no main effect on the automation performance factor, Prediction 1 was not confirmed. Also, since
the trust rating for the alert system was lower in the miss and
FA conditions than in the hit condition, Prediction B was confirmed.
Next, to investigate RQ II, we conducted analysis of the
trust rating for the automation system. Figure 3b shows the
results of the trust rating for the automation system. As a
result of the analysis, there was no significant two-way interaction (F(2, 76) = .01, n.s.). However, there was a significant main effect on the automation performance factor
(F(1, 38) = 5.09, p < .05), showing that the trust rating was
higher in the high-automation-performance condition than in
the low one. Moreover, there was a significant main effect
on the alert performance factor (F(2, 76) = 20.26, p < .001).
As a result of multiple comparison (Ryan’s method), the trust
rating was lower in the miss condition than in the hit and FA
conditions (ps < .001). There was no significant difference
in the trust rating in the hit condition versus that in the FA
condition (n.s.). From these results, since the trust rating for
the automation system was lower in the miss condition than
in the hit and FA conditions, and there was no difference in
the trust rating between the hit and FA conditions, Prediction
2 was confirmed. Also, since there was a main effect on the
automation performance factor, Prediction A was confirmed.
Based on the confirmation of Prediction A, we investigated
RQ III.
To investigate RQ III, we conducted analysis of the response time to the last TA. Figure 3c shows the result of the
response time to the last TA. As a result of the analysis, there
was neither a significant two-way interaction (F(2, 76) =
1.36, n.s.) nor a significant main effect on the automation
performance factor (F(1, 38) = 1.88, n.s.). However, there
was a significant main effect on the alert performance factor
(F(2, 76) = 9.85, p < .001). As a result of multiple comparison (Ryan’s method), the response time was slower in the
FA condition than in the hit and miss conditions (ps < .001).
There was no significant difference in the response time in
the hit condition versus that in the miss condition (n.s.).
From these results, since there was no main effect on the au-

2611

!"#$%&'()$&")*+,$-.&$"/0&)$(1()02

!3#$%&'()$&")*+,$-.&$"').2")*.+$(1()02

!4#$50(6.+(0$)*20$).$)70$/"()$%8

Figure 3: Trust ratings for alert and automation systems and response time to the last TA
tomation performance factor, Prediction 3 was not confirmed.
Also, since the response time was slower in the FA condition
than in the hit and miss conditions, Prediction C was confirmed. In addition, the analysis performed on the response
time to the first TA showed no significant interaction and no
significant main effects (Fs < 2.50, n.s.).
The result of RQ III shows that trust in the automation system did not influence the response to the last TA. However,
a question was raised about the result. In this experiment,
the automation failures were set up to occur four times in
the high- and eight times in the low-automation-performance
conditions. Also, the alert performance was controlled on the
basis of the number of times that the automation failures occurred, that is, in the miss condition, half of the automation
failures were missed, and in the FA condition, FAs occurred
the same number of times as the automation failures occurred.
Therefore, the participants experienced more missed alerts
and FAs in the high-automation-performance condition than
in the low one. As a result, there might be a possibility that
the number of instances of missed alerts and FAs, as an unexpected third factor, influenced the participants’ response to
the last TA, and we could not confirm Prediction 3. To reject
the question, we conducted Experiment 2 and investigated
RQ III.

Method

Experiment 2

Participants Thirty-eight university students participated
in this experiment.
Factorial design The experiment had a two-factor mixed
design. The factors were (1) Automation performance (high
and low) between participants; (2) Alert performance (5 hits,
miss, 2 hits, and FA) within participants.
Procedure We used the same experimental task as in Experiment 1. Experiment 2 had four trials of ten minutes and
thirty seconds each. In Experiment 2, we set up the first and
the second phases, and the two factors were separately manipulated in each phase to keep the number of instances of
missed alerts and FAs consistent among all the participants.
The other task settings were the same as those in Experiment
1.
First, the first phase took four minutes and thirty seconds.
In this phase, we manipulated the automation performance
factor. All the participants conducted the dual task without
the alert system. They had to monitor the automation per-

formance by themselves. Twenty participants were randomly
assigned to the high-automation-performance condition and
eighteen to the low-automation-performance condition. During the first phase, the automation failures occurred once in
the high-automation-performance condition and five times in
the low one. After the first phase was completed, the task
displays were grayed out, and the trust rating for the automation system was conducted as in Experiment 1. After the trust
rating was completed, the participants voluntarily pressed the
“R” key on the keyboard and started the second phase.
Next, the second phase took six minutes. In this phase, we
manipulated the alert performance factor. In terms of alert
performance, 5 hits, miss, 2 hits, and FA were controlled for
each trial, and the order of the alert performance was counterbalanced. In the second phase, we set up the 5 hits condition
and the 2 hits condition to compare the trust ratings for the
systems and the response time to a TA in the 5 hits condition
versus those in the miss condition and in the 2 hits condition
versus those in the FA condition separately, and we investigated the effects of missed alerts and FAs. In the 5 hits and
the miss conditions, automation failures occurred five times.
The participants were alerted to all the failures in the 5 hits
condition. On the other hand, in the miss condition, the participants were only alerted to the first and the last failures
and not alerted to the other failures. Moreover, in the 2 hits
and the FA conditions, the automation failures occurred two
times, and the participants were alerted to both of the failures. However, In the FA condition, FAs occurred three times
between the two automation failures. After the second phase
was completed, the trust ratings for the automation and alert
systems were separately conducted as in Experiment 1.
In the second phase, the times of the first and last automation failures were consistently maintained among all the experimental conditions, and the alert system displayed TAs for
the failures. We investigated RQ III based on the responses to
the first and last TAs.

Prediction
Regarding RQ III, if users’ trust in the automation system
influences their response to a TA from the alert system, the
following Prediction 4 would be confirmed.
• Prediction 4: The trust rating for the automation system
right after the first phase would be higher, and also, the
response time to the first TA would be slower in the highautomation-performance condition than in the low one.

2612

Also, the following Prediction 5 would be confirmed.
• Prediction 5: The trust rating for the automation system
after the second phase would be higher, and also, the response time to the last TA would be slower in the highautomation-performance condition than in the low one.
Moreover, based on the results of RQ I and II in Experiment 1, the following predictions were expected to be confirmed.
• Prediction 6: There would be no difference in the trust
rating for the alert system between the high- and lowautomation-performance conditions.
• Prediction 7: The trust rating for the automation system
would be lower in the miss condition than in the 5 hits condition. Also, there would be no difference in the trust rating
for the automation system between the 2 hits and FA conditions.

Figure 4: Response time to the first TA
performance factor (A1:F(1, 36) = 2.73, n.s.; A2:F(1, 36) =
2.46, n.s.). However, there was a marginal significant main
effect on the alert performance factor in A1 (F(1, 36) =
3.70, p = .06), showing that the trust rating was lower in the
miss condition than in the 5 hits condition. By contrast, there
was no significant main effect on the alert performance factor
in A2 (F(1, 36) = 1.53, n.s.).

Result and discussion
In the analysis of Experiment 2, we conducted 2(Automation performance: high and low)×2(Alert performance: 5 hits
and miss) ANOVA (A1: Analysis 1) and 2(Automation performance: high and low)×2(Alert performance: 2 hits and
FA) ANOVA (A2: Analysis 2) on all the dependent variables. First, we conducted analysis of the trust rating for
the automation system right after the first phase. As a result of the analysis, there was no significant two-way interaction (A1:F(1, 36) = 1.29, n.s.; A2:F(1, 36) = 1.22, n.s.).
However, there was a significant main effect on the automation performance factor (A1:F(1, 36) = 26.25, p < .001;
A2:F(1, 36) = 14.73, p < .001), showing that the trust rating was higher in the high-automation-performance condition
than in the low one. Also, there was no significant main effect on the alert performance factor (A1:F(1, 36) = .35, n.s.;
A2:F(1, 36) = .67, n.s.). From these results, since there was a
significant main effect on the automation performance factor,
based on the confirmed result, we investigated RQ III.
To investigate RQ III, we conducted analysis of the response time to the first TA. Figure 4 shows the response
time to the first TA. As a result of the analysis, there was
no significant two-way interaction (A1:F(1, 36) = .56, n.s.;
A2:F(1, 36) = .87, n.s.). However, there was a significant main effect on the automation performance factor
(A1:F(1, 36) = 7.56, p < .01; A2:F(1, 36) = 4.20, p < .05),
showing that the response time was slower in the highautomation-performance condition than in the low one. Also,
there was no significant main effect on the alert performance
factor (A1:F(1, 36) = .02, n.s.; A2:F(1, 36) = 1.46, n.s.).
From these results, since there was a main effect on the automation performance factor, Prediction 4 was confirmed.
Next, we conducted analysis of the trust rating for the automation system after the second phase. Figure 5 shows the
trust rating for the automation system after the second phase.
As a result of the analysis, there was neither a significant
two-way interaction (A1:F(1, 36) = .11, n.s.; A2:F(1, 36) =
.44, n.s.) nor a significant main effect on the automation

Figure 5: Trust rating for automation system after the second
phase
After the first phase, the trust rating for the automation system was higher in the high-automation-performance condition than in the low one. On the other hand, after the second phase, there was no such difference in the trust rating.
Since we manipulated the automation performance factor in
the first phase and the alert performance in the second phase,
the frequency of the automation failures was changed from
the first to the second phase. It is supposed that according to
that change, trust in the automation system also changed, and
the difference in the trust rating between the two conditions
disappeared. Therefore, there was no confirmed result to investigate Prediction 5. In fact, as a result of the analysis of
the response time to the last TA, there was no main effect on
the automation performance factor (A1:F(1, 36) = 1.21, n.s.;
A2:F(1, 36) = .02, n.s.), and Prediction 5 was not confirmed.
However, since the trust rating for the automation system was
lower in the miss condition than in the 2 hits condition, and
there was no difference in the trust rating between the 2 hits
and FA conditions, Prediction 7 was confirmed as the same
result found in Experiment 1.
Finally, we conducted analysis of the trust rating for the
alert system. Figure 6 shows the result of the trust rating for
the alert system. As a result of the analysis, there was neither
a significant two-way interaction (A1:F(1, 36) = .06, n.s.;
A2:F(1, 36) = .14, n.s.) nor a significant main effect on
the automation performance factor (A1:F(1, 36) = .06, n.s.;
A2:F(1, 36) = .72, n.s.). However, there was a significant
main effect on the alert performance factor (A1:F(1, 36) =

2613

126.50, p < .001; A2:F(1, 36) = 174.52, p < .001), showing
that the trust rating was lower in the miss condition than in
the 5 hits condition and lower in the FA condition than in
the 2 hits condition. From these results, since there was no
significant main effect on the automation performance factor,
Prediction 6 was confirmed as the same result found in Experiment 1.

Figure 6: Trust rating for alert system

General Discussion
In this study, we investigated the effects of the interaction between two different systems, an automation system that conducts tasks and an alert system that monitors automation performance. For RQ I, we confirmed that automation performance did not influence trust in the alert system. By contrast,
for RQ II, we confirmed that alert performance influenced
trust in the automation system. In particular, missed alerts
lowered trust in the automation system. Also, for RQ III, we
revealed that trust in the automation system influenced the
response to a TA from the alert system.
First, we will discuss RQ I. An automation system is a task
performing system, and by contrast, an alert system has a
meta-viewpoint of an automation system as a meta-system
does. A meta-system is an intelligent supervisory system that
monitors, predicts, and controls automation performance as
if a human supervisor does (Chen & Barnes, 2012). It is assumed that users would build trust in such meta-systems in
a similar manner as interpersonal trust. In the field of social
psychology, it is known that people gradually build trust in
other people based on other peoples’ behavior. Even when
others cause some mistakes or errors, people do not suddenly
lower their trust in others, but gradually lower their trust
(Mayer, Davis, & Schoorman, 1995). There is a possibility
that the participants in this study might recognize the alert
system as such a meta-system; therefore, based on the stability of interpersonal trust, the influence from the automation
failures on the alert system might be suppressed.
Next, regarding RQ II, the previous studies about alert systems showed that when users experience missed alerts, they
lower their trust in and reliance on the system, and they
monitor automation performance by themselves for possible failures (e.g., Dixon & Wickens, 2006; Wickens & Colcombe, 2007). It is supposed that the participants in this
study showed the same behavior as shown in the previous
studies and increased vigilance against the automation failures. Furthermore, the previous study about automation systems showed that when users perceive automation failures,

they extremely lower their levels of trust in the automation
system (Dzindolet, Peterson, Pomranky, & Pierce, 2003). It
is assumed that based on such sensitivity of trust in automation systems, the participants in this study who experienced
missed alerts and increased vigilance against the automation
failures lowered their levels of trust in the automation system.
Finally, regarding RQ III, an alert system is a system that
alerts users to automation failures. Therefore, users’ response
to a TA is a response to the alert and is also a response to the
automation failure. The previous studies about automation
systems showed that when users overtrust an automation system, they neglect to monitor automation performance or possibly miss the typical problems with the automation system
(e.g., Parasuraman & Manzey, 2010; Parasuraman & Wickens, 2008). It is supposed that the participants in this study
who had higher trust in the automation system decreased vigilance against the automation failures and showed slower responses to the failures even when TAs were displayed.

References
Chen, J. Y. C., & Barnes, M. J. (2012). Supervisory control
of multiple robots: Effects of imperfect automation and individual differences. Human Factors, 54, 157–174.
Dixon, S. R., & Wickens, C. D. (2006). Automation reliability in unmanned aerial vehicle control: A reliancecompliance model of automation dependence in high workload. Human Factors, 48, 474–486.
Dzindolet, M. T., Peterson, S. A., Pomranky, R. A., & Pierce,
L. G. (2003). The role of trust in automation reliance. International Journal of Human-Computer Studies, 58, 697–
718.
Lee, J. D., & Moray, N. (1992). Trust, control strategies
and allocation of function in human-machine systems. Ergonomics, 35, 1243–1270.
Mayer, C., Davis, J. H., & Schoorman, F. D. (1995). An
integrative model of organizational trust. The Academy of
Management Review, 20, 709–734.
Norman, D. A. (1988). The psychology of everyday things.
New York, NY: Basic Books.
Parasuraman, R., & Manzey, D. H. (2010). Complacency
and bias in human use of automation: An attentional integration. Human Factors, 52, 381–410.
Parasuraman, R., & Riley, V. (1997). Humans and automation: Use, misuse, disuse, abuse. Human Factors, 39, 230–
253.
Parasuraman, R., & Wickens, C. D. (2008). Humans: Still
vital after all these years of automation. Human Factors,
50, 511–520.
Rasmussen, J. (1986). Information processing and humanmachine interaction: An approach to cognitive engineering. New York, NY: Elsevier Science Publishing.
Sorkin, R. D., & Woods, D. D. (1985). Systems with human
monitors: A signal detection analysis. Human-Computer
Interaction, 1, 49–75.
Wickens, C., & Colcombe, A. (2007). Dual-task performance
consequences of imperfect. Human Factors, 49, 839–850.

2614

