UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Equations Are Effects: Using Causal Contrasts to Support Algebra Learning

Permalink
https://escholarship.org/uc/item/7fr6f015

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)

Authors
Walker, Jessica M.
Cheng, Patricia W.
Stigler, James W.

Publication Date
2014-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Equations are Effects: Using Causal Contrasts to Support Algebra Learning
Jessica M. Walker (jessicamariewalk@gmail.com)
Patricia W. Cheng (cheng@lifesci.ucla.edu)
James W. Stigler (stigler@psych.ucla.edu)
Department of Psychology, University of California, Los Angeles, CA 90095-1563 USA
Abstract
U.S. students consistently score poorly on international
mathematics assessments. One reason is their tendency to
approach mathematics learning by memorizing steps in a
solution procedure, without understanding the purpose of
each step. As a result, students are often unable to flexibly
transfer their knowledge to novel problems. Whereas
mathematics is traditionally taught using explicit instruction
to convey analytic knowledge, here we propose the causal
contrast approach, an instructional method that recruits an
implicit empirical-learning process to help students discover
the reasons underlying mathematical procedures. For a topic
in high-school algebra, we tested the causal contrast approach
against an enhanced traditional approach, controlling for
conceptual information conveyed, feedback, and practice. The
causal contrast approach yielded remarkably greater success,
especially on novel problems, across students with varying
levels of mathematical competence.
Keywords: comparison; mathematics education; causal
induction; concepts, knowledge transfer

Introduction
Mathematics experts use general goal-directed reasoning to
solve problems. Students, in contrast, are commonly
observed to use procedures without understanding, often
just following a memorized sequence of steps to solve a
problem (Stigler, Givvin, & Thompson, 2010). Because
they have not connected procedures to goals or concepts,
students lack the flexibility of experts: they often are unable
to decompose a procedure and apply the constituent
operations successfully to a novel problem for which the
learned sequence is inadequate (Cooper & Sweller, 1987).
This lack of goal-directed reasoning in mathematics may be
a major cause of the poor performance of U.S. students
(PISA, 2009, 2013; Stigler & Hiebert, 1999; TIMSS, 2007).
A critical prerequisite for goal-directed reasoning is
learning cause-and-effect relations. People, including
children and even infants, have a natural capacity for
learning causal relations (Cheng, 1997; Gopnik et al, 2004;
Leslie & Keeble, 1987). Much like language learning,
causal induction is a universal learning process that is
shared by virtually all humans. Because of its universality,
causal induction can be recruited in nearly all learners to
support goal-directed reasoning. Once an effect of interest is
identified, most learners will naturally seek to discover
causes of that effect. Moreover, they will succeed if the
requisite information for the discovery is readily available.
For example, imagine you wake up one morning,

disappointed to discover that your digital video recorder
(DVR) failed to record a special television show last night
(your goal). You would probably think back to occasions on
which your DVR successfully recorded, and compare them
to the failed attempt. If a presetting to record a regular show
is the only feature that differs between your failed and
successful attempts, you would readily determine the cause
of the failure -- the presetting interfered with your new
setting. This process enables you to discover a new causal
relation and better understand how your DVR works.
Causal induction is the process whereby we come to
know how the empirical world works; it is what allows us to
predict, diagnose, and intervene on the world to achieve an
effect. Based on the premise that action-outcome relations
in mathematics are no less causal than those in other
domains, we ask: Can we 1) activate students’ natural
capacity for causal induction during mathematical problemsolving, inducing them to formulate the goals of a
mathematical procedure, and 2) promptly have the requisite
information available, allowing students to discover the
actions that would achieve those goals, and thereby to
understand the causal structure of the solution to a problem?
If so, students may become better able to generalize their
mathematical knowledge. Although previous researchers
have noted the importance of applying causal knowledge to
solve mathematical problems (e.g., Anderson, 1990), to our
knowledge the simple but powerful process of causal
induction has never been systematically recruited in the
acquisition of the requisite causal knowledge in
mathematics education. Causal induction may at first appear
irrelevant to mathematical learning, as mathematics is
analytic whereas causation is empirical. Moreover, the
causal induction process is implicit; neither its requisite
input nor its operation is open to introspection. Thus, the
potential use of this process in mathematics learning does
not readily present itself.
Traditional instructional approaches teach students
analytically and explicitly the rules and steps for solving
specific types of problems. For example, a student presented
with this equation
(Eq. 1)
x2 – 13x – 30 = 0
is taught to first factor the expression on the left, then
determine the possible values of x using the zero-product
property (ZPP) (if a•b = 0, then a = 0 or b = 0). For a
substantial fraction of students, as evidenced by their failure
to flexibly generalize their learning to novel problems, this

1730

approach does not impart an understanding of the causal
structure of the solution, the reason behind each step in the
procedure and how the steps work together. For example,
what is the purpose of factoring the expression on the left
hand side (LHS) -- why can’t one merely rearrange the
equation to solve for x? And what is the relationship
between factoring and the ZPP? Even when students are
given opportunities to compare worked examples (RittleJohnson & Star, 2007; Rittle-Johnson, Star & Durkin,
2009), or explain solutions (Chi et al, 1989, 1994; Chi,
2000), they may still fail to formulate questions that enable
understanding of the causal structure of a problem.
The approach we are exploring is designed to support
students’ discovery of the causal structure that underlies
mathematical procedures, the causal structure that reflects
the critical mathematical concepts essential to the solution
of relevant types of problems. At the heart of our approach
is the use of targeted comparison tasks designed to activate
students’ cause-effect learning at just the points in the
problem-solving process at which critical concepts should
apply. It is not the use of comparison per se that
differentiates our approach, but rather the targeting of
critical concepts by specific comparisons designed to invoke
causal induction. To continue with the same example,
students in our approach are first asked to try solving
Equation 1. If they fail to solve Equation 1, we present them
with a contrasting problem, one that is carefully chosen to
control for confounding factors by being as similar as
possible to Equation 1 but with the features causing the
difficulty removed. The contrast aims at invoking students’
implicit causal learning process, so that they discover the
cause of their difficulty. For example, after failing to solve
Eq. 1, students are asked to solve these equations:
(Eq. 2)
x – 30 = 0
(Eq. 3)
x2 – 30 = 0
After students solve these (all our participants did), they
are asked why these problems are easier for them to solve
than Equation 1. This comparison enables students to
readily discover a cause of their failure: unlike Equations 2
and 3, Equation 1 has both an x and an x2 term, preventing
them from isolating x by simply rearranging the equation.
The newly formulated cause -- having both x and x2 terms -in turn becomes an “effect” for subsequent operations to
prevent or remove. Additional contrast comparisons enable
students to discover a conjunctive preventive relation,
namely, that factoring and applying the ZPP in combination
can result in equations that can be rearranged to isolate x.
Figure 1 displays a fragment of the causal structure of the
solution for Equation 1. Whereas causal arrows point from
the bottom up in the figure, learners construct the causal
structure by experiencing the contrasting events from the
top down. Leaving out any “cause” or “effect” would create
gaps in one’s mental causal structure of the solution.
Without their initial attempt to isolate x, learners would
have no “effect” (top node in the figure) for which to
discover its cause. This effect -- failure to isolate x by
rearranging the equation -- is not explicitly noted in

traditional instruction on solving quadratic equations and
requires formulation. Conversely, without a comparison
with Equations 2 and 3, a learner who is asked to explain
why Equation 1 is difficult may mention the effect alone,
“I’m unable to rearrange the equation to isolate x”, omitting
to identify its “cause”: having both x and x2 terms in the
same equation, the intermediate node. In turn, without the
intermediate node, there would be no “effect” for learners to
subsequently discover its preventive cause (the bottom
node), namely, factoring. Thus, each comparison is designed
to direct attention to an essential causal relation in the
structure of the solution.

Figure 1. Fragment of the causal structure of solution to a
factorable quadratic equation.
In more general terms, the comparisons we construct
provide conditional contingency information (Cheng &
Holyoak, 1995), input that enables a causal contrast. For
events involving causes and effects that are binary – the
type of events of concern here – conditional contingency
information consists of the state of an effect (e.g.,
succeeding to solve a problem or not) in the presence of a
candidate cause (a feature of the math problem, e.g., an
equation having both an x and an x2 term) and in its absence
(an equation not having both an x and an x2 term), with
alternative causes held constant.
The process repeats as the student proceeds through the
solution. Each step involves 1) the student’s attempt to solve
a difficult problem that presents an opportunity for failure –
a misunderstanding of a critical concept or an impasse – and
2) a comparison of the difficult problem with a contrasting
easier problem. The failures identify the effects and the
comparisons provide conditional-contingency information
necessary for discovering their causes.
Continuing with our example problem, suppose that when
we ask students to solve a factored version of Equation 1,
(x – 15)(x + 2) = 0,
(Eq. 4)
they do not encounter an impasse. They proceed to make
use of the ZPP and infer that
x – 15 = 0 or
(Eq. 5)
x + 2 = 0.
(Eq. 6)

1731

For some students, this success may indicate complete
understanding of the causal structure. But for many students
it may not. For example, these same students, if given the
equation
(x – 2)(x + 2) = 12,
(Eq. 7)
might erroneously infer that
(x – 2) = 12 and
(Eq. 8)
(x + 2) = 12.
(Eq. 9)
In fact, this represents a prevalent misconception; roughly
half of our subjects (72% community-college students and
39% university students) committed this error on the pretest.
To address this misconception, we ask students to
compare Equation 4 with the easier contrasting problem
x • y = 0.
(Eq. 10)
We ask, “what values of x would make x • y = 0 true,
regardless of the value of y?” In this simple form, students
are able to think through the logic of the ZPP; all our
participants answered the question correctly. We then ask
them to compare Equation 10 with Equation 4. Solving
Equation 10 and comparing it to Equation 4 enables these
students to see the analogous structure underlying the two
equations and discover their misconception of the ZPP.
Thus, what is critical to learning from causal contrasts is a
misconception of the causal structure of a difficult problem,
whether that misconception is manifested in an impasse (as
in Equation 1) or succeeding for the wrong reason (as in
Equations 7, 8, and 9).
For students who are at an impasse, unable to solve
Equation 4, the comparison with Equation 10 may allow
them to discover why they failed to transfer, for example,
that the perceptual complexity of Equation 4 prevented them
from recognizing that the equation is a product, that the
content of the parentheses is “just a number”. At this point,
the perceptual complexity can switch causal roles, taking on
the role of an effect that the student can prevent or remove.
The students may in the future pause to “see through” the
perceptual complexity to recognize when a concept applies.
To summarize, the particular sequence of problems and
contrast comparisons in our materials were designed to
reveal the causal structure of the solution by focusing
students on the goals of procedures at various levels of
abstraction: why they factor, why they apply the ZPP, and
perceptual cues indicating when the property applies. Our
approach enhances the causal structure of the solution
procedure whenever it is incomplete. Understanding the
causal structure rather than memorizing the procedure
enables generalization and transfer.

Causal Contrast Compared to Similar Methods
Although our approach builds on previous methods in the
literature (Anderson, 1990; Bjork, 1994; Chi et al, 1989; Chi
et al; 1994, Chi, 2000; Kornell, Hays, & Bjork, 2009;
VanLehn, 1988), no previous study has tested learning via
causal-contrast comparisons. Unlike our materials, materials
in previous studies were not designed to provide
conditional-contingency information for the steps in a
solution procedure.

Rittle-Johnson and Star (2007) had students compare and
contrast alternative solution procedures to a worked
example (algebra problems), or two different worked
examples that use the same solution procedure (RittleJohnson et al, 2009). Although the comparison of alternative
solutions leads to an understanding of the common and
distinct features of alternative solutions, it is not targeted at
enhancing the learning of the purposes underlying
individual steps. Without the requisite conditionalcontingency information for the steps, students may still fail
to transfer to novel problems. Indeed, there is no evidence
that solution-procedure comparisons induced transfer; the
posttest problems in Rittle-Johnson and Star (2007) differed
only slightly from the worked examples appearing in the
instruction, and could be solved using the same procedures.
Notably, what they term “conceptual problems” were solved
equally well by the comparison and no-comparison groups.
Thus, while our approach shares with Rittle-Johnson et al.
the use of comparison tasks to promote learning, the two
approaches differ both in what is compared and which type
of reasoning process they engage.
Anderson (1990) argues that causal knowledge is used to
select the operators to achieve each step in a solution.
Unlike our approach, however, his does not give learners the
contingency information necessary to discover the reasons
for the steps. Instead, learners are simply encouraged to
retrieve the causal links from memory and apply them onto
the current situation. Indeed, Anderson (1987) admits that
his approach does not afford students the ability to infer the
reasons for the procedure.
The use of conditional-contingency information also
distinguishes our approach from previous work on the role
of self-explanation in learning. There is a large body of
work that shows that simply giving a prompt of “please
explain” or “why?” during learning can result in an
enhanced ability to generalize (Chi et al, 1989; Chi et al,
1994). This self-explanation effect is especially pronounced
when learners experience an impasse prior to their
explanation (Chi, 2000; VanLehn, 1988). Chi (2000) has
argued that this effect occurs because self-explaining helps
learners find and correct inaccuracies in their mental models
of the particular domain under study. However, as we
illustrated earlier regarding a self explanation of Equation 1
without a comparison with Equations 2 and 3, self
explanations formed in the absence of conditionalcontingency information may be too general and do not
ensure closing gaps in the causal structure.
The enhancement of the self-explanation effect by
impasses is part of a large literature showing more generally
that allowing learners to reach an impasse during problemsolving enhances transfer to novel problems (VanLehn,
1988). Related work in the memory literature indicates that
encountering difficulties (e.g., retrieving wrong answers)
during study improves performance at test (Bjork, 1994;
Kornell, Hays, & Bjork, 2009). While impasses and
difficulties may enhance learning, they are not necessary for
learners to benefit from causal contrasts. What drives

1732

learning in the causal-contrast approach is awareness of a
gap in the causal structure of the solution to a problem; an
impasse simply serves as a means to alert learners to the
gap. As we illustrated with our example contrasts (the
comparison between Eqs 4 and 7), causal contrasts can alert
learners to the gap even when learners do not experience an
impasse. Finally, impasses without the requisite conditionalcontingency information are insufficient for ensuring the
construction of a complete causal structure.
In summary, the present article illustrates a causal
contrast approach to teaching mathematics. By recruiting a
universal learning process through the use of conditionalcontingency information in the instructional materials to
elicit causal-contrast comparisons, the approach enables
students to formulate a causal structure of the solution. We
hypothesize that discovering the causal structure underlying
a solution procedure – a) the goals and their
interrelationships and b) the operations to achieve each goal
– should enhance even struggling students’ ability to
generalize and use the procedure more flexibly. We tested
our approach on community-college students taking
remedial algebra as well as on university students (to test
the generality of our findings). Although our illustrative
tests focus on algebra, the method we are testing is general
and can be applied to other areas of math.

Comparing Causal-Contrast and
Traditional Instruction
This experiment compared the effect of a causal-contrast
approach to teaching algebra (as developed above) with a
traditional approach, on learning and transfer. A betweensubjects pretest/intervention/delayed-posttest design was
employed, with the interventions focusing on solving
factorable quadratic equations.

Method
Participants. Participants included (N=47) community
college students recruited through College Algebra courses
in Southern California, and university students (N=66)
recruited from the UCLA Psychology Department
undergraduate subject pool. Participants were randomly
assigned to an intervention condition after the pretest1:
causal-contrast (n=50) and traditional (n=63).
Instructional Conditions. Students in the causal-contrast
condition were given a packet with “difficult” problems –
problems they were likely to fail – presented together with
their associated contrasts. Difficult problems were selected
to represent common misconceptions, and included x2 = 25,
x2 – 13x – 30 = 0 (Eq. 1), and 3x2 – 10x + 8 = 0. Carefully
1
Participants with ceiling or floor (below 20%) scores on the
pretest were thanked for their time and not invited to participate in
the study (13% of community college students excluded for ceiling
scores, 7% for low scores; 15% of university students excluded for
ceiling scores, 1% for low scores); the extremely low scores
indicated difficulty with basic mathematical notation.

following a script, the experimenter directed students’
attention to the problems and comparisons, and gave
feedback only to indicate whether the student solved a
problem correctly. Each difficult problem-contrast set was
followed by 2 or 3 practice problems that received no
feedback.
The traditional intervention was designed to make use of
techniques representative of excellent, but traditional,
instruction. Students in the traditional condition were given
a packet of instruction based on a popular textbook
(Sullivan & Sullivan, 2007) and lesson plans from
mathematics teachers at an academically rigorous private
school. The instruction packet included worked examples,
written solutions of example problems that provide
justifications for each procedural step, followed by problem
solving with and without feedback. Carefully following a
script, the experimenter went step-by-step through the
worked examples with each subject, stating the “sub-goals”
of the critical steps in the procedure; for example, subjects
were told that a quadratic equation is rearranged to standard
form (having a zero on one side and a polynomial on the
other) so that it could be factored and solved using the ZPP.
Emphasizing sub-goals in problem-solving has been shown
to enhance transfer (Eiriksdottir & Catrambone, 2011).
When a student solved a problem incorrectly, the
experimenter demonstrated how to solve the problem using
the procedure in the worked examples. Training that
combines worked examples and problem solving has been
shown to enhance learning (Sweller & Cooper, 1985).
The interventions were identical except for the
instructional method. Both conditions contained the exact
same problems, in the same order, with the same subset of
problems receiving feedback in both conditions. Both
interventions lasted an average of 30 minutes.
Posttest. The delayed posttest occurred 2 to 3 weeks after
the intervention and took at maximum 30 minutes. It
included two types of problems: instructed and transfer.
Instructed problems could be solved using the same solution
procedures as the study problems. Transfer problems
required generalization of concepts learned in the
intervention. For example, one transfer problem,
x2(2x + 1)(x + 1) = 0
(Eq. 11)
tests generalization of the role of the ZPP. In particular, the
problem aims to test whether causal-contrast participants
truly understood the difficulty in having both x and x2 terms
in a problem (a difficulty not present in Equation 11).
Would they be stymied, for example, because the factored
equation still contains x2? Other transfer problems included
factorable quadratics in non-standard form (see Figure 3).
If causal contrasts can result in an understanding of the
underlying purposes of procedures, participants in the
contrast group should outperform those in the traditional
group, especially on the transfer problems; they should be
able to use their knowledge flexibly to solve novel
problems. The instructed problems allow us to check the
effectiveness of our traditional intervention.

1733

χ2(1, N=47)=2.57, p=.05. Recall that this problem is
especially informative. The superior performance of the
causal-contrast participants indicates that their intervention
did not mislead them into formulating a simplistic rule
regarding the joint presence of x and x2 that ignores whether
they are terms or factors. Instead, the intervention enhanced
the correct flexible use of the concepts and procedures.

Results
Pretest. The causal-contrast and traditional groups did not
differ in pretest performance for both the communitycollege students (Mcontrast = 56.1+15.2; Mtraditional =
54.1+18.4), t(45)=.64, p=.45, or university students (Mcontrast
= 62.6+13.5; Mtraditional = 64.3+16.7), t(64)=1.32, p=.67.
Error terms indicate one S.D.

!

Posttest. For each sample, we conducted a one-way
MANCOVA with two levels of instruction (causal-contrast,
traditional) and two dependent measures (transfer and
instructed problem performance), using pretest score as a
covariate.
For the community college students, pretest score was
marginally related to posttest performance, F(1, 44)=2.44,
p=.10. Figure 2 (left panel) displays the posttest
performance data (adjusted means) for the communitycollege students. As the left panel of the figure indicates, the
between-group difference on the transfer problems was
remarkable (Mcontrast = 63.7+29.3 and Mtraditional = 36.1+28.7;
F(1, 44)=9.63, p=.003, d=.95). Whereas only about a third
of the transfer problems were solved in the traditional
group, almost two thirds were solved in the causal-contrast
group. The instructed
problems were also better solved by
!"#$%&'%#'&()&)#*+&)(,+&#)&--&./"&-/*0(1(1!&
the contrast group (Mcontrast = 89.2+13.2, Mtraditional =
70.4+31.8; F(1, 44) = 6.66, p=.01, d=.77).
0"/'1'+/')2+/$*'/)
!"##$%&'()!"**+,+/)-'$.+%'/)

$F;GF9:&-8;;FG:&

722&

';<9=HF;&

62&

(9=:;IG:FB&

'
!
!"

';<9=HF;&

&
!"

52&

%
!"

42&

$
!"

32&

#
!"

2&

3%&4+5/&'()-'$.+%'/))

(9=:;IG:FB&

!"

-89:;<=:&>9?3@A&

';<BCD89<E&>9?34A&

-89:;<=:&>9?35A&

';<BCD89<E&>9?42A&

Figure 2. Post-test performance (adjusted percent correct)
as a function of condition and problem type in communitycollege students and university students.
Although pretest score was related to posttest
performance (F(2, 62)=25.1, p<.001) in the university
students, a similar pattern of results for posttest performance
was obtained (see right panel of Figure 2). Controlling for
pretest score, the causal-contrast group substantially
outperformed the traditional group on both the transfer
problems (Mcontrast = 82.0 +25.9, Mtraditional = 58.1+32.3; F(1,
63)=16.53, p<.001, d=.61) and the instructed problems
(Mcontrast = 97.6+10.3, Mtraditional = 80.1+21.4; F(1,
63)=23.04, p<.001, d=.88.
Looking more closely at the performance of the
community-college students, we can see that in transfer
Problem 6 in Figure 3, there was a large difference in
performance across conditions: 62.5% correct versus 39.1%
respectively for the causal-contrast and traditional groups,

!"##$%&'()!"**+,+-)./0%-1+/)2/"3*+#)2+/1"/#0%4+)
"#$%%&$%!"'#()&*$!
!!
!!
!!

.'/01%1#,/)!
-#,%'/$%!

!!
!!
!!
"&'+&,%!-#''&+%!

!

Figure 3. Performance on individual transfer problems by
community-college traditional and causal-contrast groups.
Problem 5 in Figure 3 shows another large between-group
difference in performance: 58.3% versus 26.1% respectively
for causal-contrast and traditional, χ2 (1, N=47)=5.0, p=.01.
The successful students expanded the LHS and rearranged
the equation to obtain a 0 on the RHS. Most traditional
subjects made the common error discussed earlier.
To further assess the acquisition of goal-directed
reasoning, we examined participants’ solutions to two types
of posttest problems. Expansion problems are factored
equations that must be expanded and rearranged in order to
apply the ZPP (e.g., see Problem 5 in Figure 3). In contrast,
the ZPP can be directly applied to non-expansion problems
(e.g., Problem 6 in Figure 3)—that is, there is no need to
perform an additional step such as expanding the LHS to
enable the application of the ZPP. Novices, however, often
circuitously expand the LHS and simplify it back to its
original form before applying the ZPP. Participants were
coded as goal-directed if they solved all expansion problems
and solved a majority (2 out of 3) of the non-expansion
problems without the unnecessary expansion. We found that
for the community-college students, 54.2% of the causalcontrast group were goal-directed, compared to only 21.7%
of the traditional group, χ2 (1, N=47)=3.94, p=.047. A
similar pattern was found in the university students (53.8%
vs 27.5%), χ2 (1, N=66)=3.60, p=.058.
The consistent difference in performance between
conditions on each transfer problem (see Figure 3)
combined with the qualitative differences across conditions
in goal-directed solving indicates that when students
understand why they are performing certain operations in a
procedure, and what those operations do in the context of
the obstacles to be overcome, they are better able to flexibly
use an operation to achieve an end, resulting in greater

1734

success on transfer problems. On the other hand, when
students merely follow a procedure that they memorized—
even if they were once told the reasons for the operations in
that procedure—they are not as proficient in flexibly using
the operations to solve novel problems; their representation
of the underlying causal structure is evidently incomplete.

Discussion
Our results show that relative to a traditional approach, the
causal-contrast approach dramatically improves algebra
generalization and transfer in both community-college and
university students. Regardless of prior knowledge, the
causal-contrast participants showed both an enhanced ability
to solve novel transfer problems and a tendency for goaldirected solving. These results support the causal contrast
hypothesis that the combination of failures and conditionalcontingency information directs attention to the relevant
causal relations in the solution structure, enhancing
students’ ability to decompose a procedure and use the
constituent operations flexibly in solving problems.
Our findings are particularly striking in view of students’
typical inability to solve novel problems; the issue of
knowledge transfer is so pervasive, in fact, that mathematics
educators have termed it the “inert knowledge problem”.
Our results show that even when traditional, analytic
instruction focuses on teaching the reasons for mathematical
procedures, students still fail to learn in a way that promotes
generalization to novel problems. In contrast, by allowing
students to use an implicit, empirical learning process to
discover the causal structure of solutions, students with
different levels of skill and knowledge are able to fill in the
missing links of their causal structure, and consequently
need not rely on their rote memory of procedures.

Acknowledgments
This research was supported with funding from the Institute
of Education Sciences, U.S. Department of Education, grant
R305C080015.

References
Anderson, J.R. (1987). Causal Analysis and Inductive
Learning. Proceedings of the Fourth International
Workshop on Machine Learning. UC Irvine.
Anderson, J.R. (1990). Adaptive Character of Thought.
Hillsdale, New Jersey: Lawrence Erlbaum Associates.
Bjork, R.A. (1994). Memory and metamemory
considerations in the training of human beings. In J.
Metcalfe & A. Shimamura (Eds.), Metacognition:
Knowing about knowing. Cambridge, MA: MIT Press.
Cheng, P.W.(1997). From covariation to causation: A causal
power theory. Psychological Review, 104(2), 367-405.
Cheng, P.W., & Holyoak, K.J. (1995). Complex adaptive
systems as intuitive statisticians: causality, contingency,
and prediction. In Comparative Approaches to Cognitive
Science, Ed. HL Roitblat, J-A Meyer. Cambridge, MA:
MIT Press.
Chi, M.H. (2000). Self-explaining expository texts: The
dual processes of generating inferences and repairing

mental models. In Glaser, R. (Ed.), Advances in
Instructional Psychology. Mahwah, NJ: Lawrence
Erbaum Associates.
Chi, M. T. H., Bassok, M., Lewis, M. W., Reimann, P., &
Glaser, R. (1989). Self-explanations: how students study
and use examples in learning to solve problems. Cognitive
Science, 13(2), 145–182.
Chi, M. T. H., de Leeuw, N., Chiu, M., & LaVancher, C.
(1994).
Eliciting
self-explanations
improves
understanding. Cognitive Science, 18, 439-477.
Cooper G. & Sweller, J. (1987). Effects of schema
acquisition and rule automation on mathematical
problem-solving
transfer. Journal
of
Educational
Psychology, 79(4), 347–62.
Eiriksdottir, E., & Catrambone, R. (2011). Procedural
Instructions, Principles, and Examples: How to Structure
Instructions for Procedural Tasks to Enhance
Performance, Learning, and Transfer. Human Factors,
53(6), 749-770.
Gopnik, A., Glymour, C. Sobel, D. Schulz, L. Kushnir, T.,
& Danks, D. (2004). A theory of causal learning in
children: Causal maps and Bayes nets. Psychological
Review, 111(1), 1-31.
Kornell, N., Hays, M. J., & Bjork, R. A. (2009).
Unsuccessful retrieval attempts enhance subsequent
learning. Journal of Experimental Psychology: Learning,
Memory, & Cognition, 35, 989-998.
Leslie A. M., & Keeble, S. (1987). Do six-month-old
infants perceive causality?'' Cognition, 25, 265-288.
PISA (2009, 2013). Programme for International Student
Assessment. International Association for the Evaluation
of Educational Achievement.
Rittle-Johnson, B., & Star, J.R. (2007). Does Comparing
Solution Methods Facilitate Conceptual and Procedural
Knowledge? An Experimental Study on Learning to
Solve Equations. Journal of Educational Psychology,
99(3), 561-574.
Rittle-Johnson, B., Star, J. R., & Durkin, K. (2009). The
importance of familiarity when comparing examples:
Impact on conceptual and procedural knowledge of
equation solving. Journal of Educational Psychology,
101, 836-852.
Stigler, J. W., Givvin, K. B. & Thompson, B. (2010). What
community college developmental mathematics
students understand about mathematics. The
MathAMATYC Educator, 10, 4-16.
Stigler J.W., & Hiebert, J. (1999). The teaching gap: Best
ideas from the world’s teachers for improving education in
the classroom. New York: Free Press.
Sweller, J. & Cooper, G. A. (1985). The use of worked
examples as a substitute for problem solving in learning
algebra. Cognition and Instruction, 2, 59-89.
Sullivan, M.I., & Sullivan, M. (2007). College algebra:
Concepts through functions. New York: Pearson
Education.
TIMSS (2007). Trends in International Mathematics and
Science Study. US Department of Education.
VanLehn, K. (1988). Toward a theory of impasse-driven
learning. In H. Mandl & A. Lesgold (Eds.), Learning
issues for intelligent tutoring systems. New York:
Springer.

1735

