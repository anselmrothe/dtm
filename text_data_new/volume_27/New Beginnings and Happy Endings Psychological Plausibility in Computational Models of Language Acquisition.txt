UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
New Beginnings and Happy Endings: Psychological Plausibility in Computational Models of
Language Acquisition

Permalink
https://escholarship.org/uc/item/2xh6x0j3

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)

Authors
Christiansen, Morten H.
Onnis, Luca

Publication Date
2005-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

New Beginnings and Happy Endings:
Psychological Plausibility in Computational Models of Language Acquisition
Luca Onnis and Morten H. Christiansen
Department of Psychology, Cornell University, Ithaca, NY 14853, USA
Abstract
Language acquisition may be one of the most difficult tasks
that children face during development. They have to segment
words from fluent speech, figure out the meanings of these
words, and discover the syntactic constraints for joining them
together into meaningful sentences. Over the past couple of
decades computational modeling has emerged as a new
paradigm for gaining insights into the mechanisms by which
children may accomplish these feats. Unfortunately, many of
these models assume linguistic knowledge likely to be beyond
the abilities of developing young children. In this paper, we
argue that for computational models to be theoretically viable
they must be psychologically plausible. Consequently, the
computational principles have to be relatively simple, and
ideally empirically attested in the behavior of children. To
demonstrate the usefulness of simple computational
mechanisms in language acquisition, we present results from
a series of corpus analyses involving a simple model for
discovering lexical categories using word beginnings and
endings.

Introduction
By their third year of life children have already learned a
great deal about how words are combined to form complex
sentences. This achievement is particularly puzzling for
cognitive science for at least three reasons: firstly, whatever
learning mechanisms children bring to bear, they are
thought to be of simpler computational complexity than
adults’; second, children acquire most syntactic knowledge
with little or no direct instruction; third, learning the
complexities of linguistic structure from mere exposure to
streams of sounds seems vastly complex and unattainable.
A particularly hard case that we consider here is
the discovery of lexical categories such as nouns and verbs,
without which adult linguistic competence cannot be
achieved. Indeed the very core of syntactic knowledge is
typically characterized by constraints governing the
relationship between lexical categories of words in a
sentence. But acquiring this knowledge presents the child
with a “chicken-and-egg” problem: the syntactic constraints
presuppose the grammatical categories in terms of which
they are defined; and the validity of grammatical categories
depends on how far they support syntactic constraints.
Given the importance of this knowledge in language
acquisition much debate has centered on how grammatical
category information is bootstrapped from raw input. Even
assuming that the categories themselves are innate (e.g.
Pinker, 1984), the complex task of assigning lexical items
from a specific language to such categories must be learned
(e.g., the sound /su/ is a noun in French (sou) but a verb in
English (sue)). Crucially, children still have to map the right
1678

sound strings onto the right grammatical categories while
determining the specific syntactic relations between these
categories in their native language.
In trying to explain the bootstrapping problem, the
field of language acquisition has recently benefited from a
wave in computational modeling. Computational models
can be seen as intermediate tools that mediate between a
purely “verbal” theory and a purely experimental paradigm
(Broeder & Murre, 2003). As a computer implementation of
a theory, a computational model requires the modeler to
make the assumptions underpinnings their theory more
explicit. Because it involves an input, a process, and an
output, it can also be subjected to experimental
manipulations that test different conditions of behavior. As
an intermediate between theory and experiment, a model
can thus be judged in terms of how well it implements the
theory as well as how well it fits the data gathered. Despite
advances in computational modeling, many models are still
far from being psychologically plausible, i.e., they typically
assume a level of a) computational power and b) a priori
knowledge of the properties of a specific language that
seems implausible in children. For instance, the Latent
Semantic Analysis model of word learning (Landauer &
Dumais, 1997) builds lexical knowledge assuming that all
words in the language are already available.
In this paper, we argue that it is possible to build
more psychologically plausible computational models of
language acquisition when at least three requisites are met:
firstly, the characterization of linguistic knowledge assumed
by the model must be commensurate to the early child’s
fragmentary and coarse-grained knowledge of the growing
language. In this respect, it is psychologically more
plausible to assume that the child will start assigning words
to very broad categories that do not correspond to adult
lexical categories (Nelson, 1973); and that the first adultlike lexical categories will be the most relevant to successful
communication, for instance noun and verb categories will
be learned earlier than mappings between conjunctions and
prepositions (Gentner, 1982). Hence, computational models
of categorizations should encompass incrementality from
coarse- to finer-grained analyses.
A second important prerequisite is that minimal
assumptions should be made about the linguistic input
available to the learning mechanism, with the most minimal
assumption being that children start constructing a language
by perceiving sequences of sounds. Thirdly, whatever
mechanism is in place, it should be useful in discovering
linguistic structure across different language typologies.
To make a case for psychological plausibility, we
start by estimating the usefulness of morphological affixes –
prefixes and suffixes – in discovering lexical categories in

English. Subsequently we argue that, even though this
source of information is potentially available in the input,
children are not spoon-fed with a list of morphological
prefixes and suffixes. Despite this, there is evidence that
children do pay particular attention to the beginnings and
ends of words. Hence, a more psychologically plausible
mechanism is one that learns to categorize words based on
beginning and endings assuming no a priori knowledge of
morphology. This is not to discount the role of morphology,
which may become very useful at later stages of language
development. After assessing the usefulness of word
beginnings and endings in English, we test the robustness of
our simple model with a language that is similar to English
(Dutch), a language that has a richer morphological
affixation than English (French) and a language that has
different structural properties and does not belong to the
Indo-European family (Japanese).

Bootstrapping syntactic categories
There are three sources of information that children could
potentially bring to bear on solving the bootstrapping
problem: innate knowledge in the form of linguistic
universals (Pinker, 1984); language-external information
(Bowerman, 1973), concerning observed relationships
between language and the world; and language-internal
information, such as aspects of phonological, prosodic, and
distributional patterns that indicate the relation of various
parts of language to each other. Though not the only source
of information involved in language acquisition, we suggest
that language-internal information is fundamental to
bootstrapping the child into syntax. Computational models
are particularly apt at investigating language-internal
information because it is now possible to access large
computerized databases of infant-directed speech and
quantify the usefulness of given internal properties of a
language.
A hypothesis that is gaining ground in the field is
that substantial information may be present in the input to
the child in the form of probabilistic cues: several studies
have already assessed the usefulness of distributional,
phonological, and prosodic cues. Distributional cues refer to
the distribution of lexical items in the speech stream (e.g.
determiners typically precede nouns, but do not follow
them, the car/*car the; e.g. Monaghan, Chater, &
Christiansen, in press; Redington, Chater & Finch, 1998).
Phonological cues are also useful: adults are sensitive to the
fact that English disyllabic nouns tend to receive initialsyllable (trochaic) stress whereas disyllabic verbs tend to
receive final-syllable (iambic) stress and such information is
also present in child-directed speech (Monaghan et al. in
press). Prosodic information provides cues for word and
phrasal/clausal segmentation and may help uncover
syntactic structure (e.g. Gleitman & Wanner, 1982).
In this paper, we assess the usefulness of another
potential source of information, namely word beginnings
and endings. Morphological patterns across words may be
informative—e.g., English words that are observed to have
1679

both –ed and –s endings are likely to be verbs (Maratsos &
Chalkley, 1980). Children may also exploit prefix
information, although to our knowledge little work has been
done to assess the usefulness of this cue. Our experiments
are based on corpus analyses, to indicate the potential
information available in the environment for lexical
categorization. A computational system operating optimally
will pick up on such signals.

Experiment 1: Morphological cues in
grammatical categorization
Method
Corpus preparation. A corpus of child-directed speech
was derived from the CHILDES database (MacWhinney,
2000). We extracted all the speech by adults to children
from all the English corpora in the database, resulting in
5,436,855 words. The CHILDES database provides (with
the exception of only a fragment of the database) only
orthographic transcriptions of words, so we derived
phonological and lexical category for each word from the
CELEX database (Baayen, Pipenbrock, & Gulikers, 1995).
Words with alternative pronunciations and more than one
grammatical class (e.g. record can be a verb or a noun),
were assigned the most frequent pronunciation and word
class for each orthographic form. This contributes noise to
the analysis and provides the weakest test of the
contribution of these cues towards categorisation. We
considered the most frequent 4500 words in the CHILDES
database.
Cue derivation. A comprehensive list of English
orthographic prefixes and suffixes was compiled, resulting
in 248 prefixes and 63 suffixes. Among these, 58 prefixes
and 23 suffixes appeared at least once in our corpus.
Because some prefixes and suffixes can have more than one
phonetic realization (for instance, -ed is pronounced /d/ or
/t/), we obtained 62 phonetic prefixes and 37 phonetic
suffixes. Each word in the corpus was represented as a
vector containing (62+37) 99 units. If the word started and
ended with one of the affixes, then its relevant unit in the
vector was assigned a 1, otherwise it was 0. At the end of
the coding the whole corpus consisted of a list of 99-cue
vectors with most cues having value 0 and one or two
having value of 1. Importantly, we tested a situation in
which the model knows about affixes but knows nothing
about lexical categories. The model simply looks for
information of these affixes to assign a lexical category to
each word. For instance, -al as an adjectival suffix will
apply both to words like musical, natural, and to words like
sandal, metal.
To assess the extent to which word prefix and
suffix cues resulted in accurate classification, we performed
a multivariate linear discriminant analysis dividing words
into Nouns, Verbs, or Other. Discriminant analysis provides
a classification of items into categories based on a set of
independent variables. The chosen classification maximises
the correct classification of all members of the predicted

groups. Despite its seemingly statistical complexity,
discriminant analysis is a simple procedure that can be
approximated by simple learning devices such as two-layer
“perceptron” neural networks (Murtagh, 1992). In addition,
a baseline ‘control’ condition was established where the
lexical category labels for each word were randomly
reassigned to a different suffix vector. In line with our
second suggestion for psychological plausibility in the
Introduction, it is reasonable to assume that a young child
bootstrapping into language will not try to map every new
word into fine-grained lexical categories, but will probably
start with the most prominent ones. There is evidence that
children first learn nouns and verbs across languages
(Gentner, 1982). Hence, a classification between Nouns,
Verbs, and Other plausibly reflects the early stages of
lexical acquisition, with Other being an amalgamated
“super-category” incorporating all other lexical categories.
Experiment 1
baseline
80%

60%

40%

20%

0%
NOUNS

VERBS

OTHER

Figure 1. Percent correct classification of English Nouns,
Verbs, and Other using affix information.

Results
When all cues were entered simultaneously, 60.7% of crossvalidated words were classified correctly, which was highly
significant (Wilk’s Lambda=.675, χ2=1836.524, p<.001). In
particular, 76.9% of nouns, 54.4% of verbs, and 29% of
other words were correctly classified using morphological
cues. To test against chance levels, a discriminant analysis
was run on the baseline condition where the 4500 words
were randomly assigned to one of the three categories,
(respecting the size of each category). We obtained an
overall correct classification of 36.1%, which was not
significant (Wilk’s Lambda=.967; χ2=156.232; p=.987). In
particular, 49.2% of nouns, 7.8% of verbs, and 34.4% of
other words were correctly cross-classified (Figure 1). The
baseline classification was also significantly lower than the
morphological classification (χ2=571.518, p<.001). The
Other category is harder to classify because it encompasses
a very heterogeneous group of words, including open- and
closed-class words. Stepwise analyses were also conducted
to assess which cues are most useful in discriminating
nouns, verbs, and other classes, where a model of
discrimination is built step-by-step. At each step, all
variables are reviewed and evaluated to determine which
one will contribute most to the discrimination between
1680

groups. That variable will then be included in the model,
and the process starts again. Percent results obtained with
the stepwise method were very similar or identical to the
discriminant analyses reported above. Of the 99 cues
entered 20 were most useful in lexical categorization: -ing, ed, -y, -s, -er/-or, -(o)ry, -ite, -id, -ant, e-, -ite, -ate, un-, -ble,
-ive, an-, pre-, out-, bi-, -ine.

Experiment 2: A linguistically naïve analysis of
word beginnings and endings
Experiment 1 suggests that morphological suffixes are
potentially useful cues for discovering lexical categories.
However, the method used implies an already sophisticated
level of linguistic analysis where suffixes are pre-analyzed
units of the lexicon. Thus, one potential objection to these
analyses is that children are not spoon-fed a list of relevant
morphological suffixes. Another objection is that infants
cannot detect suffixes at 20 months (Santelman, Jusczyk &
Huber, 2003), which is the period immediately preceding
the vocabulary spurt. Therefore, a complete morphological
system may be developed at later stages and may not
directly assist in syntactic bootrstrapping. Where does this
leave us? Does it mean that the beginnings and endings of
words are useless cues?
By one year infants will have learned a great deal
about the sound structure of their native language (for
reviews see Jusczyk, 1997; Pallier, Christophe & Mehler,
1997). Thus, when they face the syntactic bootstrapping
problem at the beginning of their second year, they are
already well attuned to the phonological regularities of their
native language. In particular, infants and children are
highly sensitive to word endings (e.g., Slobin, 1973). Recent
experimental work in adult word learning also found a
primacy and recency facilitation effect: adults repeated the
beginning and end of nonwords more accurately than the
middle of words (Gupta, in press). Since nonwords are for
adults what new words are for children, a reasonable
assumption is that whatever sequencing mechanism is
responsible for word learning, it displays a learning bias for
the beginning and ending of words. We therefore developed
a simple procedure that children could plausibly use to
discover word-edge cues without prior knowledge of
morphology and tested its classification success. We chose
the phoneme as the unit of analysis because children can
distinguish between minimal pair words involving a single
phoneme change as early as 12 months of age (see Jusczyk,
1997). Hence, in evaluating the usefulness of this source of
information it would appear that, unlike affixation, it is
perceptually available to the child.

Method
Corpus preparation. The same corpus from Experiment 1
was used.
Cue derivation. We extracted all first and final phonemes
from the words in the corpus. By selecting the smallest
phonological unit, this procedure makes minimal
assumptions about the perceptual and processing capacities

of children. Our procedure resulted in 40 beginning and 40
ending phonemes with an attached frequency distribution. A
80-unit (40+40) vector was generated for each word as in
Experiment 1. The vectors were entered in a discriminant
analysis where the cues were the independent variables and
classification for Nouns, Verbs, and Other was estimated as
in Experiment 1.

Results

on different corpora of languages such as Dutch, French,
and Japanese.

Experiment 3: Dutch
We begin by extending our simple word-edge procedure to
Dutch, a language with structural properties similar to
English in many respects (for instance, it is a stress-based
language and has a similar morphology).

An overall 58.7% of cross-validated words were classified
correctly, which was highly significant (Wilk’s
Lambda=.683, χ2= 1787.730, p< .001). In particular, 70.5%
of nouns, 58.9% of verbs, and 30.6% of other words were
correctly classified using the first and last phoneme as word
class predictors (Figure 2). As in Experiment 1, a
discriminant analysis on the baseline condition yielded an
overall correct classification of 38.5%, which was not
significant (Wilk’s Lambda=.975; χ 2=120.206; p=.723).
48.1% of nouns, 22.7% of verbs, and 32.1% of other words
were correctly cross-classified. The baseline classification
was also significantly lower than the word-edge
classification (χ 2= 385.948, p<.001). In stepwise
discriminant analyses 26 out of 80 word-edge cues were
relevant for successful lexical categorization.

Method

Experiment 2

Results
baseline

80%

60%

40%

20%

0%
NOUNS

VERBS

OTHER

Figure 2. Percent correct classification of English Nouns,
Verbs, and Other using first and last phoneme information.
Experiments 1 and 2 have established the potential
usefulness of word beginnings and endings in bootstrapping
lexical categories in English. In particular, on the
assumption that word beginnings and endings are
phonologically salient features and perceptually available to
infants in their second year of life, Experiment 2 established
that a linguistically naïve learner with no prior knowledge of
morphological structure may start bootstrapping English
lexical categories. This is particularly striking given that
several sounds are ambiguous (/s/ in English signals the this
person singular of a verb as well as the plural of countable
nouns), and that several sounds entered as cues do no carry
any specific morphological meaning (e.g. beginning /h/ was
the 11th cue entered in order of importance in the stepwise
analysis, although it does not correspond to any
morphological prefix in English). In the following
experiments, we assess the robustness of our simple model
1681

Corpus preparation. Child-directed speech from the Dutch
subcorpus of CHILDES was extracted and the 5000 most
frequent words were assigned a phonological representation
and a lexical category using the CELEX database. Words
belonging to more than one lexical category were assigned
the most frequent category.
Cue derivation. Given the good classification results
obtained with the naïve procedure in Experiment 2, the
same procedure as in Experiments 2 extracted 37 beginning
phonemes and 27 ending phonemes. Each word in the
corpus was turned into a 64-unit (37+27) vector and entered
into a discriminant analysis. The 37+27 beginnings and
endings were used as predictors in a three-way lexical
category classification (Nouns, Verbs, Other).

An overall 54.0% of cross-validated words were classified
correctly (Figure 3), which was highly significant (Wilk’s
Lambda=.707, χ2= 1725.088, p<.001). In particular, 49.3%
of nouns, 76.2% of verbs, and 42.6% of other words were
correctly classified using the first and last phoneme as word
class predictors. A discriminant analysis on the baseline
condition yielded an overall correct classification of 30.0%,
which was not significant (Wilk’s Lambda=.974;
χ2=128.393; p=.251). In particular, 26% of nouns, 28.9% of
verbs, and 41.9% of other words were correctly crossclassified. The baseline classification was also significantly
lower than the word-edge classification (χ 2= 547.953, p <
.001). Stepwise discriminant analyses revealed that 29 out
of the 64 cues were relevant for successful lexical
categorization.

Experiment 4: French
For our analyses, French is particularly interesting because
of its rich morphological system and because many word
endings are highly ambiguous (e.g., f a i t=noun/verb,
fais=verb, mais=preposition, lait=noun, all end with the
same sound).

Method
Corpus preparation. Child-directed speech from the
French subcorpus of CHILDES was extracted and its 3000
most frequent words were assigned a phonological
representation and a lexical category using the LEXIQUE
database (New, Pallier, Ferrand, & Matos, 2001). In case of

multiple categories (e.g. fait=noun,verb) the most frequent
one was assigned as in previous experiments.
Cue derivation. The same procedure extracting the first and
last phoneme of each word in the corpus was adopted,
resulting in 37 beginnings and 36 endings. Each word was
transformed into a 73-unit (37+36) vector, and entered in a
discriminant analysis where the 73 cues were used as
predictors of a three-way lexical category classification
(Nouns, Verbs, Other).

class predictors. A discriminant analysis on the baseline
condition yielded an overall correct classification of 33.8%,
which was not significant (Wilk’s Lambda=.934;
χ2=66.410; p=.664). Only 30.4% of nouns, 25% of verbs,
and 44.7% of other words correctly cross-classified. The
baseline classification was also significantly lower than the
word-edge classification (χ2=64.041, p<.001).
Experiments 3,4,5
baseline

Results

80%

An overall 53.9% of cross-validated words were classified
correctly (Figure 3), which was highly significant (Wilk’s
Lambda= .680, χ 2= 1142.593, p<.001). In particular, 52.6%
of nouns, 57.8% of verbs, and 49.7% of other words were
correctly classified using the first and last phoneme as word
class predictors. A discriminant analysis on the baseline
condition yielded an overall correct classification of 36.2%,
which was not significant (Wilk’s Lambda=.949;
χ2=154.192; p=.229). Only 35.7% of nouns, 36.8% of verbs,
and 36.3% of other words were correctly cross-classified.
The baseline classification was also significantly lower than
the word-edge classification (χ 2=405.935, p<.001).
Stepwise discriminant analyses revealed that 33 of the 73
cues were relevant for successful lexical categorization.

Experiment 5: Japanese
Our last extension of the simple word-edge procedure
applied to a non Indo-European language very dissimilar to
English, Dutch, and French. This will allow us to test the
potential robustness of our learning procedure across a
varied typology of languages.

Method
Corpus preparation. Child-directed speech from the
Japanese subcorpus of CHILDES was extracted and the
1000 most frequent words were assigned a phonological
representation and a lexical category using the
CALLHOME corpus (Canavan & Zipperlen, 1996), with
hand-coding for the most frequent 1000 words by a native
Japanese speaker. Words belonging to more than one lexical
category were assigned the most frequent category.
Cue derivation. The same procedure used in Experiments
2-4 extracted 29 beginning phonemes and 9 ending
phonemes. Each word in the corpus was turned into a 38unit (29+9) vector and entered into a discriminant analysis.
As in the previous experiments, the 38 beginnings and
endings were used as predictors in a three-way lexical
category classification (Nouns, Verbs, Other).

Results
An overall 51.5% of cross-validated words were classified
correctly (Figure 3), which was highly significant (Wilk’s
Lambda=.703, χ2=345.824, p<.001). In particular, 49% of
nouns, 64.1% of verbs, and 44.2% of other words were
correctly classified using the first and last phoneme as word
1682

60%

40%

20%

0%
DUTCH

FRENCH

JAPANESE

Figure 3. Overall percent correct classification of Nouns,
Verbs, and Other using word-edge information in Dutch,
French, and Japanese.

General Discussion
In language acquisition research, a hypothesis is gaining
ground that children may exploit various sources of lowlevel information available in the raw input to start
bootstrapping important structural linguistic relations such
as discovering lexical categories. Because most sources of
information are probabilistic, it is further hypothesized that
the child must integrate them together using simple learning
mechanisms. Although the potential importance of word
beginnings and endings was suggested (Maratsos &
Chalkley, 1980), no empirical study has assessed their
usefulness in learning syntactic categories, and we decided
to make a quantitative estimate based on corpora of childdirected speech of English, French, Dutch, and Japanese.
In this paper, we also made the theoretical
suggestion that for computational models of language
acquisition to be theoretically viable they should be
psychologically plausible. We have proposed three
benchmarks for psychological plausibility. Firstly, the
model should assume that language knowledge is
constructed progressively and that some linguistic
categorizations are learned before others. We considered
that the bootstrapping problem may be solved not by
developing an immediate fine-grained category distinction,
but rather start with distinguishing those categories that
children learn first, namely nouns and verbs. For this reason
our analyses involved a coarse distinction between nouns,
verbs, and other words where ‘other’ was a category on its
own that would be split into finer-grained categories such as
adjectives, determiners, etc. at a later stage. The second
benchmark for psychological plausibility is that minimal
assumptions should be made about the linguistic units
available to the child and their perceptual relevance at the

given age of analysis. After assessing the usefulness of
linguistically-defined morphological affixes in Experiment
1, we showed that the same good categorization results
would obtain by a naïve learner that simply focused on the
first and last phoneme of a word, a plausible assumption
because by their second year of life infants develop a
striking sensitivity to the sound patterns of their language
and also a sensitivity to word endings (Slobin, 1973).
To respect minimal assumptions about processing
capacities of children, we chose the phoneme unit (the
processing window may be limited to one phoneme in very
early stages, or other cognitive restrictions may apply).
There is also evidence that speakers of “stress-timed”
languages such as English and Dutch show greater access to
the phoneme (e.g. Cutler, Mehler, Norris, & Segui, 1986). It
may be that children are sensitive to other word beginning
and ending units larger than the phoneme. For instance,
speakers of “syllable-timed” languages (e.g., French, Italian,
Spanish, Catalan, & Portuguese) show a processing
advantage for the syllable (e.g. Sebastián-Gallés, Dupoux,
Segui, & Mehler, 1992), and Japanese adults use the mora
as the primary unit of segmentation (Otake, Hatano, Cutler,
& Mehler, 1993). In additional analyses not reported here,
we obtained good classification results with our model when
the first and last syllables were entered as predictors in
French and the first and last morae were entered as
predictors in Japanese. These results comply with our third
benchmark of cross-linguistic robustness, and suggest that
our learning mechanism may be viable for language
acquisition across the diversity of languages.
We conclude that simple computational principles
can be quite powerful even in isolation, although a complete
account of language acquisition will require a combination
of many simple computational principles for the detection
and integration of multiple sources of probabilistic
information.

Acknowledgments
Support comes from Human Frontiers Science Program
grant RGP0177/2001-B to MHC.

References
Baayen, R.H., Pipenbrock, R. & Gulikers, L. (1995). The
CELEX Lexical Database (CD-ROM). Linguistic Data
Consortium, University of Pennsylvania, Philadelphia,
PA.
Bowerman, M. (1973). Structural relationships in children’s
utterances: Syntactic or semantic? In T. Moore (Ed.),
Cognitive Development and the Acquisition of Language.
Cambridge, MA: Harvard University Press.
Broeder, P., and Murre, J. (2003). Models of language
acquisition: Inductive and deductive approaches. Oxford:
Oxford University Press.
Canavan, A., & Zipperlen, G. (1996). C A L L H O M E
Japanese Speech. Linguistic Data Consortium, University
of Pennsylvania.
Cutler, A., Mehler, J., Norris, D., & Segui, J. (1986). The

1683

syllable's differing role in the segmentation of French and
English. Journal of Memory and Language, 25, 385-400.
Gentner, D. (1982). Why nouns are learned before verbs:
Linguistic relativity versus natural partitioning. In S.
Kuczaj (Ed.) Language development, vol.2. Hillsdale, NJ:
Erlbaum.
Gleitman, L., and Wanner, E. (1982). Language acquisition:
the state of the art. In Gleitman, L. and E. Wanner. (Eds.)
Language acquisition: The state of the art. 3-48. New
York : Cambridge University Press.
Gupta, P. (in press). Primacy and Recency in Nonword
Repetition. Memory.
Jusczyk, P.W. (1997). The discovery of spoken language.
Cambridge, MA: MIT Press.
Landauer, T. K. and Dumais, S. T. A solution to Plato's
problem: The Latent Semantic Analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review, 1997, 104(2), 211-240.
MacWhinney, B. (2000). The CHILDES project: Tools for
analyzing talk (3rd ed.). Mahwah, NJ: Lawrence Erlbaum
Associates.
Maratsos, M. & Chalkley, M. (1980). The internal language
of children’s syntax. In K.E. Nelson (Ed.), Children’s
language (Vol. 2). New York: Gardner Press.
Monaghan, P., Chater, N., & Christiansen, M.H. (in press).
The differential contribution of phonological and
distributional cues in grammatical categorization.
Cognition.
Nelson, K. (1973). Structure and strategy in learning to talk.
Monographs of the Society for Research in Child
Development, 38, 149.
Murtagh, F. (1992). The multilayer perceptron for
discriminant analysis: two examples. In M. Schader (ed.),
Analyzing and Modeling Data and Knowledge, SpringerVerlag, 305-314, 1992.
New, B., Pallier, C., Ferrand, L., & Matos, R. (2001). Une
base de données lexicales du français contemporain sur
internet: LEXIQUE. L’Année Psychologique, 101, 447462.
Otake, T., Hatano, G., Cutler, A., & Mehler, J. (1993). Mora
or syllable? Speech segmentation in Japanese. Journal of
Memory and Language, 32, 258-278.
Pallier, C., Christophe, A., & Mehler, J. (1997). Languagespecific listening. Trends in Cognitive Science, 1(4), 129132.
Pinker, S. (1984). Language learnability and language
development. Cambridge, MA: MIT Press.
Redington, M., Chater, N. & Finch, S. (1998). Distributional
information: A powerful cue for acquiring syntactic
categories. Cognitive Science, 22, 425-469.
Santelmann, L., Jusczyk, P., & Huber, M. (2003). Infants
Attention to Affixes. In D. Houston, A. Seidl, G..Hollich,
E. Johnson, & A. Jusczyk (Eds.) Jusczyk Lab Report.
Sebastián-Gallés, N., Dupoux, E., Segui, J., & Mehler, J.
(1992). Contrasting syllabic effects in Catalan and
Spanish. Journal of Memory and Language, 31, 18-32.
Slobin, D.I. (1973). Cognitive prerequisites for the
development of grammar. In C.A. Ferguson & D.I. Slobin
(Eds.), Studies of child language development. New York:
Holt, Reinhart & Winston.

