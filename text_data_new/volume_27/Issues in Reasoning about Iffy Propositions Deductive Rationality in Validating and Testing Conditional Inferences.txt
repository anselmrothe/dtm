UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Issues in Reasoning about Iffy Propositions: Deductive Rationality in Validating and Testing
Conditional Inferences

Permalink
https://escholarship.org/uc/item/8rm5k6gx

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)

Author
Schroyens, Walter J

Publication Date
2005-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Issues in Reasoning about Iffy propositions:
Deductive Rationality in Validating and Testing Conditional Inferences
Walter J. Schroyens (Walter.Schroyens@hec.ca)

HEC Montréal, Université de Montréal.
3000, Chemin de la Côte-Sainte-Catherine, Montréal, Quebec, Canada H3T 2A7
Abstract
We asked people to validate conditional inferences. The results
show, first, that people are more likely to look for a falsification
versus confirmation. Second, falsification rates are lower for
logically valid versus invalid inferences. Deductively valid
inferences are inferences that follow necessarily. Our
Experiment (N = 96) shows that emphasising this logicality
constraint increases falsification rates and corroborates that
people who test an inference by considering counterexamples
are more likely to make logically correct evaluations. The
results are discussed in relation to the mental models theory of
human reasoning.

General Introduction
Our beliefs constitute hypotheses about the world we live in,
the creatures in it and our interactions with them and each
other. Such beliefs ground our expectations in virtue of the
inferences derivable from them. Forming and testing
hypotheses is thus of capital interest in a changing and
uncertain environment that we try to capture in our belief
system. Many such beliefs reflect conditional relationships.
Reasoning about conditionals accordingly attracted the
interest of cognitive scientists of all backgrounds (Edgington,
1995; Eiter &Lukasiewicz, 2000; Johnson-Laird & Byrne,
1991, 2002; Levinson, 2000; Markovits & Barrouillet, 2002;
Oaksford et al., 2000; Rips, 1994).
There exists abundant evidence that people take count of
counterexamples when reasoning about conditional
relationships (e.g., Cummins, Lubart, Alksnis, & Rist, 1991;
De Neys, Schaeken, & d’Ydewalle, 2003). All these studies,
however have been conducted with knowledge-rich materials.
These materials, mostly causal conditionals, are rich in
associated background knowledge and have been pre-tested
and classified as yielding few or many counterexamples. The
results typically show that when there are more
counterexamples, people are generally less certain about the
conclusion countered by the counterexample.
It is generally assumed that there is an automatic activation
of background knowledge associated with the content of the
premises. Theories that address commonsense reasoning
about knowledge-rich conditionals proffer a largely passive,
data-driven “search” for counterexamples (see, e.g., Oaksford
& Chater, 1998). This stands in contrast with theories of
deductive reasoning that posit an active, theory-driven search
for counterexamples. Such theories have mostly focused on
explaining performance on knowledge-lean deductive
inference problems. For instance:
(1) If letter is an A then the number is a 2.
The number is 2; hence the letter is an A.
Performance on such reasoning problems is consistent with
1961

the idea that people engage in an active search for
counterexample with the aim of testing putative inferences
that can be drawn from an initial treatment of the inferential
problem (see, e.g., Johnson-Laird & Byrne, 1991; Schroyens
et al., 2001). Being able to explain performance on problems
such as (1), however, provides only indirect and inconclusive
evidence for an active, theory-driven search for
counterexamples. We therefore set out to provide more direct,
converging evidence.
But why would people engage in a critical thinking
exercise in the first place? A counterexample search has the
interesting implication that it allows people to make logically
valid inferences. These are arguments whose conclusion must
necessarily be true, given that the premises are true. In case
there is an acceptable alternative to the conclusion, (which
means that the conclusion is possibly false), we know that the
conclusion is not necessarily true. In case there is no such
alternative, we know that the conclusion follows necessarily.
Consider the following logically valid Modus Ponens (MP)
argument.
(1) If <A>, then <C>.
<A>, therefore <C>.
Few people will have difficulty realizing that the conclusion
<C> is falsified by <not-C>. That is, the counterexample to
MP is formed by the contingency between <A> and <NOTC>. At the same time it is also a well-established
phenomenon that people judge the <A NOT-C> contingency
to be unacceptable if the conditional utterance were true (see,
e.g., Barrouillet & Lecas, 1998). That is, the counterexample
to MP is not an acceptable alternative. This means that the
conclusion cannot be false, given that the premises are true.
The conclusion follows necessarily: one cannot deny the
conclusion without contradicting at least one of the premises.
The ability to establish logical validity is an important
property (Stanovich, 2004). Indeed, one of the most robust
findings in the deductive reasoning literature is that people
are more likely to endorse valid versus invalid inferences.
Searching for alternatives seems to be the obvious way to
test one’s inferences. What else could one do? We think that
this depends on the question one asks and hence the task one
sets oneself in testing an inference. One tests an inference
because one is uncertain about it. The question, however, is
‘uncertain’ in what way, about what? One can be uncertain
about whether the conclusion is possibly correct, or one can
question whether it is necessarily correct. People often just
want to reach a plausible conclusion, not one that is logically
valid. In daily life a plausible conclusion can already provide
sufficient grounds to act upon, and it would indeed be
adaptively irrational (Anderson, 1990) if one were to act only
upon necessary believes. When we are not even certain that
something might be possibly true, it seems as yet precipitous

to ask whether this state of affairs would necessarily be the
case.
Uncertainty about possibilities seems particularly prevalent
when we are reasoning about unknown contents and cannot
rely on our semantic memory to establish whether an
envisaged contingency is really possible. Instead of searching
for alternatives that counter the initial conclusion, one might
opt to check at first whether this initial possibility is a true,
factual possibility. Consider, for instance, the following
argument:
(3) If the figure is a triangle, then it is colored blue.
The figure is not a triangle.
Thus, the figure is not colored blue.
Before checking whether there might be non-triangular
figures that are colored blue (as a counterexample to the
putative conclusion that the non-triangular figure is not blue),
we might opt to check/verify whether there are in fact nontriangular figures that are not blue.
In summary, to test whether an inference is possibly true
one can attempt to confirm it: the hypothetical possibility is
established as a factual possibility. To test an inference one
might also check whether it is necessarily true: the
hypothetical possibility is established as (im)possibly false
(i.e. it is a hypothetical necessity). When the putative
inference cannot be false, it must necessarily be true. When
the putative inference can be false it is not necessarily true.
The envisaged import of the present paper is twofold. First,
we explore the prevalence of confirmation/falsification
strategies. Second, by using abstract materials and a novel
inference-validation task we query whether people can, and
sometimes do, engage in an active goal-directed or theorydriven search for counterexamples.

Experiment
To study test procedures we decided to run a study wherein
people were confronted both with the standard inferenceevaluation task, as well as a novel reasoning task we
constructed to probe for test procedures. The four basic
inference problems are formed by an affirmation or denial of
the antecedent (A) or consequent (C):

confirmatory contingency confirms both the categorical
premise and the conclusion: one checks whether the
conclusion is possibly true. The falsifying contingency also
confirms the categorical premise but falsifies/counters the
conclusion: one checks whether the conclusion is possibly
false. It then is a straightforward matter to determine the
confirmatory or falsifying contingency for the four
conditional arguments: MP, MT, AC, and DA.
Consider <if A then C>. TT,<A–C> confirms both MP
(“A, therefore C”) and AC (“C therefore A”). The
confirmatory selection for DA (“not-A, therefore not-C”) and
MT (“not-C therefore not-A”) is captured by FF,<not-A –
not-C>. What about the falsifying contingency? First,
FT,<not-A–C> is the falsifying contingency for the logically
invalid inferences (AC/DA). Second, TF,<A– not-C> cases
falsify the logically valid inferences (MP/MT).
We suggested that the goal to falsify or confirm putative
inferences depends on whether one aims to establish that the
conclusion follows possibly or necessarily. We therefore
decided to manipulate the task instructions. Some of the
reasoners were asked to select the contingency they would
need to check to test whether “the conclusion follows”: a
weak-necessity context. Others were asked test whether “the
conclusion follows necessarily” (henceforth coined the
standard-necessity condition). In a more stringent context we
asked to test whether “the conclusion follows necessarily and
not just possibly” (strong-necessity). The weak-necessity
condition imposes the least possible constraint. The strongnecessity condition stresses the need to check the logical
validity of the conclusion. As such we can expect that there
will be an increase in the falsifications.
In summary: first, since falsification is explicitly assumed
to be the dominant strategy in mental model theory, we
expected to observe exactly this dominance of falsification
over confirmation. Second, falsification is expected to
increase as a function of the impetus placed on the constraint
that logically valid inferences are inferences that follow
necessarily and not just possibly.

MP if A then C, A; therefore C
AC if A then C, C; therefore A
DA if A then C, not-A; therefore not-C
MT if A then C, not-C; therefore not-A

Method

The arguments are classically being referred to as Modus
Ponens (MP), Affirmation of the Consequent (AC) Denial of
the Antecedent (DA) and Modus Tollens (MT). In the
inference-evaluation task people are to evaluate the
conclusion (does it follow?). The inference-validation task
asks people how they would test the inferences. They can do
this by selecting the contingency they would need to check.
There are only four possible contingencies, co-occurrences
between events wherein the antecedent and consequent does
or does not hold (is True “T” or false “F”): In relation to a
conditional of the form <if A then C> these correspond to:
TT: A and C TF: A and not-C
FT: not-A and C
FF: not-A and not-C
The status of a contingency as one that falsifies/confirms an
inference depends on the argument one is testing. The

Design. Participants solved both an inference-validation
task and an inference-evaluation task for each of the four
logically valid (MP/MT) or invalid (AC/DA) affirmation
(MP/AC) or denial problems (MT/DA). Participants were
assigned to one of three groups: the Weak-, Standard- or
Strong-Necessity group (see Materials and Procedure).
Participants. One-hundred sixteen first-year psychology
undergraduates at the University of Leuven participated. They
were randomly allocated to one of three groups: 39, 37 and 40
in respectively the ‘Weak-necessity’, ‘Standard-Necessity’
and ‘Strong-Necessity’ group.
Materials and procedure. The arguments were presented in
the following format
Rule: If the figure is a CIRCLE, then it is colored RED.
Fact: The figure is NOT a CIRCLE
Conclusion: Hence, the figure is NOT colored RED.

1962

All arguments concerned the same rule and participants were
told to assume that the rule and given fact were true. The

‘Strong-Necessity’ instruction introduced
Table 1: Relative frequency by which the truth contingencies were selected to
the validation task as follows:
test the validity of the four standard conditional inferences.
“Your task is to determine whether the
conclusion follows necessarily (and not just
Affirmation problems
Denial Problems
possibly) from the given information (the
Necessity
Necessity
rule and the given fact). This is why we first
Weak Std Strong Mean
Weak Std Strong Mean
ask you to indicate about which states of
Logically valid inferences.
affairs (figures with a particular shape, or
MP
MT
not, and a particular color, or not) you
None 18.9
27.6
20.0
21.9
16.2
13.8
16.7
15.6
would need to know whether they are
TT
18.9
17.2
10.0
15.6
0
3.4
6.7
3.1
possible or not in order for you to be able to
TF
43.2
41.4
60.0
47.9
37.8
51.7
56.6
47.9
decide whether the conclusion follows not
FT
16.2
13.8
10.0
13.5
24.3
13.8
6.7
15.6
just possibly but necessarily from the given
FF
2.7
0
0
0.0
1.0
21.6
17.2
13.3
17.7
information. A conclusion that follows
Logically Invalid inferences.
possibly is a conclusion that ‘can be true’,
AC
DA
which obviously is not the same as a
None
0
0
0
0
0
0
0
0
conclusion that is necessarily correct (‘has
TT
8.1
6.9
2.9
6.3
2.7
3.4
0
2.1
to be true’).” These instructions are open as
TF
10.8
0
2.9
5.2
8.1
3.4
0
4.2
regards the task of choosing the crucial
FT
75.7
89.7
88.2
83.3
73.0
79.3
83.3
78.1
contingency in order to determine the truth
FF
5.4
3.4
5.8
5.2
16.2
13.8
16.7
15.6
value of a given conclusion or the task of
identifying which contingencies allow one Note. Confirmatory contingencies are italicised, falsifications are underlined.
to establish the necessity degree of the
resulted in 37, 29 and 30 participants in the Weak-Necessity,
conclusion. Indeed, these tasks are conceptually related and
Standard-Necessity and Strong-Necessity group.
the latter implies the former.
The two other groups received ‘Standard Necessity’ or
Inference Validation. Table 1 presents the selected
‘Weak necessity’ instructions: “Your task is to determine
contingencies as a function of the level of necessity. The data
whether the conclusion follows [necessarily] from the given
show that falsification is the dominant strategy. Chance level
information ... to decide whether the conclusion follows
performance of falsifications would be .20 (1/5), whereas the
[necessarily] from the given information”. Reference to
mean falsification rate averaged .643 (χ² = 122.6, p < .001).
“necessity” was left out in the weak-necessity instructions.
As a function of the logical validity of the argument there is a
Strength of necessity was mirrored in the validation task:
clear difference in the likelihood of selecting the falsifying
One would need to know whether it is possible or not
contingency. People are more likely to falsify the invalid
that there are:
AC/DA inferences than the valid MP/MT inferences (.479 vs.
O CIRCULAR figures that are RED.
.807: Wilcoxon T = 112.0, N = 54, Z = 5.428, p < .001).
O NON-CIRCULAR figures that are RED
Overall, the type of problem (affirmation vs. denial) did not
O CIRCULAR figures that are NOT RED.
affect falsification rates (.656 vs. .630). When taking the
O NON-CIRCULAR figures that are NOT RED.
proportion of confirmatory selections as the dependent
to be able to decide that the conclusion follows not just
measure, problem type yielded an increase in the level of
possibly, but necessarily.
confirmations of denial (vs. affirmation) inferences (.109 vs.
.167; T = 72.0, N = 23, Z = 2.007, p < .05).
In the standard-necessity context the final sentence read “... to
The main effect of logically validity shows that a
be able to decide that the conclusion follows necessarily”
counterexample search cannot be fully disentangled from the
(versus “follows” in the Weak-necessity group). The
plausibility of these alternatives. Some people seem to select
inference-evaluation task also reflected the different levels of
a positive test because they expect that a negative test would
necessity, both in the question asked and the response options
be rejected. TF,<A not-C> falsifies the logically valid
given.
inferences. However, this contingency is impossible when
The experiment was run in groups of 20 to 40 participants.
one assumes that the conditional is true. If it were always true
When entering the auditorium they were given a booklet
that “if I cut my finger, then it bleeds’, then it would be
containing the instructions and one page per argument
impossible that my finger does not bleed when cutting it.
(presented randomly). For each argument participants first
People recognize this and actually do reject TF (see Evans et
completed the validation-task and only then evaluated the
al., 1996). Schroyens et al. (2001) already concluded that it is
arguments.
“inappropriate to adhere to a strict serial processing sequence
in which the process of constructing a hypothetically
Results and Discussion
falsifying model is primordial and independent of its
In the following we first present the results on the validationplausibility ... ” (p. 156).
task, next we present the evaluation-task and the relation
Stressing the constraint that logically valid inferences are
between the two tasks. Some participants selected more than
inferences
that follow necessarily (and not just possibly)
one validating contingency. The analyses included only those
produced
the
expected increase in falsification rates (Median
participants who selected one or no contingency. This
1963

Table 2: Inference acceptance rates as a function of whether a falsifying
or confirmatory contingency was selected in the validation task.
Logically Valid
Affirmation Denial

Logically Invalid
Affirmation
Denial

Contingency

n

MP

n MT

n

AC

Falsifying
Confirmatory

46 91.3
15 93.3

46 69.6
17 64.7

80 15.0
6 50.0

Other

35 91.4

33 72.7

10 50.0

n

DA

75 13.3
15 40.0

Test χ² = 7.616, df = 2, p = .024). The specific effects turned
out to be relatively weak. The Strong-Necessity group
showed a minor tendency to more frequently select the
falsifying contingency as compared to the Standard-Necessity
group (.717 vs. .655: Mann-Whitney U30,29 = 377.0, Z = .920,
ns), which did not exhibit significantly higher falsification
rates as compared to the Weak-Necessity group (.574, U29,37 =
461, Z = 1.011, ns). Only the comparison between the two
extreme necessity groups (weak vs. strong) reached statistical
significance (.717 vs. .574; U30,29 = 400.5, Z = 2.022, p < .05).
Inference Evaluation. Table 2 presents the proportions of
accepted arguments as a function of whether the participants
had previously made a falsifying, a confirmatory or another
selection. But let us first briefly consider the overall
acceptance rates. They are in line with the general pattern of
results in reasoning about knowledge-lean conditionals (see
Schroyens et al., 2001, for meta-analyses). That is, first, there
was a main effect of inference type (affirmation vs. denial;
.562 vs. .463: T = 180.0, N = 37, Z = 2.580, p < .05), a main
effect of logical validity (.807 vs. .219; T = 54.0, N = 77 , Z =
7.349, p < .05) and an interaction between these two factors
(T = 218.5, N = 41, Z = 2.747, p < .05).
Given that participants evaluated the inferences after
having completed the validation task it is to be expected that
the acceptance rates of the logical fallacies (AC/DA) are
lower than the level at which the fallacies are generally
accepted. Indeed, there is not a single theory that assumes
everybody spontaneously engages him/herself in a validating
search for counterexamples. Obviously, when we explicitly
instruct people to test their inferences, we can be sure that
more people do test their inferences. Our results confirm that
falsification is the dominant testing strategy. We can therefore
expect that more people than in non-instructed conditions
search for alternatives.
We noted that a counterexample search allows for
establishing logical validity. The rejection level of invalid
inferences should consequently be more prominent when
more people engage in a counterexample search. This is
exactly what we observed. The relation between rejecting
invalid inferences and a search for alternatives is further
corroborated when we look at the inference-acceptance rates
of people who search for alternatives and those who do not.
Table 2 shows that participants who select the falsifying
contingency (vs. those who do not) are much more likely to
reject both the invalid AC (15 .0 vs. 50.0; U80,16 = 416.0, Z =
3.130, p < .05) and DA inferences (13 .0 vs. 57.1; U80,16 =

442.5, Z = 4.199, p < .05). Since the alternatives
to the valid inferences are not consistent with the
conditional – and people in fact generally
evaluate them as such (see, e.g., Barrouillet &
Lecas, 1998) – considering these alternatives is
generally of little consequence for the evaluation
of the logical status of the valid inferences.

General Discussion

In the novel inference-validation task people
are
asked to test arguments by selecting test
6 100
cases. Our study showed, first, that people are
more likely to look for a falsification versus
confirmation of conditional inferences. Second, falsification
rates are generally lower for the logically valid (vs. invalid)
inferences. Third, confirmation-rates are generally higher for
denial inferences. The study also showed that increasing the
impetus on this logicality constraint increases the tendency to
look for falsifications. It also confirmed that people who test
an inference by searching for an alternative are more likely to
make a logically appropriate evaluation of these inferences.
In the following we discuss the findings relating the mental
models theory of human reasoning. Before doing so,
however, we need to address two issues. First, one might
suggest that there is nothing new to our findings. It is just
another strand of evidence in favor of the idea that people
look for counterexamples. Second, another critique (which
stands in paradoxical contrast with the previous one) is that
our evidence clashes with the results of other studies, which
would have shown no evidence for a counterexample search.
We will briefly consider these critiques.
An important difference with previous studies is the use of
knowledge-lean inference problems. These problems express
arbitrary relations between antecedent and consequent events.
This property undermines the argument that, as an
anonymous reviewer stated it, “it is thus no surprise that these
counterexamples are chosen when presented since we already
know that they are generated as part of the normal processing
of conditionals”. In the case of knowledge-lean conditionals
it seems however implausible that the alternative and/or
disabling conditions would be generated “as part of the
normal processing of conditionals”. There exist in effect no
such specific alternative or disabling conditions.
The results provide a perhaps even more compelling
argument for a theory-driven counterexample search. Indeed,
we showed that people are more likely to select
counterexamples when the presumed goal of searching for
counterexamples (validation-by-falsification) is rendered
more or less explicit by the contextual task demands. Why
would people by less likely to generate alternative and/or
disabling conditions, if generating such contingencies is not
part of a falsification strategy and is only part of the normal
processing of conditionals? We observed that people are more
likely to select a counterexample when it was indeed made
clear to them that they had to test whether the conclusions
follow necessarily and not just possibly. A purely data-driven,
bottom-up search for counterexamples cannot account for this
demonstrated top-down import of the goals that are satisfied
by generating counterexamples.
Other studies on human reasoning have also investigated

1964

the assumption that people will look for counterexamples in
the context of reasoning about knowledge-lean inference
problems (Evans, Handley, Harper, & Johnson-Laird, 1999;
Newstead, Handley, & Buck, 1999). These studies have been
taken to suggest that there is no evidence for a search for
counterexamples: people tend to satisfice. First, one potential
problem with this line of research is that it concerns
syllogistic reasoning. Syllogism (e.g. “none of the A’s are
B’s; Some of the C’as are B’s; therefore...?) are generally
more difficult then conditional arguments. A goal-directed
search for counterexamples is presumed to take time and
effort. Given that we have limited processing resources, it is
therefore not surprising that the syllogistic reasoning studies
provide less evidence for a goal-directed counterexample
search. It is our contention that we are indeed talking about
smaller effect sizes, and that care must be taken for the classic
problem of accepting the null hypothesis. Evans, Handley,
Harper’s (2001) own data suggest that making the categorical
claim that people will not look for counterexamples is an
overstatement that lacks the requisite nuance and adherence
to the complexity of the observations. Proper meta-analyses
are in order to settle such issues.
Conditional reasoning by model The mental model theory
is the one theory most explicit in positing a search for
alternatives. “Perhaps the most distinctive feature of mental
models theory is the claim that people search for
counterexamples” (Chater & Oaksford, 2001, p. 200). In all
models of conditional reasoning by model (Markovits &
Barrouillet, 2002; Schroyens & Schaeken, 2003) it is assumed
that (at least some) people will (at least sometimes) search for
counterexamples.
Many theorists still seem to identify the mental models
theory with the algorithmic-level specification of JohnsonLaird and Byrne (1991). Let us therefore first consider this
model, which is discredited in a very straightforward manner.
The “fleshing out”-model of Johnson-Laird and Byrne (1991)
proposes a specific mechanism by which people search for
alternatives. We cannot here explicate the details of the
fleshing out procedure. The crux of the argument is that
TF<A – not-C> is never considered or represented as a
hypothetical possibility that might falsify the logically valid
inferences. This means that the fleshing-out model does not
capture the observation that people will consider this
hypothetical possibility that falsifies the logically valid
inferences.
The failure of Johnson-Laird and Byrne’s (1991) fleshingout model is not problematical for the mental models theory.
Though privileged in some ways, Johnson-Laird et al.’s
specification of the theory’s general principles is just one of
many alternative specifications/models. SSCEPPTRE: a
“Syntactic-Semantic Counterexample Prompted Probabilistic
Thinking and Reasoning Engine” (Schroyens & Schaeken,
2003), for instance, adheres to the general processing
principles of reasoning by model but provides an alternative
specification of the principle that people will look for
alternative models that might falsify their initial conclusions.
In SSCEPPTRE the non-compulsory search for
counterexamples is instantiated by a goal-directed search.
People will first construct a hypothetical model that would

positively falsify the conclusion. Only then is this model’s
acceptability (and/or probability) evaluated. The hypothetical
counterexample is formed by considering the falsity of the
conclusion in combination with the truth of the categorical
premise that served as the basis to infer the conclusion in the
first place. Consider, for instance, <C> from <A> (i.e, MP).
The counterexample to this argument is consequently
captured by the contingency between <A> and <not-C>. The
present study was actually set up to test expectations derived
from SSCEPPTRE. The high prevalence of selecting
counterexample corroborates SSCEPPTRE.
Schroyens et al. (2001) identified three conceptually
distinct factors as having an import in the process of testing
the inferences. First, people must be motivated to search for
counterexamples. Second, even if they are motivated to
search for counterexamples, they must be able to construct
them. Third, once these hypothetical models are constructed,
they must be evaluated (accepted/rejected as more or less
plausible and/or acceptable). Johnson-Laird and Byrne (1991,
p. 206) already stated that “the model theory allows that the
inferential process itself can be influenced by motives: the
search for alternatives that refute a putative conclusion can be
thorough or cursory depending on motivational factors”. Our
study provides direct evidence for a contextual/motivational
effect. When an increased impetus is placed on deduction,
people are more likely to query possible alternatives (also see,
Schroyens et al., 2003). That is, deductive rationality
increases when such reasoning/rationality becomes more
important. Indeed, when asked to do so, it is adaptively
adequate behavior (and one might say, in this sense,
‘rational’) to (try to) reason deductively.
Our findings also indicate that validation-by-falsification is
not the only strategy people adopt to test their inferences.
Some people tend to check their inferences by looking for a
factual confirmation of the inference. This implies that mental
models theory needs to be elaborated to such an extent, and
can no longer identify test-procedures with a search for
counterexamples. SSCEPPTRE already incorporates a
process by which people evaluate the likelihood of the
confirmatory information. The parametric model behind
SSCEPPTRE (Schroyens et al., 2001), explicitly includes a
parameter which captures a judgment of the plausibility of the
standard conditional inferences. Hence it is shown that even
though validation-by-falsification is the hallmark of mental
models theory, confirmation of provisional conclusions is
certainly not beyond the scope of the theory – even though it
falls beyond the limits of Johnson-Laird and Byrne’s (1991)
model.
Though the processes of looking for, constructing and
evaluating potential alternatives are conceptually distinct,
they are not independent of one another. The present studies
counter the idea that there is a strict linear processing
sequence people engage in when testing their inferences.
When the alternatives are less plausible (as the alternatives of
the logically valid inference are, at least in the case of
knowledge-lean conditionals), reasoners are less likely to test
their inferences by searching for such alternatives. Or, the
other way around, when the alternatives are more plausible
(as the alternatives of the logically invalid inferences are),
reasoners are more likely to test their inferences by searching

1965

for such alternatives. This means that, not very surprisingly,
the reasoning system comprises of a mixture of bottom-up,
data-driven and top-down, theory-driven processes that can
gradually reinforce one another (see, Verschueren, Schaeken,
De Neys, d’Ydewalle, 2004, for evidence in favor of a
distinction between different types of counterexamples). This
complex interplay between motivational processing traits and
access to semantic-syntactic background knowledge that is
associated with the problem content and context is as yet not
explicitly captured in SSCEPPTRE’s computational
specification of conditional reasoning by model (though is it
is implicitly embedded within the most elaborate presentation
of the model in Schroyens et al., 2001).
In conclusion: we presented a study that showed the
import, importance and relevance of test procedures in the
processes of reasoning. We showed that when people are
motivated to test their inferences they frequently do so by
searching for alternatives, even when the problem content
provides no instigation for doing so. We also showed that a
search for alternatives, which is the hallmark of deductive
rationality and critical thinking in general, becomes more
prevalent in a context wherein such deductive rationality is
demanded. Finally, we showed that the mental model theory
is presently the one theory that is most apt to accommodate
the findings.

Acknowledgments
The present research was supported by the Flanders
(Belgium) Fund for Scientific Research (G.0320.05) and the
Canadian Natural Sciences and Engineering Research
Council (NSERC 297517). I also like to gratefully
acknowledge Walter Schaeken, Kristien Dieussaert, Niki
Verscheuren, Wim De Neys and all other R.A.T. research
members at the University of Leuven, Belgium.

References
Anderson, J. R. (1990). The adaptive character of thought.
Hillsdale, NJ: Lawrence Erlbaum.
Barrouillet, P., & Lecas, J.-F. (1998). How can mental models
theory account for content effects in conditional
reasoning? A developmental perspective. Cognition,
67(3), 209-253.
Chater, N., & Oaksford, M. (2001). Human rationality and
the psychology of reasoning: Where do we go from here?
British Journal of Psychology, 92, 193-216.
Cummins, D. D., Lubart, T., Alksnis, O., & Rist, R. (1991).
Conditional reasoning and causation. Memory &
Cognition, 19, 274-282.
De Neys, W., Schaeken, W., & d'Ydewalle, G. (2003).
Inference suppression and semantic memory retrieval:
Every counterexample counts. Memory & Cognition,
31(4), 581-595.
Edgington, D. (1995). On conditionals. Mind, 104, 235-329.
Eiter, T., & Lukasiewicz, T. (2000). Default reasoning from
conditional knowledge bases: Complexity and tractable
cases. Artificial Intelligence, 124, 169-241.

Evans, J. St. B. T., Handley, S. J., & Harper, C. N. J. (2001).
Necessity, Possibility, and belief: A study of syllogistic
reasoning. Quarterly Journal of Experimental
Psychology, 54A(3), 935-958.
Evans, J. St. B. T., Handley, S. J., Harper, C. N. J., &
Johnson-Laird, P. N. (1999). Reasoning about necessity
and possibility: A test of the mental model theory of
deduction. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 25(6), 1495-1513.
Johnson-Laird, P. N., & Byrne, R. M. J. (1991). Deduction.
Hillsdale, NJ: Erlbaum.
Johnson-Laird, P. N., & Byrne, R. M. J. (2002). Conditionals:
A theory of meaning, pragmatics, and inference.
Psychological Review, 109(4), 646-678.
Levinson, S. C. (2000). Presumptive meanings: The theory of
generalized conversational implicature. Cambridge,
M.A.: MIT Press.
Markovits, H., & Barrouillet, P. (2002). The development of
conditional reasoning: A mental model account.
Developmental Review, 22, 3-36.
Newstead, S. E., Handley, S. J., & Buck, E. (1999). Falsifying
mental models: Testing the predictions of theories of
syllogistic reasoning. Memory & Cognition., 27(2), 344354.
Oaksford, M., & Chater, N. (1998). Rationality in an
uncertain world. Hove, UK: Psychology Press.
Oaksford, M., Chater, N., & Larkin, J. (2000). Probabilities
and polarity biases in conditional inference. Journal of
Experimental Psychology: Learning, Memory, and
Cognition, 26, 883-899.
Rips, L. J. (1994). The Psychology of Proof: Deductive
reasoning in human thinking. Cambridge: MIT Press.
Schroyens, W., & Schaeken, W. (2003). A critique of
Oaksford, Chater and Larkin's (2000) conditional
probability model of conditional reasoning. Journal of
Experimental Psychology: Learning, Memory and
Cognition, 29, 140-149.
Schroyens, W., Schaeken, W., & d'Ydewalle, G. (2001a). The
processing of negations in conditional reasoning: A metaanalytic study in mental model and/or mental logic theory.
Thinking and Reasoning, 7(2), 121-172.
Schroyens, W., Schaeken, W., & Handley, S. (2003). In
Search of Counter Examples: Deductive Rationality in
Human Reasoning. Quarterly Journal of Experimental
Psychology, 57A (7), 1129-1145.
Stanovich, K. E. (2004). The robot's rebellion. Finding
meaning in the age of Darwin. Chicago: University of
Chicago Press.
Verschueren, N., Schaeken, W., De Neys, W., & d'Ydewalle,
G. (2004). The difference between generating
counterexamples and using them during reasoning.
Quarterly Journal of Experimental Psychology, 57, 12851308.

1966

