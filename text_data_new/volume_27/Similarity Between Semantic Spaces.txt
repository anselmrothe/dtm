UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Similarity Between Semantic Spaces

Permalink
https://escholarship.org/uc/item/8mx2s8qb

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)

Authors
Cai, Zhiqiang
Graesser, Arthur C.
Hu, Xiangen
et al.

Publication Date
2005-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Similarity Between Semantic Spaces
Xiangen Hu (xhu@memphis.edu)
Department of Psychology; The University of Memphis
Memphis, TN 38152

Zhiqiang Cai (zcai@memphis.edu)
Department of Psychology; The University of Memphis
Memphis, TN 38152

Arthur C. Graesser (agraesser@memphis.edu)
Department of Psychology; The University of Memphis
Memphis, TN 38152

Matthew Ventura (mventura@memphis.edu)
Department of Psychology; The University of Memphis
Memphis, TN 38152
Abstract

to compute similarities between documents (Hu et al.,
2003, 2003; Hu, Cai, Wiemer-Hasting, Graesser, & McNamara, 2005). The issue of evaluating the quality of
semantic space is very important, not only at the level
of theoretical importance, but also at the level of speci¯c
applications.
Curren tly, the quality of semantic spaces is usually
evaluated by human experts. This is done by comparing performances between applications that use a speci¯c semantic space and have human experts perform
some benchmark tests. For example, to evaluate an
LSA space with a given set of parameters (e.g., number of dimensions), an LSA similarity measure between
texts is compared with experts' judgement of the similarity of those texts (Olde, Graesser, & Tutoring Research Group, 2002). There are many possible variables
that are involved in creating semantic spaces, which
makes it impractical for human experts to evaluate all
semantic spaces.
In this paper, we present a systematic method to automatically evaluate semantic spaces. This method allows us to measure similarity between semantic spaces
that are created from di®erent sets of parameters (domain, corpus size, dimensions, etc.). Furthermore, this
metho d can even be used to ¯nd di®erences between semantic spaces that are created using entirely di®erent
metho ds, such as LSA, HAL, and NLS.

One of the challenges in Latent Semantic Analysis (LSA)
is deciding which corpus is best for a speci¯c application.Imp ortant factors of LSA in°uence the generation
of high quality LSA space including the size of the corpus, the weight (local or global) functions, number of
dimensions to keep, etc. These factors are often di±cult
to determine and as a result hard to control for. In this
paper, we provide a general method to measure similarity between semantic spaces. Using this method, one
can evaluate semantic spaces (such as LSA spaces) that
are generated from di®erent sets of parameters or di®erent corpora. The method we have develop ed is generic
enough to evaluate di®ering types of semantic spaces.

Keywords: Semantic Space, Latent Semantic Analysis, Similarity Measures Between Texts

Introduction
The use of higher dimensional semantic spaces, such as
Latent Semantic Analysis (LSA) (Landauer & Littman,
1990; Dumais, 1990; Laham, 1997; Landauer, Laham,
Rehder, & Schreiner, 1997; Landauer, Foltz, & Laham, 1998), Hyperspace Analogue to Language (HAL)
(Burgess, Livesay, & Lund, 1996; Burgess & Lund, 1997;
Burgess, 1998), Non-Latent Similarity (NLS) algorithm
(Cai et al., 2004), is very common in computational linguistics. The semantic spaces have been used in applications that involve information retrieval(Dumais, 1990),
essay grading, and text comparison (Foltz, Laham, &
Landauer, 1999). The scope and depth of the applications are so diverse that di®erent semantic spaces are
needed for di®erent purposes (Franceschetti et al., 2001).
Furthermore, the process of generating semantic spaces
is very complicated (Deerwester et al., 1990). Some of
them involve careful selection of corpora (Franceschetti
et al., 2001). From all the previous studies and applications of semantic spaces such as LSA, HAL, and NLS,
we observed that there are several key parameters (such
as dimensions, domain, corpus size, etc.) that need to
be set before generating an appropriate space for an application. There are di®erent metho ds that can be used

Observations
We ¯rst provide a mathematical model for Semantic
Space. This model is simply an abstraction of some
commonly used semantic spaces such as LSA , HAL,
and NLS. Based on this model, we provide a measure of
similarity between semantic spaces. At the end of the
paper, we outline pro cedures of how to use the similarity measures to evaluate semantic spaces. The metho d
presented in this paper is based on the following observations from semantic spaces, such as LSA , HAL, and
NLS:
1. Semantics is a property that applies to ¯ve di®er995

ent levels of language entities: word, phrase, sentence,
paragraph, and document. In any given language,

² Si (x1 ; x2 ) < 1 if x 1 and x2 have only ¯nite non-zero
elements,

² the smallest semantic units are words. For example,
"this", "is", "a", "big", "table".
² a phrase is an ordered array of words. For example,
"big table".
² a sentence is an ordered array of words and phrases.
For example, "This is a big table."
² a paragraph is an ordered array of sentences. For
example, "This is a big table. It was broken."
² a document is an ordered array of paragraphs.

² Si (x1 ; x2 ) > 0 if x1 = x2 6= 0;
² Si (x1 ; x2 ) = 0 if x1 = 0 or x2 = 0:
The similarity measure si (x 1 ; x2 ) between x 1 ; x2 2 Xi
is de¯ned in as
Si (E i (x1 ) ; Ei (x 2 ))
p
;
Si (E i (x1 ) ; Ei (x 1 )) S i (E i (x 2 ) ; E i (x 2 ))

p

where E i (x 1 ) and E i (x2 ) are not zero vectors.
For maps from lower level representations to higher
level representations follow the following constraints:

2. Semantics of any level of the language entities can be
represented numerically or algebraically.

² if x = (y 1 ; :::; yk ) 2 Xi; y1 ; :::; y k 2 Xi¡ 1 : for some
k > 0, then

² Semantics and the numerical or algebraic representation are synonymous.

E i (x) = Hi (E i¡ 1 (y 1 ) ; :::; E i¡ 1 (y k )) ;

3. Semantics of di®erent levels of the language entities
may be represented di®erently, but

k

where Hi is a function Hi : [R1 ] ¡! R1 ;

² Semantics of a higher level language entity is computed as a function of semantics of its lower level
language entities.
² Semantic relations between any two entities at the
same level can be numerically measured as a function of the semantics of the entities.

² For x 1 = (y11 ; :::y1u ) 2 Xi and x 2 = (y21 ; :::y2v ) 2 Xi;
where y 11 2 Xi¡ 1 ; Si (E i (x1 ) ; Ei (x 2 )) is in the form
u
of Eqn. (1), where Ui is a function Ui : [R1 ] £
1 v
[R ] ¡! R; for some u, v > 0:

4. The meaning of any word is represented by its (numerically measurable) relations with other words in the
same semantic space. We call such a relation induced
semantic structur e of the word in the given semantic
space.

where U = (Ei¡ 1 (y11 ) ; E i¡ 1 (y 12 ) ; :::; E i¡ 1 (y 1k 1 ))
and V = (E i¡ 1 (y 21 ) ; E i¡ 1 (y22 ) ; :::; Ei¡ 1 (y2k2 )) and
x 1 ; x2 2 Xi; y11 ; :::;y 1k 1 ; y21 ; :::; y 2k 2 2 Xi¡ 1

Si (E i (x1 ) ; Ei (x 2 )) = Ui (U; V)

(1)

De¯nition 1 is similar with the four components model
of Lowe (2001). The di®erence that arises between Lowe
and out de¯nition is that this de¯nition considers not
only the word level, but also all other levels with assumed
mapping from lower layers to higher layers. To understand the above de¯nition, consider the ¯ve language
entities, namely, word, phrase, sentence, paragraph, and
document. Each corresponds to a di®erent layer. X1 is
a set of phrases, X2 is a set of sentences, etc. For every
element, there is a vector representation in R1 . In Definition 1, we do not specify a limited dimensionality for
the vector representation. Instead we assume there is an
in¯nite dimensional vector with only a ¯nite number of
non-zero entries. To understand 5 and 4 of De¯nition 1,
one can take LSA as a simple example, where the semantic vector of a sentence is simply a vector summation of
the vectors of the words in the sentence. Furthermore,
the similarity between two words (or two sentences) is a
function of the two word (sentence) vectors. 5 of De¯nition 1 emphasizes the relations between di®erent layers.
The similar relations can be seen from LSA, where the
computation of similarity between documents is a function of the vectors of the words.
For the purpose of this paper, we next generalize the
idea of "near neighbor" of LSA in the new framework of
semantic space. From this concept, we further introduce
the idea of induced semantic structure. These two concepts will serve as the foundation for the remainder of
the paper.

De¯nitions
To formalize the above assumptions and the concept of
semantic structure, we have a formal de¯nition of vectorbased semantic spaces.
De¯nition 1 A vector-based semantic space contains
¯ve components:
1. A set of words X0 = fx 1 ; x2 ; :::; x N g ;
2. A hierarchy of layers, X1 ; :::; XM ; where an element
in the set Xi is a ¯nite ordered array of elements in
Xi¡ 1 (i = 1; :::; M );
3. Vector representation for elements in each of the layers.
4. Measure of similarity between elements within each of
the layers.
5. Maps from lower level representations to higher level
representations
For vector representations of elements in each layer, i.
e., 8x 2 Xi; there exists a vector E i (x) 2 R1 with only
¯nite non-zero entries; i = 1; :::; M ;
For measure of similarity for each layer: Assume S i :
R1 £ R1 ¡! R; i = 1; :::; M: such that
996

De¯nition 2 Given a semantic space with layers
X0 ; :::; XM ; 8x
2 Xi ; the neighbor of x is
f (y; s i (x; y))j y 2 Xi g :

The neighbor of any element in any of the layer Xi is
simply a partial ordered set. We call such an ordered set
induced semantic structur e.
De¯nition 3 Given a semantic space with layers
X0 ; :::; XM ; 8x 2 Xi ; the induced semantic structur e
Sx;i ½ Xi £ Xi is a partial order de¯ned in Eqn. (2).
½
¯
¾
¯
(x1 ; x 2 2 Xi ) and
(x 1 ; x2 ) ¯
:
¯ (si (E i (x) ; E i (x1 )) ¸ S i (E i (x) ; E i (x2 )))
(2)

Assumptions

With the above de¯nitions, we have the following assumptions. These assumptions serve as the theoretical
foundation for our similarity measure of vector based semantic spaces.
Assumption 1 The meaning of a word is embedded in
its relations with other words.
As an illustrative example (see Fig 1), the word "life"
has di®erent near neighbors for di®erent LSA spaces.

semantic structures in each of the semantic spaces. Denote them as Sx1 and Sx2 : Assume N 1 and N 2 are the
number of words in the two semantic spaces ( de¯ned in
1 of De¯nition 1), respectively, where T · min (N1 ; N 2 ) :
1
Furthermore, assume 1 Sx;T
and S2x;T are the top T nearest neighbor of word x: The combinatorial similarity for
word x between the two semantic spaces is de¯ned as
°
°
°S 1 \ S 2 °
x;T
x;T
T
°
Cx = °
(3)
° 1
2 °
°Sx;T [ Sx;T
°
where kXk is the number of items in set X: Given
T · min (N 1 ; N2 ) ; with such a de¯nition of semantic similarity for any word x. One can obtain similarity for any collection©of words,
W ½ X0 \ ª
Y 0 ; as sta¯
tistical properties of CxT ¯x 2 W ½ X0 \ Y 0 : For simplicity,
and standard derivation
¯ only consider mean
© we
ª
of C xT ¯x 2 W ½ X0 \ Y 0 ; although we may consider
other characteristics. Furthermore, we have the similarity de¯ned as a function of the value T : In fact
© T¯
ª
C x ¯x 2 W ½ X0 \ Y 0 ; 1 · T · min (N 1 ; N2 )
(4)

contains all information between the two semantic spaces
at the "combinatorial sense". Statistical properties of
(4) can be used to measure the Combinatorial Similarity
between two spaces. For example, if W is a collection
of physics glossory terms, then statistical properties of
(4); namely, mean and standard deviation, would be a
measure of semantic similarity of these terms between
the two spaces.

Assumption 2 If a given word is shared in di®erent
semantic spaces, the relation between the semantics of
the word in di®erent semantic spaces is a function of the
corresponding induced semantic structur es.
In Assumption 2, we consider only the algebraic (ordering) nature (as in Eqn. (2) ) of the near neighbors.

Permutational Similarity

Assumption 3 The relations between any two semantic
spaces are a function of the relations of the semantic
structur es of all the shared words

Permutational similarity is de¯ned in the same way as
Combinatorial Similarity , except the comparison of the
top T nearest neighbors of x in the two semantic spaces
is not only combinatorial,
alsoopermutational. Conn but
¡ 1
¢
©
ª
0
0
sider Sx;T
\ S 2x;T = x1 ; :::; x ¿ = x"1 ; :::; x "¿ and
³ 0
´
0
the orders of the nearest neighbors for x: x 1 ; :::; x¿
¡
¢
and x"1 ; :::; x "¿ for the two semantic spaces, resp ectively.
d is a function that measures the permutational distance
between two orders. It is assumed that

Assumption 3 extends Assumption 2 from the level of
the word to the entire semantic space.

Similarity Measures
With the above assumptions, we are able to measure
similarity between semantic spaces at three di®erent levels: Combinatorial Similarity, Permutational Similarity ,
and Quantitative Similarity. From 5 of De¯nition 1, we
see that all layers Xi; :::; XM of a semantic space actually depend on X0 and the mappings from lower layers
to higher layers. This makes it easier to intro duce the
general measure of semantic similarity between semantic
spaces.. In this paper, we only consider similarity measures that are derived at the layer of the basic items,
namely, X0 :

d ((x1 ; :::; x ¿ ) ; (x1 ; :::; x ¿ )) = 0;
and
³³ 0
´ ¡
³³ 0
´ ³ 0
´´
¢´
0
0
0
d x 1 ; :::; x ¿ ; x"1 ; :::; x "¿ · d x 1 ; :::; x¿ ; x¿ ; :::; x 1 :
We de¯ne the quantity
³³ 0
´ ¡
0
¢´1
0
d x 1 ; :::; x¿ ; x "1 ; :::; x"¿
¢¡0
¢¢ A C xT
PxT = @ 1 ¡ ¡¡ 0
0
0
d x 1 ; :::; x¿ ; x ¿ ; :::;x 1

Combinatorial Similarity
Based on Assumption 1, the meaning of a word is determined by its relations with all other words in a semantic
space. Using Assumption 2, we ¯rst have the Combinatorial Similarity at the level of individual word. Applying
Assumption 3, we will have the Combinatorial Similarity
at the level of semantic spaces.
Assume X0 and Y 0 are layers in two semantic spaces.
For any given item x 2 X0 \ Y 0, there are two induced

(5)

1
In some cases, there is no unique T top neighbours, because the induced semantic structure is only a partial order.
We only consider the simplest case here. We will not consider
the cases where no unique top T nearest neighbours in this
paper.

997

Assume that we have two LSA spaces 2 L1 =
(X0 ; X2 ; X3 ; X4 ; X5 ) and L2 = (Y 0 ; Y 2 ; Y 3 ; Y4 ; Y 5 ) ; and
a common set of words W = fx 1 ; x 2 ; :::; xN g = X0 \ Y0 .
Two matrices can be obtained by considering near neighbors (De¯nition 2) for all words in W : S1 = (s1ij )
and S2 = (s2ij ) ; where s kij = cosk (x i; xj ) is the cosine match between xi and x j within the LSA space k;
i; j = 1; :::; N ; k = 1; 2: Notice that such two matrices
contain all necessary information needed for all three different levels of similarities.
For the purp ose of illustration, and due to space limitation of the paper, we compute C xT ; P xT ; and QTx for
the word "life".
Table 1 lists near neighbors for several LSA spaces.
We computed C xT , P xT ; and QTx (as de¯ned in Eqn. (3),
(5) and (7)) with the value T = 50 (see Tables 2, 3,
and 4); We observed that the meaning of "life" is most
similar between 6th grade and 9th grade and between 9th
grade and 12th grade in the Touchstone Applied Science
Associates (TASA) corpus.
In this example, we have ¯ve di®erent spaces generated from TASA corpus. The same method can be used
to ¯nd semantic similarities for a group of words between di®erent corpora. When using a group of words,
instead of using quantities CxT , P xT ; and Q Tx as similarity
measures, one needs to use statistical properties of coresponding sets of quantities (de¯ned in (4),(6),(8)) such
as mean and standard deviation.

as permutational similarity of x in two semantic spaces
for a given T . Similarly, permutational similarity can be
de¯ned at the level of semantic spaces for any given set
of words, W ½ X0 \ Y0 ;
© T¯
ª
P x ¯x 2 W ½ X0 \ Y 0 ; 1 · T · min (N 1 ; N2 )
(6)

contains similarity between the two semantic spaces at
the permutational level. Consequently, statistical properties of (6), such as mean and standard deviation can
be used for such purposes.

Quantitative Similarity
Combinatorial Similarity and Permutational similarity
are based on algebraic properties of the induced semantic structure as a partial order. Quantitative Similarity is based on quantitative property of the nearest neighbor (De¯nition 2). For any x 2 X0 \ Y0 ;
and T · min (N1 ; N 2©¡
) ; there is ¢a¯ simple quantiª
1
tative relation between
y; s10 (x; y) ¯y 2 Sx;T
\ S2x;T
©¡ 2
¢¯
ª
1
2
and
y; s0 (x; y) ¯y 2 Sx;T
\ Sx;T
. For example, one
could use Pearson correlation r between the two set of
quantities. In the same way,
0
1
P 1
2
s
(x;
y)
s
(x;
y)
0
0
A CxT
QTx = @ 1 ¡ qP
(7)
2P 2
2
1
[s 0 (x; y)]
[s 0 (x; y)]
1
2
where the sum is obtained for all y 2 Sx;T
\ Sx;T
:
Furthermore, a set of quantities can be obtained for
W ½ X0 \ Y 0;
© T¯
ª
Q x ¯x 2 W ½ X0 \ Y 0 ; 1 · T · min (N 1 ; N2 ) : (8)

Table 1: Top 20 nearest neighbours of "life" for different LSA spaces (only showing 6th{12th, due to the
space limitation of the paper). The rank is taking from
http://lsa.colorado.com.
6th
9th
12th
life
life
life
reincarnation
contemplated
death
premiums
reincarnation
lifetime
policyholder
sai
hamlin
premium
pipal
pipal
sai
nirvana
nirvana
cycles
lifetime
zarathustra
holdover
death
ahuramazda
condemning
hinduism
ahriman
chekhov
afterlife
policyholder
captial
excerpted
romantics
pipal
ribman
essayists
nirvana
rea±rm
sai
hinduism
militarily
beaumarchais
span
kindless
1658
priori
condemning
pseudonym
maturity
premiums
poquelin
immoral
premium
ribman
humane
policyholder
kindless

Quantitative similarity is de¯ned as a set of statistical
properties of (8). As usual, mean and standard deviation
can be used for such purp oses.

An Example
In this section, we apply the de¯nitions, assumptions,
and similarity measures to LSA spaces. The following is
true for LSA spaces:
1. LSA contains a set of words.
2. At the layer corresponding to phrases, sentences, and
do cuments, LSA does not consider ordering of lower
layer items. One may view the "bag of words" as a
equivalent class of the ordered arrays containing the
same set of items.
3. The representation of LSA is a ¯nite dimensional vector. It can be viewed as in¯nite vector with ¯nite
non-zero entries.
4. The similarity measure S i (x1 ; x2 ) is simply the dotpro duct of the vectors
5. The maps between lower layers to higher layers is a
pool of the items from the lower layers. The vector
representation of higher layer items is simply a vector
summation of the vectors from the lower layers.

2

In applications of LSA, only X0 and very small portion
of other layers are meaningful. Due to the simple algorithm
of combining word vectors to sentence or document vectors,
items in other layers can be computed very easily.

998

² Type of corpus : LSA has been used in di®erent domains. For example, LSA has been used in tutoring
systems that teach computer literacy and qualitative
physics(Graesser et al., 2002). Usually, di®erent corpora are used for di®erent target application domains.
There are also general purpose corpora, such as the
TASA corpus. One question is how di®erent the LSA
spaces are that are created from these di®erent corpora. The similarity measure o®ered here can be used
to evaluate the LSA spaces, such as LSA spaces generated from physics text, computer literacy texts, and
TASA.

Table 2: Combinatorial similarity of the word "life" between several LSA spaces.
3rd
6th
9th
12th
College
3ed
1
6th
0:0526 1
9th
0:0204 0:2987 1
12th
0:0000 0:0989 0:2500 1
College 0:0000 0:0989 0:1111 0:2048 1

Summary
Table 3: Permutational Similarity of
tween several LSA spaces.
3rd
6th
9th
3ed
1
6th
0:0491 1
9th
0:0136 0:2078 1
12th
0:0000 0:0478 0:2130
College 0:0000 0:0462 0:0626

In this paper, we provide a general approach to measure similarity between semantic spaces. We ¯rst o®er
some observations of commonly used semantic spaces.
From these observations, we intro duce a set of general
assumptions. Finally we have a mathematical model
of semantic spaces. Based on this model, we are able
to derive three quantitative measures between semantic
spaces. Finally, we have used the similarity measures to
examine semantic similarity of a single word ('life") in
several LSA spaces.
The space limitation of this paper does not permit us
to o®er more, elaborated applications of these similarity measures. However, we have o®ered several possible
applications of the metho d. We argue that the metho d
introduced in this paper will help researchers to select
parameters in the pro cess of creating semantic spaces.

the word "life" be12th

College

1
0:1450

1

Table 4: Quantitative Similarity of the word "life" between several LSA spaces.
3rd
6th
9th
12th
College
3ed
1
6th
0:0524 1
9th
0:0202 0:2975 1
12th
0:0000 0:0979 0:2495 1
College 0:0000 0:0975 0:1101 0:2043 1

Acknowledgments
The Institute for Intelligent Systems (IIS) is an interdisciplinary research group comprised of approximately 100
researchers from psychology, computer science, physics,
engineering, linguistics, education and other disciplines
(visit http://www.iismemphis.org). This research, conducted by the authors and the IIS, was supp orted by
the National Science Foundation (REC 0106965) and
the DoD Multidisciplinary University Research Initiative
(MURI) administered by ONR under grant N00014-001-0600. Any opinions, ¯ndings and conclusions or recommendations expressed in this material are those of
the authors and do not necessarily re°ect the views of
ONR or NSF. We would like to thank Tom Landauer
and Walter Kintsch by supplying a number of corpora
used in this study.

Possible applicationsof the method
Although we only have shown a simple application of the
semantic similarity measures, namely, only at the level
of single word ("life "), we argue that the method can be
easily applied to evaluate similarities between semantic
spaces. For example, several parameters need to be set
for any give LSA space. The method introduced in this
paper can be used to measure di®erences due to di®erent
values of those parameters.
² Size of Documents : LSA created a word-do cument
matrix as the original matrix for later SVD. It is not
very clear what the ideal document size is. Using the
similarity measure we have here, we can systematically vary the document size and measure the similarity among LSA spaces with di®erent document sizes.

References
Burgess, C. (1998). From simple associations to the
building blocks of language mo deling meaning in memory with the hal model. Behavior Research Methods,
Instruments, and Computers , 30, 188-198.
Burgess, C., Livesay, K., & Lund, K. (1996). Mo deling parsing constraints in high-dimensional semantic
space: On the use of proper names. In Proceedings of
the cognitive science society. Hillsdale, N.J.: Erlbaum.

² Selection of number of dimensions to keep : After SVD,
only dimensions corresponding to the largest singular
values are kept. The number of dimensions kept is only
about 1~3% of the total number of dimensions. One
question is to address the robustness of the selection
of the dimensions. Using the similarity measure, we
can compare LSA spaces with di®erent dimensions.
999

Burgess, C., & Lund, K. (1997). Modeling parsing
constraints with high-dimensional context space. Language and Cognitive Processes , 12, 177-210.
Cai, Z., McNamara, D. S., Louwerse, M., Hu, X., Rowe,
M., & Graesser, A. C. (2004). Nls: Non-latent similarity algorithm. In K. Forbus, D. Gentner, & T. Regier
(Eds.), Proceedings of the 26th annual meeting of the
cognitive science society (p. 180-185). Mahwah, NJ:
Erlbaum.
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer,
K., T., & Harshman, R. (1990). Indexing by latent
semantic analysis. Journal of the American Society
For Information, 141, 391-407.
Dumais, S. (1990). Enhancing performance in latent
semantic indexing (lsi) retrieval (TM-ARH-017527
Technical Report). Bellcore.
Foltz, P. W., Laham, D., & Landauer, T. K. (1999).
Automated essay scoring: Applications to educational
technology. In proceedings of EdMedia '99.
Franceschetti, D., Karnavat, A., Marineau, J., McCallie, G. L., Olde, B. A., Terry, B. L., Graesser, A., &
C. (2001). Development of physics text corpora for
latent semantic analysis. In J. Mo ore & K. Stenning
(Eds.), Proceedings of the 23rd annual conference of
the cognitive science society (p. 297-300). Mahwah,
NJ: Erlbaum.
Graesser, A. C., Hu, X., Olde, B. A., Ventura, M., Olney, A., Louwerse, M., Franceschetti, D. R., & Person,
N. (2002). Implementing latent semantic analysis in
learning environments with conversational agents and
tutorial dialog. In W. G. Gray & C. D. Schunn (Eds.),
Proceedings of the 24th annual meeting of the cognitive
science society (p. 37). Mahwah, NJ: Erlbaum.
Hu, X., Cai, Z., Franceschetti, D., Penumatsa, P.,
Graesser, A., Louwerse, M., McNamara, D., & TRG.
(2003). Lsa: The ¯rst dimension and dimensional
weighting. In R. Alterman & D. Hirsh (Eds.), Proceedings of the 25rd annual conference of the cognitive
science society (p. 1-6). Boston, MA: Cognitive Science Society.
Hu, X., Cai, Z., Graesser, A. C., Louwerse, M. M., Penumatsa, P., Olney, A., & TRG. (2003). An improved
lsa algorithm to evaluate student contributions in tutoring dialogue. In G. Gottlob & T. Walsh (Eds.),
Proceedings of the eighteenth international joint conference on arti¯cial intel ligence (p. 1489-1491). San
Francisco: Morgan Kaufmann.
Hu, X., Cai, Z., Wiemer-Hasting, P., Graesser, A., & McNamara, D. S. (2005). Strengths, limitations, and extensions of lsa. In D. S. S. W. Landauer T.;McNamara
(Ed.), Lsa: A road to meaning. Mahwah, NJ: Erlbaum. (in press)
Laham, D. (1997). Latent semantic analysis approaches
to categorization. In M. G. Shafto and P. Langley
(Eds.), Proceedings of the 19th annual meeting of the
1000

Cognitive Science Society (p. 979). Mawhwah NJ: Erlbaum.
Landauer, T. K., Foltz, P. W., & Laham, D. (1998).
Introduction to latent semantic analysis. Discourse
Processes , 259-284.
Landauer, T. K., Laham, D., Rehder, B., & Schreiner,
M. E. (1997). How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans. In M. G. Shafto
and P. Langley (Eds.), Proceedings of the 19th annual
meeting of the Cognitive Science Society, 412-417.
Landauer, T. K., & Littman, M. L. (1990). Fully automatic cross-language document retrieval using latent
semantic indexing. In Proceedings of the 6th Annual
Conferenceof the Centre for the New Oxford English
Dictionaryand Text Research, 31-38.
Lowe, W. (2001). Towards a theory of semantic space. In
J. D. Mo ore & K. Stenning (Eds.), Proceedings of the
twenty-thir d annual conference of the cognitive science
society (pp. 576{581).Mahwah NJ: Lawrence Erlbaum
Associates.
Olde, B. A., Graesser, A. C., & Tutoring Research Group
the. (2002). Latent semantic analysis: What is it and
how can it improve and assess student learning? Paper
presented at the North East Regional Conference on
Excellence in Learning and Teaching, Oswego, NY.

