{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from IPython.core.debugger import set_trace\n",
    "import difflib\n",
    "import csv\n",
    "from scipy.stats import entropy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['doc_length', 'doc_topic', 'docnames', 'term_topic', 'terms', 'term_frequency', 'docs_per_year'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata_new.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "alldata_new = pickle.load(open('output/dtm_processed_output.p', 'rb'))\n",
    "doctitles=alldata_new['docnames']\n",
    "doc_year=alldata_new['docs_per_year']\n",
    "doc_ids =[0]+list(np.cumsum(doc_year))\n",
    "\n",
    "term_topic = alldata_new['term_topic']# term_topic is n_years*n_topics*n_terms\n",
    "terms = alldata_new['terms']\n",
    "term_frequency = alldata_new['term_frequency'][1:] # weirdly the first entry is empty\n",
    "doc_topicyrs = alldata_new['doc_topic']\n",
    "\n",
    "doc_topic = []\n",
    "doc_length=[]\n",
    "for year in range(len(term_topic)):    \n",
    "    doc_topic.append(alldata_new['doc_topic'][doc_ids[year]:doc_ids[year+1]])# doc_topic is nyear*n_docs given year*n_topics\n",
    "    doc_length.append(alldata_new['doc_length'][doc_ids[year]:doc_ids[year+1]]) #doc_length is nyear*n_docs given year\"\"\"    \n",
    "# rename topics by the hand-picked names\n",
    "topic_labels = pickle.load(open('topicnames.p','rb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stringdiff(a,b):\n",
    "    return sum ( a[i] != b[i] for i in range(len(a)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getlist(titles,doctitles):\n",
    "    doclist=[]\n",
    "    titlelist=[]\n",
    "    titles = [k.lower() for k in titles]\n",
    "    for doc in doctitles:\n",
    "        for title in titles:\n",
    "            matchratio = difflib.SequenceMatcher(None,title,doc).ratio() \n",
    "            if matchratio >.7:\n",
    "                print(doc+'\\n'+title)\n",
    "                doclist.append(doctitles.index(doc))\n",
    "                titlelist.append(title)\n",
    "        if set(titlelist)==set(titles):\n",
    "            break\n",
    "                \n",
    "    for t in titles:\n",
    "        if t not in titlelist:\n",
    "            print('\\ncannot find: '+t)\n",
    "    return([doclist,titlelist])\n",
    "\n",
    "# given a list of paper, what are their main topics? for analyzing like a lab or an author\n",
    "def maintopics(doclist,*topic_labels):\n",
    "    ntopics=20\n",
    "    doc_topfreq=np.empty((len(doclist),ntopics))\n",
    "    for k in range(len(doclist)):\n",
    "        if isinstance(doclist[0],int):\n",
    "            doc_topfreq[k]=alldata_new['doc_topic'][doclist[k]]\n",
    "        elif len(doclist[0])==2: # year then index\n",
    "            try:\n",
    "                doc_topfreq[k]=doc_topic[doclist[k][0]][doclist[k][1]]\n",
    "            except:\n",
    "                year=doclist[k][0]\n",
    "                \n",
    "                print('year%d'%doclist[k][0])\n",
    "                print(len(doc_topic[year]))\n",
    "                docdir = 'text_data/volume_{}/'.format(22+year)\n",
    "                alldocs = glob.glob(docdir+'*.txt')\n",
    "                print(len(alldocs))\n",
    "                set_trace()\n",
    "    doc_topfreq = np.mean(doc_topfreq,axis=0)\n",
    "    doc_topfreq = doc_topfreq/sum(doc_topfreq)\n",
    "    \n",
    "    maintopid = np.argsort(-doc_topfreq) \n",
    "    doc_topfreq=doc_topfreq[maintopid]\n",
    "    if topic_labels:\n",
    "        maintopics=[topic_labels[0][idx] for idx in maintopid]\n",
    "    return (maintopics,doc_topfreq)\n",
    "def lab_summary(titles,doctitles,label):\n",
    "    [doclist,titlelist]=getlist(titles,doctitles)        \n",
    "    (mtops,meantpfreq)=maintopics(doclist,topic_labels)\n",
    "    with open('result/lab_topic/'+label+'.txt','w') as f:\n",
    "        for k in range(len(mtops)):\n",
    "            f.write(mtops[k]+', freq={}'.format(meantpfreq[k])+'\\n')\n",
    "        f.write('\\n papers included:\\n')\n",
    "        for title in titlelist:\n",
    "            f.write(title)\n",
    "\n",
    "    with open('result/lab_topic/'+label+'.csv','w') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "        for k in range(len(mtops)):\n",
    "            csvwriter.writerow([mtops[k],meantpfreq[k]])\n",
    "    return(doclist,titlelist,mtops,meantpfreq)\n",
    "def labentropy(doclist):\n",
    "    entrop=[]\n",
    "    for doc in doclist:\n",
    "        entrop.append(entropy(alldata_new['doc_topic'][doc]))\n",
    "    return (np.mean(entrop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Attentional Learning Trap and How to Avoid It\n",
      "the attentional learning trap and how to avoid it\n",
      "\n",
      "cannot find: computationally reproducible experiments\n",
      "\n",
      "cannot find: online experiments using jspsych, psiturk, and amazon mechanical turk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([5329],\n",
       " ['the attentional learning trap and how to avoid it'],\n",
       " ['Decision making',\n",
       "  'Categorization',\n",
       "  'Consciousness and identity',\n",
       "  'Visual attention',\n",
       "  'Causal reasoning',\n",
       "  'Reasoning',\n",
       "  'Artificial network and Neuroscience',\n",
       "  'Face and emotion perception',\n",
       "  'Educational psychology',\n",
       "  'Probabilistic modeling',\n",
       "  'Sequential learning',\n",
       "  'Mathematical Psychology',\n",
       "  'Text processing and creativity',\n",
       "  'Memory',\n",
       "  'Spatial cognition and embodied cognition',\n",
       "  'Developmental psychology',\n",
       "  'non-verbal communication',\n",
       "  'Language: syntax',\n",
       "  'Knowledge structure',\n",
       "  'Language: semantics'],\n",
       " array([  5.55000388e-01,   2.32257476e-01,   4.59295041e-02,\n",
       "          4.25928275e-02,   4.25890392e-02,   3.62707537e-02,\n",
       "          1.92191536e-02,   1.14202215e-02,   1.02137483e-02,\n",
       "          4.20926924e-03,   2.97619087e-05,   2.97619054e-05,\n",
       "          2.97619054e-05,   2.97619054e-05,   2.97619054e-05,\n",
       "          2.97619053e-05,   2.97619053e-05,   2.97619052e-05,\n",
       "          2.97619052e-05,   2.97619052e-05]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label='alex'\n",
    "titles=['Computationally reproducible experiments','The Attentional Learning Trap and How to Avoid it','Online Experiments using jsPsych, psiTurk, and Amazon Mechanical Turk']\n",
    "(doclist,titlelist,mtops,meantpfreq)=lab_summary(titles,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking and evaluating natural language questions\n",
      "asking and evaluating natural language questions\n"
     ]
    }
   ],
   "source": [
    "label='anselm'\n",
    "titles=['Asking and evaluating natural language questions']\n",
    "(doclist,titlelist,mtops,meantpfreq)=lab_summary(titles,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the paper index for given titles\n",
    "label='gureckis'\n",
    "# find all titles from gureckis lab\n",
    "titles = []\n",
    "with open('lab_paper/Gureckis','r') as f:\n",
    "    for line in iter(f.readline, ''):\n",
    "        if 'Annual Conference of the Cognitive Science' in line:\n",
    "            line = line.lower()\n",
    "            ids = re.search(r\"(20[0-1][0-9])\", line).end(0)+2\n",
    "            if '\"' in line[ids:ids+4]:\n",
    "                ids +=1\n",
    "                ide = line.find('\" in ')\n",
    "            else:\n",
    "                ide = line.find('proceedings')-2\n",
    "            pptitle = line[ids:ide]\n",
    "            titles.append(pptitle)\n",
    "            \n",
    "(doclist,titlelist,mtops,meantpfreq)=lab_summary(titles,doctitles,label)\n",
    "gureckis_width=labentropy(doclist)\n",
    "pickle.dump([doclist,titlelist,mtops,meantpfreq,gureckis_width],open('result/lab_topic/'+label+'.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 of 32 papers are found for gureckis lab\n"
     ]
    }
   ],
   "source": [
    "print('%d of %d papers are found for %s lab'%(len(doclist),len(titles),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language as a Cognitive Technology English-Speakers Match Like Pirah When You Dont Let Them Count\n",
      "language as a cognitive technology: english-speakers match like pirah√£ when you don't let them count\n",
      "Principles of Generalization for Learning Sequential Structure in Language\n",
      "principles of generalization for learning sequential structure in language\n",
      "Exploring Word Learning in a High-Density Longitudinal Corpus\n",
      "exploring word learning in a high-density longitudinal corpus\n",
      "Cross-situational Word Learning Respects Mutual Exclusivity\n",
      "cross-situational word learning respects mutual exclusivity\n",
      "Informative Communication in Word Production and Word Learning\n",
      "informative communication in word production and word learning\n",
      "Contributions of Prosodic and Distributional Features of Caregivers Speech in Early Word Learning\n",
      "contributions of prosodic and distributional features of caregivers' speech in early word learning\n",
      "Beyond Transitional Probabilities Human Learners Impose a Parsimony Bias in Statistical Word Segmentation\n",
      "beyond transitional probabilities: human learners impose a parsimony bias in statistical word segmentation\n",
      "Ad-hoc scalar implicature in adults and children\n",
      "ad-hoc scalar implicature in adults and children\n",
      "Thinking for Seeing Enculturation of Visual-Referential Expertise as Demonstrated by Photo-Triggered Perceptual Reorganization of Two-Tone Mooney Images\n",
      "thinking for seeing: enculturation of visual-referential expertise as demonstrated by photo-triggered perceptual reorganization of two-tone mooney images\n",
      "The learnability of constructed languages reflects typological patterns\n",
      "the learnability of constructed languages reflects typological patterns\n",
      "Zipfian word frequencies support statistical word segmentation\n",
      "zipfian word frequencies support statistical word segmentation\n",
      "Markers of Discourse Structure in Child-Directed Speech\n",
      "markers of discourse structure in child-directed speech\n",
      "Modeling online word segmentation performance in structured artificial languages\n",
      "modeling online word segmentation performance in structured artificial languages\n",
      "Zero anaphora and object reference in Japanese child-directed speech\n",
      "zero anaphora and object reference in japanese child-directed speech\n",
      "Relating Activity Contexts to Early Word Learning in Dense Longitudinal Data\n",
      "relating activity contexts to early word learning in dense longitudinal data\n",
      "Learning from speaker word choice by assuming adjectives are informative\n",
      "learning from speaker word choice by assuming adjectives are informative\n",
      "Semantic Coherence Facilitates Distributional Learning of Word Meanings\n",
      "semantic coherence facilitates distributional learning of word meanings\n",
      "Measuring childrens visual access to social information using face detection\n",
      "measuring children's visual access to social information using face detection\n",
      "An integrated model of concept learning and word-concept mapping\n",
      "an integrated model of concept learning and word-concept mapping\n",
      "Young childrens developing sensitivity to discourse continuity as a cue to reference\n",
      "young children's developing sensitivity to discourse continuity as a cue to reference\n",
      "Modeling disambiguation in word learning via multiple probabilistic constraints\n",
      "modeling disambiguation in word learning via multiple probabilistic constraints\n",
      "Online Processing of Speech and Social Information in Early Word Learning\n",
      "online processing of speech and social information in early word learning\n",
      "Developmental and postural changes in childrens visual access to faces\n",
      "developmental and postural changes in children's visual access to faces\n",
      "The development of predictive processes in childrens discourse understanding\n",
      "the development of predictive processes in children's discourse understanding\n",
      "Measuring the comprehension of negation in 2- to 4-year-old children\n",
      "measuring the comprehension of negation in 2-to 4-year-old children\n",
      "Modeling the Development of Determiner Productivity in Childrens Early Speech\n",
      "modeling the development of determiner productivity in children's early speech\n",
      "Learning to Reason Pragmatically with Cognitive Limitations\n",
      "learning to reason pragmatically with cognitive limitations\n",
      "The structure of the lexicon reflects principles of communication\n",
      "the structure of the lexicon reflects principles of communication\n",
      "Preschoolers infer contrast from adjectives if they can access lexical alternatives\n",
      "preschoolers infer contrast from adjectives if they can access lexical alternatives\n",
      "A pragmatic account of the processing of negative sentences\n",
      "a pragmatic account of the processing of negative sentences\n",
      "Beyond Naive Cue Combination Salience and Social Cues in Early Word Learning\n",
      "beyond naive cue combination: salience and social cues in early word learning\n",
      "Discovering the Signatures of Joint Attention in Child-Caregiver Interaction\n",
      "discovering the signatures of joint attention in child-caregiver interaction\n",
      "Modeling the dynamics of classroom education using teaching games\n",
      "modeling the dynamics of classroom education using teaching games\n",
      "Referential cues modulate attention and memory during cross-situational word learning\n",
      "referential cues modulate attention and memory during cross-situational word learning\n",
      "Developmental Changes in the Relationship Between Grammar and the Lexicon\n",
      "developmental changes in the relationship between grammar and the lexicon\n",
      "Childrens Online Processing of Ad-Hoc Implicatures\n",
      "children's online processing of ad-hoc implicatures\n",
      "Conceptual complexity and the evolution of the lexicon\n",
      "conceptual complexity and the evolution of the lexicon\n",
      "The pragmatics of negation across contexts\n",
      "the pragmatics of negation across contexts\n",
      "Large-scale investigations of variability in childrens first words\n",
      "large-scale investigations of variability in children's first words\n",
      "Sources of developmental change in pragmatic inferences about scalar terms\n",
      "sources of developmental change in pragmatic inferences about scalar terms\n",
      "Signatures of Domain-General Categorization Mechanisms in Color Word Learning\n",
      "signatures of domain-general categorization mechanisms in color word learning\n",
      "Talking with tact Polite language as a balance between informativity and kindness\n",
      "talking with tact: polite language as a balance between kindness and informativity\n",
      "Measuring lay theories of parenting and child development\n",
      "measuring lay theories of parenting and child development\n",
      "Linguistic niches emerge from pressures at multiple timescales\n",
      "linguistic niches emerge from pressures at multiple timescales\n",
      "A speed-accuracy trade-off in childrens processing of scalar implicatures\n",
      "a speed-accuracy trade-off in children's processing of scalar implicatures\n",
      "Linguistic input is tuned to childrens developmental level\n",
      "linguistic input is tuned to children's developmental level\n",
      "From uh-oh to tomorrow Predicting age of acquisition for early words across languages\n",
      "from uh-oh to tomorrow: predicting age of acquisition for early words across languages\n",
      "Distinguishing processing difficulties in inhibition implicature and negation\n",
      "distinguishing processing difficulties in inhibition, implicature, and negation\n",
      "Inferring Generic Meaning From Pragmatic Reference Failure\n",
      "inferring generic meaning from pragmatic reference failure\n",
      "A performance model for early word learning\n",
      "a performance model for early word learning\n",
      "Determining the alternatives for scalar implicature\n",
      "determining the alternatives for scalar implicature\n",
      "When does passive learning improve the effectiveness of active learning\n",
      "when does passive learning improve the effectiveness of active learning?\n",
      "Childrens social referencing reflects sensitivity to graded uncertainty\n",
      "children's social referencing reflects sensitivity to graded uncertainty\n",
      "Word Identification Under Multimodal Uncertainty\n",
      "word identification under multimodal uncertainty\n",
      "An information-seeking account of eye movements during spoken and signed language comprehension\n",
      "an information-seeking account of eye movements during spoken and signed language comprehension\n",
      "Convention-formation in iterated reference games\n",
      "convention-formation in iterated reference games\n",
      "The Semantics and Pragmatics of Logical Connectives Adults and Childrens Interpretations of And and Or in a Guessing Game\n",
      "the semantics and pragmatics of logical connectives: adults' and children's interpretations of \"and\" and \"or\" in a guessing game\n",
      "I wont lie it wasnt amazing Modeling polite indirect speech\n",
      "\"i won't lie, it wasn't amazing\": modeling polite indirect speech\n",
      "\n",
      "cannot find: effects of language on color discriminability\n",
      "1.59066739688\n",
      "2.47650838587\n"
     ]
    }
   ],
   "source": [
    "label='m_frank'\n",
    "# find all titles from Mike Frank\n",
    "titles = []\n",
    "with open('lab_paper/M_Frank','r') as f:\n",
    "    for line in iter(f.readline, ''):\n",
    "        if 'Annual Conference of the Cognitive Science' in line:\n",
    "            ids = line.find('). ')+3\n",
    "            ide = line.find('Proceedings')-2\n",
    "            pptitle = line[ids:ide]\n",
    "            titles.append(pptitle)\n",
    "# lab summary in the topic space\n",
    "(doclist,titlelist,mtops,meantpfreq)=lab_summary(titles,doctitles,label)\n",
    "frank_width=labentropy(doclist)\n",
    "pickle.dump([doclist,titlelist,mtops,meantpfreq,gureckis_width],open('result/lab_topic/'+label+'.p','wb'))\n",
    "print(frank_width)\n",
    "print(entropy(meantpfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Developmental psychology',\n",
       " 'Probabilistic modeling',\n",
       " 'non-verbal communication',\n",
       " 'Language: syntax',\n",
       " 'Visual attention',\n",
       " 'Sequential learning',\n",
       " 'Mathematical Psychology',\n",
       " 'Reasoning',\n",
       " 'Language: semantics',\n",
       " 'Categorization',\n",
       " 'Memory',\n",
       " 'Text processing and creativity',\n",
       " 'Decision making',\n",
       " 'Consciousness and identity',\n",
       " 'Educational psychology',\n",
       " 'Knowledge structure',\n",
       " 'Face and emotion perception',\n",
       " 'Spatial cognition and embodied cognition',\n",
       " 'Artificial network and Neuroscience',\n",
       " 'Causal reasoning']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 of 59 papers are found for m_frank lab\n"
     ]
    }
   ],
   "source": [
    "print('%d of %d papers are found for %s lab'%(len(doclist),len(titles),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Knowledge-Resonance (KRES) Model of Category Learning\n",
      "a knowledge resonance (kres) model of category learning\n",
      "Eyetracking as an Implicit Measure of Category-Based Induction\n",
      "eyetracking as an implicit measure of category-based induction\n"
     ]
    }
   ],
   "source": [
    "label='Murphy'\n",
    "titles=['A knowledge resonance (KRES) model of category learning','Eyetracking as an implicit measure of category-based induction']\n",
    "(doclist,titlelist,mtops,meantpfreq)=lab_summary(titles,label)\n",
    "murphy_width=labentropy(doclist)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
