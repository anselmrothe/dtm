UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Design features of language emerge from general-purpose learning mechanisms
Permalink
https://escholarship.org/uc/item/7q1736rb
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Author
Monaghan, Padraic
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

     Design features of language emerge from general-purpose learning mechanisms
                                         Padraic Monaghan (p.monaghan@lancs.ac.uk)
                        Centre for Research in Human Development and Learning, Department of Psychology
                                             Lancaster University, Lancaster LA1 4YF, UK
                              Abstract                                  consistencies (rather than absolutes) in language structure
   There are certain universal properties of language that are
                                                                        occur, then, because similar processing and learning
   taken to be definitional to the concept of language itself, such     limitations are present in all language users.
   as the arbitrary relationship between sounds and meanings of            Yet, there are properties of languages that are universal,
   words. Another possibility is that these “design features” of        though these are not discussed by Evans and Levinson
   language may instead be the expressed consequences of                (2009) because they are considered definitional properties of
   general purpose learning constraints within the cognitive            language. These fundamental language properties, or
   system learning the language. To test this, generations of an        “design features” in Hockett’s (1960) terms, were listed by
   inverse model learning to map between sounds and meanings
   of words was tested. In this model, learning to associate            Greenberg       (1963)     as     discreteness,  productivity,
   phonology to semantics influences the model’s production of          arbitrariness, and duality of patterning. Discreteness refers
   phonology from semantics, and phonological productions of            to the composition of utterances in a language from smaller
   one model were used as input to the next generation. Over            elements (words/morphemes or phonemes) the combination
   generations of the model’s learning, the language became             of which provides meaning. Relatedly, productivity refers to
   easier to acquire, and demonstrated increased arbitrariness of       the ability to use the smaller elements of the language in
   mappings between phonology and semantics. The iterative
                                                                        multiple combinations – from a small, finite set of elements
   modelling demonstrated that design features of natural
   language can spontaneously emerge in a general purpose               (words/morphemes) an infinite set of utterances can be
   learning system.                                                     generated. Arbitrariness refers to the absence of
                                                                        systematicity between the sounds of words and their
   Keywords: Evolution; language acquisition; computational
                                                                        meaning (de Saussure, 1916), and duality of patterning
   modeling; arbitrariness of the sign; phonology; semantics.
                                                                        refers to the composition of words from smaller
                                                                        phonological units, where the utterance meaning is carried
                           Introduction
                                                                        by the combination of words and is unrelated to the
   Languages change extremely rapidly. Gray and Atkinson                phonological composition of these words.
(2003), for instance, estimated that all the living Indo-                  Though researchers such as Greenberg (1963) question
European dialects diverged approximately 7900 years ago.                the possibility of a language without each of these
In terms of cultural transmission from generation to                    properties, an alternative view is instead that many of these
generation, this means that language has been passed on                 properties could have been otherwise in language
only a few hundred times to produce such variation as that              (Monaghan, Christiansen, & Fitneva, 2011). It is possible to
found between English, Gaelic, Greek, Italian, Lithuanian,              conceive of a language where sounds of words do carry
and Hindi. There are of course multiple forces at work in               some aspect of the meaning. Instead, this paper takes as its
determining language change (Labov, 1994), however, the                 perspective that design features are universal properties of
fact that language has to be transmitted from one generation            language not because they are definitional but rather
to the next suggests that learnability of the language is a             because of the constraints of our cognitive systems that
critical selective pressure contributing to language evolution          mean that languages are structured to make acquisition and,
(Christiansen & Chater, 2008).                                          consequentially, transmission easier. The current study tests
   So what are these language properties that contribute to             a framework that demonstrates how such design features of
language learnability? One place to begin is the recent                 language may have become instantiated within language
discussion over patterning of language universals                       structure as a consequence of general-purpose (i.e., not
(Christiansen, Collins, & Edelman, 2009; Scalise, Magni, &              language-specific) learning mechanisms.
Bisetto, 2009). Evans and Levinson (2009) demonstrated                     Monaghan et al. (2011) showed that one of the design
that for each “language universal” proposed in the literature           features – arbitrariness of the sign – resulted in more
there is at least one extant language that violates the                 accurate language learning by participants acquiring an
prevailing pattern. Instead, Evans and Levinson (2009)                  artificial language, and by associative networks learning the
contend that it is language diversity, rather than universality,        same sound-meaning mappings. Thus, arbitrariness
that is the critical feature of human communication to be               bestowed an advantage for learning.
explained. Importantly, this diversity tends to indicate                   However, this work stopped short of demonstrating how
statistical clusters of language properties, which are                  such arbitrariness becomes incorporated within the
consistent with general cognitive constraints that assist in            language as a consequence of learning constraints. In this
learning or transmission of language that then become                   paper, I present a model of cultural transmission where
embedded in language structure. Cross-linguistic                        general purpose learning constraints that affect language
                                                                    2661

acquisition are expressed within the same model when it               simulation tests the emergent structure when the initial state
undertakes language production, which is then used to                 of the language is systematic.
entrain the next generation of models. An appropriate
architecture for achieving this effect of acquisition
influencing production is the inverse model (Jordan &
Rumelhart, 1992), which resembles the sleep-wake
algorithm implemented in the stochastic inverse Helmholtz
model (Hinton, Dayan, Frey, & Neal, 1995). Such models
have close parallels to the learning occurring in feedforward
and feedback connections in the cortex (Carpenter &
Grossberg, 1987; Mumford, 1994), and have been
effectively used to simulate development of features of
human communication, such as segmental phonology (Plaut
& Kello, 1999). These models have the advantage over
previous models of iterated learning in that they permit
more enriched representations to be learned and to influence
performance (Kirby, 2001).
   The critical feature of the modeling is that associative              Figure 1: The model of iterated learning. Solid arrows
learning forms the basis of the model’s performance, and              indicate the forward model, where the model is trained to
that the model’s approach to learning words’ meanings from            map phonology onto semantics for each word, with two
sounds influences the model’s production of the words’                categories (red/blue) centred at a different region of this
sounds from meanings. Language structures that are easier             multidimensional space. The dashed arrows indicate the
to acquire due to general purpose learning mechanisms will            inverse model, where the model produces phonology from
be learned more accurately by the model and will, over                semantics which forms the input to the next generation of
generations of learner, become stably expressed within the            learner.
words themselves. The dashed lines in Figure 1 illustrate
how the learning of the spoken input to an output meaning                 Emergent structure from random origins
representation can feedback to generate a spoken version for
each verbal input. Hence, the model as learner can adapt its          Method
version of the language to more closely match its own
internal constraints. This spoken output can then be used for         Architecture
the next generation of learner, and across multiple                   The model’s architecture is shown in Figure 1. A set of 10
generations the language can be altered in such a way to              units each represented the phonology and the semantics for
make it more reflective of the model’s learning properties            the model, and a set of 10 hidden units interconnected these
and hence more easily learnable for future generations. The           representations. For the forward run of the model, to
language can then be analysed for the key “design features”           simulate language comprehension, the verbal input was
of natural languages.                                                 connected to the hidden units which were in turn connected
   The first prediction is that the expression of the model’s         to the semantic units. These connections are illustrated with
learning constraints will result in a set of phonological             solid arrows. For the inverse run of the model, to simulate
representations that the model learns to map onto the                 language production, there were connections from the
meaning representations more quickly and accurately – so              semantics to the hidden, and from the hidden to the
future generations will find the language easier to acquire.          phonology to represent spoken output. The inverse model
The second prediction is that “design features” of natural            connections are illustrated in Figure 1 by dotted arrows.
language will be exhibited in the representations, in                 Weights on connections were given initial uniform random
particular that mappings between phonology and semantics              values in the range [-0.25,0.25].
will instantiate arbitrariness in the mapping1. However,              Training and Testing
based on previous studies (e.g., Monaghan et al., 2011) it is         The model was trained on 20 patterns mapping between
also predicted that the patterns will demonstrate                     phonology and semantics. The phonological representations
systematicity at the category level, in terms of phonology-           were initially constructed by randomly selecting values in
category mappings. The first simulation tests the emergent            the range [0,1] in 0.1 intervals for each of the 10 units in the
structure of phonology-semantics mappings, when the                   pattern. The semantic representation remained stable
phonology is initially random, so fully arbitrary. The second         throughout learning and was constructed by generating 10
                                                                      exemplars of two prototypes, one centred at a value of 0.75
                                                                      for each of the 10 units in the pattern, and the other centred
   1
                                                                      at 0.25 for each of the 10 units. Exemplars were produced
     The existence of small pockets of sound-symbolism in natural     by randomly varying the prototype values by a uniform
languages has little effect on the overall arbitrariness of the
relationship between sound and meaning (see Monaghan et al.,
                                                                      value in the range [-0.25,0.25]. There were therefore two
2011, for discussion).                                                clusters of meaning representations in the patterns to be
                                                                  2662

learned. In previous simulations, such semantic categories          a                                   b
have simulated two grammatical categories of words, e.g.,
nouns and verbs (Monaghan et al. (2011).
   The model was trained to map the phonology onto the
semantics to simulate learning to comprehend language, and
then learn to map the semantics back onto the phonology
using the same hidden unit representations as occurred
during the phonology to semantics mapping. This entailed
that the model’s learning constraints for acquiring the
language were applied during the model’s attempts to
produce the language. The forward model operated as a
standard three-layer backpropagation network: A pattern
was selected randomly and presented at the input. The               Figure 2. (a) Correlation among phonological
model’s hidden unit representation for this pattern was          representations for all patterns, for patterns within the same
recorded, and then the mean square error at the semantic         category. and across different categories, at first and tenth
representation (the difference between target and actual         iteration of learning. (b) Correlation between phonology and
semantic representations) was used to adjust the weights on      semantic representations at first and tenth iteration for all
the forward connections using the backpropagation learning       patterns, patterns within the same category, and patterns in
algorithm with learning rate of 0.1.                             different categories.
   Then, the inverse model was applied to this same pattern:
The target semantic representation was presented at the                     Table 1: Mean error (SD in parentheses) for
semantic layer of the model, and the model was required to            phonology→semantics, and semantics→phonology
reproduce the hidden unit representation for that pattern                mappings at first and tenth iteration of training.
produced during the run of the forward model. The error at
the hidden layer was propagated back to adjust weights                   Iteration    phon→sem            sem→phon
between semantic and hidden units. Then, the model was                   1            2.40 (1.42)         116.29 (32.45)
required to reproduce the phonology for that pattern given               10           1.31 (.75)          .03 (.07)
the hidden unit representation, and once again error
(between the initial input phonology and the model’s actual      Results and Discussion
phonological production) was propagated back to adjust
                                                                 The first prediction was that over iterations of the model, the
weights between hidden and phonology layers.
                                                                 model would learn to map between phonology and meaning
   At the end of training, the inverse model was presented
                                                                 (comprehension task), and meaning to phonology
with each of the semantic representations and produced a
                                                                 (production task) with less error. We compared the mean
version of the phonology for these patterns that was
                                                                 square error of the model’s actual productions versus the
influenced by its learning of the mapping. These new
                                                                 target semantic representation across the meaning layer
phonological patterns were used as the language for training
                                                                 when the phonology was inputted, and the mean square
the next iteration of the model. There were 10 iterations
                                                                 error across the phonology layer when the meaning layer
altogether of the model.
                                                                 was inputted, we compared the error after the first iteration
   We varied the number of presentations of the patterns to
                                                                 to that after the tenth iteration. The error value provides a
determine an effective level of change in the language – not
                                                                 reflection of how accurately the model reproduces the
too much, such that the language would alter radically from
                                                                 semantic and phonological representations of the words.
one generation to the next, and not too little such that no
                                                                 Table 1 shows the results.
representational change would be observed. We found that
                                                                    For both comprehension and production tasks, there was a
500,000 patterns resulted in an interpretable level of
                                                                 reduction in error over iterations, t(19) = 3.11, and 15.01,
representational change. With fewer presentations than this,
                                                                 both p < 0.01, thus the generations of learning resulted in
the model produced phonological representations via the
                                                                 easier comprehension and production of the patterns.
inverse model that were distant from the original
                                                                    To determine the changes that actually occurred to the
phonological representations, and also that were indistinct
                                                                 phonology representations as a consequence of the iterated
from one another, and so extremely difficult to learn for
                                                                 learning, we measured the cosine distance between each pair
future generations of the model.
                                                                 of phonological patterns and then computed the mean of
   We ran 20 versions of the model, varying the initial
                                                                 these distances for each simulation run. High mean cosine
phonological and semantic representations, and varying the
                                                                 values indicate that there is similarity among the patterns.
initial randomized weights on the connections between
                                                                 We took this measure at the first iteration and at the tenth
units. Each simulation run of the model was used as a
                                                                 iteration. The results are shown in Figure 2a. From first to
separate participant in the analyses.
                                                                 tenth iteration, the correlation increased among the
                                                                 phonological representations, t(19) = 18.09, p < 0.001. To
                                                                 determine whether this increased correlation was due to
                                                             2663

words of the same (semantic) category becoming more                      It may be that the development of systematicity at the
similar to one another, or whether words of distinct                  category level and arbitrariness at the individual word level
categories were becoming more aligned, we distinguished               is an intermediate stage in the model’s development to an
the cosine distances among phonological representations of            optimal representation for the language, where this final
the same category, and those among representations of                 optimal state is fully systematic. In order to rule out this
different categories. The results are also shown in Figure 2a.        possibility, the next simulation tested language change when
An ANOVA with same/different category and first/tenth                 the initial language was highly systematic.
iteration on mean cosine distance for each simulation run
resulted in a significant main effect of same/different                  Emergent structure from systematic origins
category, F(1, 19) = 8.46, p < 0.01, indicating that same
category responses were more correlated than different                Method
category. There was also a significant main effect of
iteration, F(1, 19) = 327.40, p < 0.001, reflecting the general       Architecture
increase in correlations from first to tenth iteration. The           The architecture was identical to the first simulation.
interaction was also significant, F(1, 19) = 27.72, p < 0.001,        Training and testing
showing that the correlation increased more sharply for               The training and testing were identical to the first simulation
same category responses than different category responses,            except that the initial phonological representations were
though the magnitude of the difference was small. Thus, the           highly correlated with the semantic representations for each
model introduced systematicity at the category level into the         pattern. Each phonological representation was generated by
phonological representations resulting in easier acquisition          taking the semantic representation for that pattern and
of the patterns at the end of the set of iterations.                  varying each dimension by a random value in the range [-
   To determine the extent to which the mappings between              0.25,0.25].
phonology and semantics introduced greater or less                    a                                      b
arbitrariness in the mapping, we correlated the cosine
distances between each pair of phonology representations
and each pair of semantics representations. If patterns that
are close together in phonology are also close together in
semantics and patterns that are distant in phonology are
distant in semantics then the correlation will be high,
representing systematicity in the mapping. If patterns that
are distinct in phonology are similar in semantics then the
correlation will be low, indicating arbitrariness.
   The results are shown in Figure 2b. For all patterns, there
was an increase in correlation between the phonology and
the semantic spaces from first to tenth iteration, t(19) =
4.65, p < 0.001. There is thus an increase in systematicity              Figure 3. (a) Correlation among phonological
across the iterations. To determine whether this change was           representations for all patterns, for patterns within the same
within each category, indicating that words of the same               category. and across different categories, at first and tenth
category were becoming increasingly systematic with                   iteration of learning for systematic origin model. (b)
respect to their meanings, or whether the change was due to           Correlation       between      phonology       and     semantic
systematicity across categories, we measured the correlation          representations at first and tenth iteration for all patterns,
between distances just for words within the same category,            patterns within the same category, and patterns in different
and compared this to distances for words occurring in                 categories for systematic origin model.
distinct categories.
   The results are again shown in Figure 2b. An ANOVA                      Table 2: Mean error (SD in parentheses) for phonology-
with same/different category, and first/tenth iteration was             >semantics, and semantics->phonology mappings at first
performed. There was a marginally significant main effect                and tenth iteration of training for the second simulation.
of same/different category, F(1, 19) = 3.81, p = 0.07. There
was a significant main effect of iteration, F(1, 19) = 58.25, p               Iteration    phon->sem           sem->phon
< 0.001, as correlations increased from first to tenth                        1            1.06 (0.52)         18.00 (4.92)
iteration. There was also a significant interaction, F(1, 19) =               10           15.35 (7.02)        0.003 (0.009)
4.77, p < 0.05, indicating that for the first iteration there was
little difference in the correlation between phonology and            Results and Discussion
semantics for patterns in the same versus different                   In terms of the model’s error, for the production task there
categories, but that after ten iterations, the correlation was        was a significant increase in error between first and tenth
substantially higher for patterns of different categories than        iteration, t(19) = -9.10, p < 0.001, but a significant decrease
same categories.                                                      for the comprehension task, t(19) = 16.39, p < 0.001 (see
                                                                  2664

Table 2). Thus, systematic mappings resulted in easier             themselves, and these learning constraints were expressed
acquisition, but greater difficulties in producing                 by reflecting the output category structure within the input
discriminating output, and the model adapted the                   phonology. In artificial language learning studies, such
representations to meet better the production task.                reflections of category structure within phonology has been
   The correlations among the phonological representations         shown to result in improved learning of categories (Frigo &
were compared between first and tenth iterations, and as           McDonald, 1998; St Clair, Monaghan, & Ramscar, 2009),
with the initial arbitrary mapping the correlations increased,     and may indeed be vital for effective acquisition of
t(19) = 31.15, p < 0.01. For the same/different category           grammatical categories (Braine, 1987). The model points to
comparing first and tenth iteration in an ANOVA, there was         the way such phonological characteristics of words can
a significant main effect of same/different, F(1, 19) =            become imprinted within the language as a consequence of
137.99, p < 0.001, with same category resulting in higher          general-purpose learning mechanisms exerting their
correlations. There was a significant main effect of               influence through generations of language learners.
first/tenth iteration, F(1, 19) = 962.90, p < 0.001, and a            Second, the results of the model in terms of the properties
significant interaction, F(1, 19) = 4.64, p < 0.05. The            of the mapping between phonology and semantics show in
difference between same/different category was greater at          addition that arbitrariness can sit alongside systematicity
the tenth iteration (see Figure 3a).                               indicated at the category level. For words of the same
   For the correlations between phonology and semantic             semantic category there is greater distinction between
representations comparing first and tenth iteration, the           individual phonological patterns in terms of the precise
correlation decreased with time, t(19) = 6.52, p < 0.001. An       semantic representation that they map onto. For words of
ANOVA with same/different category and first/tenth                 different categories, there is greater expression of words or
iteration resulted in a significant main effect of                 similar sound relating to meanings that are similar. This can
same/different, F(1, 19) = 52.15, p < 0.001, with same             be interpreted in terms of the coherence among the
category resulting in higher correlation than different            phonological representations being tempered by the
category. There was also a significant main effect of              additional requirement to distinguish particular semantic
iteration, F(1, 19) = 4.90, p < 0.05, and a significant            representations. Thus, emerges systematicity at the category
interaction, F(1, 19) = 17.84, p < 0.001, with a decrease in       level, but arbitrariness for mapping between individual
correlation between first and tenth iteration for same             patterns.
category but an increase in correlation for different category        In this respect, the iterative inverse modeling results
correlations (see Figure 3b). As with the model beginning          presented here relate to learning studies of static artificial
with fully arbitrary mappings, arbitrariness increased to a        languages that map between phonological and semantic
greater extent for words belonging to the same category.           representations of words. Monaghan et al. (2011) trained
                                                                   associative learning models and human participants to map
                    General Discussion                             between phonological and semantic representations for
   The results indicate that the model adapted phonological        words belonging to one of two categories. They varied the
representations to become easier to map onto semantics. The        properties of the patterns in terms of whether the mappings
model’s general learning constraints shaped the phonology          were arbitrary or systematic between phonology and
of the language to make mapping to and from semantics              semantics, and also the extent to which additional
easier for future models to acquire. Investigating the actual      phonological cues provided information about the general
changes in the phonological representations from first to          category to which the word belonged. Learning was most
tenth generation revealed that this ease of learning was           accurate for both the associative learning model and the
accomplished through two primary changes in the                    behavioural results when the mapping between phonology
representations that resembled the design features of natural      and meaning was arbitrary, but with coherence at the
language.                                                          category level. The iterative modeling presented here
   First, the iterations of the language increased the             demonstrates that similar general purpose learning
similarity among words of the same category in terms of            mechanisms imposed by requirements to associate between
their phonological representation. This accords with               two sets of representations can result in an attuning of the
observations over the variety of phonological and prosodic         representations themselves to approximate this structure as a
cues that reflect grammatical categories of words cross-           consequence of constraints imposed in learning the language
linguistically (Farmer, Christiansen, & Monaghan; 2006;            being expressed in production.
Monaghan, Christiansen, & Chater, 2007). Indeed, for                  The model was trained with a starting language that was
English, there are now more than 20 distinct phonological          either fully arbitrary or largely systematic. These situations
and prosodic properties that relate to grammatical category        can be seen to resemble two theories of the origins of
distinctions (Monaghan & Christiansen, 2008).                      language, where words emerge either from articulatory
   The inverse model presented here showed that such               noise (Jespersen, 1922), or from iconic or sound-symbolic
coherence with respect to category structure can emerge as a       forms (Ramachandran & Hubbard, 2001). In both cases, we
consequence of pressures of learning. The inverse model            have shown that there is an increase in accuracy of
instantiated learning constraints into the representations         reproduction of the language across generations, and that
                                                               2665

this is coupled with generated systematicity at the category          gender-like subclasses. Journal of Memory and
level and greater arbitrariness in the form-meaning                   Language, 39, 218-245.
mappings within those categories. Future work may also              Gray, R. D., & Atkinson, Q. D. (2003). Language-tree
permit investigation into whether the emergent                        divergence times support the Anatolian theory of Indo-
pronunciations are more likely to result from iconic or noisy         European origin. Nature, 426, 435-439.
initial forms.                                                      Greenberg, J. H., ed. (1963). Universals of language.
   The starting point for this modeling approach was to                 Cambridge, MA: MIT Press.
demonstrate how learning may, over generations of learners,         Hinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M. (1995).
affect the structure of natural languages. In this respect, the       The wake-sleep algorithm for unsupervised neural
modeling demonstrates that “design features” of languages             networks. Science, 268, 1158-1161.
may fall under the remit of the cognitive sciences in               Hockett, C. F. (1960). The origin of speech. Scientific
explaining how and why such properties are observable                   American, 203, 89-96.
within language. Plaut and Kello (1999) demonstrated how            Jespersen, O. (1922). Language: Its nature, development
an inverse model can account for the development of                   and origin. London: Allen & Unwin.
segmental phonology – a contributor to the design feature of        Jordan, M. I., & Rumelhart, D. E. (1992). Forward models:
discreteness, and we have shown here how arbitrariness of             Supervised learning with a distal teacher. Cognitive
form-meaning mappings is an emergent property of                      Science, 16, 307-354.
constraints on learning in a similar model. Though the              Kirby, S. (2001). Spontaneous evolution of linguistic
model learns only a small set of patterns, and consequently,          structure - An iterated learning model of the emergence of
the results should be treated cautiously, the observations            regularity and irregularity. IEEE Transactions on
tally closely with computational and behavioural studies on           Evolutionary Computation, 5, 102-110.
learning effectiveness from different structures of a               Labov, W. (1994, 2001), Principles of linguistic change,
language’s vocabulary. The model presented here provides              Volume 1 Internal factors, Volume 2 Social factors.
an iterative step to showing how such design features can             Oxford: Blackwell.
emerge spontaneously within a learning system. Natural              Monaghan, P. & Christiansen, M.H. (2008). Integration of
languages may possess “design features”, then, not as                   multiple probabilistic cues in syntax acquisition. In
necessary, definitional properties, but rather because having           Behrens, H. (Ed.), Corpora in language acquisition
such structure facilitates learning, and over generations this          research: History, methods, perspectives. Amsterdam:
process of learning becomes impressed within the structure              John Benjamins.
of language itself.                                                 Monaghan, P., Christiansen, M. H., & Chater, N. (2007).
                                                                        The Phonological Distributional Coherence Hypothesis:
                         References                                     Cross-linguisitic evidence in language acquisition.
Braine, M.D.S. (1987). What is learned when acquiring                   Cognitive Psychology, 55, 259-305.
   word classes: A step toward an acquisition theory. In            Monaghan, P., Christiansen, M. H., & Fitneva, S.A. (2011).
   MacWhinney, B. (Ed.), Mechanisms of language                         The arbitrariness of the sign: Learning advantages from
   acquisition. London: Lawrence Erlbaum Associates.                    the structure of the vocabulary. Journal of Experimental
Carpenter, G. A., & Grossberg, S. (1987). Neural dynamics               Psychology: General, in press.
   of category learning and recognition: Attention, memory          Mumford, D. (1994). Neuronal architectures for pattern-
   and consolidation, and amnesia. In J. Davis, R. Newburgh           theoretic problems. In C. Koch & J. L. Davis (Eds.),
   & E. Wegman (Eds.), Brain structure, learning and                  Large-scale neuronal theories of the brain. Cambridge,
   memory: Westview Press.                                            MA: MIT Press.
Christiansen, M., & Chater, N. (2008). Language as shaped           Plaut, D. C., & Kello, C. T. (1999). The emergence of
   by the brain. Behavioral and Brain Sciences, 31, 489-509.          phonology from the interplay of speech comprehension
Christiansen, M. H., Collins, C., & Edelman, S. (2009).               and production: A distributed connectionist approach. In
   Language universals. NY: Oxford University Press.                  B. MacWhinney (Ed.), Emergence of Language.
de Saussure, F. (1916). Course in general linguistics. New            Hillsdale, NJ: Lawrence Earlbaum Associates.
    York: McGraw-Hill.                                              Ramachandran, V. S. & Hubbard, E. M. (2001).
Evans, N., & Levinson, S. (2009). The myth of language                Synaesthesia: A window        into   perception,   thought
   universals: Language diversity and its importance for              and language. Journal of Consciousness Studies, 8, 3-34.
   cognitive science. Brain and Behavioural Sciences, 32,           Scalise, S., Magni, E., & Bisetto, A. (Eds.). (2009).
   429-492.                                                           Universals of Language Today. Berlin: Springer.
Farmer, T.A., Christiansen, M.H., & Monaghan, P. (2006).            St Clair, M. C., Monaghan, P., & Ramscar, M. (2009).
   Phonological typicality influences on-line sentence                Relationships between language structure and language
   comprehension. Proceedings of the National Academy of              learning: The suffixing preference and grammatical
   Sciences, 103, 12203-12208.                                        categorization. Cognitive Science, 33, 1317-1329.
Frigo, L., & McDonald, J. L. (1998). Properties of
   phonological markers that affect the acquisition of
                                                                2666

