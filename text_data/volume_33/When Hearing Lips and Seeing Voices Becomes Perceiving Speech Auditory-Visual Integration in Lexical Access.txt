UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
When Hearing Lips and Seeing Voices Becomes Perceiving Speech: Auditory-Visual
Integration in Lexical Access
Permalink
https://escholarship.org/uc/item/67b773tk
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Ostrand, Rachel
Blumstein, Sheila
Morgan, James
Publication Date
2011-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

 When Hearing Lips and Seeing Voices Becomes Perceiving Speech: Auditory-Visual
                                                Integration in Lexical Access
                                          Rachel Ostrand (rostrand@cogsci.ucsd.edu)
                                University of California, San Diego, Department of Cognitive Science
                                      9500 Gilman Drive, #0515, La Jolla, CA 92093-0515 USA
                                     Sheila E. Blumstein (sheila_blumstein@brown.edu)
                         Brown University, Department of Cognitive, Linguistic, and Psychological Sciences
                                                  Box 1821, Providence, RI 02912 USA
                                        James L. Morgan (james_morgan@brown.edu)
                         Brown University, Department of Cognitive, Linguistic, and Psychological Sciences
                                                  Box 1821, Providence, RI 02912 USA
                             Abstract                                  McGurk Effect, in which incongruent audio and visual
   In the McGurk Effect, a visual stimulus can affect the              stimuli combine to induce in listeners the perception of a
   perception of an auditory signal, suggesting integration of the     stimulus different than that of the actual sound input they
   auditory and visual streams. However, it is unclear when in         have received. This effect is remarkable because of its
   speech processing this auditory-visual integration occurs. The      illusory status – the listener perceives a token that is distinct
   present study used a semantic priming paradigm to                   from the sound signal, even with a perceptually good
   investigate whether integration occurs before, during, or after     auditory exemplar. In this case, it is clear that the auditory
   access of the lexical-semantic network. Semantic associates
   of the un-integrated auditory signal were activated when the
                                                                       and visual signals are integrated at some point during speech
   auditory stream was a word, while semantic associates of the        processing.
   integrated McGurk percept (a real word) were activated when            Theories of lexical retrieval in speech comprehension
   the auditory signal was a nonword. These results suggest that       posit a mental lexicon as a repository of stored lexical items.
   the temporal relationship between lexical access and                This comprehension lexicon is an interconnected network of
   integration depends on the lexicality of the auditory stream.       words, each containing the phonological, syntactic, and
   Keywords: lexical access; McGurk Effect; auditory-visual            semantic information necessary for speech processing. To
   integration; lexical-semantic network                               understand spoken speech, the incoming speech signal must
                                                                       activate its entry in the lexicon to retrieve the meaning of an
                          Introduction                                 input word (Aitchison, 2003; Collins & Loftus, 1975). This
Speech comprehension is a complex, multi-staged process.               look-up process, using phonological input as a search key
Language input to the perceiver consists of information                for its corresponding meaning, is known as lexical access.
from several different sources which can augment the                   The present study investigates which components of the
auditory speech stream, including visual information from              incoming speech stream influence this search process.
the speaker’s mouth and lip movements, knowledge about                    In the case of McGurk Effect stimuli, for which
the speaker’s accent and pronunciations, eye and head                  participants perceive a stimulus different from that
movements to highlight referents, and tone of voice and                presented by the auditory stream alone, the differing
body language. While speech perception is most obviously               auditory and visual inputs were necessarily integrated at
driven by the auditory signal entering the listener’s ears             some point during speech processing. However, it is unclear
(Erber, 1975), visual information from a speaker’s mouth               whether this integration happens before, after, or
and lip movements can affect and even significantly alter              coincidently with lexical access. That is, does the lexical
the perception of speech (Fort et al., 2010; Green, 1998;              representation which is ultimately activated for processing
Summerfield, 1987), especially in noisy or degraded                    the speech input correspond to the auditory input alone, or
environments (Erber, 1975; Grant & Seitz, 2000; Sumby &                to the combined auditory-visual percept, which may differ
Pollack, 1954). To be able to derive this processing                   from that of the auditory signal? The study presented here
contribution from visual information, the auditory and                 investigates whether this combined percept is simply a
visual signals must be integrated into a single                        perceptual illusion that fails to access the lexicon, or if the
representation. The present work seeks to determine when               integrated percept is treated as input to the lexicon, thereby
such integration occurs during speech processing; in                   causing activation of its own semantic associates.
particular, whether it occurs before or after access to the               To create these integrated audiovisual-percepts, a video of
lexical-semantic network.                                              a speaker mouthing an item is dubbed with an auditory track
   McGurk and MacDonald (1976) first reported the                      differing in the initial consonant’s place of articulation.
                                                                   1376

Perceivers often perceive an item created in this manner not               lexicon, it could easily be the result of a hearing or speech
as the true auditory input, but as either a fusion of the                  segmentation error. Consequently, an accompanying visual
auditory and visual signals or just the visual track alone.                signal may be treated as additional disambiguating
(For example, an auditory [ba] paired with a visual /ga/                   information and thus taken more into account when
often fuses to form the percept da while auditory [ba] paired              interpreting the input of a nonword.
with a visual /da/ may also be perceived as da.1)                             The auditory and visual streams of a bimodal stimulus
   The phonological feature of place of articulation is more               enter the mind separately and independently and, at some
easily detected visually than are the features of manner and               point during lexical processing, are integrated to create a
voicing (Binnie, Montgomery, & Jackson, 1974) and is also                  single, unified percept, as in the McGurk Effect. The present
more susceptible to auditory noise interference (Miller &                  study investigates this integration process in relation to
Nicely, 1955). Thus, the manner-place hypothesis for                       lexical access. There are three possible points at which the
interpretation of incongruent audio-visual (AV) items                      auditory and visual tracks could be integrated: before, after,
suggests that the feature of place is contributed by the visual            or coincident with access to the lexicon. If AV-integration
stream while the manner and voicing features are                           occurs before lexical processing, namely, early in the
contributed by the auditory stream. The combination of                     perceptual stages of speech comprehension, then the
these three features leads to an AV percept that can be                    combined percept (not the auditory signal alone) should be
distinct from that of the actual auditory signal (MacDonald                treated as the input for the lexicon, and thus should access
& McGurk, 1978; Summerfield, 1987).                                        its own lexical-semantic entry and associates. This would
                                                                           also imply that AV-integration operates on purely bottom-
Visual Influences on Degraded Auditory Signals                             up information: if the streams are integrated before they are
Visual information can be particularly helpful for                         looked up in the lexicon, integration cannot be dependent on
comprehending speech when the auditory signal is less than                 the lexicality or non-lexicality of one or the other tracks.
ideal. For example, as the signal-to-noise ratio (SNR)                        An alternative possibility is that AV-integration occurs in
decreases, the improvement afforded by the addition of                     post-lexical stages of processing. In this case, the two
visual information strongly increases. Sumby and Pollack                   modalities would stay separated until one or both have been
(1954) presented participants with congruent, bimodal                      sent to the lexicon and either activated a match or not.
videos and asked them to identify the words they detected.                 Insofar as speech perception is fundamentally determined by
At extremely low signal-to-noise ratios (-30 dB), bimodal                  the auditory signal, any priming effects should be those
presentation increased lexical identification by 40                        created by the auditory stimulus. Only later, after the
percentage points; at moderate SNRs, the additional visual                 lexicon has been accessed, would AV-integration take place,
information only increased identification by 20 percentage                 leading to the fused item that comprehenders perceive. As a
points, and at 0 SNR the increase in rate of identification                result, the combined percept and the word or nonword it
was negligible. Similarly, combined auditory and visual                    forms would have no contact with the lexicon and thus its
speech presentation can withstand about a 5-10 dB worse                    lexicality would be irrelevant.
SNR than can auditory-alone presentation while still                          The final possibility is that AV-integration could occur
maintaining a level of 80% correct identification (Erber,                  during lexical access. In general, the two streams would
1975). As the speech signal becomes less reliable, less                    enter the lexicon separately, where the auditory stream
information about the input can be gleaned from the                        would likely be weighted more heavily as the primary
auditory signal alone and thus the visual track has more of a              modality of speech perception. If the auditory input is, for
chance to contribute. In line with this, Bastien-Toniazzo,                 some reason, less than ideal – whether because it is
Stroumza, & Cavé (2009) found higher incidence of                          degraded, or in noise, or not a real word – and thus cannot
McGurk Effect percepts in higher-noise environments,                       activate any lexical entry sufficiently to bring it to threshold,
implying that with greater background noise comes a greater                then any other available disambiguating information,
reliance on the visual signal, and thus a greater chance of                including the visual signal, could be used to help resolve the
integrating the two streams into a McGurk percept.                         identity of the input. As a result, if lexical access is delayed
   The same holds true for clear nonword auditory input.                   due to the poor quality of the auditory stimulus, AV-
Brancazio (2004) found a strong lexical bias for incongruent               integration could take place during this time and thus affect
McGurk videos, as the visual signal contributed more                       the lexicon search outcome.
frequently when the auditory signal was a nonword than
                                                                              To compare these possibilities, two types of audio-visual
when it was a word. A nonword audio track is, in a way,
                                                                           incongruent prime stimuli were used: auditory-word/visual-
comparable to a degraded stimulus – with no match in the
                                                                           nonword items, which, when integrated, lead to a nonword-
                                                                           percept, and auditory-nonword/visual-word items, which
  1
    Here, brackets ([X]) denote the auditory track of a stimulus;          integrate to form a word-percept. If AV-integration occurs
  slashes (/X/) the visual track; and italics (X) the illusory percept     pre-lexically so that the two streams are combined early in
  resulting from the combination of the auditory and visual signals.       processing, it is the combined McGurk percept that should
                                                                       1377

access the lexicon. In this case, the word-percept items          Design and Procedure
should prime their associates but the nonword-percept items       Participants were instructed to watch the videos and listen to
should not. Alternatively, if AV-integration happens later in     the item that followed each. The task was to make a lexical
the processing stream and is post-lexical, then priming           decision on the second, auditory item by pressing either the
should be dependent on the auditory input alone, and thus         “word” or “nonword” button on the button box placed in
word-percept stimuli (with an auditory nonword) should not        front of the subject. The assignment of word or nonword to
demonstrate priming while nonword-percept stimuli (with           each button was alternated between subjects. Participants
an auditory word) should.                                         were instructed to respond to the target word as quickly as
                                                                  possible. Stimuli were displayed in two blocks separated by
                           Methods                                a self-timed break.
Participants                                                         Each participant saw the same prime video twice across
                                                                  the experiment, paired with either both nonword or both
Twenty-six Brown University undergraduates who were               word targets. Importantly, each saw a McGurk and its
native English speakers and not fluent in any other               corresponding congruent prime with the same two targets,
languages participated in the experiment. Two subjects’ data      so the reaction times could be directly compared by item
had to be discarded due to instrument malfunction. The            within subject. Trials with the same prime were separated
remaining twenty-four participants ranged in age from 18 to       between blocks as were trials with the same target.
22 years, and all except one were right-handed. There were        Participants were given 7 practice trials at the start of the
13 males and 11 females in the group.                             task which were not included in the final data analysis.
Materials                                                                                      Results
Each stimulus consisted of a bimodal prime, with either           Reaction times (RTs) were measured from the offset of the
congruent or incongruent audio and visual streams,                target item to reduce any potential effects of differences in
followed, after a 50 msec ISI, by an auditory-only target.        the durations of the auditory targets, and were divided into
Bimodal primes were defined as congruent if their audio and       two sets by McGurk percept lexicality. The RTs in each set
visual tracks came from the same utterance, and incongruent       were further separated into four categories based on the
(McGurk) if they did not and thus the onset consonant             prime-target relationship: congruent-related, congruent-
presented in the signals did not match. Twenty-four of the        unrelated, incongruent-related, and incongruent-unrelated.
incongruent bimodal primes were auditory-word/visual-             Within each subject, any responses that were more than two
nonword stimuli and 24 were auditory-nonword/visual-word          standard deviations from the average RT of their category
stimuli. The congruent bimodal primes used the audio track        were removed, along with any items on which the
from the analogous McGurk videos paired with their                participant’s lexical decision response came before the onset
corresponding visual. For example, the McGurk video               of the target word or on which they made an incorrect
[beef]/deef/ had the corresponding congruent video                response. The average latencies for the remaining items in
[beef]/beef/. The initial consonant pairs used to create the      each category were computed within-subject. Reaction time
McGurk videos were [auditory-/b/, visual-/d/], [auditory-/p/,     results for congruent-nonword/incongruent-word items
visual-/t/], and [auditory-/m/, visual-/n/]. The intended         (NWW)2 are presented in Figure 1 and congruent-
McGurk percept formed by the incongruent videos was               word/incongruent-nonword items (WNW) in Figure 2.
always the same as the visual track. As a result, for the            A 2 (congruency) x 2 (relatedness) repeated-measures
incongruent stimuli, only one of the auditory and the             ANOVA was conducted by participants separately for the
McGurk percept was a real word, allowing for a clear              NWW items and for the WNW items. For the NWW
picture of which signal was the cause of any observed             items, there was a main effect of congruency (F(1, 23) =
priming effects.                                                  6.197, p<.020), indicating that incongruent trials, which
   Half of the audio-only targets were themselves evenly          created a real word percept (e.g., [bamp]/damp/, perceived
divided between semantically-related and unrelated words.
The other half of the targets were nonwords. The                    2
                                                                       Congruent-nonword/incongruent word-percept items (e.g.,
semantically-related target words were chosen from the              [bamp]/bamp/ and [bamp]/damp/) will be referred to as
University of South Florida Free Association Norms                  NWW. (This symbol will be used for both congruent and
database (Nelson, McEvoy, & Schreiber, 1998) and the                incongruent items.) This notation recalls the fact that in the
Edinburgh Associative Thesaurus (Kiss et al., 1973). Where          incongruent stimulus, a nonword auditory stimulus becomes a
the associates provided by these two databases were                 word-percept through AV-integration. As the auditory tracks are
                                                                    the same for the congruent and incongruent stimuli of a pair, the
nonexistent or the words deemed too long, associates were
                                                                    lexicality of the congruent item is denoted by the first item of the
provided by lab members. Nonword targets were chosen                pair (here, a nonword). Similarly, congruent-word/incongruent
from the ARC nonword database (Rastle, Harrington, &                nonword-percept items (e.g., [beef]/beef/ and [beef]/deef/) will
Coltheart, 2002) and were all one or two syllables long.            be denoted as WNW.
                                                              1378

                                                                  lexicality (F(1, 23) = 7.528, p<.012), with RTs faster to
                                                                  WNW stimuli than to NWW stimuli. This result
                                                                  suggests that the auditory signal takes precedence over the
                                                                  visual: stimuli that formed real words without integration
                                                                  seem to have been activated more quickly than those that
                                                                  became lexical items only through the integration of the
                                                                  visual input. There was no interaction between any pair of
                                                                  two factors or of the three factors together.
                                                                     All types of stimuli showed a significant effect of
                                                                  priming, as measured by strong main effects of relatedness
                                                                  in all comparisons.
                                                                                            Discussion
                  Figure 1: NWW items                            The goal of this study was to examine where in the
                                                                  processing stream auditory-visual integration occurs relative
                                                                  to lexical access. This question was investigated with regard
                                                                  to whether distinct auditory and visual tracks combine to
                                                                  form a McGurk Effect percept before or after the incoming
                                                                  signal is sent to the lexicon.
                                                                     Three possibilities exist as to when in lexical processing
                                                                  auditory-visual integration could occur: before accessing the
                                                                  lexicon, after it, or simultaneously. The data support a
                                                                  hybrid account, in which AV-integration and lexical access
                                                                  occur in parallel and are inter-dependent.
                                                                     The NWW items demonstrated a strong effect of
                                                                  congruency: reaction times to targets paired with
                                                                  [bamp]/damp/ primes were faster than reaction times to
                                                                  targets paired with [bamp]/bamp/ primes. This makes a case
                  Figure 2: WNW items                            for pre-lexical AV-integration. Both primes contained the
                                                                  same audio track, differing only by the fact that
as damp), elicited faster response latencies to targets than      [bamp]/damp/ creates a real-word integrated percept (damp)
did congruent nonword trials ([bamp]/bamp/, perceived as          while [bamp]/bamp/ remains a nonword (bamp). As reaction
bamp). Additionally and importantly, there was a main             times following word primes are known to be faster than
effect of relatedness (F(1, 23) = 32.905, p<.000),                reaction times following nonword primes (e.g., Milberg et
demonstrating that priming indeed occurred, as related            al., 1988), it seems to be the integrated, word-percept damp
targets were responded to more quickly than were unrelated        that accesses its lexical associates in the case of the
targets. There was no interaction (F(1, 23) = .744, p ns)         incongruent NWW stimulus, and thus AV-integration
between the factors.                                              occurs before lexical access.
   The congruent word/incongruent nonword-percept                    However, the WNW items showed no effect of
(WNW) stimuli behaved somewhat differently. As is                congruency: incongruent [beef]/deef/ and congruent
evident from Figure 2, there was no main effect of                [beef]/beef/ primes resulted in identical reaction times to
congruency (F(1, 23) = .030, p ns) – the congruent and            both related and unrelated targets. This result suggests that
incongruent stimuli resulted in identical latencies for both      AV-integration occurs after lexical access: as these items
related and unrelated prime-target pairs. There was, again, a     contained the same audio signal but differed in their visual
significant main effect of relatedness (F(1, 23) = 41.413,        signals, it appears that the auditory stimulus was driving the
p<.000). Unsurprisingly, there was no interaction (F(1, 23)       responses. Importantly, participants did integrate the
= .002, p ns).                                                    auditory and visual information and perceived the combined
   With these results in mind, a 2 (congruency) x 2               McGurk percept in both the incongruent WNW and
(relatedness) x 2 (percept lexicality) ANOVA was                  NWW cases – average goodness ratings as determined in
conducted including both stimulus types. There was a trend        a pilot experiment did not differ between these two groups
of a main effect of congruency, with RTs to incongruent-          of items.
prime stimuli nearly significantly faster than to congruent-         Taken together, the results for the NWW and the
prime stimuli (F(1, 23) = 4.153, p<.053). There was a strong      WNW stimuli suggest that the influence of the integrated
main effect of relatedness (F(1, 23) = 77.154, p<.000).           percept on lexical access depends on the lexical status of the
There was additionally a strong main effect of percept            auditory signal. When the auditory track is a word,
                                                              1379

integration takes place post-lexically so that the semantic           An important component here is that response times to
associates of the audio signal become activated and primed         targets may be based on a different input stimulus than what
– regardless of the congruency with the visual track.              the comprehender ultimately perceives. In general, with
Conversely, if the audio is a nonword, then integration            well-constructed McGurk stimuli, the comprehender should
occurs before lexical access is complete, such that the            perceive a fused item, with the manner and voicing
combined, incongruent (word) percept results in                    information contributed by the auditory track and the place
significantly faster response times than does the congruent        of articulation supplied by the visual. However, in the case
nonword stimulus. What’s going on here?                            of an auditory-word incongruent (WNW) stimulus, the
   In normal-hearing perceivers, speech comprehension is           auditory track activates its lexical representation and
mainly driven by the auditory signal, as evidenced by the          semantic associates in the lexicon before integration occurs.
fact that auditory-only input is significantly more                Thus the auditory signal determines the word actually
comprehensible than is visual-only input (Erber, 1975). In         activated in the lexicon while the combined audio and visual
fact, listeners are adept at ignoring a visual speech signal       information determines the item the comprehender believes
when it could not have been generated by the same                  she has received.
mechanism as the attended auditory speech stream and can              This account of auditory-visual integration predicts that
use the auditory information exclusively if the visual signal      the congruent auditory-word items (e.g. [beef]/beef/) should
is irrelevant or uninformative (Grant & Seitz, 2000). When         activate their lexical entries faster than the incongruent
presented with both auditory and visual information, the two       word-percept items (e.g. [bamp]/damp/), which must wait
streams, by virtue of the fact that they come from different       for integration to take place before the lexical entry for the
modalities, enter the lexicon initially separate. While the        integrated percept can be activated. To test for this, a
two streams are still in the process of being integrated, a        congruency x relatedness x percept lexicality ANOVA was
lexical search begins on the auditory signal, due to its           conducted on the reaction time data. A strong main effect of
privileged status in speech comprehension.                         lexicality emerged, with WNW primes resulting in
   If the auditory signal is a word, it maps onto and activates    significantly faster reaction time latencies than NWW
that entry in the lexical-semantic network, thus priming its       primes. The WNW items, composed from real-word
semantic associates. In this case, once a match has been           auditory signals, could locate their input word in the lexicon
found and a word has been selected, the integrated signal          pre-integration and thus spread activation to associated
does not have a chance to influence lexical activation and         words before the AV-integration occurred, regardless of
selection. The actual integration of the auditory and visual       congruency. In contrast, incongruent NWW items, while
information takes somewhat longer to complete than the             also resulting in a word percept, must wait for integration to
spread of activation from the independent signals does, and        occur before successfully finding a match in the lexicon,
thus occurs after the lexicon has already selected a word on       thus resulting in slower response times. The congruent
the basis of the auditory signal alone.                            NWW items were simply nonwords and therefore result
   The process begins in the same manner when the auditory         in slower RTs as well.
signal is less than ideal – either because it is degraded,            This account also explains why all four types of
presented in a noisy environment, or is a nonword. Again,          combinations showed equivalent related-unrelated priming,
the auditory and visual signals enter the lexicon                  regardless of congruency or lexicality. The congruent and
independently and not integrated, and the auditory signal          incongruent WNW ([beef]/beef/ and [beef]/deef/) items
spreads through the lexicon activating the sound structure         should cause the same amount of priming as each other, as it
and meaning it encodes. However, when the auditory input           is the identical auditory signal that is selected in the lexicon
is a nonword, there is no matching lexical entry for it to         and thus the identical pattern of associates which is
activate. There is some partial activation of the nonword's        facilitated. For the incongruent NWW ([bamp]/damp/)
phonological neighbors, but not enough to bring any                items, the integrated word percept activates its associates
individual word quickly to threshold. While this insufficient      and thus results in the same amount of semantic facilitation
activation spreads through the lexicon, auditory-visual            as do the auditory-word items. Milberg and colleagues
integration has a chance to complete. As no word has yet           (1988) found a strong effect of mediated phonological-to-
reached threshold and been selected, when the signal from          semantic priming. A nonword prime one phonological
the combined percept accesses the lexicon, it activates the        feature away from a real word elicited no difference in
integrated McGurk word. As a result, when the auditory             facilitation levels to a semantically-related target than did
signal cannot activate any one lexical entry enough to bring       the real word prime itself. For example, gat, which differs
it to threshold before AV-integration takes place, and this        from cat only by the initial consonant’s place of articulation,
integration results in a real word, it is the integrated           primed dog to nearly the same extent that cat did. The
percept’s representation that is activated, leading to faster      congruent NWW stimuli in this study (e.g.
response times following incongruent word-percept primes           [bamp]/bamp/), while not forming real words, differed from
than the corresponding congruent nonword primes.                   real words by only a single feature; namely, the initial
                                                               1380

consonant’s place of articulation, and thus, unsurprisingly,             Journal of Speech and Hearing Disorders, 40, 481-492.
result in equivalent priming difference scores.                        Fort, M., Spinelli, E., Savariaux, C., and Kandel, S. (2010).
   This account suggests some future directions for research.            The word superiority effect in audiovisual speech
An important next step would be to repeat the study with                 perception. Speech Communication, 52, 525-532.
incongruent prime stimuli consisting of both auditory and              Collins, A.M. & Loftus, E.F. (1975). A spreading-activation
visual real words and a target semantically related to one of            theory of semantic processing. Psychological Review, 82,
them (e.g., prime: [bait]/date/; target: [fish] or [time]).              407-428.
According to the present account, while the listener’s                 Ganong, W.F. (1980). Phonetic categorization in auditory
perception may be that of the visual (i.e., integrated) signal,          word perception. Journal of Experimental Psychology:
the semantic associates of the auditory signal should be                 Human Perception and Performance, 6, 110-125.
primed. That is, we should observe significantly more                  Grant, K.W., & Seitz, P.F. (2000). The use of visible speech
facilitation for [bait]/date/–[fish] than for [bait]/date/–[time].       cues for improving auditory detection of spoken
   Additionally, varying the interstimulus interval between              sentences. Journal of the Acoustical Society of America,
prime and target items may produce different patterns of                 108, 1197–1208.
results. Our account predicts that an extremely short ISI              Green, K.P. (1998). The use of auditory and visual
may leave no time for AV-integration of the prime before                 information during phonetic processing: Implications for
the target plays, and thus abolish the priming effect found              theories of speech perception. In R. Campbell, B. Dodd,
for incongruent NWW items. Conversely, a longer ISI                     & D. Burnham (Eds.), Hearing by Eye II: Advances in the
might remove any reaction time differences between                       Psychology of Speechreading and Auditory-Visual
incongruent NWW items and WNW items as the two                         Speech. Hove, England: Psychology Press.
streams would have sufficient time to integrate before the             Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J. (1973).
lexical decision on the target had to be made.                           An associative thesaurus of English and its computer
   In sum, the present study suggests that audiovisual                   analysis. In Aitken, A.J., Bailey, R.W. and Hamilton-
integration occurs in parallel with lexical access. The                  Smith, N. (Eds.), The Computer and Literary Studies.
auditory signal of a bimodal input is weighted more heavily              Edinburgh: University Press.
as its activation moves through the lexicon, but if no lexical         MacDonald, J. & McGurk, H. (1978). Visual influences on
match is found by the time AV-integration occurs, the                    speech      perception      processes.    Perception     &
combined percept becomes the search item in the lexicon                  Psychophysics, 24, 253-257.
and can activate its semantic associates.                              McGurk, H. & MacDonald, J. (1976). Hearing lips and
                                                                         seeing voices. Nature, 264, 746-748.
                     Acknowledgments                                   Mertus J.A. (2008). BLISS: The Brown Lab Interactive
The authors thank Elena Tenenbaum and Naomi Feldman                      Speech System. Brown University; Providence, RI.
for providing many useful suggestions, John Mertus for his             Milberg, W., Blumstein, S., & Dworetzky, B. (1988).
assistance with the technical components of the experiment,              Phonological factors in lexical access: Evidence from an
and Liz Chrastil for appearing as the model in the video                 auditory lexical decision task. Bulletin of the
stimuli. This work was conducted while the first author was              Psychonomic Society, 26, 305-308.
at Brown University. It was funded in part by NIH grants               Miller, G.A. & Nicely, P.E. (1955) An analysis of
DC00314 to SEB and HD32005 to JLM.                                       perceptual confusions among some English consonants.
                                                                         The Journal of the Acoustical Society of America, 27,
                          References                                     338-352.
Aitchison, J. (2003). Words in the Mind: An Introduction to            Nelson, D. L., McEvoy, C. L., & Schreiber, T. A. (1998).
   the Mental Lexicon. Malden, MA: Blackwell.                            The University of South Florida word association, rhyme,
Bastien-Toniazzo, M., Stroumza, A., Cavé, C. (2009).                     and word fragment norms.
   Audio–visual perception and integration in developmental              http://www.usf.edu/FreeAssociation/.
   dyslexia: an exploratory study using the McGurk effect.             Rastle, K., Harrington, J., & Coltheart, M. (2002). 358,534
   Current Psychology Letters [Online], 25, URL:                         nonwords: The ARC Nonword Database. Quarterly
   http://cpl.revues.org/index4928.html                                  Journal of Experimental Psychology, 55A, 1339-1362
Binnie, C.A., Montgomery, A.A., & Jackson, P.L. (1974).                Sumby, W.H. & Pollack, I. (1954). Visual Contribution to
   Auditory and Visual Contributions to the Perception of                Speech Intelligibility in Noise. Journal of the Acoustical
   Consonants. The Journal of Speech and Hearing                         Society of America, 26, 212-215.
   Research, 17, 619-630.                                              Summerfield, Q. (1987). Some preliminaries to a
Brancazio, L. (2004). Lexical influences in audiovisual                  comprehensive account of audio-visual speech
   speech perception. Journal of Experimental Psychology,                perception. In B. Dodd & R. Campbell (Eds.), Hearing by
   30, 445-463.                                                          Eye: The Psychology of Lip-Reading. Hillsdale, NJ:
Erber, N. (1975). Auditory-Visual Perception of Speech.                  Lawrence Erlbaum Associates Ltd.
                                                                   1381

