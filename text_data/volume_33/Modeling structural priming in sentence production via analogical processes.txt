UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling structural priming in sentence production via analogical processes
Permalink
https://escholarship.org/uc/item/59k3p18d
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Taylor, Jason
Friedman, Scott
Goldwater, Micah
et al.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

       Modeling structural priming in sentence production via analogical processes
                                    Jason L. M. Taylor (jason.taylor@u.northwestern.edu)
                                       Scott E. Friedman (friedman@northwestern.edu)
                                          Kenneth Forbus (forbus@northwestern.edu)
                            Qualitative Reasoning Group, Northwestern University, 2133 Sheridan Road
                                                          Evanston, IL 60208 USA
                                             Micah Goldwater (micahbg@gmail.com)
                                          Dedre Gentner (gentner@northwestern.edu)
                              Department of Psychology, Northwestern University, 2029 Sheridan Road
                                                          Evanston, IL 60208 USA
                              Abstract                                  increased likelihood that the participant's description of the
                                                                        scene (the target utterance) would use a DO as in 3 (rather
   Recently there has been a surge of interest in using structural
   priming to examine sentence production. We present an                than a PP as in 4).
   analogical model of sentence production that exhibits                   3. The girl told her friend a story
   structural priming effects. It uses analogical generalization to        4. The girl told a story to her friend.
   acquire abstract language patterns from experience. To                  Structural priming is seen as evidence of abstract syntax
   construct utterances, it uses analogical retrieval to find           because it can operate across semantically different
   semantically similar utterances and generalizations, and             utterances and across intervening sentences (Bock, 1986;
   constructs a new sentence by analogy to them. Using the
   stimulus generator of Chang et al (2006), we show that this
                                                                        Chang, Bock, & Goldberg, 2003; Thothathiri & Snedeker,
   model can exhibit structural priming effects similar to those        2008). Thus the development of structural priming in
   observed in humans, but with orders of magnitude less prior          children has been used to mark the development of syntactic
   experience than required by a previous simulation.                   abstraction (e.g., Savage et al., 2003). Indeed, Chang et al.
   Keywords: structural priming; sentence production; syntax
                                                                        (2006) have suggested that the mechanisms underlying
   acquisition; analogy.                                                structural priming are the same mechanisms involved in
                                                                        learning grammar. Pickering and Garrod (2004) additionally
                          Introduction                                  suggest that structural priming is one mechanism by which
                                                                        conversational fluency between interlocutors is achieved.
     What mechanisms underlie sentence production? In
                                                                           Two highly influential models, by Chang et al. (2006) and
particular, how do speakers choose among the multiple
                                                                        by Pickering & Garrod (2004), each account for many of the
grammatical forms that are capable of expressing something
                                                                        phenomena of structural priming. However, research by
they intend to convey? Recently, there has been a surge of
                                                                        Goldwater et al. (2011) shows that some phenomena of
interest in structural priming as a way to examine sentence
                                                                        structural priming can best be captured by using the
production processes in adults and children (Bock, 1986;
                                                                        mechanism underlying analogical reasoning—structure-
Bock & Griffin, 2000; Chang, Dell, & Bock, 2006; Kaschak
                                                                        mapping (Gentner, 1983; Gentner & Markman, 1997).
& Borreggine, 2008; Savage et al., 2003). In structural
                                                                           We propose that structural priming can be modeled as a
priming, the structure of one sentence is repeated in the
                                                                        species of analogy. This proposal might seem surprising,
structure of a second sentence (Bock, 1986). Structural
                                                                        given that analogy is often considered to be a conscious
priming occurs without any intention to create syntactic
                                                                        phenomenon while priming is clearly implicit. However,
parallelism. It does not require semantic or thematic overlap
                                                                        recent results show that structure-mapping from a prior
between the utterances, although the effects can be stronger
                                                                        analog can occur without attention or awareness (Day &
when lexical items are repeated, and sentences are
                                                                        Gentner, 2007). We describe an initial computational model,
semantically similar (Branigan, Pickering, & Cleland, 2000;
                                                                        based on analogical processes of matching, generalization,
Goldwater et al., 2011; Hare & Goldberg, 1999; Pickering
                                                                        and retrieval. To provide a solid basis for comparison, we
& Garrod, 2004; Snider, 2009).
                                                                        use the experimental design and stimulus generator
   To illustrate, consider a scene of a man giving cake to his
                                                                        developed by Chang et al (2006).              We begin by
son. It could be described either by a double object dative
                                                                        summarizing the psychological experiments and the Chang
construction (DO), as in 1, or by a prepositional dative
                                                                        et al (2006) model. We then describe our analogy-based
construction (PP), as in 2.
                                                                        simulation, including its structure and operation. The
   1. The man gave his son some cake.
                                                                        results of three simulation experiments are presented.
   2. The man gave some cake to his son.
   If an experimenter describes this scene with the DO (the
prime utterance), and then shows a picture of a girl telling
her friend a story, structural priming would be shown by the
                                                                    2916

          The Chang et al. dual-path model                          was used to simulate other phenomena that we eventually
                                                                    plan to simulate as well, but these are our current focus.
In a typical structural priming experiment (e.g., Bock &
                                                                       The success of the dual-path model in simulating
Griffin, 2000) participants alternate between prime trials, on
                                                                    structural priming phenomena is very impressive. It has set
which they hear and then repeat a sentence, and target trials,
                                                                    a standard against which future models of structural priming
on which they are given a depicted scene to describe in any
                                                                    will be measured. We use this model as a basis for
way they choose. For example, in Bock and Griffin’s
                                                                    calibrating our analogy based model, showing that we can
(2000) Experiment 1 there were 48 such sequences. Any
                                                                    capture some of the same phenomena with many fewer
prime sentence would be in one of two syntactic alternates,
                                                                    training examples.
e.g., the DO or PP dative, and the dependent measure is the
frequency of matching the structure of the prime in the
target scene description vs. using the alternate structure.                  Analogical learning and priming of
   Chang et al. (2006) present a connectionist model of                                      constructions
sentence production, the dual-path model, which simulates              Our model uses analogical processing in both the training
several structural priming phenomena.             Their model       phase and the priming phase. Training is modeled as
includes one system for representing the message, i.e., the         analogical generalization (using SAGE, described below).
meaning of the sentence, and a second system for producing          During testing, when a target message is presented,
sentence structure from the message. Before simulating              analogical retrieval (via MAC/FAC) is used to efficiently
structural priming, the model was trained with 60,000               retrieve a small number of utterances (or generalizations)
message-sentence pairs, each consisting of a meaning and a          from memory that overlap in content with the target. Then
word sequence for that meaning. Using error-based learning,         analogical mapping (SME) is used to map their sentence
the model learned to produce grammatical word sequences             structure onto the role bindings in the target message.
when given a message. The model was then tested using                  We now review the components of our model. The major
conditions that mirrored structural priming experiments. In         components—SME, MAC/FAC, and SAGE—were
the prime trials, the model received both a message and a           developed prior to this study, and have been shown to be
sentence structure. In the target trials, only the message was      useful in modeling other analogy-driven phenomena. We
given to the model. On every prime sentence, the weights            begin with SME, which underlies the others. Then we
between nodes in the sequencing system were updated                 discuss analogical generalization via SAGE, which here
based on prediction error, just as in training.                     models prior language learning. Finally, we turn to retrieval,
   The stimulus-producing grammar consisted of a set of             which (along with mapping) is used to model priming.
message-sentence templates corresponding to the kinds of               Mapping: The Structure-Mapping Engine (SME)
constructions used in the experiments (see Table 1 for              (Falkenhainer, Forbus, & Gentner, 1989) is a computational
examples). Random satisfactory fillers were chosen from a           model of Gentner’s (1983) structure-mapping theory of
small fixed lexicon of concepts and bound to empty abstract         analogy.      Its inputs are base and target structured
thematic role slots in the message portion of the template, as      representations. Its output is one or more mappings that
well as to the event-semantic categories indicating the tense       describe how the two descriptions can be aligned (where
(e.g. present, past) and aspect (e.g. progressive) of the event     alignment requires finding a like relational structure in
represented. Their model uses a XYZ thematic role                   which the relations match identically).1 Each mapping
representation scheme, wherein the roles roughly                    consists of a set of correspondences linking elements from
correspond to agent, theme/patient, and recipient/location,         the base and target, a score based on the degree of overlap
respectively. The corresponding word strings for the                between them, and candidate inferences that represent
concepts in the message were then bound to corresponding            hypotheses about what elements in one description could be
slots in the sentence template. Finally, a small set of             projected to the other, based on the correspondences for that
transformations (e.g. morphemes for tense) were applied to          mapping. SME is used as a component in the other two
produce grammatical sentences for the given sentence type.          analogical processes, and is also used here to generate word
   Every structural priming test set took 100 prime–target          sequences to describe new utterances.
message pairs. Each target message was presented twice,                Retrieval: MAC/FAC (Forbus, Gentner, & Law, 1995)
preceded each time by a prime with the same message but             models similarity-based retrieval over structured
with a different syntactic alternate. There were two versions       representations. Its inputs are a probe case and a case
of each target message with a built-in bias towards one of          library. The first stage of MAC/FAC rapidly retrieves up to
the alternates, creating 4 trials per prime-target message          three candidate matches using a crude parallel vector match,
pair, yielding 400 total prime-target trial sequences.              where the vectors are automatically constructed from the
   Chang et al. (2006)’s model was able to capture several          structured representations. The second stage uses SME in
key phenomena. They simulated priming both the dative               parallel to compare the probe to the structured
alternation and the passive/active alternation. These               representations for the candidates produced by the first
constructions remained primed across multiple filler items
between prime and target, as in people (Bock & Griffin,                1
                                                                         Under some circumstances, nonidentical relations are re-
2000). (see Table 1 for examples). The dual-path model
                                                                    represented to find identical subcomponents.
                                                                2917

stage, returning the best mapping (or up to three, if very          utterance expressing a transfer of cake from father to son in
close) as the reminding for that probe.                             the double-object dative (DO) form “The man is giving his
   Generalization: SAGE (Kuehne et al., 2000) models                son some cake”, should lead to an increased likelihood for
analogical generalization. It begins by storing the first input     the system to produce the utterance “the girl is telling her
example (here, a message-sentence pair). When the next              friend a story”, rather than “the girl is telling a story to her
example arrives, SAGE compares it to the first one, using           friend”. Absent a priming example of this kind, the system
SME. If there is sufficient overlap (that is, if SME’s score is     should still be able to produce an utterance that conveys the
above a pre-set threshold) the common structure is stored as        target meaning by retrieving a generalization or exemplar
a generalization. SAGE uses MAC/FAC to retrieve                     from its LTM with a similar meaning.
generalizations and/or examples similar to new inputs. New             To populate the model, a set of sentences paired with their
examples are assimilated into existing generalizations if           meaning was generated using an input environment
sufficiently similar, and the generalization is updated based       grammar and simple lexicon based on those used in Chang
on their common structure. Otherwise, if the new example            et al. (2006). We used the same grammar and lexicon as
is very similar to a retrieved example, a new generalization        their generator, and compared the results of our generator to
is formed from their common structure. Finally, if the new          theirs to ensure that the sets of meaning-sentence pairs we
example is not sufficiently similar to anything retrieved, it is    produced were essentially the same. Some of these
stored separately, and may serve as a seed for another              meaning-sentence pairs were set aside as stimuli to use in
generalization later.                                               the priming experiments, with a distinct set used to train the
   In essence, this process of progressive alignment leads to       model, as described below. Next we describe how these
the gradual wearing away of the non-overlapping aspects of          pairs were encoded by our simulation, and the training
the examples. SAGE’s generalizations are structured                 process it underwent.
representations. They may also include some specific
features, though generally many fewer than in the input                Table 1: Sentence types included in the input environment
representations. No variables are introduced. Further, the                                     grammar
assimilation process produces probabilities attached to each
statement in the description, indicating its frequency within               Sentence type                  Example Sentence
the generalization. For each concept to be learned, the set of      Animate intransitive             “a man jump –ed”
generalizations and exemplars learned so far constitutes its        Animate with intransitive        “the girl walk –s with a dog”
generalization context.                                             Inanimate intransitive           “the ball bounce –s”
                                                                                                     “a father is go –ing around a
   An Analogical Model of Structural Priming                        Locative transitive
                                                                                                     car”
   In our model, target utterances are produced by retrieving       Theme-experiencer                “a uncle scare –s a cat”
utterances (or generalizations) from memory whose                                                    “the grandfather carry –ed a
                                                                    Cause-motion
meaning is similar to that of the given target meaning and                                           cup to the store”
mapping their sentence structure onto the target.                                                    “a woman give –s a girl a
                                                                    Transfer dative
   The system’s memory has a short-term as well as a long-                                           apple”
term component, in order to simulate the greater availability                                        “a man bake –ed a cake for
                                                                    Benefactive dative
of more recently encountered utterances. A buffer of                                                 the mother”
messages, each linked to its sentence representation, is                                             “the boy push –ed a chair for
                                                                    Benefactive transitive
stored in the system’s Short Term Memory (STM); these                                                the man”
serve as priming utterances, as well as “filler” or distractor                                       “a cat plug –s a sink with a
                                                                    State-change
utterances. Given the message of a target utterance as input,                                        ball”
the system uses analogical retrieval with the STM as the                                             “the father spray –ed paint
case library to find similar messages. Failing to find a                                             on the wall”
                                                                    Locative alternation
semantically similar utterance in STM, the system uses                                               “a uncle loaded –ed a plate
MAC/FAC with the system’s LTM as the case library. The                                               with pie”
LTM consists of the SAGE generalization context, that is,
the generalizations and ungeneralized exemplars produced
during the training phase (described below). SME is then                Structure, Structural Priming & Sentence
used to infer a sequence of words that situates the actors and                               Production
objects of the target utterance’s meaning in their
                                                                       Analogical processing assumes that people use structured,
corresponding roles.
                                                                    relational representations.         Our input encodings,
   Returning to the prior example, the intended behavior of
                                                                    automatically produced from message-sentence pairs, reflect
the model is as follows: The system is given a
                                                                    a reasonable approximation to what people would encode in
representation of an event in which a girl is telling her friend
                                                                    similar situations. A complete example of the message-
a story. In the structural priming condition, the STM
                                                                    sentence pair representation used by our model can be seen
contains meaning-sentence pairs. The presence of a prior
                                                                    in Figure 1. Sentence structure is represented by a series of
                                                                2918

word slot entities (e.g. w1), each corresponding to a word in      retrieved, SME’s alignment of that message-sentence pair
the       sequence      (e.g.      (isa       w1       (WordFn     with the input message produces candidate inferences
“grandmother”))). Sequentiality in the sequence is                 representing hypotheses about the structure of the target
represented by a set of relationships between word slots.          sentence. These candidate inferences are used to produce
Semantic structure is represented by entities representing         sentence structure for the target, by projecting word
abstract thematic role fillers (e.g. x0), whose                    information and order relationships from the retrieved
interrelationships are described via binary relations (e.g.        utterance (or generalization) to the description of the target
(roleX a0 x0)). The referential structure ties the                 message.
thematic roles to their corresponding word slot in the
sentence       structure,      e.g.     (sameObject         x0                       Priming Experiments
(WordReferentFn w1)). The use of words in the                         We next evaluate the model’s ability to produce sentences
semantic      structure     (e.g.    (isa      x0      (WordFn     from messages without primes (Experiment 1) and with two
“grandmother”))) follows the Chang et al model, which              kinds of priming alternations (Experiments 2 and 3). All
used words rather than internal concepts as fillers in their       three studies used the LTM generated by SAGE as
meaning representation. Consequently, we did the same, in          described above.
order to keep the simulations as comparable as possible.
                                                                   Experiment 1
                                                                   In Experiment 1, we tested the model’s production in two
                                                                   LTM-only conditions: a dative production condition and a
                                                                   transitive production condition. This examines the model’s
                                                                   ability to select a proper grammatical form for messages in
                                                                   the absence of specific prime sentences in STM. In each
                                                                   condition, the model was given a sequence of 50 examples
                                                                   of messages corresponding to the given construction type
                                                                   and required to produce a sentence for each. As noted
                                                                   above, this means that the model will use MAC/FAC to
                                                                   retrieve generalizations and exemplars from the LTM
                                                                   contents produced via SAGE to do the generation.
                                                                      We applied a twofold evaluation to the output of our
                                                                   model, similar to that used by Chang et al. (2006). Each
                                                                   sentence produced by the model is evaluated in terms of its
                                                                   grammaticality and its message accuracy. Grammaticality
     Figure 1: Interrelations between semantic and sentence        measures the degree to which the output sentence matches
        structure through the referential structure layer.         the prototype defined in the input environment grammar.
                                                                   Message accuracy measures the degree to which the
Seeding LTM via analogical generalization                          semantic message retrieved from memory maps to the target
  Prior to the priming tests, a training procedure was run to      message given as input. The results are summarized in
seed the system’s Long Term Memory (corresponding to               Figure 3. For both kinds of constructions the model’s
Cheng et al.’s Training phase). Our training utilized              message accuracy and grammaticality is quite high. Even
analogical generalization via SAGE. Five examples of each          with an extremely limited training set, our analogy-based
of the 24 variants of the 11 construction types in the input       model produces sentences conforming to the input grammar.
environment grammar were produced: 120 message-
sentence pairs in total. These stimuli were incrementally          Experiment 2
generalized by SAGE, using a similarity threshold of 0.9.          Next we tested the model’s performance when presented
This resulted in 45 separate generalizations of message-           with a dative prime from one of two alternates: the
sentence pairs and 15 concrete, ungeneralized message-             prepositional form and the dative form. We also wished to
sentence exemplars. SAGE required just one pass through            test whether the model would capture the finding that
the 120 examples, which is two orders of magnitude less            structural priming can persist across intervening sentences
exposures than the dual-path model required.                       (Bock & Griffin, 2000). Therefore we varied whether there
                                                                   were intervening intransitive filler sentences in STM. This
Sentence production                                                led to a 2X2 design: Alternative constructions (prepositional
Given a new semantic message mi, a prime pi, and a set of          dative vs. double-object dative) crossed with Filler
filler message-sentence pairs the prime and fillers are stored     conditions (no fillers vs. intransitive fillers). In each
in STM. Then, the system uses MAC/FAC to find the most             condition the model was given a sequence of 100 prime-
similar semantic message to mi from among the messages             target pairs with dative messages. The prime message-
present in STM, and if that fails, MAC/FAC is used on the          sentence stimulus was stored in STM and the system was
LTM. In either case, once a sufficiently similar message is        required to produce an appropriate sentence for the target
                                                               2919

message. In the no-filler condition, no additional message-         between prime and target utterances across structurally
sentence pairs were entered into STM. In the intransitive           dissimilar fillers, matching human findings.
filler condition, 10 intransitive stimuli were entered into            These findings provide evidence for the viability of
STM in addition to the prime stimulus. For both kinds of            analogical mechanisms in learning constructions and in
constructions, and with fillers and no fillers in STM, the          applying them to form utterances. That analogical processes
model matched the sentence structure of the prime in every          readily accommodate both learning and priming phenomena
trial. That is, the model was able to find a proper match in        is in accord with the idea that the two phenomena are
STM on every trial and to map its structure to the target           intimately related, as suggested by Chang et al (2006). We
without using the LTM store of sentences.                           now discuss these two aspects in more detail, including both
                                                                    implications and limitations of the current model. We begin
                                                                    with structural priming and then turn to grammar learning.
                                                                    Structural priming
                                                                       While the strong priming effects our model shows is
                                                                    encouraging support for analogical mapping as a mechanism
                                                                    of structural priming, in some sense the model’s
                                                                    performance is too good. Across Experiments 2 and 3, over
                                                                    90% of the targets conform to the structure of the prime.
                                                                    Priming effects are typically much smaller in humans; in
                                                                    general, roughly 60% of targets conform to the prime. We
                                                                    believe there are two reasons for this. The first is that we
                                                                    only consider structural priming, and not other types of
                                                                    constraints, such as distributional and semantic preferences
                                                                    connected with individual words and phrases, pragmatic
     Figure 3: Sentence production performance of model in          constraints, and discourse constraints that enter into
                 LTM-only retrieval condition                       construction selection in natural language use (e.g., Bresnan
                                                                    et al., 2007). Chang et al. (2006) dealt with this issue by
Experiment 3                                                        building in a bias into every message towards a particular
Next we tested the model’s performance when presented               construction; these bias effects can act as a competing (or
with transitive primes that were either active or passive. The      facilitating) force on priming. We are exploring ways to
experiment used the same basic 2x2 design and procedure as          capture these effects. The other reason may be the overly
in Experiment 2, and the same number of prime-target                strong reliance on an STM buffer in the current model.
message pairs. As usual, the system first attempted to              Recall that analogical retrieval is used on LTM only when
retrieve a structural match from its STM before retrieving          retrieval on STM fails. This happened only three times
from its LTM.                                                       across Experiments 2 and 3. We suspect that reducing the
  For both active and passive priming conditions without            bias towards STM retrieval, or even eliminating the STM-
fillers, the model matched the target structure to the prime        LTM distinction entirely, might more closely match human
for 100 out of 100 trials using the STM store. When there           data. Such a model would take into account both recency
were fillers, the model was able to do so for 98 of the             (thereby favoring STM) and strength of generalization
passive trials, and 99 of the active trials. That is, LTM was       (favoring LTM).
used as a basis for target sentence structure a total of 3 times
across 400 trials. The model produced grammatically                 Learning grammatical patterns
appropriate sentences with both STM and LTM retrievals.                An intriguing result is the effectiveness of analogical
                                                                    generalization, as modeled by SAGE, in learning
                                                                    grammatical patterns. SAGE was given only one pass
                         Discussion
                                                                    through 120 example message-sentence pairs, yet it
   These experiments show that our analogy-based model is           produced a set of generalizations (along with some isolated
(1) capable of forming generalizations over meaning-                examples) that was sufficient to support the construction of
sentence pairs; (2) able to use its learned memory of               grammatically and semantically accurate sentences over
generalizations and exemplars to produce sentences                  90% of the time. In contrast, the dual-path model required
conforming to the input grammar when given a meaning                8,000 examples, each trained an average of 7.5 times--
(Experiment 1); (3) able to match the structure of prime            around 60,000 trials.
sentences for either the dative alternation (Experiment 2) or          Why is our analogical model of construction
the active/passive transitive alternation (Experiment 3). As        generalization so effective? In an important sense, we
per human data, the presence of intransitive fillers had            believe this finding is real: Structural alignment and
minimal effect on the effects of a prime. The model can             abstraction is a highly effective way of extracting common
simulate structural priming when there is no lexical overlap        relational structure. For example, Kuehne et al. (2000) used
                                                                2920

SEQL (a predecessor of SAGE) to simulate the Marcus et              Childers, J. B. (2008). The structural alignment and
al. (1999) studies, in which 7-month old infants abstracted a         comparison of events in verb acquisition. Proceedings of
grammar-like rule from exemplars. The model required only             the 30th Annual Cognitive Science Society. Austin ,TX
the amount of exposure given to the infants—16 strings              Day, S. B., & Gentner, D. (2007). Nonintentional analogical
repeated 3 times each, a total of 48 strings.                         inference in text comprehension. Memory & Cognition,
   However, the obvious challenge to our results is that              35(1), 39-49.
children do not master grammar in 120 utterances, nor even          Falkenhainer, B. F., Forbus, K. D., & Gentner, D. (1989).
after many thousands. We suggest that a major source of the           The Structure Mapping Engine: Algorithm and examples.
disparity lies in the nature of the input. We can characterize        Artificial Intelligence, 41, 1-63.
learning environments on a continuum from high-alignable            Forbus, K., Gentner, D., & Law, K. (1995). MAC/FAC: A
to low-alignable. In a high-alignable environment, the                model of Similarity-based Retrieval. Cognitive Science,
learner encounters juxtaposed alignable pairs, as in the              19(2), 141-205.
Marcus et al. studies. Lab studies show dramatic learning           Gentner, D. (1983). Structure-mapping: A theoretical
under these conditions (Gentner, 2010). On the other hand,            framework for analogy. Cognitive Science, 7, 155-170.
children’s language learning takes place in a low-alignable         Gentner, D., & Markman, A. B. (1997). Structure mapping
environment; they only occasionally receive perfectly                 in analogy and similarity. American Psychologist, 52, 45-
alignable juxtapositions (Cameron-Faulkner et al., 2003)              56.
                                                                    Gentner, D. & Markman, A. B. (2006). Defining structural
A unified approach to language                                        similarity. The Journal of Cognitive Science, 6, 1-20.
   Despite the differences in specific mechanisms between           Goldwater, M. B., Tomlinson, M. T., Echols, C. H., &
our models, we share an important commitment with Chang               Love, B. C. (2011). Structural Priming as Structure-
et al.: namely, that the mechanisms of structural priming can         Mapping: Children Use Analogies From Previous
also be applied to grammar learning in children. Goldwater            Utterances to Guide Sentence Production. Cognitive
et al. (2011) found a developmental sequence towards less             Science, 35(1), 156-170.
reliance on high semantic similarity in structural priming—         Hare, M., & Goldberg, A. (1999). Structural priming: Purely
an effect specifically predicted by a structure-mapping               syntactic? Proceedings of the 21st Annual Conference of
account of grammar learning. There is also evidence that              the Cognitive Science Society (pp. 208-211). Vancouver.
analogical processes enter into learning word meanings,             Kaschak, M. P., & Borreggine, K. L. (2008). Is long-term
particularly for relational terms such as verbs (Childers,            structural priming affected by patterns of experience with
2008). If further studies bear out the hypothesis that                individual verbs? Journal of Memory and Language,
analogical processes are involved in grammar learning, this           58(3), 862-878.
will implicate analogy as a major force in language learning.       Kuehne, S. E., Forbus, K. D., Gentner, D., & Quinn, B.
                                                                      (2000). SEQL: Category learning as progressive
                    Acknowledgments                                   abstraction using structure mapping. Proceedings of the
This work was supported by the Cognitive Science program              22nd Annual Conference of the Cognitive Science Society:
of the Office of Naval Research. We thank Julie Colhoun,              Philadelphia, PA.
Andrew Lovett, and Linsey Smith for helpful comments.               Pickering, M., & Garrod, S. (2004). Toward a mechanistic
                                                                      psychology of dialogue. Behavioral and Brain Sciences,
                                                                      27(02), 169-190.
                          References                                Savage, C., Lieven, E., Theakston, A., & Tomasello, M.
                                                                      (2003). Testing the abstractness of children's linguistic
Bock, J. K. (1986). Syntactic persistence in language                 representations: lexical and structural priming of syntactic
   production. Cognitive Psychology, 18(3), 355-387.                  constructions in young children. Developmental Science,
Bock, K., & Griffin, Z. M. (2000). The persistence of                 6(5), 557-567.
   structural priming: Transient activation or implicit             Snider, N. (2009). Similarity and structural priming.
   learning? Journal of Experimental Psychology: General,             Proceedings of the 31st Annual Conference of the
   129(2), 177-192.                                                   Cognitive Science Society (pp. 815–820). Amsterdam.
Branigan, H., Pickering, M., & Cleland, A. (2000).                  Thothathiri, M., & Snedeker, J. (2008). Syntactic priming
   Syntactic co-ordination in dialogue. Cognition, 75(2), 13-         during language comprehension in three-and four-year-
   25.                                                                old children. Journal of Memory and Language, 58(2),
Cameron-Faulkner, T., Lieven, E., & Tomasello, M. (2003).             188-213.
   A construction-based analysis of child-directed speech.
   Cognitive Science, 27, 843-874.
Chang, F., Bock, K., & Goldberg, A. (2003). Can thematic
   roles leave traces of their places? Cognition, 90(1), 29-49.
Chang, F., Dell, G. S., & Bock, K. (2006). Becoming
   syntactic. Psychological Review, 113(2), 234-272.
                                                                2921

