UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Reward Prediction Error Signals are Metarepresentational
Permalink
https://escholarship.org/uc/item/60x5b66q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Author
Shea, Nicholas
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                     Reward Prediction Error Signals are Metarepresentational
                                    Nicholas Shea (nicholas.shea@philosophy.ox.ac.uk)
                                             Faculty of Philosophy, University of Oxford
                                               10 Merton Street, Oxford, OX1 4JJ, UK
                             Abstract                                metarepresentations than this. But this result does show that
   Although there has been considerable debate about the
                                                                     there is at least one variety of metarepresentation that is
   existence of metarepresentational capacities in non-human         found very widely in the animal kingdom.
   animals and their scope in humans, the well-confirmed                Metarepresentations are representations whose content in
   temporal difference reinforcement learning models of reward-      part concerns the content of another representation. The
   guided decision making have been largely overlooked. This         sentence: ‘The main headline in the Post today is in huge
   paper argues that the reward prediction error signals which       letters’ is not metarepresentational. It concerns another
   are postulated by temporal difference models and have been        representation, but not its content. The sentence: ‘The main
   discovered empirically through single unit recording and
   neuroimaging do have metarepresentational contents.               headline in the Post today is about Gaza’ is
                                                                     metarepresentational.
   Keywords: metarepresentation; metacognition; reward-                 To assess whether reward prediction error signals are
   guided decision making; temporal difference learning              metarepresentational I examine the standard information-
                                                                     processing account of their role in generating behaviour and
                         Introduction                                ask what content RPEs would have to have for that account
It is often argued that the capacity for metarepresentation is       to be vindicated.
a particularly sophisticated cognitive achievement
(Carruthers, 2008). In the animal literature authors debate                        Reward Prediction Errors
whether success on tasks that seem to require self-                  The prediction error signal postulated by temporal
monitoring can be achieved without metarepresentation                difference learning models of reward-guided decision
(Carruthers, 2009; Hampton, 2001; Smith, 2009). The same             making (Sutton & Barto, 1998) was discovered empirically
question is debated about tasks that seem to require keeping         through single unit recording in the awake behaving
track of the mental states of others (Hare, Call, &                  macaque (Schultz, Dayan, & Montague, 1997). The central
Tomasello, 2001; Heyes, 1998). It is assumed that evidence           idea is that the brain keeps track of the expected value of
that non-human animals are processing metarepresentations            various possible actions. When the animal performs an
is a sign of considerable psychological sophistication, even         action, it computes an expected value of the current
consciousness (Cowey & Stoerig, 1995; Smith, Shields, &              behaviour. When feedback does not match that expected
Washburn, 2003; Stoerig, Zontanou, & Cowey, 2002);                   value a prediction error signal is generated. The signal is
although some have argued that some forms of                         used to update the stored representation of the value
metarepresentation can be achieved more easily (Shea &               associated with that behaviour, by an amount given by the
Heyes, 2010). In developmental psychology the capacity to            learning rate. For example if an animal pulls a lever for the
have beliefs about others’ belief states is seen as a                first time and obtains a reward, that will generate a
particularly important developmental transition (Leslie,             prediction error signal. The actual reward will have
1987; Perner, Frith, Leslie, & Leekam, 1989; Wimmer &                exceeded any expectation of reward. (If the animal has
Perner, 1983), although here too there is increasing evidence        some general expectation of there being some rewards in
that some forms of very early behaviour depend upon                  this environment, then it will have a mild general
representing or keeping track of others’ representations             expectation of reward.) So the unexpected reward will
(Apperly & Butterfill, 2009; Onishi & Baillargeon, 2005;             generate a prediction error signal.
Surian, Caldi, & Sperber, 2007).                                        Normative models of reinforcement learning attempt to
   This paper argues that there is already strong evidence of        capture the best way of calculating what to do given a
metarepresentation in a different literature – one in which          history of rewarded and unrewarded actions (under various
issues about metarepresentation have seldom been                     computational constraints).         The popular temporal
canvassed. Research on reward-guided decision making has             difference models suggest that reward prediction error
produced an impressive body of converging evidence that              signals will be used to update the expected value of the
midbrain dopamine neurons generate a reward prediction               chosen action. As a result, on future occasions the animal
error signal (RPE) that is causally involved in choice               will expect slightly more from pressing the lever. How
behaviour (Rushworth, Mars, & Summerfield, 2009). I                  much more depends upon the learning rate.
argue that such RPEs carry a metarepresentational content.              After enough learning, the animal will come to expect
The system is conserved across primates and rodents, and             reward when it presses the lever. If it presses the lever and
perhaps more widely (Claridge-Chang et al., 2009). Some              receives no reward, that will again create a RPE, but in the
animals doubtless make more sophisticated use of                     opposite direction. The effect will be to reduce the animal’s
                                                                 161

expectation of obtaining a reward from that action in the           probability of choosing one option as its value relative to
future.                                                             other options increases.
   The fact that an animal’s behaviour in experimental                 When the agent has made his choice and feedback has
situations is well-described by a temporal difference               been received, the system calculates a prediction error: the
learning model is not enough to show that it is really              (signed) difference between the expected reward and the
processing over internal representations that represent the         actual reward. For example, if a moderate reward was
quantities found in the model. On an instrumentalist                expected with only low probability, a large positive RPE
approach to representation it would be enough to show that          will be generated if the reward is delivered. The same level
the model is adequate to the data and predictively accurate.        of reward would produce a much smaller RPE if it were
But that fact also gives us some evidence that the animal           anticipated. The omission of an expected reward generates a
really is processing over real internal variables that              large negative prediction error.
correspond to the quantities in the model: expected values             The RPE is then used to update the expected reward for
and prediction errors. We get stronger evidence by                  that action, which in turn is used to make the next decision.
investigating brains directly.                                      The updated expected reward is moved in the direction of
   Of course, there could be real internal representations that     the reward received. The extent to which it is moved is
are coded in a very non-obvious format. So if the search for        moderated by the learning rate. If the learning rate is low,
evidence of internal representations in the brain were to           the expected value is adjusted only slightly in the direction
deliver a negative result, that would be far from conclusive        of the reward just delivered. If the learning rate is high, the
evidence against the existence of internal representations.         adjustment is more substantial. At the limit, were the
Fortunately in the case of RPEs, it looks as if there are           learning rate equal to one, the expected value would be reset
internal representations with a fairly stable, tractable neural     to the value of the last reward.
basis. There are midbrain dopamine neurons whose firing                So the putative representations of interest that figure in
patterns correspond to the quantities found in the model.           the information processing story are as follows.1
   In single unit recording in monkeys, dopaminergic
neurons in the ventral tegmental area (VTA) and substantia               Expected value at t of option 1              V1t
nigra pars compacta have been found to have a firing profile             Expected value at t of option 2              V2t
corresponding to the RPEs posited for appetitive                         Chosen behaviour at t                        Bi (B1, B2)
conditioning (Bayer & Glimcher, 2005; Schultz, 1998;                     Actual reward at t                           rt
Schultz et al., 1997). Functional magnetic resonance                     Prediction error (having chosen i)           δt= rt – Vit
imaging (fMRI) in humans shows a similar pattern of                      Learning rate                                α
effects. By fitting temporal difference learning models to               Updated expected values:
the behavioural data, trial-by-trial estimates of a subject’s                 Chosen behaviour i                      Vit+1= Vit + αδt
representations of value and RPE are generated and                            Unchosen behaviour j                    Vjt+1= Vjt
correlated with the fMRI response. These find a BOLD
response consistent with RPEs both in the VTA (D'Ardenne,              What should we think of these values as representing if
McClure, Nystrom, & Cohen, 2008) and in areas of the                the information processing story is to make sense? We have
ventral striatum receiving dopaminergic inputs (Haruno &            to use words to capture these contents, but the with the
Kawato, 2006; McClure, Berns, & Montague, 2003;                     caveat that the words are not aiming to capture either (a)
O'Doherty, Dayan, Friston, Critchley, & Dolan, 2003).               what the system or the agent understands the contents of the
                                                                    states to be; or (b) constituent structure – the states whose
                 Content in the Model                               contents we are describing have none of the constituent
What do these prediction error signals represent? To answer         structure that is found in the sentences we use to describe
that question we need to examine the way they are produced          them.
and how they enter into subsequent processing. In order to
have a fixed target, we shall presuppose that the current           Reward and Value Representations
state of the evidence supports the conclusion that the                 Quantity rt is straightforward: it represents the value of
calculations hypothesized by temporal difference learning           the reward actually received t (the value of so many ml of
models are being performed in the brains of humans and              juice, for example): rt was received. B1 and B2 are also
other animals when they make rapid simple decisions for             straightforward. Bi has a directive content: do action i, or
probabilistic rewards, and that these calculations are              choose the action that will select option i.
responsible for the observed patterns of choice behaviour.             V1t and V2t seem to be stating facts about causal
   At the outset of a trial, when a number of behavioural           conditionals. However, they do not simply predict the value
options are presented, sometimes following a cue, the               of the next chosen action. Rather, they predict the reward
system activates an expected value for each option. A               that will be obtained on average if an option is repeatedly
decision rule makes use of these values to choose an option.
                                                                       1
For example a softmax decision rule increases the                             The symbols are used both to refer to the representations
                                                                    involved, and to pick out the quantities variably represented by
                                                                    those representations.
                                                                162

chosen in the current environment. That is, they represent           to focus in on its content. Downstream, δt is used to update
expected values in the probabilistic sense of expectation            the expected value of the reward for future trials. The best
(summing probability x magnitude): if option 1 is chosen,            way of describing how δt is being acted on cannot help but
then the expected reward will be V1t. Here expected reward           advert to the fact that it is used, not directly to select an
is an objective quantity of which V1t is the agent’s current         action, but to update a second internal register – to update
estimate. Expected rewards should not be confused with the           another internal representation.
agent’s (subjective, represented) expectations about                    Sometimes hierarchical information processing involves a
rewards. So the success condition for behaviour driven by            series of steps (e.g. filters) or the combination of
representation V1t looks to be something like this: the              information from different sources to form a new
average reward payoff that would be achieved by repeatedly           representation (e.g. conjunctive feature detectors). These
choosing option 1 in the current environment would be V1t.2          are also cases where downstream representations are
If the actual expected gain from option 1 is higher than this,       changed in reliance on upstream representations. But in
then the agent’s behaviour will be suboptimal in that it will        those cases the upstream representations are relied upon for
choose option 1 less than it should. Conversely, if the              information they carry about some external fact of the
actual expected gain from option 1 is lower than that                matter. RPEs, by contrast, are acted upon to update value
represented by V1t then the agent’s behaviour will be                representations not directly because of information they
suboptimal in that it could increase its chances of receiving        carry about reward, but because they carry information
higher overall rewards by selecting option 1 less frequently.        directly about the accuracy of previous estimates. Whether
   When an actual reward is received when there is a                 a reward has just been received or not, the job of δt is to
relatively low V1t that could be because the estimate of             reset the expected value Vit+1 by a lot if the feedback on the
expected value is wrong, or it could be that this is one of          current occasion was a long way from the average that was
those low-probability occasions where option 1 is rewarded.          expected over repeated trials, Vit, and by only a little if Vit
Temporal difference learning models finesse this                     closely matches the current feedback.
information gap by re-jigging the value representation in               Consider the kinds of things that could go wrong in neural
every case, in effect treating it as possible that this bit of       processing and why, according to the temporal difference
feedback is a sign that the current estimate is wrong (either        learning model, these would constitute errors. Suppose that
because of insufficient learning or because the environment          because of some glitch a large positive prediction error were
has changed). This leads the estimate of expected reward to          generated on an occasion where the chosen option B1 was
be altered for future trials. That recalculation is mediated by      not expected to be very rewarding and was not rewarded.
the magnitude and sign of the difference between                     We can’t understand this error straightforwardly in terms of
represented expected value and feedback: the RPE δt.                 some mistaken behaviour on the next trial, because the
                                                                     decision rule might well lead B2 to be chosen on the next
Reward Prediction Error                                              trial. The error is not in how the system acts on the next
   Characterizing the content of δt is more tricky. An input-        trial, but in how it changes its expectations on the next trial,
driven approach to content looks at the parameters with              because it will have mistakenly increased V1, its expected
which a representation covaries and uses them to ascribe             reward for option 1. Correlatively, suppose δt is produced
content (Dretske, 1981). The notorious difficulty is that a          in the regular way so it does reflect the difference between rt
given representation that correlates with some inputs will           and V1t, but is then ignored in downstream processing.
thereby correlate with very many others too (Fodor, 1987).           Here we would say that it correctly represented the
Considered informationally δt will carry some information            possibility that the previous prediction V1t was mistaken,
about the actual reward, some information about the                  but that it wasn’t acted on correctly to update V1t+1 for the
expected reward, and even more reliable information about            next trial.
the difference between them. There are good reasons to be               While these commonsense considerations do not amount
suspicious of the idea that the content of a representation is       to an unassailable argument, they do give us good reason to
that feature with which it correlates most strongly (Millikan,       take the assumptions of the normative model at face value.
1984). For example, consider a predator-detector set up to           Surprisingly, it has been little-remarked that temporal
produce lots of false positives. Its strongest correlation may       difference learning models attribute metarepresentational
be with shadows, rather than predators.                              content to δt. Its content can be roughly described as having
   This suggests that we should also look at how a                   both descriptive and directive aspects (Millikan, 1996) as
representation is used (Godfrey-Smith, 2006). The firing of          follows:
a predator detector leads to avoidance behaviour whether or
not the stimulus was just a shadow. Thus, the way a                        The reward for the current chosen option is
representation enters into downstream processing helps us               higher/lower than the predicted expected value Vit by an
                                                                        amount δt; increase/decrease Vit+1 in proportion to the
   2
          Note that we can talk sensibly about this quantity in         magnitude δt.
counterfactual terms even if the environment is changing so that
the agent does not have the opportunity repeatedly to sample
option 1 in the current environment.
                                                                 163

   Notice that both the descriptive and directive aspects of       estimate of an objective probabilistic expectation, but the
the content make reference to the content of another               temptation to think of this as metarepresentational is just a
representation: Vit / Vit+1. The reward prediction error           mistake. It probably derives from the ambiguity of
signal does not just describe some aspect of the agent’s           ‘expectation’. In Vit the agent is keeping track of an
environment. Nor does it just direct a particular action on        expectation, but ‘expectation’ here is not what the agent (or
the part of the agent. Instead we should take seriously the        anyone else) expects, but an expected value in sense of
assumption in the temporal-difference learning literature          probability theory: the average of the magnitudes of the
that the RPE’s content partly concerns the content of              available options weighted by their objective probabilities.
another representation.        That is to say, it is a             Vit is keeping track of this quantity, which is fixed by
metarepresentation.                                                external parameters of the problem space, rather than
                                                                   anything about what the agent itself expects.
     A Competing First-Order Interpretation                           However, the fact that the expected values Vi should be
In a series of papers Proust has elucidated a form of what         ascribed first-order contents is not the end of the matter.
we have been calling metarepresentation that differs from          The argument above was only that the RPE signal was
the kind of explicit conceptual-level attribution of mental        second-order.      The objection Proust considers, when
states to oneself and others that is often the focus of the        levelled against RPE, would then be that δt can be
literature on metacognition (J. Proust, 2007, 2008; 2009).         understood in terms of the agent’s chances of succeeding,
She identifies metacognitive ‘feelings’, like the feeling you      rather than keeping track of any kind of internal state. But it
know a list of names, as a locus of non-conceptual but meta-       cannot. A very small RPE is compatible with there being a
level cognition. That is a complementary body of work,             very high chance of succeeding, for example if reward
which supports the direction taken here by showing how             expectations were already high and matched the reward
meta-level cognitive phenomena arise within non-                   actually received on the current trial. But a very small RPE
conceptual thought, well before the level of explicit,             is also compatible with there being a very low chance of
conceptual re-representation of representational contents.         succeeding, for example if reward expectations were low
   In the course of one of her discussions Proust considers an     and no reward was delivered. Conversely, a large RPE is
argument that the signals processed according to temporal          compatible both with a high and a low chance of
difference learning models are first-order and do not involve      succeeding. What the RPE is telling the agent is not well
metarepresentations (Proust 2007, pp. 282-285). This is one        captured by its connection to the chance of succeeding in
of very few existing discussions of whether RPEs are               future behaviour. If the temporal difference models are
metarepresentational, so merits investigation. Proust’s            anything like on track, what the RPE signal is doing is
response to the argument is that, in the kinds of self-            telling the agent something about how well or badly its
monitoring paradigms she is interested in, it is not possible      representations of expected value for an option match the
to explain performance in terms of the agent keeping track         current feedback. What it does with that information,
of its objective chance of success. In experiments such as         namely to re-jig its reward expectations proportionately,
Hampton (2001) the animal seems to be drawing on                   also makes much more sense in the light of meta-level
information beyond that delivered by the problem situation,        contents. In short, there is no easy way to replace the meta-
but that depends upon keeping track of trial-by-trial              level contents inherent in temporal difference models of
variation in the agent’s own informational resources. That         reward-guided decision making with a first-order
is, the animal’s performance (one of the two animals, in the       reinterpretation.
Hampton experiment) seems to depend upon procedural
self-knowledge.                                                                            Conclusion
   Proust’s own response leaves the original objection, as it      The conclusion that non-conceptual metarepresentations are
applies to the ordinary cases of reward-guided decision            processed during reward-guided decision making in many
making captured by temporal difference learning models,            animals opens up several questions for further research.
standing — namely that subjects’ behaviour in these                What distinguishes these representations from the more
experiments can be fully captured in first order terms. The        sophisticated forms of metarepresentation involved in
argument is that there is no substantive difference between        keeping track of the mental states of others, or of the agent’s
keeping track of the reliability of one’s estimates of             own mental states? To what extent does the temporal
expected value (second order) and keeping track of one’s           difference model connect with decision making at the
chances of succeeding when performing particular                   personal level, or does it just describe a subpersonal system?
behaviours (first order) (Proust 2007. p. 283). That               How inferentially promiscuous are the representations
argument does indeed apply to the agent’s representation of        involved in model-free reward guided decision making?
expected reward (the Vit above). Although we could                 Are they conscious or do they have some impact on
describe these as measuring how well the agent knows that a        consciousness?
given option will be rewarding, we have seen above that a             All these questions are interesting and important. A less
first order explanation is preferable. The content to be           obvious question also merits attention.           In temporal
attributed to the Vit is rather subtle, involving a subjective     difference learning models of model-free reward-guided
                                                               164

decision making we have a well-understood, normatively-                      of Sciences of the United States of America, 98(9),
based model of behaviour with a well-confirmed neural                        5359-5362.
basis. The whole amounts to one of the strongest results of         Hare, B., Call, J., & Tomasello, M. (2001). Do chimpanzees
the project of cognitive neuroscience: of finding                            know what conspecifics know? Animal Behaviour,
psychological-level information-processing accounts of                       61(1), 139-151.
behaviour that can be mapped onto neural processes. Once            Haruno, M., & Kawato, M. (2006). Different neural
we have a good grip on the kinds of content ascriptions that                 correlates of reward expectation and reward
are supported by these theories, including the                               expectation error in the putamen and caudate
metarepresentational contents discussed here, they provide                   nucleus during stimulus-action-reward association
us with an excellent arena against which to test                             learning. Journal of Neurophysiology, 95(2), 948.
philosophical theories of content. That is, they provide            Heyes, C. M. (1998). Theory of mind in nonhuman
another test case, quite different from the usual repertoire                 primates. Behavioral and Brain Sciences, 21(01),
from perception and the cognitive psychology of concepts,                    101-114.
of which we can ask: in virtue of what do these                     Leslie, A. M. (1987). Pretense and representation: The
representations have the content they do? It will be an                      origins of “theory of mind.” Psychological review,
important      constraint     on     that   theorizing     that              94(4), 412-426.
metarepresentational contents can already be realized these         McClure, S. M., Berns, G. S., & Montague, P. R. (2003).
relatively low-level systems.                                                Temporal prediction errors in a passive learning
                                                                             task activate human striatum. Neuron, 38(2), 339-
                    Acknowledgments                                          346.
  Work on this paper has been supported by the Oxford               Millikan, R. G. (1984). Language, Thought and Other
University Press John Fell Research Fund, the Oxford                         Biological Categories. Cambridge, MA: MIT
Martin School and the Wellcome Trust (grant 086041 to the                    Press.
Oxford Centre for Neuroethics).                                     Millikan, R. G. (1996). Pushmi-pullyu Representations. In J.
                                                                             Tomberlin (Ed.), Philosophical Perspectives, vol. 9
                                                                             (pp. 185-200). Atascadero, CA: Ridgeview
                         References                                          Publishing.
Apperly, I. A., & Butterfill, S. A. (2009). Do humans have          O'Doherty, J. P., Dayan, P., Friston, K., Critchley, H., &
          two systems to track beliefs and belief-like states.               Dolan, R. J. (2003). Temporal difference models
          Psychological review, 116(4), 953.                                 and reward-related learning in the human brain.
Bayer, H. M., & Glimcher, P. W. (2005). Midbrain                             Neuron, 38(2), 329-337.
          dopamine neurons encode a quantitative reward             Onishi, K. H., & Baillargeon, R. (2005). Do 15-month-old
          prediction error signal. Neuron, 47(1), 129-141.                   infants understand false beliefs? Science,
Carruthers, P. (2008). Meta-cognition in animals: a skeptical                308(5719), 255.
          look. Mind & Language, 23(1), 58-89.                      Perner, J., Frith, U., Leslie, A. M., & Leekam, S. R. (1989).
Carruthers, P. (2009). Mindreading underlies metacognition.                  Exploration of the autistic child's theory of mind:
          Behavioral and Brain Sciences, 32(02), 164-182.                    Knowledge, belief, and communication. Child
Claridge-Chang, A., Roorda, R. D., Vrontou, E., Sjulson, L.,                 Development, 60(3), 689-700.
          Li, H., Hirsh, J., et al. (2009). Writing memories        Proust, J. (2007). Metacognition and metarepresentation: is
          with light-addressable reinforcement circuitry.                    a self-directed theory of mind a precondition for
          Cell, 139(2), 405-415.                                             metacognition? Synthese, 159(2), 271-295.
Cowey, A., & Stoerig, P. (1995). Blindsight in monkeys.             Proust, J. (2008). Epistemic agency and metacognition: an
          Nature, 373(6511), 247-249.                                        externalist view. Proceedings of the Aristotelian
D'Ardenne, K., McClure, S. M., Nystrom, L. E., & Cohen,                      Society, 108, 241–268.
          J. D. (2008). BOLD responses reflecting                   Proust, J. (2009). The representational basis of brute
          dopaminergic signals in the human ventral                          metacognition: a proposal. In R. Lurz (Ed.), The
          tegmental area. Science, 319(5867), 1264.                          philosophy of animal minds. Cambridge: C.U.P.
Dretske, F. (1981). Knowledge and the Flow of Information.          Rushworth, M. F. S., Mars, R. B., & Summerfield, C.
          Cambridge, M.A.: MIT Press.                                        (2009). General mechanisms for making decisions?
Fodor, J. (1987). Psychosemantics. Cambridge, MA.: MIT                       Current opinion in neurobiology, 19(1), 75-83.
          Press.                                                    Schultz, W. (1998). Predictive reward signal of dopamine
Godfrey-Smith, P. (2006). Mental representation, naturalism                  neurons. Journal of neurophysiology, 80(1), 1.
          and teleosemantics. In D. Papineau & G.                   Schultz, W., Dayan, P., & Montague, P. R. (1997). A neural
          Macdonald (Eds.), New Essays on Teleosemantics.                    substrate of prediction and reward. Science,
          Oxford: OUP.                                                       275(5306), 1593.
Hampton, R. R. (2001). Rhesus monkeys know when they
          remember. Proceedings of the National Academy
                                                                165

Shea, N., & Heyes, C. (2010). Metamemory as evidence of
         animal consciousness: the type that does the trick.
         Biology and Philosophy, 25, 95-110.
Smith, J. D. (2009). The study of animal metacognition.
         Trends in cognitive sciences, 13(9), 389-396.
Smith, J. D., Shields, W. E., & Washburn, D. A. (2003).
         The comparative psychology of uncertainty
         monitoring and metacognition. Behavioral and
         Brain Sciences, 26(03), 317-339.
Stoerig, P., Zontanou, A., & Cowey, A. (2002). Aware or
         unaware: assessment of cortical blindness in four
         men and a monkey. Cerebral Cortex, 12(6), 565.
Surian, L., Caldi, S., & Sperber, D. (2007). Attribution of
         beliefs by 13-month-old infants. Psychological
         Science, 18(7), 580.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement
         learning: An introduction: The MIT press.
Wimmer, H., & Perner, J. (1983). Beliefs about beliefs:
         Representation and constraining function of wrong
         beliefs in young children's understanding of
         deception. Cognition, 13(1), 103-128.
                                                             166

