UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution
Permalink
https://escholarship.org/uc/item/5rk7z59q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Baker, Chris
Saxe, Rebecca
Tenenbaum, Joshua
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

             Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution
                                                  Chris L. Baker (clbaker@mit.edu)
                                                   Rebecca R. Saxe (saxe@mit.edu)
                                                Joshua B. Tenenbaum (jbt@mit.edu)
                                            Department of Brain and Cognitive Sciences, MIT
                                                           Cambridge, MA 02139
                              Abstract                                     Several authors have recently proposed models for how
                                                                        people infer others’ goals or preferences as a kind of Bayesian
   We present a computational framework for understanding The-
   ory of Mind (ToM): the human capacity for reasoning about            inverse planning or inverse decision theory (Baker, Saxe, &
   agents’ mental states such as beliefs and desires. Our Bayesian      Tenenbaum, 2009; Feldman & Tremoulet, 2008; Lucas, Grif-
   model of ToM (or BToM) expresses the predictive model of             fiths, Xu, & Fawcett, 2009; Bergen, Evans, & Tenenbaum,
   belief- and desire-dependent action at the heart of ToM as
   a partially observable Markov decision process (POMDP),              2010; Yoshida, Dolan, & Friston, 2008; Ullman et al., 2010).
   and reconstructs an agent’s joint belief state and reward func-      These models adapt tools from control theory, econometrics
   tion using Bayesian inference, conditioned on observations of        and game theory to formalize the principle of rational ac-
   the agent’s behavior in some environmental context. We test
   BToM by showing participants sequences of agents moving in           tion at the heart of children and adults’ concept of intentional
   simple spatial scenarios and asking for joint inferences about       agency (Gergely, Nádasdy, Csibra, & Biró, 1995; Dennett,
   the agents’ desires and beliefs about unobserved aspects of the      1987): all else being equal, agents are expected to choose ac-
   environment. BToM performs substantially better than two
   simpler variants: one in which desires are inferred without ref-     tions that achieve their desires as effectively and efficiently
   erence to an agent’s beliefs, and another in which beliefs are       as possible, i.e., to maximize their expected utility. Goals
   inferred without reference to the agent’s dynamic observations       or preferences are then inferred based on which objective or
   in the environment.
                                                                        utility function the observed actions maximize most directly.
   Keywords: Theory of mind; Social cognition; Action un-
   derstanding; Bayesian inference; Partially Observable Markov            ToM transcends knowledge of intentional agents’ goals and
   Decision Processes                                                   preferences by incorporating representational mental states
                                                                        such as subjective beliefs about the world (Perner, 1991). In
                          Introduction                                  particular, the ability to reason about false beliefs has been
Central to human social behavior is a theory of mind (ToM),             used to distinguish ToM from non-representational theories
the capacity to explain and predict people’s observable ac-             of intentional action (Wimmer & Perner, 1983; Onishi &
tions in terms of unobservable mental states such as beliefs            Baillargeon, 2005). Our goal in this paper is to model hu-
and desires. Consider the case of Harold, who leaves his dorm           man ToM within a Bayesian framework. Inspired by mod-
room one Sunday morning for the campus library. When he                 els of inverse planning, we cast Bayesian ToM (BToM) as a
reaches to open the library’s front door he will find that it is        problem of inverse planning and inference, representing an
locked – closed on Sunday. How can we explain his behav-                agent’s planning and inference about the world as a partially
ior? It seems plausible that he wants to get a book, that he            observable Markov decision process (POMDP), and invert-
believes the book he wants is at the library, and that he also          ing this forward model using Bayesian inference. Critically,
believes (falsely, it turns out) that the library is open on Sun-       this model includes representations of both the agent’s de-
day.                                                                    sires (as a utility function), and the agent’s own subjective
   Such mental state explanations for behavior go well be-              beliefs about the environment (as a probability distribution),
yond the observable data, leading to an inference problem               which may be uncertain and may differ from reality. We test
that is fundamentally ill-posed. Many different combinations            the predictions of this model quantitatively in an experiment
of beliefs and desires could explain the same behavior, with            where people must simultaneously judge beliefs and desires
inferences about the strengths of beliefs and desires trading           for agents moving in simple spatial environments under in-
off against each other, and relative probabilities modulated            complete or imperfect knowledge.
heavily by context. Perhaps Harold is almost positive that the             Important precursors to our work are several computational
library will be closed, but he needs a certain book so badly            models (Goodman et al., 2006; Bello & Cassimatis, 2006;
that he still is willing to go all the way across campus on the         Goodman, Baker, & Tenenbaum, 2009) and informal theo-
off chance it will be open. This explanation seems more prob-           retical proposals by developmental psychologists (Wellman,
able if Harold shows up to find the library locked on Saturday          1990; Gopnik & Meltzoff, 1997; Gergely & Csibra, 2003).
at midnight, as opposed to noon on Tuesday. If he arrives               Goodman et al. (2006) model how belief and desire infer-
after hours already holding a book with a due date of tomor-            ences interact in the classic “false belief” task used to assess
row, it is plausible that he knows the library is closed and is         ToM reasoning in children (Wimmer & Perner, 1983). This
seeking not to get a new book, but merely to return a book              model instantiates the schema shown in Fig. 1(a) as a causal
checked out previously to the night drop box.                           Bayesian network with several psychologically interpretable,
                                                                    2469

but task-dependent parameters. Goodman et al. (2009) model           (a)                World                            (b)
                                                                                                                              R
adult inferences of an agent’s knowledge of the causal struc-
                                                                           Environment               Agent
ture of a simple device (“Bob’s box”) based on observing                      State                   State                  Xt-1 Xt
the agent interacting with the device. To our knowledge, our
                                                                                     Principle of                  Agent     Yt-1 Yt
work here is the first attempt to explain people’s joint infer-                     rational belief
ences about agents’ beliefs and desires by explicitly inverting                                                              Ot-1 Ot
                                                                                        Belief                   Desire
POMDPs – and the first model capable of reasoning about the
graded strengths and interactions between agents’ beliefs and                                      Principle of
                                                                                                 rational action
                                                                                                                             Bt-1 Bt
desires, along with the origins of agents’ beliefs via environ-
mentally constrained perceptual observations.                                                        Action                  At-1 At
               Computational Framework
                                                                      Figure 1: Causal structure of theory of mind. Grey shaded nodes
This section describes Bayesian Theory of Mind (BToM): a              are assumed to be observed (for the observer; not necessarily for the
theory-based Bayesian framework (Tenenbaum, Griffiths, &              agent, as described in the main text). (a) Schematic model of theory
                                                                      of mind. Traditional accounts of ToM (e.g. Dennett, 1987; Wellman,
Kemp, 2006) that characterizes ToM in terms of Bayesian in-           1990; Gopnik & Meltzoff, 1997) have proposed informal versions of
ference over a formal, probabilistic version of the schema in         this schema, characterizing the content and causal relations of ToM
Fig. 1(a). BToM represents an ideal observer using a the-             in commonsense terms, e.g., “seeing is believing” for the princi-
                                                                      ple of rational belief. (b) Observer’s grounding of the theory as a
ory of mind to understand the actions of an individual agent          dynamic Bayes net (DBN). The DBN encodes the observer’s joint
within some environmental context. This ideal-observer anal-          distribution over an agent’s beliefs B1:T and desires R over time,
ysis of ToM asks how closely human judgments approach the             given the agent’s physical state sequence x1:T in environment y.
ideal limit, but also what mental representations are necessary
to explain human judgments under hypothetically unbounded             agent is unsure about the location of a particular object, its
computational resources. We will first describe BToM in gen-          belief contents are worlds in which the object is in different
eral, but informal terms before progressing to the mathemati-         locations. The agent’s degree of belief reflects the subjective
cal details involved in modeling our experimental domain.             probability it assigns to each possible world.
                                                                         The principles governing the relation between the world
Informal sketch                                                       and the agent’s beliefs, desires and actions can be naturally
For concreteness, we use as a running example a simple spa-           expressed within partially observable Markov decision pro-
tial context (such as a college campus or urban landscape)            cesses (POMDPs). POMDPs capture the causal relation be-
defined by buildings and perceptually distinct objects, with          tween beliefs and the world via the principle of rational belief,
agents’ actions corresponding to movement, although in gen-           which formalizes how the agent’s belief is affected by obser-
eral BToM can be defined over arbitrary state and action              vations in terms of Bayesian belief updating. Given an ob-
spaces (for example, a card game where the state describes            servation, the agent updates its degree of belief in a particular
players’ hands and actions include draw or fold). The ob-             world based on the likelihood of receiving that observation in
server’s representation of the world is composed of the en-           that world. In a spatial setting, observations depend on the
vironment state and the agent state (Fig. 1(a)). In a spatial         agent’s line-of-sight visual access to features of the environ-
context, the state of the environment represents its physical         ment. POMDPs represent how beliefs and desires cause ac-
configuration, e.g., the location of buildings and objects, and       tions via the principle of rational action, or rational planning.
the state of the agent specifies its objective, external proper-      Intuitively, rational POMDP planning provides a predictive
ties, such as its physical location in space.                         model of an agent optimizing the tradeoff between exploring
   The observer’s theory of the agent’s mind includes repre-          the environment to discover the greatest rewards, and exploit-
sentations of the agent’s subjective desires and beliefs, and         ing known rewards to minimize costs incurred.
the principles by which desires and beliefs are related to ac-           On observing an agent’s behavior within an environment,
tions and the environment. Similar to previous models, the            the beliefs and desires that caused the agent to generate this
content of the agent’s desire consists of objects or events in        behavior are inferred using Bayesian inference. The observer
the world. The agent’s degree of desire is represented in terms       maintains a hypothesis space of joint beliefs and desires,
of the subjective reward received for taking actions in certain       which represent the agent’s initial beliefs about the environ-
states, e.g., acting to attain a goal while in close proximity to     ment state and the agent’s static desires for different goals.
the goal object. The agent can also act to change its own state       For each hypothesis, the observer evaluates the likelihood of
or the environment state at a certain cost, e.g., navigating to       generating the observed behavior given the hypothesized be-
reach a goal may incur a small cost at each step.                     lief and desire. The observer integrates this likelihood with
   The main novel component of the current model is the in-           the prior over mental states to infer the agent’s joint belief
clusion of a representation of beliefs. Like desires, beliefs are     and desire.
defined by both their content and the strength or degree with            As an example of how this works, consider Fig. 2. The
which they are held. The content of a belief is a representa-         “college campus” environment is characterized by the cam-
tion corresponding to a possible world. For instance, if the          pus size, the location and size of buildings, and the location of
                                                                  2470

                                                                                                                  Fr  ame10
                                           Frame5                                                                 Fr  ame15
Figure 2: Example experimental stimulus. The small blue sprite represents the location of the agent, and the black trail with arrows superim-
posed records the agent’s movement history. The two yellow cells in opposite corners of the environment represent spots where trucks can
park, and each contains a different truck. The shaded grey area of each frame represents the area that is outside of the agent’s current view.
several different goal objects, here “food trucks”. The agent             P (o|x, y) encodes which environments in Y are consistent
is a hungry graduate student, leaving his office and walk-                with the contents of the isovist from location x. We model
ing around campus in search of satisfying lunch food. There               observation noise with the simple assumption that ambigu-
are three trucks that visit campus: Korean (K), Lebanese (L)              ous observations can occur with probability ν, as if the agent
and Mexican (M), but only two parking spots where trucks                  failed to notice something that should otherwise be visible.
are allowed to park, highlighted with a yellow background                    The observer represents the agent’s belief as a probabil-
in Fig. 2. The student’s field of view is represented by the              ity distribution over Y; for y ∈ Y, b(y) denotes the agent’s
unshaded region of the environment.                                       degree of belief that y is the true state of the environment.
   In Fig. 2, the student can initially only see where K (but not         Bayesian belief updating at time t is a deterministic function
L) is parked. Because the student can see K, they know that               of the prior belief bt−1 , the observation ot , and the world state
the spot behind the building either holds L, M, or is empty.              hxt , yi. The agent’s updated degree of belief in environment
By frame 10, the student has passed K, indicating that they               y satisfies bt (y) ∝ P (ot |xt , y)bt−1 (y).
either want L or M (or both), and believe that their desired                 The agent’s reward function R(x, y, a) encodes the subjec-
truck is likely to be behind the building (or else they would             tive utility the agent derives from taking action a from the
have gone straight to K under the principle of rational action).          state hxt , yi. Each action is assumed to incur a cost of 1.
After frame 10, the agent discovers that L is behind the build-           Rewards result from taking the “Eat” action while at a food
ing and turns back to K. Obviously, the agent prefers K to                truck; the magnitude of the reward depends on the strength
L, but more subtly, it also seems likely that the agent wants             of the agent’s desire to eat at that particular truck. Once the
M more than either K or L, despite M being absent from the                student has eaten, all rewards and costs cease, implying that
scene! BToM captures this inference by resolving the desire               rational agents should optimize the tradeoff between the num-
for L or M over K in favor of M after the agent rejects L.                ber of actions taken and the reward obtained.
In other words, BToM infers the best explanation for the ob-                 The agent’s POMDP is defined by the state space, the
served behavior – the only consistent desire that could lead              action space, the world dynamics, the observation model,
the agent to act the way it did.                                          and the reward function. We approximate the optimal value
                                                                          function of the POMDP for each hypothesized reward func-
Formal modeling                                                           tion using a point-based value iteration algorithm over a uni-
In the food-truck domain, the agent occupies a discrete state             form discretization of the belief space. The agent’s policy is
space X of points in a 2D grid. The environment state Y is                stochastic, given by the softmax of the lookahead state-action
the set of possible assignments of the K, L and M trucks to               value function QLA (Hauskrecht, 2000): P (a|b, x, y) ∝
parking spots. Possible actions include North, South, East,               exp(βQLA (b, x, y, a)). The β parameter establishes the de-
West, Stay, and Eat. Valid actions yield the intended transi-             gree of determinism with which the agent executes its policy,
tion with probability 1− and do nothing otherwise; invalid               capturing the intuition that agents tend to, but do not always
actions (e.g., moving into walls) have no effect on the state.            follow the optimal policy.
   The agent’s visual observations are represented by the iso-               Our approach to joint belief and desire inference is closely
vist from the agent’s location: a polygonal region contain-               related the model of belief filtering in Zettlemoyer, Milch, and
ing all points of the environment within a 360-degree field               Kaelbling (2009), restricted to the case of one agent reasoning
of view (Davis & Benedikt, 1979; Morariu, Prasad, & Davis,                about the beliefs of another. Fig. 1(b) shows the observer’s
2007). Example isovists from different locations in one en-               dynamic Bayes net (DBN) model of an agent’s desires, states,
vironment are shown in Fig. 2. The observation distribution               observations, beliefs and actions over time. The observer’s
                                                                    2471

belief and reward inferences are given by the joint posterior          were crossed with 5 different trajectories: “Check-Left, go
marginal over the agent’s beliefs and rewards at time t, given         to K”; “Check-Left, go to L/M”; “Check-Right, go to K”;
the state sequence up until T ≥ t: P (bt , r|x1:T , y). This com-      “Check-Right, go to L/M”; and “No-check, go straight to K”.
putation is analogous to the forward-backward algorithm in             Four of these trajectories are shown in Fig. 4. Each path was
hidden Markov models, and provides the basis for model pre-            shown with 2 different judgment points, or frames at which
dictions of people’s joint belief and desire inferences in our         the animation paused and subjects gave ratings based on the
experiment.                                                            information shown so far. Judgment points were either at the
   To perform inference over the multidimensional, contin-             moment the student became able to see the parking spot that
uous space of beliefs and rewards, we uniformly discretize             was initially occluded (“Middle”; e.g., frame 10 in Fig. 2), or
the hypothesis spaces of beliefs and reward functions with             at the end of the path once the student had eaten (“Ending”;
grid resolutions of 7. The range of reward values was cali-            e.g., frame 15 in Fig. 2). All potential paths were crossed with
brated to the spatial scale of our environments, taking values         6 environmental contexts, generated by combining 3 different
−20, 0, . . . , 100 for each truck. Model predictions were based       building configurations: “O”, “C” and “backwards C”, (all
on the student’s expected reward value for each truck (K, L,           shown in Fig. 4) with 2 different goal configurations: “One
M) and the expected degree-of-belief in each possible world            truck” or “Two trucks” present; both shown in Fig. 4.
for each trial.                                                           After all possible trials from this design were generated,
Alternative models                                                     all invalid trials (in which the student’s path intersected with
                                                                       a building), and all “Ending” trials in which the path did not
To test whether the full representational capacity of our model        finish at a truck were removed. This left 78 total trials. Of
is necessary to understand people’s mental state attributions,         these, 5 trials had a special status. These were trials in the
we formulate two alternative models as special cases of our            “O” environment with paths in which the student began at
joint inference model. Each alternative model “lesions” a              the Right starting point, and then followed a Check-Left tra-
central component of the full model’s representation of be-            jectory. These paths had no rational interpretation under the
liefs, and tests whether it is possible to explain people’s infer-     BToM model, because the Check-Right trajectory was always
ences about agents’ desires in our experiment without appeal           a more efficient choice, no matter what the student’s initial
to a full-fledged theory of mind.                                      belief or desire. These “irrational” trials are analyzed sepa-
   Our first alternative model is called TrueBel. This model           rately in the Results section.
assumes that the state is fully observable to the agent, i.e.,
that the agent knows the location of every truck, and plans               Several factors were counterbalanced or randomized.
to go directly to the truck that will provide the maximal re-          Stimulus trials were presented in pseudo-random order. Each
ward while incurring the least cost. We hypothesized that this         trial randomly scrambled the truck labels, and randomly re-
model would correlate moderately well with people’s desire             flected the display vertically and horizontally so that subjects
judgments, because of the statistical association between de-          would remain engaged with the task and not lapse into a
sired objects and actions.                                             repetitive strategy. Each trial randomly displayed the agent
   Our second alternative model is called NoObs. In this               in 1 of 10 colors, and sampled a random male or female name
model, the agent has an initial belief about the state of the          without replacement. This ensured that subjects did not gen-
environment, but there is no belief updating – the initially           eralize information about one student’s beliefs or desires to
sampled belief remains fixed throughout the trial. We hy-              students in subsequent trials.
pothesized that this model might fit people’s belief and desire           The experimental task involved rating the student’s degree
inferences in situations where the agent appeared to move to-          of belief in each possible world (Lebanese truck behind the
ward the same truck throughout the entire trial, but that for ac-      building (L); Mexican truck behind the building (M); or noth-
tions that required belief updating or exploration to explain,         ing behind the building (N)), and rating how much the student
for instance, when the agent began by exploring the world,             liked each truck. All ratings were on a 7-point scale. Belief
then changed direction based on its observation of the world           ratings were made retrospectively, meaning that subjects were
state, NoObs would fit poorly.                                         asked to rate what the student thought was in the occluded
                                                                       parking spot before they set off along their path, basing their
                           Experiment                                  inference on the information from the rest of the student’s
Fig. 4 illustrates our experimental design. Truck labels were          path. The rating task counterbalanced the side of the monitor
randomized in each trial of the experiment, but we will de-            on which the “likes” and “believes” questions were displayed.
scribe the experiment and results using the canonical, un-                Subjects first completed a familiarization stage that ex-
scrambled ordering Korean (K), Lebanese (L), Mexican (M).              plained all details of our displays and the scenarios they de-
   The experiment followed a 3 × 5 × 2 × 3 × 2 design.                 picted. To ensure that subjects understood what the students
These factors can be divided into 30 (3 × 5 × 2) unique paths          could and couldn’t see, the familiarization explained the vi-
and 6 (3 × 2) unique environmental contexts. There were                sualization of the student’s isovist, which was updated along
3 different starting points in the environment: “Left”, “Mid-          each step of the student’s path. The isovist was displayed
dle”, or “Right”; all shown in Fig. 4. These starting points           during the testing stage of the experiment as well (Fig. 2).
                                                                   2472

                                                                                    Desire Inference (r=0.90)                           Belief Inference (r=0.76)
   Participants were 17 members of the MIT subject pool, 6                                                                     1
female, and 11 male. One male subject did not understand                        7
                                                                                                                              0.8
the instructions and was excluded from the analysis.                            6
                                                                                                                              0.6
                                                                       People                                        People
                                                                                5
Results & Discussion                                                            4                                             0.4
                                                                                3
Debriefing of subjects suggested that many were confused by                     2
                                                                                                                              0.2
the “Middle” judgment point trials; this was also reflected                     1                                              0
                                                                                    1   2   3    4   5   6     7                    0      0.2   0.4   0.6   0.8    1
by greater variability in people’s judgments within these tri-
                                                                                        Model (β=1.0; ν=0.1)                              Model (β=1.0; ν=0.1)
als. Because of this, our analyses only include trials from the
“Ending” judgment point condition, which accounted for 54                                                    BToM      TrueBel               NoObs
out of the 78 total trials.                                                                     Desire        0.90       0.67                 0.61
                                                                                                Belief        0.76       0.11                 0.39
   We begin by analyzing the overall fit between people’s
judgments and our three models, and then turn to a more de-         Figure 3: Scatter plots show overall correlations between BToM
tailed look at several representative scenarios. Two parame-        model predictions and human judgments about desires and beliefs in
                                                                    our experiment. Each dot corresponds to the mean judgment of sub-
ters β and ν were fit for the BToM model; only the determin-        jects in one experimental condition. Magenta circles correspond to
ism parameter β is relevant for the TrueBel and NoObs mod-          trials which had no rational interpretation in terms of POMDP plan-
els. Parameter fits are not meant to be precise; we report the      ning. The table shows correlations with human judgments for BToM
                                                                    and two simpler variants, which do not represent beliefs (TrueBel)
best values found among several drawn from a coarse grid.           or do not update beliefs based on observations (NoObs).
   BToM predicts people’s judgments about agents’ desires
relatively well, and less well but still reasonably for judg-       Actions often point with salient perceptual cues directly to-
ments about agents’ initial beliefs (Fig. 3). In Fig. 3, data       ward an agent’s goal or desired state. When a person wants
from the “irrational” trials are plotted with magenta circles,      to take a drink, her hand moves clearly toward the glass on
and account for most of the largest outliers. TrueBel and           the table. In contrast, no motion so directly indicates what
NoObs fit significantly worse for desire judgments and pro-         she believes to be inside the glass. Infants as young as five
vide no reasonable account of belief judgments. TrueBel’s           months can infer agents’ goals from their actions (Gergely &
belief predictions are based on the actual state of the world       Csibra, 2003), while inferences about representational beliefs
in each trial; the poor correlation with people’s judgments         seem to be present only in rudimentary forms by age one and
demonstrates that people did not simply refer to the true world     a half, and in more robust forms only by age 4 (Onishi &
state in their belief attributions. The NoObs model in prin-        Baillargeon, 2005).
ciple can infer agents’ beliefs, but without a theory of how
beliefs are updated from observations it must posit highly                                      Conclusion & Future Work
implausible initial beliefs that correlate poorly with subjects’
judgments over the whole set of experimental conditions.            Our experiment showed that human ToM inferences come
   Fig. 4 shows several revealing comparisons of human judg-        surprisingly close to those of an ideal rational model, per-
ments and model predictions in specific cases. When the             forming Bayesian inference over beliefs and desires simul-
agent follows a long path to an unseen goal (A1) it is sug-         taneously. By comparing with two alternative models we
gestive of a strong initial belief that a more desirable truck is   showed that it was necessary to perform joint inference about
present behind the wall. In contrast, going straight to a nearby    agents’ beliefs and desires, and to explicitly model the agent’s
observed truck says only that this truck is likely to be desired    observational process, as part of modeling people’s theory of
more than the others (A2). When the agent goes out of its           mind judgments. Crucially, it was also necessary to represent
way to check an unseen parking spot, sees the second truck          initial uncertainty over both the agent’s beliefs and desires.
there, and returns to the previously seen truck, it suggests a         We have not attempted to distinguish here between agents’
strong desire for the one truck not present (compare B1 to          general desires and their specific goals or intentions at par-
B2). Finally, the relative strengths of inferences about desires    ticular moments of action. In previous work we showed
and initial beliefs are modulated by how far the agent must         that inferences about which object is most likely to be an
travel to observe the unseen parking spot (compare C1 to C2,        agent’s instantaneous goal were well explained using a simi-
and C3 to C4). In each of these cases people reflect the same       lar Bayesian inverse planning framework (Baker et al., 2009).
qualitative trends predicted by the model.                          However, goals are not always about objects. In the present
   The finding that people’s inferences about agents’ desires       experiments, it feels intuitive to describe agents as attempting
are more robust than inferences about beliefs, and more con-        to maximize their overall expected utility by adopting a com-
sistent with the model’s predictions, is intriguingly consistent    bination of object- and information-seeking goals (or goals
with classic asymmetries between these two kinds of men-            intended to update the agent’s beliefs). For instance, in Fig. 4,
tal state attributions in the ToM literature. Intentional actions   B1 it looks as if the agent initially had a goal of finding out
are the joint consequence of an agent’s beliefs and desires,        which truck was parked on the other side of the wall, and then
but inferences from actions back to beliefs will frequently         after failing to find their preferred truck (M) there, set a goal
be more difficult and indirect than inferences about desires.       of returning to the previously observed second-favorite truck
                                                                2473

        Model
        People
                  A1                                  A2                                  B1                                    B2
                             L.                                   L                                    L
       K                                    K                                  K                                     K
 7                  1                  7                1                 7                  1                  7                 1
 5                                     5                                  5                                     5
 3                0.5                  3              0.5                 3                0.5                  3               0.5
 1                  0                  1                0                 1                  0                  1                 0
     K   L    M       L    M    N         K   L    M       L   M    N        K    L   M         L    M     N       K    L   M        L  M      N
                  C1                                  C2                                   C3                                   C4
                              L                                   L                                     L.                                  L.
       K                                    K                                   K                                     K
 7                  1                  7                1                 7                  1                  7                 1
 5                                     5                                  5                                     5
 3                0.5                  3              0.5                 3                0.5                  3               0.5
 1                  0                  1                0                 1                  0                  1                 0
     K   L    M       L    M    N         K   L   M        L   M    N        K    L   M         L    M     N       K    L   M        L  M      N
      Desires           Beliefs             Desires          Beliefs           Desires           Beliefs             Desires          Beliefs
Figure 4: Eight representative scenarios from the experiment, showing the agent’s path, BToM model predictions for the agent’s desires (for
trucks K, L or M, on a scale of 1 to 7) and beliefs about the unseen parking spot (for trucks L, M or no truck (N), normalized to a probability
scale from 0 to 1), and mean human judgments for these same mental states. Error bars show standard error (n=16).
(K). Our model can produce and interpret such behavior, but it             Gergely, G., Nádasdy, Z., Csibra, G., & Biró, S. (1995). Taking the
does so without positing these explicit subgoals or the corre-               intentional stance at 12 months of age. Cognition, 56, 165–193.
                                                                           Goodman, N. D., Baker, C. L., Bonawitz, E. B., Mansinghka, V. K.,
sponding parse of the agent’s motion into subsequences, each                 Gopnik, A., Wellman, H., et al. (2006). Intuitive theories of mind:
aimed to achieve a specific goal. Extending our model to                     A rational approach to false belief. In Proceedings of the Twenty-
incorporate a useful intermediate representation of goal se-                 Eighth Annual Conference of the Cognitive Science Society (pp.
                                                                             1382–1390).
quences is an important direction for future work. Even with-              Goodman, N. D., Baker, C. L., & Tenenbaum, J. B. (2009). Cause
out these complexities, however, we find it encouraging to see               and intent: Social reasoning in causal learning. In Proceedings
how well we can capture people’s joint attributions of beliefs               of the Thirty-First Annual Conference of the Cognitive Science
                                                                             Society (pp. 2759–2764).
and desires as Bayesian inferences over a simple model of                  Gopnik, A., & Meltzoff, A. N. (1997). Words, thoughts, and theo-
rational agents’ planning and belief updating processes.                     ries. Cambridge, MA: MIT Press.
                                                                           Hauskrecht, M. (2000). Value-function approximations for partially
Acknowledgments This work was supported by the JSMF                          observable Markov decision processes. Journal of Artificial Intel-
                                                                             ligence Research, 13, 33–94.
Causal Learning Collaborative, ONR grant N00014-09-0124                    Lucas, C. G., Griffiths, T. L., Xu, F., & Fawcett, C. (2009). A
and ARO MURI contract W911NF-08-1-0242. We thank                             rational model of preference learning and choice prediction by
L. Kaelbling, T. Dietterich, N. Goodman, N. Kanwisher, L.                    children. In Advances in Neural Information Processing Systems
                                                                             21 (pp. 985–992).
Schulz, and H. Gweon for helpful discussions and comments.                 Morariu, V. I., Prasad, V. S. N., & Davis, L. S. (2007). Human
                                                                             activity understanding using visibility context. In IEEE/RSJ IROS
                             References                                      Workshop: From sensors to human spatial concepts (FS2HSC).
Baker, C. L., Saxe, R., & Tenenbaum, J. B. (2009). Action under-           Onishi, K. H., & Baillargeon, R. (2005). Do 15-month-old infants
   standing as inverse planning. Cognition, 113, 329–349.                    understand false beliefs? Science, 308(5719), 255–258.
Bello, P., & Cassimatis, N. L. (2006). Developmental accounts of           Perner, J. (1991). Understanding the representational mind. Cam-
   theory-of-mind acquisition: Achieving clarity via computational           bridge, MA: MIT Press.
   cognitive modeling. In Proceedings of the Twenty-Eighth Annual          Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006). Theory-based
   Conference of the Cognitive Science Society (pp. 1014–1019).              Bayesian models of inductive learning and reasoning. Trends in
Bergen, L., Evans, O. R., & Tenenbaum, J. B. (2010). Learning                Cognitive Sciences, 10(7), 309–318.
   structured preferences. In Proceedings of the Thirty-Second An-         Ullman, T. D., Baker, C. L., Macindoe, O., Evans, O., Goodman,
   nual Conference of the Cognitive Science Society (pp. 853–858).           N. D., & Tenenbaum, J. B. (2010). Help or hinder: Bayesian
Davis, L. S., & Benedikt, M. L. (1979). Computational models of              models of social goal inference. In Advances in Neural Informa-
   space: Isovists and isovist fields. Computer Graphics and Image           tion Processing Systems 22 (pp. 1874–1882).
   Processing, 11, 49–72.                                                  Wellman, H. M. (1990). The child’s theory of mind. Cambridge,
Dennett, D. C. (1987). The intentional stance. Cambridge, MA:                MA: MIT Press.
   MIT Press.                                                              Wimmer, H., & Perner, J. (1983). Beliefs about beliefs: Represen-
Feldman, J., & Tremoulet, P. D. (2008). The attribution of mental            tation and constraining function of wrong beliefs in young chil-
   architecture from motion: Towards a computational theory (Tech.           dren’s understanding of deception. Cognition, 13(1), 103–128.
   Rep. No. RuCCS TR-87). Department of Psychology, Rutgers                Yoshida, W., Dolan, R. J., & Friston, K. J. (2008). Game theory of
   University.                                                               mind. PLoS Computational Biology, 4(12), 1–14.
Gergely, G., & Csibra, G. (2003). Teleological reasoning in infancy:       Zettlemoyer, L. S., Milch, B., & Kaelbling, L. P. (2009). Multi-agent
   The naı̈ve theory of rational action. Trends in Cognitive Sciences,       filtering with infinitely nested beliefs. In Advances in Neural In-
   7(7), 287–292.                                                            formation Processing Systems 21 (pp. 1905–1912).
                                                                       2474

