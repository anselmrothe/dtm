UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Canonical views of scenes depend on the shape of the space
Permalink
https://escholarship.org/uc/item/6358m5j2
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Ehinger, Krista A.
Oliva, Aude
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                    Canonical views of scenes depend on the shape of the space
                                         Krista A. Ehinger (kehinger@mit.edu)
                           Department of Brain & Cognitive Sciences, MIT, 77 Massachusetts Ave.
                                                 Cambridge, MA 02139 USA
                                               Aude Oliva (oliva@mit.edu)
                           Department of Brain & Cognitive Sciences, MIT, 77 Massachusetts Ave.
                                                 Cambridge, MA 02139 USA
                          Abstract                                views which make parts of the object difficult to see (Blanz,
                                                                  Tarr, & Bülthoff, 1999).
When recognizing or depicting objects, people show a
                                                                     Canonical views of objects may also reflect the ways
preference for particular “canonical” views. Are there
                                                                  people interact with objects. People show some preferences
similar preferences for particular views of scenes? We
                                                                  for elevated views of smaller objects, but ground-level
investigated this question using panoramic images, which
                                                                  views of larger objects (Verfaillie & Boutsen, 1995). The
show a 360-degree view of a location. Observers used an
                                                                  ground-level views show less of the object (because they
interactive viewer to explore the scene and select the best
                                                                  omit the top plane), but seem to be more canonical for large
view. We found that agreement between observers on the
                                                                  objects such as trucks or trains because these objects are
“best” view of each scene was generally high. We attempted
                                                                  rarely seen from above. However, these sorts of preferences
to predict the selected views using a model based on the
                                                                  may be due to greater familiarity with certain views, not
shape of the space around the camera location and on the
                                                                  functional constraints per se. Observers do not consistently
navigational constraints of the scene. The model
                                                                  select views in which an object is oriented for grasping
performance suggests that observers select views which
                                                                  (e.g., a teapot with the handle towards the viewer), and
capture as much of the surrounding space as possible, but do
                                                                  when subjects do choose these views, they don’t match the
not consider navigational constraints when selecting views.
                                                                  handle’s left/right orientation to their dominant hand (Blanz,
These results seem analogous to findings with objects,
                                                                  Tarr, & Bülthoff, 1999).
which suggest that canonical views maximize the visible
                                                                     Scenes and places, like objects, are three-dimensional
surfaces of an object, but are not necessarily functional
                                                                  entities that are experienced and recognized from a variety
views.
                                                                  of angles. Therefore, it seems reasonable to expect that
   Keywords: canonical view; scene perception; panoramic          certain views of a scene are more informative and would be
   scenes.                                                        preferred over others. However, this has not been well
                                                                  studied. Studies using artificial scenes (a collection of
                       Introduction                               objects on a surface) have shown that scene learning is
Although people can recognize familiar objects in any             viewpoint dependent, but recognition is fastest not just for
orientation, there seem to be preferred or standard views for     learned views, but also for standardized or interpolated
recognizing and depicting objects. These preferred views,         versions of the learned views (Diwadkar & McNamara,
called “canonical” views, are the views that observers select     1997; Waller, 2006; Waller, et al., 2009). For example, after
as best when they are shown various views of an object, and       learning an off-center view of a scene, viewers recognize
these are the views that people usually produce when they         the centered view of the scene about as quickly as the
are asked to photograph or form a mental image an object          learned view.
(Palmer, Rosch, & Chase, 1981).                                      There is also some evidence that there are “best” views of
   In general, the canonical view of an object is a view          real-world places. Studies of large photo databases have
which maximizes the amount of visible object surface. The         shown that different photographers tend to select the same
canonical view varies across objects and seems to depend          views when taking photos in the same location, suggesting
largely on the shape of the object. For most three-               that there is good agreement on the “best” views of these
dimensional objects (e.g., a shoe or an airplane), observers      scenes (Simon, Snavely, and Seitz, 2007). Clustering
prefer a three-quarters view which shows three sides of the       analyses of the photographs can produce a set of
object (such as the front, top, and side). However, straight-     representative views which are highly characteristic and
on views may be preferred for flatter objects like forks,         recognizable, but it is not clear that these are the “canonical”
clocks, and saws, presumably because the front of the object      views in the sense of Palmer, Rosch, and Chase (1981). For
contains the most surface area and conveys the most               example, the most commonly photographed view in a
information about object identity (Verfaillie & Boutsen,          particular cathedral could be a close-up view of a famous
1995). In addition, observers avoid views in which an object      statue in the cathedral – but this view would probably not be
is partly occluded by its parts, and they avoid accidental        considered the “best” view of the cathedral, nor would it be
                                                              2114

the view people produced if they were told to imagine the
cathedral.
   Determining the canonical view of a scene is more
complicated than finding the canonical view of an object –
in addition to rotating the view at a particular location (by
turning the head), an observer can walk around within the
space, obtaining different views from different locations.
The current study looks at only the first part of the problem:
what is the canonical view of a scene from a fixed location
within that scene? To investigate this question, we use 360-
degree panoramic images such as the one shown in Figure
1. These images are taken with a lens attached to a bell-
shaped mirror, which captures all of the views available              Figure 1: An example of a panoramic image used in the
from a particular location.                                           experiment. The smaller window shows a portion of the
                                                                      scene as it appeared in the interactive viewer during the
                          Method                                   experiment (the view shown here is the average “best view”
                                                                                      chosen by participants).
Materials
The stimuli were 624 panoramic images taken in various                                         Model
indoor and outdoor locations (classroom, lobby, chapel,
                                                                   When choosing which is the “best” view of a scene, people
parking lot, garden, athletic field, etc.). Each image was
                                                                   may attempt to maximize the amount of space visible within
3200 by 960 pixels, corresponding to 360° horizontal visual
                                                                   the view, analogous to choosing a view of an object which
angle and about 110° degrees vertical visual angle.
                                                                   shows as much of the object’s surface as possible. In
                                                                   addition, people may consider the functional constraints of
Participants
                                                                   the scene, and choose views which reflect how they would
195 people participated in the experiment through                  move in the space shown. These navigational views may be
Amazon’s Mechanical Turk, an online service where                  preferred because they are functional or because they are
workers are paid to complete short computational tasks             familiar: they are the types of views which people
(HITs) for small amounts of money. All of the workers in           experience most often as they move through the
this task were located in the United States and had a good         environment.
track record with the Mechanical Turk service (at least 100          To characterize the shape of the space around the camera
HITs completed with an acceptance rate of 95% or better).          in the panoramic scene, we marked the edges of the ground
Workers were paid $0.01 per trial.                                 plane in each image (see Figure 2a). These edges were
                                                                   defined by the boundaries of the scene (walls, fences, sides
Design                                                             of buildings) and ignored small obstructions like furniture,
Each image was seen by 10 different workers. On average, a         cars, and trees. By measuring the height of this edge in each
single worker performed 32 trials (median 9 trials).               image, were able to estimate the shape of the visible space
                                                                   around the camera, as shown in Figure 2b. (This field of
Procedure                                                          visible space around a camera location is called the “isovist”
On each trial, participants saw one panoramic image in an          in architectural research (Benedikt, 1979).) This allowed us
interactive viewing window (this window was 550 by 400             to calculate the distance to the wall in any direction around
pixels, corresponding to about 60° by 45° visual angle).           the camera (“visible depth”), the total volume of space
Observers could change the view shown in the window by             around the camera location, and, for any particular camera
clicking and dragging the image with the mouse; this gave          view, what percentage of the total space was captured
the effect of turning and looking around in the scene. The         within that view. This percentage, calculated for the full 360
initial view of the scene was chosen randomly at the start of      degrees of possible views around the camera, is the “volume
each trial.                                                        map” shown in Figure 2c.
   There were two tasks on each trial: first, type a name for        To characterize the navigational affordances of the scene,
the location shown in the panoramic image (e.g. “kitchen”);        we marked the walking paths in each image using an online
and second, manipulate the viewer window to get the best           task on Amazon’s Mechanical Turk. Workers participating
possible view of the location. Specifically, participants were     in this task saw an unwrapped panoramic image (as in
told to imagine that they were photographers trying to take        Figure 1) and were asked place arrows on each of the paths,
the best possible snapshot of the scene.                           which included sidewalks, hallways, staircases, and
                                                                   navigable spaces between furniture or other obstacles. Since
                                                                   some images did not contain clearly defined walking paths
                                                                   (for example, a large, open field may not contain any
                                                               2115

          Figure 2: (a) A panoramic image with the ground line outlined in white and arrows marking the navigational
          paths (black arrowheads represent “best” views of this scene chosen by participants). (b) An overhead view
            of the same location (the grey region represents the portion of the space captured within a single camera
                                   view). (c) The volume and navigational maps for this scene.
marked paths – it is possible to walk in any direction),           does not distinguish between random distributions of views
workers were given the option to mark a checkbox (“this is         and some types of multimodal distributions (such as views
a large, open space”) in addition to marking any paths that        clustered around two angles 180 degrees apart). Examples
they did see in the image. Along with instructions, workers        of scenes with high, moderate, and low agreement are
were given several examples of correctly- and incorrectly-         shown in Figure 3.
marked images, followed by a test in which they were                  Agreement (measured as the standard error in the views
required to correctly mark a set of example images. Three          selected by participants) was correlated with some aspects
different workers marked the paths in each image; each             of the scene layout. Specifically, standard error in views was
received $0.03 per image. None of the workers in this path-        correlated with the overall volume of space around the
marking task had participated in the experiment.                   camera location, as calculated from the volume map (r =
  A Gaussian distribution was centered on each of the              0.30). Similarly, standard error in views was correlated with
marker locations in the image and the responses from the           the percent of subjects who marked the scene as a “large,
three workers were summed to create the “navigational              open space” during the path-marking task (r = 0.22). These
map” shown in Figure 2c. This map gives an estimate of the         correlations indicate that agreement on the “best” view was
navigability of all possible views around the camera               higher in small spaces, and lower in spaces that were large
location.                                                          and open. Agreement was also related to the range of
                                                                   distances visible from the camera location. The standard
                           Results                                 error in views was negatively correlated with the standard
                                                                   deviation of visible depths (r = -0.40). In other words,
Experiment results                                                 agreement on the “best” view was higher in scenes that
                                                                   showed a variety of closer and farther views than in scenes
Trials were excluded if the worker did not name the location       where all views were about equally distant.
shown in the image (1% of trials) or did not use the viewer           Agreement was significantly higher in indoor than in
to explore the scene and simply submitted the initial view as      outdoor scenes (t(246.8) = 5.81, p < .001). This is likely due
the best view (3% of trials). 251 out of 6240 trials were          to differences in the spatial envelope of these spaces (Oliva
excluded under these criteria.                                     & Torralba, 2001): outdoor scenes tend to be much larger
  In general, agreement on the “best view” of a scene was          and more open than indoor scenes, and indoor scenes are
high: the average circular standard error of the angles            more likely to have complex shapes offering a range of
selected by observers was 12.7 degrees. Significance was           closer and farther views.
measured using Rayleigh’s test of nonuniformity, which                There was also a relationship between view agreement
tests the significance of a mean angle in a circular               and name agreement from the naming portion of the task.
distribution by comparing it to the mean angle that would be       The standard error of the angles chosen by observers was
expected from a distribution of random angles. This test           negatively correlated with the percent of people giving the
returned p < .01 for 389 scenes (62% of the image set), and        dominant name for the scene (r = -0.44). This means that
p < .05 for 466 scenes (75% of the image set)). This may be        when observers agreed on the identity of the scene, they also
a conservative estimate of agreement, since Rayleigh’s test
                                                               2116

                                                                                                                                                                 2117
Figure 3: Example scenes; black arrowheads represent views chosen by participants. The top row are the three scenes with highest agreement, followed by scenes
                      at the 75th, 50th, and 25th percentile of agreement. The bottom row shows the three scenes with lowest agreement.

tended to agree on the “best” view of the scene, but when          other hand, the volume model does show better performance
observers disagreed on a scene’s identity, they were also          in closed than in open scenes (AUC = 0.78 and 0.74,
likely to choose different “best” views of the scene.              respectively). Figure 5 shows examples of high and low
                                                                   performance from the volume and navigational models.
Model performance                                                    We also tested a combined model, which attempted to
One image was dropped from the modeling because it was a           predict selected views using both a weighted sum of the
very small space with no visible floor, so its volume map          volume and navigational maps. However, this model
was undefined Volume and navigation maps were calculated           performed worse than the volume map alone, and gave
for the remaining 623 images as described in the previous          better performances as the weight of the navigational map
section. We then tested how well each of these maps could          approached zero. This suggests that the navigational model
predict the “best” views selected by observers.                    does not add any indpendent predictive power; it performs
   Model performance was assessed using ROC curves                 above chance because it tends to select the same regions as
(Figure 4). ROC curves show the detection rate of a model          the volume map (in scenes, a view that shows a large
relative to its false alarm rate. In this case, the ROC curves     volume usually also affords navigation).
show the proportion of human observers’ “best” views
which can be predicted by each map when it is threshold at                                Conclusion
a range of values. The area under the ROC curve (AUC) can            Just as people show clear preferences for certain views of
be used as a measure of a model’s overall performance. A           objects, there seem to be agreed-upon “best” views of
model performing at chance produces an ROC curve that is           scenes. This is not surprising, given previous findings in
a diagonal line with an AUC of 0.5. AUC values closer to 1         scene research, for example, the fact that people tend to use
indicate better model performance.                                 similar viewpoints when photographing famous locations.
   The volume model gives the best prediction of the views         Overall, it seems that the way people choose a canonical
selected by observers (AUC = 0.75), but the navigational           view of a scene may be very similar to the way they select
model also performs above chance (AUC = 0.62). The                 the canonical view of an object. Choosing the “best” view
performance of the navigational model does not change              of an object or a scene poses essentially the same problem:
when very open scenes (which may not have clear paths) are         how to compress as much 3D visual information as possible
excluded from the analysis. On 426 “closed” scenes (scenes
that were never marked as “open space” during the park-
marking task), the navigational model’s AUC was 0.61; on
the remaining “open” scenes the AUC was 0.62. On the
                                                                        Figure 5: Example of a scene in which both models
    Figure 4: ROC curves for the volume and navigational              performed very well (top) and an example of a scene in
    models. The gray line represents chance performance             which both models performed poorly (bottom). Arrowheads
                      (chance AUC = 0.5).                                   mark the “best” views chosen by observers.
                                                               2118

into a necessarily limited 2D view.                                objects has been used to argue for a viewpoint-dependent
   When selecting canonical views of objects, people seem          theory of object recognition, in which objects are stored in
to be trying to maximize the amount of visible surface: they       memory as a collection of typical or informative views, and
select views which show at least two sides of the object, and      recognition involves matching incoming visual information
avoid occlusions and accidental views. Similar constraints         to these stored views (Edelman & Bülthoff, 1992; Cutzu &
seem to apply in scenes. The canonical view from a                 Edelman, 1994). The existence of canonical views of scenes
particular location is dependent on the shape of the space         could suggest a similar view-based representation for
around that location: people show preferences for views that       memory and perception of scenes.
shows as much of the surrounding space as possible. It’s not
clear whether people choose these large-volume views                                   Acknowledgments
because they wish to capture the space itself, or because          The authors would like to thank Ken M. Haggerty for his
they wish to capture the things that fill that space (objects,     work preparing stimuli for this project. K.A.E. is funded by
textures, etc.). Further work will be required to distinguish      an NSF graduate research fellowship. This research was
between these two possibilities.                                   partly funded by an NSF Career award (0546262), NSF
   There is also some evidence that the canonical view of an       grants (0705677 and 1016862) and a NEI grant (EY02484)
object reflects the way people usually see the object, or the      to A.O.
way they interact with the object. However, our results
suggest that the canonical view of a scene is not based on
functional constraints. Although the canonical view of a                                   References
scene is often a navigationally-relevant view (a walkway, a        Blanz, V., Tarr, M. J., & Bülthoff, H. H. (1999). What
corridor), our modeling results suggest that these views are         object attributes determine canonical views? Perception,
selected because they show a large amount of the                     28, 575-599.
surrounding space, not because they afford navigation.             Benedikt, M. L. (1979). To take hold of space: Isovists and
   It may be the case that the canonical view of a scene is          isovist fields. Environment and Planning B, 6, 47–65.
not the functional view. There is some evidence that people        Cutzu F., & Edelman S. (1994). Canonical views in object
do not have a specific functional view in mind when they             representation and recognition. Vision Research, 34,
choose canonical views of objects (for example, Blanz, Tarr,         3037-3056.
and Bülthoff (1999) showed that people do not prefer views         Diwadkar, V. A., & McNamara, T. P. (1997). Viewpoint
of objects oriented for grasping). On the other hand, people         dependence in scene recognition. Psychological Science,
may consider functional constraints other than navigation            8, 302–307.
when choosing a canonical view of a scene. Navigation is a         Edelman S., & Bülthoff, H. H. (1992). Orientation
very general function of scenes; most scenes also afford             dependence in the recognition of familiar and novel views
more specific functions (sitting in a theater, shopping in a         of three-dimensional objects. Vision Research, 32, 2385-
store, etc.). If canonical views of scenes do reflect                2400.
functional constraints, it seems quite likely that they would      Oliva, A., & Torralba, A. (2001). Modeling the shape of the
reflect these more specific functions rather than a general          scene: A holistic representation of the spatial envelope.
function like navigation. Further work will be needed to             International Journal in Computer Vision, 42, 145-175.
quantify these specific functional constraints and determine       Palmer, S., Rosch, E., Chase, P., (1981). Canonical
how they affect view selection in scenes.                            perspective and the perception of 40 objects. In Attention
   It should also be noted that there are many other factors         and Performance IX, Ed. J. Long, A. Baddeley (Hillsdale,
that could affect choice of view in addition to the two              NJ: Lawrence Erlbaum), pp. 135-151.
factors modeled here. As noted above, people may prefer            Simon, I., Snavely, N., Seitz, S. M. (2007). Scene
views of an environment which show a large number or                 Summarization for Online Image Collections. In Proc. Of
large variety of the objects within that environment, and this       the 11th International Conference on Computer Vision.
may explain the preference for views which show a large            Verfaillie, K. & Boutsen, L. (1995). A corpus of 714 full-
amount of the surrounding space. People may also prefer              color images of depth-rotated objects. Perception &
views which show specific objects, such as ones which are            Psychophysics, 57, 925-961.
central to the function or identity of a place (such as cars in    Waller, D. (2006). Egocentric and nonegocentric coding in
a parking lot, or the stage in a theater). Aesthetics may also       memory for spatial layout: Evidence from scene
play a role in the selection of a “best” view of a place:            recognition. Memory & Cognition, 34, 491 - 504.
people may be biased towards views which have high                 Waller, D., Friedman, A., Hodgson, E. & Greenauer, N.
symmetry or are otherwise aesthetically pleasing. Many of            (2009). Learning scenes from multiple views: Novel
these factors can be quantified and should be included in a          views can be recognized more efficiently than learned
full model of view preference in scenes.                             views. Memory & Cognition, 37, 90 - 99.
   Identifying the canonical views of scenes may help in
understanding how scenes are represented in memory and
perceptual processes. The existence of canonical views of
                                                               2119

