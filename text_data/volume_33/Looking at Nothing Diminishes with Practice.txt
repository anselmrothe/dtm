UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Looking at Nothing Diminishes with Practice

Permalink
https://escholarship.org/uc/item/21m7h5xx

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)

Authors
Scholz, Agnes
Mehlhorn, Katja
Bocklisch, Franziska
et al.

Publication Date
2011-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Looking at Nothing Diminishes with Practice
Agnes Scholz1 (agnes.scholz@psychologie.tu-chemnitz.de)
Katja Mehlhorn2 (s.k.mehlhorn@rug.nl)
Franziska Bocklisch1 (franziska.bocklisch@psychologie.tu-chemnitz.de)
Josef F. Krems1 (josef.krems@psychologie.tu-chemnitz.de)
1

2

Department of Psychology, Chemnitz University of Technology, Germany
Dept. of Artificial Intelligence and Dept. of Experimental Psychology, University of Groningen, the Netherlands

Abstract
People fixate on blank locations if relevant visual stimuli
previously occupied that location; the so-called ‘looking-atnothing’ effect. While several theories have been proposed to
explain potential reasons for the phenomenon, no theory has
attempted to predict the stability of this effect with practice.
We conducted an experiment in which participants listened to
four different sentences. Each sentence was associated with
one of four areas on the screen and was presented 12 times.
After every presentation participants heard a statement
probing one sentence, while the computer screen was blank.
More fixations were found to be located in areas associated
with the probed sentence than in other locations. Moreover,
the more trials participants had completed, the less frequently
they exhibited looking-at-nothing behavior. Fixations on
blank locations seem to occur when an attempt is made to
retrieve information associated with a spatial location as long
as it is not strongly represented in memory.
Keywords: Eye tracking, practice, spatial cognition, mental
representation, working memory

Introduction
When processing information from the visual world, human
cognition integrates visual and auditory input with abstract,
higher level mental representations (Huettig, Olivers, &
Hartsuiker, 2010). Reactivation of such a memory
representation leads the gaze back to spatial locations or
areas that were previously occupied by relevant information.
For example, when we mention something about a table
presented on a whiteboard, we might point towards the
whiteboard, even if the table is no longer there anymore.
Richardson and Spivey (2000) were among the first to
show a close link between eye movements, auditory
information processing and semantic information
processing, in an information-retrieval task. Participants
were presented with a spinning cross in one of four equalsized areas on a computer screen together with spoken
factual information. After four facts were presented,
participants heard a statement probing one of the presented
facts and had to judge the truth of the statement. During this
retrieval phase the computer screen was blank. Participants
fixated more in the critical area where the sought-after

information was presented compared to other areas on the
screen.
This so-called ‘looking-at-nothing’ behavior (Ferreira,
Apel, & Henderson, 2008) also occurs when the probed
information is presented visually (Laeng & Teodorescu,
2001; Renkewitz & Jahn, 2010; Spivey & Geng, 2001),
when information is anticipated (Altmann & Kamide, 2007),
in light and in complete darkness (Johansson, Holsanova &
Holmqvist, 2006), and for simple (Brand & Stark, 1997) and
more complex pictures (Johansson, Holsanova, &
Holmqvist, 2010).
Ferreira et al. (2008) assumes a memory representation of
an object or event that integrates visual, auditory and spatial
information and leads to a corresponding visual, linguistic,
spatial, and conceptual representation. When one part of this
integrated memory representation is reactivated, other parts
are retrieved, as well. This in turn causes gazing behavior
toward the location where the information was previously
presented. For example, seeing a table on a whiteboard leads
to the activation of a visual as well as conceptual
representation of the figure. Additionally, spoken language
leads to the formation of a linguistic representation. The
visual world leads to the activation of a spatial index
(Pylyshyn, 2001), which can be used later to direct our gaze
back to the area on a whiteboard, where the figure was
previously presented.
Huettig et al. (2010) recently proposed a general
framework to describe how linguistic and visual
representations are bound together in an integrated memory
representation. Their model, like that of Ferreira et al.
(2008), assumes the integration of information in a
connected visual, linguistic, spatial, and conceptual
representation. It further includes ideas proposed by
Altmann and Kamide (2007), Knoeferle and Krocker
(2007), and Spivey (2007). Here, we briefly introduce their
framework. It is worthwhile to note that they include a
detailed description of how integrated memory
representations can be linked to existing theories of longterm and working memory (c.f., Baddeley, 2000). Huettig et
al. (2010) propose that language–vision interactions are
based on long-term memory, where conceptual
representations (e.g., the concept of a figure or of a

1070

whiteboard) are derived from. Therefore, long-term memory
serves as a stable knowledge base. It is then working
memory that grounds cognition in space and time and leads
to the formation of short-term connections between objects
(e.g., spoken language, a figure, and a whiteboard).
Contents of working memory are linked to contents of longterm memory via spatial indices. Because of this association
working memory can instantiate a gaze back to the object.
In describing connections between memory representations,
Heuttig et al. (2010) assume that the stronger the association
between the linguistic and conceptual representations the
higher “the probability of triggering a saccadic eyemovement” (p. 5).
Richardson, Altmann, Spivey, and Hoover (2009) share
Huettig at al.’s (2010) general idea of an integrated memory
representation. In contrast, however, they suggest that only
sparse internal representations are built during the encoding
of information. They assume that during information
retrieval, an eye movement can be launched to the
associated area in order to gather more information. This
occurs when the spatial pointer (i.e., the visual part of the
integrated memory representation) does not include the
searched information: “If the pointer’s tag does not include
the attribute, then the pointer’s address to the external
environment is the next obvious resource” (Spivey, 2007, p.
298). The link between information sampling from the
environment and eye movements can be understood as the
covert orienting of visual–spatial attention (Hoffman, &
Subramaniam, 1995). Targeting a position makes it
necessary to allocate attention towards that place. Because it
is impossible to make an eye movement without an
attentional movement (Shepherd, Findlay, & Hockey,
1986), attending to information stored in an integrated
memory representation leads to eye movements towards
associated spatial areas.
Summarizing, we conclude that during the encoding of
information an integrated memory representation is formed
from different modalities. However, theories diverge in
terms of how much information is included in the memory
representation and how this in turn affects the looking-atnothing behavior. Ferreira et al. (2008) assume that the
probability of triggering an eye movement increases with
the strength of the association between the linguistic and
conceptual representation. Consequently, one could predict
that looking-at-nothing behavior becomes stronger with an
increasing association between these representations. Spivey
(2007), on the other hand, proposes that looking at nothing
mainly occurs for the purposes of gathering information not
yet included in the mental representation. In line with this
one might conclude that looking at nothing diminishes as
relevant information is included in the memory
representation.
To test these assumptions we varied the degree to which
information is included in memory representation. More
precisely, we manipulated the degree of practice in a task,
where auditory information, which is associated with
contents from a visual scene, has to be retrieved from

memory. With more practice, the strength with which
retrieval-relevant information is represented in memory
increases (e.g., Anderson & Schooler, 1991). If looking at
nothing increases with practice, then Huettig et al.’s
assumptions would be supported. On the other hand, if
looking at nothing decreases with practice, our findings
would support Spivey (2007) and conclude that looking at
nothing varies with the degree of relevant information
included in the mental representation.

Experiment
To test looking-at-nothing behavior under different levels of
practice we conducted an experiment in which participants
were presented with four different sentences. Each sentence
described an artificial scene. The same set of four sentences
was presented in each of 12 experimental trials. After every
presentation trial a retrieval phase followed in which one of
the four sentences was probed. In every trial each sentence
was associated with the same spatial location on a computer
screen.

Method
Participants. Eighteen students (14 female; age M = 22.8)
from Chemnitz University of Technology participated in the
experiment. All reported normal or corrected-to-normal
vision with contact lenses. All participants were native
German speakers.
Apparatus and material. Participants were seated in front
of a computer screen at a distance of 630 mm and instructed
to position their head in a chin rest. The eye-tracker system
SMI iView REDpt was used to sample data of the right eye
at 50Hz with a precision of 0.05°. Data were recorded with
iView X 1.7 and analyzed with BeGaze 2.3 and MatLab
7.0.1 software programs. Stimuli in the experiment were
presented using E-Prime 2.0 on a 380-mm × 305-mm
computer screen with a resolution of 800 × 600 pixels.
The visual stimuli consisted of a grid dividing the screen
into four equal-sized areas with a fixation cross at the center
of the grid. Each set of four sentences was associated with
the same symbol – a black circle with a white loudspeaker
in it – which appeared in one of the four areas of the grid
depending on the sentence that was presented.
The auditory stimuli presented in the presentation trial
consisted of four prerecorded sentences each describing
three attributes of an artificial scene (e.g., “There is a place
with a purple lighthouse, a sickle bay, and a wooden
church.”). To test gaze behavior in the retrieval phase, we
generated 24 statements: A true and a false version for each
of the four statements multiplied by three attributes (The
false statement probing the example sentence from above
was “There is a place with a wooden cottage.”). Figure 1
shows 1 of the 12 experimental trials.
Procedure. To mask study intentions, students were told
they were participating in a study concerning pupil dilation
that involved solving a memory task. No instructions

1071

concerning gaze behavior were provided. The eye tracker
was calibrated using a 9-point calibration method. This
procedure lasted between 5 and 10 min. Subsequently, the
12 experimental trials started. In each of the 12 trials, the
same four sentences were presented in random order. Every
sentence always appeared with the symbol in the same area
on the screen with a presentation duration of 30 s.

Figure 1: Example trial with the four experimental
sentences (presentation phase) and a statement probing the
first sentence (retrieval phase). Original material in German.
After presentation of the fourth sentence within a trial, the
retrieval phase followed. Participants heard a statement,
which referred to a fact from one of the four sentences, and
judged it to be true or false. To observe participants’ gaze
behavior, they were intentionally not instructed to reply as
soon as possible. Presentation of one statement lasted 4 s.
Statements were randomly assigned to trials and participants
with the restriction that every statement was probed once for
each participant. Participants had to answer the true or the
false version of a statement balanced across trials and
participants such that every participant was presented with
six true and six false statements. A true statement was
recorded when participants responded verbally with ‘right’
and a false statement with ‘wrong’. Immediately following
this response, the investigator pressed a key signaling the
start of the next trial. In this way, participants were not
required to look at the keyboard (This procedure was chosen
to prevent gazing away from the monitor towards the
keyboard, which could have led to loss in quality of eyetracking data). After depressing the key, the investigator
noted the particpant’s response on a sheet of paper. During
the 12 experimental trials and their retrieval phases, gaze
data were recorded. Afterwards, participants filled out a
questionnaire which interrogated demographic variables and
the assumed goal of the study. Before leaving, participants
were informed about the true nature of the study.
Analysis. To assess participants’ performance, we collected
data on the accuracy of their responses and response times
(i.e., the time beginning with the retrieval phase and ending
with a participant’s reply as noted by the investigator). As

reaction times are prone to error through outliers (e.g., when
an investigator does not stop recording immediately upon a
participant’s response) we did not exclude outliers but used
median reaction times for further analysis.
To assess looking at nothing, gaze data from the
beginning of the retrieval phase to a participant’s reply (i.e.,
analogous to response time) was analyzed. Four adjacent
‘areas of interest’ (AOIs) were defined corresponding to the
four areas on the screen. Numbers of fixations in every AOI
were counted per person and per trial. A fixation was
defined as having a minimum duration of 100 ms and a
maximum dispersion of 100 pixels (1.3° visual angle). The
AOI associated with a probed sentence is called the ‘critical
area’. Gaze behavior was analyzed, whereby trials were
discarded in which tracking data was missing for >40% of
the trial duration (8% of all trials). Missing tracking data
was caused by blinks, lost pupil or corneal reflectance, or
looking away from the screen.
To test the independent variable practice, we aggregated
the number of fixations in the AOIs as well as the
performance data over sets of four experimental trials. This
allowed us to compare three conditions of practice: block 1
(consisting of trials 1–4), block 2 (trials 5–8), and block 3
(trials 9–12).
Number of fixations and median reaction times were only
analyzed for trials that were answered correctly.

Results
Performance measures. Overall, mean percentage of
correct responses to the statements was M = 87.8% (SD =
20.8%), suggesting that the material was neither too difficult
to memorize nor too easy to learn. A one-way repeated
measures ANOVA revealed a significant effect for accuracy
over the three blocks, F(2,34) = 11.04, p < .001, ηp2 = .40.
Bonferroni post-hoc tests showed an increase in
performance from the first to the second block, Mb1 = 73%
vs. Mb2 = 93%, p = .004, and from the first to the third
block, Mb1 = 73% vs. Mb3=97%, p =.005. There was no
significant change in performance from the second to the
third block, Mb2 = 93% vs. Mb3 = 97%, p = 1.00.
The median reaction time to the statement in the retrieval
phase was 6206 ms (SD = 1617 ms). Over the three blocks
of practice participants became faster in correctly
responding, Greenhouse–Geisser-corrected F(1,48;34) =
9.61, p = .002, ηp2 = .36.
Bonferroni post-hoc tests confirm a decrease in the
median reaction times from the first to the second block Mb1
= 7211 ms vs. Mb2 = 5798 ms, p = .016 and from the first to
the third block, Mb1 = 7211 ms vs. Mb3=5608 ms, p = .009.
Again, there is no difference between the second and the
third block, Mb2 = 5798 ms vs. Mb3 = 5608 ms, p = 1.00.
Response accuracy and median reaction times showed that
the practice manipulation was successful. With more
practice, participants answered correctly more often and
replied more quickly to the statements.

1072

Mean number of fixations.
Exemplary gaze behavior of a typical participant. Figure 2
shows scan paths of a typical participant for the presentation
and the retrieval phase of three trials, where the critical area
was on the bottom right. Lines show saccades and circles
represent fixations with bigger circles indicating longer
fixations. Scan paths on the top left and right side of Figure
2 show a trial from block 1. In this trial, the sentence that
was associated with the symbol in the bottom right area of
the screen was probed for the first time.
Presentation phase

Retrieval phase
Block 1

Block 2

the experiment. In block 1, the participant directs several
gazes to the critical area (Figure 2, top right). With
increasing practice, fewer fixations in the critical area are
made (middle and bottom right).
Aggregated gaze behavior. Figure 3 shows the proportion of
fixations in the critical area during the retrieval phase.
Proportions were aggregated for each block and across
participants. Participants showing looking-at-nothing
behavior should fixate in the critical area during the retrieval
phase. To test this, for each block, we compared the
proportion of fixations in the critical area with a chance
level of 25%. In block 1, the proportion of fixations in the
critical area (37.2 %) is indeed above chance, tb1(17) = 2.09,
p = .051, g = .99. In blocks 2 and 3 the proportion of
fixations in the critical area were at chance levels, mean
proportion block 2: 17.9 %, tb2(17) = –1.73, p = .102, g =
.82; mean proportion block 3: 28.5 %, tb3(17) = 0.81, p =
.426, g = .38. These results suggest that looking at nothing
diminished from block 1 to block 2 and that the proportion
of fixations did not vary meaningfully from chance in block
3.

Block 3

Figure 2: Scan paths of one participant for a trial in block 1
(top), a trial in block 2 (middle) and a trial in block 3
(bottom) with the critical area at the bottom right.
Left: presentation phase (scan paths of four sentence
presentations)1, right: retrieval phase.
Scan paths on the left and right side in the middle of Figure
2 show a trial from block 2. In this trial, the sentence on the
bottom right was probed for the second time. Scan paths on
the left and right side on the bottom of Figure 2 show gaze
behavior when the sentence was probed for the third time
(block 3). Comparing scan paths from top to bottom on the
left side of Figure 2, scan paths reveal that throughout the
experiment the participant kept on following the symbols
during the presentation phase. In comparison, gaze behavior
in the retrieval phase (Figure 2, right) seems to change over
1

Longer fixations at the bottom right area are only shown by
displayed data and not systematically. To control for gaze biases
the critical area was randomized across trials.

Figure 3: Percentage of fixations in the critical area across
blocks. Error bars represent standard error, dotted line
indicates chance level.

Discussion
Theories on the link between eye movements and auditory
and semantic information processing (Huettig et al., 2010)
assume that during the encoding of information an
integrated memory representation is formed from different
modalities. However, these theories do not agree on how
much information is included in the memory representation.
Using the looking-at-nothing paradigm, we tried to shed
some light on this question.
Assuming an integrated memory representation as
proposed by Ferreira et al. (2008), the probability of
triggering an eye movement during retrieval of information
from memory will increase with the strength of the
association between the different parts of the representation.
Spivey (2007), on the other hand, proposed that only sparse
internal representations are built during the encoding of

1073

information. Consequently, eye movements during memory
retrieval occur mainly to gather information that is not yet
included in the mental representation. According to Ferreira
et al. (2008), looking at nothing should increase with
practice, while for Spivey (2007) the same behavior should
diminish with practice.
Practice was induced by presenting participants with a set
of four sentences, 12 times. Each presentation phase was
followed by a retrieval phase where one sentence was
probed. To test whether the manipulation was successful,
we first checked if participants showed increasing
performance in the retrieval task. Results show that over the
three blocks, participants indeed replied with increasing
accuracy and speed to the facts probing the presented
sentences. Accuracy as well as response times revealed that
the performance increase was stronger from the first to the
second block, than from the second to the third block. It
seems that over the three blocks of practice memory
associations for the sentences were strengthened leading to
more correct and faster responses. Therefore, we conclude
that the practice manipulation was successful.
The question we wished to answer was how looking-atnothing behavior would be affected by the content of the
memory representation. In block 1, participants looked more
often to the critical area on the screen than a chance level of
25% would predict. In blocks 2 and 3 looking at nothing
diminished. In both blocks, fixations in the critical area did
not amount to more than that predicted by a chance level of
25%.
Results of the first block replicated results of Richardson
and Spivey (2000), which showed a close relationship
between gaze behavior and language processing. In block 1,
information was not strongly represented in memory. Eye
movements were launched to the critical area on the screen
in order to collect information from the visual scene. For
blocks 2 and 3 we assumed that the looking-at-nothing
behavior would become stronger or diminish, respectively.
Our results were not in line with the predictions of Huettig
et al. (2010), which stated that looking at nothing becomes
stronger as the association in memory is strengthened.
While performance improved over the three blocks, looking
at nothing did not increase in strength. Our results seem to
support the assumption of Spivey (2007) that looking-atnothing behavior is executed to gather more information
from the environment. In blocks 2 and 3, the memory
representation might have included all relevant information.
Thus, addressing an eye movement to the critical area on the
screen became ‘unnecessary’.
We found that looking at nothing varies with the content
of the memory representation. This supports the work of
Richardson et al. (2009), who assume the existence of an
internal memory story, whereby all relevant information is
stored in an integrated memory representation, and an
external memory store (O’ Regan, 1992), which assumes
only sparse memory representations and uses a spatial index
to address the visual world. Moreover, these are not
mutually exclusive abilities of the cognitive system. Instead,

the cognitive system can use both. The question is, when do
we rely on an internal memory representation and when on
an external memory store? Hoover and Richardson (2008)
and Johansson et al. (2010) suggest that looking at nothing
helps to relieve working memory when information is
retrieved from memory. For example, Johansson et al.
(2010) presented participants with an auditory description of
a complex scene while participants had to fixate the center
of a whiteboard. In a second condition they saw the picture
of a complex scene but again had to fixate on the center of
the picture’s scene. In both conditions, when they had to
retell the information they had heard, and when they had to
describe the visual scene, they drew the scene with their
eyes on the whiteboard and did not maintain a central
fixation. In contrast, in a study reported by Brand and Stark
(1997), simple block patterns were used. During retrieval of
the block pattern, participants were allowed to look freely
around the scene but kept a central fixation. Therefore,
Johansson et al. (2010) argue that looking-at-nothing
behavior can relieve working memory load when task
demands (e.g., a complex scene description) require it.
Applying the findings of Johansson et al. (2010) to our
results suggests that when memory load is high, looking at
nothing is shown. When memory load is low – because all
relevant information has been learned – looking-at-nothing
behavior diminishes. Indeed, in block 1 of our study, when
the presented material was new to participants, looking at
nothing was shown. Later, when the material was strongly
represented in memory, looking at nothing diminished.
Decreased looking-at-nothing behavior might also be
explained as the result of participants realizing over the
course of the experiment that the visual area they refixate on
no longer includes relevant information and therefore, this
behavior becomes redundant. This implies that participants
consciously control their gaze behavior. However, eye
movements as described in the context of the looking-atnothing effect are a highly automatic and unconscious
behavior (Rayner, 2009). Furthermore, if change in gaze
behavior were due to conscious control (i.e., participants
realize that during the retrieval phase, nothing is present
anymore), we would then expect looking at nothing to
diminish within the first block. Looking at data of the first
four trials, we could not find such a tendency. Moreover, in
the post-questionnaire participants did not report that they
controlled their gaze behavior.
We also realize that looking at nothing might not only
diminish because participants have learned the material, but
because they have given an automatic response to the
stimuli that does not include fixations to the critical area. To
rule out this alternative explanation one could present
participants with the same sentences throughout the course
of the experiment and sentences that change from trial to
trial. If it is indeed the content of the integrated memory
representation that is responsible for looking-at-nothing
behavior, our results should be replicated in a way that
looking-at-nothing behavior diminishes for stable sentences
and does not diminish for new sentences.

1074

From the results of this study it can be concluded that
information is represented internally, and that under certain
conditions the external world is addressed in order to gather
more information (Spivey, 2007). We have further shown
that both ways of retrieving information are not necessarily
mutually exclusive (Richardson et al., 2009). But, when is
knowledge presented internally and when do we use an
external memory store? We propose that working memory
load may influence the decision to use either an internal or
external memory store. However, a distinct boundary need
not be imposed between these two modes of storage. Spivey
(2007) proposes that knowledge representations can be
described in a vague manner. That is, information can
belong to both internal and external storages. Bocklisch,
Bocklisch, Baumann, Scholz, and Krems (2010) highlighted
a relationship between the concept of vagueness and
knowledge representations. This link could inform future
research that tests the usefulness of this approach for the
investigation of mental representations.

Acknowledgments
We would like to thank Nina Bär for helpful comments on
previous versions of this paper and Lars Eberspach for his
help in conducting the experiment.

References
Anderson, J. R., & Schooler, L. (1991). Reflections of the
environment in memory. Psychological Science, 2, 396–
408.
Altmann, G.T. (2004). Language-mediated eye movements
in the absence of a visual world: the ‘blank screen
paradigm’. Cognition, 93, 79–87.
Altmann, G.T., & Kamide, Y. (2007). The real-time
mediation of visual attention by language and world
knowledge: linking anticipatory (and other) eye
movements to linguistic processing. Journal of Memory
and Language, 57, 502–518.
Baddeley, A. (2000). The episodic buffer: A new
component of working memory? Trends in Cognitive
Sciences, 4, 417−423.
Bocklisch, F., Bocklisch, S.F., Baumann, M.R.K., Scholz,
A., & Krems, J.F. (2010). The role of vagueness in the
numerical translation of verbal probabilities: A fuzzy
approach. In S. Ohlsson & R. Catrambone (Eds.),
Proceedings of the 32nd Annual Conference of the
Cognitive Science Society (pp. 1974−1979). Austin, TX:
Cognitive Science Society.
Brandt, S.A., & Stark, L.W. (1997). Spontaneous eye
movements using visual imagery reflect the content of the
visual scene. Journal of Cognitive Neuroscience, 9,
27−38.
Ferreira, F., Apel, J., & Henderson, J.M. (2008). Taking a
new look at looking at nothing. Trends in Cognitive
Science, 12(11), 405−410.
Hoffman, J. E., & Subramaniam, B. (1995). The role of
visual attention in saccadic eye movements. Perception &
Psychophysics, 57(6), 787−795.

Hoover, M.A., & Richardson, D.C. (2008). When facts go
down the rabbit hole: contrasting features and objecthood
as indexes to memory. Cognition, 108, 533–542.
Huettig, F., Olivers, C. N. L., & Hartsuiker, R. J. (2010).
Looking, language, and memory: Bridging research from
the visual world and visual search paradigms. Acta
Psychologica.
Advance
online
publication.
doi:10.1016/j.actpsy.2010.07.013.
Johansson, R., Holsanova, J., & Holmqvist, K. (2010). Eye
movements during mental imagery are not reenactments
of perception. In S. Ohlsson & R. Catrambone (Eds.),
Proceedings of the 32nd Annual Conference of the
Cognitive Science Society (pp. 1968-1973). Austin, TX:
Cognitive Science Society.
Johansson, R., Holsanova, J., & Holmqvist, K. (2006).
Pictures and spoken descriptions elicit similar eye
movements during mental imagery, both in light and in
complete darkness. Cognitive Science, 30, 1053–1079.
Knoeferle, P., & Crocker, M. W. (2007). The influence of
recent scene events on spoken comprehension: Evidence
from eye movements. Journal of Memory and Language,
57(4), 519−543.
Laeng, B., & Teodorescu, D.S. (2002). Eye scanpaths
during visual imagery re-enact those of perception of the
same visual scene. Cognitive Science, 26, 207–231.
O’Regan, J. K. (1992). Solving the ‘real’ mysteries of visual
perception: The world as an outside memory. Canadian
Journal of Psychology, 46, 461−488.
Pylyshyn, Z. (2001). Visual indexes, preconceptual objects,
and situated vision. Cognition, 80, 127−158.
Rayner, K. (2009). The 35th Frederik Bartlett lecture: Eye
movements and attention in reading, scene perception and
visual search. The Quarterly Journal of Experimental
Psychology, 62 (8), 1457–1506.
Renkewitz, F. & Jahn, G. (2010). Tracking memory search
for cue information. In A. Glöckner & C. Wittemann
(Eds.), Foundations for tracing intuitions: Challenges,
findings and categorizations. New York: Psychology
Press.
Richardson, D.C., Altmann, G.T.M., Spivey, M.J., &
Hoover, M.A. (2009). Much ado about eye movements to
nothing: a response to Ferreira et al.: Taking a new look at
looking at nothing. Trends in Cognitive Science, 13(6),
235−236.
Richardson, D.C., & Spivey, M.J. (2000). Representation,
space and Hollywood Squares: looking at things that
aren’t there anymore. Cognition, 76, 269–295.
Shepherd, M., Findlay, J. M. & Hockey, R. J.(1986). The
relationship between eye movements and spatial attention.
The Quarterly Journal of Experimental Psychology
Section A, 38(3), 475−491.
Spivey, M. (2007). Uniting and Freeing the Mind. In: M.
Spivey (Eds.), The continuity of mind. New York: Oxford
University Press.
Spivey, M.J, & Geng, J.J. (2001). Occulomotor mechanisms
activated by imagery and memory: eye movements to
absent objects. Psychological Research, 65, 235-241.

1075

