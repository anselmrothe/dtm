UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Zipfian word frequencies support statistical word segmentation
Permalink
https://escholarship.org/uc/item/58j0m9rq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Kurumada, Chigusa
Meylan, Stephan C.
Frank, Michael C.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

                  Zipfian word frequencies support statistical word segmentation
               Chigusa Kurumada                                Stephan C. Meylan                         Michael C. Frank
            kurumada@stanford.edu                            smeylan@stanford.edu                     mcfrank@stanford.edu
            Department of Linguistics                       Department of Psychology                 Department of Psychology
                Stanford University                            Stanford University                       Stanford University
                                Abstract                                  sentence and word lengths (Frank, Goldwater, Griffiths, &
                                                                          Tenenbaum, 2010). Nevertheless, there are many links be-
   Word frequencies in natural language follow a Zipfian dis-
   tribution. Artificial language experiments that are meant to           tween statistical segmentation and natural language learning
   simulate language acquisition generally use uniform word fre-          that still have not been made.
   quency distributions, however. In the present study we exam-              One key difference between standard segmentation
   ine whether a Zipfian frequency distribution influences adult
   learners’ word segmentation performance. Using two exper-              paradigms and natural language is the distribution of frequen-
   imental paradigms (a forced choice task and an orthographic            cies. The empirical distribution of lexical items in natural
   segmentation task), we show that human statistical learning            language follows a Zipfian distribution (Zipf, 1965), in which
   abilities are robust enough to identify words from exposures
   with widely varying frequency distributions. Additionally, we          relatively few words are used extensively (“the”) while most
   report a facilitatory effect of Zipfian distributions on word seg-     words occur only rarely (“toaster”). In particular, the absolute
   mentation performance in the orthographic segmentation task,           frequency of a word tends to be approximately inversely pro-
   both in segmenting trained material and in generalization to
   novel material. Zipfian distributions increase the chances for         portional to its rank frequency. While Zipfian distributions
   learners to apply their knowledge in processing a new speech           are ubiquitous across natural language, their consequences
   stream.                                                                for learning are only beginning to be explored (Goldwater,
   Keywords: Word segmentation; statistical learning; Zipfian             Griffiths, & Johnson, 2006). The current paper investigates
   frequency distributions.                                               the consequences of Zipfian frequency distributions for sta-
                                                                          tistical word segmentation.
                            Introduction                                     Saffran, Newport, and Aslin (1996) hypothesized that the
Humans and other animals extract information from the en-                 mechanism underlying statistical word segmentation was the
vironment and represent it so that they can later use these               computation of syllable-syllable transitions. In a uniform dis-
representations for effective recognition and inference. One              tribution, nearly every word follows every other word, so
striking example of this phenomenon is that adults, children,             these transition matrices are quite well-populated, but in a
and even members of other species can utilize statistical infor-          Zipfian language, they are very sparse. Some combinations of
mation to segment an unbroken speech stream into individual               frequent words will have high transition probability between
words after a short, ambiguous exposure (Saffran, Aslin, &                them (especially if they co-occur together frequently). If syl-
Newport, 1996; Saffran, Newport, & Aslin, 1996; Aslin, Saf-               lables are used in multiple words, the within-word transition
fran, & Newport, 1998; Hauser, Newport, & Aslin, 2001).                   probabilities for low-frequency words could be considerably
In a now-classic segmentation paradigm, Saffran, Newport,                 lower than the between-word transition probability for high
and Aslin (1996) played adults a continuous stream of synthe-             frequency words. This factor may have led to the low perfor-
sized speech composed of uniformly-concatenated trisyllabic               mance of transition-based models in computational compar-
words. After exposure to this stream, participants were able              isons (Yang, 2004; Brent, 1999). Thus, the first question we
to distinguish the original words from “part-words”—chunks                ask in the current study is whether human statistical learning
that had occurred with lower frequency and lower statisti-                abilities can succeed in segmenting Zipfian-distributed input.
cal consistency. This work, combined with demonstrations                     Examining the problem from another side, however, a Zip-
with infants, suggested that statistical segmentation could be            fian distribution might actually provide more information for
a viable method for early language learners to learn the word             segmentation. Bannard and Lieven (2009) suggest that repet-
forms of their native language.                                           itive use of restricted types of words and word combinations
   While the results of these experiments are impressive, the             in input are a strong predictor of the order of acquisition. In
ways in which these findings can be applied to understand                 addition, six-month-olds can already exploit highly familiar
natural language learning are still unclear. Recent research              words to segment and recognize adjoining unfamiliar words
has begun to close this gap. The outputs of this statistical              from fluent speech (Bortfeld, Morgan, Golinkoff, & Rathbun,
segmentation process are now known to be good targets for                 2005). Thus, our second question is whether (and under what
word-meaning mapping (Graf Estes, Evans, Alibali, & Saf-                  conditions) Zipfian input could facilitate word segmentation.
fran, 2007), and experiments with natural language sample                    To address these two questions, we compared segmen-
suggest that the processes observed in artificial language ex-            tation performance in uniform and Zipfian contexts across
periments generalize to highly-controlled natural language                three different large-scale web-based segmentation experi-
samples (Pelucchi, Hay, & Saffran, 2009). In addition, sta-               ments. Since Frank, Arnon, Tily, and Goldwater (2010) pro-
tistical segmentation has been shown robust to variation in               vided evidence that crowd-sourcing platforms reliably repli-
                                                                      2667

                                                                             Stimuli We constructed 8 language conditions by control-
   100              100               100               100                  ling patterns of frequency distribution (uniform vs. Zipfian)
                                                                             and the numbers of word types contained in lexicon (6, 12,
   60               60                60                60
                                                                             24, 36 types). Within each language condition, we created
   20               20                20                20                   16 language variants with different phonetic material. This
   0                0                 0                 0                    diversity was necessary to ensure that results did not include
                                                                             spurious phonological effects.
                                                                                Words were created by randomly concatenating 2, 3, or
   100              100               100               100
                                                                             4 syllables (word lengths were evenly distributed across
   60               60                60                60                   each language). Stimuli were synthesized using MBROLA
                                                                             (Dutoit, Pagel, Pierret, Bataille, & Van Der Vrecken, 1996) at
   20               20                20                20
                                                                             a constant pitch of 100Hz with 225ms vowels and 25ms cos-
                                                                             nonants. Each syllable was used only once. Sentences were
   0                0                 0                 0
         6  types         12  types         24  types         36  types
                                                                             generated by randomly concatenating words into strings of
                                                                             four words. The total number of word tokens was 300 and
Figure 1: Word frequencies in uniform (top) and Zipfian con-                 the number of sentences was 75 in all the languages. The
ditions of Experiment 1.                                                     token frequencies of words in each language were either dis-
                                                                             tributed uniformly according to the total type frequency (e.g.
                                                                             50 tokens each for a language with 6 word types) or given
cate lab-based experiments, we use this method to gather data                a Zipfian distribution such that frequency was inversely pro-
across a wide range of experimental conditions. Experiment                   portional to rank ( f ∝ 1/r). Frequency distributions for each
1 tests participants in a standard 2-alternative forced-choice               language are shown in Figure 1.
(2AFC) paradigm and manipulates the number of tokens in                         For the test phase, “part-words” were created by concate-
the languages used. Experiments 2 and 3 use an orthographic                  nating the first syllable of each word with the remaining syl-
segmentation task and ask whether training on and testing on                 lable of another word. These part-words were used as dis-
Zipfian-distributed materials lead to an advantage in segment-               tractors which appeared in the training input with lower fre-
ing previously heard and novel words.                                        quency than the target words.
   Our results show that Zipfian distributions neither help
nor harm segmentation performance in a traditional 2AFC                      Procedure Before the training phase began, participants
paradigm (Experiment 1). In the orthographic paradigm,                       were instructed to listen to a simple English word and type it
however, the Zipfian word frequency benefitted learners by                   in to ensure the sound is properly played and perceived. Par-
providing them with more chances to segment familiar words                   ticipants then moved to the training phase, where they were
(Experiment 2), which in turn helped to individuate neighbor-                instructed to listen to and learn a made-up language which
ing words in the speech stream (Experiment 3). These data                    they would later be tested on. To ensure compliance with
suggest that Zipfian frequency distributions have a scaffold-                the listening task for the duration of the training phase sub-
ing effect on segmentation that manifests at the stage where                 jects needed to click a button marked next to proceed though
learners use acquired knowledge to segment new sentences.                    the training sentences. In the test phase of the 2AFC condi-
                                                                             tion, participants heard 24 pairs of words consisting of a tar-
                          Experiment 1                                       get word and a length matched “part-word.” After listening to
In Experiment 1, we use 2AFC test trials where participants                  each word once, they clicked a button to indicate which one
are asked to distinguish a word from a part-word to test                     sounded more familiar (or “word-like”) given the language
whether adult learners can learn words from input following                  they had learned.
uniform and Zipfian distributions. One additional novel fea-                 Results and Discussion
ture of this experiment is that we vary the number of word
types (distinct word forms) in the experiment from 6 all the                 Figure 2 illustrates accuracy of responses in the 4 types of
way to 36, far higher than previously tested (Frank, Gold-                   languages in the each of the uniform and Zipfian distribution
water, et al., 2010). Thus, a subsidiary question is whether                 conditions. There was not a strong numerical effect of distri-
participants are able to identify words at above-chance levels               bution condition. Replicating previous results (Frank, Gold-
in these more challenging environments.                                      water, et al., 2010), performance decreased as the number
                                                                             of types increased, but participants performed slightly above
Methods                                                                      chance even in the most difficult 36 type condition.
Participants We posted 259 separate HIT (Human Intelli-                         Our analysis used mixed effects logistic regression
gence Taks: experimental tasks for participants to work on)                  (Gelman & Hill, 2006) fit to the entire dataset. This model at-
on Amazon’s Mechanical Turk. We received 246 HITs from                       tempted to predict performance on individual trials; we used
distinct individuals (a mean of 30 for each token frequency                  model comparison to find the appropriate predictors. Our first
and distribution condition).                                                 model included effects of distribution and number of types;
                                                                          2668

                                        Uniform Distribution Condition                                            Zipfian Distribution Condition
                              1.0                                                                    1.0
                              0.8                                                                    0.8
          ProportionCorrect                                                     ProportionCorrect
                              0.6                                                                    0.6
                              0.4                                                                    0.4
                              0.2                                                                    0.2
                              0.0                                                                    0.0
                                    6       12               24          36                                6          12               24          36
                                                 number of word types                                                      number of word types
Figure 2: Average proportion correct trials by number of word types in the uniform and Zipfian distribution conditions. Dots
represent individual participants and are stacked to avoid overplotting. Solid line represents means, dashed lines represent
standard errors, and the dotted line represents chance (50%).
we found no effect of distribution (β = −.226, p = .12) but                                         a uniform condition, which could create a skewed distribu-
a highly significant effect of number of types (β = −.021,                                          tion of transition probabilities between lexical items. How-
p < .0001). Further exploration revealed that better model fit                                      ever, our results indicate that 2AFC accuracy for a word is
was given by the logarithm of number of types as a predic-                                          predicted based predominantly on the (uni-gram) word fre-
tor rather than raw number of types (χ2 = 9.49, p < .0001).                                         quency in input, not on the distribution of the context.
Thus, the log number of types was the only significant pre-
dictor of performance in this model.                                                                                          Experiment 2
   In our second set of models, we introduced as additional                                         Experiment 2 tests our hypothesis about possible facilita-
trial-level predictors the frequency of the target and distrac-                                     tive effect of a Zipfian word distribution on segmentation of
tors for each trial (calculated from the input corpus for each                                      a speech stream via a different method. Because a 2AFC
language). In this model, we found that once these factors                                          asks only about a comparison between a particular target-
were added, there was no gain in model fit from log number                                          distractor pair, we hypothesized that effects of distribution
of types (χ2 (1) = .11, p = .74). Instead, the only significant                                     might be more obvious in a paradigm where words were pre-
effects were a positive coefficient on log tokens (the more                                         sented in context during testing. To test this hypothesis, we
times a word is heard, the better performance gets: β = .35,                                        use an orthographic segmentation task developed by Frank,
p < .0001), a negative coefficient on log distractor tokens (the                                    Goldwater, et al. (2010). In this task, participants were trained
more times a distractor is heard in the corpus, the worse per-                                      on either a Zipfian or a uniform distributions and later asked
formance gets: β = −.51, p = .003) and a positive interac-                                          to give explicit judgments as to where in a sentence they
tion of the two (bad distractors are worse if the target is low                                     would place word boundaries. Based on the finding in Exper-
frequency: β = .14, p = .003). The general relation here is                                         iment 1, we hypothesized that ease of identification of words
plotted in Figure 3, showing mean proportion of accuracy ac-                                        would be predicted by their input frequencies. This would
cording to the input frequency of the target words. In this final                                   benefit those who are exposed to a Zipfian distribution during
model, there was still no effect of distribution conditions (i.e.,                                  test because a large portion of a sentence consists of words
uniform vs. Zipf) (β = .05, p = .49).                                                               that were highly frequent in the input.
   To summarize: participants represented target words
equally well after being exposed to languages with radically                                        Methods
different frequency distributions and contingency statistics.                                       Participants 281 separate HITs were posted on Mechani-
The only factors that mattered in 2AFC test trials were the log                                     cal Turk. We received 250 complete HITs from distinct indi-
frequency of targets and distractors, independent of what con-                                      viduals. Participants were paid $0.50 for participation. Be-
text they were heard in. In a Zipfian condition, some words                                         cause of the increased complexity of the manual segmenta-
have significantly higher and lower frequency than those in                                         tion task, we applied an incentive payment system to ensure
                                                                              2669

                                                                                                        1.0                                      1.0
                                1.0
                                                                                                        0.8                                      0.8
probability of correct answer
                                0.8
                                                                                              f score
                                                                                                        0.6
                                                                                                                             -         f score
                                                                                                                                                 0.6
                                                                                                                                                                       -
                                0.6                                                                     0.4   -                                  0.4   -
                                                                                                        0.2                                      0.2
                                0.4
                                                                                                        0.0                                      0.0
                                                                           uniform
                                0.2                                                                     Uniform-test    Zipfian-test             Uniform-test     Zipfian-test
                                                                           zipfian
                                                                                                                  Uniform training                          Zipfian training
                                0.0
                                      0   20   40         60          80    100      120
                                                                                              Figure 4: Token F-scores (a measure of segmentation per-
                                                    token frequency                           formance for individual words) plotted for each condition of
                                                                                              Experiment 2. Points represent individual participants, bars
                                                                                              show means, and dashed lines show a permutation baseline.
Figure 3: Probability of a correct 2AFC answer plotted by
binned token frequency. Filled circles indicate uniform con-
dition, while open circles indicate Zipfian condition. Dashed                                 two letters representing a consonant and a vowel respectively
line shows chance, while the dotted and alternating lines give                                (e.g., ka, pi, ta). Each sentence was also played back at the
best fit lines for performance as a function of log token fre-                                beginning of the trial.
quency.
                                                                                              Results and Discussion
participants’ attention to the task. They were told they would                                To evaluate participants’ segmentation performance, we re-
receive an additional $1.00 if they scored in the top quartile.                               lied on precision and recall, and their harmonic mean, F-
We excluded 1 participant for F-scores of exactly 0.                                          score. These same measures are used in computational stud-
                                                                                              ies of segmentation and in previous work (Goldwater, Grif-
Stimuli The process of generating stimuli was nearly iden-                                    fiths, & Johnson, 2009; Brent, 1999). We computed preci-
tical to the 6 word condition in Experiment 1. Six words were                                 sion and recall for both boundaries and for word tokens.1 To-
generated following either a uniform or Zipfian distribution.                                 ken F-scores in each condition are plotted in Figure 4. As
Six hundred word tokens were presented in 150 sentences in                                    in Frank, Arnon, et al. (2010), we calculated an empirical
the training phase. For the test phase, 10 additional sentences                               baseline for each measure via permutation. We repeatedly
were created according to one of the two frequency distri-                                    shuffled boundary placement responses for each sentence and
butions; the same lexicon was used to generate the training                                   computed the same measures (precision, recall, and F-score).
corpus. To examine the effects of frequency distribution at                                   The mean values of baseline token F-scores in each condition
the different stages of segmentation, we applied a 2x2 facto-                                 are indicated as dashed lines in Figure 4.
rial design. Subjects were divided into four groups according                                    Because participant mean F-scores were normally dis-
to the frequency distributions at the training phase (uniform                                 tributed but trial-level F-scores were not, and because we had
vs. Zipfian training) and at the test phase (uniform vs. Zip-                                 no trial-level predictors in this experiment, we used a sim-
fian test). In each case, sentences were generated by sampling                                ple linear model to predict participants’ mean token F-scores.
words from either a uniform frequency distribution or one that                                We found a reliable main effect of the test condition (β = .10,
was generated via the same classic Zipfian formulation given                                  p < .02) and no significant effect of the training condition
above ( f ∝ 1/r).                                                                             (β = .03, p = .4) or interaction (β = .0005, p = .9). The
Procedure The training section of this experiment was                                         boundary scores exhibited the same patterns and the same
identical to that of Experiment 1 (though twice as long; ap-                                  pattern of statistical significance (see Table 1 for means):
proximately 9 minutes). In the test phase, participants were                                      1 In our example sentence (“indiangorrillaseatbananas”), we
asked to click on the breaks between syllables to indicate                                    compute these measures for a participant who gave the segmenta-
word boundaries. They were given one practice trial on an                                     tion “indian|gorillas|eatbana|nas.” Computing word boundaries, the
English sentence presented in the same format and prevented                                   participant would have 2 hits, 1 miss, and 1 false alarm, leading to
from continuing until they segment it correctly. At test, sen-                                precision of .66 (hits / hits + false alarms), and recall of .66 (hits /
                                                                                              hits + misses), for an F-score of .66. On the other hand, for word
tences were presented visually, with each syllable separated                                  tokens, the participant would have 2 hits (“indian” and “gorillas”),
by a toggleable button. All the syllables were spelled with                                   2 misses (“eat” and “bananas”) and 2 false alarms (“eatbana” and
                                                                                              “nas”), for prevision of .5, recall of .5, and F-score of .5.
                                                                                           2670

Table 1: Mean token F-scores and boundary F-scores for
overall segmentation performance in Experiment 2.
                                                                                                1.0
       Input        Test        Token F     Boundary F                                                                  ●
       Uniform      Uniform      0.39          0.58                                             0.8                                    ●
                    Zipf         0.49          0.65
                                                                           proportion correct
                                                                                                                        ●              ●●                 ●
       Zipf         Uniform      0.42          0.61
                                                                                                0.6   ●                 ●              ●                  ●●
                    Zipf         0.53          0.67                                                   ●                 ●●             ●                  ●●●●
                                                                                                0.4   ●●                ●●●●●          ●●●●●●●            ●●●●●
                                                                                                      ●●●●●
                                                                                                                     −  ●●●●●●
                                                                                                                                    −  ●●●●
                                                                                                                                                       −  ●●●●●●●●
main effect of the test (β = .07, p = .04) and no effect of                                     0.2
                                                                                                      −
                                                                                                      ●●●●●●●           ●●●●●●●        ●●●●●              ●●●
the training (β = .02, p = .4) and the interaction (β = −.009,                                        ●●●●●●●●●●        ●●●●●●         ●●●●               ●●●●●●
p = .85).                                                                                       0.0   ●●●●              ●●             ●●●●               ●●
   The critical finding in this experiment is that even when                                     Uniform test      Zipfian test   Uniform test       Zipfian test
participants were trained with Zipfian materials, where some                                               Uniform traning                    Zipfian traning
words were significantly rarer compared to the words in uni-
form distribution, people still performed better if they had
been tested on Zipfian-distributed items. What remains un-             Figure 5: Proportion correct for the identification of new
clear is whether the high concentration of familiar items in           words in Experiment 3; plotting conventions are as in Figure
the Zipfian test condition helped the learners find less fre-          4.
quent items. Due to the small number of word types in Ex-
periment 2, even the low frequency items were still heard 40           Results and Discussion
times. Thus, in Experiment 3 we test the hypothesis that Zip-          Token and boundary F-scores for overall segmentation perfor-
fian contexts support better segmentation of truly novel ma-           mance are shown in Table 2. We again fit linear models to the
terial.                                                                token and boundary F-score data. In token F-score, we again
                                                                       found the main effect of test condition (β = .13, p = .005),
                       Experiment 3                                    and there was still no effect of the training (β = .08, p = .07)
                                                                       or an interaction term between them, though there was a neg-
Experiment 3 replicated Experiment 2, but for each test sen-           ative coefficient value, indicating some sub-additivity (β =
tence, we added a single novel item. If identification of fa-          −.07, p = .29). The test effect (β = .089, p = .03) was still
miliar words improves segmentation accuracy of adjoining               significant in boundary F-score data and there was an effect
words, we would expect better identification of novel words            of the training (β = .10, p = .02), but no significant interac-
when participants were both trained and tested using Zipfian           tion, though the coefficient was again negative (β = −.047,
materials.                                                             p = .42).
                                                                          We next analysed generalization data: we coded each of
                                                                       the ten generalization trials (one novel word per sentence) as
Methods
                                                                       a binary variable: 1 if the word was segmented correctly, 0
                                                                       otherwise. Participant means are plotted in Figure 5. We then
Participants 158 separate HITs were posted on Amazon’s                 used a mixed logistic model to predict this variable on the
Mechanical Turk. We received 121 complete HITs from dis-               basis of training and test condition, including a random ef-
tinct individuals. Participants were paid $0.50 for their partic-      fect of participant. (We chose a mixed model here in order to
ipation and we again added bonus payments for participants             avoid the issue of computing a linear regression over a non-
in the top quartile.                                                   normally distributed DV). As in the overall test (and Experi-
Stimuli Sentences for training phase were created identi-              ment 2), we found main effects of training (β = .61, p < .02)
cally to Experiment 2. At test, we generated 10 new words of           and test (β = .57, p = .03), with a negative but non-significant
varying length (2, 3, and 4 syllables) based on syllables that         interaction term (β = −.52, p = .15).
appeared in the training corpus. To ensure each syllable was              To corroborate the effects of neighboring items, another
used only once in the lexicon despite the enlarged lexicon (6          mixed logistic regression model was constructed where all the
training items + 10 novel test items), an additional vowel was         words in the test sentences were coded as a binary variable: 1
added to the phonemic inventory. We added one new word                 if the word was segmented correctly, 0 otherwise. Log input
in a sentence-internal position in each test sentence. With the        frequency of the preceding word was a significant predictor
additional word, there were 5 test sentences of length 4 and 5         of correct segmentation of the current word (β = .07, p <
test sentences of length 5.                                            .02), along with other factors like input frequency (β = .12,
Procedures Procedures were identical to Experiment 2.                  p < .01) and length (β = −1.03, p < .01) of the target word.
                                                                    2671

                                                                        infants. Psychological Science, 9(4), 321-324.
Table 2: Mean token F-scores and boundary F-scores for
                                                                      Bannard, C., & Lieven, E. (2009). Repetition and reuse in
overall segmentation performance in Experiment 3.
                                                                        child language learning. Formulaic language, 2.
         Input       Test        Token F     Boundary F               Bortfeld, H., Morgan, J., Golinkoff, R., & Rathbun, K.
         Uniform     Uniform      0.26            0.47                  (2005). Mommy and me. Psychological Science, 16(4),
                     Zipf         0.39            0.56                  298.
         Zipf        Uniform      0.35            0.57                Brent, M. R. (1999). An efficient, probabilistically sound
                     Zipf         0.41            0.61                  algorithm for segmentation and word discovery. Machine
                                                                        Learning, 34(1), 71-105.
                                                                      Dutoit, T., Pagel, V., Pierret, N., Bataille, F., & Van
                                                                        Der Vrecken, O. (1996). The MBROLA project: towards
On the other hand, log input frequency of the following word            a set of high quality speechsynthesizers free of use for non
did not show such an effect (β = .04, p = .2). These results            commercial purposes. In Proceedings of the fourth inter-
suggest that participants generally moved from left to right to         national conference on spoken language (Vol. 3, pp. 1393–
segment a sentence into words incrementally.                            1396). Philadelphia, PA.
   To summarize: Experiment 3 replicates the findings from            Frank, M., Arnon, I., Tily, H., & Goldwater, S. (2010). Be-
Experiment 2 and highlights a benefit of segmentation within            yond transitional probabilities: Human learners impose a
a Zipfian language: if a learner hears a novel word, that word          parsimony bias in statistical word segmentation. In Pro-
is much more likely to be flanked by known words.                       ceedings of the 31st annual meeting of the cognitive science
                   General Discussion                                   society.
                                                                      Frank, M., Goldwater, S., Griffiths, T., & Tenenbaum, J.
The results of three experiments indicate that a Zipfian distri-        (2010). Modeling human performance in statistical word
bution of word frequency supports statistical word segmen-              segmentation. Cognition.
tation by scaffolding learners’ active use of acquired knowl-         Gelman, A., & Hill, J. (2006). Data analysis using regres-
edge. In Experiment 1, we found that learning performance               sion and multilevel/hierarchical models. Cambridge, UK:
in a 2AFC task was neither helped nor hurt by a Zipfian fre-            Cambridge University Press.
quency distribution. The only factor that predicted learning          Goldwater, S., Griffiths, T., & Johnson, M. (2006). Interpo-
was a word’s log frequency.                                             lating between types and tokens by estimating power-law
   In contrast, in Experiments 2 and 3, when target words               generators. In Y. Weiss, B. Schölkopf, & J. Platt (Eds.),
were presented in a sentential context in our orthographic seg-         Advances in neural information processing systems 18 (pp.
mentation paradigm, we saw reliable effects of Zipfian testing          459–466). Cambridge, MA: MIT Press.
materials. Crucially, regardless of whether participants were         Goldwater, S., Griffiths, T., & Johnson, M. (2009). A
trained on a Zipfian or uniform distribution, they performed            Bayesian framework for word segmentation: Exploring the
better at test when they had received a repetitive exposure to a        effects of context. Cognition, 112, 21-54.
few items in the test contexts. We also found that the reliable       Graf Estes, K. M., Evans, J. L., Alibali, M. W., & Saffran,
identification of high frequency items effectively restricted           J. R. (2007). Can infants map meaning to newly segmented
hypotheses about novel words at test (Experiment 3). In other           words? Psychological Science, 18(3), 254.
words, correctly setting boundaries around known words pro-           Hauser, M. D., Newport, E. L., & Aslin, R. N. (2001). Seg-
vide extra leverage in segmenting the entire sentence. In the           mentation of the speech stream in a human primate: statis-
word/part-word comparison paradigms that have traditionally             tical learning in cotton-top tamarins. Cognition, 78, B53-
been used to evaluate segmentation accuracy, this benefit was           B64.
absent.                                                               Pelucchi, B., Hay, J., & Saffran, J. (2009). Statistical learn-
   Our results provide evidence that the frequency structure            ing in a natural language by 8-month-old infants. Child
of natural language input provides a natural scaffolding for            development, 80(3), 674–685.
statistical word segmentation. We hope that future research           Saffran, J. R., Aslin, R., & Newport, E. (1996). Statisti-
continues to investigate aspects of artificial languages in order       cal learning by 8-month-old infants. Science, 274(5294),
to explore the interaction of human cognition and the natural           1926.
language learning environment.                                        Saffran, J. R., Newport, E. L., & Aslin, R. N. (1996). Word
                                                                        segmentation: The role of distributional cues. Journal of
                    Acknowledgments                                     Memory and Language, 35(4), 606-621.
Thanks to the members of the Language and Cognition Lab               Yang, C. (2004). Universal Grammar, statistics or both?
for valuable discussion.                                                Trends in Cognitive Sciences, 8(10), 451–456.
                                                                      Zipf, G. (1965). Human behavior and the principle of least
                          References                                    effort: An introduction to human ecology. Hafner New
Aslin, R. N., Saffran, J. R., & Newport, E. L. (1998). Compu-           York.
   tation of conditional probability statistics by 8-month-old
                                                                  2672

