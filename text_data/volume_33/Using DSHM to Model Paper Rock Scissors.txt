UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Using DSHM to Model Paper, Rock, Scissors
Permalink
https://escholarship.org/uc/item/72k0d95k
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Rutledge-Taylor, Matthew
West, Robert
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                Using DSHM to Model Paper, Rock, Scissors
                                 Matthew F. Rutledge-Taylor (mattrt@andrew.cmu.edu)
              Department of Psychology, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, USA
                                         Robert L. West (robert_west@carleton.ca)
             Institute of Cognitive Science, Department of Psychology, Carleton University, 1125 Colonel By Drive
                                                 Ottawa, Ontario, K1S 5B6, Canada
                             Abstract
   Dynamically Structured Holographic Memory (DSHM) is                              Existing Models of PRS
   architecture for modeling memory. It was originally designed     In order to provide some base-level of expectation for what
   to account for long-term memory alone. However, the current      might constitute good performance in models of PRS play,
   model of Paper, Rock, Scissors (PRS) play (and Rock=2 PRS        some existing models are briefly reviewed.
   play) provides evidence that DSHM can be used to model
   tasks where the truth of facts change quickly and
   dynamically.                                                     Perceptron Models
                                                                    It has been shown that simple perceptron-like neural
   Keywords: DSHM, memory, game playing, decision making.
                                                                    networks can be used to model human behaviour in standard
                                                                    PRS games (West, 1998; West & Lebiere, 2001). The
 Dynamically Structured Holographic Memory                          networks take sets of past opponent moves as input, and
Dynamically Structured Holographic Memory (DSHM) is                 provide choices of next move as output. The networks are
an architecture for modeling memory. It was designed to             constructed such that they each have an output layer of three
account for how stable information is stored and retrieved          nodes, one corresponding to each of the three play options:
from long-term memory. DSHM does not presume that the               paper, rock, and scissors. Each has one or more groups of
relationships between concepts are entirely static. Rather, it      input nodes. Each group includes three input nodes, one for
explicitly accounts for how concepts can evolve to form             each play option. Each input node is connected to each
associations of different strengths, over time. DSHM has            output node. The connections between nodes are assigned
been used successfully to model the fan effect (Rutledge-           integer values (or, weights), which start at 0.
Taylor & West, 2008; Rutledge-Taylor, Pyke, West and                   If a network has only a single input group, it takes only its
Lang, 2010). It has also been used as the basis for a               opponent’s last move as input. To determine what option to
recommender system (Rutledge-Taylor, Vellino & West,                play, the connections between the node in the input group
2008). In both of these applications, DSHM commits a set            corresponding to the opponent’s move and each of the
of static facts to memory.                                          output nodes are compared. The output node attached to the
   DSHM was not designed as a store for information whose           connection with the greatest value determines which move
relevance is short-lived and potentially contradictory to new       the network selects (ties are decided randomly). If the
information.        This is because there are no native             network’s decision results in a win, the relevant connection
mechanisms that cause old memories to decay, or otherwise           is rewarded by increasing its value by one. If the result is a
contribute less than recent memories in decision making.            loss, the connection is punished by reducing its value by
This sort of information, whose relevance is time dependent,        one. Ties are treated differently by two variations of this
is not uncommon in strategic decision making tasks, such as         basic network design (West & Lebiere, 2001). Networks
those in that take place in competitive games described by          called ‘passive’ treat ties as neutral events and neither
game theory (VonNeumann & Morgenstern, 1944). For                   reward or punish the connection values after a tie.
example, in the game Paper, Rock, Scissors (PRS),                   Networks called ‘aggressive’ punish connections leading to
successful players are able to detect and exploit repeating         ties by 1.
patterns of sequences of moves in the play of opponents                Networks with two or more input groups take a set of the
(West & Lebiere, 2001). Additionally, they must adapt to            opponent’s last moves as input. Each additional input group
ignoring old biases in opponents play and discover new              beyond the first corresponds to a move further back in the
biases.                                                             opponent’s play history. For example, with two input
   The fact that DSHM was not designed to model these               groups, one corresponds to the opponent’s last move as
sorts of dynamic memory tasks does not preclude the                 input, while the other takes the opponent’s second to last
possibility that it might be used to build successful models        move as input. For these networks the output is determined
of human performance in these sorts of games. This                  by summing the connections between one node from each
possibility was examined by building DSHM models of                 input group (corresponding to the move played on that past
human performance in the game PRS and a modified                    occasion) and each output node. Rewards and punishments
version of PRS. These models are presented herein.                  are applied to all connections that contribute to the output
                                                                    decision. In addition to being labelled as either passive or
                                                                2341

aggressive, these networks were also labelled as ‘lag 1’, ‘lag     opponents were not; 2) the rock=2 network performed the
2’ or ‘lag 3’ depending on whether they attended to                worst of all the network models due to the fact that
opponents’ last one, two, or three past moves.                     rewarding rock wins by two became a liability (and not the
                                                                   anticipated advantage). This larger reward unbalanced the
                                                                   reward system in such a way that the rock=2 network played
                                                                   rock too frequently, and this was exploited by the human
                                                                   players.
                                                                      An additional observation was that in Rock=2 PRS the
                                                                   frequencies with which a player played each of the possible
                                                                   moves did not predict the game’s final scores. For example,
                                                                   if two players each play paper, rock and scissors exactly 1/3
                                                                   of the time each, they will tie, on average, if they are
                                                                   playing randomly. However, it has been demonstrated that
                                                                   human players (and the network models) do not play
                                                                   randomly (West, 1998; Rutledge-Taylor & West, 2004).
                Figure 1: Perceptron Networks                         Rather than playing randomly, superior players exploit
                                                                   weaker opponents by predicting their opponents’ moves and
  Both West (1998), and West and Lebiere (2001)                    making winning moves. As a result, they are able to
concluded that the aggressive lag 2 network provided the           achieve higher win rates than would be predicted by play
best model of human PRS play. Both the humans and the              probabilities alone. This effect is particularly important in
aggressive lag 2 networks were able to beat the passive lag 2      the Rock=2 game, as being able to orchestrate rock versus
and aggressive lag 1 networks (West, 1998; West &                  scissors plays and to be able to avoid the opposite play is
Lebiere, 2001), by statistically significant margins. On           crucial to success in this game. Thus, a measure called the
average, humans lost to the aggressive lag 2 networks by a         strategy index was invented.
small margin (West & Lebiere, 2001). However, the                     The strategy index is calculated according to formula 1,
authors suggest that this may be due to imperfect attention        below. Given two players, player 1’s strategy index is
and motivation on the parts of the human participants.             player 1’s total points scored minus player 2’s total points,
                                                                   divided by the number of games played. The predicted
Perceptron Rock=2 The standard PRS game played by                  points difference is calculated using game theory (i.e., each
humans and network models in West (1998) and West and              player is assumed to have played randomly according to
Lebiere (2001) were perfectly symmetrical. The three play          probabilities determined by the actual ratios with which the
options were identical in that each beat one of the other two      options were chosen). A positive strategy index indicates a
moves and lost to the other; a rock versus scissors win was        superior ability to correctly anticipate opponent’s plays and
no different from a scissors versus paper win. In Rutledge-        achieve a higher than probabilistically predicted number of
Taylor and West (2004), a modified version of PRS, where           points. The raw strategy index is relative to the number of
rock versus scissors wins were worth two points, while the         trials per game. So, it can be also represented as a
other two outcomes were worth only one point each, was             percentage according to formula 2.
investigated. Ten human participants played one game
against each of three network opponents. The network                  (1) Strategy index = average actual points difference per
opponents were: the aggressive lag 1, the aggressive lag 2,               game – predicted points difference per game
and a ‘rock=2’ lag 1. The rock=2 network rewarded                     (2) Strategy index percentage = Strategy index / number
network connections by two when it won with rock. Table                   of trials per game
1 presents the mean points differences in the final scores of
games between the human participants and each of the                  For example, if two players play a game of 300 trials, and
network models. All games consisted of 300 trials each.            each player plays paper 100 times, rock 100 times, and
                                                                   scissors 100 times, game theory predicts that they will tie
              Table 1: Humans versus networks                      (on average). However, it is possible for one player to win
                                                                   all 300 trials by always matching the opponent’s move with
                                                                   the move that beats it. In this case, the winning player
            Network Model        Mean Pts. Diff.                   would score a perfect strategy index of 300 (or 100%). In
            Agg. Lag 1           16.5                              real games, a strategy index percentage of 3% or more is
            Agg. Lag 2           5.7                               considered very good.
                                                                      In Rutledge-Taylor & West (2004) a network model of
            Rock=2 lag 1         25.6                              human Rock=2 play was created by using a genetic
                                                                   algorithm to find a reward matrix that resulted in human like
  Two conclusions were drawn from this experiment: 1)              play. The criterion was to match as closely as possible the
humans were able to take advantage of the fact that wins           human mean points difference and the mean strategy indices
using rock were worth two, while all three network
                                                               2342

against the three opponents in table 1. The best reward                The result was that the third model provided the best
matrix was the following: rock wins = 3, paper wins = 2              match to the human data. This makes intuitive sense in that
scissors wins = 0; rock tie = -1, paper tie = -1, scissors tie =     a human player is likely to incorporate a defensive
0; and -3 for all losses. The performance of this model is           component to his or her game, which his to be wary of when
presented below.                                                     the opponent is likely to play Rock; however, he or she
                                                                     might also incorporate an offensive component which is to
ACT-R Models                                                         also focus on when the opponent might play scissors.
ACT-R models of both standard PRS (Lebiere & West,                   Winning with Paper, or losing to a Paper play, is a less
1999) and of Rock=2 PRS (Rutledge-Taylor & West, 2005)               important event in the game.
have been created. Both employ an exemplar based
approach and manipulate noise to establish a best fit.                                  DSHM PRS Models
   For both models a chunk type with four slots was used.            Given the broad similarities between DSHM and the
The isa slot was tagged with PRS to indicate that is was a           declarative memory system of ACT-R (Rutledge-Taylor &
PRS relevant chunk. The three remaining chunks encoded a             West, 2008), the DSHM models here were based on the
sequence of three moves, by the model’s opponent: lag0, is           ACT-R models described above.
the opponents current, or predicted move; lag1 is its                  The DSHM models took sequences of opponent’s plays,
previous move; and, lag2 is its second to last move. An              encoded as ordered complex items. The items consisted of
example is illustrated in figure 2.                                  two, three, or four atomic items, for lag 1, lag 2 and lag 3
                                                                     models respectively; the extra item is the predicted or
                     Goal                                            current play by the opponent.         The right-most item
                        isa PRS                                      represented the opponent’s last move, while items to the left
                        lag2 Paper                                   represented previous plays. For example, if the opponent’s
                        lag1 Rock                                    last few plays were:
                        lag0 nil
                   Figure 2: Example chunk                             …, rock, paper, paper, rock, scissors;
   When the model’s opponent makes a move, a chunk                     a lag 1 DSHM model would learn the following pattern
encoding its last three moves is put into the goal, and then         after scissors was played:
popped to make it a chunk in memory (either creating a new
chunk or reinforcing an existing chunk). To predict the                [rock:scissors];
opponent’s move, the model attempts to retrieve a chunk
from memory that matches the opponent’s last two moves                 thus, reinforcing the association between ‘scissors’ as a
(slots lag1 and lag2). The value of the lag0 slot is the move        play the follows ‘rock’. A lag 3 DSHM model would learn
the model predicts its opponent to make. It then plays the           the pattern:
move that beats it.
   In Lebiere and West (1999), both humans and the lag 2               [paper:paper:rock:scissors].
ACT-R model were pitted against lag 1 and lag 2 versions
of the ACT-R model. Consistent with the findings in West               An interesting architectural point that should be made
(1998), both humans and the lag 2 ACT-R model were able              here is that DSHM reinforces all of the combinations of
to beat the lag 1 opponent; however, exact scores were not           consecutive sets of sub-items in the input, when learning
reported.                                                            ordered complex items. Thus, given the lag 3 input above,
                                                                     the following sequences of items are reinforced after
ACT-R Rock=2 Several ACT-R models of human Rock=2                    scissors is played:
PRS play were presented in Rutledge-Taylor and West
(2004). These models were similar to those appearing in                [paper:paper:rock:scissors],
Lebiere and West (1999), however, they differed in that they           [paper:rock:scissors],
were designed to be sensitive to the unequal payoffs in the            [paper:paper:rock],
Rock=2 game. This sensitivity was achieved by reinforcing              [rock:scissors],
certain kinds of chunks more than others depending on what             [paper:rock], and
the opponent’s last play was. This was done by harvesting              [paper:paper].
these chunks twice. Rutledge-Taylor & West (2005) tested
three variations on this strategy. One model paid extra                So, in a sense, DSHM models of two or more lags
attention to cases when its opponent played rock; another            incorporate some of the learning of shorter lagged models as
attended more closely to scissors; while the third attended          well. An additional consideration is that this results in a
more closely to both rock and scissors (effectively paying           potential liability for DSHM, as shorter sequences receive
less attention to paper).                                            repeated reinforcement for several consecutive trials. In this
                                                                     example, this will have been the third time that
                                                                 2343

[paper:paper] had been reinforced: the third time                 aggressive lag 2 networks difficult (e.g, it is less fun to play
immediately after the play of ‘scissors’; the second time,        and lose against a stronger opponent).
when ‘rock’ was played; and the first time when the most
recent of the two ‘paper’ plays was made. It is also the          Results Of all the models tested, one produced results that
second time that [paper:paper:rock] will have been                came very close to the human data. The Lag 3 DSHM
reinforced (the first time being when ‘rock’ was played).         model with vector lengths of 1024 scored an average of
Thus, in this respect, DSHM models of PRS differ                  10.89 wins more than the aggressive lag 1 network, 13.24
somewhat from both the perceptron-like networks, and the          more wins than the passive lag 2, and lost to the aggressive
ACT-R models discussed above.                                     lag 2 by an average of 6.22 wins per game.
                                                                     The fact that the best DSHM model was a lag 3, not a lag
Rock=1 Simulations                                                2 model, was surprising at first. Lebiere and West (1999),
A variety of DSHM PRS players were built.                The      and West and Lebiere (2001), found that lag 2 ACT-R and
manipulated parameters were: number of lags and the length        lag 2 network models provided the best fit to the human
of the vectors used to represent items. It would not be           data. However, the fact that the DSHM lag 3 models
appropriate to embark on a complete discussion of the inner-      incorporated lag 2 and lag 1 memory behaviour makes this
workings of DSHM here (see Rutledge-Taylor, Pyke, West            result more consistent with previous findings. The DSHM
& Lang, 2010; Rutledge-Taylor & West, 2008). For now, it          lag 3 model weighs lag 1 sequences the most, lag 2
is sufficient to understand that vector length is correlated      sequences second, and lag 3 sequences the least. So, it
with memory capacity. Additionally, lower vector lengths          could be argued that, on average, the lag 3 DSHM models
contribute to noise-like effects due to an increased amount       are more like lag 2 ACT-R and network models, than the lag
of interference in the system.                                    3 ACT-R and network models.
   Each combination of two sets of values for these
parameters was used to build a unique DSHM PRS player:            Rock=1 Model Comparison
                                                                  This paper reviews the three different types of models of
   Lags: [1, 2, 3];                                               PRS play: ACT-R, DSHM, and perceptron-like networks.
                                                                  In each case, the model of human play, played games
   Vector lengths: [32, 64, 128, 256, 512, 1024, 2048].           consisting of 300 trials against the aggressive lag 1 network,
                                                                  and games of 287 trials against the passive lag 2 network.
   Each DSHM model played against the aggressive lag 1,           For each model, the mean difference in the number of wins
aggressive lag 2, and passive lag 2 network models.               scored by the model and the opponent network was
                                                                  recorded. Humans scored, on average, 9.99 more wins than
Evaluation To determine which DSHM performed most                 the aggressive lag 1 networks, and 11.14 more wins than the
like human players, data from West and Lebiere (2001) was         passive lag 2 network. The sum of the squares of the
used as a comparison: human players average 9.99 (s.d.            differences between the model’s results and the human
19.61) more wins than the aggressive lag 1 networks, after        results are presented as a basis for comparing the model’s fit
300 trials; lost to the aggressive lag 2 models by an average     to the human data.
margin of 8.89 (s.d. 19.74), after an imprecise number of            The best ACT-R model was taken from Rutledge-Taylor
trials; and, beat the passive lag 2 by 11.14 wins after 287       and West (2005). It was exemplar based, and used the
trials.                                                           following parameters: ANS=0.28, OL=NIL. The best
   Given that understanding human play against the                DSHM model was the lag 3, vector length 1024, model
aggressive lag 2 networks is difficult due confounding            discussed above.       The best network model was the
factors discussed in West and Lebiere (2001), and the fact        aggressive lag 2 network.
that an exact target win difference (after a fixed number of         New network versus network simulations were run for
trials) is not available, comparison to the data against this     this comparison: 10000 games were run against each of the
opponent was simplified.                                          two benchmark opponents. The mean difference in wins
   The DSHM models were rated according to the mean               between the aggressive lag 2 and aggressive lag 1 networks
squared difference between their average final scores             was somewhat lower than was found in West and Lebiere
against the aggressive lag 1 and passive lag 2 networks, and      (2001). However, given the high standard deviation on the
the average finals scores of humans against these models.         win differences, both the results found here and those found
Additionally, DSHM models that won against the                    in West and Lebiere (2001) may be valid.
aggressive lag 2 were disqualified as potential models of
human play. This is because all that is certain about human                  Table 2: Models of Human Rock=1 PRS
performance against the aggressive lag 2 networks is that             Opponent      Human ACT-R           DSHM Network
humans lost to these networks, on average. Additionally,              Agg. lag 1 9.99         12.30       10.89       5.76
West and Lebiere (2001) discuss factors that could make the           Pas. lag 2    11.14     8.15        13.24       10.44
interpretation of human players’ performance against the
                                                                      Rating                  14.27       5.22        18.37
                                                              2344

   Table 2 summarizes the best models of human Rock=1              Taylor and West (2004). The evaluation method for all
PRS play. The DSHM produces the closest fit to the human           types of models was the same as for the DSHM models
data, i.e., it scored the lowest mean squared error.               discussed above.
Additionally, the DSHM model lost to the aggressive lag 2            Figures 3 and 4 summarize the evaluations of the three
model by a mean win difference of 6.22 (not shown in this          model types. The confidence values for the human data in
table), which helps support this model as a good account of        Figure 3 are estimated based on the 95% confidence
human PRS play.                                                    intervals of a regression analysis of the rate of point
                                                                   difference achieved by the human players against each of
Rock=2 Simulations                                                 the three opponents.
The mechanism for building a sensitivity to the unequal
payoffs of the Rock=2 game in the DSHM models was                      40
essentially the same as for the ACT-R Rock=2 models.
Three versions of the Rock=2 DSHM models were created:                 30
one that attended to opponents’ rock plays more; another
                                                                                                                        Human
that attended to scissors more; and, one that gave preference
                                                                       20
to both rock and scissors. This extra attention was achieved                                                            ACT-R
by training the models twice on sequences of opponents’                                                                 DSHM
moves ending in these plays.                                           10
   For each of the three variations on the Rock=2 DSHM                                                                  Network
player, the combinations of the three different lags and                 0
seven vector lengths were tested.                                            Agg. lag 1   Agg. lag 2 Rock=2 lag 1
                                                                      -10
Evaluation Each of the DSHM players results were
                                                                     Figure 3: Points difference comparison for Rock=2 PRS.
compared to the human data from Rutledge-Taylor and
                                                                     Human values include estimated 95% confidence values.
West (2004). A mean squared error approach was used. Six
data point were compared: the mean points differences
versus the three network models, and the strategy indices              30
versus these opponents. Because the strategy index values              25
were, on average smaller than the point differences, and
                                                                       20
because they are crucially important to establishing strategic                                                          Human
play, these three data points were given twice the weight of           15
                                                                                                                        ACT-R
the points difference comparisons. Thus, each model’s                  10
rating was the sum of the squares of the mean point                                                                     DSHM
differences and twice the sum of the squares of the strategy             5
                                                                                                                        Network
indices.                                                                 0
                                                                        -5   Agg. lag 1   Agg. lag 2 Rock=2 lag 1
Results The results were somewhat predictable based on the
DSHM rock=2 and ACT-R Rock=2 results: The lag 3                       -10
DSHM model with vector lengths of 1024, and that paid                 Figure 4: Strategy indices comparison for Rock=2 PRS
extra attention to both rock and scissors produced the best
fit to the human data. It matched five of the six data points        All three models produced good fits to the human data.
very well. However, this model failed to defeat the                However, the DSHM model is unique in that it failed to beat
aggressive lag 2 network model. There were other DSHM              the aggressive lag 2 network. However, it correctly scored a
models that beat the aggressive lag 2 network, but failed to       negative strategy index versus this opponent. In contrast,
match the other five data points well (e.g., the margins of        the ACT-R and network models beat the aggressive lag 2,
victory were too great).                                           but did not score negative strategy indices, as did the human
                                                                   players. Thus, there is an obvious objective in building a
               Rock=2 Model Comparison                             superior model of human play. That is, to build a model
Three different kinds of models of human Rock=2 PRS play           that beats the aggressive lag 2, but does so despite a
are discussed in this paper: ACT-R, DSHM and perceptron-           negative strategy index.
like networks. The ACT-R results are taken from Rutledge-
Taylor and West (2005), the DSHM model is the one                                         Conclusions
discussed above. As with the Rock=1 comparison, new                The simulations run and analyzed here demonstrate that
network versus network data was collected. The network             DSHM can, in fact, be used successfully to model at least
model of human Rock=2 played each of the benchmark                 one memory task that relies on reconciling inconsistent
opponents 10000 times. Each model played 300 trial games           information and rapidly changing predictions based on past
against the three network opponents discussed in Rutledge-         events. Despite the fact that DSHM was designed to model
                                                               2345

only long-term memory, it may also useful as a model of         Cognitive Modeling, 255-260. Pittsburgh, PA: Carnegie
short-term memory. This also suggests that long-term and        Mellon University/University of Pittsburgh.
short term memory in humans may rely on the same basic         Rutledge-Taylor, M. F. & West, R. L. (2005). ACT-R
mechanisms.                                                     versus neural networks in rock=2 paper, rock, scissors.
                                                                Proceedings of the Twelfth Annual ACT-R Workshop, 19-
                       References                               23. Trieste, Italy: Universita degli Studi di Trieste.
Lebiere, C. & West, R. L. (1999) A dynamic ACT-R model         Rutledge-Taylor, M. F. & West, R. L. (2008) Modeling The
  of simple games. In Proceedings of the 21st Annual            fan effect using dynamically structured holographic
  Conference of the Cognitive Science Society. 296-301.         memory. In Proceedings of the 30th Annual Conference of
  Simon Fraser University: Vancouver, Canada.                   the Cognitive Science Society. 385-390. Washington, DC.
Rutledge-Taylor, M. F., Pyke, A. A., West, R. L. & Lang,       West, R. L. (1998) Zero sum games as distributed cognitive
  H. (2010) Modeling a three term fan effect. In                systems.      In Proceedings of the Complex Games
  Proceedings of the Tenth International Conference on          Workshop. Tsukuba, Japan: Electrotechnical Laboratory
  Cognitive Modeling. Philadelphia, PA: Drexel University.      Machine Inference Group.
Rutledge-Taylor, M. F., Vellino, A. & West, R. L. (2008) A     West, R. L., & Lebiere, C. (2001). Simple games as
  holographic associative memory recommender system. In         dynamic, coupled systems: Randomness and other
  Proceedings of the Third International Conference on          emergent properties. Cognitive Systems Research, 1(4),
  Digital Information Management. 87-92. London, UK.            221-239.
Rutledge-Taylor, M. F. & West, R. L. (2004) Cognitive          VonNeumann, J., & Morgenstern, O. (1944) Theory of
  modeling versus game theory: Why cognition matters. In        Games and Economic Behaviour. Princton, NJ: Princton
  Proceedings of the Sixth International Conference on          University Press.
                                                           2346

