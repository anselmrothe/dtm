UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Gaze Following as Goal Inference: A Bayesian Model
Permalink
https://escholarship.org/uc/item/0f85n446
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Friesen, Abram L.
Rao, Rajesh P.N.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                           Gaze Following as Goal Inference: A Bayesian Model
                                             Abram L. Friesen and Rajesh P. N. Rao
                                                  {afriesen, rao}@cs.washington.edu
                                            Department of Computer Science and Engineering
                                                 University of Washington, Box 352350
                                                         Seattle, WA 98195 USA
                               Abstract                                two groups did (Meltzoff & Brooks, 2008). These results
                                                                       suggest that (a) gaze following involves an inference of
   The ability to follow the gaze of another human plays a crit-
   ical role in cognitive development. Infants as young as 12          the underlying intention or goal of the head movement,
   months old have been shown to follow the gaze of adults. Re-        and (b) self-experience plays a major role in learning the
   cent experimental results indicate that gaze following is not       consequences of intentions and related actions.
   merely an imitation of head movement. We propose that chil-
   dren learn a probabilistic model of the consequences of their          In this paper, we propose a new model for gaze following
   movements, and later use this learned model of self as a surro-     and joint attention that can be viewed as a probabilistic instan-
   gate for another human. We introduce a Bayesian model where         tiation of the “Like me” hypothesis. The model itself is gen-
   gaze following occurs as a consequence of goal inference in a
   learned probabilistic graphical model. Bayesian inference over      eral and can be applied to modeling other forms of goal-based
   this learned model provides both an estimate of another’s fix-      imitation, but we focus here on gaze following. In the fol-
   ation location and the appropriate action to follow their gaze.     lowing section, we derive our framework for gaze following
   The model can be regarded as a probabilistic instantiation of
   Meltzoff’s “Like me” hypothesis. We present simulation re-          based on probabilistic graphical models. We describe how a
   sults based on a nonparametric Gaussian process implemen-           child could learn a probabilistic model of the consequences of
   tation of the model, and compare the model’s performance to         their own head movements, and later use this learned model
   infant gaze following results.
                                                                       to interpret the actions of another person. Bayesian inference
   Keywords: cognitive development; machine learning; artifi-
   cial intelligence; goal inference; Bayesian modeling; gaze fol-     over the learned graphical model provides both an estimate
   lowing.                                                             of another’s fixation location and the appropriate action to
                                                                       move one’s own gaze for joint attention. For the simulations,
                           Introduction                                a model based on Gaussian process regression was used to
Gaze following plays an important role in cognitive develop-           learn the mapping between goals, actions, and their sensory
ment. Following the gaze of an adult, for example, allows a            consequences. We present preliminary results comparing the
child to jointly attend to an object, learn its name and other         model to infant gaze following results and discuss the appli-
properties, as well as learn useful actions to perform on the          cability of the proposed framework for understanding other
object through imitation. It has been shown that children as           forms of goal-based imitation and sensorimotor planning.
young as 12 months old can follow the gaze of an adult and
engage in joint attention (Brooks & Meltzoff, 2002).                          A Bayesian Model for Gaze-Following
   Recent results have shown that gaze following is not                In the following section, we develop and explain our model
merely an imitation of head movement. For example, 14-                 for gaze-following as goal inference. We begin with a sim-
and 18-month olds do not follow the gaze of an adult who               plified explanation of our graphical model, and then give an
is wearing a blindfold, although they follow gaze if the adult         overview of the computational components.
wears the same band as a headband. This suggests that these
children do not follow gaze because they are aware of the              Hypothesis
consequences of wearing a blindfold (i.e., occlusion) and              Our hypothesis is the following: humans learn a goal-directed
unlike 12-month olds, make the inference that the adult is             mechanism for planning gaze movements. A goal location,
not looking at an object. This observation is closely related          provided by either internal or external stimuli, combined with
to Meltzoff’s “Like me” hypothesis (Meltzoff, 2005) which              the current state, determines an action. This action, again in
states that self-experience plays an important role in making          conjunction with the current state, determines the final state.
inferences about the internal states of others. In particular,         We represent this mechanism with the graphical model shown
in the case of the blindfold experiment, self-experience with          in Figure 1(a) where G is the goal, A is the action, Xi is the
own eye closure and occluders may influence gaze following             current state, and Xf is the final state. In the context of gaze
behavior. To test this hypothesis, Meltzoff and Brooks                 following, the goal is a desired fixation location, the action
provided one group of 12-month olds with self-experience               is a vector of motor commands, and the state represents head
with an opaque blindfold while two other groups either had             position and orientation.
no self-experience or had self-experience with a windowed                 With our proposed model, an artificial agent can both plan
blindfold. On seeing an adult with a blindfold turn towards            future movements given desired fixation points and determine
an object, most of the children who had had self-experience            fixation points given observed head poses. Both of these cor-
with blindfolds did not turn to the object while the other             respond to performing inference over the graphical model.
                                                                   2457

            G                                       G                    both flexible and robust. First, they are nonparametric so
                                                                         they do not limit the set of functions that h can be chosen
                                                                         from. Second, they estimate a probability distribution over
            A                               B       A                    functions instead of choosing a single most-likely candidate
                                                                         function, allowing all plausible functions to be incorporated
                                                                         into the estimation process and reducing model bias. For
           Xi        Xf                            Xi       Xf
                                                                         these reasons, GPs are effective with small numbers of
                                                                         training samples, increasing their biological plausibility.
         (a) Basic Model               (b) Model with Blindfold Ex-      We now show how we use these GPs for planning, goal
                                       perience                          inference, and gaze-following.
                           change of coords.
                    Gm                        G                          Goal-directed Planning GPs are trained via supervised
                                                                         learning: given a training dataset with noisy labels, the GP
                                                                         learns the functional mapping between the input data and the
           Bm       Am                B       A
                                                                         labels (output). The training data itself could be obtained, for
                                                                         instance, through a reinforcement-based paradigm that com-
                    Xim        Xfm            Xi        Xf               bines exploration of the goal-action-state space for training
                                                                         the transition GP with selection of data from successful trials
                                                                         for training the policy GP (see (Verma & Rao, 2006) for re-
                      (c) Gaze following model
                                                                         lated ideas). After training, it is simple for the agent to fixate
                                                                         on a goal location. We assume that the agent knows its cur-
Figure 1: Graphical models for controlling gaze: (a) contains the
basic model which relates the different variables, (b) demonstrates      rent state xi and the goal g. Given these, the agent uses its
the influence of blindfold experience on the model, and (c) shows        learned distribution over functions, GPπ , to compute a proba-
the combined graphical models for following the gaze of a mentor.        bility distribution over actions, p(a|xi , g) = N (a|µa , Σa ). This
Shaded variables demonstrate observed variables. The darker shad-
ing indicates that B is an observed discrete variable, while the rest    distribution can then be passed through GPf to estimate the
of the nodes are continuous.                                             probability of the resulting state, p(xf |xi , g) ≈ N (x f |µxf , Σxf ).1
                                                                         Thus, our model provides us both with the final head position
                                                                         and an estimate of the uncertainty in the prediction.
Furthermore, through the “Like me” hypothesis, a human can
apply their model to another person in order to infer where              Goal Inference Through the use of our computational
that person is looking and then translate that location back to          model, it is also possible to infer the goal of a head movement
their own model in order to fixate on the same location, as is           given observations of the starting and ending head poses, xi
necessary for gaze following (Figure 1(c)).                              and xf , respectively. To accomplish this, the agent must be
                                                                         able to recover the inputs to each GP given the outputs. Fortu-
Computational Model                                                      nately, results from (Rasmussen & Ghahramani, 2003) allow
We begin by showing how an agent can learn the mapping be-               us to estimate a distribution over the inputs given the outputs.
tween goals, states, and actions in order to plan how to fixate          As such, we can infer a distribution over actions given xi and
on a specific goal location. The agent first learns a transi-            xf and then use this to estimate a distribution over goals.
tion model, f , (e.g., through exploration or “body babbling”)
which translates an initial position, xi , and an action, a, to a        Gaze Following Goal-directed gaze following is accom-
final head position, xf ,                                                plished through our use of the “Like me” hypothesis. The
                                                                         agent learns a model of itself and assumes that a mentor uses
                xf = f (xi , a) + ν,    ν ∼ N (0, Σν ),           (1)    this same model. The agent observes the starting and ending
                                                                         states (head poses) of a mentor and then infers the goal loca-
where N (µ, Σ) signifies the normal distribution with mean µ             tion of the mentor, gm , by inferring what it would be looking
and covariance matrix Σ. The learned transition model can                at if it were in the mentor’s position. After inferring the men-
in turn be used to learn a policy function, π, which maps the            tor’s goal, the agent transforms that goal into its own coordi-
initial head position and a goal location g to an action                 nate frame and then infers how to fixate on that goal. For this
                                                                         paper, we assume the agent has acquired the ability to trans-
                a = π(xi , g) + υ,     υ ∼ N (0, Συ ).            (2)    form between coordinate frames through prior experience.
This function essentially determines the rotation required to            Blindfold Experiments
turn the head from its current position to face the goal.
                                                                         To demonstrate the robustness and plausibility of this model,
   We use two separate Gaussian processes (GPs)
                                                                         we recreate an experiment from (Meltzoff & Brooks, 2008)
(Rasmussen & Williams, 2006), GPπ and GPf , to learn
                                                                         and test our model on it. We incorporate a blindfold vari-
these nonlinear functions. GPs are commonly used in
machine learning to infer a latent function h(·) from noisy                  1 This is an approximation because, in general, a Gaussian passed
observations yi = h(xi ) + ν, ν ∼ N (0, σν2 ) because they are           through a nonlinear function does not remain Gaussian.
                                                                     2458

able, B ∈ {0, 1}, (Figure 1(b)) and allow the agent to learn                   & Rasmussen, 2003).
the effects of being blindfolded. Our model learns a new
Gaussian process (GPπb , to use in place of GPπ ) for when the                           µ∗ =    x∗ [ h [h(x∗ )|x∗ ]|µ, Σ] = x∗ [mh (x∗ )|µ, Σ]    (6)
agent is blindfolded. When the blindfold is in place, actions                            σ∗2 =   x∗ [mh (x∗ ) |µ, Σ] + x∗ [σh (x∗ )|µ, Σ] − µ∗
                                                                                                             2                   2            2
                                                                                                                                                   (7)
are random and goals are not causally linked to states (head
poses) or actions. The agent can learn this and then apply                     Note that this is a Gaussian approximation to the true dis-
this knowledge to a mentor agent to infer that the mentor’s                    tribution of p(h(x∗ )|µ, Σ) ≈ N (µ∗ , σ∗2 ) where the first two
goal is random in relation to the observed head movement,                      moments (mean and variance) are matched exactly. This
if it is blindfolded. However, if this alternate Gaussian pro-                 computation can be extended to the multivariate case where
cess is not learned, the agent does not know anything about                    h : D → E . The only difference is that the target dimen-
the blindfold and follows the mentor’s head movement even                      sions co-vary and the corresponding predictive covariance
if the mentor is blindfolded.                                                  matrix is no longer diagonal (as it is when the input is de-
                                                                               terministic in the multivariate case) (Deisenroth et al., 2009).
                          Technical Details
                                                                               Inference in Graphical Models
Gaussian Processes
                                                                               In this paper, we use Bayesian networks, a type of probabilis-
We briefly introduce the notation and theory of Gaussian pro-                  tic graphical model, to illustrate the relationships and condi-
cesses (GPs). More detail can be found in (Rasmussen &                         tional independencies between variables.
Williams, 2006). Formally, a Gaussian process is a collection
of random variables, any finite number of which have a joint                   Forward Inference Using the above equations, we can
Gaussian distribution. The random variables represent the                      compute p(Xf |Xi = xi , G = g) for the graphical model shown
value of the function h(x) at location x. A GP is fully specified              in Figure 1 as follows. Given a current state, xi , and
by its mean function m(·) and covariance function k(·, ·).We                   a goal, g, we first compute p(A|xi , g) = p(π(xi , g) + υ) as
write the Gaussian process as h(x) ∼ GPh (mh (x), kh (x, x0 )),                p(A|xi , g) = N (a|mπ ([xi g]| ), kπ ([xi g]| )) from GPπ using (3)
to denote the fact that the GP learns a probability distribu-                  and (4) where we use [xi g]| as x∗ and π as h. Alternatively,
tion over functions. GPs place a prior p(h) directly on the                    when gaze following, we have a distribution over goals in-
space of functions. We use a prior mean function mh = 0 and                    stead of a deterministic value. For this case, we compute
use the squared exponential (SE) kernel as our prior covari-                  p(A|xi , g) ≈ N (a|µa , σa ) using equations (6) and (7). This
                                                                               quantity can again be substituted into (6) and (7),3 where
                                                                        
ance function kh (x p , xq ) = α 2 exp − 12 (x p − xq )| Λ−1 (x p − xq ) ,
                                                                               [a xi ]| is now x∗ and f is h, in order to compute p(Xf |xi , g) =
where x p , xq ∈ D , Λ = diag([`21 , ..., `2D ]) is a diagonal matrix
                                                                               p( f (xi , a) + ν|µa , Σa ) ≈ N (xf |µxf , Σxf ) from GPf .
of squared characteristic length-scales, and α 2 is the variance
of the latent function h.2 Thus, the posterior predictive dis-                 Reverse Inference More complicated is the reverse infer-
tribution of the function value h∗ = h(x∗ ), for an arbitrary test             ence where we infer the goal from observations of the current
input x∗ , is Gaussian with mean and variance                                  and final state, p(G|Xi = xi , Xf = xf ). This computation is sim-
                                                                               ilar to the filtering and smoothing computations used in Gaus-
                                       |                    |
              mh (x∗ ) =   h [h∗ ] = k∗ (K + σε2 I)−1 y = k∗ β ,      (3)      sian dynamical systems (reviewed in (Deisenroth, 2010)). To
             σh2 (x∗ ) = varh [h∗ ] =
                                             |
                                      k∗∗ − k∗ (K + σε2 I)−1 k∗ ,     (4)      begin, we set Xi = xi and perform forward inference using
                                                                               our prior p(G = g). We obtain p(A|xi ) = N (a|µa , Σa ) and
respectively, with k∗ = k(X, x∗ ), k∗∗ = k(x∗ , x∗ ), β = (K +                 p(Xf |xi ) ≈ N (xf |µxf , Σxf ). Now, we approximate the joint
σε2 I)−1 y, K is the kernel matrix with Ki j = k(xi , x j ), X =               p(A, Xf |xi ) with a Gaussian.4
[x1 , ..., xn ] are the training inputs, and y = [y1 , ..., yn ] are the          The remaining uncomputed values, the cross terms of the
                                                                                                                      |           |
training targets.                                                              joint covariance Σaxf = axf [axf ] − µa µxf , are computed as in
    It is also possible to predict with GPs when the test input                (Deisenroth et al., 2009; Deisenroth, 2010). Now that we
x∗ ∼ N (µ, Σ) is uncertain. This corresponds to seeking                        have the joint, we incorporate the measurement xf by ap-
                                Z                                              plying standard Gaussian conditioning formulas to it to get
                                                                                                        p p                 p
            p(h(x∗ )|µ, Σ) = p(h(x∗ )|x∗ )p(x∗ |µ, Σ)dx∗ .            (5)      p(A|xi , xf ) = N (a|µa , Σa ), where µa = µa + Σaxf (Σxf )−1 (xf −
                                                                                              p                         |
                                                                               µxf ), and Σa = Σa − Σaxf (Σxf )−1 Σaxf .
For the SE kernel, we can compute the mean µ∗ and variance                        However, we want a distribution over goals, not over ac-
σ∗2 of equation 5 in closed form, following (Deisenroth, Hu-                   tions. Rather than attempting to find a closed-form solu-
ber, & Hanebeck, 2009; Quiñonero-Candela, Girard, Larsen,                     tion to the joint p(G, Xf |xi ), we sample from p(A|xi , xf ) (we
                                                                               approximate the expectation with a finite sum) and use the
     2 We determine the hyperparameters, θ , of each GP by max-
                                                                                   3 Technically, we use the formulas for multivariate inputs and out-
imizing the marginal likelihood p(y|X, θ ) with respect to the hy-
perparameters, which is equivalent to maximizing the posterior                 puts which can be found in (Deisenroth, 2010).
distribution over the hyperparameters for a flat hyperprior p(θ ).                 4 This is a standard approximation used in Gaussian filters such
This optimization was done using the GPML software, available at               as the UKF (Julier & Uhlmann, 2004) and the GP-ADF (Deisenroth
http://www.gaussianprocess.org.                                                et al., 2009).
                                                                           2459

samples as “measurements” when we compute p(G, A|xi , xf ).              dimensionalities of the training inputs and the training targets,
These “measurements” act in the same way as the measured                 respectively, and n is the size of the training set (Deisenroth
xf , above, and allow us to condition on the sampled A = a and           et al., 2009). As such, our model may not scale well to high
then marginalize out A (over all of the samples) in order to             dimensions without additional approximations; however, it
get our desired result.                                                  currently runs in sub-minute times for the dimensionality we
                                                                         are using. Additionally, the Gaussian approximations we are
                             Results                                     making have little effect because the data is unimodal and
Implementation Details                                                   close to symmetric.
                                                                            Figure 3 shows performance results for our model as it per-
To test our model, we randomly sample goal positions and
                                                                         forms forward inference, reverse inference, and gaze follow-
then compute the action required to fixate on this goal. We
                                                                         ing (combined reverse and forward inference). Other than a
add Gaussian noise to this action, compute the resulting gaze
                                                                         few outliers, the estimated values of the model are accurate
vector if this action were taken, and add Gaussian noise to this
                                                                         and precise. The model is robust to noise and is able to pro-
gaze vector. This method is equivalent to training the model
                                                                         vide strong gaze following results even though uncertainty is
with rejection sampling wherein the agent rejects all samples
                                                                         compounded by the second level of inference.
that do not result in successful fixation on the goal position.
The Gaussian processes are trained on this randomly gener-               Blindfold Self-Experience Task
ated data and then tested on separate test data.
                                                                         In order to validate the cognitive plausibility of our model,
   The default reference frame for both agent and mentor is
                                                                         we recreate a cognitive science experiment from (Meltzoff &
at the origin gazing along the x-axis. Each agent has their
                                                                         Brooks, 2008), where infants’ self-experience with a blind-
own reference frame and we assume that we know the trans-
                                                                         fold affects whether or not they follow the gaze of blindfolded
formation from the mentor’s reference frame to the agent’s.
                                                                         adults. In their tests, one third of the 96 infants are given ex-
This transformation is not learned by our model; however,
                                                                         perience with a blindfold, another third are given experience
we believe that this is a minor assumption, especially since
                                                                         with a “windowed” blindfold, and the remainder gain no ex-
we already assume the agent can observe the mentor’s po-
                                                                         perience with either. The children then interact with an adult
sition and gaze angle for our model. The mentor and agent
                                                                         experimenter for four trials where, in each trial, the experi-
are positioned as shown in Figure 2. Goal locations for the
                                                                         menter stops playing with the infant, places the blindfold over
training data were generated uniformly at random from the
                                                                         her own eyes, and then silently turns her head to align with
area between the agent and the mentor (within the rectangle
                                                                         one of two targets (placed between the experimenter and the
formed by x = [100, 500], y = [−500, 500], where the agent is
                                                                         infant but offset to the left and right). The trials are given a
at (0, 0) and the mentor is at (600, 0)).
                                                                         score of +1 if the infant looks in the direction of the target,
   We used Gaussian noise with standard deviation of 3 de-
                                                                         −1 if the infant looks in the direction of the other target, and 0
grees for angles and a standard deviation of 10 cm for loca-
                                                                         if the infant looks elsewhere. The looking score is calculated
tions and distances. For reverse inference, the prior goal state
                                                                         as the sum of correct looks, incorrect looks, and no-looks and
p(G) is a Gaussian centered halfway between the two agents
                                                                         thus the possible looking score range is [−4, +4].
along the x-axis with very high variance. While this prior is
quite weak, a single observation of (xi , x f ) is insufficient to
                                                                                                     400
overcome the prior in reverse inference. Instead, we use a
single observation of xi and five observations of x f to get an
                                                                                                     200
accurate estimate of p(G|xi , x f ). More precisely, we run re-
                                                                                  y position (cm)
verse inference with the observed values (xi , x(1)
                                                  f ) to compute                                       0
p(G|xi , x(1)
          f ), and then use this as the prior for a second run
                                                                                                           Agent                           Mentor
                                                                                                    −200
of reverse inference to   compute p(G|xi , x(1:2)
                                            f     ). We repeat this
                                    (1:5)
five times to compute     p(G|xi , x f ). We believe that this is                                   −400
a reasonable number of observations for a human to make in                                                 true fixation points
                                                                                                           inferred fixation points
the short amount of time taken for gaze following.                                                  −600
                                                                                                               0          200        400    600
                                                                                                                        x position (cm)
Model Performance
Overall, we found that the model performs quite well. It                 Figure 2: Experimental setup of a gaze following task. Inferred
                                                                         goal positions are shown (red) next to true goal positions (blue).
learns accurate transition and policy functions from small               Black arrows represent the initial and final gaze vectors of the agent
amounts of noisy training data (n = 200 data points were used            and mentor for a single test data point.
in our accuracy tests) and the nonparametric nature of Gaus-
sian processes ensures very little customization is required.               Similarly, in our experiment we train 60 separate agents
The computational complexity of evaluating our model for                 with our model on randomly generated training data. One
gaze following is O(E 3 ) + O(DE 2 n2 ), where D and E are the           third of these agents are given additional experience with a
                                                                      2460

                                  0.2                                                      0.2                                                   0.2
                                 0.15                                                     0.15                                                  0.15
                      p(error)                                                 p(error)                                              p(error)
                                  0.1                                                      0.1                                                   0.1
                                 0.05                                                     0.05                                                  0.05
                                   0                                                        0                                                     0
                                           0       10      20    30    40                        −40        −20          0                             −40       −20           0
                                               gaze error (degrees)                                    gaze error (degrees)                                  gaze error (degrees)
                                        (a) Forward inference                                    (b) Reverse inference                                   (c) Gaze following
Figure 3: Histogram plots showing the probability of an error (in degrees) between the inferred and the true gaze vector. These probabilities
were estimated from 375 test points spread uniformly over the test region. Note how the accuracy gracefully decays as more complicated
inference is performed ((a) is the simplest, while (c) is the most complex).
blindfold wherein they train an additional GPπ for their model                                                  blindfolds, we provided the agents in this experiment with
so that they now have GPπ and GPπb , where GPπ is the orig-                                                     very little training data (n = 15). If we use more training data,
inal GP of the model and GPπb is the GP with blindfold ex-                                                      the agents perform almost perfectly in this task. Results are
perience. The other 40 agents are trained normally although                                                     shown in Figure 4 and match those of Meltzoff and Brooks
one group is the baseline group and the other is the windowed                                                   (Figure 4(a)). This is indicative that gaze following involves
group. However, in our formulation, the windowed blindfold                                                      understanding the underlying intention or goal and that self-
in the model amounts to presenting the same training data as                                                    experience plays a major role in learning the consequences of
the no-blindfold case, so these two groups will be identical                                                    intentions and related actions.
except for noise. Each agent is presented with 4 trials where
it observes a mentor make a head turn to face either 45 de-                                                                              Related Work
grees to the left or 45 degrees to the right (plus noise). Trials                                               Our model is closely related to the goal-based imitation
are scored as +1 if the agent turns its head at least 30 degrees                                                model of Verma and Rao (Verma & Rao, 2006) and the in-
in the direction of the correct target and −1 if it turns its head                                              verse planning model of Baker et al. (Baker, Saxe, & Tenen-
at least 30 degrees in the direction of the wrong target.                                                       baum, 2009). Unlike these previous models, which assume
   The agents with no blindfold experience use the basic                                                        discrete state and action spaces, our model is based on a
model (which contains no blindfold knowledge) and thus as-                                                      nonparametric model that allows learning and inference in
sume that the mentor is fixating on an object to the left or to                                                 continuous state and action spaces. Also related to our ap-
the right. Those with blindfold experience observe that the                                                     proach are models for planning based on probabilistic in-
mentor is wearing a blindfold and use their learned GPπb for                                                    ference (Toussaint & Storkey, 2006; Verma & Rao, 2007;
the reverse inference (applying their model to the mentor).                                                     Botvinick & An, 2009). Again, these models are restricted
For this group, the agent has (hopefully) learned that, when                                                    to discrete state and action spaces or assume knowledge of
blindfolded, there is no correlation between the mentor’s gaze                                                  the dynamics of the world. Our model for gaze following
and the mentor’s goal position. The blindfold-experienced                                                       joins the growing number of Bayesian models for cognition
agent then uses GPπ for the forward inference (because the                                                      proposed in recent years and acknowledges the psychophys-
agent is not wearing a blindfold).                                                                              ical and neurobiological evidence for Bayesian mechanisms
                                                                                                                in perception and action (e.g., (Rao, Olshausen, & Lewicki,
                  4
                                                                                                                2002; Oaksford & Chater, 2007)).
                                                                                                                   Within the realm of gaze following models, one class of
  Looking Score
                  3
                                                                                                                models posits that young infants watch an adult’s head move-
                  2
                                                                                                                ment in space and are drawn to the correct hemi-field where
                  1                                                                                             they are attracted to a salient target object (Butterworth & Jar-
                  0
                                                                                                                rett, 1991); over time, they learn to follow gaze to an object
                        Baseline        Window     Opaque
                                                                                                                (Moore, 1999). A second class of models supports the nativist
       (a) Computational Model                                        (b) Infant Data                           view that infants have a built-in module for interpreting eye
                                                                                                                gaze in terms of visual experience in others (Baron-Cohen,
Figure 4: Comparison of computational model to actual collected                                                 1995). A third class of models adopts the developmental view
infant data from (Meltzoff & Brooks, 2008).                                                                     that gaze following behavior emerges from self-experience
                                                                                                                (Meltzoff & Brooks, 2007). Our model can be regarded as a
      In order to simulate infants with little experience with                                                  Bayesian example of this last class of models.
                                                                                                          2461

                Summary and Conclusion                               ceedings of the 26th Annual International Conference on
This paper proposes a Bayesian framework for social inter-           Machine Learning (pp. 225–232). New York, NY, USA:
action that postulates that (1) children learn a probabilistic       ACM.
model of the sensory consequences of their own movements           Julier, S. J., & Uhlmann, J. K. (2004). Unscented Filtering
through self-experience, and (2) they use this learned model         and Nonlinear Estimation. IEEE Review, 92, 401–422.
to interpret the actions of others. Specifically, we show how      Meltzoff, A. N. (2005). Imitation and other minds: The ”like
gaze following can be modeled as goal inference within such          me” hypothesis. In S. Hurley & N. Chater (Eds.), Perspec-
a framework: probabilistic inference over the unknown vari-          tives on imitation: From cognitive neuroscience to social
ables in a learned graphical model allows an agent to infer          science (pp. 55–77). Cambridge, MA: MIT Press.
another’s gaze direction as well as the action to direct gaze      Meltzoff, A. N., & Brooks, R. (2007). Eyes wide shut:
to the same location. When given self-experience with blind-         The importance of eyes in infant gaze following and un-
folds, the model learns the consequences of occlusion and            derstanding other minds. In R. Flom, K. Lee, & D. Muir
subsequently does not follow the gaze of a blindfolded agent,        (Eds.), Gaze-following: Its development and significance
replicating infant gaze following results.                           (pp. 217–241). Mahwah, NJ: Erlbaum.
   The proposed framework raises several interesting ques-         Meltzoff, A. N., & Brooks, R. (2008). Self-experience as a
tions: (1) Can the model be extended to other goal-based             mechanism for learning about others: A training study in
imitation tasks, e.g., goal-directed reaching? (2) This paper        social cognition. Developmental Psychology, 44(5), 1257–
explored a nonparametric implementation based on Gaussian            1265.
processes but what are possible ways of neurally implement-        Moore, C. (1999). Gaze following and the control of atten-
ing the learned graphical model? (3) The paper assumes that          tion. In P. Rochat (Ed.), Early social cognition: Under-
states in the environment are known (corresponding to the            standing others in the first months of life (pp. 241–256).
case of MDPs or Markov decision processes) – how does the            Mahwah, NJ: Erlbaum.
model extend to the more realistic case where only obser-          Oaksford, M., & Chater, N. (2007). Bayesian rationality:
vations of states are available (partially observable MDPs or        The probabilistic approach to human reasoning. Oxford:
POMDPs) and where learning involves reward-based mecha-              Oxford University Press.
nisms? We plan to explore these issues in future work.             Quiñonero-Candela, J., Girard, A., Larsen, J., & Rasmussen,
                                                                     C. (2003). Propagation of Uncertainty in Bayesian Kernel
                     Acknowledgments                                 Models - Application to Multiple-Step Ahead Forecasting.
                                                                     IEEE International Conference on Acoustics, Speech and
This research was supported by an NSERC fellowship to                Signal Processing, 701–704.
A. Friesen and ONR Cognitive Science program grant no.             Rao, R. P. N., Olshausen, B. A., & Lewicki, M. S. (2002).
N000140910097. We thank Marc Deisenroth, Andrew Melt-                Probabilistic Models of the Brain: Perception and Neural
zoff, and L. Elisa Celis for discussions. We also thank Marc         Function. Cambridge, MA: MIT Press.
for sharing his code.                                              Rasmussen, C. E., & Ghahramani, Z. (2003). Bayesian
                                                                     Monte Carlo. Advances in Neural Information Processing
                          References
                                                                     Systems, 15, 489–496.
Baker, C. L., Saxe, R., & Tenenbaum, J. B. (2009). Action          Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian
   Understanding as Inverse Planning. Cognition, 113, 329–           Processes for Machine Learning. Cambridge, MA: MIT
   349.                                                              Press.
Baron-Cohen, S. (1995). Mindblindness: An essay on autism          Toussaint, M., & Storkey, A. (2006). Probabilistic infer-
   and theory of mind. Cambridge, MA: MIT Press.                     ence for solving discrete and continuous state Markov De-
Botvinick, M., & An, J. (2009). Goal-directed decision mak-          cision Processes. 23rd International Conference on Ma-
   ing in prefrontal cortex: A computational framework. Ad-          chine Learning.
   vances in Neural Information Processing Systems (NIPS).         Verma, D., & Rao, R. P. N. (2006). Goal-based imitation as
Brooks, R., & Meltzoff, A. N. (2002). The importance of              probabilistic inference over graphical models. Advances in
   eyes: How infants interpret adult looking behavior. Devel-        Neural Information Processing Systems, 18.
   opmental Psychology, 38(6), 958–966.                            Verma, D., & Rao, R. P. N. (2007). Imitation Learning Us-
Butterworth, G., & Jarrett, N. (1991). What minds have in            ing Graphical Models. European Conference on Machine
   common is space: Spatial mechanisms serving joint visual          Learning, 757–764.
   attention in infancy. British Journal of Developmental Psy-
   chology, 9, 55–72.
Deisenroth, M. P. (2010). Efficient reinforcement learning
   using Gaussian processes. Karlsruhe: KIT Scientific Pub-
   lishing.
Deisenroth, M. P., Huber, M. F., & Hanebeck, U. D. (2009).
   Analytic moment-based gaussian process filtering. In Pro-
                                                               2462

