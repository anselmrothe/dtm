UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Explicit Bayesian Reasoning with Frequencies, Probabilities, and Surprisals
Permalink
https://escholarship.org/uc/item/1rw1c601
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Prime, Heather
Shultz, Thomas
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                     University of California

        Explicit Bayesian Reasoning with Frequencies, Probabilities, and Surprisals
                                              Heather Prime (heath.prime@gmail.com)
     Department of Human Development and Applied Psychology, Ontario Institute for Studies in Education/University of
                                                        Toronto, 252 Bloor Street West
                                                        Toronto, ON M5S 1V6 Canada
                                           Thomas R. Shultz (thomas.shultz@mcgill.ca)
              Department of Psychology and School of Computer Science, McGill University, 1205 Penfield Avenue
                                                       Montreal, QC H3A 1B1 Canada
                              Abstract                                   the use of frequencies instead of probabilities to describe
  To explore human deviations from Bayes’ rule in numerically
                                                                         reasoning problems and the use of surprisals, the degree of
  explicit problems, prior and likelihood probabilities or               being surprised, instead of probabilities to formulate
  frequencies are manipulated and their effects on posterior             responses.
  probabilities or surprisals are measured. Results show that
  people use both priors and likelihoods in Bayesian directions,                              Bayesian Predictions
  but the effect of likelihood information is stronger than that of
  prior information. Use of frequency information and surprisal          Bayes’ rule specifies that the posterior probability of a
  measures increase deviations from Bayesian predictions.                hypothesis h given data d is equal to the prior probability of
  There is evidence that people do compute something like the            the hypothesis times the conditional likelihood of the data
  standardizing marginal data term when asked for probability            given the hypothesis divided by the marginal probability of
  estimates, but not when asked for surprisal ratings.                   the data, defined as the sum of prior by likelihood products
   Keywords: Reasoning under uncertainty; Bayes’ rule;                   over all hypotheses H:
                                                                                            P(h ) × P(d | h )
                                                                            P(h | d ) =
   rationality; base-rate neglect; probability; surprisal.                                                                       (1)
                           Introduction                                                   ∑ P(h′)× P(d | h′)
                                                                                         h′∈H
A pressing issue in contemporary cognitive science is                       One possibility is that people reason, not with
whether people exhibit rationality in their inferences and               probabilities and Bayes’ rule, but rather with surprisals. A
learning under uncertainty, with rationality defined in terms            surprisal is mathematically defined in information theory as
of conformity to Bayes’ rule. Evidence for Bayesian                      the negative of the log to the base 2 of the probability of the
conformity comes from a wide variety of domains including                event (Cover & Thomas, 1991):
sensorimotor control (Körding & Wolpert, 2006), vision                      S ( p ) = − log 2 p                                  (2)
(Yuille & Kersten, 2006), conditioning (Courville, Daw, &                   More generally, the log in Equation 2 can be computed to
Touretzky, 2006), induction and inference (Tenenbaum,                    any base and multiplied by a constant. Here, we simplify by
Griffiths, & Kemp, 2006), and language (Chater &                         considering the constant to be 1 and using the base 2, which
Manning, 2006). In each of these areas, Bayesian models                  is suitable for the binary decisions that we consider.
account for a wide range of inferential phenonmena.                         The idea that people might reason with surprisals is based
  An unresolved problem is that these contemporary                       on the intuition that people do not often know probabilities
conclusions of Bayesian rationality appear to conflict with              and how to compute with them, but they do know whether
earlier Nobel-Prize-winning work showing that people are                 and how much they are surprised by events. For example,
rather poor Bayesians, subject to such biases as base-rate               estimate the probability that:
neglect (Kahneman, Slovic, & Tversky, 1982; Kahneman &                      1. Middle-eastern terrorists will destroy the NY trade
Tversky, 1996; Tversky & Kahneman, 1974, 1981).                          towers, the White House, the US Capitol, and the Pentagon
  The purpose of this paper is to contribute to the resolution           in the same hour. (Best before 9-11).
of this discrepancy between demonstrated Bayesian                           2. A gunman will enter a one-room Amish school house
successes and failures. It may be tempting to explain this               and kill 7 little girls.
discrepancy by noting that the successes mainly involve                     3. A modern highway overpass will collapse early one
performances in which the Bayesian ideal is only implicit,               weekend morning killing the occupants of a car passing
whereas the failures mainly involve more explicit reasoning              underneath.
with numerical problems. But why this implicit-explicit                     Although such probabilities are difficult to compute or
distinction would matter for Bayesian success would need to              estimate, people do know that they are very surprised when
be explained for this argument to be successful.                         such events happen. Perhaps surprisals would capture
  The present work focuses on factors that might be                      people’s intuitions about events more naturally than
expected to improve Bayesian performance on problems                     probabilities would.
that are typically described as explicit. These factors include
                                                                     1918

  Surprisals, defined as in Equation 2, can be interpreted as           Figure 3 shows that predicted surprisal, with marginal
bits of information with a unit measure of one bit (for a            data included, is a function of main effects of prior and
binary event with probability .5). Bayes’ rule can then be           likelihood, and an interaction between them. Surprisal
rewritten in surprisal form as the prior surprisal of the            decreases with both prior and likelihood, and the prior effect
hypothesis plus the likelihood surprisal of the data given the       is stronger at lower likelihood.
hypothesis minus the marginal surprisal of the data:                    Figure 4 reveals that, ignoring marginal data, posterior
   S (h | d ) = S (h ) + S (d | h ) − S (d )            (3)          surprisals are subject to just the two main effects of prior
The marginal surprisal of the data is the surprisal of the           and likelihood. Again, posterior drops with both prior and
probability computed in the denominator of Equation 1.               likelihood.
  Equation 3 simplifies Bayesian inference by replacing
multiplication with addition, and division with subtraction.                                        1
Because the marginal probability of the data given in the                                                              2 main effects & interaction
denominator of Equation 1, or its surprisal, is a complex                                       0.75                                                  Likelihood
computation and just a constant normalizing term, people
                                                                         Posterior
                                                                                                                                                           0.85
might also simplify by omitting that part of the computation,                                    0.5
                                                                                                                                                           0.15
whether reasoning in terms of probabilities or surprisals.
  Here, both of these hypotheses are tested – that people are                                   0.25
better Bayesians when asked for surprisals rather than
probabilities and that people simplify Bayesian inference by                                        0
omitting computation of the marginal data. It is doubtful                                                         0.85                    0.15
that ordinary people are conscious of such computations,                                                                      Prior
but these hypotheses are tested here by examining the
                                                                         Figure 2: Predicted posterior probabilities, marginal data
pattern of inferences across different problems.
                                                                                                excluded.
  Bayesian predictions are illustrated in Figures 1-4 for
scenarios in which high or low priors can be combined with
high or low likelihoods to estimate posteriors, whether in                                      6
the form of probabilities (Figures 1 and 2) or surprisals                                               2 main effects & interaction
                                                                          Posterior surprisal
                                                                                                5
(Figures 3 and 4). High values for priors and likelihoods are                                                                                         Likelihood
                                                                                                4
here .85; low values are .15. To generate these predictions,                                                                                                 0.85
probabilities are calculated with Equation 1, surprisals with                                   3
                                                                                                                                                             0.15
Equations 2 and 3.                                                                              2
  Four different predicted patterns are evident in the                                          1
prediction plots. Figure 1 shows that, when marginal data
                                                                                                0
are considered, posterior probabilities are subject to                                                          0.85                     0.15
equivalent main effects of priors and likelihoods; the higher
                                                                                                                            Prior
these inputs of priors and likelihoods, the higher the
posterior probability.                                                           Figure 3: Predicted posterior surprisals, marginal data
                                                                                                       included.
                1
                                   2 main effects only
                                                                                                6
              0.75                                                                                         2 main effects only
                                                                          Posterior surprisal
  Posterior
                                                  Likelihood                                    5
               0.5                                                                              4                                                     Likelihood
                                                         0.85
                                                                                                                                                              0.85
                                                         0.15                                   3
              0.25                                                                                                                                           0.15
                                                                                                2
                0                                                                               1
                     0.85                 0.15                                                  0
                               Prior                                                                            0.85                     0.15
                                                                                                                            Prior
     Figure 1: Predicted posterior probabilities, marginal data
                           included.                                             Figure 4: Predicted posterior surprisals, marginal data
                                                                                                       excluded.
  Figure 2, ignoring marginal data, shows two main effects
and an interaction, such that the prior effect is stronger at          Similarity of the human data to any of these four patterns,
higher likelihood.                                                   assessed by ANOVA, would indicate that inferences are
                                                                     close to Bayesian ideals, whether they are more or less so
                                                                  1919

with judgments of probability or surprisal, and whether the       probability size and type, and potential solution-carryover
marginal data are used or ignored in these computations.          across problems. All materials, including all four condition
   The frequency hypothesis was tested also, by presenting        variants of the three other content scenarios, are available
problem information with either frequencies or                    from the authors on request.
probabilities. There is evidence that people perform
somewhat better on explicit problems if the numerical             Design
information is presented in terms of frequencies, rather than     This was a mixed design with information format
probabilities (Chase, Hertwig, & Gigerenzer, 1998;                (probability vs. frequency) and question format (probability
Gigerenzer & Hoffrage, 1995; Gigerenzer & Todd, 1999).            vs. surprisal) as between-subject factors, and prior and
                                                                  likelihood information (each high vs. low) as within-subject
                           Method                                 factors. A Latin Square ensured that each of the four
                                                                  between-subject groups saw each of the four within-subject
Participants                                                      conditions with a different content. Another Latin Square
Usable data came from 333 participants, recruited from four       counterbalanced the order in which participants received the
Canadian university social-networking sites and tested            four problems, with each within-subject condition appearing
online: 170 females, 148 males, and 15 participants who did       equally in each of four positions. In anticipation of
not specify their gender. Some other participants had their       differential drop-out rates, there was an attempt to obtain
data excluded: 8 for doing numerical calculations, 31 for         approximately equal numbers of participants in each group
using Bayes’ rule, and 335 for not finishing the                  by assigning the next participant to the group with the
questionnaire.                                                    current lowest number of participants. The dependent
                                                                  variable was the posterior judgments of the participants.
Materials
                                                                  Procedure
The online experiment engine was Survey Monkey. There
were four Bayesian problems, each framed in four different        Once the participants clicked on the online ad, they were
versions, depending on what information was given and             directed to the Survey Monkey website where they agreed to
what was asked: given probabilities and asked probabilities,      a consent form, which described the experiment (answering
given frequencies and asked probabilities, given                  four inference problems testing rationality), how long it
probabilities and asked surprisals, given frequencies and         would take (5-10 minutes), and provided with a few
asked surprisals. The four problems differed in content: cab      constraints (no electronic calculators or pencils/paper to
problem, medical problem, pearl problem, and widget               make calculations). Participants were encouraged to use
problem.                                                          their intuitive judgments when answering the problems.
   A fully probabilistic version of Tversky and Kahneman’s           Each participant made a posterior judgment for every
(1982) cab problem read as follows: Two cab companies,            problem and then moved on to the next, without being given
Green and Blue, operate in the city. 85% of the cabs in the       any feedback. Following completion of the four problems,
city are green and 15% are blue. An unknown cab may have          there was a short list of questions to acquire information
been involved in an accident. A witness identified that cab       about the participants (age, gender, and math experience).
as green. Testing by the court under the same circumstances
existing on the night of the accident indicated that the                                    Results
witness correctly identified the color of cabs 85% of the
time and failed 15% of the time. What is the probability          Dropouts
(from 1-100) that the cab involved in the accident was green      As shown in Figure 4, subject loss varied with between-
rather than blue?                                                 subject condition, X2(3) = 28, p < .001. Dropouts were more
   A version with given frequencies and asked surprisals          frequent when probabilities were requested (.61) than when
went like this: Two cab companies, Green and Blue, operate        surprisals were requested (.43), X2(1) = 23, p < .001. And
in the city. 102 of the cabs in the city are green and 18 are     dropouts were more frequent when frequencies were given
Blue. An unknown cab may have been involved in an                 (.57) than when probabilities were given (.50), X2(1) = 4.11,
accident. A witness identified the cab as green. Testing by       p < .05.
the court under the same circumstances existing on the
night of the accident indicated that the witness correctly        Posteriors
identified the color of cabs 28 times and failed 5 times. How
                                                                  For each of the four between-subject conditions, posteriors
surprised would you be (on a scale of 1-9, 1 being not at all
                                                                  were subjected to a repeated-measures ANOVA with priors
surprised and 9 being extremely surprised) if the cab turned
                                                                  and likelihoods as the two within-subject factors. Patterns of
out to be green rather than blue?
                                                                  main and interactive effects can be compared to the
   Unlike many previous studies, the potency of the prior
                                                                  prediction patterns of Figures 1-4. Means and SEs for the
and likelihood differences were nearly equated. High and
                                                                  condition where probabilities were both given and asked for
low values, respectively, were 85 and 15, 84 and 16, 87 and
                                                                  are shown in Figure 6. There were main effects for both
13, or 86 and 14. This removed both confounds between
                                                              1920

prior, F(1, 75) = 24, p < .001, and likelihood, F(1, 75) =
120, p < .001, with no interaction, F(1, 75) = 2.78, p = .10,
thus making a good fit to the marginal-data-included pattern
                                                                                                                                           7
shown in Figure 1.
                                                                                                                                           6
                                                                                                                                           5
               Condition (given/asked)
                                                                                                                      Mean posterior
                                         probability/probability
                                                                                                                                           4
                                           probability/surprisal
                                                                                                                                           3
                                                                                                                                                                        Likelihood
                                          frequency/probability                                                                            2
                                                                                                                                                                               high
                                                                                                                                           1                                   low
                                               frequency/surprisal
                                                                                                                                           0
                                                                     0           0.2     0.4       0.6   0.8                                                high                      low
                                                                                                                                                                     Prior
                                                                                  Proportion dropout
                                                                                                                                           Figure 7: Mean posterior surprisals, with SEs, given
                                                   Figure 5: Proportions of subject loss.                                                                 probability input.
                                                                                                                                                        1
                                          1                                       Likelihood
                                                                                  high       low                                                0.75
                                                                                                                          Mean posterior
                          0.75                                                                                                                                                              Likelihood
  Mean posterior
                                                                                                                                                    0.5                                          high
                                         0.5
                                                                                                                                                                                                 low
                          0.25                                                                                                                  0.25
                                          0
                                                                                                                                                        0
                                                          high                         low
                                                                                                                                                              high                low
                                                                         Prior
                                                                                                                                                                     Prior
             Figure 6: Mean posterior probabilities, with SEs, given
                             probability input.                                                                               Figure 8: Mean posterior probabilities, with SEs, given
                                                                                                                                               frequency input.
   Means and SEs for the condition where probabilities were
given and surprisals were asked for are shown in Figure 7.
Again, there were main effects for both prior, F(1, 75) = 11,
p < .001, and likelihood, F(1, 75) = 85, p < .001, with no                                                                                          7
interaction, F(1, 75) = 2.68, p = .10, thus making a good fit
to the marginal-data-excluded pattern shown in Figure 4.                                                                                            6
  Means and SEs for the condition where frequencies were                                                                                            5
                                                                                                                                   Mean posterior
given and probabilities were asked for are shown in Figure
8. Again, there were main effects for both prior, F(1, 68) =                                                                                        4
16, p < .001, and likelihood, F(1, 68) = 92, p < .001, with no                                                                                      3
interaction, F(1, 68) = 0.68, p = .41, thus making a good fit                                                                                                           Likelihood
to the marginal-data-included pattern shown in Figure 1.                                                                                            2
                                                                                                                                                                               high
  Means and SEs for the condition where frequencies were                                                                                            1                          low
given and surprisals were asked for are shown in Figure 9.
In this case, there was only a main effect for likelihood, F(1,                                                                                     0
88) = 67, p < .001, with other Fs < 1, not fitting any of the                                                                                                high                       low
four Bayesian predictions.                                                                                                                                             Prior
                                                                                                                                           Figure 9: Mean posterior surprisals, with SEs, given
                                                                                                                                                           frequency input.
                                                                                                               1921

  The proportion of variance accounted for by each                                    of partial eta squared values (likelihood / prior) ranged from
significant main effect was computed for each of the three                            2.5 in the probability-given probability-asked condition, to
foregoing ANOVAs having two main effects. These partial                               3.0 in the frequency-given probability-asked condition, to
eta squared values are presented in Figure 10, revealing that                         4.5 in the probability-given surprisal-asked condition.
substantially more variance in posteriors was accounted for                              The notion that priors are relatively neglected, even when
by variation in likelihoods than by variation in priors.                              problem information is conveyed via frequencies is
                                                                                      consistent with results of previous studies (Gluck & Bower,
    probability/probability                                                           1988; Kahneman & Tversky, 1996; Slovic, Fischhoff, &
                                                                                      Lichtenstein, 1982; Tversky & Kahneman, 1973).
                                                                    Likelihood           Unlike laboratory studies, there were many dropouts from
       probability/surprisal                                        Prior             this online experiment and that may have contributed to
                                                                                      Bayesian conformity in some way, perhaps by eliminating
     frequency/probability                                                            those participants who did not have good intuitions about
                                                                                      how to solve probabilistic problems. The fact that the high
                             0    0.1   0.2     0.3      0.4    0.5    0.6     0.7    dropout rate varied with condition could thus be viewed as a
                                            Partial eta squared                       problem. However, the high dropout rate was also a blessing
                                                                                      as it documented significantly higher dropout rates for
   Figure 10: Proportion of variance accounted for by main                            responding with probabilities than with surprisals. This
                         effects in three conditions.                                 confirms the hypothesis that people are more comfortable
                                                                                      with judging their own surprise than with estimating
                                Discussion                                            probabilities. However, even though people seem to prefer
The results show that people conform to Bayesian                                      working with surprisals, surprisals do not aid conformity to
predictions by using both prior and likelihood information                            Bayes’ rule. Indeed, the opportunity to answer with
to update posteriors, with explicitly numerical problems.                             surprisals leads to greater neglect of both priors and
Somewhat surprisingly, people were closest to Bayes’ rule                             likelihoods. Surprisals are, in this sense, the junk food of
in the condition where they might be expected to do the                               probabilistic inference – preferred but unhealthy.
worst; given probability information and asked to provide                                The fact that people were more likely to drop out when
answers in probabilities. They were most deviant from                                 given frequency information than when given probability
Bayes’ rule in the condition where they might have been                               information is also interesting. Together with the finding
expected to do the best; given frequency information and                              that frequency information lessens conformity to Bayes’
asked to provide answers in surprisals. In that condition,                            rule, this dropout result is consistent with the idea that
they did use likelihood information appropriately, but they                           frequencies require additional processing (conversion to
showed no evidence of using prior probabilities at all.                               probabilities) in order to be useful in computation.
  Interestingly, giving frequency information did not help                               Surprisals are not much used in psychological research,
performance when probability inferences were required, but                            despite widespread psychological interest in manipulating
there was reliable evidence of appropriate use of both priors                         and measuring surprise. Bayesian researchers often measure
and likelihoods in that condition. The idea that frequency                            surprise at an event as 1 - p, where p is the probability
information does not help conformity to Bayes’ rule and                               estimate that the event will occur. Use of 1 - p did as well as
sometimes actually interferes is perhaps explained by noting                          surprisals, as long as the marginal data term was included in
that frequencies must be converted into some type of                                  the predictions. Neither surprisals nor 1 - p captured human
probabilistic code before inferences can be done with them.                           surprise judgments when the marginal data term was not
  Similarly, requesting surprisal responses did not help with                         included in the predictions. There is no evidence in the
probability information, but again there was evidence of                              present study that surprisals or 1 - p offer any advantage
appropriate use of both priors and likelihoods. All of this is                        over probabilities in terms of conformity to Bayes’ rule.
consistent with the view that people can be Bayesian. A                                  As for whether people compute anything like the marginal
surprise for some is that this can happen even in numerically                         data term, the present evidence is mixed. When participants
explicit problems, and that it happens most strongly with                             were asked for probability estimates, they showed only
probabilistic inputs and responses.                                                   main effects of priors and likelihoods, with no statistical
  Participants’ relative neglect of prior probabilities was                           interaction between them. This is a sign of using the
most evident in that the effect of prior information was                              marginal data term in some way. But when asked to use
considerably smaller than the effect of likelihood                                    surprisals, given probability information, participants
information, when both were used. In this respect, the data                           likewise showed only main effects of priors and likelihoods
are also consistent with the view that people tend to ignore                          without interaction. This is a sign of ignoring the marginal
prior probabilities. This tendency for base rates (priors) to                         data term. These findings are somewhat puzzling because
be used, but less so than likelihoods, has been noted in                              using the marginal data involves relatively complicated
previous data (Bar-Hillel, 1983; Kahneman & Tversky,                                  division when producing probabilities, and relatively simple
1996), but usually not quantified as precisely as here. Ratios                        subtraction when producing surprisals. This may suggest
                                                                                  1922

that inference under uncertainty is actually done with quite            Gigerenzer, G., & Hoffrage, U. (1995). How to improve
different mechanisms than Bayes’ rule.                                       Bayesian reasoning without instruction: Frequency
  One simpler alternative for computing the posterior                        formats. Psychological Review, 102, 684-704.
probability is to average prior and likelihood probabilities            Gigerenzer, G., & Todd, P. M. (1999). Simple heuristics
(McKenzie, 1994). Another is to weight and add the prior                     that make us smart. New York: Oxford University Press.
and likelihood probabilities (Juslin, Nilsson, & Winman,                Gluck, M. A., & Bower, G. (1988). From conditioning to
2009). For example, weighting the prior probability lower                    category learning: An adaptive network model. Journal
than the likelihood probability could simulate base-rate                     of Experimental Psychology: General, 117, 227-247.
neglect. Although such weight-and-add models do not                     Itti, L., & Baldi, P. (2006). Bayesian surprise attracts human
specify how various probabilities should be weighted, the                    attention. In Y. Weiss, B. Schölkopf & J. Platt (Eds.),
present results suggest that something like a 1:3 ratio for                  Advances in Neural Information Processing Systems
priors and likelihoods, respectively, could simulate human                   (Vol. 18 ).
results. Like surprisals, both averaging and adding weighted            Juslin, P., Nilsson, H., & Winman, A. (2009). Probability
probabilities can avoid more complex multiplication and                      theory, not the very guide of life. Psychological Review,
division operations. Neural networks are another possible                    116(4), 856-874.
contender for simulating human judgments because of their               Kahneman, D., Slovic, P., & Tversky, A. (1982). Judgment
potential to learn appropriate weights in a brain-like fashion               under uncertainty: Heuristics and biases. Cambridge:
(Shultz, 2007).                                                              Cambridge University Press.
  A Bayesian scheme to measure surprise as the log to the               Kahneman, D., & Tversky, A. (1996). On the reality of
base 2 of the ratio of posterior to prior probabilities (Itti &              cognitive illusions. Psychological Review, 103, 582-591.
Baldi, 2006) was also tried, but made a particularly bad fit            Körding, K. P., & Wolpert, D. M. (2006). Bayesian decision
to present data, whether or not the marginal data term was                   theory in sensorimotor control. Trends in Cognitive
included in the predictions.                                                 Sciences, 10, 319-326.
  Returning to the opening issue, there is evidence here for            McKenzie, C. R. M. (1994). The accuracy of intuitive
both Bayesian rationality and deviations from such                           judgments: Covariation assessment and Bayesian
rationality. Even with explicit judgments, people employ                     inference. Cognitive Psychology, 26, 209-239.
both prior and likelihood information in estimating                     Shultz, T. R. (2007). The Bayesian revolution approaches
posteriors. It is just that their use of priors is not as strong as          psychological development. Developmental Science, 10,
their use of likelihoods. Base-rate neglect is virtually                     357-364.
complete when people are presented with frequencies and                 Slovic, P., Fischhoff, G., & Lichtenstein, S. (1982). Facts
asked for surprisals.                                                        versus fears: Understanding perceived risk. In D.
  Several of these results are, well, surprising. Obviously,                 Kahneman, P. Slovic & A. Tversky (Eds.), Judgment
there is plenty of scope for future research to illuminate this              under uncertainly: Heuristics and biases (pp. 463-489).
range of issues.                                                             Cambridge: Cambridge University Press.
                                                                        Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006).
                     Acknowledgments                                         Theory-based Bayesian models of inductive learning and
This work is supported by a grant from the Natural Sciences                  reasoning. Trends in Cognitive Sciences, 10, 309-318.
and Engineering Research Council of Canada. James                       Tversky, A., & Kahneman, D. (1973). Availability: A
Ramsay provided some inspirational ideas about surprisals.                   heuristic for judging frequency and probability.
Artem Kaznatcheev, Kyler Brown, and Frédéric Dandurand                       Cognitive Psychology, 5, 207-232.
contributed thoughtful comments on an earlier draft.                    Tversky, A., & Kahneman, D. (1974). Judgment under
                                                                             uncertainty: Heuristics and biases. Science, 185, 1124-
                                                                             1131.
                           References                                   Tversky, A., & Kahneman, D. (1981). The framing of
Bar-Hillel, M. (1983). The base rate fallacy controversy. In                 decisions and the psychology of choice. Science, 211,
    R. W. Scholz (Ed.), Decision making under uncertainty                    453-458.
    (pp. 39-61). Amsterdam: North-Holland.                              Tversky, A., & Kahneman, D. (1982). Evidential impact of
Chase, V. M., Hertwig, R., & Gigerenzer, G. (1998).                          base rates. In D. Kahneman, P. Slovic & A. Tversky
    Visions of rationality. Trends in Cognitive Sciences, 2,                 (Eds.), Judgment under uncertainty: Heuristics and
    206-214.                                                                 Biases (pp. 153-162). Cambridge: Cambridge University
Chater, N., & Manning, C. D. (2006). Probabilistic models                    Press.
    of language processing and acquisition. Trends in                   Yuille, A., & Kersten, D. (2006). Vision as Bayesian
    Cognitive Sciences, 10, 335-344.                                         inference: analysis by synthesis? Trends in Cognitive
Courville, A. C., Daw, N. D., & Touretzky, D. S. (2006).                     Sciences, 10, 301-308.
    Bayesian theories of conditioning in a changing world.
    Trends in Cognitive Sciences, 10, 294-300.
Cover, T. M., & Thomas, J. A. (1991). Elements of
    information theory. New York: Wiley-Interscience.
                                                                    1923

