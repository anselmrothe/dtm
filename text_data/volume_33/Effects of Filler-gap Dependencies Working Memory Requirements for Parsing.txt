UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Effects of Filler-gap Dependencies Working Memory Requirements for Parsing
Permalink
https://escholarship.org/uc/item/38j9d3k8
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Author
Schuler, William
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

Effects of Filler-gap Dependencies on Working Memory Requirements for Parsing
                                               William Schuler (schuler@ling.osu.edu)
                                               Department of Linguistics, 1712 Neil Avenue,
                                                         Columbus, OH 43210 USA
                              Abstract                                                          S                          ¾
                                                                                                                             S/VP
   Corpus studies by Schuler, AbdelRahman, Miller, and                          NP                             VP
   Schwartz (2010), appear to support a model of comprehen-
   sion taking place in a general-purpose working memory store,          DT           NN                VP
                                                                                                                           
                                                                                                                           
   by providing an existence proof that a simple probabilis-                                                               
                                                                                                                           
   tic sequence model over stores of up to four syntactically-           The       president     VB             NP           VP/NN
   contiguous memory elements has the capacity to reconstruct                                                              
                                                                                                                           
                                                                                                meets      DT       NN
                                                                                                                           
   phrase structure trees for over 99.9% of the sentences in the
   Penn Treebank Wall Street Journal corpus (Marcus, Santorini,
   & Marcinkiewicz, 1993), in line with capacity estimates for                                             the
   general-purpose working memory, e.g. by Cowan (2001). But
   capacity predictions of this simple structure-based model ig-        Figure 1: Incomplete constituents in an incremental parse
   nore non-structural dependencies, such as long-distance filler-      of the sentence ‘The president meets the board on Friday,’
   gap dependencies, that may place additional demands on work-
   ing memory. Distinguishing unattached gap fillers from open          showing non-immediate dominance between incomplete con-
   attachment sites in syntactically-contiguous memory elements         stituents ‘The president ...’ and ‘meets the ...’.
   requires this contiguity constraint to be strengthened to a con-
   straint that working memory elements be semantically con-
   tiguous. This paper presents corpus results showing that this
   stricter semantic contiguity constraint still predicts working
   memory requirements in line with capacity estimates such as
   that of Cowan (2001).
                                                                        parsing is only part of comprehension. In order to obtain valid
   Keywords:
                                                                        interpretations, a comprehension model must also retain non-
                                                                        structural information like gap fillers in long-distance depen-
                          Introduction
                                                                        dency constructions. Filler-gap dependencies are common
It is tempting to think of sentence comprehension as learned            (occurring in about 20% of sentences in the Wall Street Jour-
manipulations of elements in a general-purpose working                  nal corpus), and figure prominently in the psycholinguistics
memory store. This assumption underlies many established                literature on memory bounds in parsing (Gibson, 1991; Just
comprehension models (e.g.              Johnson-Laird, 1983; El-        & Carpenter, 1992, etc.). But since the introduction of a gap
man, 1991; Gibson, 1991; Just & Carpenter, 1992; Gibson,                filler adds no unsatisfied structural attachment sites (and thus
1998; Lewis & Vasishth, 2005, making various assumptions                no need to retain additional memory elements), they are ig-
about the size and nature of this memory). Corpus stud-                 nored in the capacity predictions of the simple syntax-based
ies by Schuler et al. (2010), appear to support this hypoth-            model described above. Distinguishing unattached gap fillers
esis by providing an existence proof that a simple probabilis-          from open attachment sites in syntactically-contiguous mem-
tic sequence model over stores of up to four syntactically-             ory elements requires the syntactic contiguity constraint of
contiguous memory elements — simple random variables                    Schuler et al. (2010) to be strengthened to a constraint that
with discrete domains over pairs of constituent categories              working memory elements be semantically contiguous (that
at which other structures may attach — has the capacity                 is, linked by roles, in the sense of Gibson, 1991). This
to reconstruct phrase structure trees for over 99.9% of the             would force gap fillers into separate memory elements, re-
sentences in the Penn Treebank Wall Street Journal corpus               quiring additional memory resources to process sentences
(Marcus et al., 1993). This is in line with capacity estimates          in which filler-gap dependencies co-occur with structurally-
for general-purpose working memory, e.g. by Cowan (2001),               nested constituents.
but is also compatible with a continuously degrading avail-                 This paper presents coverage results on the same Wall
ability (McElree, 2001), since the model’s use of this store            Street Journal corpus showing that, despite their prevalence,
degrades very rapidly after one element. To the extent that             long-distance dependencies do not seem to occur in deeply
the Wall Street Journal is comprehensible (and its editors are          structurally-embedded contexts, and this stricter semantic
doing their jobs) this suggests that comprehension can take             contiguity constraint still predicts working memory require-
place in a general-purpose working memory store, using a                ments in line with capacity estimates such as that of Cowan
small set of incomplete constituent states derived from phrase          (2001). This seems to support the hypothesis that compre-
structure.                                                              hension may take place in a general-purpose working mem-
   The structural attachment sites in the memory elements of            ory store, using a simple notion of semantically-contiguous
Schuler et al. (2010) are necessary in order to accurately re-          incomplete constituents as memory elements.
construct syntactic relations in phrase structure trees. But
                                                                    501

     a)                       S                                        b)                                                     S
             NP                                 VP                                                               S/NP                       NP
                                                                                                  S/PP                           IN       Friday
        DT       NN               VP                      PP                     S/VP                              VP            on
       The    president     VB            NP         IN       NP                  NP                    VP/NN             NN
                          meets       DT       NN     on    Friday      NP/NN           NN         VP/NP         DT      board
                                                                           DT        president       VB          the
                                      the    board                         The                      meets
     c) t=0              t=1               t=2               t=3                  t=4                  t=5                 t=6
                 f11 =DT           f21 =NP          f31                f41                    f51                  f61                  f71 =S
         q10                1
                          q1 =NP/NN           1
                                            q2 =S/VP            1
                                                              q3 =S/VP              1
                                                                                   q4 =S/VP               1
                                                                                                        q5 =S/PP             1
                                                                                                                            q6 =S/NP
                 f12                 2
                                   f2 =NN             2
                                                    f3 =VB             f42                      2
                                                                                              f5 =VP                 2
                                                                                                                   f6 =IN               f72 =NP
         q20              q12               q22                 2
                                                              q3 =VP/NP             2
                                                                                   q4 =VP/NN            q52                  2
                                                                                                                            q6
                 f13               f23              f33                f43 =DT                f53 =NN              f63                  f73
         q30              q31               q32               q33                   3
                                                                                   q4                   q53                  3
                                                                                                                            q6
         x0 =The          x1 =president     x2 =meets         x3 =the              x4 =board            x5 = on             x6 =Friday
Figure 2: Phrase structure tree for the sentence ‘The president meets the board on Friday’ (a), transformed into right-corner
form (b), then mapped (in dark gray) onto a random variables in a factored sequence model (c) with three hidden levels. Circles
denote random variables (over incomplete constituents qtd and complete constituents ftd at each nesting depth d and time step t),
and edges denote conditional dependencies. Shaded circles denote variables with observed values (words in this case).
                        Background                                    structure trees (Figure 2a) with memory-minimizing trans-
Schuler et al. (2010) calculate a first approximation of              formed representations (Figure 2b). This is done by asso-
the working memory capacity required to parse the large               ciating every top-down sequence of right children between
syntactically-annotated Penn Treebank Wall Street Journal             some left child1 and its rightmost leaf (say, from the root
and Switchboard corpora, based on what was intended to be a           S to the NP ‘Friday’ in Figure 2a) with a bottom-up se-
strict requirement that only completely contiguous syntactic          quence of incomplete constituents, each having the original
structures could occupy a single working memory element. In           left child as its active component and one of the original right
particular, each syntactically contiguous chunk is constrained        children as its awaited component (producing the sequence
to the form of an incomplete constituent state A/B, consisting        S/VP, S/PP, S/NP in Figure 2b). This representation converts
of a single active but unfinished constituent A lacking a sin-        right-expanding sequences of complete constituents into left-
gle awaited constituent B yet to be attached, somewhere in the        expanding sequences of incomplete constituents, leaving only
right progeny of the active constituent. Syntactic relations be-      center-expanding sequences (alternating expansions of left
tween these incomplete constituent chunks are underspecified          and right children) to require additional memory resources
as non-immediate dominance relations between the awaited              in a bottom-up time-order traversal.
and active components of successive incomplete constituents              This memory-minimizing representation can then be
(see Figure 1). This can be thought of as a highly-constrained        mapped to random variables in a sequence model (Figure 2c),
version of the non-immediate dominance relations in Tree              with incomplete constituents mapped to store state vari-
Adjoining Grammar (Joshi, 1985) or Description Tree Gram-             ables qtd and complete constituents mapped to final state vari-
mar (Rambow, Weir, & Vijay-Shanker, 1995) in processing               ables ftd . Connections among these variables define proba-
models proposed by Stabler (1994) and Mazzei, Lombardo,               bilities for partial utterances, in which values are hypothe-
and Sturt (2007), except that here, all syntactic information         sized for each random variable with probability conditioned
other than the categories of active and awaited constituents at       on only its adjacent antecedent variables (those connected by
the frontier of an incomplete constituent is discarded.               outgoing arcs).2 Each time step in the model (correspond-
   This austere definition still allows the complete specifi-         ing to columns in Figure 2c) defines a set of incomplete con-
cation of phrase structure trees from stores of incomplete            stituents recognised thus far. For example, the store qt1.D at
constituents arranged in time order (see Figure 2). This                   1 For the purpose of this definition, the root of a tree is considered
correspondence can be defined through a reversible right-             to be a left child (e.g. of a right-branching supra-sentential discourse
corner transform (Schuler et al., 2010), a variant of the             structure).
                                                                           2 The probability of a partial utterance at any time step t, subsum-
left-corner transform of Johnson (1998), associating phrase
                                                                      ing a store state qt1..D , and the set of observed words x1..t to that time
                                                                  502

 a)                        NP                                                               b)                 NP
        NP                                RC                                                       NP                      RC
   DT           NN           WHNP-1                    S
                                                                                              DT        NN       WHNP              S-gNP
   the        person           who        NP                       VP
                                                                                              the     person      who       NP              VP-g
                                       officials        VB                    S
                                                        say        NP-1             VP                                   officials    VB           VP
                                                                     ε        VB         NP                                           say     VB       NP
                                                                             stole     millions                                              stole   millions
Figure 3: Mapping from the movement-based notation in the Penn Treebank (a) to a purely binary-branching structure (b) on
which the right-corner transform is defined.
t=4 corresponds to the set of incomplete constituents in Fig-                              Bachrach, Cardenas, & Schuler, 2010).
ure 1 prior to encountering the word ‘board’.
    Figure 2c shows a particular instantiation of random vari-                                                     Revised Model
ables corresponding to one hypothesized analysis of the ex-                                The active and awaited components of incomplete con-
ample sentence, ‘The president meets the board on Friday.’                                 stituents retained in this model are the only sites at which
In processing, several such analyses are maintained in par-                                subsequently recognised phrase structure will attach, either
allel at each time step, competing with one another prob-                                  above or below the incomplete constituent. But in order to ex-
abilistically in subsequent transitions. The parallelism in a                              tract propositional content from an utterance or read sentence,
probabilistic sequential process such as this one can be main-                             a comprehension model must also retain semantic referents of
tained using distributed, independent computations of prob-                                unattached gap fillers, which are not necessarily manifested
ability mass in a particle filter (Gordon, Salmond, & Smith,                               as structural attachment sites. This retention can be accom-
1993), which may resemble distributed processing in human                                  plished by redefining incomplete constituents to be semanti-
cognition (Levy, 2008b).                                                                   cally contiguous as well as syntactically contiguous.
    As a probabilistic sequence model, this model is recurrent                                Following Schuler et al. (2010) and Lewis and Vasishth
(in that it is stationary, using the same model at each time                               (2005), the model described in this paper will assume a
step) and connectionist (in that it is defined entirely in terms                           purely binary-branching phrase structure with no empty con-
of interconnected nodes). But unlike other recurrent connec-                               stituents, in order to simplify the definition of the compre-
tionist models, which are typically specified at the level of                              hension process. This can be generated from trace-annotated
excitatory and inhibitory relations between individual neu-                                corpora (see Figure 3a) by eliminating each empty constituent
ral elements, probabilistic sequence models may be speci-                                  (e.g. NP-1), and the constituent attaching it to the non-empty
fied directly over linguistic states (in this case over recog-                             portion of the tree (the S dominating NP-1), then propagating
nized incomplete constituents at successive time steps). Syn-                              the trace index up the tree from this gap position to the cor-
tactic probabilities in a sequence model transformed from a                                responding filler position (WHNP-1), attaching a ‘-g’ tag to
probabilistic context free grammar (PCFG) can be defined                                   each traversed constituent category indicating that constituent
to generate the same tree and sentence probabilities as the                                contains a gap but no corresponding filler (see Figure 3b). At
original PCFG (Schuler, 2009). These probabilities are pro-                                the topmost gapped constituent in this traversal, the category
posed to have a significant role in processing (Jurafsky, 1996;                            of the filler is added to the tag (producing S-gNP), indicating
Hale, 2001; Levy, 2008a, among others), and probabilities                                  the constituent at which the need for a gap is introduced in a
from partial sequences in this model have been shown to cor-                               time-order traversal.
relate with reading time delays in self-paced reading (Wu,                                    In a complete comprehension model, it would be desirable
                                                                                           to allow semantic dependencies from referents of fillers to
step, is:
                                                                                           referents of constituents containing gaps to be expressed in-
                     def                                                                   teractively, so fillers could statistically influence subsequent
    P(qt1..D x1..t ) = ∑ P(qt−1  1..D
                                      x1..t−1 ) · P(xt | qt1..D )                          parsing decisions. This can be done through the conditional
                        q1..D                                                
                          t−1
                 D                                                                         dependencies among random variables in the sequence model
                                        d−1
     · ∑                          d
                ∏ P( ftd | ftd+1 qt−1 qt−1  ) · P(qtd | ftd+1 ftd qt−1
                                                                   d
                                                                       qtd−1 )    (1)     defined in the previous section. But, since fillers and gaps
         ft1..D d=1
                                                                                           are not generally adjacent in phrase structure trees, they are
in which the probability terms within brackets have conditional de-                        not guaranteed to be adjacent in a right-corner transformed
pendencies defined by the network in Figure 2c, and the probability                        tree. For example, the right-corner transform defined in
terms outside the brackets are recursive store state probabilities and                     the previous section would transform the simple top-down
evidence probabilities of a Hidden Markov Model (Rabiner, 1990).
                                                                                           right-branching sequence of NP, RC, S-gNP, VP-g, etc. in
                                                                                       503

    a)                  NP                                             b)                                               NP
           NP                      RC                                                                      NP/NP                     NP
                                                                                               NP/VP                     VB      millions
      DT       NN        WHNP              S-gNP
                                                                                NP/S-gNP                 S-gNP/VP       stole
      the     person      who       NP             VP-g
                                                                             NP/RC       VB      S-gNP/VP-g        VB
                                 officials    VB           VP                 NP         who          NP           say
                                              say    VB        NP        DT        NN              officials
                                                    stole    millions    the     person
      c) t=0               t=1                t=2             t=3                t=4                t=5                t=6
                   f11 =DT            f21 =NP          f31              f41                f51                 f61               f71 =NP
          q10                 1
                            q1 =NP/NN            1
                                               q2 =NP/RC         1                  1
                                                               q3 =NP/S-gNP q4 =NP/S-gNP q5 =NP/VP     1                  1
                                                                                                                        q6 =NP/NP
                   f12                f22 =NN          f32 =WHNP        f42 =NP            f52 =/VP            f62 =VB           f72 =NP
          q20               q21                q22             q23                q24 =S-g/VP-g      q25                q26
                   f13                f23              f33              f43                f53 =VB             f63               f73
          q30               q31                q32             q33                q34                q35                q36
          x0 =the           x1 = person        x2 =who         x3 =officials      x4 = say           x5 =stole          x6 =millions
                 Figure 4: Sample store sequence containing long-distance dependency in a filler-gap construction.
Figure 3b into a simple bottom-up left-branching sequence              then resumes constructing incomplete constituents from the
NP/RC, NP/S-gNP, NP/VP-g, etc. This would leave the                    bottom up in time order, as described in the previous section.
random variables associated with the filler and gapped con-            This transformed representation can then be mapped to ran-
stituent separated by two time steps in the sequence model.            dom variables in a sequence model in Figure 4c as described
Moreover, some of the resulting incomplete constituents (e.g.          in the previous section, except that the redundant active com-
the NP/VP-g dominating ‘the person who officials’) would               ponent of the final state at the end of the gap sub-structure
have no fixed semantic dependency between referents of                 (the S-gNP at f52 ) is elided, in order to preserve the model
their active and awaited components (between the person and            definition and topology.
whatever the officials are going to do), violating an assump-             Implicit in this analysis of fillers is the assumption that
tion that memory elements contain coherent or contiguous               each component of an incomplete constituent has a single as-
chunks.                                                                sociated referent. In the pure binary-branching phrase struc-
   The right-corner transform is therefore defined to explicitly       ture tree shown in Figure 4a, the constituent at which the
retain the filler as the awaited component of an incomplete            gap is introduced (S-gNP) does not have a single referent.
constituent, now both syntactically and semantically contigu-          But when this constituent forms the split point between two
ous. It does this by halting on this initial gap (see Figure 4a, a     incomplete constituents (NP/S-gNP and S-gNP/VP in Fig-
copy of Figure 3b), treating the initial gap constituent as a left     ure 4b), the awaited component of the upper incomplete con-
child, then transforming the sequence of right children below          stituent may take on the referent of the filler (the referent of
it into a left-expanding sequence of incomplete constituents           the ‘NP’ in S-gNP, described by ‘person’ in this example)
as a sub-structure (the sub-structure subsuming ‘officials say’        and the active component of the lower incomplete constituent
in Figure 4b). At the last constituent with a gap tag (VP-g in         may take on the referent of the original phrase structure con-
the figure), the transform halts again and terminates this sub-        stituent (the referent of the ‘S’, described by ‘say’ in this
structure with an incomplete constituent (S-gNP/VP in the              example). This allows the filler to be made available when
figure) consisting of the initial gap constituent as the active        the gap is encountered, while also potentially allowing sub-
component and the right child of the current constituent as            sequent structures to attach above S-gNP/VP to modify the
the awaited component. Finally, this incomplete constituent            ‘say’ event.
is connected to the incomplete constituent preceding it (NP/S-
gNP subsuming ‘the person who’) by attaching both as chil-                                          Results
dren of another incomplete constituent (NP/VP in the figure),          This structural analysis of fillers as awaited components of
consisting of the active component of this previous incom-             new incomplete constituents allows them to be explicitly re-
plete constituent and the awaited component of the incom-              tained and associated with gap constituents in comprehen-
plete constituent dominating the sub-structure. The transform          sion, despite the fact that they are not semantically related
                                                                   504

          a) memory load             words    coverage                      500000
             0 store elements       39,882      4.76%
             1 store element      465,977      60.25%                       450000
             2 store elements     290,550      94.85%                       400000
             3 store elements       41,587     99.79%
             4 store elements        1,745     99.99%                       350000
             5 store elements           24    100.00%
             TOTAL                839,765     100.00%                       300000
          b) memory load             words    coverage                      250000
             0 store elements       39,882      4.76%                       200000
             1 store element      464,044      60.01%
             2 store elements     291,426      94.71%                       150000
             3 store elements       42,454     99.77%
             4 store elements        1,934     99.99%                       100000
             5 store elements           25    100.00%
             TOTAL                839,765     100.00%                        50000
          c) memory load             words    coverage                           0
                                                                                   0  1      2    3      4     5     6    7     8     9
             0 store elements       39,882      4.76%
             1 store element      360,806      47.82%
             2 store elements     264,265      79.36%                   Figure 5: Number of words at each nesting depth for at-
             3 store elements     112,936      92.84%                   tested corpus (solid line) and randomly sampled corpus (dot-
             4 store elements       40,090     97.62%
             5 store elements       13,455     99.23%                   ted line).
             6 store elements        4,440     99.76%
             7 store elements        1,400     99.92%
             8 store elements          499     99.98%
             9 store elements          109     99.99%
             TOTAL                837,882     100.00%                      The study was then repeated using the modified right-
                                                                        corner transform described in the previous section, to allow
Table 1: Percent coverage of right-corner transformed Wall              the model to explicitly retain gap fillers as awaited compo-
Street Journal Treebank sections 2–21, using original trans-            nents of incomplete constituents. Results are shown in Ta-
form (a), and revised transform (b), and corpus randomly                ble 1b. Although results show increased requirements at ca-
sampled from PCFG, using revised transform (c).                         pacity 3 and 4, there is no substantial increase at capacity
                                                                        5 or beyond, suggesting that long-distance filler-gap depen-
                                                                        dencies do not occur in memory-taxing portions of syntactic
                                                                        structure.
to the intervening words. But the addition of new incom-                   In order to determine whether these nesting limits arise
plete constituents to hold fillers places additional demands            from purely syntactic statistical tendencies (e.g. toward right-
on working memory. If filler-gap dependencies regularly co-             side branching in English) or from bounded-memory effects,
occur with ordinary structural nesting, the capacity require-           the above results were compared to memory load results for a
ments of this model may become incompatible with indepen-               corpus of phrase structure trees that were randomly generated
dent estimates of general-purpose working memory capacity.              from the PCFG estimated from the relative frequency of each
In order to determine whether this model is still plausible,            branch in the binarized Treebank corpus, with no sensitivity
its capacity requirements were evaluated on the Wall Street             to nesting depth (shown in Table 1c and Figure 5). The ca-
Journal corpus (Marcus et al., 1993).                                   pacity requirements of the randomly generated corpus form a
   First, the Schuler et al. (2010) study was replicated as a           relatively mild peak at d=1 and exhibit a gradual exponential
baseline. Sections 2–21 of the Penn Treebank Wall Street                decay at each higher capacity requirement (the dashed line in
Journal corpus were binarized, right-corner transformed, and            Figure 5). This is to be expected form a pure PCFG model,
mapped to elements in a bounded memory store as described               since additional nestings at each level are generated with the
in Schuler et al. (2010). Coverage of this corpus, in words that        same probability. In contrast, the capacity requirements for
can be processed by a recognizer using one to five memory el-           the actual sentences in the Wall Street Journal corpus us-
ements, is shown in Table 1a. These results show that a sim-            ing the revised model of filler-gap dependencies described
ple syntax-based chunking into incomplete constituents, us-             in this paper (solid lines) peak much more sharply at d=1,
ing the right-corner transform defined in Schuler et al. (2010),        and then fall off much more rapidly. The difference between
allows a vast majority of words in the Wall Street Journal cor-         these distributions is statistically significant over sentences
pus (over 99.7%) to be recognized using three or fewer ele-             (p < .01) using a two-tailed t-test at all depths below d=5,
ments of memory, with no sentences requiring more than five             for which non-zero counts exist in both corpora, except for
elements, as predicted by a general-purpose working memory              the crossover point at d=2 which is not significant. This sug-
model.3                                                                 gests that bounded-memory effects play a significant role in
    3 This measure is more fine-grained than Schuler et al. (2010),
                                                                        syntactic structure beyond what can be explained by syntac-
which counted only the maximum depth at each sentence.                  tic preferences in a PCFG, even when accounting for memory
                                                                    505

required to connect filler-gap dependencies. This may argue          Johnson-Laird, P. N. (1983). Mental models: towards a cog-
in favor of the use of factored sequence models in place of            nitive science of language, inference, and consciousness.
pure PCFG models as a source of incremental probability es-            Cambridge, MA, USA: Harvard University Press.
timates in modeling comprehension.                                   Joshi, A. K. (1985). How much context sensitivity is nec-
                                                                       essary for characterizing structural descriptions: Tree ad-
                          Discussion                                   joining grammars. In L. K. D. Dowty & A. Zwicky (Eds.),
This paper has presented predicted capacity requirements               Natural language parsing: Psychological, computational
for incremental parsing with a stricter condition for seman-           and theoretical perspectives (pp. 206–250). Cambridge,
tically contiguous memory elements showing that the re-                U.K.: Cambridge University Press.
quired working memory capacity does not significantly in-            Jurafsky, D. (1996). A probabilistic model of lexical and
crease when a more sensitive semantic contiguity constraint            syntactic access and disambiguation. Cognitive Science: A
is introduced. The model is similar to Lewis and Vasishth              Multidisciplinary Journal, 20(2), 137–194.
(2005), except that the focus is on the estimation model rather      Just, M. A., & Carpenter, P. A. (1992). A capacity theory of
than the time course.                                                  comprehension: Individual differences in working mem-
   This result covers filler-gap dependencies, but may still ig-       ory. Psychological Review, 99, 122-149.
nore other types of semantic dependencies that cause discon-         Levy, R. (2008a). Expectation-based syntactic comprehen-
tiguities. One source of such discontiguities may be quantifier        sion. Cognition, 106(3), 1126–1177.
scope raising. The definition of chunks in the right-corner          Levy, R. (2008b). Modeling the effects of memory on human
sequence model described here can be readily extended to               online sentence processing with particle filters. In Proceed-
incremental calculation of quantifier scope by allowing se-            ings of NIPS (pp. 937–944). Vancouver, BC, Canada.
mantic structures for quantifiers and restrictors (the seman-        Lewis, R. L., & Vasishth, S. (2005). An activation-based
tics of noun phrases) to attach to exposed active constituents         model of sentence processing as skilled memory retrieval.
less deeply nested in an analysis, while the syntax transi-            Cognitive Science, 29(3), 375–419.
tions the syntactic state of the lowest incomplete constituent.      Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993).
Like filler-gap dependencies, this might predict additional re-        Building a large annotated corpus of English: the Penn
quired memory capacity to maintain semantically contigu-               Treebank. Computational Linguistics, 19(2), 313–330.
ous, monotonically growing chunks. But the effect on pre-            Mazzei, A., Lombardo, V., & Sturt, P. (2007). Dynamic
dicted capacity requirements must await corpora annotated              tag and lexical dependencies. Research on Language and
with quantifier scope.                                                 Computation, 5, 309–332.
                                                                     McElree, B. (2001). Working memory and focal attention.
                          References                                   Journal of Experimental Psychology, Leanrning Memory
Cowan, N. (2001). The magical number 4 in short-term mem-              and Cognition, 27(3), 817–835.
   ory: A reconsideration of mental storage capacity. Behav-         Rabiner, L. R. (1990). A tutorial on hidden Markov models
   ioral and Brain Sciences, 24, 87–185.                               and selected applications in speech recognition. Readings
Elman, J. L. (1991). Distributed representations, simple               in speech recognition, 53(3), 267–296.
   recurrent networks, and grammatical structure. Machine            Rambow, O., Weir, D., & Vijay-Shanker, K. (1995). D-tree
   Learning, 7, 195–225.                                               grammars. In Proceedings of the 33rd annual meeting of
Gibson, E. (1991). A computational theory of human linguis-            the association for computational linguistics (ACL’95) (pp.
   tic processing: Memory limitations and processing break-            151–158).
   down. Unpublished doctoral dissertation, Carnegie Mel-            Schuler, W. (2009). Parsing with a bounded stack using
   lon.                                                                a model-based right-corner transform. In Proceedings of
Gibson, E. (1998). Linguistic complexity: Locality of syn-             naacl (pp. 344–352). Boulder, Colorado.
   tactic dependencies. Cognition, 68(1), 1–76.                      Schuler, W., AbdelRahman, S., Miller, T., & Schwartz, L.
Gordon, N. J., Salmond, D. J., & Smith, A. F. M. (1993).               (2010). Broad-coverage incremental parsing using human-
   Novel approach to nonlinear/non-gaussian bayesian state             like memory constraints. Computational Linguistics, 36(1),
   estimation. IEE Proceedings F (Radar and Signal Process-            1–30.
   ing), 140(2), 107–113.                                            Stabler, E. (1994). The finite connectivity of linguistic struc-
Hale, J. (2001). A probabilistic earley parser as a psycholin-         ture. In Perspectives on sentence processing (pp. 303–336).
   guistic model. In Proceedings of the second meeting of              Lawrence Erlbaum.
   the north american chapter of the association for computa-        Wu, S., Bachrach, A., Cardenas, C., & Schuler, W. (2010).
   tional linguistics (pp. 159–166). Pittsburgh, PA.                   Complexity metrics in an incremental right-corner parser.
Johnson, M. (1998). Finite state approximation of constraint-          In Proceedings of the 48th annual meeting of the associ-
   based grammars using left-corner grammar transforms. In             ation for computational linguistics (ACL’10) (pp. 1189–
   Proceedings of COLING/ACL (pp. 619–623). Montreal,                  1198).
   Canada.
                                                                 506

