UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Artificial Grammar Investigation into the Mental Encoding of Syntactic Structure
Permalink
https://escholarship.org/uc/item/4868w88d
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Whan Cho, Pyeong
Szkudlarek, Emily
Kukona, Anuenue
et al.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                                         An Artificial Grammar Investigation
                                 into the Mental Encoding of Syntactic Structure
                                          Pyeong Whan Cho (pyeong.cho@uconn.edu)1,2
                                       Emily Szkudlarek (emily.szkudlarek@uconn.edu)1
                                         Anuenue Kukona (anue.kukona@uconn.edu)1,2
                                          Whitney Tabor (whitney.tabor@uconn.edu)1,2
                      1
                        Department of Psychology and Cognitive Science Program, University of Connecticut,
                                           406 Babbidge Road U-1020, Storrs, CT 06269 USA
                              2
                                Haskins Laboratories, 300 George St., Suite 900, New Haven, CT 06511 USA
                                Abstract                                right hand side of a starting rule until no more substitutions
   We explore neural network learning and parallel human
                                                                        can be made; the resulting right hand side is the generated
   learning on an artificial language task. The task generates rich     sentence. Grammar G generates the sentences “1 2 3 4” (a
   data on human interaction with syntactic systems, including          Level 1 sentence), “1 1 2 3 4 2 3 4” (Level 2), “1 1 1 2 3 4 2
   recursive ones. Studying the network’s properties, we argue for      3 4 2 3 4” (Level 3), etc. In formal language terminology, a
   a “Structured Manifold” view of syntactic representation. The        case where the system shifts to a deeper level of embedding
   “Structured Manifold” lies in the parameter space (weight            (here, 1 after 1)—is called a “push” and a case where it
   space) of the network. It exhibits (1) loci of high order,
                                                                        shifts back (2 after 4) is called a “pop”. Keeping track of the
   corresponding to complex rule systems, (2) continuity, which
   explains how one rule system can morph into another one, and         syntactic dependencies requires correlating the pops with
   (3) “recursion approximation”, a concept related to symbolic         the pushes. The term “recursion” refers to the situation in
   recursion, which addresses some of the puzzles about                 which a rule can be invoked an unbounded number of times.
   embedding patterns in human behavior.                                “Center embedding recursion” is the case in which the
                                                                        symbol for such a repeatedly used rule occurs in the middle
   Keywords: artificial grammar learning; artificial neural             of one of the rules with symbols on either side of it (e.g., in
   networks; recurrent networks; simple recurrent networks;
   sequence learning; recursion; center embedding; rules.               G, “S” occurs with “1” to its left and “2” to its right in the
                                                                        first rule). Center embedding context free grammars are of
                           Introduction                                 particular interest because a system for generating or
                                                                        recognizing all and only the sentences produced by a center
                           0B
What kind of structural system underlies human syntactic                embedding grammar needs an unbounded memory.
processing ability? Much work in linguistics addresses this                It is generally recognized that some degree of center
question by exploring syntactic behaviors in natural                    embedding is present in natural languages, for there are
languages. Work on artificial grammars offers a chance to               many situations where natural languages employ patterns
obtain detailed information about human interaction with                within patterns of the same type—e.g., in relative clauses.
formal syntactic systems in the absence of semantic content             This suggests that minds have recursive rule systems at their
or task-independent pragmatic function. Here, we introduce              disposal for keeping track of these patterns. The recursive
a variant on existing artificial grammar learning tasks that            rule system is appealing as an explanation because it permits
supports careful comparison between human and artificial                efficient description of many cases and predicts the way
neural network models. The results help clarify the                     people exercise their language knowledge in many new
difference between standard, rule-based conceptions of                  combinations of words and phrases (Pinker, 1994).
grammatical knowledge and the claims of the neural net                     Yet humans have great difficulty processing more than a
perspective, providing some evidence that, at least in the              few levels of embedding in natural language (see Lewis,
artificial grammar task, humans resemble the networks. We               1996). Similar findings characterize artificial grammar work
focus, in particular, on the status of center embedding                 on recursion (de Vries, Monaghan, Knecht, & Zwitserlood,
recursion, which many authors view as an important feature              2008; Poletiek, 2002). If a symbol processing system must
of natural language systems, but whose status in the theory             only handle a few levels of embedding, then it is not strictly
of representation has been much debated (e.g., Chomsky,                 necessary to employ a recursive process—a weaker, finite-
1957; Christiansen & Chater, 1999; Friederici, 2002).                   state device, which has a limited memory capacity, can do
   Center-embedding recursive patterning can be generated               the job. Proponents of recursive rules have suggested that
by context free grammars. Context free grammars are rule                memory limitations obscure a fundamentally infinite
systems like Grammar G (Table 1) in which rules take the
form (A  X1 X2 ... XN, for N a finite number), and there are                 Table 1: Grammar G. Both rules are starting rules.
designated starting rules. The grammar is said to generate a
finite sequence of symbols, called a “sentence”, if it is                               S→1S234
possible to make successive substitutions for symbols on the                            S→1234
                                                                    1679

mechanism. But even if humans had such a mechanism, it              knowledge of one structural feature of the environment
would be impossible for finite life-span researchers to             transfers to another structural feature which is recursively
observe its infinite behavior. Thus, the argument for human         related to the first. 1 The knowledge need not transfer
                                                                                           F F
employment of recursive systems seems to founder on a               perfectly and thus the system may not follow the recursive
shoal of infinity: true center-embedding recursion is               rule to arbitrary levels, but to the degree that the system’s
distinguished by its employment of infinite memory, but we          knowledge is iteratively effective, it will be said to form a
cannot observe infinite memory, so it is hard to justify            “good” approximation of the recursion. Thus the definition
recursive rules.                                                    clarifies the sense in which a network can be “close to” a
   Relevant to this discussion, artificial neural networks have     recursive behavior without embodying it. The definition
been used to model many aspects of natural language                 also allows perfect recursion to be present at a locus in
behavior and they are often claimed to do so without                parameter space, in keeping with formal analyses of some
recourse to “explicit” rules (e.g., McClelland & Patterson,         recurrent networks (Tabor, 2000; 2009). There is also a way
2002; see also Pinker & Ullman 2002). Elman’s Simple                of gleaning empirical evidence for recursion approximation:
Recurrent Network or “SRN” (Elman 1991) is a model of               statistical evidence that a system bases its behavior with a
this sort that processes structured sequences. The SRN and          more embedded case on its knowledge about a less
its relatives have learned some elaborate patterns of center-       embedded one counts as such.
embedded recursion and have successfully generalized from              The remainder of the paper is organized as follows: in
training on less deeply embedded cases to prediction of             “Task” we introduce the grammar learning task. In “Simple
more deeply embedded cases (Rodriguez, 2001; Wiles &                Recurrent Network Model”, we describe the outcome of
Elman, 1995). However, they also do not typically extend            training many SRNs on the task and testing three
the patterns very far beyond their training (Christiansen &         hypotheses—Grouping, Continuous Interpolation, and
Chater, 1999). In light of the difficulty that humans have          Recursion Approximation—generated by the Structured
with processing deep center embeddings, Christiansen and            Manifold view. In “Human Grammar Learning
Chater argue that the networks’ behavior provides an                Experiment”, we report on a parallel study with human
appealing alternative account to the recursive rule approach.       grammar learners. “General Discussion” concludes.
   However, the network representations are not well
understood. In particular, if the networks do not employ                                           Task
                                                                                                   1B
rules, it is not clear what kinds of order they predict should      We employed a grammar learning task called the Box
occur; nor is it clear why observed behaviors can often be          Prediction Task that is a variant of sequence learning tasks
given a parsimonious description with systems of rules. We          (Clegg, Di Girolamo, & Keele, 1998). In sequence learning,
suggest that it will help to look closely at the nature of the      a popular task is the Serial Reaction-Time task (Nissen &
network representations, in conjunction with detailed               Bullemer, 1987) where stimuli are presented sequentially
measurement of human behavior on a task that both                   and participants respond to each stimulus (e.g., by clicking
networks and humans can perform well. Through such an               on the place where the stimulus appeared). Participants’
approach, we can acquire some insight to the conundrums of          responses trigger the presentation of the next stimulus. In
human recursive patterning.                                         patterned sequential data, reaction times often reflect the
   In particular, we suggest that the network view is well          predictability of the sequence, suggesting that participants
described as a “Structured Manifold” account. We use the            develop a structured encoding of the data. However, it is
term manifold to draw attention to the fact that the network        difficult to tell from the data in such a task when a
parameters are real-valued so they can change continuously,         participant has reliably detected complex dependencies like
and continuous change of parameter values is associated             those that occur in center-embedding.
with continuous change in the network’s behavior (see                  In the Box Prediction Task, stimuli are presented
Spivey, 2007). This property is useful for explaining the           sequentially but participants are asked to predict the next
learning phenomena—it makes it so the networks can be               stimulus instead of simply reacting to the current stimulus.
sensibly described as “getting closer” to a particular              Human participants predict by clicking a box on a screen.
structural behavior before the behavior actually appears. On        They immediately get feedback because the correct box
the other hand, the structured part of “Structured Manifold”        changes color (from black to green or blue). The networks
refers to the fact that the network behaviors in the context of     predict by activating output nodes corresponding to boxes.
a particular environment tend to concentrate around a few           They also get immediate feedback in the form of a vector
types. These types correspond to qualitatively distinct             indicating which symbol the grammar produced next.
lawful patterns in the network’s relationship to its
environment. They are closely related to rule-systems, for
they correspond to systematic insights about the patterns in           1
                                                                         We assume, for analysis purposes, that the environment
the world. In particular, the Structured Manifold approach          contains patterns which are describable by recursive rules. This
suggests a way of understanding “recursion” that avoids the         assumption does not commit us to claiming that actual
“shoal of infinity” mentioned above. We say that a pattern          environments have infinite patterning. Instead, one can think of
of behavior approximates a recursive mechanism if                   this assumption as a tool for understanding the structure of human
                                                                    and network behaviors.
                                                                1680

                  Figure 1: Network architecture.                               Figure 2: Network Clusters. Li_j(k) means
                                                                                        Level i, j’th word, symbol k.
   Many artificial grammar learning tasks have tested
languages with high nondeterminism (e.g., Reber, 1967).                 For (1 “Grouping”) we used a cluster analysis. After
Performing the Box Prediction Task with such data would              training, we fixed the weights of each network and tested it
be very frustrating because only a few responses are likely          on a Level-1, a Level-2, and a Level-3 sentence, thus
to be correct. We therefore employed a grammar (G) with              examining a total of 24 word-to-word transitions. Level-1
very little nondeterminism and we color-coded the pushes             and Level-2 sentences occurred in training, but Level-3 did
(blue), which are the only nondeterministic transitions,             not. We interpreted the network’s output nodes as
telling the participants that they need not predict them.            probabilities by using the Luce Choice Rule with base e10
   Sentences generated by the grammar were concatenated              and computed the expected accuracy of each network at
to form long training sequences that were presented                  each transition from these probabilities. We then applied K-
sequentially to networks and human participants.                     means clustering to the 24 accuracy values. A standard
                                                                     method of choosing the number of clusters, selecting the
      Simple Recurrent Network Simulations
      2B                                                             “knee” in the plot of within-group sum of squares vs.
Method. 22 Simple Recurrent Networks (SRNs) with the                 number of clusters, suggested 3, 4, or 5 clusters. 2 For   F F
same architecture (Figure 1) were constructed and the initial        simplicity, and for alignment with the analysis of human
weights were randomly set (uniform distribution on [-0.1,            data reported below, we focused on the 3-cluster case.The
0.1]). Each network was trained twice from the same initial          accuracies of the three clusters are shown in Figure 2
weights on a sequence of Level 1 and Level 2 sentences. In           (means shown in bold). The means of Cluster 1 indicate that
the first sequence, the average frequency of Level 2                 Cluster 1 networks tend to employ a “Simple Markov”
sentences increased over the course of 8000 trials (Table 2).        strategy: 12, 23, 34, 41 (these numbers refer to
In the second sequence, it decreased. We expected Sequence           grammar symbols). Each prediction by the network is
1, which emphasized Level 1 before Level 2, to produce               conditioned strictly on the input, even if a push or a pop
better recursion approximation because, in recursive                 produces a violation of expectation. The means of Cluster 2
generalization, (Level) 2 to 3 parallels 1 to 2, not 2 to 1.         indicate that Cluster 2 networks also use the rules, 12,
Results. We asked three questions about the ensemble of              23, and 34, but the networks switch between two
networks: (1 “Grouping”) Can the networks be grouped into            modes of responding to input 4: if the previous successor of
a few, qualitatively distinct behaviors which correspond to          4 was 1, then the next response to 4 is 1. If the previous
rational responses to the task environment? (2 “Continuous           successor of 4 was 2, then the next response to 4 is 2. This
Interpolation”) Do the networks favor intermediate states, in        “2-Mode Perseverater” has some memory for the past, but
which they blend the qualitative behaviors just mentioned?           cannot keep track of the correlation between pushes and
(3 “Recursion”) Is there evidence that the more successful           pops. The Cluster 3 means indicate that Cluster 3 networks
individuals approximate a recursive mechanism?                       expect a Level 1 sentence if the sentence begins 1-2, and
                                                                     they expect a Level 2 sentence if the sentence begins 1-1-2.
 Table 2: Distribution of sentences in the 2 sequence types.         However, they don’t, on average, generalize the dependency
                                                                     to level 3; instead, most of them tend to treat 1-1-1-2 the
         Sequence          No. of sentences per phase                same as 1-1-2, thus failing on the second pop of Level 3
           Type      Phase1 Phase2 Phase3 Phase4           Total     sentences.       Nevertheless, these “Fragile 1-Counters”
     Sequence 1                                                      approximate the behavior of the unbounded recursion
      L1 sentence     2000      2000       2000       2000 8000      generating process better than the other two types (Mean
      L2 sentence      200       400        600        800 2000
     Sequence 2                                                         2
      L1 sentence     2000      2000       2000       2000 8000
                                                                          We also sought a maximum of the Calinski-Harabasz pseudo-
                                                                     F statistic (Calinski & Harabasz, 1974), another standard method,
      L2 sentence      800       600        400        200 2000
                                                                     but there was no clear maximum.
                                                                 1681

       Table 3: Networks per cluster for each condition.             the sentence. Indeed, by this criterion, three of the networks
                                                                     from Cluster 3 exhibited correct performance on Level 3
                      Cluster 1 Cluster 2 Cluster 3                  sentences. Moreover, a regression analysis showed that even
      Sequence 1          11           1            10               when these three networks were removed from the data set,
      Sequence 2           6           5            11               better performance on Level 1 and 2 sentences predicted
Accuracies: Simple Markov 77%, 2-Mode Perseverator                   better performance on Level 3 sentences (r = 0.32, p < .05).
77%, Fragile 1-Counters 83%). In sum, the clustering                 These observations suggest that the networks that do well
analyses reveal that the networks fall into distinct qualitative     on Level 3 sentences do so in virtue of their ability to do
categories which are associated with distinct systematic             well on Level 2 sentences, even if they do not precisely
responses to the task. Although one might expect Sequence            embody the recursive generating process. Under the
1 to encourage 1-counting and Sequence 2 to discourage it,           definition given in the Introduction, this observation
a likelihood ratio test showed no effect of training condition       suggests that the networks approximate a recursive
on the distribution of clusters, even with clusters 2 and 3          mechanism.
treated as one (χ2(1) = 2.42, p = .120) (Table 3).
   To investigate (2 “Continuous Interpolation”), we                           Human Grammar Learning Experiment
                                                                               3B
contrasted two hypotheses: (a) network behaviors are
distributed as cluster prototypes + noise (equal distortion in       Method
                                                                     4B
all directions); (b) the networks approximate blends of                 Participants. 44 college students from the University of
behaviors associated with the various cluster prototypes.               Connecticut participated for course credit.
Under both (a) and (b), a network could be proximal to a                Materials. Two sequences of 400 trials each were created.
pure complex behavior (e.g., a recursive grammar) without               In both sequences 1 and 2 there were 38 Level-1 sentences,
precisely embodying it. But in (a) deviations have low                  25 Level-2 sentences, and 4 Level-3 sentences. The last 99
likelihood of leading to purer recursion because they can               trials of both sequences were identical. The first instance of
occur in any direction; in (b) deviations are more likely to            a level-3 sentence occurred at trial 302 so trials 1-301
lead to purer behavior because the models are restricted to a           served as an analog of network training and trials 302-400
low-dimensional manifold. In this sense, proximity of a                 served as an analog of network testing. In Sequence 1 the
network in case (b) to an ideal complex behavior is a more              density of Level 2 sentences changed from low to high over
reliable indication that the network will robustly exhibit              the course of trials 1-301. Sequence 2 had the reverse
complexity, than in (a). How can we tell (b) apart from (a)             progression. A windows PC with speakers on the monitor
empirically? If (a) holds, then the variation of each                   and a standard mouse were used for the display and input.
network’s behavior on each transition is expected to be                 The experiment was run in E-Prime.
equal. If (b) holds, then the individual networks are                   Procedure. Participants saw 4 black boxes on a screen. The
expected to show greater variation on transitions in which              boxes were positioned in a circle with grammar G numbers
networks tend to contrast than on transitions in which all              associated counterclockwise, but not indicated on the
networks tend to agree. We tested this hypothesis by                    screen. Each participant ran only one sequence. When a
comparing the variances of individual networks’ behaviors               participant clicked a box, one of the 4 boxes would turn a
on different types of transitions during the last 99 trials to          different color indicating that it was the next box in the
the global variances on the same types of transitions (global           sequence. The participant was instructed to try to predict
variance on a transition T is the total variance across all             which box would next change color and click on it. It was
network behaviors on T). Examining all 44 trained                       emphasized that the goal was prediction, and not to simply
networks, we considered the 24 transition types associated              click the box which had previously changed color. If a
with 1-level, 2-level and 3-level sentences. We hypothesized            participant predicted the wrong box on non-push trial, a
that each 24-element vector of individual variances would               short beep sounded. No sound was played if the participant
be more aligned with the 24-element vector of global                    predicted the correct box. The correct next box generally
variances than with a 24-element vector of uniform                      changed from black to green, except on push trials, where
variances. Indeed, a paired t-test on our 44 network sample             the second 1 box changed to blue, and the third to cyan (in a
showed that the cosine of the angle between the individual              Level 3 sentence). Participants were instructed that they
variance vector and the global variance vector was                      need not predict the blue/cyan boxes. The computer
significantly bigger than the cosine of the angle between the           recorded the accuracy of the participants’ predictions.
individual variance vector and any positive uniform
variance vector (p < .001).                                             Results
                                                                        5B
   For (3 “Recursion”), we examined individual network                     Mean accuracies over the course of the task are shown in
response patterns to see if any of the networks generalized                Figure 3, separated into 3 classes: sequential transitions
to correct performance on Level 3, provided they had                       pops, and pushes. A logistic regression analysis supported
learned correct performance on Level 2. We counted a                       staged learning—e.g., final pops learned before intermediate
network as having correct performance on a sentence if its                 pops in Level 2 sentences for Sequence 1 (p < .001). We
accuracy was above 0.5 on all deterministic transitions in
                                                                 1682

                                                                  predicting the correlation between pushes and pops. In fact,
                                                                  the Cluster 3 mean accuracy on the second Level 3 pop is
                                                                  above 0.5. These results indicate that the human behaviors,
                                                                  like the network behaviors, can be grouped into several
                                                                  different coherent responses to the task, though the human
                                                                  cluster prototypes are associated with somewhat different
                                                                  strategies than the network cluster prototypes. Table 4 gives
                                                                  the number of participants in each cluster as a function of
                                                                  training sequences. A likelihood ratio test of showed no
                                                                  effect of training condition on the distribution of clusters,
                                                                  even with Clusters 2 and 3 treated as one (χ2(1) = 2.34, p
                                                                  = .126).
      Figure 3: Mean accuracy change during the task.                Regarding (2 “Continuous Interpolation”), comparison of
        Level 3 2nd pop trials (L3_10(2)) are circled.            variance vectors confirmed that, for humans, like networks,
                                                                  most of the individual variation was on dimensions on
                                                                  which there was high global variance (p < .001). As with the
                                                                  nets, this result suggests that, when the humans diverge
                                                                  from the coherent behaviors associated with the clusters,
                                                                  they tend to diverge in the direction of other coherent
                                                                  behaviors. Interestingly, when we performed the global
                                                                  variance test on a cluster by cluster basis, Clusters 2 (N = 7)
                                                                  and 3 (N = 18) showed significant correlation, but Cluster 1
                                                                  (N = 19) did not, even though Cluster 1 had the largest
                                                                  sample size. These results provide suggestive evidence that
                                                                  the Push Predicters and the Push Blindsiders are hamstrung
                                                                  between the pull of their cluster prototypes and the impulse
                                                                  to be like Simple Markov processes, or like each other,
                                                                  while the Simple Markov processors are, on average,
                   Figure 4: Human clusters.
                                                                  insensitive to the non Markovian structure in the data.
focused our analysis on the last 99 trials, when both                Regarding (3 “Recursion”), there were five participants,
sequence types experienced the same sequence of boxes.            all Push Blindsiders, whose mean accuracy on all
  (1 “Grouping”) In the human case, the Calinski-Harabasz         deterministic Level 1, Level 2, and Level 3 transitions never
pseudo-F statistic had a clear maximum at 3 clusters so we        strayed more than 0.5 away from the predictions of the
examined this case (Figure 4). Figure 4 suggests that the         generating process over the last 99 trials. These people can
participants in Human Cluster 1, like the networks in             be said to have mastered the push-pop correlation across the
Network Cluster 1, employ the Simple Markov system.               three levels, providing suggestive evidence that they employ
Cluster 2 is much more sensitive to the temporal structure of     a recursive mechanism. Furthermore, a regression analysis
pops and pushes, for these participants perform more              showed that mean accuracy on Level 3 sentences was
accurately on the Level 2 pop and the first Level 3 pop, than     positively correlated with Level 1/2 accuracy in the last 99
Cluster 1 participants. Cluster 2 participants tend to employ     trials (b = 0.633, t = 3.38, p < .01). This result is consistent
the rule, 1  1, so they have relatively high accuracies on       with the recursion approximation hypothesis: the correlation
pushes (even though the instructions said that the blue boxes     between Level 1/2 and Level 3 suggests that the structural
need not be predicted). Although these “Push Predictors”          insight about Level 1/2 is being used to solve Level 3.
did numerically better than Cluster 1 on the second pop of        However, the humans, unlike the networks, can keep on
Level 3, their mean performance on this transition was less       learning during the “test” trials, so the correlation might
than 0.5, suggesting that they are not robustly sensitive to      stem from a greater learning facility in some humans than
the correlation between pushes and pops. Cluster 3 uses a         others: those who have greater learning facility will learn
different strategy with pushes—they generally predict 2           Level 1 and 2 sentences better during trials 1-301 and they
after 1, thus failing to predict the pushes and successfully      will also learn Level 3 sentences better during trials 302-
predicting the finite state transitions from 1 to 2. These        400, but they might not use any of their knowledge of Level
“Push Blindsiders” are even better than Cluster 2 at              1 and 2 sentences to solve Level 3. However, in a separate
                                                                  analysis, Sequence Type predicted Level 1/2 accuracy (b = -
     Table 4: Participants per cluster for each condition.        0.052, t = -2.53, p < .05). These results are unexpected on
                                                                  the Learning Facility account for two reasons: the density
                      Cluster 1    Cluster 2    Cluster 3         manipulation does not change the total amount of exposure
      Sequence 1           7            3          12             to Level 1 and Level 2 sentences, so if these were simply
      Sequence 2          12            4           6             learned on the basis of exposure, there would be no reason
                                                              1683

for a Sequence Type effect on Level 1/2. Second if, as               Chomsky, N. (1957). Syntactic structures. The Hague:
claimed by the Learning Facility account, Level 3 sentences            Mouton and Co.
are learned independently of structural insight gleaned from         Chomsky, N. (1981). Lectures on Government and Binding.
Levels 1 and 2, then there would be no reason to expect                Mouton de Gruyter.
Level 3 variation to be related to anything except participant       Christiansen, M. & Chater, N. (1999). Toward a
identity. Instead, the data suggest that Sequence type                 connectionist model of recursion in human linguistic
influences the learning of Levels 1 and 2, and the nature of           performance. Cognitive Science, 23, 157-205.
this learning, in turn, influences performance on Level 3s,          Clegg, B. A., DiGirolamo, G. J., & Keele, S. W. (1998).
consistent with the recursion approximation hypothesis.                Sequence learning. Trends in Cognitive Sciences, 2, 275-
                                                                       281.
                    General Discussion
                    6B                                               De Vries, M. H. Monaghan, P., Knecht, S., & Zwitserlood,
The similarities between the network and human results                 P. (2008). Syntactic structure and artificial grammar
provide some evidence that the Structured Manifold is a                learning: The learnability of embedded hierarchical
good framework for understanding human syntactic                       structures. Cognition, 107, 763-774.
encoding, at least in artificial grammar learning.                   Elman, J. (1991). Distributed representations, simple
   The network analysis helps clarify the notions of                   recurrent networks, and grammatical structure. Machine
“Grouping”, “Continuous Interpolation” and “Recursion                  Learning, 7, 195-225.
Approximation”. In particular, the Grouping results provide          Friederici, A. (2002). Towards a Neural Basis of Auditory
evidence that network interaction with the environment                 Sentence Processing. Trends in Cognitive Sciences, 6, 78-84.
focuses on a small finite number of coherent behaviors.              Lewis, R. (1996). Interference in short-term memory: The
Even the Simple Markov system, though it is not optimal                magical number two (or three) in sentence processing.
for the task, detects a level of regularity which is inherent in       Journal of Psycholinguistic Research, 25, 93-115.
the task structure---the so-called “second-order” statistical        Magnuson, J., Tanenhaus, M., Aslin, R., Dahan, D. (2003).
approximation. In dynamical systems terms, it seems likely             The time course of spoken word learning and recognition:
that these structures are attractors of some sort. It may be           Studies with artificial lexicons. Journal of Experimental
helpful to ask what the nature of their stability is within the        Psychology: General, 132, 202-227.
panoply of dynamical stabilities (see Tabor, 2009).                  McClelland, J. & Patterson, K. (2002). Rules or connections
   The Continuous Interpolation results are related to                 in past-tense inflections: what does the evidence rule out?
parameter-setting models of syntax (e.g. Chomsky, 1981) in             Trends in Cognitive Sciences, 6, 465-472.
the sense that they provide a reduce-dimension description           Nissen, M. J., & Bullemer, P. (1987). Attentional
of the range of expected behavior. An important difference             requirements of learning: evidence from performance
between the current model and linguistic parameter setting             measures. Cognitive Psychology, 19, 1-32.
models, is that the structure of the “parameters” was derived        Pinker, S. (1994). The Language Instinct: How the Mind
from the interaction of a very general-purpose learning                Creates Language. New York: Perennial Classics.
mechanism with the environmental data. Thus, this appears            Pinker, S. & Ullman, T. (2002). The past and future of the
to be a less nativist kind of parameter setting.                       past tense. Trends in Cognitive Sciences, 6, 456-463.
   Finally, the Recursion Approximation analysis suggests a          Poletiek, F. H. (2002). Implicit learning of a recursive rule in
way of reconciling the desirable properties of recursive rules         an artificial grammar. Acta Psychologica, 111, 323-335.
with the facts that human behavior is imperfect and cannot           Reber, A.S. (1967) Implicit learning of artificial grammars.
be infinitely observed. Combined with the Continuous                   Journal of Verbal Learning and Verbal Behavior, 6, 855–863.
Interpolation observation, the results suggest understanding         Rodriguez, P. (2001). Simple recurrent networks learn
states of a system as being related not just to one but to             context-free and context-sensitive languages by counting.
many ideal forms. This suggests shifting away from a view              Neural Computation, 13, 2093-2118.
of organisms as “having a knowledge system” and toward a             Spivey, M. (2007). The Continuity of Mind. New York:
view in which they can be “in the sphere” of multiple                  Oxford University Press.
systems. Their actual behavior is not static, and may be             Tabor, W. (2000). Fractal encoding of context-free grammars
understood as a structured trajectory through these spheres.           in connectionist networks. Expert Systems, 17, 41-56.
                                                                     Tabor, W. (2009). A dynamical systems perspective on the
                       Acknowledgments                                 relationship between symbolic and non-symbolic
                                                                       computation. Cognitive Neurodynamics, 3, 415-427.
                       7B
We gratefully acknowledge support from NICHD grant                   Wiles, J. & Elman J. (1995). Learning to count without a
HD40353 to Haskins Laboratories and NICHD Predoctoral                  counter: a case study of dynamics and activation
NRSA HD060414 to AK.                                                   landscapes in recurrent networks. Proceedings of the 17th
                                                                       Annual Cognitive Science Conference (pp. 482-487).
                          References
                          8B
                                                                       Mahwah, NJ: Lawrence Erlbaum Associates.
Calinski, T., & Harabasz, J. (1974). A dendrite method for
   cluster analysis. Communications in Statistics, 3, 1-27.
                                                                 1684

