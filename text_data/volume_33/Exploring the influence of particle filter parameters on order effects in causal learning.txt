UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Exploring the influence of particle filter parameters on order effects in causal learning
Permalink
https://escholarship.org/uc/item/3k26g6wn
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Abbott, Joshua T.
Griffiths, Thomas L.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                    Powered by the California Digital Library
                                                                        University of California

                           Exploring the Influence of Particle Filter Parameters
                                         on Order Effects in Causal Learning
                                             Joshua T. Abbott (joshua.abbott@berkeley.edu)
                                           Thomas L. Griffiths (tom griffiths@berkeley.edu)
                     Department of Psychology, University of California at Berkeley, Berkeley, CA 94720 USA
                               Abstract                                   Monte Carlo methods in particular: importance sampling and
                                                                          particle filtering. Importance sampling draws samples from
   The order in which people observe data has an effect on their
   subsequent judgments and inferences. While Bayesian mod-               a known proposal distribution and weights these samples to
   els of cognition have had some success in predicting human             correct for the difference from the desired target distribu-
   inferences, most of these models do not produce order effects,         tion. Particle filters are a sequential Monte Carlo method that
   being unaffected by the order in which data are observed. Re-
   cent work has explored approximations to Bayesian inference            uses importance sampling recursively. When approximating
   that make the underlying computations tractable, and also pro-         Bayesian inference, the posterior distribution is represented
   duce order effects in a way that seems consistent with human           using a set of discrete samples, known as particles, that are
   behavior. One of the most popular approximations of this kind
   is a sequential Monte Carlo method known as a particle fil-            updated over time as more data are observed. These meth-
   ter. However, there has not been a systematic investigation of         ods can be shown to be formally related to existing psycho-
   how the parameters of a particle filter influence its predictions,     logical process models such as exemplar models (Shi et al.,
   or what kinds of order effects (such as primacy or recency ef-
   fects) these models can produce. In this paper, we use a simple        2008), and can be used to explain behavioral data inconsistent
   causal learning task as the basis for an investigation of these        with standard Bayesian models in categorization (Sanborn et
   issues. Both primacy and recency effects are seen in this task,        al., 2006), sentence parsing (Levy et al., 2009), and classical
   and we demonstrate that both kinds of effects can result from
   different settings of the parameters of a particle filter.             conditioning experiments (Daw & Courville, 2008). How-
                                                                          ever, there has not previously been a systematic investigation
   Keywords: particle filters; order effects; causal learning; ra-        of how the parameters of these Monte Carlo methods affect
   tional process models
                                                                          the predictions they make.
                           Introduction                                       In this paper we explore how the parameters of particle
How do people make such rapid inferences from the con-                    filters affect the predictions that they make about order ef-
strained available data in the world and with limited cogni-              fects, using a simple causal learning task to provide a context
tive resources? Previous research has provided a great deal               for this exploration. It is a common finding that the order
of evidence that human inductive inference can be success-                in which people receive information has an effect on their
fully analyzed as Bayesian inference, using rational models of            subsequent judgments and inferences (Dennis & Ahn, 2001;
cognition (Anderson, 1990; Oaksford & Chater, 1998; Grif-                 Collins & Shanks, 2002). This poses a problem for rational
fiths, Chater, Kemp, Perfors, & Tenenbaum, 2010). Ratio-                  models based on Bayesian inference as the process of updat-
nal models answer questions at Marr’s (1982) computational                ing hypotheses in these models is typically invariant to the or-
level of analysis, producing solutions to why humans behave               der in which the data are presented. Previous work has shown
as they do, whereas traditional models from cognitive psy-                that particle filters can produce order effects similar to those
chology tend to analyze cognition on Marr’s level of algo-                seen in human learners (e.g., Sanborn et al., 2006). However,
rithm and representation, focusing instead on how cognitive               this work has focused on primacy effects, in which initial ob-
processes support these behaviors. Although Bayesian mod-                 servations have an overly strong influence on people’s con-
els have become quite popular in recent years, it remains un-             clusions. In other settings, people produce recency effects,
clear what psychological mechanisms could be responsible                  being more influenced by more recent observations. Causal
for carrying out these computations. Of particular concern                learning tasks can result in both primacy and recency effects,
is that the amount of computation required in these models                with surprisingly subtle differences in the task leading to one
becomes intractable in real-world scenarios with many vari-               or the other (Dennis & Ahn, 2001; Collins & Shanks, 2002).
ables, yet people make rather accurate inferences effortlessly            Causal learning thus provides an ideal domain in which to
in their everyday lives. Are people implicitly approximating              examine how the parameters of particle filters influence their
these probabilistic computations?                                         predictions, and what kinds of order effects these models can
   Monte Carlo methods have become a primary candidate                    produce.
for connecting the computational and algorithmic levels of                    The plan of the paper is as follows. In the next section
analysis (Sanborn, Griffiths, & Navarro, 2006; Levy, Reali,               we discuss previous empirical and theoretical work on human
& Griffiths, 2009; Shi, Feldman, & Griffiths, 2008). The ba-              causal learning, showing different kinds of observed order ef-
sic principle underlying Monte Carlo methods is to approxi-               fects and providing the Bayesian framework we will be work-
mate a probability distribution using only a finite set of sam-           ing in. We then formally introduce particle filters, followed
ples from that distribution. Recent work has focused on two               by our investigation of how varying certain particle filter pa-
                                                                      2950

rameters controls order effects. After this, we use our new-
found understanding of the parameters to model different or-            B                C          Figure 1: Directed graph in-
der effects in previous experiments. Finally, we discuss the                                        volving three binary variables
implications of our work and future directions for research.               S0         S1            – (B)ackground, (C)ause, and
                                                                                                    (E)ffect – relevant to causal induc-
          Order Effects in Causal Learning                                                          tion, and two edge weights – s0
                                                                                E                   and s1 – indicating how strongly
We focus our investigation of order effects in causal learning                                      B and C influence E, respectively.
on a pair of studies based on sequences of covarying events.
Dennis and Ahn (2001) presented participants with a series
of trials indicating whether or not a plant was ingested and           Using this graphical model we are interested in the condi-
whether or not this resulted in an allergic reaction. The se-       tional probability P(E|B,C) and want to evaluate how well
quence of trials was split into two equal blocks of covarying       the strength weights predict the observed data. Motivated
events; one primarily indicating a generative causal relation-      by the models used in Griffiths and Tenenbaum (2005) and
ship between plant and reaction, and the other primarily in-        Cheng (1997), we define the conditional probability using
dicating a preventative relationship. The overall contingency       the noisy-OR and noisy-AND-NOT functions for generative
of the combined blocks was 0. The blocks were presented             and preventative causes respectively. Assuming a background
one after the other, with the initial block chosen randomly,        cause is always present (ie. B = 1), we will get a sequence of
and after observing all trials participants were asked to make      events in the form “Cause was (C = 1) or was not (C = 0)
a strength judgement (-100 to 100) on the causal relationship       present and an effect did (E = 1) or did not (E = 0) occur”.
they thought existed between plant and reaction. After an-          This gives us four possible conditions to evaluate. Depending
swering, the blocks were presented again in reverse order and       on the sign of s1 – the strength of C causing/preventing E, we
the subjects were asked to make another judgement. If the           compute either the noisy-OR (s1 ≥ 0, a generative cause) or
generative relationship block was presented first, followed by      the noisy-AND-NOT (s1 < 0, a preventative cause). Table 1
the preventative block, participants responded with a pref-         presents the resulting probabilities.
erence for a generative relationship (M=17.67, SD=25.66).
However, if the preventative block was presented first, partic-
                                                                    Table 1: The noisy-OR (s1 is positive) and noisy-AND-NOT
ipants responded that only a weak preventive causal relation-
                                                                    (s1 is negative) functions
ship existed (M=-5.50, SD=22.27). These results indicate a
primacy effect in favor of generative causal relationships.                 C    E          s1 is positive        s1 is negative
   The primacy effect found by Dennis and Ahn (2001) was                    1    1         s0 + s1 − s0 s1          s0 (1 + s1 )
contradictory to previous models of associative strength that               1    0      1 − (s0 + s1 − s0 s1 )   1 − [s0 (1 + s1 )]
showed recency effects, and a subsequent follow-up study                    0    1                s0                     s0
was conducted that examined the role of judgment frequency                  0    0              1 − s0                 1 − s0
in producing different kinds of order effects (Collins &
Shanks, 2002). In this study, the authors used exactly the             To complete the definition of this Bayesian model, we
same trial sequence as Dennis and Ahn (2001), but asked the         need to specify the prior distribution that is assumed on the
participants for a causal strength judgment after every 10 tri-     strengths s0 and s1 . Our starting assumption is that these
als rather than just one final strength judgement. Making fre-      weights are each drawn from a uniform prior over their en-
quent judgments resulted in recency effects, with participants      tire range, with s0 ∼ Uniform(0, 1) and s1 ∼ Uniform(−1, 1),
producing more generative estimates when they saw the gen-          where negative values of s1 imply a preventative cause, as de-
erative data most recently (M=58.4, SD=34.2), and more pre-         tailed above. This basic model assumes that s0 and s1 remain
ventative estimates when they saw the preventative data most        constant over time. A slightly more complex model would
recently (M=-23.3, SD=39.3).                                        allow the strength of causes to drift, taking on a value that is
                                                                    close to the value on the previous trial but with some stochas-
      Causal Learning as Bayesian Inference                         tic variation. We can do this by assuming that s0 and s1 have
We can formulate this causal learning problem as a problem          a prior on each trial that is based on their value on the pre-
of Bayesian inference through the use of a causal graphical         vious trial. We assume that s0 follows a Beta distribution,
model framework similar to that used in Griffiths and Tenen-        Beta(λs + 1, λ(1 − s) + 1) where s is the value on the pre-
baum (2005). In this framework, we assume a single directed         vious trial and λ controls the rate of drift, with large values
graph structure defined over three binary variables: an effect      indicating a slow drift. We assume that s1 preserves its sign
E, a potential cause C, and a background cause B which cap-         (and is thus fixed as generative or preventative), but its abso-
tures all other causes of E that are not C. Additionally, there     lute value is drawn from a Beta distribution in the same way.
are strengths, s0 and s1 , that indicate how strongly B and C       For the first trial, s0 and s1 are assumed to follow the uniform
influence the presence, or lack thereof, of E. This graphical       prior distributions given above.
model is shown in Figure 1.                                            With our Bayesian model of causal learning defined, we
                                                                2951

now turn to the problem of inference. In the next two sections             weights, and the weights are reset,
we introduce the general schema for a particle filter and then
                                                                                        (i)          ( j) ( j)             (i)
indicate how it can be applied to the specific model of causal                         zt ∼ ∑ wt δ(z̃t )                 wt = 1/N
learning we have outlined in this section.                                                        j
                              Particle Filters                             for i = (1, . . . , N), where where δ(z̃) is a distribution that puts
                                                                           all its mass on z̃.
A particle filter is a sequential Monte Carlo method that can
                                                                               At the final time T we have an approximation to the pos-
be used to approximate a sequence of posterior distributions,
                                                                           terior P(z0:T |y1:T ), corresponding to the discrete distribution
as is necessary when performing Bayesian inference repeat-
                                                                           obtained by assigning each particle its normalized weight.
edly in response to a sequence of observations (Doucet, Fre-
itas, & Gordan, 2001). When using a particle filter, it is                 Parameters of Particle Filters
typically assumed we have a sequence of unobserved latent                  We can introduce variation into this algorithm by exploring
variables z1 , . . . , zt , where z0:t is modeled as a Markov pro-         different methods of particle selection. Resampling with re-
cess with prior distribution P(z0 ) and transition probability             placement after every observation quickly reduces the diver-
P(zt |zt−1 ). We then have a sequence of observed variables                sity in the set of particles, as a few highly weighted parti-
y1 , . . . , yt , and are attempting to estimate the posterior distri-     cles can take over the population. Thus, a common addition
bution P(z0:t |y1:t ). The posterior is given by Bayes’ rule, for          to the bootstrap filter is to vary how often resampling takes
any time t, as:                                                            place, using some measure of the amount of variability seen
                                                                           in the weights of the particles. Resampling with replacement
                      P(z0:t |y1:t ) ∝ P(y1:t |z0:t )P(z0:t ).
                                                                           can also result in identical copies of particles. Markov chain
We can obtain a recursive formula for this as:                             Monte Carlo (MCMC) is often used in conjunction with re-
                                                                           sampling as a rejuvenation step to restore diversity into the
       P(z0:t+1 |y1:t+1 ) ∝ P(z0:t |y1:t )P(yt+1 |zt+1 )P(zt+1 |zt ).      set of particles (Chopin, 2002).
                                                                               These choices about how to implement a particle filter have
Assuming we have a set of samples from P(z0:t |y1:t ), im-                 implications for the way that it behaves, but the consequences
portance sampling can be used to approximate this pos-                     of manipulating these parameters on sensitivity to trial order
terior distribution by sampling from P(zt+1 |zt ) for each                 have not been systematically explored. In the following sec-
value of zt in our sample, weighting each value of zt+1 by                 tion, we set up a particle filter for our Bayesian model of
P(yt+1 |zt+1 ), and then resampling from this weighted distri-             causal learning, and use it to investigate how different re-
bution. The result will be a set of samples that approximate               sampling methods affect our predictions. This investigation
P(z0:t+1 |y1:t+1 ). The recursive nature of this approximation,            seems particularly interesting given the potential psychologi-
where we can obtain samples from P(z0:t |y1:t ) given samples              cal interpretation of each of these parameters: resampling and
from P(z0:t−1 |y1:t−1 ), leads to a natural algorithm. This algo-          rejuvenation require greater computation than simply contin-
rithm, in which a set of samples is constantly updated to re-              uing to update the weights on particles, and might thus be
flect the information provided by each observation, is known               used strategically as a form of more deliberative reasoning
as a particle filter. The samples are referred to as particles.            that is triggered by some aspect of the state of the learner, or
                                                                           the task they are performing.
Particle Filter Template
We will examine variants of a particle filter based on the boot-              Particle Filter Parameters and Order Effects
strap filter presented in Doucet et al. (2001). There are three            With our Bayesian model of causal learning defined and an
steps to this filter:                                                      algorithm for a general purpose particle filter proposed, we
Initialization (t = 0): A set of N particles and associated                now turn to exploring the effects of varying the parameters of
importance weights are initialized,                                        the particle filter. We first modify the template given above to
                      (i)                          (i)                     fit our problem:
                     z0 ∼ P(z0 )                w0 = 1/N                   Initialization (t = 0): A set of N particles, where each par-
                                                                                   (i)
for i = (1, . . . , N).                                                    ticle z0 holds a pair of strength estimates (s0 , s1 ), and asso-
Importance Sampling: After each observation, a new set of                  ciated importance weights is initialized. s0 and s1 are drawn
particles is proposed based on the previous set of particles               from the prior defined above, and the weights are set to be
                                                                                        (i)
and the importance weights are computed,                                   uniform, w0 = 1/N.
                                                                           Importance Sampling: After each observation, a new set of
               (i)                             (i)     (i)     (i)         particles is proposed from the Beta distribution defined above,
             z̃t ∼ P(zt |zt−1 )             wt = wt−1 P(yt |z̃t )
                                                                           and the importance weights are computed using Table 1.
for i = (1, . . . , N).                                                    Selection: This step is where the four models we analyze
Selection: A new set of particles is sampled with replace-                 diverge. In Model 1, we never resample, simply letting the
ment from a distribution based on the normalized importance                importance weights determine the strength estimates. In the
                                                                       2952

other models, we resample particles based on a multinomial             Model 4 - ESS Resample with Rejuvenation: The results
distribution defined on the importance weights. In Model 2,            in Figure 2 (d) show that this model rises and falls in its sen-
we resample at each trial t. In Models 3 and 4, we resam-              sitivity to order as a function of the number of particles, as
ple only if the variance of the weights is too large as defined        in Model 3. However, the Metropolis-Hastings rejuvenation
by the Effective Sample Size (ESS). The ESS is ≈k wt k−2 ,             step produces a wider range of particle strengths after resam-
and we set a threshold at 0.10N, ten percent of the num-               pling. This is illustrated in Figure 3 (b) where we focus on
ber of particles. Model 4 has an extra step after resampling           this model’s behavior at each trial for a set of 50 particles.
where we perform rejuvenation on the particles. We per-                After resampling and rejuvenation, the diversity of particles
form 10 iterations of Metropolis-Hastings with new values              is much broader than simply resampling; resulting in the pre-
for (s0 , s1 ) drawn from a from a Normal distribution centered        dictions following the most recent trials.
on (sold     old
      0 , s1 ) with a standard deviation of 0.10 and accept the
proposed (snew       new
                0 , s1 ) pairs following the Metropolis-Hastings
                                                                          Modeling Order Effects in Causal Learning
acceptance rule.                                                       Now that we have established the effects of different pa-
                                                                       rameters of the particle filter, we can consider what settings
Results on the Causal Learning Task
                                                                       are required to reproduce the empirical data from the causal
We applied all four models to a simulated version of the               learning experiments discussed earlier in the paper. We show
causal learning task of Dennis and Ahn (2001), using the               that both primacy and recency effects can be produced using
same contingencies they listed for Experiment 3. For each              the sequence of 80 trials presented in both Dennis and Ahn
of the four different resampling methods, we averaged per-             (2001) and Collins and Shanks (2002), if the particle filter
formance over 500 runs, varied the number of particles from            parameters are selected to reflect the difference in the proce-
1 to 1000, and set λ = 10, 000. We presented the generative-           dures used in these two experiments.
preventative sequence first, and then re-initialized the particle
filters and ran the preventative-generative sequence. The re-          Dennis and Ahn (2001): Primacy Effect
sults are depicted in Figure 2.                                        Using the ESS resampling model (Model 3) with a stronger
Model 1 - Never Resample: In Figure 2 (a), we see that this            prior for positive s1 strengths, we can observe that the per-
model predicts a strength of 0 because the importance weights          formance of the particle filter with a small number of parti-
will average out over the trials. At first the particles with pos-     cles predicts similar primacy effects to those found in Dennis
itive strengths will have higher weights but will drop when            and Ahn (2001). Our use of a stronger prior was motivated
the negative trials begin. The opposite effect occurs for parti-       by Schustack and Sternberg (1981), where it was found that
cles with negative strengths. At the end of the simulation the         people weight generative evidence higher than inhibitory evi-
average of the weights goes to 0 since the overall contingency         dence. Figure 4 (a) presents these results, where it is apparent
between C and E in the combined sequence is 0.                         that the generative-block first performance shows more of a
Model 2 - Always Resample: As shown in Figure 2 (b),                   primacy effect than than preventative-block first.
this model exhibits a strong primacy effect, with strength es-
timates ending up at values consistent with the first block            Collins and Shanks (2002): Recency Effects
presented (positive for generative, negative for preventative)         We observed recency effects with our ESS resampling with
across a wide range of numbers of particles. This happens be-          rejuvenation model (Model 4), however, to maintain con-
cause particles with opposite strength to the current trial are        sistency in our modeling efforts we use the modified ESS
replaced very early in the sequence. This destroys diversity in        resampling model defined above with the addition that ev-
the particle set and only particles with strengths in common           ery 10 trials we resample with rejuvenation. This resam-
with the first few trials of the particular sequence remain.           pling scheme is motivated by the procedure in the Collins and
Model 3 - ESS Resample: Figure 2 (c) shows that when                   Shanks (2002) experiment, where participants were asked to
we resample the particle set only once the variance in parti-          estimate the strength of the relationship every 10 trials. Mak-
cle weights becomes high, we see primacy effects for smaller           ing such a judgment could trigger the kind of deliberative rea-
numbers of particles and then a convergence to 0 with larger           soning that resampling and rejuvenation reflect. Figure 4 (b)
numbers of particles. Since the ESS threshold is based on              shows that the performance of this model more accurately
a percentage of the number of particles, smaller numbers of            predicts the recency effects with a small number of particles.
particles are more likely to lead to frequent resampling be-           We observe too that the recency effect occurs much sooner
cause it is less likely they will contain a good set of candidate      under this resampling scheme than in our original simulation
strength values. Larger populations take a longer time to meet         with Model 4.
the ESS threshold, producing behavior that is more similar to
Model 1. We get a better understanding of this model’s be-                     General Discussion and Conclusions
havior by focusing on the predictions of 50 particles at each          While Bayesian models have become quite popular as ratio-
trial. Figure 3 (a) shows that after resampling, the diversity of      nal explanations of human inductive inference, a number of
the particle set narrows. This results in a primacy bias, albeit       significant criticisms remain unresolved. In particular, it is
a smaller effect than Model 2 due to infrequent resampling.            not clear how these computational level analyses connect to
                                                                   2953

            (a)                                                    (b)
            (c)                                                    (d)
Figure 2: The performance of different resampling methods in our particle filter. The strength estimate produced for the
generative-preventative (blue) and preventative-generative (red) versions of the contingencies is plotted against the number of
particles used for each of four models: (a) never resample, (b) always resample, (c) resample only when the effective sample
size (ESS) falls below a threshold, and (d) resample based on the ESS, and rejuvenate the particles (see text for details). The
mean across 500 runs is shown with a heavy line, and the shaded fill indicates the standard deviation.
the algorithmic level of analysis that characterizes existing          These particle filter approximations to a Bayesian model
psychological process models. Using Monte Carlo methods             of causal learning provide a more consistent explanation of
to approximate Bayesian inference while also linking to mod-        the observed order effects in behavioral data. Our analysis is
els of psychological processes creates a new approach to the        the first attempt at modeling order effects in causal learning
question of human inductive inference, resulting in what have       using particle filters. We aim to explore other causal learn-
been termed rational process models (Shi et al., 2008).             ing and contingency learning data with particle filter approx-
                                                                    imations in the future. This will include work to model not
   We have demonstrated how different resampling methods
                                                                    just how people estimate causal strength, but how they infer
in a particle filter can produce different order effects in a
                                                                    causal structure. We are currently conducting laboratory ex-
causal learning task, potentially expanding the scope of the
                                                                    periments to further explore how particle filter performance
effects that can be explained using these models. Using a
                                                                    degrades with fewer particles in comparison to human per-
model with a bias for generative relationships and a sam-
                                                                    formance degrading with higher cognitive load.
pling scheme that resamples only after the variance in particle
weights becomes too high resulted in primacy effects similar        Acknowledgments. This work was supported by grant number FA-
to the results in Dennis and Ahn (2001) for small numbers of        9550-10-1-0232 from the Air Force Office of Scientific Research.
particles. Adding a rejuvenation step to this model after every
10 trials to match the experimental procedures of Collins and                                References
Shanks (2002) gives way to the observed recency effects in          Anderson, J. R. (1990). The adaptive character of thought. Hills-
the literature.                                                        dale, NJ: Erlbaum.
                                                                2954

                                                       ESS Resample − 50 particles over 80 trials                                                      ESS Resample with MH − 50 particles over 80 trials
           (a)                                1                                                             (b)                                1
                                            0.9                                                                                              0.9
                                            0.8                                                                                              0.8
                                            0.7                                                                                              0.7
                                            0.6                                                                                              0.6
                                            0.5                                                                                              0.5
                  mean strength estimate                                                                           mean strength estimate
                                            0.4                                                                                              0.4
                                            0.3                                                                                              0.3
                                            0.2                                                                                              0.2
                                            0.1                                                                                              0.1
                                              0                                                                                                0
                                           −0.1                                                                                             −0.1
                                           −0.2                                                                                             −0.2
                                           −0.3                                                                                             −0.3
                                           −0.4                                                                                             −0.4
                                           −0.5                                                                                             −0.5
                                           −0.6                                                                                             −0.6
                                           −0.7                                                                                             −0.7
                                           −0.8                                                                                             −0.8
                                           −0.9                                                                                             −0.9
                                            −1                                                                                               −1
                                                  0   10     20     30     40     50     60     70   80                                            0      10     20     30     40    50     60     70       80
                                                                     trial number                                                                                        trial number
Figure 3: The performance of our particle filter over time for (a) Model 3, and (b) Model 4. The strength estimates produced
by 50 particles for the generative-preventative data are plotted over trial number. The particles are represented as blue dots with
the size of each dot a non-linear transformation of a particle’s weight (for presentation purposes). The mean strength estimate
over the set of particles for each trial event is given as a black curve.
           (a)                                                                                              (b)
Figure 4: (a): The predictions of the modified ESS resampling particle filter on the contingency data from Dennis and Ahn
(2001), and (b): the modified ESS resampling with rejuvenation particle filter on the task of Collins and Shanks (2002). The
strength estimates produced for the generative-preventative (blue curves) and preventative-generative (red curves) versions of
each task are plotted against the number of particles used, together with a shaded fill showing the standard deviation.
Cheng, P. (1997). From covariation to causation: A causal power                                                causal induction. Cognitive Psychology, 51(4), 334–384.
  theory. Psychological Review, 104, 367-405.                                                                Levy, R., Reali, F., & Griffiths, T. (2009). Modeling the effects
Chopin, N. (2002). A sequential particle filter method for static                                              of memory on human online sentence processing with particle
  models. Biometrika, 89(3), 539-551.                                                                          filters. Advances in neural information processing systems, 21,
Collins, D., & Shanks, D. (2002). Momentary and integrative                                                    937–944.
  response strategies in causal judgment. Memory & Cognition,                                                Marr, D. (1982). Vision. San Francisco, CA: W. H. Freeman.
  30(7), 1138.                                                                                               Oaksford, M., & Chater, N. (Eds.). (1998). Rational models of
Daw, N., & Courville, A. (2008). The pigeon as particle filter.                                                cognition. Oxford: Oxford University Press.
  Advances in neural information processing systems, 20, 369–376.                                            Sanborn, A., Griffiths, T., & Navarro, D. (2006). A more ratio-
Dennis, M., & Ahn, W. (2001). Primacy in causal strength judg-                                                 nal model of categorization. In Proceedings of the 28th annual
  ments: The effect of initial evidence for generative versus in-                                              conference of the cognitive science society (pp. 726–731).
  hibitory relationships. Memory & Cognition, 29(1), 152.                                                    Schustack, M., & Sternberg, R. (1981). Evaluation of evidence in
Doucet, A., Freitas, N. de, & Gordan, N. (Eds.). (2001). Sequential                                            causal inference. Journal of Experimental Psychology: General,
  monte carlo methods in practice. Springer.                                                                   110(1), 101.
Griffiths, T., Chater, N., Kemp, C., Perfors, A., & Tenenbaum, J.                                            Shi, L., Feldman, N., & Griffiths, T. (2008). Performing Bayesian
  (2010). Probabilistic models of cognition: Exploring representa-                                             inference with exemplar models. In Proceedings of the 30th an-
  tions and inductive biases. Trends in Cognitive Science, 14, 357.                                            nual conference of the cognitive science society (pp. 745–750).
Griffiths, T., & Tenenbaum, J. (2005). Structure and strength in
                                                                                                          2955

