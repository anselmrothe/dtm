UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Onset of Syntactic Bootstrapping in Word Learning: Evidence from a Computational
Study
Permalink
https://escholarship.org/uc/item/0653c6qv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Alishahi, Afra
Pyykkonen, Pirita Pirita
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

The Onset of Syntactic Bootstrapping in Word Learning: Evidence from
                                             a Computational Study
                               Afra Alishahi                                      Pirita Pyykkönen
               Computational Linguistics and Phonetics              Computational Linguistics and Phonetics
                      Saarland University, Germany                        Saarland University, Germany
                      afra@coli.uni-saarland.de                          pirita@coli.uni-saarland.de
                           Abstract                              their language yet. A continuous picture of the develop-
                                                                 mental path of word learning is lacking.
   The syntactic bootstrapping hypothesis suggests that             In this paper, we propose a novel approach to studying
   children’s verb learning is guided by the structural cues
   that the linguistic context provides. However, the onset      this problem. We use an existing computational model of
   of syntactic bootstrapping in word learning is not well       early verb learning which incrementally learns syntactic
   studied. To investigate the impact of linguistic infor-       constructions of language from usage data. We adapt this
   mation on word learning during early stages of language
   acquisition, we use a computational model of learning         model to the task of identifying target words in novel sit-
   syntactic constructions from usage data, and adapt it to      uations given different sets of (perceptual and linguistic)
   the task of identifying target words in novel situations.     cues. Our results show that having access to linguistic in-
   Our results show that having access to linguistic infor-
   mation significantly improves performance in identifying      formation significantly facilitates identifying verbs in later
   verbs (but not nouns) in later stages of learning, yet no     stages of learning, but no such effect is observed at the
   such effect can be observed in earlier stages.                earlier stages. For identifying nouns, additional linguistic
                                                                 information does not affect performance at all.
                       Introduction
                                                                 Time Course of Syntactic Bootstrapping
Learning verbs is a challenging task for young children:
their early vocabulary contains many more nouns than             Several preferential-looking studies have shown that chil-
verbs, and they learn new nouns easier than new verbs            dren are sensitive to the structural regularities of lan-
of the same frequency (e.g., Imai et al., 2005; Waxman,          guage from a very young age, and that they use these
2006). The acquisition of nouns is mainly attributed to          structural cues to find the referent of a novel word (e.g.,
cross-situational evidence, or regularities across different     Naigles & Hoff-Ginsberg, 1995; Gertner et al., 2006). In a
situations in which a noun is used (Quine, 1960). In con-        typical setup, children are given more than one interpreta-
trast, learning verbs seems to depend on the syntactic           tion for an utterance (e.g., different activities displayed on
frames that they appear in. It has been suggested that           parallel screens), and their looking behaviour reveals their
children draw on syntactic cues that the linguistic context      preferred interpretation. However, these studies cannot
provides in verb learning, a hypothesis known as syntactic       compare the impact of different cues in word learning,
bootstrapping (Gleitman, 1990). According to this view,          since the same type of input is available to subjects in
verbs are learned with a delay because the linguistic in-        different conditions.
formation that supports their acquisition is not available          In contrast, HSP studies manipulate the number and
during the early stages of language acquisition.                 type of cues that subjects receive for performing a task
   To investigate the impact of linguistic and extralinguis-     across conditions, and thus evaluate the impact of each
tic cues in identifying words, Gillette et al. (1999) pro-       set of cues. In their influential study, Gillette et al. (1999)
posed the Human Simulation Paradigm (HSP): adult par-            provided their adult subjects with various combinations
ticipants watch videos of caregivers interacting with their      of visual cues (videos), a list of co-occurring words, the
toddlers, and are asked to identify target words marked          syntactic pattern of the sentence, and the full transcript
by a beep. Videos are displayed without sound, and sub-          of the narration. Their findings and those of later studies
jects are provided with different degrees of information         have consistently shown that the more linguistic infor-
about the linguistic context of the target verbs. Various        mation adult subjects receive, the more accurately they
HSP studies have shown that having access to linguis-            identify missing verbs.
tic and structural cues significantly improves the perfor-          Piccin & Waxman (2007) used HSP for studying seven-
mance of adults in identifying verbs. Piccin & Waxman            year-olds as well as adults. Subjects in each age group
(2007) adopted the HSP paradigm for testing school-age           were randomly assigned to either ‘no linguistic informa-
children, and showed that children also rely on linguistic       tion’ (-LI) or ‘full linguistic information’ (+LI) condition.
information for identifying verbs, but their performance         In the -LI condition, participants heard no audio other
is inferior to adults. These findings hint at a gradual de-      than beeps indicating the target words. In the +LI con-
velopment of syntactic bootstrapping, but it is uncertain        dition, participants heard all the surrounding speech as
whether the same effect can be observed in much younger          well as the beeps. After watching each clip, subjects were
children who have not mastered the syntactic structure of        asked to guess the target word (a noun or a verb). Their
                                                             587

results show a similar pattern of behaviour for adults         word-concept mappings as well as for learning syntactic
and children: In the -LI condition, all subjects identified    constructions. Ideally, we need a computational model of
nouns more successfully than verbs. In the +LI condition,      syntactic bootstrapping to draw on such usage-based in-
linguistic information significantly improved the identifi-    formation in order to acquire form-meaning associations
cation of verbs (but not nouns) by children as well as         at word and sentence levels.
adults. The performance of both age groups was compa-             We investigate the time course of using syntax in
rable in the -LI condition, but adults significantly outper-   word learning through computational simulation, using
formed 7-year-olds in the +LI condition. That is, adult        the construction learning model of Alishahi & Stevenson
subjects were more successful in incorporating linguistic      (2010). The model uses Bayesian clustering for learn-
information in the task of identifying verbs.                  ing the allowable frames for each verb, and their group-
   Due to the nature of the word guessing task, HSP is not     ing across verbs into constructions. Each frame includes
suitable for very young children. Therefore, it is yet un-     the conceptual properties of an event and its participants
known whether linguistic information can facilitate word       (the cross-situational evidence), and the linguistic prop-
learning in the very early stages when the acquisition of      erties of the utterance that accompanies the observed
syntax is still in progress. Computational modeling is         event. A construction is a grouping of frames which share
an appropriate tool for tackling this problem: it allows       form-meaning associations; these groupings typically cor-
us to examine the time course of word learning and the         respond to general constructions in the language such as
contribution of linguistic input from the very beginning.      intransitive, transitive, and ditransitive.
Existing Computational Models                                     By detecting similar frames and clustering them into
                                                               constructions, the model forms probabilistic associations
Many computational models have demonstrated that               between syntactic positions of arguments with respect to
cross-situational learning is a powerful mechanism for         the verb, and the conceptual properties of the verb and
mapping words to their correct meanings and explain-           the arguments. These associations can be used in vari-
ing several behavioural patterns in children (e.g., Siskind,   ous language tasks where the most probable value for a
1996; Fazly et al., 2010). These models ignore the syn-        missing feature must be predicted based on the available
tactic properties of utterances and treat them as unstruc-     features. We simulate HSP in this fashion, where the
tured bags of words. Probabilistic models of Yu (2006)         most probable values for a missing head predicate (verb)
and Alishahi & Fazly (2010) integrate syntactic categories     or an argument (noun) are predicted based on the (per-
of words into a model of cross-situational learning and        ceptual and linguistic) information cues available in the
show that this type of information can improve the over-       current scene, using the acquired constructions.
all performance. In these models, perfect categories are
                                                                  The following sections review the model and describe
assumed to be formed prior to cross-situational learning.
                                                               the simulation of the word identification task.
   There are only a few computational models that explic-
itly study the role of syntax in word learning. Maurits et     Input and Frame Extraction
al. (2009) investigate the joint acquisition of word mean-
ing and word order using a batch model. This model is          The input to the learning process is a set of scene-
tested on an artificial language with a simple relational      utterance pairs that link a relevant aspect of an observed
structure of word meaning, and limited built-in possi-         scene (what the child perceives) to the utterance that de-
bilities for word order. The Bayesian model of Niyogi          scribes it (what the child hears). From each input pair,
(2002) simulates the syntactic and semantic bootstrap-         our model extracts a frame, containing the following form
ping effects in verb learning (i.e. drawing on syntax for      and meaning features:
inducing the semantics of a verb, and using semantics for      • Head words for the main predicate (i.e., verb) and its
narrowing down possible syntactic forms in which a verb           arguments (i.e., nouns or pronouns).
can be expressed). This model relies on extensive prior        • Syntactic pattern, or the word order of the utterance.
knowledge about the associations between syntactic and         • Number of arguments that appear in the utterance.
semantic features, and is tested on a toy language with
                                                               • Basic (conceptual) characteristics of the event (or
very limited vocabulary and syntax. None of these mod-
                                                                  verb), e.g., {cause,change,rotate,. . . }.
els investigate the time course of syntactic bootstrapping
                                                               • Conceptual properties of the arguments which are in-
and the differences between learning verbs and nouns.
                                                                  dependent of the event that the argument participates
   Overview of our Computational Model                            in, e.g., {woman,adult,person,. . . }.
In a typical language learning scenario, a child observes      • Event-based properties that each argument takes on
an event which involves a number of participants, and             in virtue of how it participates in the event, e.g.,
at the same time hears a natural language utterance de-           {moving,volitional,. . . }.
scribing the observed scene. Such scene-utterance pair-        In the Experimental Results section, we explain the se-
ings are the main source of input for the acquisition of       lection of semantic properties in our simulations.
                                                           588

Learning Constructions                                              Identifying Nouns and Verbs
Each extracted frame is input to an incremental Bayesian            In our model, language use is a prediction process in
clustering process that groups the new frame together               which unobserved features in a frame are set to the most
with an existing group of frames—a construction—that                probable values given the observed features. For exam-
probabilistically has the most similar properties to it. If         ple, sentence production predicts the most likely syntactic
none of the existing constructions has sufficiently high            pattern for expressing an intended meaning, which may
probability for the new frame, then a new construction is           include semantic properties of the arguments and/or the
created, containing only that frame.                                predicate. In comprehension, semantic elements may be
   Adding a frame F to construction k is formulated as              inferred from a word sequence.
finding the k with the maximum probability given F :                   The probability of an unobserved feature i displaying
           BestConstruction(F ) = argmax P (k|F )             (1)   value j given other feature values in a partial frame F is
                                            k                       estimated as             X
where k ranges over the indices of all constructions, with                       Pi (j|F ) =     Pi (j|k)P (k|F )            (6)
index 0 representing recognition of a new construction.                                       k
Using Bayes rule, and dropping P (F ) which is constant                                    =
                                                                                             X
                                                                                                 Pi (j|k)P (k)P (F |k)
for all k:
                        P (k)P (F |k)                                                         k
            P (k|F ) =                 ∼ P (k)P (F |k)        (2)   The conditional probabilities P (F |k) and Pi (j|k) are de-
                            P (F )
                                                                    termined as in the learning module. Ranging over the
The prior probability P (k) indicates the degree of en-
                                                                    possible values j of feature i, the value of an unobserved
trenchment of construction k, and is given by the relative
                                                                    feature can be predicted by maximizing Pi (j|F ):
frequency of its frames over all observed frames. The pos-
terior probability of a frame F is expressed in terms of                         BestValuei (F ) = argmax Pi (j|F )          (7)
                                                                                                        j
the individual probabilities of its features, which we as-
sume are independent, thus yielding a simple product of                Identifying a target verb as in HSP can be simulated
feature probabilities:                                              as finding the head verb j with the highest Pverb (j|F ), or
                                   Y                                estimating BestValueverb (F ). Here F is a partial frame
                 P (F |k) =                 Pi (j|k)          (3)
                                                                    which can include only the semantic features, or addi-
                             i∈Features(F )
                                                                    tional linguistic and syntactic features. Similarly, identi-
where j is the value of the ith feature of F , and Pi (j|k) is      fying a target noun which corresponds to an argument i
the probability of displaying value j on feature i within           in a scene is modeled as estimating BestValuenouni (F ).
construction k. This probability is estimated using a
smoothed version of this maximum likelihood formula:                                Experimental Results
                                  counti (j, k)                     We use our computational model to investigate the role
                     Pi (j|k) =                               (4)
                                      nk                            of linguistic input in learning verbs and nouns. Following
where nk is the number of frames participating in con-              Piccin & Waxman (2007), we pursue three main goals in
struction k, and counti (j, k) is the number of those with          our experiments. First, to simulate the task of identifying
value j for feature i.                                              a target word in the presence of visual stimuli, and to
   For single-valued features (head words, number of ar-            study the impact of linguistic input on performance.
guments, syntactic pattern), counti (j, k) is calculated by         Second, to investigate whether verbs benefit more than
simply counting those members of construction k whose               nouns from linguistic cues. Third, to examine the role of
value for feature i exactly matches j. However, for fea-            linguistic input in identifying verbs versus nouns in early
tures with a set value (semantic properties of the verb and         stages of learning.
the arguments), counting the number of exact matches
between the sets is too strict, since even highly similar           Factors and conditions. We model different factors in
words very rarely have the exact same set of properties.            the study of Piccin & Waxman (2007) as follows:
We instead assume that the members of a set feature are
independent of each other, and calculate the probability            • Word category: we simulate the identification of
of displaying a set sj on feature i in construction k as               a target verb and a target noun as estimating
                    1 Y                      Y                         BestValueverb (F ) and BestValuenouni (F ) respectively,
   Pi (sj |k) =              Pi (j|k) ×             Pi (¬j|k) (5)      based on Eqn. (7).
                 |S(i)| j∈s
                           j            j∈S(i)−sj                   • No (-LI) vs. full (+LI) linguistic information: the in-
Pi (j|k) and Pi (¬j|k) are estimated as in Eqn. (4) by                 formation cues available to subjects are reflected by the
counting members of construction k whose value for fea-                included features in partial frame F in equations (6)
ture i does or does not contains j. The product is rescaled            and (7). In the -LI condition, included features are the
by the length of S(i), which is the superset of all the val-           properties of the event and the conceptual and event-
ues that feature i can take.                                           based properties of the arguments (observable from a
                                                                589

   muted clip). In the +LI condition, the following fea-         world experience of children. However, our dataset has
   tures are also included: number of arguments, the head        the same statistical characteristics as the input data that
   words of the main verb and the arguments (except for          children receive, and therefore we hope that the general
   the target word), and the syntactic pattern (available        patterns observed in our experiments are representative
   from the narration of the clip).                              of the developmental patterns in children.
• Age groups: we train our model on a set of scene-
   utterance pairs before evaluating it on a word iden-          Linguistic Input and Identifying Words
   tification task. The age of the model is determined by        In the experiments reported below, we simulate the iden-
   its exposure to input data prior to performing the task.      tification of nouns and verbs in the -LI and +LI condi-
   We simulate different age groups by varying the size of       tions across different simulations, where each simulation
   the training data.                                            represents a subject. For each simulation, we randomly
Evaluation. In evaluating the model when identifying             generate a set of 500 training items and 30 test items,
target words in a test set, we use the following criteria:       using the input generation lexicon discussed above. We
                                                                 monitor the performance of the model in intervals: af-
• Absolute accuracy: the number of test items for which          ter processing every 10 training items, we independently
   BestValuei (F ) in Eqn. (7) returns the correct value for     predict the head verb and a randomly selected argument
   the target word i.                                            noun for each of the test items, and evaluate the predic-
• Probabilistic accuracy: the sum of the probabilities           tions using the evaluation measures mentioned above.
   Pi (target|F ) for each target (the correct answer) in the       A simple linear model of probabilistic accuracy with
   test set, where i is the focus feature and Pi (target|F )     input type and age as predictors reached a significant
   is calculated using Eqn. (6). This probability reflects       interaction between the predictors for both nouns
   the confidence of the model in predicting target.             (F = 3.983, p = 0.003) and verbs (F = 7.375, p = 0.000).
• Improvement: the gain achieved by using our pre-               The same was observed in the improvement data (nouns:
   diction model instead of a simple frequency baseline:         F = 6.376, p = 0.000 ; verbs: F = 10.249, p = 0.000). That
   (Pi (target|F ) − baseline(target))/Pi (target|F ). The       is, different age groups behave differently when process-
   baseline only relies onP the relative frequency of each       ing linguistic input. In order to further examine the
   head word (freq(w)/ w0 freq(w0 ), where freq(w) is the        effect of linguistic information throughout development,
   frequency of w in the input).                                 we conducted separate analyses for each word category
                                                                 across all age groups, as reported below.1
Data. We used the Brown corpus of the CHILDES
database (MacWhinney, 2000) for constructing the input
to our model. We extracted the 20 most frequent verbs            Verbs. Figure 1 shows the absolute accuracy and im-
in mother’s speech to each of Adam, Eve, and Sarah,              provement of identifying verbs for 30 test items, in in-
and selected 13 verbs from those in common across these          tervals of 10 over a total of 500 input items, averaged
three lists. We constructed an input-generation lexicon          over 50 simulations. (The improvement and probabilis-
based on these 13 verbs, including their total frequency         tic accuracy plots show a very similar trend.) As can be
among the three children. We also assigned each verb a           seen from the top panel, a target verb can be identified
set of possible argument structure frames and their rel-         more accurately when the model has access to linguistic
ative frequencies, which were manually compiled by the           information about the co-occurring words and syntactic
examination of 100 randomly sampled uses of a verb from          pattern of the utterance. The bottom panel shows the
all conversations of the same three children. Finally, from      same pattern, and further emphasizes the benefit of using
the sample verb usages, we extracted a list of head words        perceptual and linguistic features in our model compared
(total 259) that appeared in each argument position of           to predicting verbs based on their frequency of observa-
each frame, and added these to the lexicon.                      tion in the input data. Performance is boosted as early
   For each noun in the lexicon, we extracted a set of lex-      as processing 100 training items (A100), a stage at which
ical properties from WordNet (Miller, 1990) as follows.          relatively robust constructions are formed by the model.
We extracted all the hypernyms for the first sense of each       The gap between the accuracy and improvement curves
word, and added one member from each hypernym synset                 1
                                                                       The probabilistic accuracy and improvement were ana-
to the list of its properties. For each verb frame, we man-      lyzed with linear mixed models with the condition (+LI, -LI)
ually compiled a set of semantic primitives for the event        as a fixed factor and subjects and items as a crossed-random
as well as a set of event-based properties for each of the       factor in order to allow by-subject and by-item variation in one
                                                                 model. Estimates (Est) report the regression coefficients for
arguments. We chose these properties from what we as-            the fixed effect, and p-values for estimates were obtained us-
sumed to be known to the child at the stage of learning          ing Markov-Chain Monte Carlo (MCMC) sampling with 10000
being modeled.                                                   replications. The absolute accuracy was analyzed with a lo-
                                                                 gistic mixed model with the condition as a fixed factor and
   Due to the limited lexicon, the scale of the learning         subjects and items as a crossed-random factor. P-values for
problem in our experiments is much smaller than the real-        estimates were obtained from z-statistics.
                                                             590

                                          0.40                                                                                          0.40
               Verb Absolute Accuracy                                                                        Noun Absolute Accuracy
                                          0.35                                                                                          0.35
                                          0.30                                                                                          0.30
                                          0.25                                                                                          0.25
                                                                                  −LI                                                                                           −LI
                                                                                  +LI                                                                                           +LI
                                                 0   100   200       300   400   500                                                           0   100   200       300   400   500
                                                             Input Items                                                                                   Input Items
                                        0.020                                                                                         0.020
          Verb Improvement                                                                              Noun Improvement
                                        0.018                                                                                         0.018
                                        0.016                                                                                         0.016
                                        0.014                                                                                         0.014
                                                                                 −LI                                                                                           −LI
                                        0.012                                                                                         0.012
                                                                                 +LI                                                                                           +LI
                                                 0   100   200      300    400   500                                                           0   100   200      300    400   500
                                                            Input Items                                                                                   Input Items
Figure 1: Absolute accuracy and improvement of identi-                                        Figure 2: Absolute accuracy and improvement of identi-
fying verbs, averaged over 50 simulations.                                                    fying nouns, averaged over 50 simulations.
in the -LI and +LI conditions widens slowly but consis-                                       More importantly, exploiting linguistic input significantly
tently as the model processes more input items; that is,                                      facilitates identifying verbs, and older subjects can use
the model can use linguistic input more efficiently as it                                     this information more efficiently than younger ones. How-
ages. In both age groups, the probabilistic accuracy was                                      ever, identifying nouns does not benefit from additional
positively affected by the presence of linguistic context                                     linguistic input.2 The gradual improvement of verb iden-
(A100: Est = 0.041, pMCMC = 0.0001; A500: Est = 0.053,
                                                                                              tification in the +LI condition brings us back to our orig-
pMCMC = 0.0001). Same effect was also found in improve-
                                                                                              inal question: when does syntax begin to play a role in
ment measurements (A100: Est = 0.041, pMCMC = 0.000;                                          verb identification? We address this issue next.
A500: Est = 0.053, pMCMC = 0.0001). The effect on abso-                                       Onset of Syntactic Bootstrapping
lute accuracy was significant only in the last age group
                                                                                              In order to investigate the contribution of linguistic in-
(Est = 0.157, p = 0.040) and marginally significant for age
                                                                                              put in identifying words in the earlier stages of learning,
group A200 (Est = 0.108, p = 0.161).
                                                                                              we zoom in on the performance of the model during pro-
   The behaviour of the model at the A100 stage is                                            cessing the first 100 training items. Figure 3 shows the
similar to that of the 7-year-old subject group in                                            absolute accuracy of predicting verbs and nouns for 30
Piccin & Waxman (2007), whereas the A500 stage is                                             test items, in intervals of 5 over the course of processing
more similar to their adult subject group. Note that                                          100 input items, averaged over 50 simulations. (The im-
due to the small number of verbs in our lexicon and                                           provement plots are not included due to lack of space.)
their relatively restricted syntactic behaviour, our model                                    The curves show an interesting trend: for both verbs and
learns much more efficiently from a small training corpus.                                    nouns, linguistic information does not help at first. For
                                                                                              verbs, the positive effect of +LI was absent in the earliest
Nouns. Figure 2 shows the absolute accuracy and im-                                           age group for both probabilistic accuracy and improve-
provement of identifying nouns, averaged over (the same)                                      ment (A10: p > 0.6), and for absolute accuracy as well
50 simulations. These results show a different pattern                                        (p > 0.1). However, the accuracy curve in the +LI condi-
than those of verbs: the model outperforms the fre-                                           tion takes over the -LI condition around A50, and shows
quency baseline, but there is no clear advantage of us-                                       significant influence of linguistic input in both probabilis-
ing linguistic input. In the oldest age group (A500),                                         tic accuracy (Est = 0.028, pMCMC = 0.001) and improve-
neither probabilistic accuracy nor improvement were af-                                       ment (Est = 0.028, pMCMC = 0.0001), but not in absolute
fected by +LI and -LI manipulation (p > 0.2). Inter-                                          accuracy (p > 0.6).
estingly, in the age group A100, both probabilistic ac-                                          The results for nouns are more surprising: there is a
curacy (Est = −0.019, pMCMC = 0.0294) and improvement                                         significant negative effect of +LI for the A10 and A50 age
(Est = −0.011, pMCMC = 0.040) were negatively affected by                                     groups in both probabilistic accuracy (A10: Est = −0.036,
the linguistic context. The absolute accuracy showed no                                       pMCMC = 0.0008; A50: Est = −0.028, pMCMC = 0.0018)
effects for the oldest age group (p > 0.6) and a marginal                                     and improvement (A10: Est = −0.031, pMCMC = 0.0001;
effect in A100 (Est = −0.151, p = 0.054).                                                     A50: Est = −0.021, pMCMC = 0.0008). Absolute accuracy
   Our results are in line with the findings of Piccin &                                         2
                                                                                                   Due to the incomparable number of verbs and nouns in our
Waxman (2007): older subjects (A500) perform better                                           lexicon and different methods of identifying them, we cannot
than younger ones (A100) in identifying verbs and nouns.                                      directly compare performance in predicting verbs and nouns.
                                                                                        591

                                          0.45
                                                                                             training corpus might also play a role: it contained many
                Verb Absolute Accuracy
                                          0.40
                                          0.35
                                                                                             more nouns (259) than verbs (13), and most verbs were
                                          0.30
                                                                                             not restrictive. Therefore, the same nouns can appear
                                          0.25
                                                                                             as arguments of different verbs, or many different nouns
                                          0.20
                                                                                             can be potential candidates for a verb argument, yielding
                                                                                 −LI
                                          0.15
                                                                                 +LI         several correct answers for a noun-guessing task.
                                                 20     40           60    80   100             It should be noted that the cross-situational scenario in
                                                             Input Items
                                                                                             the setup of our model is not realistic as there is no refer-
                                         0.45
                                                                                             ential uncertainty in our data (i.e., there are no referents
          Noun Absolute Accuracy
                                         0.40
                                                                                             in the scene which are not mentioned in the utterance),
                                         0.35
                                         0.30
                                                                                             an issue we plan to address in the future. But it only
                                         0.25
                                                                                             highlights our point that syntactic bootstrapping can fa-
                                         0.20
                                                                                             cilitate verb learning even in low-ambiguity situations,
                                                                                −LI
                                         0.15
                                                                                +LI          given that the learner has been exposed to enough input
                                                 20     40          60     80   100          to form a reliable knowledge of the structure of language.
                                                         Input Items
Figure 3: Average absolute accuracy of identifying verbs                                                             References
and nouns in early stages of learning.                                                       Alishahi, A., & Fazly, A. (2010). Integrating syntactic knowl-
                                                                                                edge into a model of cross-situational word learning. In
showed a significant negative effect of +LI in A10 (Est =                                       Proceedings of CogSci’10.
−0.172, p = 0.039) and a marginal effects in A50 (Est =                                      Alishahi, A., & Stevenson, S. (2010). A computational
−0.148, p = 0.062).                                                                             model of learning semantic roles from child-directed lan-
                                                                                                guage. Language and Cognitive Processes, 25 (1), 50–93.
                                                                                             Fazly, A., Alishahi, A., & Stevenson, S. (2010). A probabilis-
                                                      Discussion                                tic computational model of cross-situational word learning.
                                                                                                Cognitive Science, 34 (6), 1017–1063.
The results of our computational simulations replicate the                                   Gertner, Y., Fisher, C., & Eisengart, J. (2006). Learning
experimental findings of Piccin & Waxman (2007) that                                            words and rules: Abstract knowledge of word order in early
syntactic information boosts the identification of verbs                                        sentence comprehension. Psych. Science, 17 (8), 684–691.
by adults and young children. Our results also suggest                                       Gillette, J., Gleitman, H., Gleitman, L., & Lederer, A. (1999).
                                                                                                Human simulations of vocabulary learning. Cognition,
that the boosting effect comes into play with a delay,                                          73 (2), 135–176.
and only after enough input data is processed and a                                          Gleitman, L. (1990). The structural sources of verb meanings.
relatively stable knowledge of syntactic constructions is                                       Language Acquisition, 1 , 135–176.
formed. Our computational approach allows us to inves-                                       Imai, M., Haryu, E., & Okada, H. (2005). Mapping novel
                                                                                                nouns and verbs onto dynamic action events: Are verb
tigate word identification throughout various stages of de-                                     meanings easier to learn than noun meanings for Japanese
velopment, and examine syntactic bootstrapping for age                                          children? Child Development, 76 (2), 340–355.
groups which cannot be easily studied in experimental                                        MacWhinney, B. (2000). The CHILDES project: Tools for
settings. Specifically, our model predicts that very young                                      analyzing talk. Lawrence Erlbaum Associates Inc, US.
                                                                                             Maurits, L., Perfors, A. F., & Navarro, D. J. (2009). Joint
children’s verb learning might not be modulated by lin-                                         acquisition of word order and word reference. In Proceedings
guistic information, even though a significant impact can                                       of CogSci’09.
be found in the later stages of development. This pre-                                       Miller, G. (1990). WordNet: An on-line lexical database.
diction is in line with previous suggestions that gener-                                        International Journal of Lexicography, 17 (3).
alization of syntactic information takes time to manifest                                    Naigles, L., & Hoff-Ginsberg, E. (1995). Input to verb learn-
                                                                                                ing: evidence for the plausibility of syntactic bootstrapping.
(e.g., Gillette et al., 1999). Importantly, this prediction                                     Developmental Psychology, 31 (5), 827–37.
is not inconsistent with findings on the sensitivity of very                                 Niyogi, S. (2002). Bayesian learning at the syntax-semantics
young children to syntax during comprehension (see Al-                                          interface. In Proceedings of CogSci’02.
ishahi & Stevenson (2010) for simulating such effects us-                                    Piccin, T., & Waxman, S. (2007). Why nouns trump verbs
                                                                                                in word learning: New evidence from children and adults in
ing the same computational model).                                                              the Human Simulation Paradigm. Language Learning and
   Our results make another (somehow surprising) pre-                                           Development, 3 (4), 295–323.
diction: linguistic context might have a negative effect on                                  Quine, W. (1960). Word and object. Cambridge: MIT Press.
identifying nouns during the early developmental stage.                                      Siskind, J. M. (1996). A computational study of cross-
                                                                                                situational techniques for learning word-to-meaning map-
The performance of our model in guessing nouns for the                                          pings. Cognition, 61 , 39–91.
younger age groups was poorer when the linguistic infor-                                     Waxman, S. R. (2006). Early word learning. In D. Kuhn &
mation was provided, and no effect on performance by                                            R. Siegler (Eds.), Handbook of child psychology. Wiley.
linguistic information was observed in later age groups.                                     Yu, C. (2006). Learning syntax–semantics mappings to boot-
This might be due to the fact that most early nouns                                            strap word learning. In Proceedings of CogSci’06.
refer to observable concepts, and are less dependent on
the structure of their linguistic context than verbs. Our
                                                                                       592

