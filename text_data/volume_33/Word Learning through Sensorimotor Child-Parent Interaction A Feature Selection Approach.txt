UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Word Learning through Sensorimotor Child-Parent Interaction: A Feature Selection Approach
Permalink
https://escholarship.org/uc/item/6qq402mq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Yu, Chen
Xu, Jun-ming
Zhu, Xiaojin
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

  Word Learning through Sensorimotor Child-Parent Interaction: A Feature Selection Approach
                           Chen Yu, Jun-Ming Xu* and Xiaojin Zhu* (chenyu@indiana.edu)
              Department of Psychological and Brain Sciences, and Cognitive Science Program, Indiana University
                                                     Bloomington, IN, 47405 USA
                               * Department of Computer Sciences, University of Wisconsin-Madison
                                                        Madison, WI, 53706 USA
                              Abstract                                    Previous studies have examined early word learning in
   This paper presents a computational model of word learning
                                                                      constrained   experimental tasks with only one or two objects
   with the goal to understand the mechanisms through which           in view. The adult partner (usually the experimenter)
   word learning is grounded in multimodal social interactions        focuses on the child and on effective teaching, and provides
   between young children and their parents. We designed and          clear  and repeated signals of her attention to the object
   implemented a novel multimodal sensing environment                 being named (E.g. Baldwin, 1993; Tomasello & Akhtar,
   consisting of two head-mounted mini cameras that are placed        1995). In this way, the attentional task is simple, and easily
   on both the childâ€™s and the parentâ€™s foreheads, motion             described in discrete and categorical terms (the attended
   tracking of head and hand movements and recording of               object vs. the distractor). These contexts are not at all like
   caregiverâ€™s speech. Using this new sensing technology, we          the real world in which word learning is embedded in a
   captured the dynamic visual information from both the
   learnerâ€™s perspective and the parentâ€™s viewpoint while they
                                                                      stream of activity -- in which parents both react to and
   were engaged in a free-play toy-naming interaction. We next        attempt to control toddler behaviors and in which toddlers
   implemented various data processing programs that                  react to, direct, and sometimes ignore parents as they pursue
   automatically extracted visual, motion and speech features         their own goals.
   from raw sensory data. A probabilistic model was developed            To truly understand the mechanisms of word learning, we
   that can predict the childâ€™s learning results based on             need to focus on more micro-level behaviors as they unfold
   sensorimotor features extracted from child-parent interaction.     in real time in the richly varying and dynamically complex
   More importantly, through the trained regression coefficients      interactions of children and their mature partners in more
   in the model, we discovered a set of perceptual and motor          naturalistic tasks (such as toy play). Further, whereas the
   patterns that are informatively time-locked to words and their
   intended referents and predictive of word learning. Those
                                                                      studies at the macro-level clearly demonstrate many
   patterns provide quantitative measures of the roles of various     intelligent behaviors in infant word learning, they have not
   sensorimotor cues that may facilitate word learning, which         yet  led to a formal account of the underlying mechanisms.
   sheds lights on understanding the underlying real-time             Thus, we want to know not only that learners use various
   learning mechanisms in child-parent social interactions.           cues in social interaction to facilitate learning (see a good
   Keywords: computational modeling, word learning,
                                                                      example of macro-level modeling by Frank, Goodman &
   embodied cognition, perception and action.                         Tenenbaum, 2009), but also exactly how they do so in terms
                                                                      of the real-time processes in the naturalistic tasks wherein
                          Introduction                                everyday language learning must take place.
                                                                          To this end, we developed a novel paradigm with two
Just as in many other cognitive learning tasks, a critical            critical components. First, we developed a multisensory
problem in word learning is the uncertainty and ambiguity             experimental environment to capture multimodal data with
in the learning environment â€“ young word learners need to             the goal to study the dynamics of child-parent social
discover correct word-referent mappings among many                    interactions, that ultimately lead to word learning, at the
possible candidate words and many possible candidate                  sensorimotor levels â€“ in the bodily gestures and as well as
referents from potentially many objects that are                      momentary visual and auditory perception of the
simultaneously available. At the macro level, we know a               participants. We developed various signal processing tools
great deal about object name learning and how it seems to             to automatically annotate such rich dataset. Second, we
be characterized by attentional biases to attend to the shape         proposed and implemented a new probabilistic model based
of the whole object (Landau, Smith & Jones, 1998), by                 on state-of-the-art machine learning techniques to discover
conceptual biases that make some kinds of word-meaning                the perceptual and motor patterns that are informatively
mappings more likely than others (Markman, 1989), and by              time-locked to words and their intended referents and
all sorts of linguistic bootstraps whereby children use the           predictive of word learning. In the following sections, we
words they already know to help figure out new meanings               first describe our experimental setup and data. We then
(Gleitman, 2005). But we know very little about how any of            introduce our model of word learning. After that, we present
this works in real time and in the cluttered context of the           the results from a set of simulation studies. Finally, we
real world interactions of toddlers and parents, contexts             offer some general discussions and conclude our work.
typically characterized by many interesting objects, many
shifts in attention by each participant, and many goals                                       Experiment
(beyond teaching and learning words).                                 As shown in Figure 1, the naturalistic interaction of parents
                                                                      and toddlers in the task of table-top toy play was recorded
                                                                  1601

by three cameras from different perspectives: 1) A                    parent sat opposite each other at a small table and the parent
lightweight mini camera mounted on a sports-headband and              was instructed to interact naturally with the child, engaging
placed low on the forehead of the child provided                      their attention with the toys while teaching the words for
information about the scene from the child learnerâ€™s point of         them.
view. This is a particularly important and novel component            Parent-child free play session. The instructions given to the
of our set-up. The angle of the camera is adjustable, and has         parent were to take all three objects from one set, place
a visual field of approximately 90o. 2) A ceiling camera              them on the table, play with the child and after hearing a
provided a top-down third-person view, allowing a clear               command from the experimenters, remove the objects in this
observation of exactly what was on the table at any given             trial and move to the next set to start the next trial. Parents
moment (mostly the participantsâ€™ hands and the objects                were given the names of the objects that they were to use
being played with). 3) Another head-mounted camera                    and were instructed to teach the children those object
provided the parentâ€™s viewpoint. In addition, our                     names. However, there was no special instruction as to what
multimodal system recorded participantsâ€™ body movements               the parents had to say or what they had to perform, just that
through a motion tracking system as well as the parentâ€™s              they were to engage their child. All the names were artificial
speech through a headset.                                             words. There were a total of four trials with each object set
Participants. The target age period for this study was 18 to          repeated twice, each about 1 minute long. The interaction
20 months. We invited parents in the Bloomington, Indiana             between parent and child lasted between 4 and 7 minutes
area to participate in the experiment. 13 dyads of parent and         and was free-flowing in form.
child were part of the study (5 male and 7 female). 7                 Name-comprehension test. After the period of free
additional children were not included because of failure to           interaction, the experimenter tested the childâ€™s
keep the head camera on. For the child participants                   comprehension of the object name for each of the 6 objects.
included, the mean age was 19.6, ranging from 17 to 20                This was done by placing three objects out of reach of the
months. All participants were white and middle-class.                 child about 30 inches apart, one to the left of the child, one
Stimuli. Parents were given two sets, with three toys in each         in the middle, and one to the right. The experimenter then
set, in this free-play task. The toys were rigid plastic objects      looked directly into the childâ€™s eyes, said the name of one of
of simple shapes and were painted with one primary color.             the objects and asked for it. For this portion of the
Each set had a red, a green and a blue object.                        experiment, a camera was focused on the childâ€™s eyes.
Procedure. The task was a common one in the everyday                  Direction of eye gaze â€“ looking to the named object when
lives of children and parents â€“ to take turns in jointly acting       named â€“ was scored as indicating comprehension. These
on, attending to, and naming objects. This is a common                recorded eye movements were coded (with the sound off) by
context in which children learn names for things. The toys            a scorer naÃ¯ve to the purpose of the experiment. Each word
used in this experiment were novel items. The child and               was tested twice with a score ranging from 0, 1 to 2: A word
                                                                      was given a score 2 if the child selected the correct target in
                                                                      both testing trials of that word, score 1 if the child
                                                                      successfully selected the correct one only once and score 0
                                                                      if the child failed to select the correct one twice.
                                                                             Multimodal Data and Data Processing
                                                                      Given the multimodal data from child-parent interactions,
                                                                      we have developed various image and sensory processing
                                                                      tools to automatically annotate the data. This section briefly
                                                                      reviews our solutions to these problems. Technical details
                                                                      can be found in (Yu, Smith, Shen, Pereira, and Smith,2009).
                                                                      Video Processing The recording rate for each of the three
                                                                      cameras was 30 frames per second. In the preliminary
                                                                      studies, there were 3 toy-play trials (with different sets of
                                                                      toys), each lasting about 60 seconds. Thus we collected
                                                                      approximately 24,300 (30 Ã— 90 Ã— 3 Ã— 3) image frames from
                                                                      each interaction. The resolution of each image frame is
                                                                      720*480. We analyzed the image data in two ways: (1) At
 Figure 1. Our multimodal sensing system. The child and the           the pixel level, we used the saliency map model developed
 mother played with a set of toys at a table. Two mini cameras
                                                                      by Itti, Koch, & Niebur (1998) to measure which areas in an
 were mounted on the childâ€™s and the parentâ€™s forehead,
 respectively to collect visual information from two first-person
                                                                      image are most salient based on motion, intensity,
 views. A third camera mounted on the top of the table recorded       orientation and color cues. Itti et al.â€™s saliency map model
 the birdâ€™s eye view of the whole interaction. They also wore         applies bottom-up attention mechanisms to encode for
 motion sensors to track their head movements. A headset was          conspicuity (or ``saliency'') at every location in the visual
 used to record the parentâ€™s speech.                                  input. (2) At the object level, the goal was to automatically
                                                                  1602

extract visual information, such as the locations and sizes of
objects, hands, and faces, from sensory data in each of three
cameras. These are based on computer vision techniques,
and include three major steps. The combination of using
pre-defined simple visual objects and utilizing start-of-the-
art computer vision techniques resulted in high accuracy in
visual data processing. The technical details can be found in
(Yu, et al., 2009).
Motion data processing Six motion tracking sensors on
participantsâ€™ head and hands (3 sensors on each participant)
recorded 6 DOF of their head and hand movements at the
frequency of 240 Hz. Given the raw motion data from each
sensor, the primary interest in the current work was the
overall dynamics of body movements. We grouped the 6
DOF data vector into position {x, y, z} and orientation {h,
p, r}. We then developed a motion detection program that
computes the magnitudes of both position movements and
orientation movements. In addition, we manually annotated          Figure 2. Derived temporal variables from multimodal data. Left:
                                                                   the childâ€™s view image and saliency map. Right: the parentâ€™s view
which objects were in the childâ€™s or the parentâ€™s hands.
                                                                   image and saliency map. The naming of variables follows this
Speech processing We first segmented the continuous                standard:      C(hild)/P(arent)_A(ction)/V(ision)_meaning.    Eg.
speech stream into multiple spoken utterances based on             C_V_target_size means the size of the target object in the childâ€™s
speech silence. Next, we asked human coders to listen to the       view.
recording and transcribe the speech segments. From the             regression coefficients, our main goal was to infer which
transcriptions, we calculated the statistics of linguistic         features contribute to the learning outcome.
information, such as the size of vocabulary and the average           For notational simplicity, we begin by introducing our
number of words per spoken utterance. Moreover, we                 model in the case of a single parent-child interaction
extracted the onset and offset timestamps wherein an object        session, and only a single word was taught in that session.
name occurred in transcription and used them to define a           This will be generalized later. During the session the child is
naming event. In the next section, we will use these naming        given #ğ¸ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘  training naming events to learn the target
events to determine the learning patterns in visual and            word. These are encoded by some d-dimensional feature
motion data streams.                                               vectors ğ±1 â€¦ ğ± #ğ¸ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘  âˆˆ â„ğ‘‘ . As customary in machine
   As a result of our data processing, multiple heterogeneous      learning, we assume that each feature vector is augmented
time series were derived from multimodal raw data. In the          by an extra dimension with constant value 1 for bias. We
present study, a set of 28 temporal sequences were selected        define the â€œgainâ€ (from the child's perspective) from the ğ‘˜-
which covered a wide range of sensorimotor dynamics in             th training naming event as
child-parent interaction, from the childâ€™s visual perception,                                      ğ° âŠ¤ ğ±ğ‘˜
to the childâ€™s hand and head actions, to the parentâ€™s hand         where ğ° is a learning weight vector to be estimated. The
and head actions. Figure 2 illustrates the meanings of some        total gain for the word is the sum over training naming
temporal variables.                                                events:
                         The Model                                             âˆ‘#ğ¸ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘ 
                                                                                 ğ‘˜=1      ğ° âŠ¤ ğ±ğ‘˜ = ğ° âŠ¤ âˆ‘#ğ¸ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘ 
                                                                                                          ğ‘˜=1     ğ±ğ‘˜ â‰¡ ğ° âŠ¤ ğ—
We correlated the number of naming events for each object          where ğ— â‰¡ âˆ‘#ğ¸ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘ 
                                                                                   ğ‘˜=1     ğ± ğ‘˜ . Let ğ‘§ âˆˆ {0, 1} be the hidden binary
name with the score (0, 1 or 2) at test. We did not find a         variable indicating whether the child actually learns the
strong correlation between these two (r=0.1; p=0.28). The          word. We model ğ‘§ with a logistic function:
average number of naming events is 9 for a learned object                                                      1
name and 11 for an unlearned object name. Some object                           ğ‘ƒ(ğ‘§ = 1 âˆ£ ğ°, ğ—) =
                                                                                                       1 + exp (âˆ’ğ° âŠ¤ ğ—)
names that were provided just once or twice were actually          such that a larger total gain leads to a higher probability of
learned, while others labeled by parents 5 or 6 times were
                                                                   learning. We cannot observe ğ‘§ directly. Instead, #ğ‘‡ğ‘’ğ‘ ğ‘¡ test
not learned. This suggests that what matters were the
                                                                   events are conducted after training. In each test event, the
specific contexts where those object names were named,
                                                                   child had to choose the target object out of ğ‘š different
what both parents and children visually attended to at those
                                                                   objects (m=3 in our case). Let ğ‘¦ğ‘™ âˆˆ {0, 1} be the observed
moments, and what they were doing at that time.
                                                                   variable on whether the child succeeded on the ğ‘™-th test
   To discover those important sensorimotor features that led
                                                                   event, for ğ‘™ = 1 â€¦ #ğ‘‡ğ‘’ğ‘ ğ‘¡. We assume that if the child has
to successful learning through social interaction, we
                                                                   learned the word (ğ‘§ = 1), she would most likely pick the
developed a formal model that predicts word learning
                                                                   correct object (but there is a still probability that she may
results from multimodal features. Through the estimated
                                                                   not pick the correct answer even when z = 1). This
                                                                   variability is captured by:
                                                               1603

                                    ğ›¾,     ğ‘¦ğ‘™ = 1                     the child has to pick out the object corresponding to the
              ğ‘ƒ( ğ‘¦ğ‘™ âˆ£ ğ‘§ = 1 ) = ï¿½
                                  1 âˆ’ ğ›¾, ğ‘¦ğ‘™ = 0                       word from ğ‘š different objects.
where ğ›¾ is a parameter less than 1. If the child does not             We assume that all parent-child pairs share the same weight
learn the word (ğ‘§ = 0), we assume that she will randomly              vector ğ° âˆˆ â„ğ‘‘ . For parent-child pair ğ‘– on word ğ‘—, the gain
pick a test object with equal probability, resulting in               from learning experience ğ‘˜ is
                                   1/ğ‘š,       ğ‘¦ğ‘™ = 1                                                ğ° âŠ¤ ğ±ğ‘–ğ‘—ğ‘˜
           ğ‘ƒ( ğ‘¦ğ‘™ âˆ£ ğ‘§ = 0 ) = ï¿½
                               (ğ‘š âˆ’ 1)/ğ‘š, ğ‘¦ğ‘™ = 0                      For parent-child pair ğ‘– on word ğ‘— the total gain from learning
These assumptions therefore model the likely noise in                 experiences is
testing data.                                                                                  âˆ‘#ğ¸ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘ (ğ‘–ğ‘—)
                                                                                                 ğ‘˜=1         ğ° âŠ¤ ğ± ğ‘–ğ‘—ğ‘˜
   To make a Bayesian probabilistic model, we introduce a             In the same way, we can define ğ‘§ğ‘–ğ‘— and ğ‘ƒ(ğ‘¦ğ‘–ğ‘—ğ‘™ âˆ£ ğ‘§ğ‘–ğ‘— ). The
prior distribution on ğ°. Sparse ğ° (i.e., only a small number          different pairs are independent to each other, so the MAP
of features are relevant to learning) is preferred for                problem becomes
interpreting the model, but we are also interested in                     minğ‘‘ ğ›½1 ||ğ°||1 + ğ›½2 ||ğ°||22 +
including all related variables even they are pairwise                   ğ°âˆˆâ„
                                                                                 #Events(i)
correlated. Therefore, we employ the elastic net (Zou &               âˆ‘#Pairs
                                                                        i=1    âˆ‘j=1         log ï¿½âˆ‘zij =0,1 ğ‘ƒï¿½ğ‘§ğ‘–ğ‘— âˆ£ ğ°, ğ— ğ‘–ğ‘— ï¿½ğ‘ƒï¿½ ğ²ğ¢ğ£ âˆ£âˆ£ ğ‘§ğ‘–ğ‘— ï¿½ï¿½
Hastie, 2005), which corresponds to the prior,                        This optimization problem is non-convex and we optimize it
                             d
      ğ‘ƒ(ğ°) = â„(ğ›½1 , ğ›½2 ) ï¿½       exp (âˆ’ğ›½1 |ğ‘¤ğ‘“ | âˆ’ ğ›½2 ğ‘¤ğ‘“2 )            with the Constrained Concave Convex Procedure (CCCP)
                             f=1                                      (Yuille & Rangarajan, 2003).
where ğ›½1 and ğ›½2 are non-negative parameters which control
the tradeoff between prior and likelihood. The complete                                Simulations and Results
graphical model is given below.                                       Several parameters have to be set before we can identify
                                                                      interesting features with our model. First, in our model, we
                                                                      assume that the child may choose an incorrect object in the
                                                                      test events even when ğ’› = ğŸ and denote the probability as
                                                                      ğœ¸. However, there is no well-studied ğœ¸ we can use. ğœ·ğŸ and
                                                                      ğœ·ğŸ are the weights of regularizers, which control the
                                                                      tradeoff between fitness to data and model complexity. To
                                                                      set them appropriately, we first chose several candidate
                                                                      parameters and use cross-validation to choose the best
   We are now ready to state the modeling problem: Given              settings based on log-likelihood on the training set.
                                                                      Intuitively, ğœ¸ should be greater than 0.5 and therefore the
training naming events ğ±1 â€¦ ğ± #Events (ğ— = âˆ‘#ğ¸ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘ 
                                                ğ‘˜=1     ğ± ğ‘˜ ) and
                                                                      candidate set of {ğŸ. ğŸ”, ğŸ. ğŸ•, â€¦ , ğŸ. ğŸ} was chosen. We tested
test outcomes ğ² = (ğ‘¦1 â€¦ ğ‘¦#ğ‘‡ğ‘’ğ‘ ğ‘¡ )âŠ¤ (and hyper-parameters ğ›½1
                                                                      several different ğœ·ğŸ values {ğŸğŸâˆ’ğŸ’ , ğŸğŸâˆ’ğŸ‘ , â€¦ , ğŸğŸğŸ } to produce
and ğ›½2 ), what is the most likely weight coefficients ğ°? The
                                                                      different solutions with different levels of sparsity and
non-zero (and large magnitude) elements can then be
interpreted as the subset of features contributing to learning.       meanwhile the candidates of ğœ·ğŸ were evaluated with a
The hidden variable ğ‘§ is of less interest, and is integrated          larger but sparser grid {ğŸğŸâˆ’ğŸ” , ğŸğŸâˆ’ğŸ’ , â€¦ , ğŸğŸğŸ’ }. The thirteen
out. Formally, we solve the maximum a posteriori (MAP)                parent-child pairs were randomly split into seven folds.
problem                                                               Each time, one fold was left out as a tuning set and a model
arg max log ğ‘ƒ( ğ° âˆ£ ğ—, ğ² ) = arg max log ğ‘ƒ( ğ°)ğ©(ğ² âˆ£ ğ°, ğ— )             was trained on the remaining folds with each combination of
     ğ°âˆˆâ„ğ‘‘                          ğ°âˆˆâ„ğ‘‘                               three candidate parameters. The parameter setting with the
The objective function can be equivalently written as                 highest average tuning set log-likelihood was selected,
         minğ°âˆˆâ„ğ‘‘ ğ›½1 ||ğ°||1 + ğ›½2 ||ğ°||22                               which is {ğœ¸ = ğŸ. ğŸ•, ğœ·ğŸ = ğŸ. ğŸğŸğŸ, ğœ·ğŸ = ğŸ. ğŸğŸ} . We fixed
                      + logï¿½âˆ‘ğ‘§=0,1 ğ‘ƒ(ğ‘§ âˆ£ ğ°, ğ—) ğ‘ƒ(ğ² âˆ£ ğ‘§)ï¿½              this setting in the following experiments.
where ğ‘ƒ(ğ‘§ âˆ£ ğ°, ğ—) and ğ‘ƒ(ğ² âˆ£ ğ‘§) have been defined earlier.                Through applying the model to sensorimotor features and
   Now, we are ready to extend the model to the multiple              showing that the model can predict word learning results
parent-child pairs and multiple words case. Let ğ‘– =                   (y=0, 1 or 2) based on cross-validation, our main goal of the
1 â€¦ #ğ‘ƒğ‘ğ‘–ğ‘Ÿğ‘  be the index for parent-child pairs. The ğ‘–-th pair         present study here was to gather and analyze the weights
studied ğ‘— = 1 â€¦ #ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ (ğ‘–) different words. Note the ğ‘–-th              ğ° of sensorimotor features ğ± from training, and interpret
pair's first word may be different than ğ‘– + 1-th pair's first         those weight results to better understand what sensorimotor
word and so on. For the ğ‘–-th pair's ğ‘—-th word, there were             features may be predictive to learning results and therefore
ğ‘˜ = 1 â€¦ #ğ¸ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘ (ğ‘–ğ‘—) training naming events. These                     contribute to successful learning. To this end, our first study
naming events need not happen consecutively in time. We               examined sensorimotor dynamics around object naming
use ğ± ğ‘–ğ‘—ğ‘˜ âˆˆ â„ğ‘‘ to denote the feature vector for the ğ‘˜-th              events with the assumption that what happened at those
naming event. Similarly, for the ğ‘–-th pair's ğ‘—-th word, there         moments was more relevant to learning than other moments.
                                                                      In practice, we decomposed the whole training dataset into
were ğ‘™ = 1 â€¦ #ğ‘‡ğ‘’ğ‘ ğ‘¡(ğ‘–ğ‘—) test events ğ‘¦ğ‘–ğ‘—ğ‘™ . In each test event,
                                                                      three groups based on when object naming events happened:
                                                                  1604

1) â€œduringâ€ moments defined by the onset and offset of a
naming event; 2) â€œbeforeâ€ moments defined by 5 seconds
prior to the onset of a naming event to that onset; 3) â€œafterâ€
moments defined by the offset of a naming event to 5
seconds after that offset. For each group based on the above
timing definition, we extracted 28 features from the
continuous time series ( a subset of them were illustrated in
Figure 2) and fed the data into the model as feature vector
ğ± to predict ğ². This is done separately for before, during
and after moments.
                                                                     Figure 4. The top 5 features within before/during/after naming
                                                                     events that are critical for predicting word learning results.
                                                                    attention for a certain period of time. All these led to better
                                                                    learning.
                                                                       Since there were different types of sensorimotor features
                                                                    in training data, our next study focused on discovering
                                                                    which types of features are more important. To do so, we
                                                                    divided 28 features into four semantic subsets: childâ€™s
Figure 3. Normalized log-likelihood values from three temporal      perception, parentâ€™s perception, hand action and head
groups compared with the baseline (the dotted line).                movement. We then fed the features in each subset into the
                                                                    model and asked the model to predict word learning only
   First, the log-likelihood results shown in Figure 3 indicate
                                                                    based on those features within each subset. Figure 5 shows
the fitness of data from those three moments compared with
                                                                    the results of log-likelihood values, suggesting the childâ€™s
a baseline calculation. Sensorimotor features from â€œduringâ€
                                                                    visual perception is more directly predictive than both the
and â€œafterâ€ moments are more predictive to word learning
                                                                    childâ€™s and the parentâ€™s actions. One plausible explanation is
than those features from â€œbeforeâ€, while all three features
                                                                    that the ultimate role of actions in learning is to select visual
sets are predictive as compared with chance (the dotted line
                                                                    information for the internal learning processes.
in Figure 3). Next, we asked exactly what features in each
                                                                    Interestingly, just head movements of the child and the
moment are predictive to learning. This information can be
                                                                    parent can also somehow predict learning (not perfectly but
inferred from the trained regression coefficients ğ°. In
                                                                    far above chance). The stability and dynamics of head
practice, we selected top 5 features that have gained largest
                                                                    movements are good indicators of sustained attention of the
absolute weights based on training (note weights can be
                                                                    learner and the teacher if they jointly attend to the same
positive or negative). As shown in Figure 4, some
                                                                    object. Lastly, the parentâ€™s perception is less relevant to
sensorimotor features consistently played a role in all of the
                                                                    learning, suggesting that the parentâ€™s perception may
three temporal moments. For example, the childâ€™s holding
                                                                    determine what action the parent may generate next and this
of the target object (C_A_target_holding) appeared to be a
                                                                    next action can indirectly influence the childâ€™s action and
predictive cue in all of the three moments, suggesting that
                                                                    the childâ€™s perception, but the parentâ€™s perception may not
the target object held by the learner is more likely to be
                                                                    be directly relevant to learning.
learned. Manually holding the named object around the
moments of hearing that object name indicates the learnerâ€™s            We next closely examined each group and analyzed what
sustained attention and interest on the named object. In            features in each group contribute more toward predicting
addition, in both â€œduringâ€ and â€œafterâ€ moments, the size of         word learning results. Due the page limit, we selected only
the target object in the childâ€™s view (C_V_target_size) and         two most influential groups (childâ€™s perception and hand
the     parentâ€™s      holding     of    the      target   object    actions) and within each group, we selected only two most
(P_V_target_holding) are good for learning but probably for         influential features to show in Figure 6. The childâ€™s holding
different reasons. The size of target object in the childâ€™s         of the target object didnâ€™t matter before/after naming
view is a direct measure of visual saliency of that object â€“        moments but played a critical role exactly during naming
an indicator of the learnerâ€™s attention. On the other hand,         moments. In contrary, the parentâ€™s holding of the target
holding the target object by parents may facilitate learning if     object had a negative weight before naming, no influence
and only if this action can attract the childâ€™s attention â€“ an      during naming, and became critical after naming. There are
open question worth more studies. Moreover, the stability of        two plausible interpretations of those patterns. One is that
the childâ€™s head (C_A_head_rotSpeed) before and during              the child held the target object while the parent named it and
naming also predicts good learning as compared with other           then passed that object to the parentâ€™s hands. The other
cues. That is, the learner not only paid attention to the right     possibility is that those patterns were mixed from two
object, manually held the object, but also stabilized their         different interaction modes, both of which can lead to
                                                                1605

                                                                          to name objects and for the learner to build a mapping
                                                                          between names and objects, and how those feature may
                                                                          work together to facilitate learning. Those results derived
                                                                          from our modeling efforts (e.g. the regression coefficients of
                                                                          features) provided quantitative measures of how various
                                                                          bodily cues and sensory features may be relevant to learning
                                                                          at different moments and through different ways. Some
                                                                          results confirmed our original hypotheses and others were
                                                                          rather surprising, opening up new research questions with
                                                                          the potential to lead to new findings that we do not know
                                                                          yet. The present paper represents our first efforts in
Figure 5. Normalized log-likelihood values of the four semantic           modeling sensorimotor dynamics in child-parent social
feature groups.                                                           interaction. With more fine-grained data and advanced
successful learning. In one mode, the child led the                       computational modeling methods, we have the opportunity
interaction by holding the named object during naming. In                 to discover a more complete mechanistic explanation of
the other mode, the parent named the object first and                     early word learning.
manually held the object to attract the childâ€™s attention after
naming it. We need further studies to understand this better.             Acknowledgments: We thank Charlotte Wozniak, Amanda
Also shown in Figure 6 (right), the sizes of objects (both the            Favata, Alfredo Pereira, Amara Stuehling, and Andrew
target and other objects) seem to be a more direct measure                Filipowicz for data collection, and Thomas Smith for
of what the child visually attended to compared with other                developing data management and preprocessing software.
visual features (e.g. distance to the center or visual                    This research was supported by NSF BCS 0924248, NSF
saliency). In particular, the size of the target object in the            IIS-0953219, AFOSR FA9550-09-1-0665 and AFOSR
childâ€™s view is weakly relevant before and after naming, but              FA9550-09-1-0313.
right at the naming moment, this visual property seems to be                                        References
critical. In contrast, the size of other objects played a                 Baldwin, D. (1993). Early referential understanding:
negative role after naming. It is an open question why the                   Infantâ€™s ability to recognize referential acts for what they
size of other objects has a positive impact during naming.                   are. Developmental psychology, 29, 832-843.
      6                                 2                                 Frank, M. C., Goodman, N. D., & Tenenbaum, J. (2008). A
                                                                             Bayesian framework for cross-situational word learning.
      4                                 1                                    Advances in Neural Information Processing Systems, 20.
                                                                          Gleitman, L., Cassidy, K., Nappa, R., Papafragou, A., &
      2                                 0
                                                                             Trueswell, J. (2005). Hard words. Language Learning
                                           before during     after           and Development, 1(1), 23-64.
      0                                -1
                                                                          Itti, L., Koch, C., Niebur, E. (1998). A Model of Saliency-
          before    during     after
     -2                                -2        C_V_target_size
                                                                             Based Visual Attention for Rapid Scene Analysis, IEEE
               child holding target                                          Transactions on Pattern Analysis and Machine
                                                 C_V_other_size
     -4        parent holding target   -3                                    Intelligence, Vol. 20, No. 11, pp. 1254-1259.
                                                                          Landau, B., Smith, L. B., & Jones, S. S. (1998). Object
Figure 6. Left: the weights of the childâ€™s holding and the parentâ€™s
holding of the target object. Right: the weights of the target object        shape, object function, and object name. Journal of
size and the size of other objects.                                          Memory & Language, 38 (1), 1â€“27.
                                                                          Markman, E. M. (1989). Categorization and naming in
        General Discussions and Conclusion                                   children. Cambridge, MA: MIT Press.
Most of childrenâ€™s word learning takes place in messy                     Tomasello, M., & Akhtar, N. (1995). Two-year-olds use
contexts â€“ like the tabletop play task used here. There are                  pragmatic cues to differentiate reference to objects and
multiple objects, multiple shifts in attention and multiple                  actions. Cognitive Development, 10, 201-224.
bodily cues by both partners, and many object names                       Yu, C., Smith, L., Shen, H., Pereira, A., & Smith, T. (2009).
simultaneously available. In this paper, we used advanced                    Active Information Selection: Visual Attention Through
sensing equipment and state-of-the-art experimental                          the Hands. IEEE Transactions on Autonomous Mental
paradigms to collect multiple streams of real-time sensory                   Development, 2, 141â€“151.
data in parent-child interactions. Given such fine-grained                Yuille, A.L. & Rangarajan A. (2003). The concave-convex
data, we developed a formal model to analyze these                           procedure. Neural Computation,15(4):915â€“936.
multisensory data and to extract statistical regularities in the          Zou, H. & Hastie, T. (2005). Regularization and variable
physical and social learning environment. We conducted                       selection via the elastic net. Journal of the Royal
two simulation studies to address questions such as which                    Statistical Society: Series B (Statistical Methodology),
types of sensorimotor features are more important for                        67(2):301â€“320.
learning, what moments are the right moment for the teacher
                                                                      1606

