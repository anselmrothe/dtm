UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
In Defense of Spatial Models of Lexical Semantics
Permalink
https://escholarship.org/uc/item/1d28323f
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Jones, Michael
Gruenfelder, Thomas
Recchia, Gabriel
Publication Date
2011-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                               In Defense of Spatial Models of Lexical Semantics
                            Michael N. Jones, Thomas M. Gruenenfelder, & Gabriel Recchia
                                           [jonesmn][ tgruenen][grecchia]@indiana.edu
                                            Department of Psychological and Brain Sciences
                                              Indiana University, Bloomington, Indiana USA
                             Abstract                                      Tversky (1977) has noted that spatial models must
  Semantic space models of lexical semantics learn vector
                                                                       respect several metric axioms. Firstly, in a metric space the
  representations for words by observing statistical                   distance between a point and itself must be zero by any
  redundancies in a text corpus. A word’s meaning is                   Euclidean metric, 𝑑 𝑥, 𝑥 = 0 (non-negativity). Secondly,
  represented as a point in a high-dimensional semantic space.         distance must respect symmetry: 𝑑 𝑥, 𝑦 = 𝑑 𝑦, 𝑥 . Thirdly,
  However, these spatial models have difficulty simulating             distance must respect the triangle inequality: If x and y are
  human free association data due to the constraints placed            proximal and y and z are proximal, then x and z are likely to
  upon them by metric axioms which appear to be violated in
                                                                       be proximal points as well (specifically, 𝑑 𝑥, 𝑧 ≤
  association norms. Here, we build on work by Griffiths,
  Steyvers, and Tenenbaum (2007) and test the ability of spatial       𝑑 𝑥, 𝑦 + 𝑑 𝑦, 𝑧 ). As Tversky & Gati (1982) have
  semantic models to simulate association data when they are           demonstrated, human judgments of similarity routinely
  fused with a Luce choice rule to simulate the process of             violate these axioms (specifically, symmetry and the
  selecting a response in free association. The results provide an     triangle inequality). Tversky used human violations of the
  existence proof that spatial models can produce the patterns of      metric axioms to argue against spatial models of similarity,
  data in free association previously thought to be problematic.       and instead proposed an additive feature comparison model.
   Keywords: Semantic space model; latent semantic analysis;           The spatial debate, however, has a long history in cognitive
   semantic networks; word association; metric axioms.                 science, with Tversky’s work being followed by
                                                                       explanations of how metric spaces could produce violations
                        1. Introduction                                of metric axioms (e.g., Krumhansl’s (1978) notion of
A longstanding belief in theories of lexical semantics                 density or Holman’s (1979) similarity and bias model).
(dating back at least to Osgood, 1952) is that words can be                Griffiths et al. (2007) note that word association norms
represented as points in a multidimensional semantic space.            also violate metric axioms, making them problematic for
Similarity between words is then defined as some function              semantic space models such as LSA. Probabilistic
of their distance in space. This classic notion of mental              representations, however, are not subject to the same metric
space has had an obvious impact on modern computational                restrictions as spatial representations, and Griffiths et al.
semantic space models, such as Latent Semantic Analysis                provide an elegant demonstration of how their Topic model
(LSA; Landuaer & Dumais, 1997). Models such as LSA                     can naturally account for the qualitative nature of these
borrow techniques from linear algebra to infer the semantic            violations that LSA cannot.
representation for words from their contextual co-                         Word association norms contain a significant number of
occurrences in linguistic corpora. In the resulting space, a           asymmetric associations: For example, the probability of
word’s meaning is represented by a vector over latent                  generating baby as a response to stork as a cue is much
dimensions. Inter-word similarity is based on Euclidean                greater than the reverse. Part of this effect is due to a bias to
geometry: Words that are more similar are more proximal in             respond with a high frequency target independent of the cue,
the learned space.                                                     but part appears to be due to some sort of asymmetry in
     In contrast to spatial models, the recent popularity of           similarity. In addition, word association norms contain
probabilistic models of cognition has led to the development           apparent violations of the triangle inequality axiom: To use
of Bayesian models of semantic representation, such as the             the example from Griffiths et al. (2007), asteroid is strongly
LDA-based Topic model of Griffiths, Steyvers, and                      associated with belt, and belt is strongly associated with
Tenenbaum (2007). In the Topic model, a word’s                         buckle, but asteroid and buckle have little association.
representation is a probability distribution over latent               Finally, Steyvers and Tenenbaum (2005) demonstrate that
semantic “topics.” Given that LSA and the Topic model                  association norms contain neighborhood structure that is
provide similar quantitative accounts of many semantic                 incompatible with spatial models. If one constructs an
tasks, a popular misconception is that the models are                  associative network with nodes representing words and
isomorphic and that the Topic model is simply a more                   connecting edges based on nonzero association
modern and generative version of LSA. However, the issue               probabilities, the resulting networks are scale-free: they
of whether humans represent meaning as a coordinate in                 have power law degree distributions and high clustering
space or as a conditional probability is a fundamental                 coefficients. Griffiths et al. demonstrate that while LSA
question in cognitive science, and has implications for                (based on a thresholded cosine) cannot reproduce this
downstream models that make use of these representations.
                                                                   3444

network structure, the Topic model naturally produces                                                 𝛽! 𝜂!,!                 (1)
scale-free and small-world networks.                                               p(𝑅! |𝑆! ) =
                                                                                                   !"# 𝛽! 𝜂!,!
     However, it is important to recall that an observable
behavior such as free association is the product of a
                                                                    where βj is the response bias for item j, and ηi,j is the
cognitive process operating on a memorial representation
                                                                    similarity between stimuli i and j. Given the restrictions of
(Estes, 1975). This notion is ubiquitous in cognitive science.
                                                                    metric spaces, the total probability over all responses sums
For example, Nosofsky (1986) uses a spatial representation
                                                                    to one. Most applications of the choice rule include
of stimuli, but the complex classification behavior of his
                                                                    exponential scaling of similarity based on Shepard’s (1987)
model is the result of applying a simple choice rule to this
                                                                    universal law of distance and perceived similarity. Hence,
spatial representation, not spatial distance itself. Similarly,
                                                                    this general formula is often referred to as the Shepard-Luce
semantic space models are models of memory structure; the
                                                                    choice axiom:
structural model should not be expected to simulate a
complex behavior like memory retrieval without the benefit                                       𝛽! 𝑒 !!"(!! ,!! )            (2)
of a process account to explain how the memory structure is                    p(𝑅! |𝑆! ) =               !!"(!! ,!! )
used in a particular task. This also enhances the models’                                      !"# 𝛽! 𝑒
generalizability across different tasks that tap semantic
structure, and is particularly appealing given the low              where d is a psychological distance function, and λ is a free
correlation between different tasks thought to utilize the          parameter for the slope of the exponential (indicating a
same semantic structure (Maki & Buchanan, 2008).                    subject’s sensitivity to stimulus differences).
   Griffiths et al. (2007, p. 224) imply that a “more                    Due to computational complexity that would be required
complex” spatial metric based on LSA (similar to                    to fit free parameters in the choice rule for our simulations,
Nosofsky’s 1986, 1991 use of a similarity-choice function)          we evaluate a very simple parameter-free version of the
could potentially account for the metric axiom violations in        choice rule here. Firstly we assume λ = 1, and ignore
association norms. We return to the issue of complexity             exponential scaling. Secondly, although it is reasonable to
with regard to spatial and probabilistic models in the              fix β to normative log word frequency for each word in the
discussion. The bulk of this paper will be focused on               lexicon, we also ignore bias in our application here to make
evaluating their suggestion by fusing spatial semantic              the similarities easily comparable to previous work. Hence,
models with a parameter-free version of Luce’s (1959)               given a semantic similarity matrix for all words in the
similarity-choice model to evaluate their ability to account        lexicon (for example, using LSA cosines) we simulate the
for the problematic data identified by Griffiths et al. In          probability of producing a target word in response to a cue
doing so, we provide an existence proof that semantic space         word in free association as:
models can indeed produce asymmetries, violations of the
triangle inequality, and scale-free network structure with an                                    cos (cue, target)            (3)
appropriate process rule. It is premature to reject spatial               p(target|cue) =      !
                                                                                               !!! cos   (cue, word[i])
models of semantic representation based on violations of
metric axioms in association data.                                  where τ is a threshold parameter. Hence, this is a very
                                                                    simple version of the Luce choice rule, and performance
         2. A Generic Spatial Choice Model                          should only be seen as a baseline—the model could
In this paper, we evaluate the application of Luce’s (1959)         obviously produce better predictions with parameter fitting.
choice rule to simulate the cognitive process involved in the
task of free association when applied to various (metric)                   3. Testing the Semantic Choice Model
semantic space models, gradually increasing in complexity.             In this section, we test the ability of the simple Luce
Although similarity and distance in the semantic spaces             choice rule (free of parameters except for a maximum
respect the metric axioms, the behavior of the choice rule          candidates threshold in the denominator) to account for
applied to these spaces need not (cf. Nosofsky, 1991). The          violations of the metric axioms. Each of the metric spaces
Luce choice rule was selected as our generic output model           conform to the metric axioms, but the simple behavior of
here due to its ubiquity in models of cognitive phenomena;          making a choice in this space does not.
it has been successfully applied to choice behavior ranging
from low-level neural networks to high-level economic
                                                                    3.1. Training Corpus
models of group choice behavior.
   The Luce choice rule simulates how humans select from            We trained each semantic space model on the standard
possible choice alternatives given a stimulus similarity            TASA corpus (Landauer & Dumais, 1997), and duplicated
space, governed by probabilities conditioned on the choice          the modifications to the corpus made by Griffiths et al.
set. Given a set of stimulus similarities (where similarity is      (2007) for easy comparison to their results. The models
defined as an inverse monotonic function of psychological           were restricted to words that occurred with a frequency of at
distance) the Luce choice rule states that the probability of       least 10, and were not contained on the standard LSA
responding to stimulus Si with response Rj is defined as:           stoplist of function words. This reduced the model
                                                                3445

vocabularies to 26,240 words with ~4.2 million tokens in            with elements sampled randomly from 𝑁 0, !! , where D is
the modified TASA, consistent with the version used by
                                                                    an arbitrary vector dimensionality. As BEAGLE
Griffiths et al.
                                                                    experiences sentences, the model updates the memory
                                                                    vectors for each word in the sentence as the sum of the
3.2. Semantic Space Models                                          random initial vectors representing each other word in the
We tested three semantic space models, ranging in                   sentence. Across learning, semantically similar words
assumptions and complexity: LSA, POC, and BEAGLE.                   naturally develop similar distributed vector patterns because
The semantic spaces produced by each of the models                  they have had common random vectors summed into their
conform to the three metric axioms (non-negativity,                 memory representations. This has the effect that words
symmetry, and the triangle inequality).                             which frequently co-occur develop similar vectors (a pattern
                                                                    learned by POC), but also that words which occur in similar
Latent Semantic Analysis (LSA): LSA spaces were                     sentences develop similar vectors (a pattern learned by
constructed based on a word-by-document (W x D) co-                 LSA), even if they never directly co-occurred. Note that the
occurrence matrix, in which each word is initially                  original BEAGLE model of Jones and Mewhort (2007) also
represented as a frequency distribution over documents.             uses holographic binding to learn grammatical information
Entries are then weighted inversely proportionate to the            about word usage—however, here we just use the very
word’s frequency and entropy (− 𝑝 log 𝑝 ) over                      simple random vector summation to learn semantic structure
documents. Singular value decomposition was applied to              in TASA (the convolution-based holographic learning
this matrix, and only the components with the largest               mechanism would introduce unnecessary complexity, as
eigenvalues were retained. The resulting word vectors are           grammatical similarity is unlikely to play a large role in free
thought to represent the latent semantic dimensions which           association). Here, we use BEAGLE trained with 1024
best explain the co-occurrence pattern of words over                dimensions trained on TASA using context learning only.
contexts.
  We constructed LSA spaces using both 300 and 700                  3.3. Simulating Asymmetric Associations
dimensions. Similar to Griffiths et al. (2007), we found little     We do not bother with simulations based on the raw
difference in performance on the association task as a              semantic spaces here, as they are obviously unable to
function of dimensionality. Our 300-dimensional version             simulate asymmetries in free association (i.e., cos(A,B) =
matched the version on the LSA website. However, we use             cos(B,A)). However, a choice rule applied to these spaces to
the 700-dimensional version here so our results are directly        simulate the process of free association need not respect
comparable to Griffiths et al.                                      symmetry. The reason for this is very similar to
                                                                    Krumhansl’s       (1978) notion of similarity density. In
Proportion of Co-Occurrence (POC): We also tested a                 addition, the density asymmetry in semantic space models
much simpler direct co-occurrence metric based on mutual            has been previously identified and discussed elsewhere
information, as recent work has demonstrated superior               (Burgess & Lund, 2000; Jones & Kintsch, 2006).
performance on some semantic tasks using simple models                 Although the distance between baby and stork is equal in
based only on the surface form of language (e.g., Recchia &         either direction, the structure of the landscape is not. If one
Jones, 2009), suggesting that “deep” models like LSA may            computes and ranks the similarity of every word in the
be over-thinking the problem of human semantic learning.            lexicon to baby and stork, baby is the 22nd most similar
Here we use proportion of co-occurrence (POC), a variant            word to stork, but stork is only the 9,279th more similar
of mutual information metrics:                                      word to baby (cosines from BEAGLE). Hence, while the
                                𝑓!"                      (4)        numerator of the choice rule is the same for both baby-stork
                 POC!,! =                                           and stork-baby, the denominator changes drastically
                           𝑓! + 𝑓! − 𝑓!"
                                                                    depending on the ratio of similarity to other competitors.
where 𝑓! and 𝑓! are the raw frequencies of words i and j,           When a simple choice rule is applied to a metric space, baby
respectively, and 𝑓!" is the co-occurrence frequency of i and       comes to mind easily when cued with stork, but it is
                                                                    extremely unlikely to respond with stork when cued with
j together in the same document. POC is essentially the
                                                                    baby due to strong competition from the many other words
intersection of the Venn, and can be applied efficiently to
                                                                    that come to mind more easily.
the same W x D matrix LSA learns from, but without the
                                                                       We reproduced Griffiths et al.’s (2007) method of
complexity of inferring latent semantic dimensions.
                                                                    selecting asymmetric pairs from Nelson’s association
                                                                    norms. Two words were asymmetrically associated in the
BEAGLE: In addition to LSA and POC, we use a model                  norms if 1) one word was produced as a response to the
intermediate to the two in complexity—the context learning          other with greater than zero probability, and 2) the forward-
mechanism from the BEAGLE model of Jones and                        backward ratio of cue-response probability was greater than
Mewhort (2007), which is similar in spirit to other random          an order of magnitude. This procedure produced 38,740
accumulation models (Kanerva, 2009). BEAGLE begins by               asymmetric associations.
assigning initial random vectors to all words in the corpus,
                                                                3446

   We then tested the ability of the choice rule to correctly        words in a metric semantic space, the distance between any
predict the direction of the asymmetry in these pairs. Note          pair of words in a triple may be no greater than the sum of
that the raw semantic space models would produce baseline            the distances of the other two pairs, d(x,z) ≤ d(x,y) + d(y,z).
chance of ~50%. For each model we varied the threshold                  Tversky has demonstrated violations of the triangle
parameter τ in the denominator of the choice rule. This              inequality in similarity judgments of low-dimensional
represents the τ most similar words to the cue considered as         stimuli, in which humans weight feature matches more
competitors to the target—τ was fixed across pairs within a          heavily than geometry suggests they should. However, it is
given simulation (so all had the same threshold).                    difficult to determine what hard constraints the triangle
Performance did not vary widely depending on τ anyways,              inequality places on semantic similarity spaces. Griffiths et
so we present only performance with the best τ per model             al. (2007) interpret the triangle inequality as implying that if
here (with τ hand fit).                                              x is similar to y and y is similar to z, then x must be similar
                                                                     to z. In word association, this leads to the expectation that if
                          90                                         P(w2|w1) is high and P(w3|w2) is high, then P(w3|w1) must be
                          80                                         high. However, they note that this constraint is violated in
% Assymetries Predicted
                          70                                         free association norms—as P(w2|w1) and P(w3|w2) are
                          60                                         selected to exceed an increasing threshold, P(w3|w1) remains
                          50                                         unaffected. To use their example, although asteroid is
                                                                     highly associated with belt, and belt is highly associated
                          40
                                                                     with buckle, asteroid and buckle have little association.
                          30                                            It is important to note, however, that the triangle
                          20                                         inequality is difficult to explore (and impossible to test)
                          10                                         with association data. The inequality does not state that if x
                           0                                         and y are close points and y and z are close points, then x
                                                                     and z must also be close points; it simply states that x and z
                                                                     may be no further apart than the sum of the distances
                                                                     between x-y and y-z. Hence, the triple asteroid-belt-buckle
Figure 1. Percentage of asymmetries in association norms             in free association may conform to the triangle inequality
predicted by each choice model (horizontal line is chance).          (rather than being a violation). Asteroid and buckle need not
                                                                     be similar under the inequality, just not dissimilar.
   Figure 1 shows the percentage of asymmetric pairs for                It is difficult to determine from free association data
which the choice model predicted the correct direction,              whether the triangle inequality has been violated because
varying semantic space. For comparison, the horizontal line          association is a coarse indicator of similarity—a word is
is chance performance without a choice model, and we have            produced in response to a cue word or not. But the fact that
inserted Griffiths et al.’s (2007) Topics model performance          a target is not produced in response to a cue is not evidence
for the same pairs, and raw frequency of the target word.            that they have no similarity, nor is it evidence of violating
   The first pattern to notice in Figure 1 is that LSA did not       the triangle inequality. Griffiths et al. (2007) demonstrate
perform any better with a choice rule than it could without.         that even as P(w2|w1) and P(w3|w2) increase in the norms,
We found this puzzling, but consistent across a wide range           there are still many cases in which P(w3|w1) = 0. While they
of τ (and the model often did worse than chance). While this         are careful to note that this only suggests a violation of the
could be taken as evidence against spatial models in                 triangle inequality, we worry about the reliance on zero
isolation, notice that both POC and BEAGLE improve                   probabilities in this type of analysis. A zero probability
considerably with the choice rule; both perform as well as           association simply means that the event did not occur. It can
word frequency and the Topic model. This is particularly             be problematic to make inferences based largely on events
intriguing given that POC is not a “deep” inductive model.           that were unobserved (although this is a strength of
When fused with an appropriate process model to simulate             Bayesian models). In addition, the practice assumes that all
the task of free association, however, it easily predicts the        word pairs with zero probability (unobserved) have equal
correct pattern of asymmetry in the association norms.               similarity, an assumption that is certain to be wrong.
                                                                        We duplicated the thresholding analysis conducted by
3.4. The Triangle Inequality                                         Griffiths et al. (2007), but instead used only triples for
The triangle inequality is more difficult to test because there      which all three pairs exist in the association norms. Hence,
is disagreement about what constraints it places on a                all probabilities in our analysis are nonzero, and we can
semantic similarity space, and how these constraints should          examine whether P(w3|w1) is related to systematic increases
be manifest in a free association task. The triangle                 in P(w2|w1) and P(w3|w2), relying on variance of observed
inequality comes from Euclidean geometry, in which the               events only. Our selection resulted in 80,212 triples. We
shortest path between two points is a line. Given this               systematically increased the threshold τ above which
observation, the inequality states that the length of any side       P(w2|w1) and P(w3|w2) were required to lie, and examined
of a triangle must be less than the sum of the other two             the distribution of P(w3|w1) values. In the analysis by
sides. Hence, when translated to proximities among three
                                                                  3447

Griffiths et al. (2007) which included zero probabilities,         distributions and clustering properties that closely matched
they essentially found that P(w3|w1) was uncorrelated with τ.      association networks. It is unclear, however, whether LSA’s
However, in our data (which excluded zero probabilities),          failure to reproduce the structure of the association network
we observed a significant correlation between the median of        is common to all spatial models, or whether LSA would fail
the P(w3|w1) distribution and τ, r = 0.42. This indicates that     to produce the correct structure if it had the benefit of the
the triangle inequality may indeed apply to association data       Luce choice rule to simulate the process of free association.
when missing values (zero probabilities) are removed.                 We constructed semantic networks analogously to
   With the Luce choice rule applied to simulate the process       Griffiths et al. (2007) both for LSA based on raw cosines as
of selecting a response in a free association task given a         they did, but also for LSA, POC, and BEAGLE with the
spatial semantic similarity space, metric models can produce       addition of the Luce choice rule to simulate free association.
violations of the triangle inequality. However, given that it      Here, we discuss only undirected networks. Only normed
is unclear whether humans violate this axiom in free               words were used to create the networks. For each model, a
association, it is important to note that metric models also       threshold was set to determine whether to connect two
can conform to the inequality. This is particularly important      nodes in the network (based either on cosine for raw LSA,
given that we are still uncertain as to whether or not human       or cue-target probability predictions from the Luce rule for
free associations actually contain evidence of a mental space      the others). For each network, we fit the degree distribution
that violates the inequality. In addition, it would seem from      to both a power and exponential function, and computed the
other types of semantic data that the triangle inequality is       clustering coefficient (Watts & Strogatz, 1998). The results
alive and well in the head. For example, mediated priming is       are displayed in Table 1 (LC = Luce choice rule applied to a
a well-established semantic phenomenon that relies on              similarity space). For comparison, we have also added the
triangulation: priming with lion facilitates recognition of        network properties from the free association norms in the
stripes due to their mediated relationship through tiger.          first row of Table 1.
3.5. Semantic Network Structure                                    Table 1. Network structure statistics for word association
In addition to constraints from metric axioms, the                 norms, raw LSA, and spatial + choice models (LC).
neighborhood structure of semantic spaces (specifically
LSA) is inconsistent with what is suggested from word                Network        Power R2      Exp R2        CC     CC/CCer
association. To create the mental connectivity structure
necessary to produce association norms, LSA would need             Association         .877        .571        .187      42.59
more words with extremely dense semantic neighborhoods             LSA-Raw             .882        .872        .449      85.41
than it appears to have. For example, Steyvers and                 LSA-LC              .830        .909        .352      72.58
Tenenbaum (2005) created network graphs based on free
association norms and then investigated the ability of             POC-LC              .952        .939        .092      18.81
different growth models to produce this structure, as well as      BEAG-LC             .882        .550        .290      59.03
the network graphs of WordNet and various thesauri.
   Steyvers and Tenenbaum (2005) created graphs based on              Although the degree distribution for raw LSA was
association norms in which each word is a node and nodes           slightly better fit by a power function than an exponential, it
are connected if they have nonzero probability of                  shows little preference between the two, and the clustering
association. The resulting graphs are scale-free, a common         properties of LSA are several orders of magnitude greater
property of connectivity in nature. If a word’s degree k is        than the association network. The final column in Table 1
defined as the number of other words connected to it, a            gives the ratio of the clustering coefficient in the model’s
scale-free network is one in which the distribution of             network to the clustering coefficient expected in a random
degrees over all nodes follows a power law, 𝑃(𝑘) ~𝑘 !!             Erdos-Rényi graph constructed with the same density. The
where γ is the constant rate of the power function. If             CC/CCer ratio for raw LSA is much greater than that
both 𝑃(𝑘) and k are plotted on a log scale, the result is a        observed in the association network. As with the asymmetry
straight line with a slope of – γ. In addition, Steyvers and       simulation, the Luce choice rule integrated with LSA
Tenenbaum found that association networks had much                 actually produces network structure more incompatible with
higher clustering of interconnected nodes than would be            the association network than did the raw LSA space,
expected in a randomly constructed network. LSA was                producing an exponentially distributed degree distribution.
unable to reproduce this scale-free small-world structure for      In contrast, POC-LC produces relatively weak clustering.
a variety of generation methods attempted by Steyvers and             When fused with the Luce choice rule, BEAGLE
Tenenbaum: LSA produces degree distributions that fall off         produces network structure that is remarkably similar to the
too slowly for small values of k and then too steeply as k         structure observed in the association network. The degree
increases, and LSA’s clustering properties are both too high       distributions show a strong preference for a power function
and are qualitatively distinct from association networks.          over an exponential, and the slope of the power function for
   In contrast, Griffiths et al. (2007) found that networks        BEAGLE (γ = 2.22) is very close to that of the association
created from the Topic model produced power law degree             network (γ = 2.25). For comparison, the slope of the power
                                                               3448

fit for LSA-LC was γ = 3.96. Figures 2 and 3 show the log-           asymmetry, the triangle inequality, and can produce
log degree distributions for the Luce choice version of LSA          association networks that are small-world and scale-free.
and BEAGLE, respectively. Recall that the log-log degree                As an existence proof, these results should not be taken as
distribution of the association network is linear with a slope       evidence against any particular model. Even with the Luce
of γ = 2.25. Hence, while network connectivity structure is a        choice rule, LSA had difficulties with network structure and
more difficult test for these models, BEAGLE demonstrates            the violations of metric axioms. However, this may be due
that it is certainly possible for a spatial model to produce the     to our assumptions when fixing parameters of the choice
connectivity structure observed in association norms with            model. The choice rule should be able to reproduce the
the benefit of a process model to simulate the task of free          behavior of the raw space (with free parameters), so it is
association.                                                         suspect that it did worse than raw LSA on occasion. Fitting
                                                                     the sensitivity and bias parameters to the data may well have
                                                                     produced a model that performed very well when applied to
                                                                     LSA. Nonetheless, the performance of the simpler
                                             γ = 3.96
                                                                     BEAGLE-LC and POC-LC models make it clear that spatial
                                                                     representations of semantics are still viable models.
                                                                                              Acknowledgements
                                                                     This research was supported by grants from Google
                                                                     Research and NSF BCS-1056744 to MNJ.
                                                                                                    References
                                                                        Estes, W.K. (1975). Some targets for mathematical psychology. Journal
                                                                     of Math. Psyc., 12, 263-282.
                                                                        Griffiths, T.L., Steyvers, M., & Tenenbaum, J. (2007). Topics in
                                                                     semantic representation. Psychological Review, 114, 211-244.
                                                                        Holman, E. W. (1979). Monotonic models for asymmetric proximities.
     Figure 2. Log-log degree distribution for Luce-LSA              Journal of Mathematical Psychology, 20, 1-15.
                                                                        Jones, M. N., & Mewhort, D. J. K. (2007). Representing word meaning
                                                                     and order information in a composite holographic lexicon. Psychological
                                                                     Review, 114, 1-37.
                                                                        Jones, M. N., & Kintsch, W. (2006). Asymmetric similarity in a self-
                                                                     organizing lexicon. Paper presented at the 47th Meeting of the Psychonomic
                                             γ = 2.22
                                                                     Society.
                                                                        Kanerva, P. (2009). Hyperdimensional computing: An introduction to
                                                                     computing in distributed representations with high-dimensional random
                                                                     vectors. Cognitive Computation, 1, 139-159.
                                                                        Krumhansl, C. (1978). Concerning the applicability of geometric models
                                                                     to similarity data: The interrelationship between similarity and spatial
                                                                     density. Psychological Review, 85, 450-463.
                                                                        Landauer, T.K., & Dumais, S.T. (1997). A solution to Plato’s problem:
                                                                     The latent semantic analysis theory of acquisition, induction, and
                                                                     representation of knowledge. Psychological Review, 104, 211-240.
                                                                        Luce, R. D. (1959). Individual Choice Behavior: A Theoretical Analysis.
                                                                     New York: Wiley.
                                                                        Maki, W.S., & Buchanan, E. (2008). Latent structure in measures of
                                                                     associative, semantic, and thematic knowledge. Psyc Bulletin & Review,
                                                                     15, 598-603.
                                                                        Nosofsky, R.M. (1986). Attention, similarity, and the identification-
  Figure 3. Log-log degree distribution for Luce-BEAGLE              categorization relationship. JEP: General, 115, 39-57.
                                                                        Nosofsky, R.M. (1991). Stimulus bias, asymmetric similarity, and
                                                                     classification. Cognitive Psychology, 23, 94-140.
                          4. Discussion                                 Osgood, C. E. (1952). The nature and measurement of meaning.
                                                                     Psychological Bulletin, 49, 197-237.
The purpose of this paper is simply to provide an existence             Recchia, G. L., & Jones, M. N. (2009). More data trumps smarter
proof that spatial models can produce the structure observed         algorithms: Comparing pointwise mutual information with latent semantic
                                                                     analysis. Behavior Research Methods, 41, 657-663.
in free association data provided that they have a plausible            Shepard, R. N., (1987). Toward a universal law of generalization for
process model to simulate the association task. It is                psychological science. Science, 237, 1317-1323.
premature to reject spatial models of lexical semantic                  Steyvers, M., & Tenenbaum, J. B. (2005). The large-scale structure of
representation simply because the raw spaces must respect            semantic networks: Statistical analyses and a model of semantic growth.
                                                                     Cognitive Science, 29, 41-78.
metric axioms but human behavior does not. Human                        Tversky, A. (1977). Features of similarity. Psychological Review, 84,
semantic memory may also respect metric axioms, but the              327-352.
behavior produced when a choice mechanism is applied to                 Tversky, A., & Gati, I. (1982). Similarity, separability and the triangle
this memorial representation can produce violations of               inequality. Psychological Review, 89, 123-154.
                                                                 3449

