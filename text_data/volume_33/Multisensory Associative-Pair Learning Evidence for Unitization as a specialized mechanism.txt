UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Multisensory Associative-Pair Learning: Evidence for ‘Unitization’ as a specialized
mechanism
Permalink
https://escholarship.org/uc/item/6439j38d
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Barenholtz, Elan
Davidson, Meredith
Lewkoicz, David
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                      University of California

 Multisensory Associative-Pair Learning: Evidence for ‘Unitization’ as a specialized
                                                               mechanism
                                           Elan Barenholtz (elan.barenholtz@fau.edu)
                                         Department of Psychology, 777 Glades Road
                                                         Boca Raton, FL 33433
                                             Meredith Davidson (mdavid14@fau.edu)
                                         Department of Psychology, 777 Glades Road
                                                         Boca Raton, FL 33433
                                              David Lewkowicz (lewkowic@fau.edu)
                                         Department of Psychology, 777 Glades Road
                                                         Boca Raton, FL 33433
                              Abstract                                     Eichenbaum, 1997; Eichenbaum, 1997; Eichenbaum &
                                                                           Bunsey, 1995). This view is represented in a number of
Learning about objects typically involves the association of               theories of face recognition which hold that associating the
multisensory attributes. Here, we present three experiments                face and voice of an individual depends on integrating
supporting the existence of a specialized form of associative              distinct informational streams into a single, ‘Personal
learning that depends on ‘unitization’. When multisensory pairs
(e.g. faces and voices) were likely to both belong to a single object,
                                                                           Identity Node’, or PIN (Bruce & Young, 1986; Burton,
learning was superior than when the pairs were not likely to belong        Bruce, & Johnston, 1990; Ellis, Jones, & Mosdell, 1997).
to the same object. Experiment 1 found that learning of face-voice            Unitizing multisensory properties may make multisensory
pairs was superior when the members of each pair were the same             object-knowledge more efficient, since each observed
gender vs. opposite gender. Experiment 2 found a similar result            property of that object may be associated with all other,
when the paired associates were pictures and vocalizations of the          previously observed, properties via a single link, rather than
same species vs. different species (dogs and birds). In Experiment         maintaining associations among many disparate properties.
3, gender-incongruent video and audio stimuli were dubbed,                 An additional potential advantage to a unitized
producing an artificially unitized stimulus reducing the congruency
                                                                           representation, implicit in the PIN model, is that it may help
advantage. Overall, these results suggest that unitizing
multisensory attributes into a single object or identity is a
                                                                           to organize associations that go beyond specific stimulus-
specialized form of associative learning                                   stimulus pairings to more abstract properties of an
                                                                           underlying ‘object’. For example, if one has encountered a
                           Introduction                                    specific auditory utterance of an individual, along with his
   Learning about objects typically involves the detection                 or her face, it would be advantageous to associate a different
and association of multisensory attributes. For example, we                utterance by the same individual with that face. Presumably,
may be able to identify certain foods based on their visual,               this depends on extracting ‘invariant’ properties of the
gustatory, tactile as well as olfactory properties. Likewise,              underlying voice from the sample. Representing individual
‘knowing’ a person typically means being able to associate                 face and voice stimuli as properties of the same underlying
his or her face with his or her voice. How do we encode the                individual may facilitate this process.
multisensory properties of objects? One possibility is that                   Despite the potential theoretical advantages to unitization,
such “object knowledge” simply consists of a network of                    there has been no direct behavioral support for the idea that
associations among each of an object’s unisensory                          multisensory unitization is a specialized form of associative
properties. According to this view, our knowledge about                    learning. In the current study, we compared associative
unitary objects may depend on the same learning                            learning of visual/auditory pairs under conditions where the
mechanisms as other types of object memory, such as                        members of the pair were either likely or unlikely to belong
associations between different objects or between objects                  to the same object by virtue of their membership in the same
and other properties of the environments in which they                     or different category. Specifically, we compared face/voice
appear. A second possibility is that multiple unisensory                   learning when the members of each pair were of the same or
object properties are all linked via an intermediate                       opposite gender (Experiment 1) or the same or different
‘supramodal’ representation of the object (Mesulam, 1998).                 species (Experiment 2). We reasoned that since only
According to this view, associating intra-object information               congruent pairs are consistent with belonging to the same
is a special class of associative learning, involving the                  object (for example, our experience is that people with male
creation of a ‘unitized’ representation (Cohen, Poldrack, &                faces always have male voices) they would be likely to be
                                                                       225

‘unitized’ into a single object or identity, while incongruent      Procedure
pairs would be remembered based on simple associative                 The procedure was identical in both conditions.
processes, without unitization. This difference may be              Participants were instructed that they would be performing a
reflected in better learning of the congruent pairs.                task in which they must learn to match faces and voices and
                                                                    that that they would receive feedback on correct or incorrect
                        Experiment 1                                responses. In the Incongruent condition subjects were
   In Experiment 1, we compared learning of face-voice              additionally informed that the faces and voices would be of
pairs of the same gender (congruent), versus learning of            opposite gender. Each participant took part in a Learning
pairs of the opposite genders (incongruent). Importantly,           Phase and a Generalization Phase. On each trial of the
because the task in both conditions was to learn arbitrarily        Learning Phase, participants were presented with a voice
matched faces and voices, they were—in terms of inherent            recording of one of the three recorded sentences, while four
task demand— equally difficult. Critically, we hypothesized         faces were presented on the screen with the numbers 1-4
that the pairs in the gender-congruent condition were more          below them (Figure 1). One of the four faces was the
likely to be unified into a single identity and that this would     ‘match’ to the voice, as determined prior to the experiment
result in better learning performance.          We measured         as described above, while the other three served as
performance in an initial learning phase in which                   distracters. The subjects were instructed to choose which of
participants had to learn associations between pictures of          the four faces was matched with the voice. An incorrect
specific faces and specific utterances (single sentences)           response resulted in a low beeping sound. The correct
using a forced-choice task with feedback. We then measured          selection was flashed once—regardless of whether subjects
generalization of learning in a second phase where                  had chosen it or not— before the stimuli were replaced by a
participants had to match each previously learned face with         white screen. The face-voice stimuli were presented in
new utterances (2 novel sentences) produced by the same             groups, with each group containing four faces and voices;
voices as before. All of the experiments used a between-            the faces within a single group were either all male or all
subjects design.                                                    female. There were four groups (2 male, 2 female), which
                                                                    were repeated, in six separate experimental blocks, for a
                                                                    total of 96 trials (4 trials per group X 4 groups X 6 blocks)
Methods
                                                                    per participant.
Participants                                                          The Generalization Phase began immediately after the
                                                                    subjects completed the Learning Matching Phase. The
  Fifty undergraduate psychology students (25 assigned to           procedure in the Generalization Phase was identical to the
each of the two experimental conditions), naïve to the              Learning Phase except that recordings of two new
purposes of the experiment, participated for course credit.         sentences, not heard in the Learning Phase, were used and
Each student was screened after the experiment and asked            that subjects did not receive feedback. The task of the
whether they personally knew any of the people whose                participant was to match the face to the new voice
faces/voices were shown during the experiment. Participants         recording, based on the face-voice pairs they had learned in
who recognized one of the people used in the stimuli were           the Learning Phase. Each participant performed two test
not included in the analysis.                                       blocks, one for each of the two new voice recordings: each
                                                                    test block consisted of four groups of four faces as in the
Stimuli                                                             Learning Phase for a total of 32 (4 groups of four faces X 2
  Stimuli consisted of photographs and voice recordings of 8        blocks) per participant.
Caucasian females and 8 Caucasian males ranging in age
from 18-26. Each individual was photographed and also                Results and Discussion
recorded speaking three sentences: 1) “There are clouds in            Figure 1a shows the results of the initial learning phase as
the sky”, 2) “The boy took his sister to the park”, and 3)          a function of block for the two congruency conditions.
counting from one to five. All photographs displayed the            While learning is apparent in both the Congruent and
head and shoulders of the person from a frontal viewpoint.          Incongruent conditions, it was much more efficient in the
Before the beginning of the experiment, each of the 16 face         Congruent condition (peaking at 75% correct; chance
images was matched with a single recorded voice as the              performance was 25%) than in the Incongruent condition
‘pair’ to be learned by the subject. In the Congruent               (peaking at 50% correct). A two-way ANOVA on the
conditions each picture was uniquely paired with one                Learning data found a significant main effect of both block
randomly chosen voice of the same gender, with the                  number F(5,72) = 31.536, p < .00001] and Congruency
constraint that it not be the true matching voice. In the           condition. [F(1,72) = 178.962, p < .00001] and no
Incongruent condition, each of the female faces was paired          significant interaction.
with a single randomly chosen male voice and vice versa.              Performance in the Generalization Phase was reduced in
                                                                    both conditions relative to performance in the learning
                                                                    phase (Figure 1b) but it was still well above chance for the
                                                                    congruent pairs [t(28)=6.86; p<.001], indicating that
                                                                226

  a                                                                       b
Figure 1: Experiment 1 Results. (a) Performance in the Learning phase as a function of block for the two conditions. (b)
Performance in the Generalization Phase for the two conditions.
      participants successfully generalized their learning to new        the same methods as in Experiment 1 except that this time
      utterances, whereas performance for incongruent pairs              we presented pictures and vocalizations of dogs and birds
      declined to near chance (31%; chance was 25) [t(28)=1.23;          and compared learning of congruent pairs (e.g. a specific
      p<.05]. There was a significant difference in performance          dog picture and a specific bark) with incongruent pairs (a
      between the Gender-Congruent and Incongruent conditions            specific dog picture with a specific bird song). Then, to
      by t-test [t (48) = .325, p = .001].                               provide converging evidence for the concept of
        These findings indicate that generalization of learning          multisensory unitization, rather than testing for
      was much more successful when the face-voice pairs were            generalization of learning, we re-tested learning after a 10-
      gender-congruent.                                                  minute delay to determine whether within category learning
       Overall, learning was more efficient and more generalized         might be more robust than simple associative learning.
      when the faces and voices making up the pairs were the
      same gender then when they were of the opposite gender.            Methods
      Since the inherent task difficulty was the same in both
      conditions, (i.e. the congruency did not yield any task-           Participants
      relevant information) the difference in performance is likely       Sixty undergraduate psychology students (30 for each of
      due to the fact that the incongruent pairs could not be            the two experimental conditions), naïve to the purposes of
      unitized into a single identity and that learning depended on      the experiment participated for course credit.
      simple associative learning of the pairs.
                                                                         Stimuli
                              Experiment 2                                Stimuli consisted of pictures of cropped faces of 8 ‘mid-
                                                                         sizes’ dogs (chosen based on subjective judgment) and
      Experiment 2 investigated whether the congruency                   pictures of 8 typically sized birds as well as sound
      advantage observed in Experiment 1 is specific to human            recordings of 8 different mid-range dog barks and 8
      faces and voices. Evidence indicates that human face and           different bird chirps (photos and audio recordings were
      voice processing are specialized processes that depend on          obtained from the internet).
      dedicated brain regions (Belin, Zatorre, Lafaille, Ahad, &         Procedure
      Pike, 2000; Kanwisher, McDermott, & Chun, 1997; Puce,
      Allison, Gore, & McCarthy, 1995) and/or visual expertise            As in Experiment 1, each participant first performed a
      (Gauthier, Skudlarski, Gore, & Anderson, 2000; Gauthier &          Learning Phase, in which they were given feedback while
      Tarr, 1997) and that learning face-voice pairs preferentially      learning specific picture-vocalization pairs across six
      leads to cross-activation of these unimodal selective              blocks. After the Learning Phase, participants took a 10
      areas(von Kriegstein et al., 2008) This raises the possibility     minute break in which they viewed unrelated videos on the
      that the multisensory unitization that we found in                 web after which they performed a final Test block
      Experiment 1 is restricted to the learning of human faces          consisting of the same exact task as in the Learning Phase,
      and voices rather than a reflection of a general learning          but without feedback.
      process. To test this possibility, in Experiment 2 we used
                                                                     227

    a                                                             b
 Figure 2: Experiment 2 results. (a) Mean correct in the Learning Phase as a function of block number, for the two
 conditions. (b) Results of the Learning Phase and the Test Phase for the two conditions.
                                                                   us to compare the effect of motion on the Congruent and
Results and Discussion                                             Incongruent conditions. In particular, we were interested in
                                                                   the possibility that motion would produce a larger benefit in
  Figure 2a shows the results of the learning phase across the     the Gender-Incongruent condition because it could
two conditions as a function of block number. Even though          encourage unitization for pairs of stimuli that would
participants were able to learn both congruent and                 otherwise not be unitized.
incongruent pairs, they exhibited a significant advantage in
learning the species-congruent pairs vs. the incongruent           Methods
pairs [t (58) = 2.736; p < .01]. Figure 2b shows the
performance in the initial Learning Phase compared to the
                                                                   Participants
Test Phase for the two conditions. Performance did not drop
                                                                    One hundred and twenty undergraduate psychology
significantly following the 10 min delay for the congruent
                                                                   students (30 for each of the four experimental conditions),
pairs [t (29) = .61; p > .5] but did decline significantly for
                                                                   naïve to the purposes of the experiment participated for
the incongruent pairs [t (29) = 2.23; p < .05]. These results
                                                                   course credit.
again suggest that pairs that may be unitized into a single
object lead to a different learning pattern than non-
                                                                   Stimuli
unitizable stimuli.
                                                                    Stimuli were movies featuring the same individuals and
                                                                   utterances as in Experiment 1. Each movie was created by
                       Experiment 3                                dubbing the audio recording of one person’s utterance onto
                                                                   the synchronized video of a different person speaking the
   The results from Experiments 1 & 2 indicate that learning       same utterance1. In the Static condition only a still frame of
of multisensory associations is better when the paired             each movie clip was shown (as described below) while in
properties belong to the same object. However, this                the Motion condition, the dubbed movie was shown.
advantage alone does not indicate that the difference in
performance is due to ‘unitization’ per se rather than some        Procedure
other effect of their congruency. In Experiment 3, we used           As in Experiment 1, each participant first took part in a
the same method as in Experiment 1 except that here we             learning phase, in which they were given feedback while
also presented some subjects with ‘dubbed’ movies during           learning specific face-voice pairs in groups of four.
the pair-learning phase. This consisted of presenting faces        However, before performing the forced choice task, each
that could be seen and heard talking in synchrony. Because         face in the group was presented in conjunction with the
temporal audio-visual synchrony can be a powerful cue to           recording of the matched voice. In the ‘Motion’ conditions,
the integration of visual and auditory stimulation                 the face was a video of the person speaking, accompanied
(Lewkowicz, 2010), we expected that synchrony might                by the matched voice. In the ‘Static’ conditions, the face
encourage subjects to unitize the face-voice pairs even in the     was a still-frame taken from the video sequence. This initial
gender-incongruent condition. If that is the case, this, in
turn, might reduce the congruency advantage.                          1
   Experiment 3 included four between-subject conditions:               In order to facilitate synchronization, individuals were
                                                                   recorded uttering each phrase while listening on headphones to a
Gender-Congruent and Incongruent (as in Experiment 1),
                                                                   recording of a repeated, ‘standard’ version of that phrase. This
each with a Motion version (which included the                     yielded high degrees of synchrony across different individuals’
dynamically speaking faces) and a Static version (in which         recordings with only a small amount of editing needed to bring
only a static picture of the face was shown). This allowed         them into a high degree of alignment.
                                                               228

a                                                                         b
  Figure 3: Experiment 3 Results. (a) Mean correct for the Learning Phase, as a function of block number, for the four conditions. (b)
  Mean correct in the Generalization Phase for the four conditions.
       sequence of four face-voice presentations was then followed        ones. Thus, synchrony cues do not facilitate learning or
       by the exact same forced-choice task as in Experiment 1.           generalization when multisensory information is easily
       During the learning phase, participants were tested on four        unitized but does facilitate them when the information is not
       groups of four people (8 male face and 8 female faces)             likely to be unitized.
       repeated across four blocks of trials for a total of 64 trials
       per participant.                                                                      General Discussion
         After the learning phase, participants completed a               The current results demonstrate a previously unreported
       generalization phase in which they had to try to match each        phenomenon in associative-pair learning. We found that
       learned face with the previously paired voice, now uttering a      learning to pair multisensory stimulus properties was much
       new sentence. On each trial, participants were presented           more efficient, robust, and general when the paired
       with two face-voice stimuli (either static or moving,              properties were members of the same category vs. when
       depending on condition) in succession: one in which the            they were not. This advantage is likely due, at least in part,
       face was matched with the same voice it had been paired            to the ability to unitize the pairs in the congruent category
       with in the learning phase and one where it was paired with        conditions since artificially encouraging unitization —as in
       one of the voices that had been paired with a different face       Experiment 3—significantly decreased the congruency
       in the learning phase. Participants had to chose which of the      differential. The current results with regard to faces and
       two stimuli matched the learned face-voice pairings. No            voices are consistent with earlier theories of personal
       feedback was given.                                                identity representation, such as Bruce and Young’s (1986)
                                                                          theory in which multiple properties are integrated via a
       Results                                                            single node. However, the extension of the congruency
          Figure 3a shows the results of the learning phase for each      advantage to visual and auditory pairs of other species—as
       of the four conditions (Gender Congruent/Incongruent in            in Experiment 2— suggests that unitization may be a
       both Motion and Static Cases). Participants exhibited              general mechanism, that extends to other kinds of objects. If
       learning of congruent and incongruent pairs in both the            so, these results may suggest a fundamental dichotomy
       dynamic and static conditions (main effects for block              between ‘simple associative learning’— which applies to
       number [F (3, 116) = 49.89; p < .0001]. As in the previous         associations among properties of different objects—and
       experiments, the two gender-congruent conditions yielded           unitization— which applies to associations of stimulus
       better performance than the two gender-incongruent                 properties corresponding to a single object. Indeed, the
       conditions [F (1, 116) = 77.75; p < .0001]. There was no           current behavioral results bear interesting relations to
       significant effect of motion (p > .1). However, as Fig. 3a         previous findings in both the neuropsychology and
       shows, learning was marginally greater for gender-                 neuroimaging literatures suggesting that “intra-item” and
       mismatched pairs when the stimuli were dynamic, and thus           “inter-item” memories are encoded in distinct neural
       synchronized, than when they were static [t(48) = 1.675; p         substrates (Cohen et al., 1997; Eichenbaum, 1997;
       = .06]. However, learning was not enhanced by synchrony            Eichenbaum & Bunsey, 1995). This raises the intriguing
       for gender-congruent pairs (p > .5). Figure 3b shows that          possibility that the different learning patterns observed in
       performance in the generalization phase, where chance              our study for congruent vs. incongruent pairs may represent
       performance was .5, mirrored the performance in the initial        neurally separable mechanisms.
       learning phase. Here, response to the gender-matched pairs              The process of unitization discussed here has clear
       was equivalent regardless of whether synchrony cues were           relations to the concept of ‘binding’ in attention and short-
       provided or not [t (48) = .964, p > .1], but was more robust       term memory. The so-called ‘Binding Problem’ refers to the
       for the moving gender-incongruent pairs than for the static        process by which different properties—typically visual
                                                                      229

properties such as shape and color—are identified and              Kanwisher, N., McDermott, J., & Chun, M. M. (1997). The
remembered as belonging to a single object during a task                   fusiform face area: A module in human extrastriate
such as visual search or identification. Generally, this                   cortex specialized for face perception. The Journal
process is thought to involve a specialized process, requiring             of Neuroscience, 17(11), 4302-4311.
attentional mechanisms, in order to integrate the separate         Lewkowicz, D. (2010). The Ontogeny of Human
properties into a single ‘object-file’ (Treisman & Gelade,                 Multisensory Object Perception: A Constructivist
1980). This mechanism is also thought to underlie the                      Account. In Multisensory Object Perception in the
capacity limitations of working memory (Luck and Vogel,                    Primate Brain: Springer Press.
1997). The object-files formed in these cases are assumed to       Mesulam, M. M. (1998). From sensation to cognition.
be inherently short-lived, lasting perhaps only as long as the             Brain, 121, 1013–1052.
stimulus remains in working memory (Wheeler and                    Puce, A., Allison, T., Gore, J. C., & McCarthy, G. (1995).
Treisman, 2002. However, the current results suggest the                   Face-sensitive regions in human extrastriate cortex
existence of a long-term object-file mechanism as well.                    studied by functional MRI. Journal of
                                                                           Neurophysiology, 74(3), 1192-1199.
                                                                   Treisman, A. M., & Gelade, G. (1980). A feature-integration
                    Acknowledgments                                        theory of attention. Cognitive Psychology, 12(1),
This work was supported in part by an NSF grant # BCS –                    97-136.
0958615 to E.B. and NSF grant # BCS-0751888 to D.L.                von Kriegstein, K., Dogan, z. r., Grüter, M., Giraud, A.-L.,
                                                                           Kell, C. A., Grüter, T., et al. (2008). Simulation of
                                                                           talking faces in the human brain improves auditory
                          References                                       speech recognition. PNAS Proceedings of the
                                                                           National Academy of Sciences of the United States
Belin, P., Zatorre, R. J., Lafaille, P., Ahad, P., & Pike, B.              of America, 105(18), 6747-6752.
          (2000). Voice-selective areas in human auditory
          cortex. NAture, 403, 309-312.
Bruce, V., & Young, A. (1986). Understanding face
          recognition. British Journal of Psychology, 77(3),
          305-327.
Burton, A. M., Bruce, V., & Johnston, R. A. (1990).
          Understanding face recognition with an interactive
          activation model. British Journal of Psychology,
          81(3), 361-380.
Cohen, N. J., Poldrack, R. A., & Eichenbaum, H. (Eds.).
          (1997). Memory for items and memory for
          relations in the procedural/declarative memory
          framework: Mayes, Andrew R.; Downes, John
          Joseph (1997). Theories of organic amnesia.
Eichenbaum, H. (1997). Declarative memory: Insights from
          cognitive neurobiology. Annual Review of
          Psychology, 48, 547-572.
Eichenbaum, H., & Bunsey, M. (1995). On the binding of
          associations in memory: Clues from studies on the
          role of the hippocampal region in paired-associate
          learning. Current Directions in Psychological
          Science, 4(1), 19-23.
Ellis, H. D., Jones, D. M., & Mosdell, N. (1997). Intra- and
          inter-modal repetition priming of familiar faces and
          voices. British Journal of Psychology, 88(1), 143-
          156.
Gauthier, I., Skudlarski, P., Gore, J. C., & Anderson, A. W.
          (2000). Expertise for cars and birds recruits brain
          areas involved in face recognition. Nature
          Neuroscience, 3(2), 191-197.
Gauthier, I., & Tarr, M. J. (1997). Becoming a "greeble"
          expert: Exploring mechanisms for face recognition.
          Vision Research, 37(12), 1673-1682.
                                                               230

