UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Capturing mental state reasoning with influence diagrams
Permalink
https://escholarship.org/uc/item/9hn6n2hd
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Jern, Alan
Kemp, Charles
Publication Date
2011-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                      Capturing mental state reasoning with influence diagrams
                                                    Alan Jern and Charles Kemp
                                                        {ajern,ckemp}@cmu.edu
                                                        Department of Psychology
                                                        Carnegie Mellon University
                              Abstract                                    Computer scientists have previously used IDs to model the
                                                                       behavior of intentional agents, particularly in games (Gal &
   People have a keen ability to reason about others’ mental           Pfeffer, 2008; Koller & Milch, 2003), but this research has
   states, which is central for communication and cooperation.
   A core question for cognitive science is what mental represen-      focused primarily on relatively complex scenarios involving
   tations support this ability. We offer one proposal based on        multiple agents, rather than the simple scenarios that have
   the framework of influence diagrams, an extension of Bayes          been the subject of most theory of mind research—like the
   nets that is suited for representing intentional goal-directed
   agents. We evaluate this framework in two experiments that          example at the beginning of this paper. And although there
   require participants to make inferences about what another per-     are several existing computational models of mental state rea-
   son knows or values. In both experiments, participants’ judg-       soning (Baker, Saxe, & Tenenbaum, 2009; Oztop, Wolpert, &
   ments were better predicted by our influence diagrams account
   than by several alternative accounts.                               Kawato, 2005; Schultz, 1988; Wahl & Spada, 2000), the ID
   Keywords: theory of mind; social cognition; influence dia-
                                                                       framework has received little attention in the psychological
   grams; Bayes nets                                                   literature. We argue that IDs serve as a useful model of the
                                                                       mental representations that support human reasoning.
   People rarely articulate everything they are thinking. Thus,           The rest of the paper is organized as follows. First, we
one of the major inductive problems we face is how to infer            describe the ID framework and discuss some of its strengths
other people’s thoughts from their observable behavior. As a           for reasoning about other people’s mental states and behavior.
concrete example, suppose your friend asks you to join him             Then, we apply the framework to a specific task that involves
in visiting the art museum today, a Monday, which is a day             inferring what someone else knows or values and evaluate its
on which you happen to know the museum is closed. You                  ability to predict human performance on the task.
might infer then that your friend does not know the museum
is closed on Mondays, or that he does not know that today is                               Influence diagrams
Monday, or perhaps neither. You suggest going to the natu-             We will introduce the ID framework using the following sim-
ral history museum instead, which is open but is both more             ple scenario. Alice is playing a game in which a two-color
expensive and farther away than the art museum. Your friend            die is rolled. Alice chooses a color and receives a reward if
declines. Now you might infer that he did not want to spend            her choice matches the color of the die. Thus, there are three
more money, or that he did not want to travel so far, or per-          variables: the color of the rolled die, Alice’s chosen color,
haps both; it’s also possible that your friend simply doesn’t          and the value of the reward. Two variations of this scenario
like the natural history museum.                                       can be represented by the IDs in Figure 1a and 1b, where the
   People generally find these types of inferences about others        three variables are denoted R, D, and U, respectively.
natural and exhibit relatively rich intuitive theories of mental          IDs differ from standard Bayes nets in that they allow for
states and behavior (D’Andrade, 1987). Mental state reason-            the representation of three semantically distinct types of vari-
ing, also called theory of mind, poses some standard ques-             ables, each of which is shown in the example IDs. First are
tions for cognitive science: Namely, what mental representa-           chance variables, depicted by ovals, which represent proba-
tions support mental state reasoning and what computations             bilistic events like the outcome of the die roll R. Just as in
are carried out over these representations (Perner, 1991). We          causal Bayes nets, incoming edges to chance nodes represent
propose that these representations are similar to influence dia-       causal dependencies between events; therefore, we will refer
grams (IDs), an extension of Bayes nets that includes a notion         to these edges as causal edges. Second are decision variables,
of goal-directed action (Howard & Matheson, 2005).                     depicted by rectangles, which represent intentional decisions,
   The ID framework provides a graphical language for rep-             like Alice’s choice D. Incoming edges to decision nodes rep-
resenting decision problems and an associated formal seman-            resent information available when making the decision; we
tics that supports quantitative predictions. The framework re-         will refer to these edges as knowledge edges. For example,
tains all of the strengths of Bayes nets, including the ability        the IDs in Figure 1a and 1b differ in the presence of a knowl-
to make a distinction between the existence of relationships           edge edge from R to D. The ID in 1a represents a situation
among variables and the strength of those relationships, to            where Alice knows nothing about the roll before making her
concisely specify a distribution over many variables, and to           choice and the ID in 1b represents a situation where Alice
predict the outcomes of interventions (Sloman, 2005). IDs,             gets to see the rolled color before making her choice. Lastly
however, build on Bayes nets by providing a formal way of              are utility variables, depicted by diamonds, which represent
making predictions and inferences about intentional behavior.          a decision maker’s utility, like Alice’s reward U. Incoming
                                                                   2498

  (a)                                    (b)                                sistent with probability matching than maximizing (Vulkan,
          D           R                          D            R
                                                                            2002). Thus, another reasonable decision function is a utility
                      D  R   U                                D  R U
                      r  r   1                                r  r 1        matching function.
                      r  b   0                                r  b 0
                U                                       U
                      b  r   0                                b  r 0                                          EU(di )
                      b  b   1                                b  b 1                              σ(di ) =                               (2)
      Decision function: Max                  Decision function: Max                                        ∑ j EU(d j )
                                                                               IDs and Bayes nets are closely related, and it is possible to
  (c)         D    R                                  D    R
               r   r                                   r   r                “compile” any ID into an equivalent Bayes net by converting
               b   b                                   b   b
                                                                            all nodes to chance nodes and choosing CPDs that are con-
          D           R                          D            R
                               Utility function                             sistent with the ID’s decision function. For example, the ID
                      D  R   U                                D  R U
                      r  r   1 changes                        r  r 0
                                                                            in Figure 1b can be compiled into the Bayes net shown to
                 U    r
                      b
                         b
                         r
                             0
                             0                           U    r
                                                              b
                                                                 b
                                                                 r
                                                                   1
                                                                   1
                                                                            the left of Figure 1c, where the CPD for D is constructed by
                      b  b   1                                b  b 0        assuming that Alice acts to maximize her utility. The criti-
                                                                            cal difference between the two representations is that the ID
Figure 1: (a) An ID for a single decision with no information.
                                                                            makes the notion of utility maximization explicit, which of-
(b) An ID for a decision with complete information. (c) A
                                                                            fers two important advantages. First, the ID representation
deterministic Bayes net representation of the ID in panel b.
                                                                            supports explanations of intentional action (Malle, 1999). If
When the utility function changes, the Bayes net incorrectly
                                                                            the rolled color is red, then the ID can be used to explain that
predicts that Alice will continue to match the rolled color.
                                                                            Alice chooses red in order to maximize her utility. The Bayes
                                                                            net offers no such explanation, and can only indicate that Al-
edges to utility nodes represent the information that is rele-              ice always chooses red when the rolled color is red. Second,
vant to the decision maker’s state of satisfaction; we will refer           the ID representation automatically predicts how Alice’s ac-
to these edges as value edges. In our example IDs, there are                tions will change if the utility function changes. Suppose the
value edges from D and R to U representing the fact that Al-                game changes and Alice is now rewarded for choosing the
ice’s reward depends on the values of both of these variables.              opposite of the rolled color. After updating the utility func-
   Just as a full Bayes net specification consists a set of con-            tion accordingly, the ID representation predicts that Alice will
ditional probability distributions (CPDs) as well as a graph                now choose a color different from the rolled color. Figure 1c
structure, an ID requires some additional components: a CPD                 illustrates, however, that updating the utility node U in the
for each chance node, and a utility function that maps the                  Bayes net leaves the CPD for the decision node unchanged.
joint values of each utility node’s parents to a utility value. In          As a result, the Bayes net incorrectly predicts that Alice will
our example, Alice is rewarded only when her chosen color                   continue to match the rolled color.
matches the rolled color, as shown in the utility tables in Fig-
ures 1a and 1b, where r and b correspond to the two colors                            Modeling other people’s decisions
red and blue.                                                               Although IDs were initially proposed as a way for decision
   Once a utility function is specified, the expected utility EU            makers to compute optimal decisions under uncertainty, they
associated with each possible action di can be computed by                  can also be used to represent other people’s decisions. From
summing over the unknown variables. For example, when                       this perspective, IDs can be used to understand two kinds of
Alice cannot see the outcome c of the roll before making her                mental state inferences: prediction and learning.
choice, as in Figure 1a, the expected utility associated with                  Prediction is possible when a person has full information
choosing red is EU(r) = ∑c∈{r,b} u(r, c)P(c), where u(·, ·) is              about another person’s decision problem, that is, a fully spec-
the utility function shown in the table in the figure. However,             ified ID can be constructed for that person. Predictions can
if Alice is able to see the outcome of the roll before mak-                 then be made about the utilities that person will assign to
ing her choice, as in Figure 1b, there is no uncertainty in the             possible actions, or the action he or she will take (e.g., by
expected utility computation: EU(r) = u(r, c).                              Equation 1). Additionally, because IDs can represent causal
   The final component of IDs is a decision function σ that                 relationships using chance nodes and causal edges, it is pos-
specifies the probability of selecting an action di for that de-            sible to make predictions about events, just as with standard
cision node D. A simple choice of σ is a utility maximizing                 Bayes nets.
function, which characterizes the behavior of a rational agent.                In cases where some details about the decision problem
                        (                                                   are uncertain or unknown, it may be possible to learn these
                           1, if di = arg maxd EU(d)                        details by observing the person make some decisions. These
             σ(di ) =                                                (1)
                           0, otherwise                                     situations involve two types of learning problems: structure
                                                                            learning and parameter learning. In terms of IDs, these two
If the die in our example has five red sides and one blue side              problems correspond to learning the graph structure and the
and Alice maximizes her utility, she will choose red if she                 ID parameterization, respectively. For example, a person’s
cannot see the outcome of the roll, as in Figure 1a. Under                  utility and decision functions may be known but not what in-
some conditions, however, people’s behavior is more con-                    formation is available when he or she makes a decision. This
                                                                        2499

(a)
                                                                         shapes. In each round of the game, the machine randomly
                                                                         selects two different shapes from the set of three and displays
                                                                         them in the left two boxes (Boxes 1 and 2). The player then
                                                                         gets to select one of the three shapes to display in the third
(b)
                                                                         box (Box 3). The second component (Figure 2b) is a card
                                                                         with holes in it, the “player card”, that is placed over the ma-
(c)
       Box 1         Box 2   Box 1         Box 2 Box 1         Box 2     chine at the beginning of the round. There are three different
                                                                         cards: one card covers Boxes 1 and 2 of the machine, one
             Box 3                 Box 3               Box 3             card covers just Box 1, and one card covers no boxes. Thus,
                                                                         depending on the card, the player may be unable to see one
                                                                         or both of the shapes picked by the machine before picking a
             Utility               Utility             Utility
                                                                         shape. The goal of the game is to pick a shape that is different
                                                                         from the shapes picked by the machine. Players are awarded
(d)                                                                      10 points for each pair of mismatching shapes for a maximum
       Box 1         Box 2   Box 1         Box 2 Box 1         Box 2
                                                                         of 20 points per round.
             Box 3                 Box 3               Box 3                  In the inference task, a record of 10 rounds from another
                                                                         player is provided, which shows the three shapes from each
                                                                         round but not the card. It is assumed that the same card was
             Utility               Utility             Utility           used in all 10 rounds. The goal is to infer the card used in the
                                                                         game, based on the player’s record of gameplay.
Figure 2: (a) The machine and the three shapes the player                Model
may pick from in the shape game. (b) The three different
                                                                         IDs representing the shape game for each card are shown in
cards in the game. (c) IDs representing the decision prob-
                                                                         Figure 2c. In these graphs, the contents of Boxes 1 and 2 are
lem for each card in Experiment 1. (d) IDs representing the
                                                                         represented by chance nodes, the player’s choice for the shape
decision problem for each card in Experiment 2.
                                                                         in Box 3 is represented by a decision node, and the awarded
                                                                         points are captured by a utility node. A player’s score always
corresponds to learning what knowledge edges are present in              depends on the contents of all three boxes, but some cards
the ID. Similarly, one might learn what a person values (what            hide the contents of the machine’s boxes before the player
value edges are present) or what causal dependencies exist               makes a decision. Thus, the IDs differ only in the presence
(what causal edges are present). Parameter learning applies              of knowledge edges. In other words, inferring the card used
when an ID graph structure is known, but the precise nature              involves making an inference about what a player knows, or
of the relationships between variables is not. This can involve          what knowledge edges are present.
learning the CPDs of chance nodes or the utility functions of                 Fully specifying the IDs in Figure 2c requires a decision
utility nodes.                                                           function that defines a probability distribution over the three
    Structure and parameter learning for chance nodes have               options of each decision di ∈ {, 4, #}. Later we present
been previously explored in the context of causal Bayes nets             modeling results based on both the utility maximizing func-
(Griffiths & Tenenbaum, 2005; Lagnado & Sloman, 2004;                    tion (Equation 1) and utility matching function (Equation 2).
Steyvers, Tenenbaum, Wagenmakers, & Blum, 2003). The                     Finally, because rounds are independent, given an ID I j and a
remainder of this paper will focus on the less-studied prob-             record of n rounds d = (d1 , . . . , dn ), σ(d|I j ) = ∏i σ(di |I j ).
lem of structure learning applied to decision and utility nodes,              The inference task can now be framed as a model selec-
that is, learning what knowledge and value edges are present             tion problem where the models are the IDs corresponding to
in an ID.                                                                the three cards. We use Bayes’ rule to compute the probabil-
                                                                         ity of each ID given a set a observed decisions. For an ID
                           Experiment 1                                  I j , P(I j |d) ∝ σ(d|I j )P(I j ). We assume a uniform prior dis-
                                                                         tribution P(I j ), reflecting the fact that all cards are equally
The goal of our first experiment was to examine whether the
                                                                         probable.
ID framework can be used to capture how people reason about
what other people know. We addressed this question by de-                Method
vising a game called the shape game that allowed us to ask
participants about what other players knew during the game               Participants Fifteen Carnegie Mellon undergraduates
based on records of their gameplay.                                      completed the experiment for course credit.
                                                                         Design and Materials There are three possible outcomes
The shape game                                                           for each round: all different shapes (outcome D), matching
The shape game consists of two components. The first com-                shapes in Boxes 1 and 3 (outcome M1), or matching shapes in
ponent (Figure 2a) is a machine with three boxes that display            Boxes 2 and 3 (outcome M2). It is not possible for the same
                                                                     2500

    Condition     Gameplay record                                    the two-hole and three-hole cards after the first M1 outcome,
                                                                     but decreases the probability assigned to the three-hole card
                  D, D, D, D,      D, D,    D,   D,    D, D          until the first M2 outcome is observed, at which point this
                                                                     probability immediately rises to 1.
                  D, D, D, M1, D, M1, M1, M1, D, D
                                                                     Human judgments Mean human judgments are shown in
                  D, D, D, M1, D, M1, M1, M2, D, D                   the first row of Figure 3a. In order to convert participants’
                                                                     judgments on the 1 to 7 scale to approximate probabilities,
Table 1: Gameplay records used in the three conditions of            the ratings in each round were first decremented by 1 to make
Experiments 1 and 2.                                                 0 the lowest value. Then the ratings were normalized by di-
                                                                     viding by their sum to obtain ratings that summed to 1.
                                                                        In every round of the three conditions, the ordering of par-
shape to be in all three boxes because the machine always            ticipants’ ratings is consistent with the model’s predictions.
picks two different shapes. Participants saw three gameplay          Overall, the model captures many of the qualitative trends in
records made up of these three outcomes, creating three con-         the human data, resulting in a high correlation between the
ditions, shown in Table 1. These conditions were randomly            human data and the model’s predictions (r = 0.95). One de-
ordered and the specific shapes that appeared in each record         viation from the model can be found in the later rounds of
were randomly generated for each participant.                        the one-hole card condition. Whereas the model predicts cer-
    These sequences were designed to instill some uncertainty        tainty in favor of the one-hole card, participants’ judgments
in the earlier rounds about the card being used, but to strongly     were less certain and decreased in the final two rounds. This
favor one of the three cards by the final round. For example,        effect, however, appears to have been driven by a subset of
in the first sequence consisting entirely of D outcomes, it is       participants who took into account the possibility that the
possible for a player who cannot see Box 1 or Box 2 to get           solitary M2 round in this condition was a mistake, a possi-
lucky and choose a mismatching shape every time, but this            bility that was explicitly noted in several participants’ expla-
outcome becomes less likely as the length of the sequence            nations.
increases. In the third sequence, there is increasingly strong       Alternative models We compared the human judgments to
evidence that the player was using the card with two holes un-       two alternative models designed to test the importance of
til the M2 round, when the one-hole card seems more likely.          the two key components of our model: utility maximiza-
    The entire experiment was conducted using a graphical in-        tion and probabilistic inference. We tested the maximizing
terface on a computer. The outcome of each round was shown           assumption by implementing a utility matching model that
as a machine like the one in Figure 2a with all three boxes          used a utility matching decision function (Equation 2). This
filled with a shape.                                                 model’s predictions are shown in the third row of Figure 3a.
Procedure Participants were first familiarized with the              Clearly this model offers a poor account of human behavior
shape game by playing six rounds with each of the three              (r = 0.61), especially in the second and third conditions. This
cards. Once they indicated that they understood the game,            suggests that, as predicted, in this simple task, participants
they began the inference task. The sequences of rounds were          assumed that the player they were assessing behaved mostly
displayed one at a time with all previous rounds remaining on        optimally.
the screen. After viewing each round, participants were asked           Next, we tested the probabilistic assumption of the first
to judge how likely it was that the player had been using each       model by comparing it to a purely logical model. The util-
of the three cards for the entire sequence of gameplay. They         ity maximizing model assigns increasing probability to the
made their judgments for each card on a scale from 1 (very           three-hole card in the first condition because a long sequence
unlikely) to 7 (very likely). They were also asked to give a         of D outcomes is highly improbable under any other circum-
brief explanation for their judgments.                               stances. This outcome, however, is logically consistent with
                                                                     any one of the three cards. Thus from a logical standpoint,
Results                                                              only the M1 and M2 rounds are definitively informative. Pre-
Model The first model we considered used a utility maxi-             dictions based on this approach are shown in the fourth row of
mizing decision function (Equation 1). Given the simple na-          Figure 3a. Contrary to the logical model’s predictions, how-
ture of the game and participants’ own experience with it, we        ever, participants did gradually adjust their ratings on rounds
predicted that they would expect other players to generally          that weren’t definitively informative (r = 0.45), consistent
play optimally. Predictions from this model are shown in the         with the utility maximizing model.
second row of Figure 3a. In the first condition, the model              Finally, we examined whether these results could be ac-
assigns increasing probability to the three-hole card as the         counted for by a standard Bayes net structure learning model.
number of rounds (all D outcomes) increases. In the second           Recall that any ID can be compiled into an equivalent Bayes
condition, the model rapidly changes its probabilities in fa-        net. Compiling the IDs in Figure 2c into Bayes nets and per-
vor of the two-hole card after the first M1 outcome. In the          forming model selection over these networks is one way to
third condition, the model raises the probability assigned to        implement our ID model. This approach, however, still relies
                                                                 2501

 (a)                        Condition                                 (b)                     Condition
               1
Rating
              0.5                                           Human                                                           Human
               0
               1
Probability
                                                            Max                                                             Max
              0.5                                           r = 0.95                                                        r = 0.92
               0
               1
Probability
                                                            Match                                                           Match
              0.5                                           r = 0.61                                                        r = 0.51
               0
               1
Probability
                                                            Logical                                                         Logical
              0.5                                           r = 0.45                                                        r = 0.71
               0
               1
Probability
                                                            Bayes net                                                       Bayes net
              0.5                                           r = 0.86                                                        r=0
               0
                    Round    Round              Round                         Round            Round             Round
Figure 3: Experiment results and model predictions for (a) Experiment 1 and (b) Experiment 2. In both experiments, the
utility maximizing ID model (labeled Max) was the best fitting model we considered. In all plots, the cyan line (× markers)
corresponds to the three-hole card, the blue line (+) corresponds to the two-hole card, and the red line (·) corresponds to the
one-hole card. The error bars in the human plots are standard errors. For the models, r is the correlation coefficient between
the model’s predictions and the human judgments.
critically on the assumption of utility maximization. We im-                                 Experiment 2
plemented a third alternative model to test whether a Bayes             Experiment 1 showed that people’s inferences about what an-
net approach could account for our results without incorpo-             other player knew in the shape game were highly consistent
rating this assumption. We treated the graphs in Figure 2c              with a model selection account using IDs and a maximizing
as four-node Bayes nets with a known CPD for the utility                utility function. The purpose of Experiment 2 was to apply
node and implemented a standard Bayesian structure learn-               this same account to a task involving an inference about what
ing model with uniform Dirichlet priors on the CPDs for the             another person values.
other nodes (Heckerman, Geiger, & Chickering, 1995). The
model’s predictions are shown in the last row of Figure 3a.             Revised shape game
                                                                        In order to address this question, we made a slight modifi-
                                                                        cation to the shape game. In the previous version, the cards
   The model performs reasonably well overall (r = 0.86) but            were placed over the machine at the beginning of the round.
is inferior to the utility maximizing ID model in two respects.         In the current version, the cards—now called “judge cards”—
First, after only one round, the Bayes net model assigns equal          were not placed over the machine until the end of each round.
probability to all three cards, since a single round provides           Thus, in the judge card version of the game, players are able
no information about the existence of causal relationships be-          to see the shapes in all boxes when making their selections.
tween the boxes. The ID model, however, assumes that the                   The judge card determines how the score for each round
player is attempting to choose a shape for Box 3 that does not          is computed: Only the shapes not covered by the card are
match Box 1 or Box 2, and observing a single round where                counted. Thus, when the judge card covers Box 1, the maxi-
this goal is achieved suggests that the player is able to see           mum number of points is 10, when the player’s shape is dif-
both boxes. The second limitation of the Bayes net model                ferent from the shape in Box 2. When the judge card covers
is that it fails to predict the dramatic change in participants’        Boxes 1 and 2, there are no shapes to mismatch and 10 points
ratings after the first M1 round.                                       are awarded no matter what shape the player picks.
                                                                   2502

Model                                                                 way to supplement Bayes nets with the knowledge that ac-
IDs representing the decision problem for each card in the            tions are chosen in order to achieve goals. We propose that
judge card version of the shape game are shown in Figure 2d.          any successful account of mental state reasoning will need to
The player always gets to see the contents of Boxes 1 and 2,          represent this knowledge in a transparent and explicit way.
but the awarded points may not depend on the contents of all
                                                                                                References
boxes. Thus, the IDs in Figure 2c differ only the presence of
value edges. In other words, inferring the card used involves         Baker, C. L., Saxe, R., & Tenenbaum, J. B. (2009). Action
making an inference about what a player values, or what value            understanding as inverse planning. Cognition, 113, 329-
edges are present. The remaining details of the model were               349.
identical to those in Experiment 1.                                   D’Andrade, R. G. (1987). A folk model of the mind. In
                                                                         D. Holland & N. Quinn (Eds.), Cultural models in lan-
Method                                                                   guage and thought. Cambridge: Cambridge University
All of the participants from Experiment 1 also participated in           Press.
Experiment 2 (in a random order), with two additional par-            Gal, Y., & Pfeffer, A. (2008). Networks of influence di-
ticipants whose data from Experiment 1 were lost due to an               agrams: a formalism for representing agents’ beliefs and
error (total N = 17). Experiment 2 was identical to Experi-              decision-making processes. Journal of Artificial Intelli-
ment 1 except participants made judgments about the judge                gence Research, 33(1), 109-147.
card version of the game.                                             Griffiths, T. L., & Tenenbaum, J. B. (2005). Structure and
                                                                         strength in causal induction. Cognitive Psychology, 51,
Results                                                                  354-384.
                                                                      Heckerman, D., Geiger, D., & Chickering, D. M. (1995).
The mean human judgments and model predictions are shown
                                                                         Learning Bayesian networks: The combination of knowl-
in Figure 3b. The utility maximizing and logical models make
                                                                         edge and statistical data. Machine Learning, 20, 197-243.
the same predictions as in Experiment 1. This is because a
                                                                      Howard, R. A., & Matheson, J. E. (2005). Influence dia-
player who is unable to see the shape in Box 1 is effectively
                                                                         grams. Decision Analysis, 2(3), 127-143.
equivalent to a player who does not care about the contents
                                                                      Koller, D., & Milch, B. (2003). Multi-agent influence di-
of that box. The prediction that the two experiments produce
                                                                         agrams for representing and solving games. Games and
similar results is largely supported by the human data, which
                                                                         Economic Behavior, 45(1), 181-221.
are similar across the two experiments, and utility maximiz-
                                                                      Lagnado, D. A., & Sloman, S. A. (2004). The advantage of
ing model once again performs well (r = 0.92). The util-
                                                                         timely intervention. Journal of Experimental Psychology:
ity matching model produces different predictions in the two
                                                                         Learning, Memory, and Cognition, 30(4), 856-876.
experiments due to the slightly different point assignment
                                                                      Malle, B. F. (1999). How people explain behavior: A new
policies, but again offers a poor account of the human data
                                                                         theoretical framework. Personality and Social Psychology
(r = 0.51). Finally, the Bayes net model is unable to make
                                                                         Review, 3(1), 23-48.
any inferences in the judge card version of the game. This re-
                                                                      Oztop, E., Wolpert, D., & Kawato, M. (2005). Mental state
sult is a consequence of the fact that observed actions cannot
                                                                         inference using visual control parameters. Cognitive Brain
be used to make inferences about a utility function without
                                                                         Research, 22, 129-151.
some assumption about how actions and utility are related
                                                                      Perner, J. (1991). Understanding the representational mind.
(e.g., by a decision function).
                                                                         Cambridge, MA: MIT Press.
                         Conclusion                                   Schultz, T. R. (1988). Assessing intention: A computational
                                                                         model. In J. W. Astington, P. L. Harris, & D. R. Olson
The results of our two experiments suggest that people take              (Eds.), Developing theories of mind. New York: Cam-
both decision functions and probabilistic information into ac-           bridge University Press.
count when reasoning about mental states. The different pre-          Sloman, S. A. (2005). Causal models: How people think
dictions of the utility maximizing and utility matching models           about the world and its alternatives. New York: Oxford
supported the idea that people expected others to play nearly            University Press.
optimally, a reasonable expectation in our simple task. How-          Steyvers, M., Tenenbaum, J. B., Wagenmakers, E.-J., &
ever, this utility maximizing assumption alone was not suffi-            Blum, B. (2003). Inferring causal networks from obser-
cient to capture people’s inferences, as indicated by the differ-        vations and interventions. Cognitive Science, 27, 453-489.
ent predictions of the utility maximizing and logical models.         Vulkan, N. (2002). An economist’s perspective on probability
   The influence diagram framework accounted well for both               matching. Journal of Economic Surveys, 14(1), 101-118.
experiments, and performed better than a Bayes net model              Wahl, S., & Spada, H. (2000). Children’s reasoning about
that did not incorporate the notion of utility maximization.             intentions, beliefs, and behaviour. Cognitive Science Quar-
Although Bayes nets share many of the strengths of IDs, they             terly, 1, 5-34.
are not naturally suited for reasoning about intentional agents.
The influence diagram approach can be viewed as a natural
                                                                  2503

