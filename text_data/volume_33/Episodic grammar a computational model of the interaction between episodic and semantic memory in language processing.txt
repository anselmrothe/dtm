UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Episodic grammar: a computational model of the interaction between episodic and semantic
memory in language processing
Permalink
https://escholarship.org/uc/item/2265k26n
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Borensztajn, Gideon
Zuidema, Willem
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

    Episodic grammar: a computational model of the interaction between episodic
                                 and semantic memory in language processing
                                                Gideon Borensztajn (gborensztajn@uva.nl)
                                                     Willem Zuidema (zuidema@uva.nl)
                                    Institute for Logic, Language and Computation, 904 Science Park
                                                  1098 XH, Amsterdam, The Netherlands
                              Abstract                                   A substantial part of our memory is dedicated to representing
   We present a model of the interaction of semantic and episodic        linguistic knowledge in more or less abstracted form. The re-
   memory in language processing. Our work shows how lan-                search we report here starts from the observation that the sci-
   guage processing can be understood in terms of memory re-             entific debate on the relation between semantic and episodic
   trieval. We point out that the perceived dichotomy between
   rule-based versus exemplar-based language modelling can be            memory can be meaningfully linked to an ongoing contro-
   interpreted in a neuro-biological perspective in terms of the in-     versy about modeling language: one side in this controversy
   teraction between a semantic memory system that encodes lin-          is focusing on empirical evidence for abstract, rule-based
   guistic knowledge in the form of abstract rules, and an episodic
   memory that stores concrete linguistic events. We implement           grammars [e.g. Marcus, 2001], while the other side empha-
   the idea of a semantic-episodic memory integration in a prob-         sizes the item-based nature of grammar with a role (partic-
   abilistic grammar, and evaluate its performance as a syntactic        ularly in acquisition) for concrete sentence fragments larger
   parser on corpora of natural language. Our labeled precision
   and recall results are competitive with state-of-the-art syntac-      than rules [e.g., Tomasello, 2000]. In our view, the rule-based
   tic parsers, with F-scores up to 90.68 on section 22 of the Penn      grammars are best thought of as instances of semantic mem-
   WSJ corpus.                                                           ory, since they encode abstract, relational linguistic knowl-
                                                                         edge. The item-based approach, on the other hand, suggests
                          Introduction                                   a role for episodic memory in sentence processing.
Our ability to understand language depends, in part, on the
                                                                            In computational linguistics Scha [1990] and Bod [1998]
declarative memory system, which is involved in representing
                                                                         proposed Data-oriented Parsing (DOP) as a model that rec-
factual knowledge and experiences with the external world.
                                                                         onciles exemplar- and rule-based views. The productive units
Declarative memory is, since Tulving [1972], further divided
                                                                         of DOP can range from single words to a complete phrase-
into two components: (i) Semantic memory is a person’s gen-
                                                                         structure trees. Such models however, as well as more recent
eral world knowledge, including language, in the form of
                                                                         Bayesian models [e.g., O’Donnell et al., 2009], operate on the
concepts that are systematically related to each other; (ii)
                                                                         functional level, but make no effort to include details from
Episodic memory is a person’s memory of personally expe-
                                                                         real cognitive or neural processes. The model we develop
rienced events or episodes, embedded in a temporal, spatial
                                                                         in this paper is also used for parsing: it assigns grammat-
and emotional context. For example, the memory of the walk
                                                                         ical structure to new sentences, and disambiguates, using a
from your home to the bakery on a rainy Monday morning
                                                                         probability model, between potentially very many grammati-
is part of the episodic memory system, while the concept of
                                                                         cal analyses of each sentence. Assigning grammatical struc-
bread with all its associations is part of semantic memory.
                                                                         ture (rather than grammaticality judgments) as an intermedi-
   Although there is general consensus that episodic mem-
                                                                         ate step before semantic interpretation is, in this tradition, the
ory and semantic memory are not separate modules, the ex-
                                                                         essential function of grammar.
act nature of their relation is still an open question. A com-
mon view is that episodic memories are constructed as point-                Unlike the vast majority of existing parsing models, how-
ers that bind together items stored in semantic memory, both             ever, we also aim to take some basic assumptions about
in temporal relations and in relations between roles or par-             episodic memory into account: (i) all experiences that occur
ticipants in an event [Shastri, 2002]. Such a conception of              in the lifespan of a human being, and that can be consciously
episodic memory fits with a popular theory in cognitive neu-             remembered, leave physical memory traces in the brain. In
roscience, the so-called reinstatement hypothesis of episodic            the language domain, this means that every sentence ever pro-
retrieval, which says that during episodic memory retrieval              cessed will leave traces in memory; (ii) episodic memories
memory traces are triggered, thereby reactivating the cortical           are stored as temporal sequences of static semantic memories
circuits that were involved in encoding the episodic memory              [e.g., Eichenbaum, 2004]; (iii) episodic memories are content
[e.g., Woodruff et al., 2005]. Yet, to date there exists no              addressable. This entails that their retrieval can be primed by
theory of episodic memory that is sufficiently formalized to             cues from semantic memory.
make quantitative and testable predictions about the relative               In the next section we will give an outline of the model
contributions of episodic memory and semantic memory in                  and discuss two different ways of representing earlier lan-
any realistic cognitive task.                                            guage experiences (corresponding to top-down and left-
   The distinction between episodic and semantic memory is               corner derivation). The ensuing section details how we col-
also important for a cognitive approach to language research.            lect the relevant statistics from a corpus of syntactically an-
                                                                     507

notated sentences and define the probability model. In the             the traces (for a top-down derivation) are identified by these
remainder of the paper, we present our empirical results and           two numbers, indicated as hs, ki inside the treelets. Note, that
discuss the relation between our approach and other work in            after hearing many sentences a single treelet will store traces
statistical parsing and memory research.                               for all sentences that have visited it, which are distinguished
                                                                       by their sentence number, and possibly multiple visits from
              Episodic grammar: model outline                          the same sentence.
In our approach we take the point of view that a grammar                  The episodic sentence memories distributed across the
is instantiated in the brain as a network of interconnected            traces can also be recruited for the purpose of processing
autonomous syntactic processing units (henceforth treelets).           novel, unseen sentences. The idea is that when the derivation
For the sake of simplicity we assume in this paper that                of a novel sentence arrives at a treelet, the traces encountered
context-free rules (for instance S → NP VP) from traditional           within the treelet trigger memories of stored exemplars. The
grammars correspond one-to-one to such treelets. Treelets              strength of the activation of those memories depends on how
can serially combine with each other through dynamic bind-             much the stored derivations have in common with the cur-
ing (resulting in substitution), and they possess a register (in-      rent derivation (described more formally in the next section).
ternal memory) that keeps track of the correct order of appli-         Every next step in the derivation is determined by competi-
cation of the binding operations. This perspective on syntax           tion between traces of different exemplars, each having its
generalizes to a connectionist account, which is worked out            own preference for a successor treelet, and its own activa-
in [Borensztajn et al., 2009].                                         tion strength. In this view sentence processing can be under-
   The network of interconnected treelets in our model con-            stood as subject to a priming effect: the traces prime or reac-
stitutes a semantic memory of a grammar. We propose that               tivate derivations of previously processed sentences (through
the episodic memory of a sentence is distributed across the            content addressability), and restore the memory of previous
treelets, and consists of physical traces, contained inside the        parser decisions.
treelets, that keep a record of the treelet’s participation in the
derivation of the processed sentence. This is illustrated in
Figure 1, which shows the episodic memory traces left behind
in the network after processing the sentences girl who dances
likes tango (orange traces) and boy likes mango (blue traces),
corresponding to a top-down leftmost derivation strategy.
                                                                       Figure 2: Episodic memory traces in the LCE grammar after
                                                                       deriving the sentence girl who dances likes tango.
                                                                          One of the advantages of the episodic approach is that it
                                                                       allows comparison of different derivation strategies within a
Figure 1: Episodic traces of two sentences (drawn as colored           single framework, to find the effect of a different order of
ovals) are stored in local memories of visited processing units        application of operations on treelets. An interesting parsing
(indicated by triangles and rectangles). Note, that by virtue of       strategy from a cognitive point of view is left corner pars-
their ordinal number the traces implement pointers to succes-          ing [Rosenkrantz and Lewis II, 1970], since it proceeds in-
sor treelets in a derivation (drawn for the first sentence alone)      crementally from left to right and combines top-down and
                                                                       bottom-up parsing. Figure 2 shows an episodic left corner
   Whereas in the context-free grammar framework a deriva-             derivation for the sentence girl who dances likes tango. In left
tion of a sentence is a sequence of rule instantiations, in our        corner parsing treelets are introduced bottom-up by a project
framework, a derivation is a sequence of visits to treelets:           operation. As long as there are no completed (i.e., fully pro-
ht0 , t1 , . . . , tn i. In order to remember the correct order of     cessed) treelets the next word in the sentence is introduced by
derivation (which can vary depending on the chosen deriva-             a shift operation; otherwise the derivation can either project
tion strategy) the episodic traces encode the sentence number          to a new treelet, or attach to a not yet completed treelet that
(s) as well as the position (k) within a derivation. In Figure 1       has been previously introduced (In figure 2 these operations
                                                                   508

 are abbreviated with sh, pr and att). Note, that we have intro-              much to the parser decision. A convenient choice for the ac-
 duced special treelets that execute the shift to the next word               tivation of a trace is
 (e.g., RC∗ → dances). These treelets employ starred non-
 terminals (e.g., RC∗): one or more stars indicate the register                                      A(exi ) = αCH(exi ,d)                         (1)
 position in the treelet from where the shift operation origi-
                                                                              where α is a parameter of the model. Depending on the
 nates (e.g., RC → WHO ∗ VI) 1 .
                                                                              chosen derivation strategy (e.g., top-down or left corner) the
    One important difference with the top-down derivation
                                                                              traces have different CH’s, hence receive different activations.
 strategy is that upon every attach operation treelets are reen-
                                                                              All information to calculate these activations is stored inside
 gaged in the derivation. It is therefore necessary to asso-
                                                                              treelet tq ; computations are thus local, in line with our desire
 ciate an episodic trace with a specific register state of the
                                                                              to define a cognitively and neurally plausible model.
 treelet, which keeps track of the operations (project, attach)
                                                                                 The probability of moving to tq0 in the next step of the
 performed on the treelet. This is indicated in Figure 2 by
                                                                              derivation is simply the sum of activations of traces that point
 adding a dot before or after the trace2 .                                                                                                     t 0
                                                                              to tq0 , divided by the sum of all activations. Let Etqq be the
     Statistical Parsing with Episodic Grammar                                set of traces in treelet tq that point to treelet tq0 , and Etq the
                                                                              full set of traces in treelet tq . Then, the probability of moving
 To evaluate our model of episodic grammar we use a com-
                                                                              the derivation to treelet tq0 is
 mon task in the field of statistical natural language process-
 ing: disambiguation between competing syntactic analyses of                                                        P
                                                                                                                            t 0 A(e)
 a sentence (parses). Probabilistic grammars assign probabili-                                                         e∈Etqq
                                                                                                  P (tq0 |tq ) = P                 0
                                                                                                                                                   (2)
 ties to different parses of a sentence and, in the most common                                                        e0 ∈Et A(e )
                                                                                                                             q
 setup, select the most probable one. One can estimate the pa-
 rameters of a probabilistic grammar from a treebank, which                      The probability of a complete derivation D is given by:
 is a corpus consisting of natural language sentences manually                                                                 n
 annotated with phrase structure trees.
                                                                                                                              Y
                                                                                          P (D = ht0 , t1 , . . . , tn i) =      P (ti |ti−1 )     (3)
    After deciding on a derivation strategy (i.e., top-down or                                                               i=1
 left-corner), the episodic grammar is trained by distribut-
 ing a trace e = hs, ki in every visited treelet tk of deriva-                   This probability can be computed dynamically, while si-
 tion x = ht0 , . . . , tk , . . . , tn i of sentence #s in the treebank.     multaneously updating the common histories (and activa-
 Specifically, given a treebank, the model:                                   tions) of all traces at every step of the derivation. Let tq and
                                                                              tq0 be two successive treelets in the pending derivation d, and
1. creates an empty treelet for every unique context free pro-                let e0 = hs, ji be a trace stored in tq0 . Then its CH is updated
    duction extracted from the treebank.                                      according to
2. determines, for every treebank parse, the sequential order
    of treelets ht0 , t1 , . . . , tn i, according to the chosen deriva-                         CH(e0 , dq0 ) = CH(e, dq ) + 1                    (4)
    tion strategy.
3. leaves, for every step k in the derivation of sentence #s, a               if there exists a trace e = hs, j − 1i in tq (i.e., a predecessor
    trace in the visited treelet, encoded as hs, ki.                          of e0 ). Otherwise, CH(e0 , dq0 ) = 0.
                                                                                 In order to obtain a non-zero parse probability for test set
    Once all relevant statistics have thus been gathered, we can              derivations that contain words or productions not observed in
 use the model to assign probabilities to candidate parses of a               the training set, we use standard smoothing techniques3 .
 new sentence. Given an ongoing derivation d that has arrived                    The final step is to provide the episodic grammar model
 at a certain treelet tq , we define the probability of continuing            with candidate parses, that are assigned a probability accord-
 the derivation to any other treelet tq0 based on the activation              ing to equation (3). Since we don’t have a specialized parser,
 values of the episodic traces stored in treelet tq . The activa-             but only a probability model, we use our model as a reranker.
 tion A(exi ) of the trace exi (in tq ) of earlier derivation x is a          This entails that one takes a list of n best parses for every sen-
 function of the common history CH(exi , d) of derivation x (of               tence produced by a third party parser (in this case Charniak’s
 which exi is the ith trace) with the ongoing derivation d. The                   3
                                                                                    For unknown words we adopted the approach of [Sangati and
 CH is given by the number of derivation steps (i.e., treelets)               Zuidema, 2011], replacing words that occurred less than 5 times
 that the stored derivation x and the pending derivation d have               in the train set by a class label. The treebank parses were bina-
 shared the same path before arriving at tq . Episodic traces                 rized and horizontally Markovized, as proposed by [Klein and Man-
                                                                              ning, 2003]. Further, three levels of linear interpolation smooth-
 that share a long common history should contribute relatively                ing were used. The first level backs off to a non-episodic version
     1
                                                                              of the used derivation strategy (e.g., PCFG probabilities), the sec-
       The addition of shift treelets allows for computing sentence           ond level backs off to less specific non-terminal labels, and the last
 probabilities, and thus turns the grammar into a language model.             level assigns uniform probabilities to all possible combinations of
 This approach is similar to the left corner parser of van Uytsel et al.      non-terminals that form unary and binary productions. These levels
 [2001].                                                                      were parametrized by back-off parameters λ1 , λ2 and λ3 . For tech-
     2
       There can be as many register positions as there are children in       nical details please refer to the webpage accompanying this paper:
 the treelet.                                                                 http://staff.science.uva.nl/∼gideon/cogsci2011.
                                                                          509

maximum entropy parser [Charniak, 1999]), and reranks the            episodic top-down reranker surpasses the Charniak F-scores
list by assigning a probability to each parse under the model        by a slight margin, and overall does much better than the
of interest. We then used the PARSEVAL metric to evaluate            PCFG reranker (corresponding to history 0) and the random
labeled precision (LP), labeled recall (LR) and their harmonic       reranker.
mean (F-score) of the parses that receive the highest probabil-
ity under the reranker [Manning and Schütze, 2000, p. 432].
   Reranking does have some limitations as an assessment of
the model’s performance, since the n best parses list produced
by the third party parser has upper and lower bound precision
and recall scores. For comparison we give the scores of a
random reranker, that selects a parse from the list by chance.
Confidence in the results of the reranker increases with the
size of the n-best list (e.g., see Figure 4).
              top down reranker       left corner reranker
  max his  LR       LP      F       LR       LP        F
  0        87.11 90.01      88.54 87.93 90.31 89.10                  Figure 3: F-scores compared between the top-down and the
  1        89.53 90.27      89.90 89.35 90.22 89.79                  left corner episodic reranker as a function of conditioning his-
  2        89.64 90.23      89.94 89.49 90.30 89.89                  tory. Also shown is a LCE reranker that uses discontiguities.
  3        90.15 90.45      90.30 89.64 90.43 90.04
  4        90.15 90.39      90.27 89.79 90.53 90.16                                                   .
  5        90.27 90.45 90.36 89.91 90.63 90.27
  6        90.23 90.41      90.32 89.96 90.58 90.27
  7        90.19 90.37      90.28 90.13 90.76 90.44
                                                                        As can be seen from Table 1, the left corner episodic gram-
  8        90.09 90.21      90.15 90.32 90.90 90.61                  mar (LCE) performs better across the board than the top down
  9        90.14 90.27      90.20 90.29 90.84 90.56                  episodic grammar (TDE), and this is mainly due to improved
  10       90.03 90.16      90.09 90.23 90.79 90.51                  labeled precision scores. It also does better than the proba-
  11       89.98 90.14      90.06 90.10 90.74 90.42
  12       89.91 90.11      90.01 90.07 90.67 90.37                  bilistic left corner model of [Manning and Carpenter, 1997],
  Ran                    88.15 87.89 88.02                           corresponding to his= 0. Note, that for the LCE reranker the
  Ch                     90.23 90.15 90.19                           peak is reached at history 8, and the F-scores stay high until
                                                                     history 14; this could be an indication that the order of condi-
Table 1: Precision and recall scores of the episodic top-down
                                                                     tioning in an LCE derivation better approximates human sen-
reranker (columns 1-3) and left corner reranker (columns 4-6)
                                                                     tence processing than in a TDE derivation. It is remarkable
as a function of the maximum history considered (nBest=5;
                                                                     that the LCE grammar – in its clean implementation, without
α=4; λ1 =λ2 =λ3 =0.2).
                                                                     any tricks popular in statistical parsing (like “head annota-
                                                                     tion”) – robustly improves on the Charniak parser, while the
                 Experiments and results                             latter is the result of a very non-trivial engineering effort.
We evaluated the performance of the episodic grammar on
parsing the Wall Street Journal (WSJ). As is standard in sta-
tistical NLP, we used WSJ sections 2-21 for training. We used
sentences of length upto 40 words from section 22 for testing
(we reserve section 23 for evaluation of future versions of
the model). The precision and recall results of the episodic
top-down reranker, applied to the top 5 Charniak parses, are
given in the first three columns of Table 1 as a function of
the maximum common history that is taken into account by
the episodic grammar (the column max his). CH’s larger than
the maximum history are thresholded, resulting in maximum            Figure 4: F-scores of the left corner episodic reranker applied
values for the trace activation, according to equation 1. The        to the top 5, top 10 and top 20 Charniak parses.
bottom 2 rows give the the scores for the random reranker and                                         .
for our run of the Charniak parser. We have experimented
with different parametrizations of α, λ1 , λ2 , λ3 . Optimal re-        To assess the robustness of the reranking method we have
sults were obtained for α = 4, and λ1 , λ2 , λ3 in the range         also applied the LCE reranker to the top 10 and the top 20 lists
between 0.1-0.3, with only little variance.                          of Charniak parses. In the latter case the random baseline is
   In Table 1 and Figure 3 one can see a clear effect of             significantly lower than for the top 5 reranker (F-score = 86.2
conditioning history, peaking at history 5 for the top-down          resp. 88.0), but still the LCE reranker performs almost as
reranker, and at history 8 for the left corner reranker (best        good as Charniak (F-score=90.15 for history 8), while the top
scores are indicated in boldface). For histories 3-7 the             10 reranker does even better (F-score=90.34 for history 9). In
                                                                 510

Figure 4 it can further be seen that although the differences        independence of structural context, that is made by the stan-
in performance between the top 5, top 10 and top 20 reranker         dard PCFG model, is not very realistic either. For instance, in
are large for low histories, they converge for histories of 6-       the WSJ treebank a noun phrase (NP) expands 9 times more
10, when the episodic approach starts to make a difference.          often to a personal pronoun in subject position than in ob-
Nevertheless, the fact that performance drops with larger n          ject position [Manning and Carpenter, 1997]. Johnson [1998]
is a sign that our left-corner models are not yet capturing all      showed that the parsing accuracy of the treebank PCFG is
relevant information usable for statistical parsing.                 greatly increased by incorporating structural information in
                                                                     the node labels, for instance by enriching the labels with the
                                                                     parent label.
Discontiguous episodes An interesting way to extend the
episodic grammar is by including discontiguous episodes.                In the episodic left corner grammar both lexical context
Our intuition is that language users often reuse memorized           and structural context are integrated in the conditioning his-
sentence fragments, even if they do not exactly match the            tory without any need for preprocessing of the labels. As
sentence that is currently being processed. We implemented a         such it is comparable to the tradition of history based pars-
variation of the LCE grammar that can exploit episodes with          ing, which exploits the idea that the parser moves are condi-
‘gaps’. It stores discontinued episodes for later use, when it       tioned on n previous parser decisions in the derivation his-
identifies a trace belonging to the same exemplar derivation as      tory. A weakness of the latter approach is however that it
the interrupted episode. A fixed fraction f of the activation of     leads to very large grammars and data sparsity, since all con-
the interrupted episode is then copied to the new trace. Best        ditioning events are saved explicitly in equivalence classes.
results were obtained when we let the activation of unused           In the episodic grammar, parser decisions are conditioned on
discontiguous episodes decay by some percentage d at every           arbitrarily long histories, at no cost to the grammar size, be-
step of the derivation. With d = 0.95 and f = 0.6 the ad-            cause conditioning context is implicit in the representation,
dition of discontiguous episodes gives a minor improvement           and is constructed explicitly only during on-line processing
over the base LCE model, as can be seen from Figure 3. The           of a novel sentence. Since every exemplar is stored only once
highest F-score is 90.68, which is reached for history 10. The       in the network, the space complexity of the episodic grammar
effect of including discontiguous fragments seems to be that         is linear in the number of exemplars. Another difference with
longer histories play an even more prominent role.                   history-based parsers is that in the latter the association be-
                                                                     tween the conditioning event and the sentence from which it
                                                                     originates is lost, whereas in the episodic grammar the iden-
Shortest derivation reranker We have also implemented                tity of an exemplar that has contributed to a derivation step is
a shortest derivation LCE reranker that selects parses from          preserved. Therefore, history-based parsers cannot make use
the n-best list according to a preference for derivations that       of discontiguous episodes, but the episodic grammar can.
use fragments from the fewest episodes. With a best F-score
                                                                        It is also interesting to compare the episodic grammar with
of 90.44 (for history 9) it performs worse than the base LCE
                                                                     Data Oriented Parsing (DOP) [e.g., Bod, 1998], which is a
reranker, but still better than the Charniak parser.
                                                                     framework for exemplar-based statistical parsing. In DOP
                                                                     the primitive units of the grammar are not CF rules, but sub-
                           Discussion
                                                                     trees of arbitrary size, which are extracted from the parses
The role of sentence context in natural language processing          of a treebank. In a certain sense DOP and episodic parsing
(NLP) has in recent years seen renewed appreciation as is ev-        are complementary: whereas in DOP the substitution of an
ident from the increasing popularity of statistical NL parsers       arbitrary large subtree is conditioned on a single nontermi-
that weaken in one way or another the context independence           nal, in the episodic parser the application of a context-free
assumptions of probabilistic context free grammars (PCFGs).          rule is conditioned on an arbitrary large episode. An advan-
Context free grammars fail to take advantage of two rela-            tage over DOP is that in the episodic framework a derivation
tively independent sources of contextual information for dis-        can also be broken down into fragments according to other
ambiguating between parses: lexical context, which captures          generative processes than top-down, for instance left-corner.
the dependency on previous words in the sentence, and struc-         This opens the possibility to utilise the episodic grammar as
tural context, which captures the dependency on the relative         a language model in speech recognition. Moreover, in the
position in a parse tree.                                            episodic grammar it is not necessary to store every possible
   One remedy that is often used to cope with the lack of lex-       tree fragment explicitly. This is potentially an advantage over
ical context sensitivity of the PCFG is to lexicalize the gram-      existing DOP implementations, which suffer from computa-
mar. Assuming that lexical dependencies are mostly carried           tional inefficiency due to very large grammars. The fact that
between the head words of phrases and their dependents, one          stored episodes are automatically reconstructed from traces
may enrich the constituent labels in the treebank trees with         during the derivation of a novel sentence obviates a time-
their head words, which are percolated up in the tree, and           expensive search through an external memory (i.e., a tree-
subsequently estimate the parameters of the PCFG from the            bank of fragments), and makes the episodic grammar content-
lexicalized trees [e.g., Charniak, 1999]. The assumption of          addressable.
                                                                 511

   Hence, the episodic framework provides a promising new                 Acknowledgements We thank Federico Sangati for tech-
framework to unify traditional rule-based and exemplar-based              nical assistance and Rens Bod, Stefan Frank and Remko
approaches to (probabilistic) syntax within a single, neural              Scha for useful comments. This research was funded
perspective, and interprets them as semantic and episodic                 by the Netherlands Organization for Scientific Research
memory respectively. Table 2 shows how our results compare                (NWO), through the Vici-project “Integrating Cognition” (nr.
to state-of-the-art parsers. Note, that the latter are evaluated          277.70.006) to Rens Bod and the Veni-project “Discovering
on section 23 of WSJ, while all our results are on section 22.            Grammar” (nr. 639.021.612) to Willem Zuidema.
                Various parser strategies (on WSJ sec 23)                                              References
  Parsing model                                      F (≤ 40) F (all)
                                                                           R. Bod. Beyond Grammar: An experience-based theory of lan-
  Charniak (1999) (max. entropy)                       90.1    89.7
  Petrov and Klein (2007)                              90.6    90.1           guage. CSLI Publications, Stanford, CA, 1998.
  Sangati & Zuidema (2011) (DOP)                          -    87.8        G. Borensztajn, W. Zuidema, and R. Bod. The hierarchical predic-
  Charniak and Johnson (2005) (reranker, n = 50)          -    91.0           tion network: towards a neural theory of grammar acquisition. In
                      This paper (on WSJ sec 22)                              Proceedings CogSci’09, 2009.
  TDE reranker (n = 5)                                 90.4      -         E. Charniak. A maximum-entropy-inspired parser. Technical re-
  LCE reranker (n = 5)                                 90.6    90.1           port, Providence, RI, USA, 1999.
  LCE + disctg (n = 5)                                 90.7      -         E. Charniak and M. H. Johnson. Coarse-to-fine n-best parsing and
                                                                              maxent discriminative reranking. In Proceedings ACL’05), pages
Table 2: Comparison of the episodic reranker to state-of-the-                 173–180, 2005.
art parsers, for sentences of length upto 40, or all sentences.            H. Eichenbaum. Hippocampus: Cognitive processes review and
                                                                              neural representations that underlie declarative memory. Neuron,
                                                                              44:109–20, 2004.
   We see our work not only as an application in computa-
                                                                           M. H. Johnson. Pcfg models of linguistic tree representations. Com-
tional linguistics, but also as a contribution to memory re-                  putational Linguistics, 24:613–632, 1998.
search, which offers an explicit computational instantiation               D. Klein and C. D. Manning. Accurate unlexicalized parsing. In
of the reinstatement hypothesis of episodic retrieval. We                     Proceedings ACL’03, 2003.
have proposed a representation of episodic memory, in which                W. Levy. A sequence predicting ca3 is a flexible associator that
episodic memory is stored in distributed form inside local                    learns and uses context to solve hippocampal-like tasks. Hip-
memories of associated semantic memory units. It can be                       pocampus, 6:579–590, 1996.
conceived of as an imaginary life-long thread spun through                 C. D. Manning and B. Carpenter. Probabilistic parsing using left
semantic memory. These ideas are consistent with contem-                      corner language models. In Proceedings ACL’97, San Francisco,
                                                                              CA, 1997.
porary research in neuroscience, which emphasizes the con-
                                                                           C. D. Manning and H. Schütze. Foundations of Statistical Language
strual of episodes in the hippocampus as contextually bound                   Processing. MIT Press, Cambridge, MA, 2000.
sequences of semantic memories [e.g., Eichenbaum, 2004].                   G. F. Marcus. The Algebraic Mind: Integrating Connectionism and
The hippocampal model of Levy [1996] shows that dur-                          Cognitive Science. MIT Press, Cambridge, MA, 2001.
ing episodic sequence learning special ‘context neurons’ are               T. J. O’Donnell, N. D. Goodman, and J. Tenenbaum. Fragment
formed that uniquely identify (part of) an episode. These may                 grammars: Exploring computation and reuse in language. Tech-
function as a neural correlate of the counter that we imple-                  nical report, 2009. MIT-CSAIL-TR-2009-013.
mented in the traces. The episodic grammar model repre-                    S. Petrov and D. Klein. Improved inference for unlexicalized pars-
sents a first attempt to validate this theory of episodic memory              ing. In Proceedings NAACL HLT’07, pages 404–411, 2007.
                                                                           D. Rosenkrantz and P. Lewis II. Deterministic left corner parsing.
within the language domain.
                                                                              In 11th Annual Symposium on Switching and Automata Theory,
                                                                              pages 139–152, New York, 1970. IEEE Press.
                          Future work                                      F. Sangati and W. Zuidema. A recurring fragment model for accu-
Parsing with episodic grammars looks like a very promising                    rate parsing: Double-DOP. (under review), 2011.
                                                                           R. Scha. Taaltheorie en taaltechnologie: competence en perfor-
direction in both cognitive modelling and statistical parsing,
                                                                              mance. volume 11 of 7–22, pages 409–440.
of which we have only started to exploit the possibilities. We             L. Shastri. Episodic memory and cortico-hippocampal interactions.
are currently developing a full episodic chart parser, that com-              Trends in Cognitive Science, 6(4), 2002.
putes parse probabilities online via the principle of spread-              M. Tomasello. The item-based nature of children’s early syntactic
ing activation from stored derivations to (the traces in) its                 development. Trends in Cognitive Science, 4(4):156–163, 2000.
states. This work is part of a larger project, which aims at               E. Tulving. Episodic and semantic memory. In E. Tulving, editor,
modeling the episodic-semantic memory interaction within a                    Organization of memory. Academic Press, New York, 1972.
connectionist framework, through integration with the HPN                  D. van Uytsel, F. van Aelten, and D. van Compernolle. A struc-
network [Borensztajn et al., 2009], resulting in a system that                tured language model based on context-sensitive probabilistic
                                                                              left-corner parsing. In Proceedings of NAACL’01, 2001.
can learn syntactic treelets and their substitutability relations
                                                                           C. C. Woodruff, J. D. Johnson, and M. D. Rugg. Content-specificity
from unannotated text without supervision. We further intend                  of the neural corelates of recollection. Neuropsychologia, 43,
to investigate syntactic priming effects, and recency effects in              2005.
sentence processing.
                                                                      512

