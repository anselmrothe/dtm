UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Coordinating Touch and Vision to Learn What Objects Look Like
Permalink
https://escholarship.org/uc/item/3ms7822t
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Talbott, Walter
Fasel, Ian
Miolina, Javier
et al.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

      Coordinating Touch and Vision to Learn What Objects Look Like
           Walter A. Talbott1,4 , Ian Fasel2 , Javier Rodriquez Molina3 , Virginia de Sa1 , and Javier Movellan4
                1
                  Department of Cognitive Science, University of California San Diego La Jolla, CA 92093
                       2
                         Department of Computer Science, University of Arizona, Tucson, AZ 85721
                               3
                                 CalIT2, University of California San Diego, La Jolla, CA 92093
                    4
                      Machine Perception Lab, University of California San Diego, La Jolla, CA 92093
                            Abstract                               grabbing an object and moving it around, it is possible
                                                                   to create dramatic changes in the visual appearance of
   We use contemporary machine learning methods to ex-
   plore Piaget’s idea that active interaction across modal-       the object while maintaining invariant information from
   ities may be the engine for constructing our knowledge          the tactile and proprioceptive (joint angles) sensors in
   about objects. We identified the existence of modality-         the hand.
   specific invariances as a potential mechanism by which
   Piaget’s ideas may be implemented in practice. For ex-             Recently several research groups have begun to explore
   ample, object segmentation and pose invariant recogni-          the interactions between sensory modalities for improv-
   tion are very difficult in the visual domain but trivial in
   the tactile/proprioceptive domain; touching an object           ing the performance of systems that interact with ob-
   easily delineates its physical boundaries. We can also          jects. In (Grzyb, Chinellato, Morales, & Pobil, 2009),
   grab an object and rotate it without modifying the pro-         the grasping actions of a robotic system are planned by a
   prioceptive and tactile information from our hands. We
   hypothesize that this information may provide invari-           visual algorithm that estimates the shape of unmodeled
   ants that could be useful for training a visual system to       objects. After this initial planning, the tactile sensors
   recognize and segment objects.                                  guide the final approach of the grasp to correct errors in
   To explore this hypothesis we developed the instrumen-          the visual estimation. Motor commands and visual input
   tation necessary to simultaneously collect tactile, propri-     are used in (Gold & Scassellati, 2009) to learn a repre-
   oceptive and visual information of a person interacting
   with everyday objects. We then developed a system that          sentation of a robot’s arm through contingencies between
   learns pose invariant visual representations using pro-         the commands and observed motion. In (Saxena, Wong,
   prioceptive and tactile information as the only training        & Ng, 2008), a camera is positioned, by a robotic arm, in
   signal.
                                                                   several orientations, and the proprioceptive information
   The classifiers that developed from this approach were
   accurate and robust to variations in pose and to a wide         from the arm at each orientation helps locate a grasping
   range of occlusions. They were more accurate (average           point on an object. (Orabona, Caputo, Fillbrandt, &
   2AFC= 0.98) than the classifier trained with human-             Ohl, 2009) and (Noceti et al., 2009) directly train a su-
   specified location information (average 2AFC = 0.93).
   This suggests a specific mechanism using multi-modal            pervised mapping between the visual and haptic sensors
   information could to construct knowledge about objects,         from human interaction with an object. This mapping is
   as originally proposed by Piaget.                               used to estimate missing tactile input from the visual in-
                                                                   put, and classification using the reconstructed input and
                       Introduction                                the visual information is more accurate than with the vi-
Pose invariant object recognition is one of the most diffi-        sual input alone. In (Fitzpatrick & Metta, 2003), object
cult computer vision problems, since objects can change            properties are learned through interaction by making a
appearance drastically depending on the orientation.               robot poke the object and examining changes to the vi-
Particularly difficult is to learn the appearance of ob-           sual scene after contact. Also, (Sinapov & Stoytchev,
jects in an unsupervised manner, i.e., without any labels          2009) and (Bergquist et al., 2009) train a robot to cate-
telling us where the objects of interest are or whether            gorize household objects using visual paired with acous-
they are present in the image at all. Yet humans ap-               tic and proprioceptive cues during supervised interac-
pear to have no problems learning the visual appear-               tion, where object labels were given to the system by
ance of objects relatively independent of their pose and           humans.
in a fully unsupervised manner. Developmental psy-                    Here we focus on the problem of using tactile and pro-
chologists like Piaget (Piaget, 1953), have long argued            prioceptive information from the hand to train a visual
that infants construct knowledge about objects based               system to recognize objects. First, unsupervised meth-
on the mutual interaction (assimilation and accommo-               ods are used to cluster the tactile and proprioceptive
dation) between different modalities (grasping, sucking,           sensory data while freely interacting with two objects: a
looking). Here we take some first steps toward under-              drinking glass and a plate. The cluster labels provided
standing how this may work in practice. We focus on                by the hand are then used to train a weakly-supervised
the interaction between tactile and visual modalities and          visual object recognition algorithm. This approach does
note that the hands may provide invariants that could              not use human labels as a training signal, but instead
be used to train the visual system. In particular, by              attempts to learn objects in an unsupervised way, which
                                                               562

sets it apart from the previous work in multi-modal ob-
ject recognition.
                  Data Collection
To collect the data required for the project, we developed
a suite of sensors. We first attached 12 piezoresistive
pressure sensors to a sports glove. The pressure sensors
were located at the fingertips, in the palm at the base of
each finger, and the last two at the bottom of the palm,
as shown in Figure 2. These sensors vary their resistance
based on the force applied on their surface. The touch
data was collected at a 1KHz sampling rate.
   We also equipped the glove with six PhaseSpace mo-
tion capture (Phasespace Motion Capture, n.d.) LEDs,           Figure 3: Example images from the interactions with
placed on the back of the hand, wrist, forearm, and el-        the objects. Interactions were unscripted, with varying
bow. Each of these markers provides a measurement of           backgrounds, and could include both objects in the im-
the three dimensional position of the arm and hand in          age. The objects were additionally present in multiple
space with a 400Hz sampling rate. These sensors were           orientations.
then integrated on top of a Cyberglove (Cyberglove Sys-
tems, n.d.), which provided measures of the angles of 18
                                                                  The goal of the study was to investigate mechanisms
joints of the hand and fingers at a 150Hz sampling rate.
                                                               by which a modality (tactile/proprioceptive) may train
   Visual input was captured using a head-mounted cam-
                                                               another modality (visual) in an unsupervised manner so
era. Since the objects were not fixed in place during the
                                                               as to construct object concepts. To this end we started
interactions, it was important to ensure that the visual
                                                               with what we thought would be a relatively simple prob-
input matched what the human was seeing while manip-
                                                               lem, to visually recognize the presence of two objects: a
ulating the objects. To this end, the output from the
                                                               drinking glass and a plate. We collected data from 6
head-mounted camera was directed to a pair of VGA
                                                               minutes of unscripted human interactions with these ob-
glasses that gave the only view the human had of the in-
                                                               jects, shown in Figure 1.
teraction. Data were recorded at 20 Hz with a resolution
                                                                  After the data were collected, each sensor was down-
of 640x480 pixels. Three PhaseSpace LEDs were placed
                                                               sampled to the 20Hz rate of the video capture, and the
on the head to capture the 3D position and orientation
                                                               images were converted to grayscale at a resolution of
of the head with respect to the world (See Figure 3).
                                                               320x240 pixels. To make the motion capture informa-
                                                               tion invariant to location in space, the three dimensional
                                                               coordinates were converted into angles between vectors
                                                               defined by the points tracked by the markers on the
                                                               arm. The data processing leaves 7521 samples with 5
                                                               motion capture angle readings, 18 cyberglove readings,
                                                               12 force sensor readings, and the corresponding 320x240
                                                               grayscale image. Of these samples, 1152 were during
                                                               grasps of the drinking glass, and 1602 from grasps of the
Figure 1: Images of the two objects, a drinking glass and      plate.
a plate. Because they were both on the table during the
interactions, images can include both objects.                         Learning to Recognize Objects
                                                               The problem of recognizing the visual appearance of
                                                               the target objects turned out to be much more diffi-
                                                               cult than we had originally anticipated. In the past,
                                                               we have worked with standard datasets popular in the
                                                               computer vision literature in which the task is to recog-
                                                               nize hundreds of object categories (Fasel, 2006) and so
                                                               were surprised by the difficulty of our dataset. We do
                                                               not have a clear explanation yet, but believe that while
                                                               our database includes only two target objects, it presents
Figure 2: Layout of the 12 force sensors on the fingertips     some difficult challenges: (1) The objects appeared in a
and palm.                                                      wide range of poses and locations in the image plane.
                                                               In many cases, the pose was such that the drinking glass
                                                           563

looked more like a plate and viceversa (see Figure 4). (2)
                                                               Table 1: Results of clustering samples based on haptic
The hand was visible and occluded different parts of the
                                                               information. Entries show number of samples in each
object. The recognition system had to implicitly factor
                                                               cluster separated by true label.
out the hand and later recognize the target objects on
                                                                                           Actually Holding
their own. (3) In a large proportion of images both ob-
                                                                                       Glass Plate Neither
jects were visible simultaneously. (4) The target objects
were transparent.                                                          Cluster 1     836       72        39
                                                                           Cluster 2        3 1387          262
                                                                           Cluster 3     313     143      4466
                                                               for each of the two clusters with highest force sensor
                                                               values, the number of images that contain each object.
                                                               Notice that while these clusters were not generated from
                                                               visual information, they separate the objects well; only
                                                               4.5% of the images with one object are incorrectly clus-
                                                               tered.
Figure 4: Images of the two objects, a drinking glass
and a plate. From certain angles, these objects look           Table 2: Number of objects present in the images of
more similar to each other than to themselves at different     each cluster. Numbers here represent images that con-
angles.                                                        tain part or whole of each object.
                                                                                   Neither Glass Plate Both
                                                                       Cluster 1          2     279      70     596
            Unsupervised Clustering                                    Cluster 2          3       0 1193        456
We first tried clustering the data using K-means and
spectral clustering. K-means is one of the simplest
and most popular approaches to unsupervised clustering.        Visual Clustering
Spectral clustering makes use of the spectrum of the sim-
ilarity matrix of the data and is one of the most popular      Table 3 contains the same data as Table 2 but for the
algorithms used for unsupervised image segmentation.           clusters generated from visual information alone. Clus-
We found that, in this case, spectral clustering provided      ter 2 may be taken to represent plates, but neither of the
marginally worse results than K-means and thus here we         other clusters represents a drinking glass cluster. Sim-
focus on the results with the K-means clustering.              ilar results were obtained with spectral clustering ap-
                                                               proaches. Thus, for this particular dataset, vision alone
Tactile Clustering                                             does not seem to easily separate the data into the target
The 5 motion capture angles, the 18 cyberglove read-           object clusters.
ings, and the 12 force-sensor readings were normalized
and combined into a single vector for each sample. Clus-
ters were then computed using K = 3, with an aim of            Table 3: Content of the clusters obtained from clus-
capturing the two object classes plus a background class       tering the visual data (used to train the Plate-Vision
for when the hand was not interacting with an object.          and Glass-Vision classifiers) used as reference for perfor-
   After combining all of the tactile sensors and cluster-     mance. Entries are the number of images in each cluster
ing, the samples were separated into groups as shown in        that contain images of each, neither, or both objects.
Table 1. A measure of accuracy can be computed be-             For training the classifiers, Cluster 1 and Cluster 2 were
tween the two clusters that correspond to samples when         chosen.
the glass and plate are held. The measure takes the sum                            Neither Glass Plate Both
of the samples that are truly glasses in what is inter-                Cluster 1        76      450     756 1539
preted as the glass cluster (the cluster with more glass               Cluster 2        27      118 1332        424
images) and the samples that are truly plates in what is               Cluster 3        74      418     967 1340
interpreted as the plate cluster, and divides by the total
number of samples in both clusters. This measure gives            No matter which clustering algorithm is used, the im-
the clustering an accuracy of 0.8553.                          ages from the clusters can contain both visual objects.
   Thus, the clusters appear to represent the object cat-      Given the noisy, unlabeled data of transparent objects
egories, albeit imperfectly. These clusters are based          at different orientations, it seems a difficult task for a
on the haptic information, and therefore do not reflect        classifier to learn to distinguish the objects from these
which objects are actually in the image. Table 2 shows,        data.
                                                           564

     Learning The Visual Appearance of                                  from the background images, and Kj (b̂) is a measure of
                           Objects                                      how well the background explains the image assuming it
                                                                        contains segment j. Additionally, f is of the form
Here we attempted to use the labels obtained from the
clustering of a modality to train another modality. Since                                              Xt
the clustering methods (tactile or visual) are unlikely to                                    f (x) =       αi hi (x).
perfectly separate the data into the target object classes,                                            i=1
it is important to choose a learning algorithm that can
operate in the presence of a large number of errors in the              In the current case, each hi (x) is a step function of the
training labels. In addition, the clustering algorithms                 output from a Haar-like feature applied to image seg-
provide information about image types but no informa-                   ment x which can take values of ±1, and αi is a real-
tion about where the object is in the image. Thus a                     valued weight. The goal of learning is to find the model
learning algorithm is needed that can work with such                    f that maximizes the log likelihood L. However, be-
weak labels.                                                            cause computing the K terms is intractable, we instead
   We chose the Segmental Boltzman Fields (SBF) al-                     will attempt to maximize
gorithm (Fasel, Fortenberry, & Movellan, 2005; Fasel,                                               ns                  n
2006). This approach assumes that images are gener-
                                                                                                   X                   X
                                                                                  L(x|f, b̂) = log      ef (xj ) − log     ef (x̃k )
ated as a collection of rectangular patches. Each of these                                         j=1                 k=1
patches generates either the background or the object of
interest from the respective distributions describing the               since we are concerned only with the choice of f , and the
pixel values in a patch. To perform inference, each pos-                K terms are always positive and constant with respect
sible patch in an image is assigned a probability based                 to this choice.
on the likelihood the pixels in the patch were generated                   This likelihood function is maximized using functional
by the object distribution. This likelihood is combined                 gradient ascent by boosting the components of the fore-
with a prior probability that an object was contained                   ground model, hi (x).
in that image patch. Training this model involves the
estimation of the likelihood ratio between these two dis-                                          Results
tributions for image patches.                                           In order to get a standard benchmark, we first developed
   After the tactile/proprioceptive clustering separated                a plate and a drinking glass detector using a supervised
the data into three groups, the two groups that had the                 learning approach. For these supervised approaches, the
highest mean readings on the force sensors were used as                 objects were cropped by hand from the images separated
the labeled groups. This procedure relies on the assump-                by the tactile clustering. The negative sets for these
tion that the system prefers situations where objects are               classifiers were the whole, uncropped images from the
present in the hand to when the hands are empty. Call                   other cluster (glass for plate, and vice versa). These sets,
the two clusters A and B. Two classifiers were trained.                 positive and negative, were given to the SBF algorithm
One of these was trained using A as positive, foreground,               (Fasel, 2006), which learned supervised classifiers from
examples and B as negative, background, examples. The                   them. The classifiers trained this way are called Super-
classifier trained this way can then be assumed to rec-                 vised classifiers in tables 4 and 5. The performance of
ognize objects contained in A. The other classifier was                 the classifiers was measured in terms of the performance
trained using B as positive examples and A as negative                  in a 2 alternative forced choice task (2AFC).
examples, so that it will recognize the object in B.                       We then developed visual classifiers using the labels
Segmental Boltzman Fields                                               provided by the tactile/proprioceptive clusters. The two
                                                                        classifiers, one for each cluster, were trained on subsets
The model of foreground (images that contain the object                 of 64 images chosen at random from the pool of images
of interest) and background images (images to that do                   separated by the clustering. The classifiers were evalu-
not contain the object) specified in (Fasel et al., 2005;               ated on two sets of data: The first set contained images
Fasel, 2006) gives a log likelihood function for a fore-                of the instrumented hand grasping the objects, similar to
ground image x as follows                                               the data on which the classifiers were trained. The sec-
                  Xns                        Xn                         ond set contained images of the objects alone, without
 L(x|f, b̂) = log     ef (xj ) Kj (b̂) − log     ef (x̃k ) + log(n)     the instrumented hand present in the image. It also con-
                  j=1                        k=1                        tained two distractor objects, a pen and a mug. Neither
                                                                        of these distractor objects was present in the training
where f and b̂ are the foreground and background mod-                   set. This second set was designed to rule out the possi-
els, xj is segment j from image x, ns is the number                     bility that the system was learning to recognize the fact
of segments in the positive image, n is the number of                   that the hands look different when grasping different ob-
segments in the background image set, x̃k is segment k                  jects, rather than learning the appearance of the objects
                                                                    565

themselves. The classifiers trained using the tactile clus-
ters are called Tactile in the tables. Results presented
below are from classifiers for each category that had the
highest cross-validated performance on the training set.
   As an additional control, we developed classifiers
trained using clusters generated from the images alone,
without any tactile information. The images were split
into three clusters. Since there was no signal to dis-          Figure 5: Example localization estimated by the Glass
tinguish which contained an object and which did not,           classifier. Each image contains a box around the area it
the clusters that best contained instances of the plate         estimates is most likely to have generated the image of
and glass were selected by hand, and trained with SBF.          the object. The lower section of the images is a heatmap
The content of these clusters is described in Table 3.          of the probability that an object is present in the im-
These classifiers represent how how well the objects            age at each location. This heatmap is normalized indi-
could be learned from the dataset’s visual information          vidually for each image, so direct comparisons between
alone. These are called the Vision classifiers in the ta-       heatmaps are difficult.
bles.
                                                                at different orientations, and the classifier manages to
Table 4: Performance of the classifiers on the data con-
                                                                locate the plate even in a side-on view. The fourth im-
taining the hand manipulating the objects. These data
                                                                age shows a mistake. The base of the glass and some of
were similar to the data on which the classifiers were
                                                                the background (not shown) are occasionally selected as
trained.
                            2AFC                                the location of the object in the image.
               Tactile Vision Supervised
                0.968     0.782      0.938
Table 5: Performance of the two classifiers on the data
with the objects alone. These data were tested to show
that the classifier had not learned to recognize the hand.
                            2AFC                                Figure 6: Example localization estimated by the Plate
               Tactile Vision Supervised                        classifier.
                0.983     0.529      0.935
                                                                                      Conclusion
Object Localization                                             The goal of this project was to use contemporary ma-
The SBF algorithm provides posterior probability maps           chine learning methods to investigate Piaget’s idea that
representing the presence or absence of target objects.         active interaction across modalities may be the engine
The intensity of a pixel on these maps indicates the prob-      for the construction of object knowledge (Piaget, 1953).
ability that that particular pixel renders the object of        We identified the existence of modality-specific invari-
interest. In addition, the algorithm can select the most        ances as a potential mechanism by which Piaget’s ideas
probable location of the target object.                         may be implemented in practice. For example, object
   Figure 5 shows the estimated locations for the clas-         segmentation and pose invariant recognition are very
sifier trained to identify glasses. The first two images        difficult in the visual domain but trivial in the tac-
show examples of what the classifier estimated for im-          tile/proprioceptive domain. This creates an opportunity
ages with and without the hand. The classifier seems to         for tactile information to be used to learn the location
have picked out the curve of the glass rim in the first         and appearance of objects in images.
image, but manages to cover a more complete area of                To test these ideas we developed the instrumentation
the glass when the hand is present. In the third im-            necessary to simultaneously collect tactile, propriocep-
age, notice that when the glass is viewed horizontally,         tive and visual information of a person interacting with
the classifier is not confident enough to predict that the      two everyday objects (a drinking glass and a plate).
object is present in the image.                                 While the dataset we obtained contained only two tar-
   Figure 6 shows posterior probability maps from the           get objects, there were a large number of occlusions (due
classifier that learned to detect images with plates. The       to the presence of the hands) and a wide variety of 3D
first three images are examples of when the classifier          poses. When given the supervised location of the object,
identified the plate well. These images contain the plate       the relatively mediocre results (average 2AFC of 0.93)
                                                            566

of the resulting classifier indicate that the task was not        Butko, N. J., & Movellan, J. R. (2010, October Novem-
easy.                                                                   ber). Detecting contingencies: An infomax ap-
   We then used a simple unsupervised clustering method                 proach. Neural Networks, 23 (8-9), 973–984.
on the tactile/proprioceptive information to separate the         Cyberglove Systems.          (n.d.).      Available from
observed data into three clusters. These clusters roughly               http://www.cyberglovesystems.com/
mapped into episodes of interaction with each of the two          Fasel, I. (2006). Learning real-time object detectors:
target objects and episodes in which the target objects                 probabilistic generative approaches. Unpublished
were not present. The information provided by unsu-                     doctoral dissertation, UCSD.
pervised clustering in the tactile/proprioceptive channel         Fasel, I., Fortenberry, B., & Movellan, J. (2005, April).
was then used to train a visual classifier.                             A generative framework for real time object de-
   The classifiers that developed out of this approach                  tection and classification. Computer Vision and
were accurate and robust to variations in pose and to                   Image Understanding, 98 (1), 182–210.
a wide range of occlusions. They were more accurate               Fitzpatrick, P., & Metta, G. (2003). Grounding vision
(average 2AFC= 0.98) than the classifiers trained in a                  through experimental manipulation. Philosophical
supervised manner (average 2AFC = 0.93). If confirmed                   Transactions: Mathematical, Physical and Engi-
by future studies, this would be a remarkable result, sug-              neering Sciences, 361 , 2165–2185.
gesting that tactile information may indeed provide crit-         Gold, K., & Scassellati, B. (2009). Using probabilistic
ical invariances for the construction of object detectors               reasoning over time to self-recognize. Robotics and
in the visual domain. Additionally, the superiority of the              Autonomous Systems, 57 (4), 384–392.
tactile and visual classifier over the visual only classifier     Grzyb, B., Chinellato, E., Morales, A., & Pobil, A. del.
lends support to the idea that multi-modal integration                  (2009). A 3D grasping system based on multimodal
may be better than individual sensory modalities when                   visual and tactile processing. Industrial Robot: An
used to generate object knowledge.                                      International Journal , 36 (4), 365–369.
   The results are a first step. We need to study how the         Noceti, N., Caputo, B., Castellini, C., Baldassarre, L.,
proposed approach scales up as we add a larger num-                     Barla, A., Rosasco, L., et al. (2009). Towards
ber of objects. In this project, we addressed the percep-               a theoretical framework for learning multi-modal
tion problem decoupled from the motor control problem,                  patterns for embodied agents. In Proceedings of
i.e., we let a human move his hands and change his vi-                  the 15th international conference on image analy-
sual input at will. In practice infants face a combined                 sis and processing (p. 248). Springer.
perceptual and control problem and they may use this              Orabona, F., Caputo, B., Fillbrandt, A., & Ohl, F. W.
opportunity to optimize the knowledge gained about ob-                  (2009, June). A theoretical framework for trans-
jects. In some conditions they may choose to move an                    fer of knowledge across modalities in artificial and
object in front of their eyes while maintaining a constant              biological systems. 2009 IEEE 8th International
hold of the object. In such cases, the tactile system may               Conference on Development and Learning, 1–7.
provide useful invariances to train the visual system. In         Phasespace Motion Capture. (n.d.). Available from
other conditions, they may choose to look at a stationary               http://www.phasespace.com/
object while touching it in different locations, when the         Piaget, J. (1953). The origins of intelligence in children.
visual system would provide invariances to train the tac-               London: Routledge and Kegan Paul.
tile system. One of our immediate goals is to formalize           Saxena, A., Wong, L. L. S., & Ng, A. Y. (2008). Learn-
this problem from the point of view of information max-                 ing grasp strategies with partial shape information.
imization approaches to motor control (Butko & Movel-                   In Aaai’08: Proceedings of the 23rd national con-
lan, 2010). We are also planning to test such formalism                 ference on artificial intelligence (pp. 1491–1494).
on Diego-San, a humanoid robot we developed to help us                  AAAI Press.
understand cognitive development from a computational             Sinapov, J., & Stoytchev, A. (2009). From acoustic
point of view.                                                          object recognition to object categorization by a
                                                                        humanoid robot. In Proceedings of the rss 2009
Acknowledgment This research was supported in                           workshop: Mobile manipulation in human environ-
part by ONR DURIP Award #N000140811114 (H.                              ments.
Poizner, PI).
                       References
Bergquist, T., Schenck, C., Ohiri, U., Sinapov, J., Grif-
       fith, S., & Stoytchev, A. (2009). Interactive ob-
       ject recognition using proprioceptive feedback. In
       Proceedings of the iros 2009 workshop: Semantic
       perception for mobile manipulation.
                                                              567

