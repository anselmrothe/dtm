UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Grow your own representations: Computational constructivism
Permalink
https://escholarship.org/uc/item/0jm6n7pq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Austerweil, Joseph
Griffiths, Thomas
Gureckis, Todd
et al.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                Grow your own representations: Computational constructivism
      Joseph L. Austerweil (Joseph.Austerweil@gmail.com) Robert L. Goldstone (rgoldsto@indiana.edu)
        Thomas L. Griffiths (Tom Griffiths@berkeley.edu) Todd Gureckis (todd.gureckis@nyu.edu)
              Kevin Canini (kevin@eecs.berkeley.edu)                            Matt Jones (mcj@colorado.edu)
Keywords: representational change, Bayesian modeling,                 meant by representation change and how well each model can
Connectionism, features, categories                                   explain human representation change.
   From a cognitivist standpoint, one main interest of psy-              The symposium will focus on a wide variety of methods
chology is the study of representations of the human mind as          for representation learning from some of the most popular
they mediate how people react to stimuli in their environment         computational paradigms in computational cognitive science:
(Palmer, 1978). This can explain why two people that en-              nonparametric Bayesian modeling (Austerweil & Griffiths;
counter the same stimulus can behave in very different ways           Canini & Griffiths), connectionist modeling (Gureckis; Gold-
(Chomsky, 1959). For example, an art historian viewing a              stone), and reinforcement learning (Jones). Importantly, each
Jackson Pollock painting may exclaim “this is beautiful” due          presenter will focus on how their computational proposals ex-
to her representation of his work as a rejection of painting          plain human experimental data and discussing what exactly is
with a brush; however, a lay person may say “this is ugly” due        a representation in their framework and how they are inferred.
to his representation of the painting as a cluttered mess of col-     Thus, the symposium should be interesting to a broad audi-
ors. Without knowledge of the representations of each person          ence of cognitive scientists (from computation modelers to
in this example, it would be nearly impossible to explain their       experimentalists to philosphers). We hope it inspires a growth
behavior when interacting with the Jackson Pollock painting.          of new computational models and human experiments in this
   Over the last three decades, cognitive psychologists have          underdeveloped, yet incredibly important, aspect of cognitive
demonstrated that the representations people use can change           science.
flexibly to capture changes in their environment (Hoffman &           Introduction and Nonparametric Bayesian Models of Fea-
Richards, 1985; Schyns, Goldstone, & Thilbaut, 1998; Gold-            ture Learning
stone, 2003). However, if the representations we use are de-          Austerweil and Griffiths Cognitive psychology is concerned
termined by the stimuli in our environment, this threatens the        primarly with representations and how they mediate the re-
explanatory utility of representations as it could be superflu-       sponse to stimuli. In this talk, we present a framework for
ous to use representations to explain people’s reaction to stim-      exploring the principles underlying human feature learning
uli if the representations are determined by the stimuli. Thus,       using nonparametric Bayesian statistics. We show that our
cognitive psychologists need to explicitly formulate how rep-         framework can capture how people infer features using sta-
resentations change with experience.                                  tistical information of the observed images, spatial informa-
   Although computational modelers, from connectionists to            tion from the observed images, and categorization cues. Next,
Bayesians, disagree on many things, one thing they do agree           we extend our initial framework to infer features that are in-
on is the importance of representations in their models (Mc-          variant over a set of transformations and demonstrate that the
Clelland et al., 2010; Griffiths, Chater, Kemp, Perfors, &            model infers new invariant features like people do. Although
Tenenbaum, 2010). Recently, there has been a growing in-              most shapes and features can be transformed by translations
terest in exploring computational models that adapt their rep-        and rescalings, some shapes and features lose their identity
resentations with experience in ways that match this human            when rotated. We show how our model is easily extended to
capacity. In this symposium, we explore computational mod-            capture how people infer the allowable set of transformations
els that adapt their representations with experience in ways          of an object from their observations of the object. Finally, we
that are inspired by the human capability.                            conclude with the implications of our framework for refer-
   Recently, there have been several proposals for computa-           ence frames in shape perception and feature-based cognitive
tional models whose representations flexibly adapt to the in-         models and compare it to other approaches for inferring rep-
put data like people do; however, there has not been a thor-          resentations.
ough comparison of the different models. The goal of the              Building flexible categorization models by grounding
symposium is the compare and contrast the different meth-             them in perception
ods, evaluate their ability to capture of human representation        Goldstone One limitation of most existing models of catego-
learning, and make explicit what is meant in each model by            rization is that they do not start with a perceptually grounded
“representation change” as this can be a controversial claim          representation of the objects that they categorize. Instead,
(Schyns et al., 1998). Currently, it is not clear whether or not      they use dimensional or featural representations that discard
the different proposals mean the same thing by a “representa-         information about the spatial relations among an object’s
tion” and if they are competing proposals to explain the same         parts. This restricts the models’ ability to create psycho-
aspect of human cognition or different levels of explanation.         logically plausible object representations that can be flexibly
Thus, the symposium will emphasize understanding what is              adapted to meet categorization demands. I will describe a
                                                                  2635

neural network model, C-PLUS, that creates part-based rep-          egories and interrelated systems of categories. Its behavior
resentations of objects that honor perceptual constraints such      can replicate that of prototype models, exemplar models, and
as proximity and good continuation. Using a modified com-           more recent mixture models, as it adjusts the complexity of
petitive learning algorithm for object segmentation, it decom-      its representations in response to the observed data. The HDP
poses a set of incrementally presented objects into parts that      can also be used to introduce dependencies in the learning
can be composed together to regenerate the set of objects to        of multiple categories, allowing us to give a formal account
be categorized. These parts are learned at the same time that       for previously unexplored aspects of human category learning
weights from the parts to categories are learned, allowing          such as transfer learning and taxonomy induction.
perceptual representations not only to guide categorization,        Endnote: Breaking Sticks or Building Clusters? Repre-
but categorization to guide perceptual representations as well.     sentation Building, Learning, and the Brain
The model is applied experimental results on the unitization        Gureckis Traditional models of human learning tend to focus
of object elements into complex wholes, learned differentia-        on parameter inference, in that learning involves adjusting the
tion of originally fused encodings into parts, and experience-      internal parameters of an a-priori fixed architecture. How-
dependent changes to selective attention abilities.                 ever, a key feature of human learning is the discovery and
Constructing representations through reinforcement                  growth of new representations that help us to interpret and in-
learning by improving generalization                                teract with the world. The work reviewed in this symposium
Jones One critical role of representations in cognition is that     offers at least two distinct ways of thinking about this psy-
they determine patterns of similarity, and hence general-           chological process. Innovations in non-parametric Bayesian
ization, among stimuli or situations. To the extent that two        statistics have ushered a new generation of probabilistic mod-
stimuli have similar representations, past experience with          els that can flexibly adjust the complexity of their represen-
one will have a large influence on the learner’s response           tation using stochastic process priors (e.g., the “stick break-
to the other. Thus a reasonable goal is to develop repre-           ing process”). Other theorists take a bottom-up approach to
sentations that induce appropriate generalization, in that          representation building, focusing on the incremental learning
stimuli with similar consequences or appropriate actions            mechanisms that give rise to representational change (e.g.,
will tend to have similar representations. This connection          incremental clustering models). In my talk, I explore the ten-
suggests a mechanism for representation learning, based on          sion between these two approaches using examples from the
improving generalization in response to prediction error. We        categorization and sequential pattern learning literatures. I
present a formal framework instantiating this idea, in which        place a particular emphasis on the psychological content of
representation learning is driven by the temporal-difference        each approach as well as consistency with the neural systems
(TD) error from reinforcement learning. The model explains          thought to be involved in particular types of representation
patterns of human learning to shift attention among stimulus        building (e.g., episodic memory systems). Ultimately, I argue
features, according to how well different features capture          that the gulf between these approaches need not be wide, if
the structure of a task. We will also present evidence              both sets of theorists are clearer about the critical importance
supporting a counterintuitive prediction of the model in            of the inference mechanism used to drive predictions in their
which reduced training can lead to improved asymptotic              models.
performance, resulting from order effects that emerge from
                                                                                               References
the model’s incremental learning mechanism. This finding
                                                                    Chomsky, N. (1959). A review of B.F Skinner’s Verbal Behavior.
illustrates an important advantage of mechanistic modeling             Language, 31, 26-58.
over computational-level (e.g., Bayesian) approaches                Goldstone, R. L. (2003). Learning to perceive while perceiving to
                                                                       learn. In Perceptual Organization in Vision: Behavioral and Neu-
A nonparametric hierarchical Bayesian framework for                    ral Perspectives (p. 233-278). Mahwah, NJ: Lawerence Erlbaum
modeling human categorization                                          Associates.
Canini and Griffiths                                                Griffiths, T. L., Chater, N., Kemp, C., Perfors, A., & Tenenbaum,
                                                                       J. B. (2010). Probabilistic models of cognition: exploring rep-
   Traditional models of human categorization typically fall           resentations and inductive biases. Trends in Cognitive Sciences,
into one of two groups: prototype models, which use mini-              14(8), 357-364.
                                                                    Hoffman, D. D., & Richards, W. A. (1985). Parts in recognition.
mally complex category representations, and exemplar mod-              Cognition, 18, 65-96.
els, with maximal complexity. Previous work showed that             McClelland, J. L., Botvinick, M. M., Noelle, D. C., Plaut, D. C.,
these can both be described in the framework of probability            Rogers, T. T., Seidenberg, M. S., et al. (2010). Letting struc-
                                                                       ture emerge: connectionist and dynamical systems approaches to
density estimation. Within this framework, we can identify             cognition. Trends in Cognitive Sciences, 14(8), 348-356.
a new class of psychological models using mixture distribu-         Palmer, S. E. (1978). Fundamental aspects of cognitive representa-
tions. Indeed, several researchers have begun to explore this          tion. In Cognition and categorization (p. 250-303). Hillsdale, NJ:
                                                                       Lawrence Erlbaum Associates, Inc.
possibility. We present a unifying model for categorization         Schyns, P., Goldstone, R. L., & Thilbaut, J.-P. (1998). The de-
models based on the statistical tool of nonparametric hier-            velopment of features in object concepts. Behavioral and Brain
archical Bayesian modeling. The overarching model, called              Sciences, 21, 1-54.
the hierarchical Dirichlet process (HDP), provides a flexible,
formal account of category learning for both individual cat-
                                                                2636

