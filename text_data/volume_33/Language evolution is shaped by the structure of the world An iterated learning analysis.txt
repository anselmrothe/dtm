UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Language evolution is shaped by the structure of the world: An iterated learning analysis
Permalink
https://escholarship.org/uc/item/0pp267gx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Prefors, Amy
Navarro, Daniel
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                  University of California

                     Language evolution is shaped by the structure of the world:
                                                 An iterated learning analysis
                                            Amy Perfors (amy.perfors@adelaide.edu.au)
                                                School of Psychology, University of Adelaide
                                        Daniel Navarro (daniel.navarro@adelaide.edu.au)
                                                School of Psychology, University of Adelaide
                              Abstract
   Human languages vary in many ways, but also show strik-
   ing cross-linguistic universals. Why do these universals ex-
   ist? Recent theoretical results demonstrate that Bayesian learn-
   ers transmitting language to each other through iterated learn-
   ing will converge on a distribution of languages that depends
   only on their prior biases about language and the quantity of
   data transmitted at each point; the structure of the world being
   communicated about plays no role (Griffiths & Kalish, 2005,
   2007). We revisit these findings and show that when certain as-
   sumptions about the independence of languages and the world
   are abandoned, learners will converge to languages that depend
   on the structure of the world as well as their prior biases. These
   theoretical results are supported with a series of experiments
   showing that when human learners acquire language through              Figure 1: (a) Schematic illustration of the typical iterated learning
   iterated learning, the ultimate structure of those languages is        paradigm, which assumes that learner n acquires language on the
   shaped by the structure of the meanings to be communicated.            basis of the language data produced by learner n − 1. (b) A dif-
   Keywords: language evolution; iterated learning; Bayesian              ferent view of iterated learning recognizes that because individuals
   modeling; linguistic structure                                         produce language in order to communicate about the world, the data
                                                                          available to learners includes meanings in the world as well as the
                           Introduction                                   linguistic data produced by the learner before them.
Human languages have rich structure on many levels, from                  from previous learners). Previous research has shown that
phonology to semantics to grammar. Where does this struc-                 when learners are individually Bayesian, an iterated learning
ture come from? Most researchers agree that linguistic struc-             chain converges in the limit to the prior distribution over all
ture is shaped by the structure of our minds – that our brains            possible languages (Griffiths & Kalish, 2005, 2007). How-
contain prior biases that favor the acquisition or retention of           ever, the proof of this assumes a priori that a language carries
some linguistic systems over others. As such, debate gen-                 no assumptions about the frequencies of events in the world.
erally centers around the nature and origin of these biases.              As we will show, when this assumption is relaxed, the iterated
Some suggest that the human language faculty is genetically               learning process converges to a distribution that depends on
specified, with natural selection operating on genes for lan-             the distribution of meaningful events in the world as well as
guage (e.g., Pinker & Bloom, 1990; Nowak, Komarova, &                     the prior biases of the learner. We experimentally test these
Niyogi, 2001; Komarova & Nowak, 2001) or else selecting                   theoretical results in a lab-based iterated learning experiment
for other capabilities (e.g., Hauser, Chomsky, & Fitch, 2002).            (as in, e.g., Kirby, Cornish, & Smith, 2008) and find that par-
Others have suggested that humans easily learn language not               ticipants converge on different languages depending on the
because of a language-specific genetically encoded mecha-                 structure of the space of meanings they are shown.
nism, but because language evolved to be learnable and use-
able by human brains (e.g. Zuidema, 2002; Brighton, Smith,                Iterated learning
& Kirby, 2005; Christiansen & Chater, 2008). While these                  The iterated learning modeling (ILM) framework is widely
accounts disagree in many particulars, they agree that the                used in language evolution research (e.g., Kirby & Hurford,
structure of language arises from the structure of the brain.             2002; Griffiths & Kalish, 2007; Kirby et al., 2008; Smith,
   In this paper we argue that language evolution is shaped by            2009; Reali & Griffiths, 2009). It views the process of lan-
the structure of the world in addition to pre-existing cognitive          guage evolution in terms of a chain of learners (or genera-
biases. Because language involves communicating about the                 tions), shown schematically in Figure 1(a). The first learner
world, the structure of that world (i.e., the things to be com-           in the chain sees some linguistic data (e.g., utterances), forms
municated) can interact with people’s prior biases to shape               a hypothesis about what sort of language would have gener-
the languages that develop. We offer theoretical and exper-               ated that data, and then produces their own data, which serves
imental support of this proposition. On the theoretical side,             as input to the next learner in the chain. Over time, the lan-
we take as our starting point recent work within the “iterated            guages that emerge from this process become non-arbitrary:
learning” framework (in which new learners receive their data             Griffiths and Kalish (2005, 2007) (henceforth, GK) demon-
                                                                      477

                                                                              learner – generated by the world – correspond to the black
                                                                              dots, which fall naturally into two clusters. We might intu-
                                                                              itively expect that a language like hc would be a better fit
                                                                              to this world (and hence be more likely to evolve) than a
                                                                              language like ha , even though ha has higher prior probabil-
                                                                              ity. The results of GK appear to suggest otherwise. Is our
                                                                              intuition simply wrong, or is there a mismatch between the
                                                                              GK derivation and the problem of language evolution within
                                                                              a structured world? In the next section, we argue for the latter.
Figure 2: (a) Intuitive illustration of the results of Griffiths & Kalish                            Theoretical result
(2005, 2007) (GK). Given a 2-dimensional semantic space, a learner
with a prior bias to favor one dimension of that space (the x-axis)
                                                                              We formalize the iterated learning framework in much the
and languages with fewer words might have a prior distribution over           same way as Griffiths and Kalish (2005). A learner sees
languages that puts more probability on ha and less on hb or hc .             m meanings or events, denoted x = {x(1) . . . x(m) }. These
GK demonstrate that the languages that evolve will converge to this           meanings are paired with m corresponding utterances de-
prior distribution. (b) If the natural categories in the world have a         noted y = {y(1) . . . y(m) }. The first learner in the chain is
different structure, we might intuitively expect that languages that
capture that structure, like hc , should be more likely to evolve.            shown some initial data consisting of meaning-utterance pairs
                                                                              (x0 , y0 ). Then, when shown new events x1 , the learner pro-
strate that when the learners are Bayesian, we should expect                  duces utterances y1 , so that (x1 , y1 ) are the input to the next
an iterated learning chain to converge to the prior distribution              learner. In general, learner n + 1 sees data (xn , yn ) and gener-
over all possible languages. That is, the probability of any                  ates yn+1 given events xn+1 , so that the next learner receives
given language emerging does not depend on the structure of                   input (xn+1 , yn+1 ). The goal of each learner is to estimate
the world or independent properties of the language – only                    the mapping between meanings and utterances, which cor-
the assumptions of the learner. The existence of a linguis-                   responds to learning the language they are exposed to. It is
tic bottleneck (in which only a small amount of information                   assumed that each learner has the same countable hypothesis
is transmitted at each link in the chain) can speed the rate                  space H of possible languages, such that each h ∈ H corre-
of convergence or create a pressure for certain kinds of lin-                 sponds to one language. For any learner, acquisition involves
guistic structure like compositionality, but this result implies              a learning step and a production step.
that neither the structure of the meaning space nor the nature                   In the learning step, learner n + 1 sees (xn , yn ) and com-
of the initial language should have an affect on the language                 putes a posterior distribution over possible languages hn+1 .
that eventually evolves.                                                      Bayes’ rule implies that we can express this posterior distri-
   GK’s result can be conceptualized in intuitive terms as fol-               bution as follows:
lows. Suppose learners must acquire languages that describe
a two dimensional semantic space of some sort. For illustra-                                                  P(yn |xn , hn+1 )P(hn+1 |xn )
                                                                                          P(hn+1 |xn , yn ) =                                 (1)
tive purposes, suppose further that the learners have a prior                                                 ∑h∈H P(yn |xn , h)P(h|xn )
bias to prefer languages with fewer words and to pay more
                                                                              In their derivation GK assume that each language h makes
attention to one of the dimensions, as occurs in human cate-
                                                                              no assumption about which events x are more likely than any
gory learning and development (e.g., Landau, Smith, & Jones,
                                                                              other; given that assumption, they note that P(h|x) = P(h),
1988). This prior bias might impose a distribution over hy-
                                                                              and proceed with a version of Equation 1 based on that modi-
potheses h about possible languages, like the illustrative one
                                                                              fication. Alternatively, however, it might be that the language
shown in Figure 2(a): languages like ha with a few words
                                                                              carries with it certain assumptions about what events are pos-
that classify according to the preferred dimension (the x-axis
                                                                              sible or probable in the world, in which case the GK assump-
in this case) have higher prior probability than languages like
                                                                              tion is untenable.1 In other words, simply observing mean-
hb , which have many words, or hc , whose words classify ac-
                                                                              ingful events x may bias the learner to prefer some languages
cording to the dis-preferred dimension. GK suggest that lan-
                                                                              over others. If this is the case, then P(h|x) does not equal
guages evolving to describe this space will converge to the
                                                                              P(h), and the learning step is described by Equation 1.
prior distribution: 40% of the time ha will emerge, 10% of
the time hb will emerge, and so forth. Although this prior and                   To see what this shift does to the iterated learning chain,
these precise numbers are imaginary, the picture provides a                   we now turn to the production step. In this step, the learner
schematic illustration of what GK’s results mean.                                 1 More formally, GK assume that each language h specifies
   It also, however, highlights an apparent oddity within these               P(y|h, x), the conditional distribution over utterances y given the
results. Suppose that the world possesses structure in the form               events x. Our formulation corresponds to assuming that each lan-
of natural categories of some sort, and these natural categories              guage maps onto a joint (subjective) probability distribution over
                                                                              events and utterances, P(x, y|h). We can factorize the joint dis-
happen to group items according to the non-preferred dimen-                   tribution P(x, y|h) = P(y|x, h)P(x|h). Moreover, since P(h|x) ∝
sion, as shown in Figure 2(b): the items observed by the                      P(x|h)P(h), in our set up P(h|x) 6= P(h).
                                                                          478

encounters new meanings xn+1 , generated from the (objec-                     The assumption these results depend on is relatively weak:
tive) distribution Q(x) of meanings in the world. Given these             all it requires is that the events or meanings each learner sees
meanings, the learner generates the new utterances yn+1 by                be a representative sample from the true generating distribu-
sampling them from P(yn+1 |xn+1 , hn+1 ), where hn+1 is the               tion Q(x). In the limit where no learner sees any data, the
learner’s language (assumed to be sampled from the posterior              stationary distribution converges to the prior, since P(h|x) =
distribution in Equation 1).                                              P(h) in that situation. But as the amount of data increases,
   Since all people in the chain follow the same learning and             the languages that evolve will depend on the posterior distri-
production steps, we can calculate P(hn+1|hn ), the probabil-             bution P(h|x) and the distribution of meanings in the world
ity that learner n + 1 acquires language hn+1 given that the              Q(x). Since the posterior depends on both prior and likeli-
previous learner used the language hn , in the following way:             hood (P(h|x) ∝ P(h)P(x|h)), this means that the languages
                                                                          that evolve will be sampled from a distribution depending on
       P(hn+1|hn ) =    ∑ ∑ P(hn+1|x, y)P(y|x, hn )Q(x).        (2)       which ones are favored a priori as well as which ones best
                       x∈X y∈Y
                                                                          capture the meanings in the world. The additional Q(x) term
Thus we have a sequence of random variables h1 , h2 , h3 , . . .          means that the distribution of those meanings matters as well.
describing the languages acquired by each person in the                   These results suggest that languages like hc might be more
chain. This is generated by a Markov chain whose transition               likely to evolve in a world like the one in Figure 2(b) than the
probabilities are given by P(hn+1|hn ). Assuming the chain is             prior distribution over languages might suggest.
ergodic, then its stationary distribution π(h) satisfies                      In the next section we report experimental results support-
                                                                          ing these theoretical findings.
                π(hn+1 ) =    ∑    P(hn+1 |hn )π(hn )           (3)
                            h n ∈H                                                                     Experiment
for all hn+1 . Put another way, the probability distribution over         Method
languages hn approaches π(hn ) as n → ∞.                                  We adopt the standard iterated learning paradigm, in which
   In the set up used by GK, the stationary distribution π(h)             participants form chains in which the output of the nth partic-
corresponds to the prior P(h). However, under our formal-                 ipant is the input of participant n + 1 and the input for the first
ization this is no longer the case. To find the stationary dis-           participant is random. In a training phase, participants see a
tribution in this situation, we make the following “represen-             number of meaning-word pairs and are asked to learn them.
tativeness” assumption: that the posterior probability of a               In a test phase, they are shown meanings and asked to pro-
hypothesis given an actual dataset x is close to its expected             duce the corresponding word; these are the pairings for the
posterior probability given the generating distribution Q(x).             next participant and correspond to the “language” that exists
In other words, we assume that P(h|x) ≈ EQ(x′ ) [P(h|x′ )] =              at that point in the chain. Our question is whether the lan-
∑x′ P(h|x′ )Q(x′ ), for some x ∼ Q(x). The math demonstrates              guages that evolve over the course of a chain depend on the
that if this assumption holds, then the stationary distribution           distribution of meanings Q(x).
is approximately π(h) = ∑x P(h|x)Q(x). That is, the chain                     In our experiments, the “meanings” consisted of 36 pos-
converges to the expected posterior distribution over lan-                sible squares differing in size and color, as shown in Fig-
guages given meaningful events in the world. This is because              ure 3(a). In the CONTROL condition, the stimuli continuously
for π(h) = ∑x P(h|x)Q(x) to be the stationary distribution it             varied along two dimensions: color and size.2 In this condi-
must be true that:                                                        tion there is no obvious or privileged way of categorizing the
                                                                          stimuli. In the SIZE condition, the stimuli were more discon-
π(hn+1 )   =    ∑ P(hn+1 |hn )π(hn )                                      tinuous along the size dimension while in the COLOR condi-
                hn
                                                                          tion they were discontinuous along the color dimension.3
           =    ∑ ∑ ∑ P(hn+1 |x, y)P(y|x, hn )Q(x)π(hn )                      These conditions, then, correspond to worlds with different
                 x  y hn
                                                                          event distributions Q(x), and each favors languages that par-
           =    ∑ ∑ ∑ P(hn+1 |x, y)P(y|x, hn )Q(x) ∑ P(hn |x′ )Q(x′ )     tition the stimuli in different ways, as shown in Figure 3(b).
                 x  y hn                             x′
                                                                          In the SIZE condition one would expect the words to cate-
           ≈    ∑ ∑ ∑ P(hn+1 |x, y)P(y|x, hn )Q(x)P(hn |x)                gorize by size, in particular, to correspond to the distinction
                 x  y hn
                                                                          between smaller (w1 ) and larger (w2 ) items. Conversely, one
           =    ∑ ∑ P(hn+1 |x, y)Q(x) ∑ P(y|x, hn )P(hn |x)               would expect the words in the COLOR condition to evolve
                 x  y                  hn
           =    ∑ ∑ P(hn+1 |x, y)Q(x)P(y|x)                                    2 Color varied from 0% brightness (black) to 100% brightness
                 x  y                                                     (white) in increments of 20%, and size from smallest (10x10) to
           =    ∑ Q(x) ∑ P(hn+1 |x, y)P(y|x)                              largest (60x60) in increments of 10.
                                                                               3 In particular, stimuli 2 and 5 from the CONTROL condition be-
                 x       y
                                                                          came 3 and 4, with the new 2 and 5 intermediate in value. Thus in
           =    ∑ Q(x)P(hn+1 |x)                                          the SIZE condition the final sizes were 10x10, 15x15, 20x20, 50x50,
                 x                                                        55x55, and 60x60, and in the COLOR condition the final colors were
           =    π(hn+1 )                                                  0%, 10%, 20%, 80%, 90%, and 100% brightness.
                                                                      479

Figure 3: (a) Space of stimuli seen in each of the three condi-               Figure 4: Final languages (at the 20th participant in the chain) in
tions of the experiment. Stimuli in the CONTROL condition varied              each of the two chains in each of the conditions. It is evident that
continuously along the dimensions of size and color; in the SIZE              the structure of the stimulus space has a considerable impact on the
condition they varied discontinuously according to size, and in the           structure of the resulting language; both languages in the SIZE con-
COLOR condition they varied discontinuously along the color di-               dition evolved words that categorized more according to size, both
mension. These different spaces thus impose different event dis-              languages in the COLOR condition evolved words that categorized
tributions Q(x). (b) Schematic illustration of the predictions about          more according to color, and both languages in the CONTROL con-
what the evolved language should look like in each condition. In the          dition were not strongly driven by either dimension.
SIZE condition, the words should evolve to categorize the stimuli ac-
cording to size, with one word (w1 ) applying to the smaller objects          tal) in which each stimulus was shown on a computer screen
and the other (w2 ) applying to the larger ones; in the COLOR condi-          with the corresponding word printed below it. In the testing
tion the words should split the space into the dark (w1 ) and light (w2 )     phases, participants were shown the stimuli and asked to type
objects. Predictions for the CONTROL condition are more uncertain,
since there are no natural boundaries within this space.                      the corresponding word; they were never given feedback. The
                                                                              testing phases in the first two rounds contained a random half
to distinguish between darker (w1 ) and lighter (w2 ) stimuli.                of the SEEN set and a random half of the UNSEEN set (18 trials
Because the CONTROL condition contains stimuli that vary                      total). The final round of testing contained the entire stimulus
continuously along both dimensions, it is more unclear what                   set (i.e., all 36 stimuli).
the resulting language should look like. If participants have                    The first participants in each chain were shown a language
a prior bias to favor one dimension more than another, one                    consisting of 36 consonant-vowel-consonant (CVC) words
might expect the resulting language to have six words, one                    randomly assigned to each of the possible 36 possible stim-
for each value along the most important dimension; if they do                 uli. For subsequent participants, the language consisted of the
not have any strong prior bias, one might expect languages to                 meaning-word pairs given by the previous participant in their
vary idiosyncratically, or to evolve towards having one word                  final round of testing. We performed no filtering at any stage.
for all stimuli. Which of these happens is somewhat irrelevant                Results
for our purposes; the main goal of running the CONTROL con-
                                                                              The final languages in the two chains in each condition are
dition was to provide a comparison for the other conditions,
                                                                              shown in Figure 4. It is evident that there was a substantial
and to make apparent any prior biases that might exist.
                                                                              effect of condition on the structure of the resulting languages;
   Our main question was whether the structure of the result-
                                                                              both chains in the SIZE condition evolved words whose pri-
ing language would be different in the SIZE and COLOR con-
                                                                              mary categorization divided the stimuli by size, and both
ditions. We tested this by running two chains of 20 partici-
                                                                              chains in the COLOR condition evolved words which catego-
pants in each of the conditions using a methodology based on
                                                                              rized according to color (although this effect was stronger for
Kirby et al. (2008). For each participant, stimuli were pseudo-
                                                                              Chain A than Chain B).
randomly divided into two sets of equal size: the SEEN and
                                                                                 The difference between conditions can be quantified using
UNSEEN sets.4 Each participant acquired the language in a
                                                                              the adjusted Rand Index (adjR) of Hubert and Arabie (1985).
single session consisting of three rounds, each containing a
                                                                              This measure captures the similarity between clusterings; an
training and a testing phase, with an optional break in be-
                                                                              adjR of 1 indicates that the clusters are identical, while 0 is the
tween rounds. In the training phases, participants were shown
                                                                              score one would expect when comparing two random cluster-
two randomized exposures to the SEEN set (36 trials in to-
                                                                              ings; scores below 0 indicate that the clusters match less than
    4 Stimuli were randomly assigned except for the constraints that          one would expect by chance. Here, each of the resulting lan-
there had to be at least 4 stimuli from each quadrant and 1 stimulus          guages corresponds to one “clustering” of the stimuli; for in-
from each row and column in the SEEN set.                                     stance, the language in Chain A of the COLOR condition cor-
                                                                          480

Figure 5: All participants in all of the chains in the iterated learning experiment. Languages in different conditions evolved in different ways,
reflecting the different structure of the meaning space across conditions. Different shades indicate different words.
responds to a clustering in which the 18 darkest stimuli are in                                         Discussion
one cluster and the 18 lightest stimuli are in another. We can               Our work indicates that if there is no a priori assumption that
compare each of the actual clusterings to the canonical color                a learner’s hypotheses about languages are independent of the
and size clusterings in Figure 3(b). The results are shown in                world they inhabit, then the languages evolved by Bayesian
Table 1. It is evident that the languages in the COLOR condi-                learners through iterated learning will converge to a distribu-
tion have a much higher adjR when compared to the canonical                  tion that depends on the posterior probability over languages
color clustering, and languages in the SIZE condition have a                 as well as the structure of the meaning space. Here we con-
much higher adjR when compared to the canonical size clus-                   sider some of the implications and limitations of our findings.
tering. These results are somewhat preliminary since they                        Our results differ significantly from previous results by
incorporate only two chains per condition; nevertheless, they                Griffiths and Kalish (2005, 2007) that suggest that the sta-
are consistent, and this number of chains is not unusual for                 tionary distribution of a chain of Bayesian iterated learners
iterated learning studies.                                                   depends only on their prior. This divergence arises because
                           canonical size   canonical color                  GK assume that learners’ distribution over languages is in-
            CONTROL           -0.0204            0.0618                      dependent of the structure of the world,5 whereas we make
               SIZE            0.704              0.079                      no such assumption. Which assumption is correct is an open
             COLOR             0.065              0.696
                                                                             question, although we suggest that in at least some circum-
Table 1: Average adjR values for the final languages in each con-            stances – especially in the case of semantic categories – ours
dition (rows), compared to the canonical clusterings according to            is plausible. Language learners only start acquiring words af-
size and color (columns). The languages in the CONTROL condition
match with both of the canonical sortings no more than they would            ter having observed many objects and events in the world, and
by chance, but the languages in the other conditions match with their        it seems reasonable for them to expect word meanings to map
canonical clusterings far above chance.                                      onto these objects and events in a sensible way. The map-
   Our mathematical derivation implies that an iterated learn-               ping between grammar and world structure is less obvious,
ing chain will converge to a distribution over languages, not                but one might expect that learners’ grammatical expectations
a single language. We therefore examine the languages at                     are affected by their observations of the world (e.g., expect-
each step in the chain, shown in Figure 5. They support the                  ing salient or frequent characteristics, like number or gender,
theoretical result: after an initial period in which the number                  5 One might be tempted to just redefine the prior P(h) in GK’s
of words decreases dramatically, which is typical for iterated               results to include the collection of items in the world. However,
learning experiments, the chains in different conditions sta-                unless all learners have observed the exact same set of items, their
                                                                             formalism cannot not in fact be interpreted this way, since their proof
bilize on languages that carve up the meaning space in ways                  assumes that all learners share the same prior. Nor is this consistent
appropriate to the structure of that space in that condition.                with how the GK results are usually discussed in the literature.
                                                                         481

to be marked grammatically).                                               questions in addition to these, but our results indicate that
   It is important to clarify one subtle point that may be con-            the world may matter more than we previously thought. Per-
fusing. The original Griffiths and Kalish (2007) did identify              haps language has the structure it does not just because of our
a dependence on the quantity of data transmitted each gen-                 brains, but because of the world as well.
eration. However, this is a very different dependence than
we identify here. When learners sample languages from their                                       Acknowledgments
posterior, the only effect of increasing quantities of data is             We thank Natalie May, Tin Yim Chuk, Jia Ong, and Kym Mc-
                                                                           Cormick for their help recruiting participants and running the exper-
to decrease the rate of convergence to the prior; it does not              iment. DJN was supported by an Australian Research Fellowship
change the actual stationary distribution. They also show that             (ARC grant DP0773794).
if learners maximize the posterior rather than sample from it,
the stationary distribution is centered at the maximum of the                                           References
posterior. However, this is still different from our results, be-          Beppu, A., & Griffiths, T. L. (2009). Iterated learning and the cul-
                                                                             tural rachet. In Proc. 31st CogSci conf. (p. 2089-2094).
cause there is no role of the structure of meaning space Q(x).             Brighton, H., & Kirby, S. (2001). Meaning space structure deter-
   There has been a lot of experimental work supporting the                  mines the stability of culturally evolved compositional language
finding that iterated learning experiments reveal human learn-               (Tech. Rep.). Language Evolution and Computation Research
                                                                             Unit: University of Edinburgh.
ers’ inductive biases (e.g., Kalish, Griffiths, & Lewandowsky,             Brighton, H., Smith, K., & Kirby, S. (2005). Language as an evolu-
2007; Griffiths, Christian, & Kalish, 2008; Kirby et al., 2008;              tionary system. Physics of Life Reviews, 2, 177-226.
Reali & Griffiths, 2009; Smith & Wonnacott, 2010). How do                  Burkett, D., & Griffiths, T. L. (2010). Iterated learning of multiple
                                                                             languages from multiple teachers. In Evolang (Vol. 8).
we reconcile our results with this research? First, we do not              Christiansen, M., & Chater, N. (2008). Language as shaped by the
deny that prior biases are a factor; our results simply suggest              brain. Behavioral and Brain Sciences, 31, 489-558.
that they are not the only factor. Second, in all of these ex-             Griffiths, T. L., Christian, B., & Kalish, M. (2008). Using category
                                                                             structures to test iterated learning as a method for identifying in-
periments, the world never has significant structure: the set of             ductive biases. Cognitive Science, 31(1), 68-107.
meanings x occur with approximately equal probability. The                 Griffiths, T. L., & Kalish, M. (2005). A Bayesian view of language
                                                                             evolution by iterated learning. In Proc. 27th CogSci conf.
world structure Q(x) is also never manipulated between con-                Griffiths, T. L., & Kalish, M. (2007). Language evolution by iterated
ditions: all participants see the same distribution of events.6              learning with Bayesian agents. Cognitive Science, 31, 441-480.
As a result, any effect of world structure may be easy to miss.            Hauser, M., Chomsky, N., & Fitch, W. T. (2002). The faculty of
                                                                             language: What is it, who has it, and how did it evolve? Science,
Our work does not invalidate any of these results, since none                298(5598), 1569–1579.
of these experiments were made investigate the role of world               Hubert, L., & Arabie, P. (1985). Comparing partitions. Jn of Clas-
structure. We do predict that in these experiments, significant              sification, 193–218.
                                                                           Kalish, M., Griffiths, T. L., & Lewandowsky, S. (2007). Iter-
changes in the distribution Q(x) should result in different sta-             ated learning: Intergenerational knowledge transmission reveals
tionary distributions of the chains.                                         inductive biases. Psych. Bulletin and Review, 14(2), 288-294.
   Our findings may also resolve an apparent contradiction in              Kirby, S. (2001). Spontaneous evolution of linguistic structure – an
                                                                             iterated learning model of the emergence of regularity and irregu-
the literature. While many results have suggested that lan-                  larity. IEEE Trans. on Evolutionary Computation, 5(2), 102-110.
guage evolution should converge to the prior, there is also                Kirby, S., Cornish, H., & Smith, K. (2008). Cumulative cultural
work showing that the structure of the meaning space can                     evolution in the laboratory: An experimental approach to the ori-
                                                                             gins of structure in human language. Proceedings of the National
also affect the nature of the evolving language (e.g., Kirby,                Academy of Sciences, 105(31), 10681-10686.
2001; Brighton & Kirby, 2001; Smith, Kirby, & Brighton,                    Kirby, S., & Hurford, J. (2002). The emergence of linguistic struc-
2003; Maurits, Perfors, & Navarro, 2010). Our result offers                  ture: An overview of the iterated learning model. In A. Cangelosi
                                                                             & D. Parisi (Eds.), (p. 121-148). London: Springer Verlag.
an explanation for why such a dependence might exist.                      Komarova, N., & Nowak, M. (2001). Natural selection of the critical
   This work is still preliminary. Additional experimental                   period for language acquisition. Pr Roy Soc B, 268, 1189-1196.
tests of our theoretical predictions include varying the fre-              Landau, B., Smith, L., & Jones, S. (1988). The importance of shape
                                                                             in early lexical learning. Cognitive Development, 3, 299–321.
quency of meanings and initializing chains with languages                  Maurits, L., Perfors, A., & Navarro, D. J. (2010). Why are some
that do not match the space of meanings (e.g., initializing                  word orders more common than others? A uniform information
participants who see the meaning space from the COLOR con-                   density account. In NIPS (Vol. 23, p. 1585-1593).
                                                                           Nowak, M., Komarova, N., & Niyogi, P. (2001). Evolution of uni-
dition with a language conforming to the canonical size pat-                 versal grammar. Science, 291, 114-118.
tern). In addition, a great deal of theoretical work remains.              Pinker, S., & Bloom, P. (1990). Natural language and natural selec-
Existing work investigates how GK’s results are affected if                  tion. Behavioral and Brain Sciences, 13(4), 707-784.
                                                                           Reali, F., & Griffiths, T. L. (2009). The evolution of frequency
the chain consists of more than one learner per generation                   distributions: Relating regularization to inductive biases through
(Smith, 2009; Burkett & Griffiths, 2010), or if learners are ca-             iterated learning. Cognition, 111, 317-328.
pable of “teaching” subsequent learners in the chain (Beppu                Smith, K. (2009). Iterated learning in populations of Bayesian
                                                                             agents. In Proc. 31st CogSci conf. (p. 697-702).
& Griffiths, 2009). How would our results be affected un-                  Smith, K., Kirby, S., & Brighton, H. (2003). Iterated learning: A
der these circumstances? There are many remaining open                       framework for the emergence of language. Art. Life, 9, 371-386.
                                                                           Smith, K., & Wonnacott, E. (2010). Eliminating unpredictable vari-
    6 Note that in some experiments, for instance Kalish et al. (2007)       ation through iterated learning. Cognition, 116, 444-449.
and Griffiths et al. (2008), the mapping between meanings x and            Zuidema, W. (2002). How the poverty of the stimulus solves the
utterances (or utterance equivalents) y is different, at least for the       poverty of the stimulus. In NIPS (Vol. 15).
initial person in the chain. However, Q(x) itself is never varied.
                                                                       482

