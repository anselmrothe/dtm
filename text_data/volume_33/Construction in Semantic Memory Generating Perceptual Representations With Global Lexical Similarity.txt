UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Construction in Semantic Memory: Generating Perceptual Representations With Global
Lexical Similarity
Permalink
https://escholarship.org/uc/item/3jk6d4pk
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Johns, Brendan
Jones, Michael
Publication Date
2011-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                        Construction in Semantic Memory: Generating Perceptual
                                Representations With Global Lexical Similarity
                                            Brendan T. Johns (johns4@indiana.edu)
                                           Michael N. Jones (jonesmn@indiana.edu)
                                Department of Psychological and Brain Sciences, Indiana University
                                            1101 E. Tenth Street, Bloomington, In., 47405
                              Abstract                                   Perceptual symbol systems theory (PSS; Barsalou, 1999),
   The literature currently contains a dichotomy in explaining        one of the cornerstones of the grounded cognition
   how humans learn lexical semantic representations for words.       movement (Barsalou, 2008), has been proposed as a
   Theories generally propose either that lexical semantics are       competitor to distributional models as an explanatory theory
   learned through perceptual experience, or through exposure to      for the emergence of lexical semantic structure in memory.
   regularities in language. We propose here a model to integrate
                                                                      The basis of PSS is the dismissal of amodal symbols as the
   these two information sources. The model uses the global
   structure of memory to exploit the redundancy between              central component underlying human mental representation.
   language and perception in order to generate perceptual            Rather, the PSS approach proposes that the symbols used in
   representations for words with which the model has no              reasoning, memory, language, and learning are grounded in
   perceptual experience. We test the model on a variety of           sensory modalities.
   different datasets from grounded cognition experiments.               In the realm of lexical semantics, PSS proposes that the
   Keywords: Semantic memory; co-occurrence models; LSA.              mental representation of a word is based on the perceptual
                                                                      states that underlie experiences with the word‘s physical
                          Introduction                                referent (Barsalou, 1999). Across many experiences with
                                                                      words, the underlying neural states tend to stabilize and
Modern computational models of lexical semantics (e.g.,
                                                                      create an accurate perceptual representation of a word that is
latent semantic analysis (LSA); Landauer & Dumais, 1997)
                                                                      grounded across sensory areas in the cortex. There is
infer representations for words by observing distributional
                                                                      considerable evidence, across both behavioral and
regularities across a large corpus of text. Although their
                                                                      neuroimaging experiments, that the perceptual associates of
specific learning mechanisms may differ considerably, all
                                                                      words do play a central role in language processing (for a
members of this class of model rely on statistical
                                                                      review see Barsalou, 2008).
information in text to infer semantic structure. Distributional
                                                                         Although distributional models and PSS are often
models have seen considerable success at accounting for an
                                                                      discussed as competing theories, the two are certainly not
impressive array of behavioral data in tasks involving
                                                                      mutually exclusive. PSS is unable to make claims about the
semantic cognition. Since their beginning, however,
                                                                      meanings of words that have no physical manifestation—it
distributional models have been heavily criticized for their
                                                                      is fairly limited to concrete nouns and action verbs
exclusive reliance on linguistic information (e.g., Perfetti,
                                                                      (although these are the most commonly used experimental
1998), essentially making them models of learning meaning
                                                                      stimuli). Further, PSS is silent regarding the simple
―by listening to the radio‖ (McClelland).
                                                                      observation that humans are quite capable of forming
   More recently, empirical research has demonstrated that
                                                                      sophisticated lexical representations when they have been
distributional models fail to account for a variety of
                                                                      given nothing to ground those representations in. This is the
semantic phenomena in the realm of embodied cognition
                                                                      situation in which distributional models excel—inferring the
(e.g., Glenberg & Robertson, 2000). This failure is not a
                                                                      meaning of words in the absence of perceptual information.
great surprise given that distributional models do not receive
                                                                      However, distributional models certainly fail when given
perceptual input, and they actually perform surprisingly well
                                                                      tests that stress the use of perceptual information—the
on many tasks believed to require perceptual learning due to
                                                                      situation in which PSS excels. Hence, the two theories
the amount of perceptual information redundantly coded in
                                                                      should not be viewed as competitors, but rather as
both language and the environment (for a review, see
                                                                      complimentary (see Riordan & Jones, 2010). What is
Riordan & Jones, 2010). Distributional models do not argue
                                                                      needed is research into how humans might integrate the two
that perceptual information is unimportant to semantic
                                                                      types of information to make full use of both the structure of
learning. Perceptual information is still statistical
                                                                      language and the perceptual environment.
information; what is required is a mechanism by which
                                                                         Here we explore whether a central component of PSS,
these two sources of information may be integrated.
                                                                      perceptual simulation, may be integrated with a
Attempts to integrate linguistic and perceptual information
                                                                      distributional model to infer perceptual information for
in a unified distributional model are now emerging (e.g.,
                                                                      words that have never been ―perceived‖ by the model based
Andrews, Vigliocco, & Vinson, 2009; Jones & Recchia,
                                                                      on global lexical similarity to words that have been
2010). However, there is little connection in these models
                                                                      perceived. Further, we test the model‘s ability to infer the
to existing theories of modal perceptual symbol learning.
                                                                  126

likely linguistic distributional structure for a word in            situational), and are unable to represent more complex
absence of linguistic experience from its perceptual                perceptual information such as embodied interaction;
similarity to words with which the model has had linguistic         nonetheless, they are a useful starting point. A word‘s full
experience. In this sense, the model‘s goals are similar to         lexical representation in the model is simply the
previous integrative attempts (Andrews et al., 2009; Jones &        concatenation of its linguistic and perceptual vectors (even
Recchia, 2010), but is theoretically linked to important            if one of the two is completely empty). We demonstrate that
mechanisms in PSS.                                                  this model is able to use a simple perceptual simulation
   PSS proposes that simulations (based on past experiences)        mechanism to account for a diverse set of both behavioral
play a central role in conceptual and semantic processing,          and neuroimaging results in studies of language processing.
and there is a considerable amount of evidence that this is a          Linguistic co-occurrence vectors for words were
mechanism of central importance in human cognition                  computed from counts across 250,000 documents extracted
(Barsalou, et al., 2003). PSS presumes that each lexical            from Wikipedia (Recchia & Jones, 2009). Perceptual
representation is a multi-modal simulation of the perceptual        vectors will depend on the particular simulation, but will
experience of that word (e.g. the simulator for horse may           include feature generation norms (McRae, Cree, Seidenberg,
contain what a horse looks like, feels like, sounds like, how       & McNorgan, 2005; Vinson & Vigliocco, 2008), and
you ride one, etc…), which is reinstated whenever one               modality exclusivity norms (Lynott & Connell, 2009). Each
experiences a word. For example, when reading the word              word‘s representation in the full memory matrix is a
metal, your semantic representation is a simulation of              concatenation of its linguistic and perceptual vectors. The
previous perceptual experiences with the word‘s referent,           goal of the model is to infer the perceptual vector for a word
including its texture, experiences of hard and cold, etc. The       from global linguistic similarity to other words, using this
word‘s meaning is not disembodied from its perceptual               limited data to generalize to the entire lexicon.
characteristics.                                                       Borrowing from Hintzman‘s MINERVA model (see also
   We by no means have a solution as to how to formalize            Kwantes, 2005), our model attempts to create an abstraction
this simulation process, but instead evaluate a type of             of a word‘s full lexical vector using a simple retrieval
simulation that may underlie our ability to make correct            mechanism. When a partial probe is compared to memory
inferences about the perceptual representation of                   (say, a word with a linguistic vector, but a zero perceptual
ungrounded words. Instead of relying upon the structure of          vector), a composite ‗echo‘ vector is returned consisting of
neural states during experience, it instead relies upon the         the sum of all lexical vectors in memory weighted by their
grounded representations of other words. That is, a word‘s          similarity to the probe. Across the lexicon, this returns a
perceptual simulator can be constructed not by the current          stable full lexical estimate for a word, including an inferred
perceptual state, but by the perceptual states of similar           perceptual vector. Specifically, perceptual representations
words in memory. The importance of a given word‘s state is          are constructed in a two-step abstraction process, based on
determined by the associative strength between the two              Hintzman‘s process of ‗deblurring‘ the echo.
words, derived from the statistical structure of how those             In step 1 each representation in memory with a zero
words are used in the language environment. Hence, global           perceptual vector has an estimated perceptual vector
lexical similarity (similarity of a word to all other words in      constructed based on its weighted similarity to lexical
memory) may be used by a generation mechanism to ‗fill-             entries that have non-zero perceptual vectors:
in‘ the perceptual representation for a specific word. We
integrate this idea of experiential simulation into a global                            ∑             (       ) ,        (1)
memory model of semantics, based loosely on Hintzman‘s
                                                                    Where M represents the size of the lexicon, T represents the
(1986) MINERVA 2 model.
                                                                    lexical trace for a word, S is similarity function (here, vector
                                                                    cosine), and  is a similarity weighting parameter. Lambda
      Generating Perceptual Representations
                                                                    is typically set to 3 (Hintzman, 1986), but we will fit this
It is important that we are clear at the outset in our              parameter for each of the different norms (due to differences
definitions of linguistic, perceptual, and lexical information      in their dimensionality and structural characteristics), and
in this model, as they are clearly oversimplifications. A           also for the two different steps of inference (due to
word‘s linguistic information in the model is simply a              differences in the number of traces being used to create an
vector representing its co-occurrence structure across              echo). Step 1 utilizes only a limited number of traces and so
documents in a text corpus. If the word is present in a given       each trace should add more information, while in Step 2 the
document, that vector element is coded as one; if it is             entire lexicon is used, and so each word trace should be
absent, it is coded as zero. A word‘s perceptual information        more limited in its importance.
in the model is a probability vector over perceptual features          In step 2, the process from step 1 is iterated, but inference
generated by human subjects. For example, the feature               for each word is made from global similarity to all lexical
<has_fur> will have a high probability for dog, but a low           entries (as they all now contain an inferred perceptual
probability for pig, and a zero probability for airplane. It is     vector). Hence, representations in step 1 are inferred from a
important to note that these types of feature norms include         limited amount of data (only words that have been
much information that is non-perceptual (e.g., taxonomic,           ―perceived‖ by the model). In step 2, representations for
                                                                127

each word are inferred from the full lexicon—aggregate            a perceptual representation for the blanked out word based
linguistic and perceptual information inferred from step 1.       on its associative similarity to other words in the lexicon
   This two-step process is illustrated in Figure 1. Prior to     across our two inference steps. Finally, the correlation is
the inference process, only linguistic information is             computed between the inferred perceptual vector and the
contained in memory with a limited amount of perceptual           true perceptual vector in the norms for the target word. This
information. Across the two-step abstraction process, the         procedure was conducted across all words in each of the
model is able to use the associative structure of memory,         norms. For perceptual norm and step, the parameter was
along with this initially limited amount of data, and infer a     hand fit to the data.
perceptual representation for each word. The essential claim
of this model is that the global similarity structure that is           Table 1. Model Predictions for each Word Norm
contained in the lexicon is sufficient to make sophisticated
predictions about the perceptual properties of words.                       Word Norm                  Step 1                Step 2
                                                                            McRae, et al.              0.42                  0.72
                                                                            Vinson & Vigglioco         0.42                  0.77
                                                                            Lynott & Connell           0.83                  0.85
                                                                                              * All correlations significant at p < 0.001
                                                                     The correlations for each of the word norms across the
                                                                  two steps are displayed in Table 1. This table shows that for
                                                                  each of the norms that model is able to infer an accurate
                                                                  perceptual representation is at a high level, with all three
                                                                  norms achieving a correlation above 0.7.
                                                                  Simulation 1.2: Effect of Lexicon Size
                                                                  A second simulation was conducted to manipulate the
                                                                  number of words in the lexicon used to create the inferred
                                                                  perceptual representations. This was done by varying the
                                                                  number of words in the lexicon from 2,000  24,000 in
                                                                  steps of 2,000. The lexicon was arranged by frequency from
    Figure 1. The two-step process of global construction.        the TASA corpus such that only the most frequent set of
                                                                  words are included. This simulation exclusively used the
            1. Testing Model Foundations                          norms from McRae, et al. (2005).
Our preliminary examination of this model will be done by            The magnitude of correlation as a function of lexicon size
manipulating core aspects of the framework, including             is shown in Figure 2. This figure shows that a consistent
training the model with different perceptual norms,               increase in fit is attained as the size of the lexicon grows,
changing the lexicon size, and testing on different corpora.      until about a size of 14,000. From that point on, the model
                                                                  produces a reduced fit. The reason for this is that after
Simulation 1.1: Word Norms                                        14,000 words the amount of noise that is accumulated
                                                                  within the echo vector exceeds the benefits of the added
Two different types of perceptual norms were used for             resolution created by the additional associative structure
evaluation: feature generation norms (McRae, et al., 2005;        provided by the increased lexicon size. In the following
Vinson & Vigglioco, 2008) and modality exclusivity norms          simulations only the first 14,000 words will be utilized by
(Lynnott & Connell, 2009). Feature generation norms are           the generation mechanism.
created from hundreds of subjects producing the perceptual
features for a set of target words. Aggregated across
subjects, the result is a vector across possible features for
each word, with elements representing the generation
probability of a given feature for a given word. Modality
exclusivity norms are created by having subjects rate how
much a target word is based in each of the five sensory
modalities. The result is a five-element vector per word,
with each element representing the strength of that modality
for a given word.
   To evaluate how well the model is able to infer a word‘s
perceptual representation, we used a cross-validation
procedure. For each sample, a word was randomly selected
from the perceptual norm of interest, and its perceptual
vector in the lexicon was zeroed out. The model then infers                     Figure 2. Effect of lexicon size
                                                              128

Simulation 1.3: Effect of Corpus Size                                              2. Behavioral Simulations
Recchia & Jones (2009) have demonstrated that increasing            The set of simulations in this section uses the global
the size of a corpus (i.e. increasing the number and diversity      similarity model to evaluate the model‘s predictions of a
of the contexts that a word appears in) also increases the fit      variety of behavioral phenomena from grounded cognition.
to semantic similarity ratings, independent of abstraction
algorithm. To evaluate this trend for inferring perceptual
                                                                    Simulation 2.1: Affordances
representations in our global similarity model, we compared
                                                                    In a test of the strength of distributional models
the goodness-of-fit for the model predictions of the McRae,
                                                                    (specifically, LSA) Glenberg & Robertson (2000) conducted
et al. (2005) norms over a small corpus (the TASA corpus,
                                                                    a study in which they assessed subjects‘ (and LSA‘s) ability
composed of 37,600 documents) and a large corpus (a
                                                                    to account for affordance ratings to different objects within
Wikipedia corpus, composed of 250,000 documents). The
                                                                    a given sentence. Objects ranged from being realistic within
fit for the TASA corpus was r = 0.34 after the first step, and
                                                                    the context of the sentence, to being afforded, or non-
r = 0.64 after the second step. However, with the larger
                                                                    afforded. For example, subjects were given the sentence
Wikipedia corpus, a correlation of r = 0.42 after the first
                                                                    ―Hang the coat on the ______‖, and were asked to give
step, and an r = 0.77 after the second step. This shows that
                                                                    ratings on three words (realistic = coat rack, afforded =
there is an impressive increase in fit between the model‘s
                                                                    vacuum cleaner, and non-afforded = cup). Unsurprisingly,
predictions and data with the use of a larger corpus, even
                                                                    realistic objects had a higher score than both afforded and
though the TASA corpus is of higher quality. This is an
                                                                    non-afforded objects, and afforded objects had a higher
important result: It demonstrates that the greater the amount
                                                                    rating than non-afforded objects. However, the stimuli were
of experience the model has with language, the better its
                                                                    constructed such that LSA could not discriminate between
inferences are about a word‘s perceptual representation.
                                                                    afforded and non-afforded conditions.
Simulation 1.4: Reverse Inference                                     Our model is not a model of sentence comprehension
An interesting aspect of this model is that it is capable of        (neither is LSA), so a simpler test was conducted using
making reverse inferences. Given the perceptual                     Glenberg and Robertson‘s (2000) stimuli. The central action
representations for words, the model should be able to              word that described the affordance was used (e.g. ―hang‖
estimate the likely linguistic distributional structure for a       instead of ―Hang the coat on the ______‖). Then the cosine
word. To test reverse inference in the model, we estimated          between this target word and the three different object
each word‘s perceptual representation using (1). A word‘s           words were calculated for both the inferred feature vectors
inferred linguistic vector was then estimated with (1), but         and the raw co-occurrence vectors. The norms from McRae
rather than summing across the perceptual representations in        et al. (2005) were used for this test. The results of this
the lexicon, the linguistic vectors were used (and similarity       simulation are displayed in Figure 3.
was based on similarity of perceptual vectors). The inferred
linguistic vector was then correlated with the word‘s
retrieved co-occurrence vector, where the probe vector is
co-occurrence representation is word, and the representation
of other words is summed, similar to Kwantes (2005).
   The correlation between the inferred linguistic
representations for the concrete nouns from the McRae, et
al. norms was r = 0.67, p < 0.001. For all other words in the
lexicon, this correlation is r = 0.5, p < 0.001. The second set
is lower than the concrete nouns for two reasons: 1) the
perceptual space of the McRae norms does not extend to all
words, and 2) not all words have a strong perceptual basis
(e.g. abstract words) and so the inferred perceptual vector
not diagnostic of that word‘s meaning. However, this simple                  Figure 3. Simulation of results using stimuli
analysis does show that the model is capable of this reverse                     from Glenberg & Robertson (2000).
inference: it can, given the perceptual representation of a
word, construct a fairly accurate approximation of the                As shown in Figure 3, the inferred feature vectors are able
linguistic co-occurrence structure of that word.                    to generate the correct pattern of results – that is, the
   This is a central finding for the model because it allows        average cosine for the realistic words is greater than for the
for lexical inferences to be made in two directions, both           afforded and non-afforded words, and also the average
from linguistic to perceptual and from perceptual to                cosine for the afforded words is greater than for non-
linguistic. Hence, the model can take in either perceptual or       afforded words. The difference between realistic words and
linguistic information about a word and infer the other type        non-afforded words was significant [t(14) = 2.137, p <
of representation from it, allowing for both aspects of             0.05], and the difference between afforded and non-afforded
memory to be filled in when information is missing.                 was moderately significant [t(14) = 1.8, p = 0.08]. The
                                                                    difference between realistic and afforded words was not
                                                                129

significant [t(14)=0.54, p>0.1], but the trend is in the right      Simulation 2.3: Phrase/referent similarity
direction. When the raw co-occurrence representation is             Wu & Barsalou (2009) had subjects rate the familiarity of
used, however, the pattern changes: the average cosine for          novel and familiar noun phrases consisting of a concrete
the non-afforded words was statistically equal to afforded          noun preceded by a modifier (e.g. ―smashed tomato‖ vs.
words [t(14) = 0.064, n.s.]. In addition, unlike the                ―sliced tomato‖). Wu & Barsalou argue from the results
constructed perceptual representations, realistic and non-          that conceptual combinations seem to be based on a
afforded words did not differ [t(14) = 1.56, p > 0.1].              perceptual simulation of the combined concept. This model
                                                                    is not capable of this advanced simulation process, but we
Simulation 2.2: Sensory/motor based priming                         simply wanted to test whether the inferred perceptual
Similar to the previous experiment, Myung, Blumstein, &             representations are better able to account for the familiarity
Sedivy (2006) tested whether facilitation occurred when a           ratings from Wu and Barsalou‘s study. Assessing familiarity
target word was primed by a word that has sensory/motor             is the first step to being able to determine conceptual
based functional information in common with the target, but         combination, by determining the overlap between the two
not associative information (e.g. ‗typewriter‘ preceded by          words‘ representations.
‗piano‘). The prime-target pairs focused on manipulation               The ten novel and ten familiar noun phrases were taken
knowledge of objects (e.g. what one can do with a given             from Wu & Barsalou (2009). Five of the twenty modifiers
object). Using a lexical decision task, Myung, et al. found         had to be replaced with their closest synonym (as defined by
significant facilitation in this condition.                         WordNet) as they were not in the model‘s lexicon (due to
   To simulate their experiment, we used the same prime-            their very low frequency). To assess familiarity, the cosine
target word pairs from Myung, et al. (2006) and the same            between the two words was computed for both the inferred
unrelated primes. Because some of the words in this                 perceptual representation and the raw co-occurrence
experiment were compounds (‗baby carriage‘, ‗safety pin‘,           representation. In addition to examining overall magnitude
etc…), they were transformed to single words (‗carriage‘,           differences between the conditions, a correlation analysis
‗pin‘). Where this changed the meaning of the concept, the          was conducted over the specific familiarity ratings given to
word pair was removed from the test. This procedure                 the different noun phrases. Wu & Barsalou published two
resulted in 23 word pairs being tested, with each pair having       sets of familiarity ratings: 1) phrase familiarity: how often
both a related-target and unrelated-target condition. Priming       subject‘s had experienced that specific phrase, and 2)
was computed in the model as the related-target cosine              referent familiarity: how often subject‘s had seen that
minus the unrelated-target cosine.                                  specific object.
                                                                       A marginally significant difference was found between
                                                                    the novel and familiar conditions for both the inferred
                                                                    perceptual representations [t(9) = 2.0, p = 0.07] and the raw
                                                                    co-occurrence representations [t(9) = 1.79, p = 0.1].
                                                                    However, the item-level fits between the model‘s
                                                                    predictions and subject‘s familiarity ratings for phrases were
                                                                    also tested. A significant correlation was found between the
                                                                    inferred perceptual representations and subject ratings, for
                                                                    both phrase familiarity [r = 0.48, p < 0.05] and referent
                                                                    familiarity [r = 0.49, p < 0.05]. However, this was not the
                                                                    case for the co-occurrence representations, as a non-
                                                                    significant correlation was found for both phrase familiarity
                                                                    [r = 0.12, n.s.] and referent familiarity [r = 0.16, n.s.]. This
  Figure 4. Simulation of perceptual priming results.
                                                                    demonstrates that the inferred perceptual structure is able to
                                                                    simulate item-level variance in familiarity, while the co-
The magnitude of priming was assessed for both the inferred
                                                                    occurrence representations are not.
perceptual representations and the raw co-occurrence
representations. The result of this simulation is depicted in
Figure 4, which shows that both representation types show a
                                                                    Simulation 2.4: Inferred Modality Representation
                                                                    As a final simulation, we tested the ability of the model to
priming effect. The magnitude of facilitation (related >
                                                                    infer the modality rating data from the Lynott & Connell
unrelated) for the co-occurrence representations was not as
                                                                    (2009) norms. In these norms, subjects rate the prominence
pronounced as the inferred perceptual representations, and
                                                                    of the five modalities in representing a target word. As with
was not significant [t(22) = 1.35, n.s.]. However, the
                                                                    the McRae et al. (2005) feature vectors, each word was
facilitation effect for the inferred perceptual representations
                                                                    represented as a probability distribution across the five
was significant [t(22) = 2.05, p < 0.05]. This again
                                                                    modalities. In Lynott & Connell‘s norms, subjects tended to
demonstrates that the perceptual representations inferred by
                                                                    rate vision as consistently more important than other
this model contain a considerable amount of knowledge
                                                                    modalities. To reduce this bias in the model, a preprocessing
about      the    perceptual      underpinning    of    words.
                                                                    normalization procedure was conducted. Before normalizing
                                                                130

each word vector to a probability distribution, each column       semantics. As such, there are currently many shortcomings.
was normalized to have a total magnitude of one, which has        One major issue is that the only ―perceptual‖ features that
the effect of standardizing the amount of information that        may be inferred are fixed to those used to describe the 541
each modality provides. Each word vector was then                 concrete nouns normed by McRae et al. (2005), which may
normalized to a probability distribution.                         make it difficult to generalize those features to other types
   The model‘s ability to generate inferred modality ratings      of words in the lexicon. While this shortcoming is no
was evaluated over a large number of target words from            different than other attempts to integrate perceptual and
various sources. For the visual, auditory, and tactile            linguistic information (e.g. Andrews et al., 2009), it is rather
modalities the words were taken from van Dantzig, et al.          inflexible (and clearly wrong) to believe that the ~2,500
(2008), who conducted a property verification study on            features generated by McRae et al.‘s subjects are sufficient
these modalities. For the olfactory modality, words were          to describe the perceptual structure of the entire lexicon. In
taken from Gonzalez, et al. (2005) who found an increase in       addition, the model is subject to making errors of ―illusory
activation in olfactory brain regions to words that have a        feature migrations‖ (Jones & Recchia, 2010); e.g., inferring
strong smell association. Gustatory words were taken from         that honey has wings. Nonetheless, the phrasal priming
Goldberg, et al. (2006) who found greater activation in the       simulations demonstrate that this type of information
orbitofrontal cortex to food words. In order to model this,       migration affords the model sufficient power to simulate
the strength of the proposed modality was measured for            difficult effects in grounded cognition. Furthermore, the
each word, and compared against a comparison set of words         model takes important steps towards the integration of
drawn randomly from another modality. The results of this         distributional models, global memory frameworks, and
simulation are displayed in Figure 5. All differences among       creates links to theories of grounded cognition.
groups are significant. This demonstrates that this model is
able to create correct inferences about the modality basis of                                         References
words, given a limited amount of starting information.            Andrews, M., Vigliocco, G., & Vinson, D. (2009). Integrating experiential and
                                                                     distributional data to learn semantic representations. Psychological Review, 116,
                                                                     463-498.
                                                                  Barsalou, L. W. (1999). Perceptual symbol systems. BBS, 22, 577-660.
                                                                  Barsalou, L. W. (2008). Grounded cognition. Annual Review of Psychology, 59, 617-
                                                                     645.
                                                                  Glenberg, A. M. & Robertson, D. A. (2000). Symbol grounding and meaning: A
                                                                     comparison of high-dimensional and embodied theories of meaning. Journal of
                                                                     Memory and Language, 43, 379-401.
                                                                  Gonzalez, J., et al. (2006). Reading cinnamon activates olfactory brain regions.
                                                                     NeuroImage, 32, 906-912.
                                                                  Goldberg, R. F., Perfetti, C. A., Schneider, W. (2006). Perceptual knowledge retrieval
                                                                     activates sensory brain regions. The Journal of Neuroscience, 26, 4917-4921.
                                                                  Hintzman, D. L. (1986). ―Schema abstraction‖ in a multiple-trace memory model.
                                                                     Psychological Review, 93, 411-428.
                                                                  Johns, B. T., & Jones, M. N. (2010) Evaluating the random representation assumption
                                                                     of lexical semantics in cognitive models. Psychonomic Bulletin & Review, 17, 662-
                                                                     672.
      Figure 5. Level of strength for different modalities.       Jones, M. N., & Riordan, B. (2010). Redundancy in perceptual and linguistic
                                                                     experience: Comparing feature-based and distributional models of semantic
                                                                     representation. Topics in Cognitive Science, 3, 303-345.
                                                                  Jones, M. N., & Recchia, G. (2010). You can‘t wear a coat rack: A binding framework
                          Discussion                                 to avoid illusory feature migrations in perceptually grounded semantic models.
Here we have proposed a simulation process, similar in               Proceedings of the 32nd Annual Cognitive Science Society.
                                                                  Kwantes, P. J. (2005). Using context to build semantics. Psychonomic Bulletin &
spirit to that suggested by the PSS framework, to generate           Review, 12, 703-710.
inferred perceptual representations for words through the         Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato‘s problem: The latent
                                                                     semantic analysis theory of the acquisition, induction, and representation of
use of global lexical similarity. The perceptual                     knowledge. Psychological Review, 211-240.
representations are constructed by integrating the already        Lynott, D., & Connell, L. (2009). Modality exclusivity norms for 423 object
formed (either learnt or inferred) representations of other          properties. Behavior Research Methods, 41, 558-564.
                                                                  McRae, K., Cree, G. S., Seidenberg, M. S., & McNorgan, C. (2005). Semantic feature
words, and these are weighted by the scaled associative              production norms for a large set of living and nonliving things. Behavior Research
strength among words in the lexicon. Across many words               Methods, Instruments, & Computers, 37, 547-559.
                                                                  Myung, J., Blumstein, S. E., & Sedivy, J. C. (2006). Playing on the typewriter, typing
this simulation process produces a stable representation             on the piano: manipulation knowledge of objects. Cognition, 98, 223-243.
containing useful perceptual information about how the            Perfetti, C. (1998). The limits of co-occurrence: Tools and theories in language
                                                                     research. Discourse Processes, 25, 363-377.
referent of the word is used. The model was capable of            Recchia, G. L., & Jones, M. N. (2009). More data trumps smarter algorithms:
using multiple norm sets, which in turn allowed for a                Comparing pointwise mutual information to latent semantic analysis. Behavior
diverse set of data to be tested. The power of this model is         Research Methods, 41, 657-663.
                                                                  Riordan, B., & Jones, M. N. (2010). Redundancy in perceptual and linguistic
not in complex inference or learning mechanisms, but                 experience: Comparing feature-based and distributional models of semantic
instead is contained in the structure of lexical memory,             representation. Topics in Cognitive Science.
                                                                  van Dantzig, S., Pecher, D., Zeelenberg, R., & Barsalou, R. W. (2008). Perceptual
which has been shown to be an important information                  processing affects conceptual processing. Cognitive Science, 32, 579-590.
source in cognitive modeling (Johns & Jones, 2010).               Vinson, D. P., & Vigliocco, G. (2008). Semantic feature production norms for a large
                                                                     set of objects and events. Behavior Research Methods, 40, 183-190.
   This model is obviously in the very early stages as an         Wu, L., & Barsalou, L. W. (2009). Perceptual simulation in conceptual combination:
attempt to integrate PSS and distributional models of lexical        Evidence from property generation. Acta Psychologica, 132, 173-189.
                                                              131

