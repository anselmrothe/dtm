UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Outcomes and Mechanisms of Transfer in Invention Activities
Permalink
https://escholarship.org/uc/item/4rx036z0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Roll, Ido
Aleven, Vincent
Koedinger, Ken
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                    Outcomes and Mechanisms of Transfer in Invention Activities
                                                    Ido Roll (ido@phas.ubc.ca)
                     Carl Wieman Science Education Initiative, University of British Columbia
                              6224 Agricultural Road, Vancouver, BC V6T-1R9, Canada
                                              Vincent Aleven (aleven@cs.cmu.edu)
                         Human Computer Interaction Institute, Carnegie Mellon University,
                                     5000 Forbes Avenue, Pittsburgh, PA 15213, USA
                                         Kenneth R. Koedinger (koedinger@cmu.edu)
   Human Computer Interaction Institute and Department of Psychology, Carnegie Mellon University,
                                     5000 Forbes Avenue, Pittsburgh, PA 15213, USA
                            Abstract                                 (MD), that is, the mean absolute difference from
  Invention activities are structured tasks in which students        the mean. One key aspect of invention activities is
  create mathematical methods that attempt to capture deep           the use of contrasting cases (Chase, Shemwell, &
  properties of data (e.g., variability), prior to receiving
  instruction on canonical methods (e.g., mean deviation).           Schwartz, 2010). Contrasting cases are carefully
  While experiments have demonstrated the learning benefits of       designed examples that emphasize target features
  invention activities, the mechanisms of transfer remain
  unknown. We address this question by evaluating the role of
                                                                     by changing just those and no other features. For
  design in invention activities, identifying what knowledge is      example, the contrasting cases shown in Figure 1
  acquired during invention activities, and how it is applied in     (middle) emphasize distribution while fixing
  transfer tasks. A classroom experiment with 92 students
  compared the full invention process to one in which students       sample size, range, average, etc.
  evaluate predesigned methods. Results show that students in          The Invention Lab facilitates invention activities
  the full invention condition acquired more adaptive                in three steps (Roll, Aleven, & Koedinger, 2010).
  knowledge, yet not necessarily better procedural knowledge
  or invention skills. We suggest a mechanism that explains          Students are first asked to rank the contrasting
  what knowledge invention attempts produce, how that                cases according to the target property (e.g, the
  knowledge is productively modified in subsequent instruction,
  and how it improves performance on some measures of
                                                                     variability of the left graph is lower than that of
  transfer but not others.                                           the right graph, see Figure 1). Students then
  Keywords: Invention activities, transfer, intelligent tutoring     design a mathematical method and calculate the
  systems, modular knowledge, generation.                            target property for the given data (e.g., design
                                                                     “range / N” and apply it to both data sets). Last,
                         Introduction                                students evaluate their method by comparing its
Invention activities ask students to design and                      inferred ranking to the initial qualitative ranking.
evaluate mathematical methods that capture deep                      Once the invented method ranks the contrasting
properties of given examples. For instance, the                      cases successfully, students are given new data to
task in Figure 1 asks students to invent a general                   work with. Students often make progress during
method for calculating variability. Invention                        the invention process, yet they rarely invent a
activities are designed to augment and precede                       valid general method (Schwartz & Martin, 2004).
traditional teacher-led instruction (Roll, Aleven,                     Classroom evaluations found that the
& Koedinger, 2009; Schwartz & Martin, 2004).                         combination of invention activities and show-and-
Following the invention attempt, whether                             practice instruction improves performance on
successful or not, students receive instruction on                   transfer measures, compared with show-and-
canonical methods for the same problem                               practice alone, controlling for overall time on task
(“show”), and apply these methods to different                       (Schwartz & Martin 2004; Roll et al., 2009;
problems (“practice”). For example, after                            Kapur, 2008). However, while the positive effect
inventing measures of variability, students receive                  of invention activities is well documented, not
show-and-practice instruction on Mean Deviation                      enough is known about how this effect is
                                                                 2824

achieved. In this paper we investigate the                      transfer. This is especially interesting given that
cognitive processes and knowledge outcomes that                 students usually fail to generate successful
are associated with invention activities, and the               methods. Hypothesis 2(a) suggests that students
manner in which invention activities foster                     acquire domain-specific knowledge that prepares
performance on measures of transfer.                            them to better encode the subsequent instruction
                                                                (Schwartz & Bransford, 1998; Schwartz, Sears, &
Invention vs. Evaluation                                        Chang, 2007). Hypothesis 2(b), on the other hand,
Our first goal is to evaluate the role of a key                 suggests that students acquire domain-
process within the invention activity – the design              independent habits or skills that help them
of methods. Hypothesis 1(a) suggests that the                   approach transfer items. For example, Taylor et
design of methods is a key process in achieving                 al. (2010) found that invention activities help
the results of invention activities, akin to the                students come up with more ideas during open-
generation effect (Richland et al., 2005).                      ended transfer problems (yet the quality of each
However, the worked example effect suggests that                idea was not affected by the invention process).
novice learners may benefit from analyzing given                  Last, we propose a mechanism that explains
solutions more than from engaging in problem                    how knowledge that is acquired during invention
solving (Sweller, 2006). In addition, evaluating                activities interacts with show-and-practice
given methods may take less time than designing                 instruction to improve ability to transfer.
and evaluating methods. Therefore, Hypothesis
1(b) suggests that evaluation of given (imperfect)                                         Method
methods, using contrasting cases, is more
                                                                Design
beneficial than designing methods.
                                                                We address these questions by comparing two
Knowledge Outcomes and Mechanisms of Transfer                   versions of invention activities. Students in the
Our second goal is to identify the knowledge that               Design and Evaluate condition were instructed to
is acquired during invention activities and enables             design methods and evaluate them, while students
    Figure 1: The Invention Lab. Students invent methods for calculating variability by qualitatively ranking the contrasting
cases, inventing a mathematical procedure, and comparing their initial ranking with their invented procedure. The cover story
     compares the consistency of two trampolines. The contrasting cases target students’ demonstrated knowledge gaps.
                                                           2825

in the Evaluate Only condition were asked                            the invention process. The second invention
evaluate predesigned methods. These predesigned                      activity followed a similar structure and duration,
methods were taken from a previous study in the                      and was divided across two days. Following the
same school with the same age group of students                      summary discussion of the second activity,
(Roll, et al., 2009). Each of the chosen methods                     students received about 7 minutes of instruction
failed to incorporate one or more critical features                  on MD. Students then worked individually with a
of the target domain, variability. For example, one                  designated intelligent tutoring system for MD. All
method that was given to students was “range /                       students finished all practice problems within 25
N”. This method fails to use all data points in each                 minutes. The study concluded with a post-test.
of the contrasting cases, and thus fails to generate
a correct ranking for the cases shown in Figure 1.                   Materials
Students were instructed to test each method                         Students in both conditions used the Invention
against multiple sets of data. As soon as a method                   Lab (Roll et al., 2010), an intelligent tutoring
was found not to work, students were asked to                        system created using the Cognitive Tutor
move to the next method.1                                            Authoring Tools (Aleven et al, 2009). The first
                                                                     invention activity asked students to evaluate
Participants.                                                        which of two trampolines is more consistent,
Ninety-two 7th grade students from a public                          given data from multiple bounces for each
middle school participated in the study. 33                          trampoline (see Figure 1). The first set of
students were randomly assigned to the Design                        contrasting cases emphasized the range of the data
and Evaluate condition and 59 students were
                                                                      Procedural fluency:
assigned to the Evaluate Only condition. Students
                                                                         What is the MD of 2, 4, 7, and 3? (1)
were enrolled in two levels of math classes
(regular and advanced), split evenly between                          Conceptual knowledge:
conditions.                                                             The average temperature in Montreal and Vancouver is
                                                                        similar, but the MD is much higher in Montreal. What
Procedure                                                               does it mean?
                                                                          a. Montreal is always warmer.
The study spanned four class periods over two
                                                                          b. Vancouver is always warmer.
days. The first day began with a pre-test, followed                       (c). The temperatures in Montreal are more extreme.
by an Invention Lab tutorial. Students then moved                         d. The temperatures in Vancouver are more extreme.
on to the first invention activity, which was
limited to 25 minutes. Instruction for the                            Debugging items:
                                                                        Marlene invented the following method:
invention activity differed based on condition.                           Step 1: Find the average.
Design and Evaluate students were asked to                                Step 2: Find the distances from the average.
design and evaluate methods, while Evaluate Only                          Step 3: Add up all the distances.
students were asked to implement and evaluate                           What do you think about this method?
methods from a given booklet. Students invented                           a. It works.
                                                                         (b). It does not work because there is different number
in pairs and chose their partners. With few
                                                                             of numbers in each set.
exceptions, the pairs did not change throughout                           c. It does not work because the method does not use
the study. The invention activity concluded with a                           all the numbers.
short whole-class discussion in which students
(from both conditions) shared the successes and                       PFL items:
                                                                        Sonly is testing a new distance detector. The device is
limitations of their methods. The discussion was                        not accurate and small errors are permitted. Sonly
used to motivate students to try their best during                      measured the same distance several times and received
                                                                        3, 5, 6, and 10 feet. What is the MD of the new device,
  1                                                                     if errors of 1 foot or less should be ignored? (1.75)
    Half of the Evaluate Only students were also prompted to
explain why each method failed or succeeded. We collapse these            Figure 2: MD assessment items. Correct answers appear
groups because it was evident that students ignored these prompts        in parentheses. All items but procedural fluency items
and we found no other associated differences in learning.                       were given in the context of story problems.
                                                                 2826

(1,3,5,7,9 vs. 3,4,5,6,7). Subsequent contrasting        main effect for condition on performance on
cases were created by the Invention Lab in real          conceptual items; Design and Evaluate = .52,
time based on the weaknesses of students'                Evaluate Only = .49, F(4,87) = 18, p < .0005.
methods. The second invention activity asked             There was also a main effect for condition on
students to evaluate the consistency of machines         debugging items; Design and Evaluate: .44,
that pack candies into bags in a candy factory.          Evaluate Only: .34. F(4,87) = 42, p = .002. Both
  Instruction on how to apply MD was delivered           effects remain significant after applying
by the first author and included graphical and           Bonferroni correction. There was no effect for
mathematical explanations of the structure of the        condition on PFL items with or without resource
formula and a few examples. In subsequent                (p = .2 and .3 respectively).
practice students applied MD to 20 different data
sets and interpreted results.                              Table 1: Performance on post-test by type of item, M(SD)
Measures and Analysis                                                                   Design and        Evaluate
                                                                                        Evaluate          Only
  The pre- and post-tests included assessments of          Procedural Fluency           .64 (.44)         .62 (.45)
procedural and conceptual knowledge. The post-             Conceptual understanding     .52 (.25)***      .49 (.24)
test also included two types of transfer items that        Debugging                    .44 (.30)**       .34 (.34)
required students to construct or analyze modified         PFL
                                                                       W/o resource     .09 (.20)         .06 (.16)
versions of the taught procedure: Preparation for                      With resource    .46 (.45)         .40 (.45)
                                                           ** - p < .01; *** - p < .001
Future Learning (PFL) items asked students to
apply a modified version of the taught procedure                                  Discussion
(Bransford & Schwartz, 2001). For example, after
                                                         Overall, students who designed their methods
learning to calculate variability by averaging the
                                                         outperformed students who received predesigned
errors (MD), students were asked to apply MD
                                                         methods on measures of debugging ability and
while ignoring certain types of errors (e.g.,
                                                         conceptual understanding. There was no effect of
include only large errors, see Figure 2).
                                                         condition on procedural fluency or PFL items.
Debugging items asked students whether a
modified version of the taught procedure still           Invention vs. Evaluation
achieves its goal, and if it fails, to identify the      The results presented above show the importance
source of failure (Roll, 2009).                          of students designing their own methods, and thus
  The effect of condition on students’ learning          support Hypothesis 1(a). This effect is especially
was evaluated using ANCOVAs with condition,              interesting given that all students failed to design
class level, and their interaction as factors, and       valid methods.
pre-test score as a covariate. Five separate               There are several possible explanations for the
ANCOVAs were calculated, one for each type of            benefits of design. First, students who design their
items. Bonferroni correction was applied to              methods have greater agency. Second, students
account for multiple comparisons.                        may learn better from failures of methods they
                      Results                            designed since they understand the intended
                                                         function of each component in their methods.
There was a large, yet insignificant, difference         Third, students who work with pre-designed
between conditions at pre-test: Design and
                                                         methods might have an overly specific goal – to
Evaluate: 15%; Evaluate Only: 27%; F(2,88)=10,           test given methods using given data. Students who
p = .19. On identical items in the pre- and post-
                                                         design methods may have a more scientific goal
tests, students improved from 23% to 58% across          to understand the features of the domain and
conditions and class levels; t(90) = 7.0, p < .0005.
                                                         create a method that captures them. More studies
  Table 1 summarizes the results of the posttest.        are required to evaluate these explanations.
There was no effect for condition on procedural
fluency items (p = .21). There was a significant
                                                     2827

Knowledge Outcomes and Mechanisms of Transfer            some, but not all, of these requirements. For
Performance on conceptual items shows that               example, few methods included division by N. In
students who design their methods acquire better         fact, most requirements are left unsatisfied.
domain knowledge, in support of Hypothesis 2(a).           The invention process helps students identify
Their improved performance on debugging items            concrete, and possibly explicit, requirements, and
might suggest that students who design also              examples of successful and unsuccessful ways to
acquired better domain-independent debugging             achieve them. Later, during show-and-practice,
skills, as suggested by Hypothesis 2(b). However,        students can complete the puzzle by noticing how
students in both conditions practiced the relevant       the canonical method satisfies these requirements.
debugging skills during invention. In fact,              Comprehension of instruction involves a mapping
Evaluate Only students had more time to practice         process in which students can identify how each
debugging on tasks isomorphic to the assessment          component of the canonical method fills a certain
items. Therefore, a more likely interpretation is        function. In the case of MD, division by N
that the improved conceptual understanding, and          accounts for sample size; the minus operator
not improved debugging skills, helped students
who designed their methods perform better on
debugging items.
  Accumulated evidence from this and previous
studies shows that invention activities do not help
students apply the canonical methods (in
procedural items), yet they help students modify
the learned methods (in PFL items) and diagnose
variants of these methods (in debugging items;
Roll, 2009; Roll, et al., 2009; Schwartz & Martin,
2004). A main question to ask, therefore, is how
the experience of failing to invent prepares
students to modify and adapt their acquired
knowledge following instruction.
  As described above, students in invention
activities invent using contrasting cases. The
contrasting cases are designed to direct students’
attention to deep features of the domain, often one
feature at a time. In order for their methods to
work, students need to find mathematical ways to
represent the target features in their methods. For
example, the contrasting cases shown in Figure 1
help students realize that their methods should use
all available data, since relying only on extreme
points may give the wrong answer. Thus, one
explanation for the benefits of failed invention
attempts is that the invention process is essentially
a process in which students acquire requirements
for a valid method (see Figure 3.a). Within the
domain of variability, by using contrasting cases,          Figure 3: A possible explanation for learning and transfer
students may realize that a valid method should             from invention activities. While inventing, students can
                                                         identify requirements from valid solutions (a). During show-
measure distances, use all given data, account for
                                                            and-practice students realize how the taught formalism
sample size, and use positive values. Students            addresses these requirements (b). Such functional encoding
may find mathematical operations that satisfy                      helps students adapt their knowledge (c, d)
                                                     2828

calculates distances; the sum function uses all the                          Acknowledgements
points; and absolute value makes sure that               This work is supported by the Pittsburgh Science
distances do not cancel each other out. The              of Learning Center, National Science Foundation
outcome is knowledge that is more elaborated and         award SBE-0354420.
is composed of critical functional components,
each of which are backed up by the relevant                                       References
rationale and supporting experiences.                    Aleven, V., McLaren, B., Sewall, J., & Koedinger, K. R.
  Students in the Evaluate Only condition have             (2009). Example-tracing tutors: A new paradigm for
some of these experiences, but these are not               intelligent tutoring systems. International Journal of
                                                           Artificial Intelligence in Education, 19: 105-154.
customized to their own preconceptions. More             Bransford, J. D., & Schwartz, D. L. (2001). Rethinking
importantly, these students focus on evaluating            transfer: A simple proposal with multiple implications.
methods, and not on identifying requirements.              Review of Research in Education, 24(3), 61-100.
  The     modular      encoding     of    functional     Chase, C. C., Shemwell, J. T., & Schwartz, D. L. (2010).
components (i.e., relating each component in the           Explaining across contrasting cases for deep
                                                           understanding in science: An example using interactive
method to its specific function) allows students to        simulations. In proceedings of the International
adapt their knowledge to the task. For example, in         Conference of the Learning Sciences 2010.
debugging items, students can map the debugged           Kapur, M. (2008). Productive failure. Cognition and
procedures onto the set of requirements they have          Instruction, 26(3), 379 - 424.
come to understand and identify which                    Richland, L. E., Bjork, R. A., Finley, J. R., & Linn, M. C.
                                                           (2005). Linking cognitive science to education:
requirement is not met.                                    Generation and interleaving effects. In B. G. Bara, L.
                                                           Barsalou, & M. Bucciarelli (Eds.), Proceedings of the
                     Summary                               27th annual conference of the cognitive science society.
We present a study that compares invention                 Mahwah, NJ: Lawrence Erlbaum.
activities that include design and evaluation of         Roll, I. (2009). Structured invention activities to prepare
                                                           students for future learning: Means, mechanisms, and
methods to evaluation of predesigned methods,              cognitive processes. Ph.D. Thesis, Pittsburgh, PA.
prior to receiving instruction. Our results show         Roll, I., Aleven, V., & Koedinger, K. R. (2009). Helping
that the design of methods enhances the effect of          students know 'further' - increasing the flexibility of
invention activities. The results also show that           students' knowledge using symbolic invention tasks. In N.
students who design their methods acquire better           A. Taatgen, & H. van Rijn (Eds.), Proceedings of the 31st
                                                           annual conference of the cognitive science society.
domain knowledge, and not necessarily better               Austin, TX: Cognitive Science Society.
invention skills. Further analysis suggests that the     Roll, I., Aleven, V., & Koedinger, K. R. (2010). The
invention process helps students set requirements          invention lab: Using a hybrid of model tracing and
from the general method. Students later identify           constraint-based modeling to offer intelligent support in
functional components that satisfy these                   inquiry environments. In V. Aleven, J. Kay, & J. Mostow
                                                           (Eds.), Proceedings of the international conference on
requirements within the canonical method,                  intelligent tutoring systems. Berlin: Springer Verlag.
resulting in more differentiated and modular             Schwartz, D. L., & Bransford, J. D. (1998). A time for
knowledge. The acquired knowledge can be                   telling. Cognition and Instruction, 16(4), 475-522.
applied flexibly by reusing and recombining the          Schwartz, D. L., & Martin, T. (2004). Inventing to prepare
functional components.                                     for future learning: The hidden efficiency of encouraging
                                                           original student production in statistics instruction.
  The study makes contributions by identifying             Cognition and Instruction, 22(2), 129-184.
the cognitive benefits of invention activities and       Schwartz, D. L., Sears, D., & Chang, J. (2007).
by explaining how these benefits interact with the         Reconsidering prior knowledge. In M. C. Lovett, & P.
taught material to improve the ability to transfer         Shah (Eds.), Thinking with data. (pp. 319-44). New York:
the target knowledge. Students who invent gain             Routledge.
                                                         Sweller, J. (2006). The worked example effect and human
the knowledge of key requirements of formalisms,           cognition. Learning and Instruction, 16(2), 165-169.
of reasons for these requirements, and of                Taylor, J. L., Smith, K. M., van Stolk, A. P., & Spiegelman,
mathematical tools that satisfy (and fail to satisfy)      G. B. (2010). Using invention to change how students
these requirements.                                        tackle problems. CBE-Life Sciences Education, 9(4).
                                                     2829

