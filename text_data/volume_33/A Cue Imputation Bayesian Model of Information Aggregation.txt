UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Cue Imputation Bayesian Model of Information Aggregation
Permalink
https://escholarship.org/uc/item/02g2z0rx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Trueblood, Jennifer
Kachergis, George
Kruschke, John
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                   A Cue Imputation Bayesian Model of Information Aggregation
                              Jennifer S. Trueblood, George Kachergis, and John K. Kruschke
                                           {jstruebl, gkacherg, kruschke}@indiana.edu
                                      Cognitive Science Program, 819 Eigenmann, 1910 E. 10th St.
                                                        Bloomington, IN 47406 USA
                               Abstract                                 on a set of cues. In some situations, the advisers saw only a
                                                                        portion of the six cues. In these cases, not all advisers saw the
   Decision makers are sometimes faced with aggregating ad-
   vice from multiple advisers without knowing what informa-            same subset of cues. Then, the decision maker was asked to
   tion is driving each adviser’s opinion. Following Budescu and        use the advisers’ judgments to provide a probability estimate
   Yu (2006, 2007), we conducted an experiment in which par-            for disease A. The decision maker was told which binary cues
   ticipants first learned to estimate the probability of a disease
   based on multiple test results. Next, subjects made the same         each adviser saw; however, the decision maker was not privy
   judgments solely on the basis of probabilities given by multi-       to the cue values (i.e., positive or negative). The number of
   ple advisers who may have only received partial information.         advisers giving advice was either two or three.
   Experimental results confirm previous findings that decision
   makers give extreme estimates when advisers are in agreement            The experimental results indicated that in some situations
   and compromise estimates when advisors are in disagreement.          decision makers gave estimates more extreme than both ad-
   Unlike previously proposed models that can only account for          visers’ values. In general, when the advisers displayed a high
   extreme or compromise estimates but not both, we develop a
   new Bayesian model that explains both types of judgments.            level of agreement, decision makers produced extreme an-
   This model provides a rational explanation of information ag-        swers. On the other hand, when the advisers’ displayed dis-
   gregation by assuming that decision makers use the probability       agreement, the decision makers seemed to average the ad-
   estimates of advisers to infer underlying data before making
   probability judgments.                                               visers’ estimates. A simple Bayesian model presented by
   Keywords: information aggregation, decision making,                  Budescu and Yu predicts that decision makers will always
   Bayesian model                                                       produce extreme estimates. Since this was not found to be
                                                                        the case, Budescu and Yu (2006) claimed that decision mak-
                           Introduction                                 ers are not always Bayesian. However, we hope to show that
In many real world judgment problems, decision makers of-               a novel Bayesian model can account for compromise judg-
ten make inferences by aggregating information from mul-                ments when there is a discrepancy among advisers’ probabil-
tiple outside sources. For example, a juror might be asked              ity estimates and extreme judgments when advisers display a
to determine the probability that a defendant is guilty based           high level of agreement.
on the testimony of expert witnesses such as a medical ex-
aminer and a forensic scientist. In this paper, we discuss in-                                    Experiment
ference problems in which a decision maker must aggregate               We conducted an experiment with a learning stage followed
judgments from multiple expert advisers whose judgments                 by an information aggregation stage, similar to Experiment
might be based on partial information. Our goal is to de-               2 in Budescu and Yu (2007). Participants first learned to di-
velop a probabilistic framework that can account for the ag-            agnose, using the binary results of three tests, the probability
gregated judgments of decision makers. Specifically, we pro-            that a fictitious disease was present in a series of patients. The
pose a new Bayesian model to explain aggregated judgments               probability of a disease given a set of test results was deter-
and compare our model to a previously proposed Bayesian                 mined by a causal graph. Specifically, it was assumed that
model (Budescu & Yu, 2006, 2007) and a weighted averag-                 a disease caused hidden intermediate states that determined
ing model. The newly proposed Bayesian model is able to                 the test results. For simplicity, we assumed that the interme-
account for both extreme and conservative judgments unlike              diate states were binary like the test results. For example,
previous models of information aggregation.                             lung cancer causes malignant tumors resulting in a positive
   The modeling and experimental work described in this pa-             lung biopsy. Since Budescu and Yu allowed for tests to ei-
per was motivated by a two stage study of information aggre-            ther be correlated or not, we used the causal graph structure
gation conducted by Budescu and Yu (2006). In this experi-              to represent correlation among tests. We denoted diagnostic
ment, subjects first completed a learning stage in which they           tests that were driven by common underlying causes as corre-
learned to distinguish between two diseases (A and B) based             lated. For example, a malignant tumor causes both a positive
on a set of six cues (i.e., medical tests) with binary values (i.e,     lung biopsy and a positive PET scan. Diagnostic tests that
positive or negative test results). Subjects were told that the         were driven by different causes were called uncorrelated. In
cues were equally valid, not perfectly diagnostic, and could            our lung cancer example, a malignant tumor causes a posi-
be correlated. In the second stage of the experiment, some              tive lung biopsy and a high white blood cell (WBC) count
subjects were selected to be advisers and other subjects were           produces a positive blood test.
selected to be decision makers. The advisers provided the de-              After the training stage, participants were given a series of
cision makers with probability estimates of disease A based             trials on which two experts gave the participant the proba-
                                                                    1298

bility a patient has the disease. Participants could see which        number of results they were privy to, and the outcomes of the
test results each expert was privy to, but could not see the re-      results. As a reminder, for the first 12 adviser trials, decision
sults, themselves. The probability given by each expert was           makers also saw the test results the advisers saw (e.g., Fig-
produced from the causal graphs, and thus depends on the              ure 1, right). These trials and the remaining 44 adviser trials
number and values of test results they were privy to. Faced           were identical for decision makers in both conditions, except
with advice from two experts, participants may use a number           that the particular probabilities given as advice depended on
of strategies to aggregate this information. Given that the ad-       the condition (based on the two types of causal graphs: cor-
visers have overlapping information in cases when they saw            related and uncorrelated). The 44 trials included all combi-
some of the same test results, and unique information in cases        nations of strength (i.e., number of agreeing test results) and
when advisers saw disjunct (and perhaps conflicting) test re-         amount (i.e., number of test results seen: 0–3) of evidence
sults, a rational strategy would be for participants to infer the     seen by an adviser, as well as various combinations of the
union of the results that the advisers saw, and thence produce        amount seen by both advisers.
a probability estimate as they did in the learning phase.
                                                                         Trials could vary by the amount of overlapping (i.e., re-
Subjects                                                              dundant) evidence seen by the two advisers: all of the results
71 undergraduates at Indiana University participated in the           seen by the advisers may overlap, none of them may over-
experiment for course credit.                                         lap (e.g., Figure 1, right), or they may partially overlap (e.g.,
                                                                      Figure 1, left). On a given trial, when advisers are privy to
Stimuli & Procedure                                                   non-overlapping test results (e.g., A and B in Figure 1, left),
Participants were instructed that they would be learning to           and the two advisers agree on the diagnosis (both less than
diagnose patients with a fictional disease (e.g., ‘nomitis’) on       50, or both greater), the decision maker, inferring what each
the basis of results from three tests. As in Budescu and Yu           test result is, should normatively extremify. In other words,
(2007), participants were told that each test is equally diag-        the decision maker should respond further from 50 than ei-
nostic, that the probability of the disease in the patient popu-      ther adviser. The reason is that, by inferring the test results,
lation is 0.5, and that results of the medical test may be corre-     the decision maker has more information than either adviser,
lated. A high prevalence of disease was used in order to match        and the decision maker knows that this inferred information
the experimental paradigm of Budescu and Yu (2007). Sub-              is stronger than the individual pieces of information seen by
jects were told that there was an epidemic in order to make           each advisers. On other trials, when one adviser sees test
the high base rate of the disease seem plausible.                     results that are a subset of the other adviser’s, the decision
   On each of 88 training trials, participants were shown the         maker should match the rating of the adviser who saw the
binary results (‘+’ or ‘−’) of three tests (‘A’, ‘B’, or ‘C’), and    most, because the adviser who saw the subset of tests pro-
asked to indicate the probability (0-100) that a patient with         vides no additional information. Finally, in cases where ad-
these test results has the disease. Probabilistic feedback was        visers saw non-overlapping results but disagree on the diag-
provided on each trial: participants were told that this partic-      nosis (e.g., Figure 1, right), decision makers should compro-
ular patient did or did not have the disease (proportional to         mise, and give a rating somewhere between what the advisers
the predetermined disease probabilities), and were also told          gave. Like the case of extremifying, the decision maker is
whether or not a patient exhibiting these results would typi-         able to infer more information than either adviser. However,
cally have the disease.                                               in this scenario, the inferred information is contradictory and
   Appearing after the initial 88 trials, an additional 19 train-     leads to a compromising estimate.
ing trials showed only 0, 1, or 2 test results, and no feed-
back was given after participants responded. These trials in-
troduced the possibility that only a subset of the test results
may be seen, as is true in many of the information aggregation
trials.
   For 36 of the participants, test results were uncorrelated,
whereas test results for the remaining 35 participants were
correlated.
   In the second stage of the experiment, participants were
told that they had been promoted to a supervisory position
in which they would continue to judge the probability a pa-           Figure 1: Information given on adviser trials: partial over-
tient has the same disease. However, instead of looking at            lap on which decision makers should extremify (left), and no
test results themselves, participants would see the probabil-         overlap on which decision makers should compromise (right).
ities given by two advisers who have had the same training            Actual test results (+/-) were only given on the first 12 adviser
they have had, but who may see only some of the test results          trials.
(e.g., Figure 1, left). Participants were instructed to keep in
mind that each adviser’s advice would be based on both the
                                                                  1299

Results & Discussion                                                   Bayesian models were two of the many different models ex-
The best theoretical accuracy during training is 0.68 for both         amined by Budescu and Yu (2006). We selected these models
conditions, possible only if a participant gives a probability         to use in our comparison because the WMLO and adjusted
rating in accord with the more likely outcome for the test re-         naive Bayesian model fit the data from Budescu and Yu well
sults on all 88 complete trials. In the last half of training, ac-     from a global point of view. Thus, we consider these models
curacy in the uncorrelated tests condition (M = .660) is higher        as good competitors for testing our new Bayesian model.
than accuracy in the correlated tests condition (M = .615).            An Averaging Model
In a Bayesian estimate of the difference between groups, the
mean of the posterior was 0.045, with the 95% highest den-             The weighted mean log-odds model is an averaging model
sity interval extending from 0.0283 to 0.0613, and with 100%           which accentuates differences between extreme probabili-
of the posterior distribution falling above 0.02 (Kruschke,            ties. By using log-odds, the high and low probabilities are
2011). Not surprisingly, correlated test results confuse learn-        stretched out before they are averaged. Letting E be the tar-
ers somewhat. Since we are primarily interested in the way             get event (i.e., presence of a disease), we say that adviser j
that experts aggregate advice in the second stage, we chose to         provides a probabilistic forecast p j = p(E|C j ) based on the
exclude participants who did not become experts (i.e., those           set of cues C j . The WMLO model is given by the following
who were more than a standard deviation below the group                formula
                                                                                                              eβ
mean of their condition). In the uncorrelated condition, this                                   W MLO =                              (1)
criterion (M − σ = .57) excludes 6 participants, and in the                                                 1 − eβ
correlated condition (M − σ = .52) excludes 7 participants.            where
                                                                                                         J
   An additional participant was excluded from the correlated                                    1                    pj
                                                                                        β= J          · ∑ (n j · ln        )         (2)
condition because they responded ‘50’ on every adviser trial.                                ∑ j=1 n j j=1          1 − pj
Thus, the adviser trials of 30 participants in the uncorre-
lated condition and 27 participants in the correlated condition        and where J is the number of advisers and n j is number of
were analyzed. To evaluate aggregation performance, we ex-             cues seen by adviser j. This model can out perform a simple
amined the proportion of times each participant extremified,           averaging model because it can better account for extreme
compromised, and matched relative to the number of times               judgments; however, it cannot extremify. In other words, the
they should have shown each of these behaviors according to            model cannot respond further from 50 than either adviser.
our normative model of the task. We found that the graph               Thus, if we compared the model predictions to the norma-
structure had no notable effect on these proportions, so Table         tively correct behavior as was done in Table 1 for the behav-
1 shows the aggregate performance of all 57 participants. For          ioral data, the cell of the table indicating an extremifying pre-
each type of normative behavior, indicated by rows in Table            diction when extremifying is the normatively correct behavior
1, the most often observed behavior was the normative one.             would be 0. Also note that this is a parameter free model.
For example, when the normative behavior was to extremify,             Naively Adjusted Bayesian Model
44% of responses were extremifications, which is more than
                                                                       We begin our discussion of the two Bayesian models of
any other type of response.
                                                                       information aggregation by describing the naively adjusted
                                                                       Bayesian model (Budescu & Yu, 2006). This model is an ex-
Table 1: Adviser trial behavior by normatively correct behav-          tension of a naive Bayesian model that calculates the poste-
ior. Each cell shows p(observed | normative).                          rior probability of an event, E, assuming conditional indepen-
                                                                       dence of the advisers’ forecasts. By the conditional indepen-
                                                                       dence assumption and Bayes’ rule, the posterior probability
                                  Observed
                                                                       is given by
  Normative        Extremify    Compromise        Match     Other
  Extremify           .44             .12           .14       .29              \                        ∏Jj=1 p j /p(E)J−1
  Compromise          .19             .68           .13        0        p(E |    Cj) =
                                                                                         ∏Jj=1 p j /p(E)J−1 + ∏Jj=1 (1 − p j )/p(E)J−1
  Match               .32             .21           .40       .07
                                                                                                                                     (3)
  Overall             .32             .34           .22       .12
                                                                       where J denotes the number of advisers and p j is defined as
                                                                       above. As noted by Budescu and Yu, this is very similar to
                                                                       the aggregation model by Bordley (1982) and the aggregation
                           Models                                      rule by Genest and Schervish (1985).
Using the data collected from the new experiment, we                      This naive Bayesian model assumes that decision makers
compared three different aggregation models, a weighted                treat the judgments from each adviser as perfectly reliable.
mean log-odds model (WMLO), a naively adjusted Bayesian                In order to allow decision makers to adjust their judgments
model, and the new cue imputation Bayesian model. Be-                  based on different assumptions of reliability, Budescu and Yu
fore we discuss the model comparison, we describe each of              incorporated a discounting function into the model. Specif-
the three models in detail. The WMLO and naively adjusted              ically, before the probability judgments are aggregated, they
                                                                   1300

are discounted according to the model by Karmarkar (1978)
given by
                                                                                  D                                 D
                                    pλj
                       p0j =                                 (4)
                             pλj + (1 − p j )λ                                    S                            S1      S2
where 0 < λ < 1 is a parameter associated with the level of
                                                                             T1       T2
                                                                                      T2                       T1      T2
discounting. When λ = 1, no adjustment is made and the
probability judgment is considered perfectly reliable. How-
ever, when λ = 0, the probability is transformed into 0.5, and
the probability judgment is considered not diagnostic. The
overall effect of this model is shrinkage of extreme probabili-      Figure 2: Causal graphical models for two correlated tests
ties before aggregation. The naively adjusted Bayesian model         (left) and for two uncorrelated tests (right).
can provide a better account of conservative probability es-
timates than the simpler naive Bayesian model; however, it           Table 2: Example of Conditional Probabilities for Uncorre-
cannot compromise. In other words, the model cannot give a           lated Tests
rating somewhere between the advisers’ ratings. If we com-
pared the model predictions to the normatively correct behav-
ior as was done in Table 1 for the behavioral data, the cell of        Diseases          Intermediate States           Tests
the table indicating a compromise prediction when compro-              p(D= +) = .5      p(Si = + | D= +) = .8         p(Ti = + | Si = +) = .9
mising is the normatively correct behavior would be 0.                 p(D= −) = .5      p(Si = − | D= +) = .2         p(Ti = − | Si = +) = .1
                                                                                         p(Si = + | D= −) = .2         p(Ti = + | Si = −) = .1
A Cue Imputation Bayesian Model                                                          p(Si = − | D= −) = .8         p(Ti = − | Si = −) = .9
Because both the information aggregation study by Budescu
and Yu and our new experiment consisted of a learning stage          ture) on trial t by
and an evaluation stage, we include in our Bayesian descrip-
tion of the task a model of the learning process. None of the                                           p(Xt |h) · p(h)
models considered by Budescu and Yu included a description                               p(h|Xt ) =                     .                 (5)
                                                                                                            p(Xt )
of the learning stage of the task.
   Begin by letting Xt be the data seen on trial t. For our          The likelihood is determined by the causal graph under con-
purposes, Xt is a set of dichotomized test results related to        sideration,
diagnosing the potential disease. For the case of three tests                                              n
as in the experiment described above, we might have Xt =                  p(Xt |h) = p(T1 , ..., Tn |h) = ∏ p(Ti |parents(Ti ), h)        (6)
{+, +, −}. Let the hypothesis space, H, be the set of causal                                              i=1
graphs (or Bayesian networks) representing the relationship
                                                                     where n represents the number of cues (or tests) and Ti is a
between the disease and the test results. It is assumed that a
                                                                     random variable representing the result on test i. To model
disease causes hidden intermediate states (labeled S below)
                                                                     learning, we define the posterior of one trial as the prior for
that drive the test results (labeled T below).
                                                                     the next trial where the initial prior is 1/|H| where |H| is sim-
   For simplicity, consider the case when there are only two         ply the number of elements or graphs in the hypothesis space.
diagnostic tests. In this case, the correlated causal graph          Thus, the probability of the disease being present on trial t is
shows that a disease causes a single intermediate state which        given by
drives the results of the two tests (see Figure 2 left). On the
other hand, the uncorrelated causal graph shows that a disease                   p(D = +|Xt ) =      ∑ p(D = +|Xt , h) · p(h)             (7)
causes two intermediate states which each drive the results of                                      h∈H
corresponding tests (see Figure 2 right). We can represent
causal graphs by conditional probabilities. For example, we          where p(D = +|Xt , h) is easily calculated from the causal
might purpose Table 2 for our uncorrelated causal graph.             graph h. Please note that in the current formulation, only p(h)
                                                                     is learned and not p(Ti |S j ) or p(Si |D).
The Learning Stage In the learning stage, the model learns
the probability that the tests are correlated. Let h ∈ H repre-      The Information Aggregation Stage Our approach to-
sent a causal graph. In the case with two tests, we might as-        wards information aggregation differs from the one taken by
sume that there are only two causal graphs as shown in Figure        Budescu and Yu. Specifically, we assume that decision mak-
2. Of course, a larger hypothesis space and more complicated         ers first attempt to reconstruct the data observed by advisers
graphical structures can be used. By Bayes’ Rule, we define          before calculating the probability of a target event (e.g. the
the posterior probability of each hypothesis (i.e. causal struc-     presence of the disease). In this way, decision makers use
                                                                 1301

both the advisers’ probability estimates along with informa-            probability of the target event. However, we assume that the
tion about which cues the advisers saw, not merely how many             aggregation process occurs when decision makers reconstruct
cues the advisers saw. In the Bayesian model presented by               the possible data seen by advisers instead of when they eval-
Budescu and Yu, cue information was not inferred in the de-             uate the probability of a target event. Like Budescu and Yu,
cision maker’s aggregation process.                                     we assume that the advisers’ forecasts, p j , are conditionally
   Suppose that there are J advisers reporting the probability          independent.
of the presence of the disease to a decision maker. Further,                Finally, the decision maker creates a Bayesian posterior us-
assume that each adviser sees the results of a subset of all            ing his or her own prior for cue correlations, p(h):
possible binary tests (or cues). The decision maker knows
which tests each adviser saw, but not the results of the tests.                    p(D = +|Xi ) =     ∑ p(D = +|Xi , h)∗ · p(h).    (11)
                                                                                                     h∈H
Let p j be the probability the disease is present reported by
adviser j. (The probability the disease is absent is 1 − p j .)         To allow for the fact that decision makers might not learn
The decision maker then infers a belief distribution over the           the exact probabilities in the graphical models, we make the
possible data that adviser j might have observed. Specifically,         following adjustment
we define the data space as the set of all possible complete                                               p(D = +|Xi , h)γ
data vectors. Thus, for n cues, the data space has 2n elements           p(D = +|Xi , h)∗ =
                                                                                               p(D = +|Xi , h)γ + (1 − p(D = +|Xi , h))γ
since the cues are assumed to be binary. In our experiment,                                                                         (12)
n = 3 and the data space has 23 = 8 elements. For simplicity,           where p(D = +|Xi , h) is calculated directly from the graphi-
let us index the elements of the data space as Xi where i ∈             cal model h, and γ is allowed to freely vary. Thus, the proba-
{1, 2, 3, ..., 2n }. Then, for adviser j we have that the posterior     bility the disease is present is given by
probability of data vector Xi is
                                                                                                       2n
                                   p(p j |Xi , h) · p(Xi |h)                           p(D = +) = ∑ p(D = +|Xi ) · p(Xi )           (13)
                  p(Xi |p j , h) =                              (8)
                                          p(p j |h)                                                   i=1
                                                                        where p(Xi ) = p(Xi |
                                                                                                T
                          n
where p(p j |h) = ∑2i=1 p(p j |Xi , h) · p(Xi |h) and where we de-                                j pi ).
fine p(p j |Xi , h) = 1 if p(Xi |h) = p j and 0 otherwise. By ex-                           Model Comparisons
amining the relationship between probabilities, p(Xi |h), from
the graphical structure and probabilities, p j , from the adviser,      Using the data collected in the experiment described above,
we assume that the adviser only produces probabilities in ac-           we evaluated the three models of interest: the WMLO model,
cord with the graphical structure. By restricting p(p j |Xi , h) to     the naively adjusted Bayesian model, and our new Bayesian
the set {0, 1}, we also assume there is no noise in the adviser’s       inference model. Since the WMLO model and the naively
stated value. The model can be altered to relax these assump-           adjusted Bayesian model do not contain learning models, we
tions, but we felt that this was not necessary for modeling the         only used the information aggregation data in fitting the mod-
current experiment. We acknowledge that the lack of variabil-           els and performing model comparisons. Because there was
ity or bias in advisers’ ratings in unrealistic, but the assump-        no difference is aggregation performance for correlated ver-
tion that advisers produce probabilities in accordance with             sus uncorrelated tests, we fit the models to all 57 partici-
the graphical structure is parsimonious. In the case where a            pants. We evaluated WMLO as a parameter free model and
judge sees a partial set of cues X j∗ such as {+, ?}, we define         fit the naively adjusted Bayesian model and our new Bayesian
p(p j |Xi , h) = 1 if Xi ⊆ X j∗ and p(Xi |h) = p j . For example,       model by minimizing the sum of the squared error (SSE) be-
in the case with two tests, suppose adviser j sees a positive           tween the model predictions and the probability judgment
result on the first test and nothing on the second. In other            data from the experiment. Future work may use Bayesian
words, probability p j is based on the data X j∗ = {+, ?}. Since        model comparison, but for current purposes the simpler point
we only want to define a probability distribution over com-             estimates suffice to demonstrate qualitative differences be-
plete data, we consider both {+, +} and {+, −} as candidate             tween models. Both the naively adjusted Bayesian model and
data vectors. By summing over all possible cue relationships,           our new Bayesian model contain one parameter (λ and γ re-
we have                                                                 spectively). We fit the three models to the individual data for
                   p(Xi |p j ) = ∑ p(Xi |p j , h) · p(h).       (9)     the 44 information aggregation trials. Thus, we obtained three
                                 h∈H                                    model fits for each subject for 57 subjects. For each subject,
   Next, the decision maker creates a belief distribution over          we compared the one parameter naively adjusted Bayesian
all possible data by combining across advisers:                         model with the one parameter cue imputation Bayesian model
                                                                        and the parameter free weighted mean log-odds model. The
                     \             ∏ j p(Xi |p j )/p(Xi )J−1            mean squared error (MSE) averaged over subjects for each
              p(Xi |     p j) =                               (10)
                                 ∑i ∏ j p(Xi |p j )/p(Xi )J−1           model is given in Table 3. From the table, we see that our
                      j
                                                                        new Bayesian model has the smallest MSE.
where p(Xi ) = ∑h∈H p(Xi |h) · p(h). This formulation is iden-              We also computed a quantitative goodness-of-fit measure
tical to the one Budescu and Yu used to calculate the posterior         to a qualitative partitioning of the data. For each subject,
                                                                    1302

         Table 3: MSE for Three Aggregation Models                     Table 5: Rank Correlation for Three Aggregation Models
                   Model                MSE
                   WMLO                 0.0182                         model                mean     median      std     min    max
                   Naive Adj Bayes      0.0700                         WMLO                  0.24     0.26      0.22    -0.32   0.64
                   Imputation Bayes     0.0149                         Naive Adj Bayes       0.10     0.09      0.21    -0.29   0.53
                                                                       Imputation Bayes      0.36     0.38      0.17    -0.09   0.64
we found the observed frequencies of responses in the four
categories (extremify, match, compromise and other) given            model cannot account for extremifying judgments, the new
the normatively correct response. In other words, we com-            Bayesian model can account for both. The model provides a
puted a table similar to Table 1 except the proportions were         rational explanation of information aggregation by assuming
replaced by frequencies. We then computed the same fre-              that decision makers use the probability estimates of advisers
quencies for the three models using the best fit parameters to       to infer the underlying data before calculating the probability
each subject’s data. We calculated the SSE between the ob-           of an event. Thus, the model postulates that decision mak-
served frequencies and model frequencies for each subject.           ers use all of the available information in an environment.
This procedure is very similar to computing a chi-square test-       Decision makers do not merely use the number of cues advis-
statistic except that we do not weight the differences between       ers saw, but they use this knowledge along with probability
observed frequencies and model frequencies by the recipro-           estimates from the advisers to infer information. Often, the
cal of the model frequencies. We felt that using such weights        information inferred by decision makers is more informative
placed too much emphasis on instances where models have              than the information seen by either adviser.
low frequencies, and we have no theoretical conviction that             Our goal was to illustrate that it is possible to provide an
these instances should be weighted heavily. Table 4 shows            account of probability judgments in situations of information
several descriptive statistics for the SSE values. From the ta-      aggregation by using the axiomatic principles of probability
ble, we see that our new Bayesian model out performs the             theory rather than heuristics. While heuristic models such
other two models.                                                    as WMLO provide an initial means for studying probability
                                                                     judgment phenomena, these approaches lack a rational foun-
                                                                     dation and only provide an ad hoc explanation for probability
   Table 4: Frequency SSE for Three Aggregation Models
                                                                     judgment phenomena. However, probabilistic models such
                                                                     as the Bayesian model introduced in this paper offer a more
  model                  mean     median    std    min    max        coherent account of human probability judgments.
  WMLO                    347       254     278    26    1154
  Naive Adj Bayes         679       650     398    24    1512                            Acknowledgments
  Imputation Bayes        298       230     217    10    1014        The first author was supported by the NSF/IGERT Training
                                                                     Program in the Dynamics of Brain-Body-Environment Sys-
                                                                     tems at Indiana University.
   We used a qualitative evaluation of the models to see how
closely the models were capturing ordinal trends in the data.                                 References
We calculated the Kendall rank correlation coefficient (or           Bordley, R. (1982). A multiplicative formula for aggregating
Kendall’s τ ) for every subject to measure the association be-          probability assessments. Management Sci, 28, 1137-1148.
tween the subject’s response proportions and the predicated          Budescu, D., & Yu, H.-T. (2006). To bayes or not to bayes?
proportions from the models in the four categories (extremify,          a comparison of two classes of models of information ag-
match, compromise and other) given the normatively correct              gregation. Decision Analysis, 3, 145-162.
response. Like the previous comparison, we used the best fit         Budescu, D., & Yu, H.-T. (2007). Aggregation of opinions
model parameters for each subject. Table 5 shows descrip-               based on correlated cues and advisors. J. Behav. Decision
tive statistics for the τ values. A τ value of 1 implies perfect        Making, 20, 153-177.
agreement between the rankings and a τ value of −1 implies           Genest, C., & Schervish, M. J. (1985). Modeling expert
perfect disagreement between the rankings. From the table,              judgements for bayesian updating. Ann. Statist, 13, 1198-
we again see that our new Bayesian model performs better                1212.
than the other models.                                               Karmarkar, U. (1978). Subjectively weighted utility: A de-
                           Discussion                                   scriptive extension of the expected utility model. Organ.
                                                                        Behav. Human Performance, 21, 61-72.
The cue imputation Bayesian model gives a more comprehen-            Kruschke, J. K. (2011). Doing Bayesian data analysis: A tu-
sive account of information aggregation than previously pro-            torial with R and BUGS. Burlington, MA: Academic Press
posed models. Whereas the naively adjusted Bayesian model               / Elsevier.
cannot account for compromising judgments and the WMLO
                                                                 1303

