UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Topic Shift in Efficient Discourse Production
Permalink
https://escholarship.org/uc/item/6b0712jg
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Qian, Ting
Jaeger, T. Florian
Publication Date
2011-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                                   Topic Shift in Efficient Discourse Production
                                                  Ting Qian (tqian@bcs.rochester.edu)
                                             T. Florian Jaeger (fjaeger@bcs.rochester.edu)
                                  Department of Brain and Cognitive Sciences, University of Rochester
                                                           Rochester, NY 14627 USA
                                Abstract                                  defined as the negative log-probability of a sentence. The dif-
   Speakers have been hypothesized to organize discourse con-             ference between unconditional and conditional information
   tent so as to achieve communicative efficiency. Previous work          relates to whether sentence information reflects the effect of
   has focused on indirect tests of the hypothesis that speakers          discourse context. These considerations led Genzel and Char-
   aim to keep per-word entropy constant across discourses to
   achieve communicative efficiency (Genzel & Charniak, 2002).            niak (2002) to propose the Constant Entropy Rate hypothesis,
   We present novel and more direct evidence by examining the             according to which unconditional sentence information is ex-
   role of topic shift in discourse planning. If speakers aim for         pected to increase over time if conditional sentence informa-
   constant per-word entropy, they should encode less uncondi-
   tional per-word entropy (as estimated based on only sentence-          tion stays more or less uniform (for more detail, see Qian
   internal cues) following topic shifts, as there is less relevant       and Jaeger (submitted)). Thus, the finding of a positive cor-
   context to condition on. Applying latent topic modeling to a           relation between unconditional information and sentence po-
   large set of English texts, we find that speakers are indeed sen-
   sitive to the recent topic structure in the predicted way.             sition can be taken as evidence for communicative efficiency
   Keywords: discourse production; topic shift; communicative
                                                                          of language.
   efficiency                                                                 However, the observed positive correlation is a rather weak
                                                                          confirmation of communicative efficiency, primarily because
                           Introduction                                   it is only a necessary, but not sufficient, condition of the hy-
Recent years have seen a surge in accounts motivated by in-               pothesized uniform distribution of conditional information in
formation theory that consider language production to be par-             discourses. One way of obtaining stronger results is to esti-
tially driven by a preference for communicative efficiency                mate the conditional information of sentences and then tests
(Aylett & Turk, 2004; Ferrer i Cancho & Dı́az-Guilera, 2007;              whether those estimates indeed form a uniform distribution.
Genzel & Charniak, 2002; Jaeger, 2010; Levy & Jaeger,                     This would require one to obtain a discourse-sensitive lan-
2007). Here we focus on evidence from discourse produc-                   guage model, from which conditional information estimates
tion that speakers distribute information across sentence so              can be derived. One can also work with unconditional in-
as to hold the conditional entropy associated with a word                 formation estimates and try to identify variables that max-
constant, which would facilitate efficient information transfer           imally correlate with discourse context. The effect of sen-
(Genzel & Charniak, 2002). As language production unfolds                 tence position on unconditional information is expected to
over time, information needs to be computed with reference                be subsumed by such predictors, since they have essentially
to conditional (contextualized) probabilities. This raises the            compensated for the lack of discourse context in the uncon-
question as to what cues are integrated into contextualized               ditional estimates of information. Here we present a series
probabilities, and how they are integrated. This issue so far             of studies that apply both methods through the use of topic
has received little to no attention. We investigate whether               modeling. We derived two partially conditioned estimators
shifts in the latent topics of a discourse influence the amount           of sentence information by estimating the topics in the pre-
of information encoded in each sentence.                                  ceding discourse (a fully conditioned estimator would at least
   Previous research on efficiency in discourse production has            incorporate world knowledge that is relevant to the discourse,
revealed an interesting relation between the information con-             which is almost implausible). At the same time, topic mod-
tent of a sentence and its position in a discourse: on average,           eling also allowed us to measure topic shifts in a discourse.
sentences that occur later in a discourse tend to contain more            When a discourse undergoes a large topic shift, the previously
unconditional information per word than earlier ones (Genzel              mentioned materials are less predictive of the upcoming ma-
& Charniak, 2002, 2003; Keller, 2004; Piantadosi & Gib-                   terials, leading to higher information in those sentences. This
son, 2008; Qian & Jaeger, 2009). Unconditional information                intuitively suggests that topic shift will be a good predictor of
refers to the information a word (or sentence) carries if only            sentence information. We test this hypothesis in both studies.
sentence-internal cues are considered (i.e. without consider-                 We begin with a review of previous work that motivates the
ation of preceding discourse context). Why would speakers                 approach taken here.
distribute discourse information in such a way? Information
theoretic considerations about efficient communication pro-               Estimating Sentence Information
vide a possible explanation. Shannon’s noisy channel theo-                Previous studies mostly used ngram to estimate how much in-
rem implies that an efficient communication system should                 formation a sentence contains independent of discourse con-
transmit information at a constant rate close to the channel              text. Under those models, the probability of a word is con-
capacity (Shannon, 1948). The information of a sentence is                ditioned on certain within-sentence elements (e.g. the n-1
                                                                      3313

preceding words in a sentence for an ngram model). The                   on their position in the discourse were a better fit.
probability estimate of a sentence is simply a product of the               However, the decay functions in Qian and Jaeger (2010)
probability estimates of its words.                                      only superimposed a general nonlinear pattern onto data
   Besides the obvious problem that these estimators of sen-             based on the idea of topic shift. The actual sizes of topic
tence information do not consider the effect of discourse con-           shifts were not measured. Our current studies adopt a more
text at all, it is difficult to intuitively understand whether these     direct approach by estimating topic shifts throughout the dis-
estimates represent how much information the speaker has                 course. If topic shift correlates with discourse context, the
planned or how much information the listener may perceive.               positive correlation between sentence position and uncondi-
A hypothesis of efficient language production should distin-             tional estimates of sentence information is expected to dis-
guish between these two possibilities.                                   appear. In addition, topic shift itself is expected to be a sig-
   We approach this problem by first considering what in-                nificant predictor of sentence information. When there is a
formation may be privileged to the speaker in the process                large topic shift, the preceding discourse context may not be
of discourse production. One such factor is the topics of a              so useful in predicting the upcoming sentence, and a rational
discourse. In discourse production, the speaker typically has            speaker should encode less information in the upcoming sen-
more information about the intended topics than the listener             tence. Therefore, a negative correlation between topic shift
before producing the corresponding utterances. The listener,             and sentence information is expected.
on the other hand, has to infer the topics of the discourse after
observing the utterances. Thus, the amount of information                                           Methods
that is in a sentence may appear differently to the speaker
and to the listener. This raises the question whether speakers
                                                                         Data
distribute information, taking into consideration the listener’s         We used the Brown corpus in the form provided by the Python
uncertainty about the topic.                                             Natural Language Toolkit (Bird, Loper, & Klein, 2009). The
   We present two studies designed to address this question.             data set consists of 500 English articles. We divided the cor-
Study 1 proposes a topic conditional estimator of sentence               pus into a training set of 400 articles for building the topic
information, which is an attempt to estimate how much in-                model, a development set of 50 articles for monitoring the
formation a speaker has planned in each sentence given the               quality of the trained model, and a testing set of 50 articles
current discourse topic. Study 2 proposes a latent bigram es-            for conducting the studies. Each group has a random mix-
timator of sentence information, which marginalizes over all             ture of topic categories as labeled in the corpus. To normalize
possible topics to preserve maximal uncertainty – arguably               the lengths of articles, only the first 50 sentences of each arti-
a rational strategy that the listener might adopt (although an           cle are included in data sets. We excluded all function words
interpretation in terms of production is also possible, since            and content words that appeared less than 15 times or more
speakers may not have perfect certainty about what they said             than 450 times in the training data. This exclusion criterion
and how they intend to convey their current message). Be-                aimed to keep only the semantically significant content words
cause the topics of a discourse usually stretch over a few sen-          in sentences.
tences, these estimators are implicitly sensitive to discourse
context in a limited fashion (see below for more detail).                Modeling Topics
Explaining Nonlinear Patterns                                            The topic of a sentence can influence the predictability of
                                                                         words. For example, the two words “the wall” are almost
One additional issue is that many early studies have implic-             certainly followed by “street” in an article that discusses the
itly assumed a linear correlation between unconditional sen-             financial market; whereas in a fairy tale, they are much more
tence information per word and sentence position (Genzel &               likely to be a complete noun phrase. An ngram model, which
Charniak, 2002, 2003; Keller, 2004). This assumption is                  only considers the surface dependency of word tokens, will
challenged by recent studies that found sublinear relations              predict “street” is equally likely in both contexts.
between a sentence’s position in the discourse and its un-
                                                                            Work in natural language processing uses topic models to
conditional information (Piantadosi & Gibson, 2008; Qian
                                                                         estimate the latent topics of a collection of texts, for exam-
& Jaeger, 2009, submitted). Qian and Jaeger (2010) derive
                                                                         ple, in order to find “hot” research topics published in jour-
this pattern from the assumption that the informativity of con-
                                                                         nals (Griffiths & Steyvers, 2004). Here, we adopt the genera-
textual cues on average decays with increasing distance (cue
                                                                         tive approach described in Blei, Ng, and Jordan (2003). Each
weight decay hypothesis). This assumption is based on the
                                                                         topic t is defined by a multinomial probability distribution φ
intuition that discourses typically consist of several topics so
                                                                         over words w, and each text d is a multinomial distribution θ
that contextual cues that have been introduced under Topic A
                                                                         of topics:
may have little predictive power over the content of Topic B.
To test this hypothesis, we applied generic decay functions
which lower the predictive power of distant discourse contex-
                                                                                               w|t, φ ∼ Discrete(φ)
tual cues, and found that the resulting predictions about the
average unconditional per-word entropy of sentences based                                         d|θ ∼ Discrete(θ)
                                                                     3314

   The generative assumption entails that a speaker produces            P(T ) is the prior belief about the distribution of topics; re-
the content of a discourse by first selecting a mixture of top-     sults of using a flat prior and an informative prior will be pre-
ics θi that they want to convey to their audience, and then         sented. In either case, the belief about the distribution of top-
sampling from topic distributions φi for words. In training         ics is updated on-line so that the posterior belief carries over
the topic model, each individual sentence in the training set       to be the prior belief at the next sentence.
is provided as a single training “document” to the model for            As a result, topic shifts can be conveniently quantified by
discovering latent topics. The Python package deltaLDA              measuring the distance between the posterior and prior be-
(Andrzejewski, Mulhern, Liblit, & Zhu, 2007), which imple-          lief distributions of topics. One metric for such a distance is
ments a Gibbs sampler as described in Griffiths and Steyvers        Kullback-Leibler (KL) divergence. The KL divergence be-
(2004), was used. To determine the optimal number of la-            tween the two belief distributions of topics is computed as:
tent topics, we monitored the cross-entropy on the develop-
ment set as the number of topics was varied. The number                                                        p(t|s)
                                                                                           KL = ∑ log2 p(t|s)                         (5)
was determined to be 80. When the topic model was trained,                                        t             p(t)
two probability distribution were obtained: P(W |T ), which             In other words, it’s the logarithmic difference between the
gives the conditional probability of content words given top-       posterior and the prior belief distributions of topics, weighted
ics, and P(T |D), which gives the conditional probability of        by the posterior distribution. This distance intuitively repre-
topics given training documents. The latter distribution will       sents the amount of topic shifts between sentences.
be useful in computing the marginal probability of a topic:
                                                                                                    Study 1
                 p(T = t) = ∑ p(T = t|D = d)                (1)
                              d
                                                                    Methods
which allows us to compute an informative prior on the prob-        We assume that the speaker plans the topic of a sentence
abilities of topics.                                                tintended before actually producing the content of that sen-
                                                                    tence si = {wi1 . . . win } (i.e. we assume that speakers know
Estimating Sentence Information                                     with certainty which topic they intend to talk about). Thus,
Sentence information refers to the negative log-probability of      the amount of information in that sentence, to the speaker,
a sentence:                                                         is an a posteriori estimate, depending on the intended topic:
                                                                     ˆ i ) = I(si |tintended ).
                                                                    I(s
                        I(s) = − log p(s)                   (2)         The first task is then to infer the topic of a sentence si . Un-
                                                                    fortunately, we as researchers (as opposed to the speaker her-
   Estimates of per-word sentence information were ob-
                                                                    self) can only estimate this topic in a post-hoc fashion since
tained by dividing sentence information estimates by sen-
                                                                    we are unable to foretell the speaker’s plan before observing
tence lengths, yielding a normalized quantity that can be com-
                                                                    the production results. Therefore, we take the most probable
pared across sentences. We describe the detailed methods as
                                                                    topic given the sentence content as the closest estimate of the
part of the studies.
                                                                    speaker’s intended topic:
Measuring Topic Shifts
We model the changes in discourse topics using Bayesian                           tˆintended = argmax p(t|s = {w1 . . . wn })         (6)
                                                                                                   t
belief updating. At the beginning of a discourse, either the
                                                                        Equation (6) is a maximum a posteriori (MAP) estimate of
speaker or the listener is assumed to have an initial belief
                                                                    the topic of a sentence, where
about which topics may be likely and which may not based
on their knowledge of probable topics. As the discourse con-
tinues, this belief is updated at the end of each sentence to                                          p(s = {w1 . . . wn }|t)p(t)
reflect the most likely topic to date. According to the Bayes’               p(t|s = {w1 . . . wn }) =                                (7)
                                                                                                                  p(s)
rule, the posterior belief about the distribution of topics is
proportional to the product of the likelihood function and the          The first term of the numerator in Equation (7) is the prob-
prior:                                                              ability of a sentence given a topic t, which follows the same
                                                                    conditional independence assumption as shown in Equation
                     P(T |s) ∝ p(s|T )P(T )                 (3)     (4). The second term is the prior probability of a topic. We
                                                                    used a flat prior for estimating the posterior distribution of
   The likelihood function p(s|T ) refers to the probability of
                                                                    topics for the first sentence in a discourse. When an informa-
the sentence given a particular topic. To simplify the model,
                                                                    tive prior was used, differences were minimal and thus were
we assume that all words are conditionally independent given
                                                                    not reported separately. For each subsequent sentence posi-
a topic. Thus, the probability of a sentence is the product of
                                                                    tion, an on-line updating procedure ensues. In some sense,
the probabilities of its words:
                                                                    the probability estimate of a sentence is implicitly sensitive
                                                                    to the entire preceding discourse, since the belief distribution
                      p(s|T ) = ∏ p(w|T )                   (4)
                                w∈s                                 of topics has been carried over consecutively.
                                                                3315

   We refer to the corresponding estimator of sentence infor-
                                                                            Sentence information (topic conditional estimator)
                                                                                                                                                                                                                          ●          ●   ●     ●●                        ●
mation (shown below in Equation (8)) as a topic conditional                                                                                                                                      ●
                                                                                                                                                                                                 ●
                                                                                                                                                                                                                ●
                                                                                                                                                                                                                ●        ● ●●
                                                                                                                                                                                                                         ●●●
                                                                                                                                                                                                                            ●●
                                                                                                                                                                                                                            ●●
                                                                                                                                                                                                                                    ●
                                                                                                                                                                                                                                  ●● ●●
                                                                                                                                                                                                                                        ●
                                                                                                                                                                                                                                        ●
                                                                                                                                                                                                                                            ●●
                                                                                                                                                                                                                                              ●
                                                                                                                                                                                                                                              ● ●
                                                                                                                                                                                                                                                    ● ● ●
                                                                                                                                                                                                                                                          ●
                                                                                                                                                                                                                                                         ●●
                                                                                                                                                                                                                                                          ●
                                                                                                                                 12.4                                                                           ●        ●        ●● ●        ●
                                                                                                                                                                                                 ●                       ●● ●               ●   ●●●      ● ●
                                                                                                                                                                                                                                        ●       ●●●
estimator (TCE) of sentence information. The information                                                                                                     ●
                                                                                                                                                             ●●
                                                                                                                                                             ●                               ●
                                                                                                                                                                                                 ●
                                                                                                                                                                                                                ●
                                                                                                                                                                                                                ●
                                                                                                                                                                                                                         ●● ●
                                                                                                                                                                                                                         ● ●
                                                                                                                                                                                                                            ●     ●●
                                                                                                                                                                                                                                    ●
                                                                                                                                                                                                                                    ●●●
                                                                                                                                                                                                                                        ●●●
                                                                                                                                                                                                                                          ●
                                                                                                                                                                                                                                          ●
                                                                                                                                                                                                                                            ●
                                                                                                                                                                                                                                            ●
                                                                                                                                                                                                                                                  ●● ●
                                                                                                                                                                                                                                                ●● ●●●●
                                                                                                                                                                                                                                                         ●●●
                                                                                                                                                                                                                                                            ●
                                                                                                                                                                                                                  ●               ●     ●                   ●
                                                                                                                                                                                             ●●                   ●●                      ●     ●
estimate for sentence s based on the TCE is:                                                                                                                 ●●
                                                                                                                                                                     ●        ●●●
                                                                                                                                                                                                       ●
                                                                                                                                                                                                       ●
                                                                                                                                                                                                      ●●
                                                                                                                                                                                                                  ● ●
                                                                                                                                                                                                                ●●●●
                                                                                                                                                                                                                ●             ●●●
                                                                                                                                                                                                                                ●
                                                                                                                                                                                                                                  ● ● ●●●●●●●
                                                                                                                                                                                                                                  ●
                                                                                                                                                                                                                                      ●
                                                                                                                                                                                                                                      ●   ●
                                                                                                                                                                                                                                      ●● ● ●●●
                                                                                                                                                                                                                                                  ●    ●● ●
                                                                                                                                                                                                                                                       ●
                                                                                                                                                                                                                                                      ●●●●●●
                                                                                                                                                                              ●                                                             ●       ●
                                                                                                                                 12.2                        ●                                                                                                        ●
                                                                                                                                                         ●           ●        ●●        ●●       ●●   ●    ●●●●●                ●●
                                                                                                                                                                                                                               ●●                  ●           ●    ●●●
                                                                                                                                                                                                                                                           ●
    Iˆtce (s;t) = − log2 p(s|t) = − ∑ log2 p(w|tˆintended )   (8)                                                                            ●
                                                                                                                                                                                        ●
                                                                                                                                                                                                            ●
                                                                                                                                                                                                                                                       ●                     ●
                                   w∈s                                                                                           12.0
                                                                                                                                                     ●       ●           ●●                      ●    ●●●●          ●●                       ●●●       ●       ●
                                                                                                                                                         ●                                                                                    ●        ●
   This sentence information estimate represents the amount
                                                                                                                                                                                             ●
                                                                                                                                                 ●                                  ●
of information that the speaker has planned for the utterance.                                                                                           ●       ●                                                               ●
                                                                                                                                 11.8
Mixed Model Analysis To investigate how speakers orga-                                                                                  ●●           ●                    ●    ●                                                                                     ●
nize the content of a discourse, we conducted linear mixed                                                                              ●
model analyses.1 Mixed models are regression models that                                                                                1 3 5 7 9                        12     15          18       21   24    27       30     33   36      39    42          45   48
provide ways to account for potential clusters in the data,                                                                                                                                      Sentence position
which would otherwise lead to inflated Type I errors (Pinheiro
& Bates, 2000). The dependent variable is per-word sen-                 Figure 1: Boxplot shows the actual distribution of sentence
tence information as estimated by the topic conditional esti-           information against sentence position. Blue curve shows the
mator. The independent variables (i.e. predictors) are sen-             combined predicted effects of both predictors (random effects
tence position and topic shift. In addition to these fixed ef-          are not shown).
fects, we also included random intercepts for individual dif-
ferences between writers. As introduced above, we predict               for (flat prior: χ2 = 62.41, p < 0.0001; informative prior:
that topic shift will be negatively correlated with sentence in-        χ2 = 65.6, p < 0.0001).
formation, and such an effect may subsume the effect of sen-               Figure 1 summarizes the results graphically. Note that the
tence position.                                                         predictor topic shift allows our model to fit a high level of
   To determine the significance of independent variables, we           nonlinearities in the profile of sentence information. Previ-
compared models with and without a given variable in terms              ous studies have attempted to fit these nonlinear patterns with
of the χ2 distributed differences in deviance. We compared              log- or restricted cubic spline (RCS) transformed sentence
the model with two different models, one of which has only              position (Qian & Jaeger, 2010). A post-hoc test revealed that
sentence position as the independent variable, the other of             topic shift remained a significant predictor even after these
which only topic shift. If the model without sentence posi-             transformations were applied (for log-transformed sentence
tion was significantly different from the model with both in-           position: χ2 = 303, p < 0.0001; for RCS-transformed sen-
dependent variables, we could conclude sentence position is             tence position: χ2 = 320.47, p < 0.0001), indicating the non-
significant predictor. We would then look at its coefficient in         linear patterns captured by topic shift are additional to these
the regression model to determine if the effect was in the pre-         general nonlinear functions.
dicted direction. The same method also applies to the analysis          Discussion
of topic shift. We report χ2 -based p-values instead of those
associated with error-based t-values in parameter estimation.           The results of Study 1 suggest that speakers are sensitive
This approach avoids problems with inflated standard errors             to topic shift. When the content of an upcoming sentence
for collinear predictions.                                              leads to a significant change in the belief of likely topics, the
                                                                        speaker typically plans less information in that sentence. This
Results                                                                 shows that the speaker is aware of the fact that the predictive
The prediction that sentence information should be nega-                power of discourse context is not as powerful as it would have
tively correlated with topic shift is confirmed in the current          been if there were no topic shift.
study. We found that when the planned content of the up-                   The effect of sentence position can be interpreted in two
coming sentence shifts the prior belief distribution of top-            ways. One possibility is that topic shift is not a perfect corre-
ics by 1 bit, the content will be encoded with 0.43 bits of             late of discourse context. Thus, when topic shift is controlled
information per word less than the case where there is no               for, not all of the effect of sentence position is subsumed. An
topic shift (models with a flat prior for the belief distribu-          alternative view is that the topic conditional estimator of sen-
tion of topics: χ2 = 301.31, p < 0.0001; informative prior:             tence information do not reflect the actual measure of infor-
χ2 = 273.29, p < 0.0001). At the same time, the effect of               mation that speakers use to organize a discourse. In the next
sentence position is also significant. Speakers were found to           study, we used a different estimator to explore this question.
encode 0.001 more bits of information per word in each sub-
sequent sentence, after the effect of topic shift is controlled                                                                                                                              Study 2
   1 Following Qian and Jaeger (2010), we also conducted nonlinear      In Study 1, the topic conditional estimator calculates the in-
mixed model analysis, which yielded qualitatively similar results.      formation of a sentence by conditioning sentence content on
Those results are not reported here separately due to space limit.      the topic (that we as researchers assume to be) intended by the
                                                                     3316

speaker. Since the intended topic of a sentence can only be           effect of sentence position is no longer significant. The es-
known to the speaker, a listener must adopt a different strat-        timated slope is close to 0 and is non-significant (flat prior:
egy in estimating sentence information. One possible strategy         p > 0.5; informative prior: p > 0.6). Importantly, when topic
is to infer the topics of the discourse word by word, and pre-        shift is removed from the regression model, sentence position
serve the uncertainty in the inference process by performing          is a significant predictor on its own. Taken together, these re-
marginalization over all possible topics. We refer to it as a         sults showed that the effect of topic shift subsumes the effect
latent bigram estimator, the details of which are described           of sentence position.
below.
Methods
                                                                          Sentence information (Latent bigram estimator)
                                                                                                                           12.50
The latent bigram estimator of sentence information estimates
the information of a sentence s as follows:
                                                                                                                           12.48
      Iˆlbe (s) =   ∑ − log2 p(wi |wi−1 )                                                                                  12.46
                    wi ∈s
              =     ∑ ∑ − log2 p(wi |T = t)p(T = t|wi−1 )    (9)
                    wi ∈s t                                                                                                12.44
   The last step of Equation (9) shows that the probability of
a word wi is obtained by marginalizing over a full distribu-                                                               12.42
tion of latent topics, which is inferred from the context word                                                                     0.00   0.05   0.10       0.15       0.20     0.25   0.30
wi−1 . Thus, the latent bigram estimator is different from a tra-                                                                                Topic shift (KL distance)
ditional bigram estimator that examines only the correlations
between surface tokens. An illustration of the difference is          Figure 3: Sentence information (estimated by latent bigrams)
shown in Figure 2.                                                    is negatively correlated with topic shift. Scatterplot shows
                                                                      the actual distribution. Blue line shows the predicted effect.
                                                                      Random effects are not shown.
                                                                      Discussion
                                                                      Study 2 replicated the predicted negative correlation between
                                                                      topic shift and sentence information found in Study 1. With
            (a) Bigrams                (b) Latent bigrams
                                                                      sentence information estimates reflecting how much infor-
Figure 2: Regular bigram models vs. latent bigram models.             mation that the listener perceives, it shows that the speaker
                                                                      organizes discourse content in such a way that the listener
                                                                      would perceive less information when the discourse under-
  Inferring the posterior probability of a topic p(T = t|wi−1 )
                                                                      goes a topic shift.
requires a simple manipulation of the Bayes’ rule:
                               p(wi−1 |T = t)p(T = t)                                                                                                   TCE (Study 1)         LBE (Study 2)
           p(T = t|wi−1 ) =                                 (10)          Partial – Topic shift                                                               13.69%                10.32%
                                       p(wi−1 )
                                                                          Partial – Sent position                                                              2.89%                 0.01%
   Terms in Equations (9) and (10) were directly obtained                 Total R2                                                                            17.10%                21.44%
from the training results P(W |T ). The prior distribution of
topics P(T ) was computed using Equation (1). The probabil-
ity of the first word of a sentence is conditioned on the last                                                                 Table 1: Partial and total R2 of the both models.
word of the previous sentence. This sentence information es-
timates represents how much information that faces a rational            The fact that the effect of sentence position becomes non-
listener in a sentence.                                               significant when topic shift is controlled for implies that topic
                                                                      shift accounts for the variance that is originally predicted by
Results                                                               sentence position. This might be taken to mean that marginal-
We used the same statistical procedure as in Study 1. Topic           ization over topics is the more appropriate model of what
shift remained a highly significant predictor of unconditional        speakers try to hold constant during discourse production.
per-word sentence information (as estimated using latent bi-          This is, however, potentially misleading. While overall more
grams). On average, speakers encode 0.05 bits less infor-             variance is accounted for in Study 2 (Table 1), the proportion
mation in a sentence for every 1 bit of topic shift (model            accounted for by the topic shift predictor out of the overall
with a flat prior: χ2 = 252.77, p < 0.0001; informative prior:        variance is actually smaller in Study 2 (48.1%) than in Study
χ2 = 257.56, p < 0.0001; see Figure 3). Interestingly, the            1 (80.1%). In other words, the higher R2 of the model in
                                                                   3317

Study 2 is mostly due to more variance being captured by the               compatible with the hypothesis that speakers plan utterances
random intercepts (which adjust for individual differences be-             from the listeners’ perspective to achieve communicative ef-
tween writers). It is hence unclear which of the two models                ficiency.
presented in this paper is more appropriate.                                                        References
                                                                           Andrzejewski, D., Mulhern, A., Liblit, B., & Zhu, X. (2007).
                       General Discussion                                     Statistical debugging using latent topic models. In ECML
Unlike the test for a positive correlation between uncondi-                   (p. 6-17).
tional information and sentence position, the topic modeling               Aylett, M. P., & Turk, A. (2004). The smooth signal redun-
approach employed here directly tests whether unconditional                   dancy hypothesis: A functional explanation for relation-
sentence information is uniform once discourse context has                    ships between redundancy, prosodic prominence, and du-
been taken into account. It is thus a stronger test of the hy-                ration in spontaneous speech. Lang Speech, 47(1), 31-56.
pothesis that language production is communicative efficient               Bird, S., Loper, E., & Klein, E. (2009). Natural language
(Genzel & Charniak, 2002; Jaeger, 2010; Levy & Jaeger,                        processing with python. O’Reilly.
2007). The results of current studies showed a clear negative              Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirich-
correlation between sentence information (based on content                    let allocation. J Mach Learn Res, 3, 993-1022.
words only) and topic shift, no matter which estimator was                 Brown, P. M., & Dell, G. S. (1987). Adapting production to
used for sentence information. When sentence information is                   comprehension: The explicit mention of instruments. Cog-
estimated from the listener’s perspective, topic shift can even               nitive Psychology, 19, 441-472.
account for all the variances in sentence information that is              Clark, H. H., & Marshal, C. R. (1981). Elements of discourse
originally predicted by sentence position.2 These results sug-                understanding. In A. K. Joshi, B. L. Webber, & I. A. Sag
gest that speakers distribute less information in parts of a dis-             (Eds.), (chap. Definite reference and mutual knowledge).
course where discourse context is less relevant.                              Cambridge University Press.
   Study 2 suggested that the speaker may be optimizing dis-               Ferrer i Cancho, R., & Dı́az-Guilera, A. (2007). The global
course content for the ease of comprehension. If confirmed                    minima of the communicative energy of natural communi-
by further studies (e.g. on other languages) and under the                    cation systems. J Stat Mech-Theory E. (P06009)
assumption that the latent topic model employed Study 2                    Genzel, D., & Charniak, E. (2002). Entropy rate constancy
cannot be motivated by production-internal considerations                     in text. In ACL (pp. 199–206).
alone, this finding may be taken to speak to the question                  Genzel, D., & Charniak, E. (2003). Variation of entropy
as to whether production is adapted to comprehension (but                     and parse trees of sentences as a function of the sentence
see the caveats discussed above). Models of audience design                   number. In EMNLP (p. 65-72).
have emphasized the importance of a speaker taking their lis-              Griffiths, T., & Steyvers, M. (2004). Finding scientific topics.
tener’s knowledge and belief into consideration during pro-                   PNAS, 5228-5235.
duction (Clark & Marshal, 1981). There is also evidence that               Jaeger, T. F. (2010). Redundancy and reduction: Speakers
speakers do not adapt to a specific listener, but to the way                  manage syntactic information density. Cognitive Psychol,
how language is comprehended in general (Brown & Dell,                        61, 23-62.
1987). Our modeling results do not distinguish between these               Keller, F. (2004). The entropy rate principle as a predictor of
two views. To test a strong hypothesis of audience design, it                 processing effort: An evaluation against eye-tracking data.
would be necessary to first pick a maximum a posteriori esti-                 In EMNLP (p. 317-324).
mate of the listener’s perceived topic, like the speaker’s MAP             Levy, R., & Jaeger, T. F. (2007). Speakers optimize informa-
topic in Study 1, and then examine if sentence information                    tion density through syntactic reduction. In NIPS (p. 849-
estimates derived under such conditions follow the predicted                  856).
pattern. Our approach deviates from such a method since it                 Piantadosi, S., & Gibson, E. (2008). Uniform information
remains agnostic about what specific topics that the listener                 density in discourse: a cross-corpus analysis of syntactic
is committed to. We would like to pursue this direction with                  and lexical predictability. In CUNY.
behavioral methods in future work.                                         Pinheiro, J. C., & Bates, D. M. (2000). Mixed-effects models
                                                                              in S and S-PLUS. New York, NY: Springer-Verlag.
                            Conclusion                                     Qian, T., & Jaeger, T. F. (2009). Evidence for efficient lan-
We provide evidence that speakers adjust the amount of un-                    guage production in chinese. In CogSci09 (p. 851-856).
conditional information encoded in a sentence according to                 Qian, T., & Jaeger, T. F. (2010). Close = relevant? the role
topic shifts in a discourse. When sentence information was                    of context in efficient language production. In ACL 2010,
estimated from a rational listener’s perspective (Study 2), this              CMCL Workshop.
effect subsumes previously reported effects. This finding is               Qian, T., & Jaeger, T. F. (submitted). Cue weight decay in
    2 Results were compared with an ngram estimator. The effect
                                                                              communicatively efficient discourse production.
of topic shift holds. The effect of sentence position is only signifi-     Shannon, C. E. (1948). A mathematical theory of communi-
cant for the first 11 sentences. This closely matches previous studies        cations. Bell Labs Tech J, 27(4), 623-656.
where the effect tends to be reported only for early discourse.
                                                                       3318

