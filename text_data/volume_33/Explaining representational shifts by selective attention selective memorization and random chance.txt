UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Explaining representational shifts by selective attention, selective memorization, and
random chance
Permalink
https://escholarship.org/uc/item/2jx7p1r0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Matsuka, Toshihiko
Honda, HIdehito
Matsuura, Sou
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

  Explaining representational shifts by selective attention, selective memorization,
                                                       and random chance
                                    Toshihiko Matsuka (matsukat@muscat.L.chiba-u.ac.jp)
                                         Hidehito Honda (hito@muscat.L.chiba-u.ac.jp)
                                       Sou Matsuura (matsuura@muscat.L.chiba-u.ac.jp)
                                            Department of Cognitive and Information Science
                                                               Chiba University
                                                                Chiba, JAPAN
                              Abstract                                   this model was built on the basis of a stochastic optimiza-
                                                                         tion method, and it assumes that less-accurate simpler strate-
   Recent studies in category learning have shown that there are         gies (i.e., rule) are more likely to be realized in earlier stages
   shifts in category representation. In the present study, we de-
   velop three models categorization that consisted of different         of learning, while more complex strategies that bear higher
   learning objectives to examine cognitive mechanism underly-           classification accuracy (i.e., exemplar-like representation) are
   ing the representational shifts. The results of simulation indi-      more likely to be maintained and applied in latter stages of
   cated that the representational shift observed in Johansen &          learning.
   Palmeri (2002) can be explained by selective attention, se-
   lective exemplar memorization, or mere random chance. Al-
   though these models could not be differentiated based on clas-                          Computational Models
   sification generalization patterns, a detail examination of ac-       Overview
   quired model coefficients were conducted in order to design
   future studies.                                                       In the present paper, we used ALCOVE (Kruschke, 1992) as
                                                                         the model of categorization, and CLEAR framework (Mat-
                                                                         suka, Sakamoto, Chouchourelou & Nickerson, 2008) as the
                          Introduction                                   model of learning. CLEAR framework is a straightforward
Recent studies in category learning and usage have shown                 application of stochastic optimization method, namely Evolu-
that there are shifts in category representation. For exam-              tional Strategy to human learning models. We refer CLEAR-
ple, Johansen and Palmeri (2002) showed that participants’               augmented ALCOVE to as CaALCOVE. Three variants of
category generalization patterns in early learning stages were           CaAlCOVE were tested in the present study, namely standard
more consistent with rule-like representations, but as learn-            CaALCOVE, attention-penalizing CaALCOVE, and (exem-
ing progressed they exhibited generalization pattern that were           plar) memorization-penalizing CaALCOVE. The difference
more consistent with an exemplar-based representation. Jo-               among those models is their learning objectives and we test
hansen and Palmeri (2002) hypothesized that the selective at-            how the learning objectives affect acquisition of different
tention process was the key cognitive mechanism that pro-                types of internal representations.
duced the representational shifts. They developed and suc-
cessfully tested a model that initially paid attention to a single       Categorization Algorithm - ALCOVE
feature dimension and then gradually distributed its attention           In ALCOVE (Kruschke, 1992), categorization decision is
to achieve accurate classification. In contrast, Bourne and his          based on the activations of stored exemplars. As shown in
colleague (Bourne, Healy, Kole & Graham, 2006; Bourne,                   Eq. 1, each exemplar’s activation in ALCOVE, scaled by
Healy, Parker & Richard, 1999) suggested that representa-                specificity, β (which determines generalization gradient), is
tional shifts can occur both rule-to-exemplar and exemplar-              based on the inverse distance between an input, x, and a
to-rule fashion, depending on cognitive demands of tasks be-             stored exemplar, ψj , in multi-dimensional representational
ing performed. In particular, Bourne et al. (1999, 2006)                 space where each dimension (i) is scaled by non-negative se-
claimed that memorabilities of strategies plays an important             lective attention weights, ai . The exemplar activations are
role in representational or strategy shifts.                             then fed forward to the k-th output node (e.g., output for cate-
   In the present study, we developed three models of cate-              gory k), Ok , weighted by wkj , which determines the strength
gorization that differed in their learning objectives. The first         of association between exemplar j and output node k:
model was built on the basis of Johansen and Palmeri’s (2002)                                       [  (                           )]
idea that selective attention causes representational shifts. In             (n)
                                                                                        ∑J
                                                                                                (n)
                                                                                                               ∑I
                                                                                                                     (n)
particular, the model tries to acquire accurate categorization             Ok (x) =          wkj exp −β ·          ai |ψji − xi |       (1)
                                                                                        j=1                    i=1
strategies while paying attention to a small number of dimen-
sions. The second model was built on the basis of Bourne                 where superscript n indicates n-th categorization strategy be-
et al. (1999,2006) claim that the strategy memorability is               ing utilized. Given that CaALCOVE’s learning algorithm
the key process in representational shifts. This model tries             are based on stochastic optimization, dimensional attention
to acquire accurate strategies while memorizing and using a              weights takes the following form to obtain reasonable stabil-
smaller number of exemplars. The last model was somewhat                 ity:
different from previous two models in that it assumes that                                          (       (         ))−1
                                                                                            (n)                   (n)
representational shifts occur by random chance. Given that                                ai = 1 + exp −Di                              (2)
                                                                    3078

where Di is a pseudo-attention weight. In CaALCOVE, Ds                 egy with the highest predicted utility (e.g., accuracy, score,
(not as) are updated in learning.                                      etc.) to make one response at a time (e.g., categorize an in-
   The probability of categorizing input instance x to cate-           put instance). The functions for estimating the utility for each
gory C is based on the activation of output node C relative to         strategy is described in a later section.
the activations of all output nodes:
                                                                       Hypotheses Combinations In CaALCOVE, randomly se-
                                   (            )
                                          (ν)                          lected pairs of strategies exchange information to create a new
                             exp φ · Oc (x)
                                     (            ).                   strategy. For the sake of simplicity, we use the following nota-
               P (C|x) = ∑                                     (3)
                                            (ν)                        tion {w(n) , D(n) } ∈ θ (n) . CaALCOVE utilizes discrete re-
                              k exp φ · Ok (x)
                                                                       combination of coefficients and intermediary recombination
where φ controls decisiveness of the classification response,          of the coefficient for self-adaptation. Thus,
and the superscript ν indicates the strategy adopted to make a                                   {
                                                                                                    (p1)
categorization response.                                                                   (c)     θl      if UNI ≤ 0.5
   Although CaALCOVE would always have multiple strate-                                  θl    =    (p2)                               (4)
                                                                                                   θl      otherwise
gies in mind, it opts for and applies a single strategy with
the highest predicted utility, indicated by the superscript ν,
to make one response at a time (e.g., categorizing an input            where UNI is a random number drawn from the Uniform dis-
                                                                                                                   (c)             (p1)
instance).                                                             tribution. For self-adapting strategy, σi = 0.5 · (σi            +
                                                                         (p2)
   In the traditional ALCOVE model, a single strategy con-             σi ). This combination process continues until the number
sisting of attention (i.e., ai ) and association weights (i.e.,        of children strategies produced reaches the memory capacity
wkj ) is updated by a gradient descent method to minimize the          of CaALCOVE.
classification error. CaALCOVE optimizes multiple strate-              Hypotheses Modifications After the recombination pro-
gies on the basis of their utility using an evolutionary comput-       cess, CaALCOVE randomly modifies its strategies, using a
ing method. We now describe the algorithms for optimizing              self-adapting strategy. Thus,
the utilities of strategies.
                                                                                       (n)             (n)
Learning Algorithms - CLEAR                                                          σθl (t + 1) = σθl (t) · exp(N (0, γ))             (5)
Overview of Learning Algorithm In CLEAR framework
(see Matsuka, et al., 2008 for detailed discussion about its ef-                   (n)             (n)              (n)
                                                                                 θl (t + 1) = θl (t) + N (0, σθl (t + 1))              (6)
fectiveness and descriptive validity), Evolution Strategy (ES)
method was used as learning processes. As in a typical                 where t indicates time, l indicates coefficients, γ defines
ES application, we assumed three key processes in learn-               search width (via σ’s), and N (0, σ) is a random number
ing: crossover, mutation, and (survivor) selection. In the             drawn from the Normal distribution with the corresponding
crossover process, randomly selected categorization strate-            parameters.
gies are combined to form a new strategy. In human cog-
                                                                       Selection of Surviving Hypotheses After creating new sets
nition, the crossover process can be interpreted as concep-
                                                                       of strategies, CaALCOVE selects a limited number of strate-
tual combination in which new strategies are created based on
                                                                       gies to be maintained in its memory. In CaALCOVE, the
merging ideas from existing useful strategies. In the mutation
                                                                       survivor selection is done deterministically, selecting best 10
process, each model coefficient is randomly altered, which
                                                                       strategies on the basis of estimated utility of strategies or
can be interpreted as strategy modification by randomly ad-
                                                                       knowledge. The function defining utility of knowledge is de-
justing local attributes. In the selection process, a certain
                                                                       scribed in the next section.
number of strategies are deterministically selected on the ba-
sis of their ”usefulness.” in relation to the situational char-
                                                                       Knowledge Utility Estimation
acteristics. Those selected strategies will be kept in CaAL-
COVE’s memory trace, while non-selected strategies become              The utility of each strategy or a set of coefficients deter-
obsolete or are forgotten.                                             mines the survivor selection process in CaALCOVE, which
   Unlike previous modeling approaches to category learning            occurs twice. During categorization, it selects a single strat-
research, which modify a single strategy (i.e., a single set of        egy with the highest predicted utility to make a categoriza-
coefficients), CaALCOVE maintains, modifies, and combines              tion response (referred to as concept utility for response or
a set of strategies. The idea of having a population of strate-        UR hereafter). During learning, it selects best fit strategies to
gies (as opposed to having an individual strategy) is important        update its knowledge (utility for learning or UL hereafter). In
because it allows not only the selection and concept combi-            both selection processes, the strategy utility is subjectively
nation in learning, but also the creation of diverse strategies,       and contextually defined, and a general function is given
making learning more robust. Thus, unlike previous mod-                as: U (θ n ) = Υ (E(θ n ), Q1 (θ n ), ..., QL (θ n )) where Υ is a
els, CaALCOVE assumes that humans have the potential to                function that takes concept inaccuracy (i.e., E) and L contex-
maintain a range of strategies and are able to apply a strategy        tual factors (i.e., Q) and returns an estimated strategy utility
most suitable for a particular set of situational characteristics.     value (Note that learning is framed as a minimization prob-
In CaALCOVE framework, one individual could simultane-                 lem).
ously have multiple representation schemes.                               In CaALCOVE, the predicted (in)accuracy of a strategy
   Although CaALCOVE always has multiple strategies in                 during categorization is estimated based on a retrospective
its knowledge space, it opts for and applies a single strat-           verification function, which assumes that humans estimate
                                                                   3079

the accuracies of the strategies by applying the current strate-           Attention-penalizing CaALCOVE
gies to previously encountered instances retrieved from a
                                                                           Attention-penalizing CaALCOVE (CAL-AP, hereafter) as-
memory trace . Thus,
                                                                           sumes that strong selective attention causes the represen-
                                                                           tational shift. CAL-AP allocates most of its attention to
                                                                           a smaller number dimensions in earlier stages of learning,
               ∑G ∑  K     (           )[                 (      )]2       but it gradually allocates its attention to other dimensions
                             (g)             (g)      (n)
  E(θ (n) ) =            Ξ dk , x(g) dk − Ok                x(g)           to achieve more accurate categorization. That is, it penal-
               g=1 k=1
                                                                           izes distributed attention in earlier stages, but the penalization
                                                                   (7)     weakens as learning progresses. The underlying idea of CAL-
where superscript g indicates a particular input-output pair,              AP is basically the same as the model proposed by Johansen
G is the number of unique training pairs, and the exemplar                 & Parmeli (2002).
retention function Ξ returns the retrieval strength g-th input-
                                                                              The knowledge utility function for CAL-AP is given as fol-
output pair. The last term is the sum of squared error with d
                                                                           lows;:
being the desired output.
   By assuming category structures being deterministic, the                                                               (       )2
following exemplar retention function, based on Anderson                        (      )      (      )       ∑              ai
                                                                                                                              (n)
and Schooler’s learning-forgetting function (1991), is used in                U θ (n) = E θ (n) + λa              (      )2      I (       )2
                                                                                                                     (n)        ∑      (n)
the present simulation study. Thus,                                                                            i    ai      +         al
                                                                                                                                l=1
                                   ∑           (i)      −δ
                                                                                                                                              (9)
                                            (τ     + 1)                    This function encourages CAL-AP to pay attention to a
                              ∀i|x(i) =x(g)                                smaller number of feature dimensions, or it penalizes CAL-
          Ξ(d(g) , x(g) ) = ∑        ∑                             (8)
                                              (τ (i) + 1)−δ                AP when it selectively pays attentions to many dimensions.
                             g ∀i|x(i) =x(g)                               Note that the knowledge utility is estimated based on selective
                                                                           attention weight a, but not pseudo-selective attention weight
Memory decay parameter, δ, controls the speed of memory                    D. λa is a scalar that balances the trade-off between cate-
decay, and τ indicates how many instances were presented                   gorization accuracy and the attention penalization. The value
since x(g) appeared, with the current training being repre-                for λa decreases as learning progresses, like the annealing
sented with “0.” Thus, τ = 1 indicates x(g) appeared one                   function used in Johansen & Palmeri (2002).
instance before the current trial. The denominator in the ex-
emplar retaining function normalizes retention strengths, and              Memorization-penalizing CaALCOVE
thus it controls the relative effect of training exemplar, x(g) ,          Memorization-penalizing CaALCOVE (CAL-MP, hereafter)
in evaluating the accuracy of knowledge or strategies. E(θ)                assumes that selective memorization and usage of particular
is strongly influenced by more recently encountered training               exemplars causes the representational shift. That is, it as-
exemplars in early training trials, but it evenly accounts for             sumes that a smaller numbers of exemplars are memorized
various exemplars in later training trials, simultaneously ac-             and utilized in earlier stages of learning, causing CaALCOVE
counting for the Power Law of Forgetting and the Power Law                 to exhibit categorization pattern that is consistent with a rule-
of Learning (Anderson & Schooler, 1991; Newell & Rosen-                    like representation. As in CAL-AP, this model also weakens
bloom, 1981).                                                              its penalization weight as learning progresses.
                                                                              The knowledge utility function for CAL-AP is given as fol-
                           Simulation                                      lows;
Three variants of CaAlCOVE were tested in the present study,                                                              (        )2
namely standard CaALCOVE, attention-penalizing CaAL-                            (     )      (      )        ∑              wkj
                                                                                                                               (n)
COVE, and (exemplar) memorization-penalizing CaAL-                           U θ (n) = E θ (n) + λw               (      )2      I (         )2
                                                                                                                     (n)         ∑       (n)
COVE. The difference among those models is their learning                                                     kj    wkj     +         wkj
objectives (i.e., knowledge utility functions).                                                                                 l=1
                                                                                                                                             (10)
Standard CaALCOVE                                                          This function encourages CAL-MP to form a smaller set of
                                                                           active links (i.e., a link whose relative value is higher in
The learning objective function for the standard CaALCOVE                  its magnitude than other links) from exemplars and category
was given as Eq 7. This model assumes that the representa-                 nodes, or it penalizes CAL-MP when it associates categories
tional shifts during category learning occur by mere random                with many exemplars (in terms of the relative values). In
chance. That is, it assumes that simpler categorization strate-            other words, Eq 10 promotes CAL-MP to maintain a smaller
gies (i.e., rule-like representation) are more likely to be hy-            number of useful exemplars. Thus, when the memorization
pothesized and thus heavily utilized in earlier stages of learn-           penalization weight (λw ) is high, CAL-MP is more likely to
ing. But as learning progresses, more complex and accurate                 acquire a rule-link representation. In contrast, if its penal-
strategies based on exemplar-like representation will be re-               ization weight is small (e.g. knowledge accuracy outweighs
alized and tested, simply because creations of a larger num-               selective memorization of exemplars) then it would acquire
ber of hypotheses allow sufficient exploration of the solution             an exemplar-like representation. As in CAL-AP, the value for
space.                                                                     λw decreases as learning progresses.
                                                                       3080

Figure 1: Results of Simulation. Generalizations profiles for Observed data (upper left), standard CaALCOVE (upper right),
attention-penalizing CaALCOVE (lower left), and memorization-penalizing CaALCOVE (lower right). R1 Gen. indicates
generalization pattern that is consistent with the categorization rule based on Dimension 1, and R3 Gen based on Dimension 3.
Ex. Gen indicates generalization pattern that is consistent with exemplar usage.
Method                                                               Palmeri for detailed discussion about generalization patterns),
The basic simulation procedures followed those of the orig-          and R3 Generalization is based on Dimension 3 (i.e. respond-
inal study (Johansen & Palmeri, 2002). Table 1 shows                 ing BBABA to the above mentioned stimuli). Exemplar Gen-
schematic representation of stimulus set, which was adapted          eralization (Ex. Gen.) indicates a generalization pattern that
from Medin & Schaffer (1978). The models were run in a               is consistent with exemplar usage (i.e., responding ABBBA
simulated training procedure with 32 trial blocks, where each        to the stimuli). Note that as in the original simulation study,
block consisted of a random presentation of the nine unique          we calculated the proportion of R1 Generalization, for exam-
training exemplars (see Table 1) exactly once, in order to           ple, as the proportion of exact R1 Generalizations (AABBB)
learn the correct classification responses for the stimulus set.     and those that differed by one response.
After, 2nd, 4th, 8th, 16th, 24th, and the last training blocks,         In general, all three models were successful in replicat-
the transfer tests were conducted using testing exemplars (i.e.,     ing the observed phenomena. All three models tended to ex-
T1 - T7). As in the original study, the responses to only T1,        hibit the rule-based generalization patterns in earlier stages of
T2, T4, T5, and T6 were considered in the present simula-            learning and gradually shifted to the exemplar-based general-
tion. The model parameters were arbitrary selected: For all          ization pattern. The attention-penalizing CaALOVE (CAL-
three models β = 1.5, φ1 = 1. For standard CaALCOVE,                 AP) seemed more successful than other models in showing
δ=1, γ=1, while those for other models were 0.5 and 0.5, re-         increased exemplar usage. However, at the same time CAL-
spectively. Within each model, knowledge utility for learning        AP seemed least successful in not showing steady usages of
(UL) and knowledge utility for response (UR) were assumed            the rules (i.e., usage of the rules did not declined in the ob-
to be identical. There were a total of 100 simulated subjects        served data). It is rather intriguing that different learning ob-
for each model.                                                      jectives resulted in similar generalization patterns. The re-
                                                                     sults of the present simulation suggest that representational
Results                                                              shifts observed in Johansen & Palmeri (2002) may be ex-
                                                                     plained by selective attention (i.e. CAL-AP), selective mem-
Figure 1 shows observed and predicted generalization pro-            orization of exemplars (CAL-MP), or mere random chance
files. R1 Generalization (R1 Gen. in figure). indicates a            (CaALCOVE).
generalization pattern that is consistent with the categoriza-
tion rule based on Dimension 1 (i.e. responding AABBB                Discussion
to transfer stimuli T1, T2, T4, T5, and T6, see Johansen &
                                                                     Since the predicted generalization patterns alone could not
    1
      The value of φ does not affect model predictions               differentiate the three models, we examined how each of the
                                                                3081

                                                                     random chance. Although three models exhibited very simi-
Table 1: Schematic representation of stimulus set used in sim-       lar classification generalization patterns, their acquired inter-
ulations                                                             nal representations (via different learning objective functions)
          Training                              Transfer             were rather different. Since there is no empirical data to eval-
  Cat D1 D2 D3 D4 ID D1 D2 D3 D4                                     uate the models, we could not infer their descriptive validity.
   A       1     1     1      0    T1     1     0      0     1       However, the results of simulation can be helpful in designing
   A       1     0     1      0    T2     1     1      1     1       future empirical studies.
   A       1     0     1      1    T3     0     1      0     1
   A       1     1     0      1    T4     0     0      1     1                            Acknowledgment
   A       0     1     1      1    T5     1     0      0     0       This work was in part supported by the Japan Society for the
   B       1     1     0      0    T6     0     0      1     0       Promotion of Science KAKENHI (Grant No. 20700235) and
   B       0     1     1      0    T7     0     1      0     0       the Support Center for Advanced Telecommunications Tech-
   B       0     0     0      1                                      nology Research (SCAT).
   B       0     0     0      0
                                                                                              References
                                                                     Anderson, J. R. & Schooler, L. J. (1991). Reflections of the
                                                                        environment in memory. Psychological Science, 2, 396-
three models exhibited such generalization patterns by ana-             408.
lyzing acquired selective attention and exemplar-to-category
association weights.                                                 Bourne, L.E., Jr., Healy, A., Kole, J. A. & Graham, S. M.
   Figure 2 shows acquired selective attention weights after            (2006). Strategy shifts in classification skill acquisition:
2nd (left column), 16th (middle column), and the last training          Does memory retrieval dominate rule use?. Memory &
block (right column). The top row shows acquires attention              Cognition, 34, 903-913.
weights for standard CaALCOVE, the middle row for CAL-               Bourne, L. E., Jr., Healy, A. F., Parker, J. T., & Rickard, T. C.
AP, and the bottom row for CAL-MP. Although the three                   (1999). The strategic basis of performance in binary clas-
models exhibited similar generalization patterns, the patterns          sification tasks: Strategy choices and strategy transitions.
of selective attention distributions differed somewhat greatly.         Journal of Memory & Language, 41, 223-252.
   In early stages of learning, CAL-AP tended to allocate ex-        Hampton, J. A. (1987). Inheritance of attributes in natural
treme attention weights to a smaller number of dimensions               concept conjunctions. Memory & Cognition, 15, 55-71.
as its learning objective function suggested. CAL-MP, on the
other hand, tended to evenly allocate its selective attention        Johansen, M. K., & Palmeri, T. J. (2002). Are there repre-
weights, while CaALCOVE exhibited intermediate behav-                   sentational shifts during category learning? Cognitive Psy-
iors. These tendencies generally hold throughout the learning           chology, 45, 482-553.
processes.                                                           Kruschke, J. K. (1992). ALCOVE: An exemplar-based con-
   A similar trend was obtained for association weights.                nectionist model of category learning. Psychological Re-
CAL-MP tended to have a smaller number of active links                  view, 99, 22-44.
between exemplars and categories, while CAL-AP tended to             Matsuka, T. & Corter, J. E. (2008). Observed attention allo-
have a somewhat larger number of active links. CaALCOVE                 cation processes in category learning. Quarterly Journal of
showed an intermediate pattern.                                         Experimental Psychology, 61, 1067-1097.
   These analyses only confirm that the three models exhib-
ited similar generalization patterns with different internal rep-    Matsuka, T., Sakamoto, Y., Chouchourelou, A., & Nickerson,
resentation schema. We cannot infer which model(s) more                 J. V. (2008). Toward a descriptive cognitive model of hu-
accurately accounts for the representational shift, because             man learning. Neurocomputing, 71, 2446-2455.
there is no empirical data. However, these analyses are help-        Medin, D. L., & Schaffer, M. M. (1978). Context Theory
ful in designing future empirical studies. For example, data            of Classification Learning. Psychological Review 85, 207-
on selective attention allocation pattern (e.g. Matsuka &               238.
Corter, 2008) would allow us to evaluate the three models            Newell, A., & Rosenbloom, P. S., (1981). Mechanism of skill
examined in the present study, which in turn provides rich in-          acquisition and the law of practice. In J. R. Anderson (Ed.),
formation for understanding cognitive mechanism underlying              Cognitive skill and the their acquisition (pp 1-55). Hills-
representational shifts.                                                dale, NJ: Lawrence Erlbaum Associates.
                         Conclusions
Recent studies in category learning and usage have shown
that there are representational shifts during category learning
(Johansen & Palmeri, 2002, Bourne et al., 1999, 2006). In the
present study, we develop three models categorization that
consisted of three different learning objectives. The results
of simulation study indicated that the representational shift
observed in Johansen & Palmeri (2002) can be explained by
selective attention, selective exemplar memorization, or mere
                                                                 3082

Figure 2: Learned relative selective attention weights for standard CaALCOVE (top row), attention-penalizing CaALCOVE
(middle row), and memorization-penalizing CaALCOVE (bottom row). Left column shows learned attention weights at after
2nd training block, middle for 16th block, and right column for the last block.
                                                             3083

