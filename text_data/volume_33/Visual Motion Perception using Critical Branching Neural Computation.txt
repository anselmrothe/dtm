UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Visual Motion Perception using Critical Branching Neural Computation
Permalink
https://escholarship.org/uc/item/7542m3s0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Szary, Janelle K.
Kello, Christopher T.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

           Visual Motion Perception using Critical Branching Neural Computation
                                            Janelle K. Szary (jszary@ucmerced.edu)
                                         Christopher T. Kello (ckello@ucmerced.edu)
                        Cognitive and Information Science, 5200 North Lake Rd., Merced, CA 95343 USA
                              Abstract                                 are non-linearly separable (e.g. XOR and parity), simple
   Motion processing in visual systems is supported by various
                                                                       linear classification can be used to categorize inputs based
   subcortical and cortical microcircuits. However, all motion         on the liquid spike patterns they produce. Thus, simple
   processing requires a basic capacity to integrate and combine       learning algorithms can be used to interpret complex spiking
   information over time, as may be true for all microcircuits         dynamics for non-trivial functions and tasks. Moreover, the
   that support perceptual and cognitive functions. In the present     authors showed that functions of past inputs can be learned
   study, a generic microcircuit model is presented that self-         from current spike patterns because memory is an inherent
   tunes its recurrent spiking dynamics to its critical branching      property of recurrent dynamics that are not overly
   point. The model is shown to have generic memory capacity
   that can be tapped for the purpose of visual motion                 convergent or divergent.
   processing. These results suggest that critical branching              The efficacy and simplicity of reservoir computing makes
   neural networks may provide general bases for spiking               it an attractive generic mechanism for computational
   models of motion processing and other perceptual and                perception tasks. In fact, Verstraeten et al. (2005) used
   cognitive functions.                                                LSMs to classify spoken words, and Maass et al. (2002a)
   Keywords: Critical branching; reservoir computing; leaky            used them to classify the shapes and velocities of visual
   integrate-and-fire neural networks; motion perception.              objects in motion. Burgsteiner et al. (2006) used real-time
                                                                       video footage from the RoboCup Soccer challenge to show
                          Introduction                                 that an LSM could be used to predict the future location of a
Communication in neural networks largely occurs via                    soccer ball in motion.
thresholded spiking signals between neurons, which are
                                                                       Visual Motion Perception
connected by characteristically recurrent loops varying in
spatial and temporal scale (Buzsáki, 2006). This                       Biological motion perception starts with motion detectors in
connectivity structure produces patterns of network activity           the retina, where ganglion cells just two or three synapses
that are continually in flux, and in this sense network                away from rods and cones detect motion in preferred
dynamics cannot be characterized by simple point or limit              directions (Vaney et al., 2000). Behaviorally, sensitivity to
cycle attractors. This limits the extent to which attractor-           the direction of motion is demonstrated in motion coherence
based neural network models, common in cognitive science               studies (reviewed in Braddick, 1997), where even minute
(e.g. connectionist recurrent networks), can relate the                asymmetries in the percentage of motion in one or another
brain’s spiking dynamics with cognitive functions.                     direction can be detected. Information about the direction of
   In order to investigate how computational functions might           motion is used for navigation, such as when coherent
be based in non-attractor network dynamics, Maass et al.               motion across the visual field constitutes optic flow, and for
(2002b) and Jaeger (2001) developed the concept of                     the detection of objects in motion, such as when local
reservoir computing. The basic idea is that recurrent                  motion differs from global motion (Srinivasan, 2000).
networks can produce dynamics with a generic capacity for                 Tracking objects is another important function of motion
computation, and so-called “read-out” networks can learn to            perception, i.e. the ability to predict future locations of
interpret reservoir dynamics for task-specific purposes.               objects in motion. This kind of prediction is hypothesized to
   The present work is focused on Maass et al.’s (2002b)               be necessary for accurate smooth-pursuit eye movements
liquid state machine (LSM) framework, which consists of a              (Carpenter, 1988), and it enables organisms to either avoid
pool of leaky integrate-and-fire (LIF) units with random               collisions with moving objects, or coincide with them as in
recurrent connectivity that is roughly constrained to be local         catching or hitting a baseball. For instance, McBeath (1990)
and sparse. Time-varying input signals are fed to the                  showed that the motor action of aiming and swinging a bat
network to drive and alter the “liquid” spiking dynamics.              must be implemented when the ball is just over halfway to
Due to recurrence, thresholding, and other non-linearities,            the plate, using a prediction of where the ball will be.
the resulting spatial and temporal spike patterns are                  Similarly, studies of cricket batters’ eye movements show
complex, unknown, and unlearned functions of their inputs.             predictive eye fixations on where the ball will be when it
   Nonetheless, these spike patterns will contain information          crosses the mound, rather than where it currently is (Land &
about past inputs to the extent that their dynamics are not            McLeod, 2000).
overly convergent or overly divergent (i.e. with Lyapunov                 The present work focuses on both of these functions of
exponents near one; Bertschinger & Natschläger, 2004).                 motion processing: direction classification and future
Maass et al. (2002b) found that even when the input patterns           location prediction. Reservoir computing techniques are
                                                                       particularly well-suited for motion perception tasks, as both
                                                                   1691

are inherently dynamical in nature. That is, microcircuits             and top/bottom edges wrap around). Inputs created and
involved in motion processing must support a wide range of             perturbed ongoing spiking dynamics in a pool of LIF
visual functions in the face of constantly changing inputs,            reservoir neurons. A group of perceptron units was used to
and must be sensitive to the time course of these inputs.              “read out” spiking dynamics for two different functions of
                                                                       motion processing: Direction classification and future
Critical Branching Applied to Motion Processing                        location prediction. These functions are similar, except that
In the present study, motion processing tasks are used to              direction classification requires generalization across
investigate the efficacy of a generic circuit implemented              position (and has a higher chance rate of performance),
using the reservoir computing framework. The model builds              whereas future location prediction is position-specific (and
on previous work by Maass et al. (2002a) and Burgsteiner et            has a lower chance rate of performance).
al. (2006) who developed LSMs for predicting future                       In Simulation 1, we test whether critical branching
locations of objects in motion. Here we replicate and extend           spiking dynamics have the kind of memory capacity capable
their work by 1) developing a new model for tuning                     of simultaneously supporting these two motion processing
reservoir dynamics of LIF units, and 2) applying the model             functions, using simple straight-line motions that require
to a wider range of tests that more thoroughly examine its             only modest memory capacity for accurate performance. In
capacity for motion processing.                                        Simulations 2 and 3, we employ more complex zig-zag and
   The present model is based on Kello and Mayberry’s                  spiral motions that require greater memory capacity.
(2010) work on critical branching neural computation (see
also Kello & Kerster, 2011; Kello et al., 2011). The basic                          Reservoir Computing Model
idea is that the computational capacity of reservoir spiking           Our model was based on the LSM framework, which
dynamics should be enhanced when near their critical                   consists of three layers: input units, reservoir units (tuned to
branching point. Any spiking neural network can be viewed              critical branching, hereafter referred to as the “CB layer”),
as a branching process whereby a spike occurring at time t             and readout units (Figure 1). The input layer was a 12x12
may subsequently “branch” into some number of spikes at                grid, and each unit projected a synaptic connection onto
time t+Δt over the neurons connected via its axonal                    each of the reservoir units with probability 0.5. There were
synapses. Let us call the former an “ancestor” presynaptic             400 LIF units in the CB layer, and each one projected
spike, and the latter “descendant” postsynaptic spikes. The            recurrently onto each other with probability 0.5. There were
number of descendants divided by ancestors is the                      four direction classification readout units, and twenty-four
branching ratio of a spiking network: σ=Npost/Npre. If σ<1,            location prediction readout units (representing two 12-unit
spikes diminish over time and information transmission                 axis coordinates). Each unit in the CB layer projected onto
through the network is inhibited by dampened propagation               every readout unit. Input patterns were diamonds with sides
of spiking signals. If σ>1, spikes increase over time and              approximately 7-8 units in length.
eventually saturate the network, which also inhibits
information transmission. σ=1 is the critical branching point
at which spikes are conserved over time, and so propagate
without dying out or running rampant. A related concept of
“homeostatic synaptic scaling” is being investigated in
cortical circuits, and refers to the idea that these circuits also
adjust themselves to achieve a balanced level of overall
activity in terms of mean firing rate (for a review, see
Turrigiano, 2008).
   Computational models verify the information processing              Figure 1: Model architecture. Gray squares show example
advantage of a network with σ=1. For example, an                       spike patterns, and blue lines represent synaptic connectivity
analogous critical point between convergent and divergent
dynamics (i.e Lyapunov exponents near one) has been                    Model Variables and Update Equations. LIF units
shown to maximize memory capacity in a recurrent network               generally have the following variables (Roman letters) and
of threshold gating units (Bertschinger & Natschläger,                 parameters (Greek letters): A membrane potential Vi for
2004). Here we formulate a reservoir computing model                   each neuron i, a membrane threshold θi and membrane leak
using LIF spiking neurons that are more biologically                   λi, and a level of potentiation wj for each axonal synapse j,
realistic compared with threshold gate neurons. Our model              where wj>=0 for excitatory neurons and wj<=0 for inhibitory
includes a simple algorithm that probabilistically potentiates         neurons. Models may also include variable synaptic delays
and de-potentiates synapses so that reservoir dynamics                 τj, as well as parameters governing the time course of action
approach their critical branching point.                               potentials and postsynaptic potentials (e.g. membrane
   We presented inputs to the model that come from a                   resistance).
simple, abstracted visual field. The visual field consisted of            Our model included all of the above, except that action
nothing other than a diamond-shaped moving object on a                 potentials and postsynaptic potentials were instantaneous for
12x12 grid with periodic boundary conditions (i.e. left/right          the sake of simplicity. We sought to enhance the
                                                                   1692

biologically plausibility of the model with 1) variable                When a given neuron spikes, its local estimate of σ is
updates that were local in time and local with respect to           reset, Npost,i 0. For each axonal synapse’s first spike
immediately connected synapses and neurons (numerical               occurring at time t, Npost,i was incremented by              .
values were not transmitted over connections among                  For each increment, each descendant spike was weighted as
neurons, as they are in e.g. backpropagation), and 2)               a decaying function of the time interval between pre- and
synaptic and neuronal updates were asynchronous and                 postsynaptic spikes, with maximal weighting when the
event-based. The latter criterion helped further the                former was immediately followed by the latter.
plausibility of our critical branching tuning algorithm.               The sum of time-weighted descendants is used (before it
   Each update event in the model begins when a given               is reset to zero) each time the neuron spikes to update
neuron receives as input a postsynaptic potential Ij at time t,     weights on its axonal synapses. In particular, if Npost,i<1,
which may either come from another neuron within the CB             then the update wj φj is performed for each synapse j with
layer, or from the input layer:                                     probability
                                               ,             [1]                                                ,              [2]
where        denotes the instantaneous update of a variable,        where η is a global tuning rate parameter (fixed at 0.1), and
                                                                    U is the number of synapses available for potentiation.
and is the previous time that Vi was updated. Thus, the
                                                                    f(si)                 if neuron i was excitatory, and
model included continuous exponential leak, applied each
                                                                    f(si)            if inhibitory. If Npost,i>1, then perform the
time a given neuron received an input. Immediately after
                                                                    update wj 0 with probability set according to Eq 2, except
each Vi update, if Vi>θi, then Vi 0, and a postsynaptic
                                                                    U is the number of synapses available for de-potentiation,
potential Ij was generated for each axonal synapse of i. Each
                                                                    and the assignment of f(si) is switched for excitatory versus
Ij=wj, and was applied at time t+τj.
                                                                    inhibitory neurons.
   In a typical connectionist model, wj can be any real-
                                                                       In essence, the critical branching algorithm potentiates
valued number, possibly bounded by some minima and
                                                                    synapses when too few descendant spikes occur, and de-
maxima. However, neurophysiological evidence indicates              potentiates when too many occur. Spikes are time-weighted
that synapses may only have a limited, discrete number of           because effects of ancestor spikes on descendant neurons
levels of potentiation, possibly just two (Petersen et al.,         diminish according to their leak rates. Critical branching
1998). Therefore we used binary-valued synapses in order to         weight updates increase in likelihood as local branching
limit the number of potentiated synapses (wj≠0), and to             ratio estimates diverge from one, and depend on spike
enable a stochastic tuning algorithm. In particular, each           timing. With regard to spike timing, excitatory synapses are
synapse had two possible strengths i.e. levels of                   more likely to be (de)potentiated when postsynaptic neurons
potentiation, 0 or φj. Each LIF model neuron has two free           have (not) fired recently, which helps to spread spikes
parameters, λi and θi, and each synapse has two free                across neurons. The same principle leads to the opposite
parameters, τj and φj. Values for all four free parameters          rule for inhibitory neurons.
were sampled randomly from uniform distributions whose
ranges were set to reasonable default values. In particular,        Readout Layer. Readout units were not spiking units.
values were real numbers in the ranges 1 < θi < 2, 0.5 < λi <       Instead, the normalized exponential function (i.e. softmax)
1, 1 < τj < 1.5, 1 < φj < 2 for excitatory units, and 0.1 < φj <    was used to compute their outputs from their summed
1 for inhibitory units. The decision to set φj higher for           inputs. The readout layer consisted of three normalized
excitatory units was driven by performance considerations,          groups of units: 4 direction classification units (up, down,
rather than neurophysiological data.                                left, and right), 12 X-coordinate position units, and 12 Y-
   The set of membrane potentials V and postsynaptic                coordinate position units. Unlike synapses projecting into
potentials I comprise the dynamics of neurons in our LIF            reservoir units, connections into readout units were real-
model. These variables are governed by event-based updates          valued and initialized in the range [-0.1, 0.1].
(Eq 1, plus threshold dynamics) that may occur                         For each unit time interval of simulation, reservoir spikes
asynchronously across neurons (at any point in continuous           resulted in normalized output activations over readout units.
time, simulated with arbitrary precision). The set of synaptic      During training, readout units received 1-of-N targets for
weights w comprise the dynamics of synapses, and are                each of the three normalized readout groups. Targets were
governed by the critical branching algorithm described next.        compared with outputs using the divergence error function,
                                                                    and the resulting error signal was used to update connection
Self-Tuning Algorithm. The objective of the self-tuning             weights with the delta learning rule (using momentum = 0.5,
algorithm is to potentiate and de-potentiate synapses so that       learning rate = 0.00001). At testing, the maximally active
each ancestor spike is followed by one descendant spike on          readout unit in each group was compared with the targeted
average. A local estimate for σ is computed over the                output to assess model accuracy (both X- and Y-coordinate
interspike interval (ISI) for each model neuron i. This means       units had to match their targets for location prediction to be
that only Npost,i need be estimated, because Npre,i=1 by            considered accurate). Location prediction was always one
definition, with respect to a given neuron’s ISI. Thus, to          unit time interval into the future.
achieve critical branching, Npost,i should sum to one.
                                                                1693

   Only weights on connections into readout units were              begun to perturb ongoing spiking activity. Performance on
trained. Levels of synaptic potentiation on connections into        both tasks then gradually ramps up to asymptotic
reservoir units were set by the critical branching tuning           performance. This ramp-up shows that spiking dynamics,
(given the diamond input patterns), and were not effected by        while not attractor-based, increasingly encode motion
readout units’ error signals. Thus, task-specific performance       information as it consistently accumulates over successive
of readout units was based on generic, task-neutral spiking         inputs. The ramp-up in both results shows that the same,
dynamics tuned near their critical branching point.                 generic reservoir dynamics can be simultaneously tapped to
                                                                    compute different functions of motion.
                         Simulations
Each simulation consisted of a set of tuning trials, and a set
of trials used for both training and testing. Each trial began
with the diamond-shaped input pattern initialized at a
random location on the input grid (with periodic boundary
conditions, as shown in Figure 2). Each trial proceeded for
20 time intervals over which the diamond was moved for 20
increments, each one corresponding to approximately one
unit on the input grid. At each increment, input units
corresponding to the position of the diamond were induced
to spike, and other input units did not spike for that time
interval.
                                                                       Figure 3: Accuracy on direction and location prediction
                                                                                          tasks in Simulation 1
                                                                       Figure 3 also shows that direction classification was more
                                                                    difficult than location prediction. Given that both functions
                                                                    essentially coded the same information (i.e. 1-of-4 different
                                                                    directions and 1-of-4 future locations given the current
                                                                    location), we can conclude that this difference arises in part
                                                                    because the direction task requires generalization over
    Figure 2: Diamond-shaped input pattern with periodic            position. Additionally, fewer weights encoded the direction
   boundary conditions shown in gray. Arrows demonstrate            function (4 versus 24 readout units). Further work is needed
   straight-line (Simulation 1; blue), zig-zag (Simulation 2;       to pull these two factors apart.
    red), and spiral (Simulation 3; orange) motion patterns
                                                                    Simulation 2: Zig-Zag Motion
   Reservoir dynamics were not reset after each trial; the
                                                                    The straight-line motion task in Simulation 1 required
model ran continuously as spikes were input to the network
                                                                    minimal integration of past information to achieve accurate
trial after trial. There were a total of 1,000 tuning trials,
                                                                    performance. That is, it was sufficient to “remember” where
followed by 1,000 training and testing trials. The critical
                                                                    the diamond was for any two or more of the previous time
branching tuning algorithm was engaged only during tuning
                                                                    intervals. However, motion processing can be more
trials, and readout units were engaged only during training
                                                                    difficult, in that information about the sequence of several
and testing trials. Each simulation was run five times (with
                                                                    past inputs may be required for accurate performance. In
parameters initialized anew each time), and mean
                                                                    Simulation 2, a zig-zag motion pattern was used as a more
performance over the five runs is presented. Reservoir units
                                                                    stringent test of the memory capacity of reservoir dynamics.
successfully approached their critical branching point by the
                                                                       Each zig-zag trial began with the diamond-shaped input
end of tuning (mean estimated branching ratio of .88; for
                                                                    pattern initialized in a random start location. Motion again
related results on critical branching, see Kello et al., 2011).
                                                                    began in one of four randomly chosen directions (up, down,
Simulation 1: Straight-Line Motion                                  left, right), but here the direction of motion alternated by
                                                                    right angles at regular intervals to create a zig-zag pattern.
For Simulation 1, movement of the diamond-shaped input              The direction of the initial 90˚ turn was chosen randomly at
pattern on each trial was in a straight line in one of four         the onset of each trial, and motion alternated between this
randomly chosen directions (up, down, left, right).                 and the original direction for the duration of the trial. The
                                                                    simulation was performed with inter-turn intervals (zig-zag
Results. Readout performance as a function of trial time            lengths) ranging from 2 to 5.
interval is shown in Figure 3. At the beginning of each trial,         In order for readout units to achieve accurate
results from both direction and location tasks are near             performance, reservoir dynamics must now encode more
chance performance because input spikes have not yet                specific information about inputs in the past. In particular,
                                                                1694

reservoir dynamics must encode the time and direction of           predicting changes in direction was difficult because it
the previous zig-zag change, which may have occurred               required greater memory capacity. As motion continued
several time intervals in the past.                                linearly between each turn, performance gradually rose until
                                                                   it fell again at the next turn.
                                                                      The most important result was the overall increase in
                                                                   performance, which is best seen in zig-zag lengths 2 and 3.
                                                                   This trend indicates that reservoir dynamics accumulated
                                                                   information about the zig-zag pattern itself as inputs came in
                                                                   over time, and were more capable of anticipating the turns.
                                                                   This trend is rather small for length 4, and mostly absent
                                                                   from length 5. These results show limitations in the
                                                                   reservoir’s memory capacity as the length of time increased
                                                                   between turns that must be remembered. This generic
                                                                   memory capacity increases with the size of the reservoir, so
                                                                   these limitations are not absolute. It is an open question
                                                                   whether there may be other means of increasing memory
                                                                   capacity, either generically or for task-specific purposes.
                                                                   Finally, one can also see from Figure 4 that the relative
                                                                   performance of the two tasks differs from that observed in
                                                                   Simulation 1, especially with shorter zig-zag lengths. This
                                                                   result requires further investigation, but it presumably has to
                                                                   do with the position-general nature of direction coding,
                                                                   versus position-specific nature of location prediction.
                                                                   Simulation 3: Spiral Motion
                                                                   The zig-zag motions in Simulation 2 minimally required
                                                                   memory about only the last turn. In Simulation 3, an
                                                                   expanding spiral motion pattern was used to test whether
                                                                   reservoir dynamics can encode memory about all prior
                                                                   turns. On each trial, motion again began in one of four
                                                                   randomly chosen directions, and turns were either 90
                                                                   degrees clockwise or counter-clockwise, also chosen
                                                                   randomly. The sides of the spiral expanded over time in the
                                                                   sequence 1, 1, 2, 2, 3, 3, 4, and 4 (for a sum of 20 intervals;
                                                                   see Figure 2 for motion depiction). Simulation methods
                                                                   were otherwise the same.
 Figure 4. Accuracy for direction and location prediction in
    Simulation 2 (dashed lines show changes in direction)
Results. Results are shown in Figure 4. Performance in
                                                                     Figure 5. Accuracy on direction and location prediction in
Simulation 2 was worse overall than in Simulation 1. This
                                                                       Simulation 3 (dashed lines show changes in direction)
difference reflects the need for weights into readout units to
extract more conjunctive information about past inputs.
                                                                   Results. Results are shown in Figure 5. As expected, overall
Notably, performance followed a zig-zag pattern aligned
                                                                   performance on the spiral motion simulation was even
with the zig-zag of motion (direction changes shown by
                                                                   worse than on the zig-zag motion, and performance dipped
dotted lines). This pattern indicates that, as expected,
                                                                   at each turn. However, performance once again showed a
                                                               1695

slight upward trend over the entire trial interval, indicating      148, GMD – German National Research Institute for
that reservoir dynamics contained information about more            Computer Science.
than just one previous change in direction.                       Kello, C. T., & Kerster, B. (2011). Power laws, memory
                                                                    capacity, and self-tuned critical branching in an LIF
                        Conclusion                                  model with binary synapses. Proceedings of
The present results are a proof-of-concept that generic,            Computational and Systems Neuroscience 2011. Salt
recurrent spiking dynamics have memory for visual inputs            Lake City, UT.
that can be tapped for motion processing tasks. Spike             Kello, C. T., Kerster, B., & Johnson, E. (2011). Critical
dynamics were tuned to their critical branching point, which        branching neural computation, neural avalanches, and 1/f
has been shown to enhance the memory and computational              scaling. To appear in Proceedings of the 33rd Annual
capacity of reservoir spiking networks (Kello & Mayberry,           Cognitive Science Society Conference. Boston, MA.
2010). Furthermore, recordings from cortical slice                Kello, C. T., & Mayberry, M. (2010). Critical branching
preparations and EEG have provided evidence of critical             neural computation. IEEE World Congress on
branching neural activity (Poil et al., 2008). Thus, critical       Computational Intelligence (pp. 1475-1481). Barcelona,
branching may be a basic principle utilized by motion               Spain.
processing microcircuits, as well as neural networks in           Land, M. F., & McLeod, P. (2000). From eye movements to
general. Model results did not reach the level of accuracy          actions: How batsmen hit the ball. Nature Neuroscience,
necessary for simulating human motion processing                    3, 1340-1345.
capabilities, but the pattern of results showed that critical     Li, N., & DiCarlo, J. (2008). Unsupervised natural
branching spiking dynamics are capable of the kinds of              experience rapidly alters invariant object representations
motion processing demonstrated in human behavior.                   in visual cortex. Science, 321, 1502-1507.
   Behavioral research has shown that an object’s motion is       Maass, W., Legenstein, R., & Markram, H. (2002a). A new
an important cue for object recognition (Newell et al.,             approach towards vision suggested by biologically
2004), and researchers are currently investigating the idea         realistic neural microcircuit models. Lecture Notes in
that the temporal contiguity of an object’s image on the            Computer Science, 2525/2002, 282-293.
retina contributes to the development of invariant object         Maass, W., Natschläger, T., & Markram, H. (2002b). Real-
recognition (Li & DiCarlo, 2008). Because the recurrent             time computing without stable states: A new framework
dynamics of the model described here are shown to be                for neural computation based on perturbations. Neural
capable of integrating information from multiple time steps,        Computation, 14(11), 2531-2560.
future work will investigate whether this memory can be           McBeath, M. K. (1990). The rising fastball: Baseball’s
tapped to form invariant object representations.                    impossible pitch. Perception, 19, 545-552.
                                                                  Newell, F. N., Wallraven, C., & Huber, S. (2004). The role
                   Acknowledgements                                 of characteristic motion in object categorization. Journal
                                                                    of Vision, 4(2), 118-129.
This work was funded by a DARPA contract to IBM, and a            Petersen, C. C. H., Malenka, R. C., Nicoll, R. A., &
University of California, Merced, Faculty Mentor Program            Hopfield, J. J. (1998). All-or-none potentiation at CA3-
Fellowship award. The authors would like to thank                   CA1 synapses. Proceedgins of the National Academy of
reviewers for helpful comments and suggestions.                     Sciences of the United States of America, 95, 4732-4737.
                                                                  Poil, S.-S., van Ooyen, A., & Linkenkaer-Hansen, K.
                        References                                  (2008). Avalanche dynamics of human brain oscillations:
Bertschinger, N., & Natschläger, T. (2004). Real-time               Relation to critical branching processes and temporal
   computation at the edge of chaos in recurrent neural             correlations. Human Brain Mapping, 29, 770-777.
   networks. Neural Computation, 16(7), 1413-1436.                Srinivasan, M. V. (2000). Visual navigation: The eyes know
Braddick, O. (1997). Local and global representations of            where their owner is going. In J. M. Zanker & J. Zeil
   velocity: Transparency, opponency, and global direction          (Eds.), Motion vision: Computational, neural, and
   perception. Perception, 26, 995-1010.                            ecological constraints. Berlin: Springer.
Burgsteiner, H., Kröll, M., Leopold, A., & Steinbauer, G.         Turrigiano, G. G. (2008). The self-tuning neuron: Synaptic
   (2006). Movement prediction from real-world images               Scaling of Excitatory Synapses. Cell, 135, 422-435.
   using a liquid state machine. Applied Intelligence, 26(2),     Vaney, D. I., He, S., Taylor, W. R., Levick, W. R. (2000).
   99-109.                                                          Direction-selective ganglion cells in the retina. In J. M.
Buzsáki, G. (2006). Rhythms of the Brain. New York:                 Zanker & J. Zeil (Eds.), Motion vision: Computational,
   Oxford University Press.                                         neural, and ecological constraints. Berlin: Springer.
Carpenter, R. H. S. (1988). Movements of the eyes, 2nd ed.        Verstraeten, D., Schrauwen, B., Stroobandt, D., & Van
   London: Pion Press.                                              Campenhout J. (2005). Isolated word recognition with the
Jaeger, H. (2001). The “echo state” approach to analyzing           Liquid State Machine: A case study. Information
   and training recurrent neural networks. GMD Report               Processing Letters, 95, 521-528.
                                                              1696

