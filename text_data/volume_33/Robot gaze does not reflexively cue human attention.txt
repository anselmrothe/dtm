UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Robot gaze does not reflexively cue human attention

Permalink
https://escholarship.org/uc/item/3pq1v9b0

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)

Authors
Admoni, Henny
Bank, Caroline
Tan, Joshua
et al.

Publication Date
2011-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Robot gaze does not reflexively cue human attention
Henny Admoni, Caroline Bank, Joshua Tan, Mariya Toneva and Brian Scassellati
Department of Computer Science, Yale University
51 Prospect Street, New Haven, CT 06511 USA
henny@cs.yale.edu, caroline.bank@yale.edu, joshua.tan@aya.yale.edu,
mariya.toneva@yale.edu, scaz@cs.yale.edu
Abstract
Joint visual attention is a critical aspect of typical human interactions. Psychophysics experiments indicate that people exhibit strong reflexive attention shifts in the direction of another
person’s gaze, but not in the direction of non-social cues such
as arrows. In this experiment, we ask whether robot gaze elicits
the same reflexive cueing effect as human gaze. We consider
two robots, Zeno and Keepon, to establish whether differences
in cueing depend on level of robot anthropomorphism. Using psychophysics methods for measuring attention by analyzing time to identification of a visual probe, we compare attention shifts elicited by five directional stimuli: a photograph of
a human face, a line drawing of a human face, Zeno’s gaze,
Keepon’s gaze and an arrow. Results indicate that all stimuli
convey directional information, but that robots fail to elicit attentional cueing effects that are evoked by non-robot stimuli,
regardless of robot anthropomorphism.

Introduction
Joint visual attention is an important aspect of typical social interactions. A single gaze communicates information—
there are predators hiding behind that tree; a tasty source
of food is over there; you are crossing into my territory—
and supports social conventions such as conversational turntaking and joint referencing. As robots become more integrated into daily human life, social interactions occur with
increasing frequency between humans and robots, as well:
robots assist nurses in hospitals, act as companions for the
elderly, and interact with children in schools and in therapy.
In this paper, we investigate whether people are responsive to
joint attention cues from robots. We focus on one aspect of
joint attention: recognizing the direction of another person’s
gaze and shifting one’s own attention in that direction.
Evidence from psychophysics suggests that typical humans
readily shift their attention in response to a directional cue,
such as averted eyes or an arrow. In traditional non-predictive
cueing experiments, participants view a centrally-presented
stimulus followed by a peripherally-presented visual probe,
and press a keyboard key in response to the probe. Key
press response times are theoretically correlated with attention: participants will respond more quickly to probes located
in the direction to which they are already attending. Studies
have found that when the stimulus contains directional information (such as a face with averted eyes, or an arrow pointing in one direction), people respond more quickly to probes
at cued locations, in which the probe is on the same side as
indicated by the stimulus, than to probes at uncued locations,
even when they are told that the cue does not indicate probe
location and should be ignored (Downing, Dodds, & Bray,
2004; Driver et al., 1999; Friesen & Kingstone, 1998; Langton & Bruce, 1999). Attention shifting via directional cue

seems to be an early and reflexive skill for humans: children
as young as three months old attend more quickly to a peripheral probe on cued trials than on uncued trials when the cue
is a human face (Hood, Willen, & Driver, 1998).
When cues are counterpredictive of probe location, however, social stimuli such as faces and eyes elicit different patterns of behavior than other directional stimuli. In counterpredictive cueing paradigms, probes appear with significantly
higher probability opposite the location cued by centrallylocated stimuli (Driver et al., 1999). For example, when the
centrally-located stimulus is directed toward the left, probes
have a 75% chance of appearing to the right of center, and
vice versa. In counterpredictive experiments, it is beneficial
for participants to orient attention away from the cued direction; therefore, shorter response times to probes in cued
directions are attributed to reflexive or uncontrollable attention shifts. In contrast, shorter response times to probes
in uncued (but predicted) locations are interpreted as volitional orienting of attention. Counterpredictive experiments
reveal that participants reflexively orient in the direction of
eyes (Driver et al., 1999) but volitionally orient away from
the direction of arrows (Friesen, Ristic, & Kingstone, 2004)
or extended tongues (Downing et al., 2004). A stimulus that is
ambiguously social will elicit reflexive attention shifts when
presented to participants as a social cue (a picture of eyes),
but not when presented as a non-social cue (a picture of a
car) (Ristic & Kingstone, 2005). Furthermore, the effect of
this cue on reflexive attention persists if the cue is presented
first as social and then as non-social, but not vice versa. This
reflexive cueing effect seems strongest for faces, but not necessarily unique to them: arrows have also been shown to trigger reflexive orienting, with magnitude of reflexive orienting
toward arrow cues positively correlated with individuals’ voluntary attention control (Tipples, 2008), suggesting that dissimilarities in attention directed at eyes and arrows are differences of magnitude (strong versus weak), rather than of kind
(reflexive versus volitional).
Eye-tracking and brain-imaging studies reveal similar results. People make more erroneous eye saccades in the direction of a “distracter” cue they are told to ignore if that cue
is a face, rather than an arrow (Ricciardelli, Bricolo, Aglioti, & Chelazzi, 2002). Functional MRI studies show that
the same cue activates different neural pathways depending
on whether it is perceived as eyes or as a non-social directional image (Kingstone, Tipper, Ristic, & Ngan, 2004). Attentional orienting to gaze cues and to arrow cues activates
different cortical networks, with attentional orienting to arrow

1983

cues relying on a pathway associated with voluntary shifts
of attention (Hietanen, Nummenmaa, Nyman, Parkkola, &
Hämäläinen, 2006). In a different fMRI study, however, the
same cue activated the same extensive cortical network regardless of whether it was interpreted as an eye or an arrowhead, though the eye cue more strongly engaged some
parts of this network (Tipper, Handy, Giesbrecht, & Kingstone, 2008).
Psychologists have suggested that shared attention is a
precursor to developing a theory of mind for other people,
and that lacking ability to interpret others’ visual attention
might indicate social disorders such as autism (Baron-Cohen,
1995). Children with autism fail to show preferential sensitivity to socially relevant cues such as human gaze: they
demonstrate similar response times to both arrows and faces
on a counterpredictive cueing task (whereas typically developing children are cued by faces but not by arrows) (Senju,
Tojo, Dairoku, & Hasegawa, 2004), and they avoid shifting
their gaze in response to non-predictive gaze cues (Ristic et
al., 2005). Participants’ scores on the Autism-Spectrum Quotient have also been negatively correlated with reflexive gaze
cueing magnitude (Bayliss, Pellegrino, & Tipper, 2005).
In summary, evidence suggests that for non-predictive
cues, both social and non-social directional stimuli elicit reflexive attention shifts in cued directions, but that for counterpredictive cues, socially relevant stimuli (such as human
faces) continue to elicit reflexive attention shifts while nonsocial directional stimuli, such as arrows, exhibit weak or no
reflexive attentional influence. This paper applies the psychophysical methods used to isolate attention shifts for faces
and arrows to novel stimuli to inform the field of human-robot
interactions (HRI). HRI is interested in how people perceive
robots and how designers can create robots that interact naturally with people. To date, there has been little research on
the cognitive effects of robots on human attention. As the
presence of robots in day-to-day social situations increases,
however, it becomes important to evaluate robots’ cognitive
influence to better understand the roles robots can perform
and to improve designs of human-assistive robots.
Some evidence suggests that robots can use gaze cues to
“leak” information to humans. In conversations between
robots and human participants, robots were able to define
participants’ roles (addressee, bystander, or eavesdropper)
through visual attention cues (Mutlu, Shiwa, Kanda, Ishiguro, & Hagita, 2009). Another study found that robots can
influence people’s decisions in a game by shifting their eyes
briefly to a target, even when participants do not report seeing those cues (Mutlu, Yamaoka, Kanda, Ishiguro, & Hagita,
2009). In the latter study, robot appearance influenced the effectiveness of gaze cues: Geminoid, a very human-like robot,
was more effective at revealing intentions through gaze cues
than Robovie, a robot with more abstract human features.
In this paper, we ask: will robots be treated like humans
or like arrows? That is, will robot gaze be interpreted by humans’ cognitive systems as a social cue on par with human

gaze, with attendant reflexive shifts of attention in the gaze
direction? Or will robots be perceived by humans as nonsocial entities, such as arrows or cars, allowing participants
to override reflexive attention shifts in favor of volitional orienting toward predicted probe locations? Because robots are
designed with varying levels of anthropomorphism, we use
two robot stimuli, one from a very human-like robot called
Zeno, and one from a less anthropomorphic robot named
Keepon. Cueing effects from human faces have been found to
be stronger for schematic faces than for real faces (Hietanen
& Leppänen, 2003), suggesting that cueing information contained in schematic faces is overshadowed by the complexity
of real faces. For this reason, we also use two types of human
face stimuli: a photograph of a human face and a line drawing of a face. Finally, we use an arrow as a non-social but
directional stimulus.

Methods
This experiment employs two commercially available robot
platforms. Zeno is produced by Hanson Robotics as a realistic, expressive robot (Figure 1(c)). In addition to eyes and a
nose, Zeno’s face has human-like features such as eyebrows,
lower eyelids, an expressive mouth, and hair. In contrast,
Keepon’s features are less human-like (Figure 1(d)). Developed by Hideki Kozima, Keepon is a 20 cm tall robot made of
two stacked yellow spheres of deformable rubber; its eyes are
white circles overlapped by smaller, concentric black circles
and its nose is a black circle. Keepon’s deformable body and
eyes with sclera suggest biological features, but its form and
color (bright yellow) clearly indicate that it is robotic. The
aim of selecting such different robots is to identify whether
human-like features are necessary to evoke the same (purportedly social) response as to a human face.
Participants were 41 male and 29 female Yale University
students between the ages of 18 and 34 (mean age 21.4). Each
participant was assigned to a single stimulus condition (human, line drawing, Zeno, Keepon, or arrow). Participants
were recruited in person or with flyers around campus, and
were rewarded with candy at the end of the experiment.

Stimuli
The human gaze stimulus is a head-and-neck photograph of
a woman (Figure 1(a)). Her head subtends a visual angle of
6.2 ◦ horizontally. Each eye subtends 1.0 ◦ and the center of
each eye is 1.2 ◦ to the right or left of center. This stimulus
was chosen as a social analogue to photographs of the robots.
The line drawing stimulus, re-created from Friesen et al.
(2004), is a black-and-white line drawing of a face with circular eyes and nose, and a line for the mouth (Figure 1(b)).
The head subtends 7.5 ◦ ; each eye subtends 1.0 ◦ and its center
is 1.0 ◦ left or right of image center, where the nose is located.
This stimulus has been previously shown to elicit both reflexive and volitional shifts of attention (Friesen et al., 2004).
Zeno is example of a highly anthropomorphic robot (Figure 1(c)). The Zeno stimulus is a head-and-neck photograph

1984

(a) Human gaze stimulus

(b) Line-drawn face stimulus

Figure 2: Time course for a single (predicted) trial of the
Keepon gaze condition. Setup is similar for other stimuli and
gaze directions.
(c) Zeno gaze stimulus

(d) Keepon gaze stimulus

Figure 3: Three types of trials were presented: cued, in which
probe and gaze are congruent; predicted, in which probe location is opposite to gaze direction; and not-predicted-not-cued
or NPNC, in which probe is on a different axis than gaze.
Percentages indicate probability of occurrence.

(e) Arrow stimulus

Figure 1: Each subject was assigned to one of five stimulus
conditions. This figure shows the front, right, up and down
versions of each stimulus; left versions are mirrors of rightfacing versions and are omitted here for brevity.

of the robot, with face subtending 6.7 ◦ horizontally (7.8 ◦ including hair) and each 1.0 ◦ eye located 1.3 ◦ to the left or
right of center. Keepon is a less anthropomorphic robot (Figure 1(d)). The Keepon stimulus is a full-body photograph of
the robot, subtending 6.2 ◦ horizontally, with each 1.0 ◦ eye
located 1.75 ◦ to the left and right of center.
The arrow stimulus is 7.1 ◦ long and drawn over a 6.2 ◦ fixation cross; equal amounts of visual information are presented
at the head and tail of the arrow, thereby avoiding the possibility that cueing results simply from additional features in
the cued direction (Figure 1(e)).
Each stimulus had left-, right-, up- and down-facing variants (see Figure 1). In a single trial of the cueing condition,
the front-facing variant was presented for 500 milliseconds,
followed by one of the other (“turned”) variants. After a 400
millisecond SOA (or a 600 millisecond SOA in human and
Zeno conditions), a probe letter, either “T” or “L,” appeared
on the screen either above, below, to the left, or to the right of

the stimulus image. Each probe letter was 0.9 ◦ tall and wide,
and was presented along the midline 4.8 ◦ from center. Cue
and probe remained on screen until participants responded by
pressing a keyboard key or until 2 seconds elapsed. (See Figure 2 for an example.)
Following Friesen et al. (2004), for each trial of the cueing condition, the probe had a 75% chance of appearing on
the opposite (predicted) side of where the cue directed, and a
25% chance of appearing in one of the other three locations
(approximately 8% chance each)—on the same side as where
the cue directed (cued), or orthogonal to the direction of the
cue (not-predicted-not-cued or NPNC), as shown in Figure 3.
Once participants responded to the probe or 2 seconds
elapsed, all images were replaced by a prompt asking participants to press any key to proceed to the next trial.

Procedure
Participants were seated approximately 60 cm in front of a 29
cm by 18 cm laptop screen. They were told which stimulus
they would observe and the sequence of images they would
see during the experiment (as in Figure 2). Participants were
told they would first observe a front-facing stimulus, replaced

1985

Stimulus
Human

Line

Zeno

Keepon

Arrow

Trial type
cued
predicted
NPNC
cued
predicted
NPNC
cued
predicted
NPNC
cued
predicted
NPNC
cued
predicted
NPNC

Avg. RT (ms)
444
428
462
458
449
474
473
452
473
464
428
469
453
433
461

SD
46
54
61
73
73
70
147
108
116
65
52
55
66
44
53

N
15

16

13

14

12

Table 1: Average response times and standard deviations, in
milliseconds, for all participants (rounded to the nearest millisecond). Each row represents a stimulus condition separated
into trial types. The last column indicates the number of participants in each condition.

Figure 4: Mean response times in milliseconds for each trial
type and stimulus condition. A single asterisk indicates significant differences (p < 0.05), a double asterisk indicates
borderline significant differences (p < 0.10).

by a “turned” stimulus, then a probe letter (“T” or “L”). They
were also informed that the probe was three times more likely
to appear on the side opposite where the gaze or symbol directed. Participants were asked to press the keyboard key of
the letter appearing on the screen as quickly and accurately
as possible. These instructions were also presented textually
on the screen before the start of the experiment. Key press
response times were recorded for analysis.
All participants saw 99 trials, consisting of 96 test trials
and 3 additional practice trials drawn at random from the test
trials and presented first. The set of test trials comprised 72
predicted trials (the probe appeared opposite where the cue
indicated), 8 cued trials (the probe appeared on the side indicated by the cue), and 16 NPNC trials (the probe appeared
on a different axis than the one directed by the cue), with “T”
and “L” presented equally frequently.

Results
Mean response times and standard deviations are listed by
condition and trial type in Table 1. Figure 4 shows mean
response times by stimulus condition and trial type.
Data from four participants were excluded for noncompliance (not following directions to respond as quickly
as possible, or pressing keys at random as evidenced by high
error rates). Trials in which participants incorrectly identified probe letters, response times exceeded 1.5 seconds, or
response times were less than 100 ms were treated as errors
and excluded from analysis. The error rate was 3.9% for analyzed data. In total, results from 70 particpants across the five
conditions were analyzed, as shown in Table 1.
A repeated measures analysis of variance with stimulus
type (human, line drawing, Zeno, Keepon, and arrow) as the

between-subjects variable and trial type (cued, predicted and
NPNC) as the within-subjects variable revealed significant
main effects for trial type (F(2, 130) = 19.819, p < 0.001)
though not for stimulus condition (F(4, 65) = 0.196, p =
0.939). There was no interaction between stimulus type and
trial type (F(8, 130) = .673, p = 0.703).
Because there was a significant main effect of trial type,
we further analyzed the data within each stimulus condition
with a repeated measures analysis of variance on trial type,
which found significant main effects for trial type on most
conditions (human: F(2, 28) = 3.675, p = 0.038; line drawing: F(2, 30) = 4.328, p = 0.022; Zeno: F(2, 26) = 3.409,
p = 0.048; Keepon: F(2, 26) = 13.558, p < 0.001), and
borderline significance main effects in the arrow condition
(F(2, 22) = 2.672, p = 0.091). In all conditions, pairwise
comparisons reveal that each stimulus elicited significantly
faster response times to predicted than to NPNC trials (human: mean difference = 33.921, sd = 8.764, p = 0.002; line
drawing: mean difference = 24.892, sd = 5.902, p = 0.001;
Zeno: mean difference = 24.515, sd = 8.335, p = 0.011;
Keepon: mean difference = 39.878, sd = 9.410, p = 0.001;
arrow: mean difference = 27.875, sd = 11.120, p = 0.029).
Only in the robot conditions, however, were there significant
or borderline-significant differences between predicted and
cued trials as well (Zeno: mean difference = 23.746, sd =
12.712, p = 0.084; Keepon: mean difference = 36.698, sd =
8.613, p = 0.001).

Discussion
Results suggest that participants recognized the directional
significance of all stimuli, but only responded to the cueing

1986

significance of non-robot stimuli. Though they were able to
extract directional information from robot gaze, participants
in either robot condition were not susceptible to reflexively
reorienting in the direction of robot gaze, in contrast to participants in face and arrow conditions.
Response times were statistically faster for predicted trials than for baseline NPNC trials in all stimulus conditions,
which indicates that participants directed their attention toward predicted locations—where they expected a stimulus to
appear—more than toward not-predicted not-cued (NPNC)
locations. For robot stimuli (Zeno and Keepon), response
times were also statistically faster for predicted than for cued
trials (borderline significance in the Zeno case, p = 0.084).
In other words, participants directed their attention significantly more quickly toward predicted locations than toward
cued locations, and thus show no evidence of having been
cued by robot gaze: participants in robot conditions attended
to cued locations just as infrequently as NPNC locations that
were neither cued nor predicted. In contrast, response times
were not significantly different between predicted and cued
trials in the non-robot conditions (human face, line drawing
of a face, and arrow). Participants in these conditions were
not significantly more attentive to predicted than to cued locations; in fact, Figure 4 shows that cued trial response times
were, on average, greater than predicted trial response times
but less than NPNC trial response times. This suggests that
non-robot stimuli attracted participants’ reflexive attention to
cued locations despite the fact that participants were no more
motivated to look at cued locations than at NPNC locations.
This counterpredictive cueing task involved four possible
probe locations on each trial: a cued location, in the direction
of gaze or pointing; a predicted location, opposite the cued
location, where participants were told probes were likely to
appear; and two not-predicted not-cued locations (NPNC),
which have the same probability of probes appearing at each
of them as at the cued location. NPNC trials provide a good
baseline because they involve an identical task (responding to
a probe with a key press) but do not represent cued or predicted locations. In this study, participants were significantly
faster at responding to probes at predicted locations than at
NPNC locations for every stimulus, revealing that they recognized the direction indicated by the stimulus and used that
to inform them of predicted probe position.
In contrast, participants in both robot conditions were also
significantly faster at responding to probes at predicted locations than those at cued locations. In essence, participants
seem to be ignoring the natural interpretation of robot gaze
in favor of the counterpredictive interpretation demanded
by the task. This behavior has been observed in children
with autism, who are able to ignore non-predictive gaze
cues, while their typically-developing peers are susceptible
to reflexive cueing from non-predictive stimuli (Ristic et al.,
2005). The fact that robots do not seem to cue reflexive attention, in a way that even non-social stimuli such as arrows
do, suggests that robots are cognitively processed differently

than common directional symbols or social entities.
Previous studies (e.g., Friesen et al., 2004; Tipples, 2008)
use a similar counterpredictive experimental design in which
participants are asked to press a key when any probe appears. This detection task elicits covert attention shifts, in
which participants’ eyes do not move (in fact, Friesen et al.
(2004) tracked several participants’ eyes to ensure that they
did not shift). The task in the current experiment requires
identifying probes (either “T” or “L”) by pressing corresponding keyboard keys, so results from this identification task are
not directly comparable to results from previous detectionbased experiments. It would be interesting, however, to analyze covert attention effects of various robots using detection
tasks. Some robotics studies (e.g., Mutlu, Yamaoka, et al.,
2009; Mutlu, Shiwa, et al., 2009) suggest that highly anthropomorphic robots are more successful than less anthropomorphic robots at conveying intentions through gaze, suggesting
that robot anthropomorphism influences covert attention.
Attentional cueing is more pronounced with schematic
drawings of faces than with real faces (Hietanen & Leppänen,
2003), so this study included both a photograph of a human face and a line drawing of a face as stimuli. Both
faces elicited significantly faster responses to predicted versus NPNC trials, but not to predicted versus cued trials.
Though the arrow stimulus also showed this effect statistically, differences between NPNC and cued trial response
times are larger for the two social stimuli, with 17.183 ms
average difference for the human face, and 16.140 ms average difference for the line drawing, compared with 7.548 ms
average difference for the arrow.
Some stimuli were tested at 400 ms SOA (line drawing, arrow, and Keepon) while others were tested at a 600 ms SOA.
This represents a methodological change undertaken partway
through the experiment, in order to align more precisely with
previous research. Both SOA times are within the threshold for “short” SOAs as described by Friesen et al. (2004),
and reflexive cueing effects have been found at up to 600 ms
SOAs (Friesen et al., 2004; Tipples, 2008). Therefore, we
believe these SOAs to be comparable.
This study provides some of the first insight into cognitive
processing of robot stimuli, using psychophysical techniques
common in cognitive psychology but largely unused in the
field of human-robot interaction (HRI). There is significant
information to be gained from analyzing the cognitive effects
of robots on human attention, both for cognitive scientists interested in which features cue attention, and robot designers
interested in creating robots that engage in natural social interactions with people. Robot stimuli provide a “real life”
testbed for cognitive attention experiments by allowing researchers to manipulate robotic features to test theories about
what features cue reflexive attention. Robot designers can
use this information in their designs, which would improve
robot usability by allowing people to employ the same social
cues with robots as they do with other humans. The current
study suggests that these two robots, Zeno and Keepon, are

1987

unable to cue human attention in this task the way real faces,
schematic faces, or even arrows do. These results should be
further explored to identify what kinds of visual manipulations can make robots appear reflexively social.

Conclusions
Human eyes elicit strong attentional shifts in the direction
of their gaze, even when this shift is detrimental to viewers’ goals, while non-social directional cues such as arrows
have demonstrated weaker attentional cueing effects. Little
evidence currently exists for the cognitive effects of robot
gaze cues, however. Using an established counterpredictive
cueing experiment, we analyzed the attentional influence of
two robots that vary in level of anthropomorphism, and compared our findings to attentional effects of human faces and
arrows. Results indicate that all of the stimuli conveyed directional information to participants, but that neither robot
stimulus evoked reflexive attentional cueing, though all nonrobot stimuli elicited this effect. These findings suggest that
when participants are motivated to look away from a directed
location, common directional symbols still engage an automatic attention shift to the directed location, while robots do
not. This paper is among the first to apply psychophysical
techniques to the analysis of cognitive effects of robot appearance, and further experimentation using these techniques
might reveal what features influence natural social responses
from people, and may help robot designers who are interested
in creating social robots.

Acknowledgments
Thank you to Brian Scholl, Lisa Brandes, Brad Hayes, Taylor Brown and Anna Blazejowskyj for help in devising, conducting, and presenting this research. This work is supported
by NSF awards #0835767 (Understanding Regulation of Visual Attention in Autism through Robots and Human Social
Development) and #0968538 (Modeling Agency and Intentions in Dynamic Environments as a Precursor to Efficient
Human-Computer Interaction), the DARPA Computer Science Futures II program, Microsoft and the Sloan Foundation.
This work was made possible through a software grant from
QNX Software Systems Ltd and hardware grants by Ugobe
Inc. The first author is supported by an NSF Graduate Research Fellowship.

References
Baron-Cohen, S. (1995). Mindblindness: An essay on autism
and theory of mind. Cambridge, MA: MIT Press.
Bayliss, A. P., Pellegrino, G. di, & Tipper, S. P. (2005).
Sex differences in eye gaze and symbolic cueing of attention. The Quarterly Journal of Experimental Psychology,
58A(4), 631–650.
Downing, P. E., Dodds, C. M., & Bray, D. (2004). Why does
the gaze of others direct visual attention. Visual Cognition,
11(1), 71–79.
Driver, J., Davis, G., Ricciardelli, P., Kidd, P., Maxwell, E., &

Baron-Cohen, S. (1999). Gaze perception triggers reflexive
visuospatial orienting. Visual Cognition, 6(5), 509–540.
Friesen, C. K., & Kingstone, A. (1998). The eyes have it!
Reflexive orienting is triggered by nonpredictive gaze. Psychonomic Bulletin and Review, 5(3), 490–495.
Friesen, C. K., Ristic, J., & Kingstone, A. (2004). Attentional
effects of counterpredictive gaze and arrow cues. Journal
of Experimental Psychology: Human Perception and Performance, 30(2), 319–329.
Hietanen, J. K., & Leppänen, J. M. (2003, December). Does
facial expression affect attention orienting by gaze direction cues? Journal of Experimental Psychology: Human
Perception and Performance, 29(6), 1228–1243.
Hietanen, J. K., Nummenmaa, L., Nyman, M. J., Parkkola,
R., & Hämäläinen, H. (2006). Automatic attention orienting by social and symbolic cues activates different neural
networks: An fMRI study. NeuroImage, 33, 406–413.
Hood, B. M., Willen, J. D., & Driver, J. (1998, March).
Adult’s eyes trigger shifts of visual attention in human infants. Psychological Science, 9(2).
Kingstone, A., Tipper, C., Ristic, J., & Ngan, E. (2004). The
eyes have it!: An fMRI investigation. Brain and Cognition,
55, 269–271.
Langton, S. R. H., & Bruce, V. (1999). Reflexive visual
orienting in response to the social attention of others. Visual
Cognition, 6(5), 541–567.
Mutlu, B., Shiwa, T., Kanda, T., Ishiguro, H., & Hagita,
N. (2009, March). Footing in human-robot conversations:
How robots might shape participant roles using gaze cues.
In Human robot interactions (HRI’09) (pp. 61–68). La
Jolla, California, USA: ACM.
Mutlu, B., Yamaoka, F., Kanda, T., Ishiguro, H., & Hagita,
N. (2009, March). Nonverbal leakage in robots: Communication of intentions through seemingly unintentional
behavior. In Human robot interactions (HRI’09). La Jolla,
California, USA: ACM.
Ricciardelli, P., Bricolo, E., Aglioti, S. M., & Chelazzi, L.
(2002, December). My eyes want to look where your eyes
are looking: Exploring the tendency to imitate another individual’s gaze. NeuroReport, 13(17), 2259–2264.
Ristic, J., & Kingstone, A. (2005). Taking control of reflexive
social attention. Cognition, 94, B55–B65.
Ristic, J., Mottron, L., Friesen, C. K., Iarocci, G., Burack,
J. A., & Kingstone, A. (2005). Eyes are special but not for
everyone: The case of autism. Cognitive Brain Research,
24, 715–718. (Short communication)
Senju, A., Tojo, Y., Dairoku, H., & Hasegawa, T. (2004,
March). Reflexive orienting in response to eye gaze and
an arrow in children with and without autism. Journal of
Child Psychology and Psychiatry, 45(3), 445–458.
Tipper, C. M., Handy, T. C., Giesbrecht, B., & Kingstone, A.
(2008). Brain responses to biological relevance. Journal of
Cognitive Neuroscience, 20(5), 879–891.
Tipples, J. (2008). Orienting to counterpredictive gaze and
arrow cues. Perception and Psychophysics, 70(1), 77–87.

1988

