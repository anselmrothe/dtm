UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Cloze but no cigar: The complex relationship between cloze, corpus, and subjective
probabilities in language processing
Permalink
https://escholarship.org/uc/item/69s3541f
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Smith, Nathaniel
Levy, Roger
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

          Cloze but no cigar: The complex relationship between cloze, corpus, and
                                 subjective probabilities in language processing
                                                                Nathaniel J. Smith
                                                            njsmith@cogsci.ucsd.edu
                                               UC San Diego Department of Cognitive Science
                                          9500 Gilman Drive #515, La Jolla, CA 92093-0515 USA
                                                                      Roger Levy
                                                                   rlevy@ucsd.edu
                                                    UC San Diego Department of Linguistics
                                          9500 Gilman Drive #108, La Jolla, CA 92093-0108 USA
                                                                                                       a
                              Abstract                                                                   True language
                                                                                                          statistics "in
   When performing online language comprehension, compre-                                                   the world"
   henders probabilistically anticipate upcoming words. Psy-
   cholinguistic studies thus often depend on accurately estimat-
   ing stimulus predictability, either to control it or to study it, and
   this estimation is conventionally accomplished via the cloze                              b                        e
   task. But we do not know how effectively — or even, strictly                                    Individual            Corpus
   speaking, whether — cloze probabilities reflect comprehender                                  knowledge of              text
   predictions. This is both methodologically worrisome and an                                     language
   obstacle to detailed understanding of online predictive mecha-
   nisms. Here, we demonstrate first that cloze probabilities vary
   substantially and systematically from normative corpus statis-              c                             d                 f
   tics, and secondly that some portion of these deviations are also              Predictions used                Cloze           Computational
   reflected in online comprehension measures. Therefore, while                    by individual in           responses          language model
   there is some reason to be concerned that cloze norming may                     comprehension                                      estimates
   be distorting the results of psycholinguistic studies, these ap-
   parent distortions may instead reflect genuine errors in native
   speakers’ probabilistic models of their language.
                                                                               Figure 1: An informal illustration of the situation faced by
   Keywords: Psychology; Linguistics; Prediction; Language
   Understanding; Reading; Rationality                                         those who wish to study linguistic prediction. Language is
                                                                               actually used in some particular ways in the real world (a);
   There’s currently a great deal of interest in how the brain                 some subset of these uses are recorded in corpora (e), and
makes and uses predictions (Bar, 2009). Within psycholin-                      may be used to train computational language models (f). A
guistics, this interest dates back 30 years, to the discovery                  different subset is experienced by human language users, who
that the predictability of a word — its probability of occur-                  use these experiences to create some internal model of the
rence given preceding context — has large and robust effects                   statistics of their language (b). They then draw on this inter-
on both reading times (Ehrlich & Rayner, 1981) and event-                      nal model to make predictions during online linguistic com-
related brain potentials (Kutas & Hillyard, 1984). These early                 prehension (c) and also, presumably, when responding in the
studies, and innumerable others since, rely on the cloze task                  cloze task (d). But the actual relationship between the items
(Taylor, 1953) to measure the predictability of their stimuli.                 on the left side of the diagram remains obscure — do cloze
Many more studies use cloze to control for predictability in                   completions match online predictions? Do online predictions
order to isolate some other variable of interest. Yet despite                  match real-world statistics?
its ubiquitous use as an estimate of predictability, we know
almost nothing about what this task is actually measuring.
                                                                               based measurements.
   The cloze task consists of presenting a large group of par-
                                                                                  We know that the participants in a cloze task have some
ticipants with sentence stems like In the winter and                     ,
                                                                               knowledge of their language (Fig. 1b), which they presum-
and asking each to fill in the blank with some plausible con-
                                                                               ably draw on when producing continuations. But isn’t clear
tinuation — some might write spring, others summer, and
                                                                               how they use this knowledge. If they generated their cloze
so on. We then count up what proportion of participants re-
                                                                               responses by sampling from their subjective probability dis-
sponded with each word; this proportion is called the cloze
                                                                               tribution (‘probability matching’), then cloze probabilities
probability of that word in that context. Our goal is to get
                                                                               would be identical to subjective probabilities.1 But cloze
some estimate of the subjective probability distribution over
                                                                               norming is an offline, untimed, and rather unnatural task,
continuations which skilled comprehenders compute implic-
                                                                               which leaves ample room for conscious reflection and other
itly during online comprehension; Fig. 1 summarizes the logi-
                                                                               strategic effects to distort this process — if participants are
cal relationship between these subjective probability distribu-
tions, cloze probability distributions, and alternative corpus                     1 At least if we ignore inter-subject variation, as is conventional.
                                                                           1637

even probability matching in the first place. So our first ques-            ing times. While more work remains to fully isolate these
tion is: what distortions, if any, are introduced by the pro-               effects, we find preliminary evidence to at least rule out the
cesses that produce d from b?                                               possibility that strategic effects in the cloze task are the sole
   This question has important methodological implications,                 source of these biases.
because if there are systematic biases in cloze estimates, then
in the worst case attempts to measure or control for pre-                                               Experiment 1
dictability might actually introduce confounds. For instance,               Methods
if there were some measure M that affected cloze but did not
affect reading times, then a reading time study that compared               Materials We selected 300 four-word sentence initial stems
cloze-matched high-M and low-M items might find a spuri-                    from the Web 1T 5-gram corpus (Brants & Franz, 2006),
ous effect of M on reading times, because at a fixed level                  which was compiled from one trillion words of English web
of cloze, variation in M would be confounded with variation                 text. (By ‘stem’ we mean nothing more or less than four
in true predictability. This result would indicate not that M               words which begin a sentence.) The messy nature of this cor-
affected language processing, but only that it allowed us to                pus required a complex selection procedure; we summarize
better estimate predictability by correcting the biased cloze               the most important points: Our stems were required to have
estimates.                                                                  occurred often enough to allow reasonable probability esti-
   The ideal solution to this problem would be to measure                   mates (median count 1906, minimum count 250), to meet a
subjective probabilities and cloze on the same items for com-               minimum perplexity threshold according to a separate trigram
parison. Sadly, this is impossible, since we have no reliable               model trained on the British National Corpus (The British Na-
independent measure of subjective probabilities. Fortunately,               tional Corpus, version 3 (BNC XML Edition), 2007), to in-
several extremely large corpora have recently become avail-                 duce mostly open-class word continuations (≥ 90%), and to
able, which allow us in Experiment 1 to compare cloze distri-               vary substantially in the range of probability for their most-
butions to true distributions of continuations in large corpora             likely and second-most-likely continuations. They were then
of real text (e).2 Logically, the relationship between cloze                screened by hand to eliminate obvious spam (any phrase used
and corpus distributions is determined by the arrows linking                in a spam web page is repeated in many locations, which
them. If none of the processes denoted by a→e, a→b, or                      causes it to be over-counted relative to its actual usage),
b→d introduced distortions of any kind, then cloze and cor-                 high-frequency stereotyped phrases (e.g., Designated trade-
pus distributions would be identical. Of course, it turns out               marks and brands. . . ), excessively technical usages that we
that we instead find large and systematic differences. So the               judged participants were unlikely to have had much experi-
next question is where these distortions arise.                             ence with (The study protocol was. . . ), or web-specific us-
   If they come from strategic task effects in the b→d link,                ages (on the web, If you leave the. . . is usually followed
then that has important methodological implications, as dis-                by . . . field blank. . . , because web pages are very concerned
cussed above. But another possibility is that they arise from               about explaining web forms). Finally, whenever two stems
learning or processing biases in the a→b link — that is, bi-                were judged ‘too similar’ to each other (e.g., because they
ases in cloze responses might reflect actual errors or ineffi-              differed from each other only in the gender of pronouns), one
ciencies in language users’ predictions about upcoming ma-                  of them was eliminated.
terial. Such errors would be of great theoretical interest, but             Procedure Participants performed a computerized sentence
have not previously been possible to study, since you cannot                continuation (cloze) task, in which they were given each stem
measure biases when your measuring tool has a matched set                   and asked to type one or more words which naturally contin-
of biases. And, of course, we must also consider the less in-               ued the sentence. Spelling was corrected by hand, with com-
teresting possibility that some portion of these differences are            puter assistance.
simply caused by biases in the sampling process (a→e) used
                                                                            Participants 140 students from UC San Diego participated
to construct our corpus. In Experiment 2, we begin to dis-
                                                                            for course credit. All were native English speakers. 114 par-
tinguish these possibilities by directly comparing cloze and
                                                                            ticipated via an online web form; of these, 6 were eliminated
corpus probability in their ability to explain self-paced read-
                                                                            for admitting in a post-test questionnaire that they had used
    2 Note that this is quite different from comparing cloze probabili-     Google to find continuations. The remaining 26 participants
ties to probabilities generated by the computational language models        performed the identical task in a lab environment.
(f) that are also sometimes used in research (Hale, 2001; McDonald
& Shillcock, 2003; Demberg & Keller, 2008; Levy, 2008; Smith &              Results and Discussion
Levy, 2008). Those models can use sophisticated mathematics (rep-
resented by the e→f link) to estimate predictabilities, but for com-        As the online and in-person participant groups performed
putational reasons are still limited in considering a small fraction of
available context — usually just one to two words, or a parse tree          similarly in all analyses reported here, we present only pooled
stripped of word identity — which makes them unrealistic models             data (N = 134).
of human performance. Here, we use materials constructed so that               When we started this project, the Web 1T corpus was the
our critical word always appears after just four words of context, and
then use a corpus so large that we can simply count how often that          only corpus available that was large enough for our purposes,
word appears given the full context.                                        and so it was used to optimize the design of both this and
                                                                        1638

                    He played a key. . .                    After a cup of. . .
                           role    94%             42%            coffee 39.6%                28%                                                          Web      Books
                           part 2.3%              3.8%               tea 39.1%                61%
                                                                     hot       3.0%             —                  Corpus probability                    0.79        0.79
                                                                                                                     Corpus constraint (log10 )         −0.40       −0.33
                    When she began to. . .                  The time needed to. . .                                  Stem frequency (log10 )            −0.23        0.19
                         speak 5.2%               9.0%        complete      41%               6.7%
                           cry 2.5%               19%              reply   3.2%                  —                 Familiarity                           0.96        0.78
                          work 2.2%               1.5%            finish   0.3%               10%                  Concreteness                          0.14        0.03
                    It usually takes the. . .               In the winter and. . .                                 Imageability                         −0.14        0.07
                            form      34%         1.5%            spring     66%              40%                  Age of acquisition                    0.28        0.24
                           shape      23%            —             early     13%                —                  Frequency (log10 )                    0.20        0.16
                       following 2.7%                —          summer      4.2%              32%
                            cake        —         8.3%               fall   2.3%              19%                  Contextual diversity (log10 )         0.72        0.31
                                                                                                                   Length                                0.00       −0.11
  Table 1: Sample continuation distributions from Experiment                                                       Lexical prime probability             4.37        6.33
  1. In each case, the left column is corpus probability, and
  the right column is measured cloze probability. ‘—’ denotes                                               Table 2: Estimated coefficients from a log-linear model re-
  continuations that were never observed.                                                                   gressing cloze responses against corpus probability and other
                                                                                                            measures. Shaded cells are significant. Positive/green cells
                                                                                                            indicate a response preference, while negative/red cells indi-
                                          Web                                  Books
                    1                                                                                       cate a response dispreference; e.g., cloze participants use fa-
                                                                                                            miliar words more than would be expected given corpus prob-
Cloze probability
                                                                                                            ability but may avoid long words. Corpus constraint and stem
                    10−1                                                                                    frequency are entered as modulating the effect of corpus prob-
                                                                                                            ability, rather than having an independent effect. Not much
                                                                                                            should be read into the absolute magnitude of coefficients,
                    10−2                                                                                    since different predictors are on different scales.
                                                 r = 0.52                              r = 0.59
                    10−3                                                                                    the form
                           10−3    10−2         10−1     1    10−3      10−2           10−1          1                                     1     (α +α center(StemProp1i )+··· )
                                                 Corpus probability                                             P(responsei j |stemi ) =      × pi j 0 1
                                                                                                                                           Zi
                                                                                                                                              × exp(β1 WordProp1 j + · · · )
  Figure 2: Cloze versus corpus probability. Each point rep-
  resents a single stem/continuation pair that appeared in the                                              Here, pi j indicates the corpus probability of continuation j
  corpus and was given by at least one cloze participant. (Re-                                              given stem i, computed as the number of times we observed
  gression analyses included responses that were given by zero                                              this continuation following the stem divided by the total num-
  participants, but they can’t be shown on this log scale.)                                                 ber of times that we observed the stem. α0 is a free parame-
  Correlations are computed in log space. Red crosses mark                                                  ter that measures the sensitivity of cloze to corpus probabil-
  stem/continuation pairs that were selected for use as stimuli                                             ity. An α0 of 0 would indicate no sensitivity, and a value of
  Experiment 2.                                                                                             1 would indicate that cloze matches corpus probability per-
                                                                                                            fectly (at least, until the word-specific parameters come in to
                                                                                                            further influence matters). A value between 0 and 1 would in-
                                                                                                            dicate that cloze distributions are overall flatter (have higher
  the next experiment. However, a second large corpus has re-                                               entropy) than the corresponding corpus distribution, while a
  cently become available, derived from scanned books rather                                                value greater than 1 would indicate that cloze distributions are
  than the raw web (Michel et al., 2011; we use the subset con-                                             more peaked (have lower entropy), as might happen if par-
  taining American English since 1960, consisting of ∼89 bil-                                               ticipants preferred to provide the most-probable continuation
  lion words). This corpus seems more representative of real                                                instead of probability matching. StemProp1, . . . are proper-
  usage (i.e., it has no spam), but is smaller and our experi-                                              ties of the stem which might modulate the overall effect of
  mental design is not optimized to take full advantage of it;                                              α0 . WordProp1, . . . are word properties that might cause par-
  therefore, we perform all analyses with both corpora.                                                     ticipants to give particular responses more or less often than
    Fig. 2 shows the overall relationship between cloze and                                                 predicted by corpus probability alone. And Zi is a normaliz-
  corpus probability; our first result is that their correlation is                                         ing constant (not a free parameter).
  only moderate. So the next question is, if corpus probability                                                Stem predictors included the corpus constraint,
  does not determine cloze, then what does?                                                                 maxi P(continuationi |stem), the total number of times
                                                                                                            that the stem was observed in the corpus (a proxy for
                    To find out, we fit cloze responses to a log-linear model of                            participants likely amount of experience with each particular
                                                                                                         1639

stem). Word predictors included familiarity, concreteness,                    Context            Target
imageability, and age of acquisition (from Wilson, 1988;             In the winter and fall the little town...
Stadthagen-Gonzalez & Davis, 2006; Nelson, McEvoy, &
                                                                                                           Critical region
Schreiber, 1998), word frequency and contextual diversity
(from Brysbaert & New, 2009), word length, and a measure
                                                                               Figure 3: Sample stimulus for Experiment 2.
of interlexical priming from the stem to the target (‘lexical
prime probability’). This measure was computed by looking
up the probability pi that each word i in the stem would pro-        words which are long — perhaps because they require more
duce the continuation as a response in a free-association task       effort to type — and also avoid words which are imageable or
(Nelson et al., 1998), and then combining these probabilities        are acquired early — perhaps because in the formal context
as 1 − ∏i (1 − pi ).                                                 of an experiment they attempt to use more formal language.
   We analyzed the subset of the data for which all of these
norming values were available, for which the continuation                                    Experiment 2
was recorded in the corpus, and, for the book corpus anal-           Having established that cloze and corpus probability vary in
ysis, for which the continuation was observed in the corpus          substantial and systematic ways, our next question is whether
with a stem frequency of >100. (This allowed the analysis of         to attribute these effects to biases in the corpus sampling
5015 responses for the web data, and 4636 for the book data.)        (Fig. 1, a→e), to biases in language acquisition and process-
The model was fit by maximum-likelihood, with all predic-            ing (a→b), or to biases in cloze task performance (b→d). If
tors entered simultaneously, and significance computed with          these effects were caused by the cloze task alone, then we
the likelihood ratio test and corrected for multiple compar-         would expect corpus probabilities to be more closely corre-
isons by sequential Bonferroni. The results of this analysis         lated with online subjective probabilities than cloze proba-
are shown in Table 2.                                                bilities are, and therefore corpus probabilities should outper-
   Reassuringly, the exponent α0 on corpus probability is            form cloze probabilities in explaining performance in an on-
significantly greater than 0 (web: χ2 (1) = 992, p  0.001,          line comprehension task. So in this experiment, we pit cloze
books: χ2 (1) = 784, p  0.001), indicating that cloze is sen-       probabilities against corpus probabilities in explaining self-
sitive to corpus probability, as expected. However, it is also       paced reading times.
significantly smaller than 1 (web: χ2 (1) = 62, p  0.001,
books: χ2 (1) = 44, p  0.001), indicating that cloze dis-           Methods
tributions are systematically more variable (higher entropy,         Materials Experiment 1 produced 2350 stem-plus-
more flattened) than corpus distributions. If we interpret           continuation pairs for which we had both cloze and corpus
cloze task responses as reflecting participant predictions, then     predictability measurements. From these we selected 179
this might suggest that our participants are substantially more      four-word stems, then for each stem selected 2 target
confused about upcoming linguistic material than would be            continuations, producing a total of 358 five-word sentence
expected of an optimal rational agent (compare Griffiths &           beginnings. We then completed each sentence and divided
Tenenbaum, 2006). This increase in entropy is more pro-              them into two 179-sentence lists, so that no participant saw
nounced for contexts that are particularly constraining (web:        any stem or any target more than once. These stems and
χ2 (1) = 36, p  0.001, books: χ2 (1) = 29, p  0.001).              target continuations were selected to maximize our ability to
   The effect of high frequency stems is more complicated. In        distinguish cloze and web corpus probability according to a
the web corpus, these stems produce particularly high entropy        power analysis. No fillers were used.
cloze distributions (χ2 (1) = 54, p  0.001); in the book cor-       Procedure Participants read sentences in random order in
pus, they produce cloze distributions that are lower entropy         a self-paced moving-window paradigm (Just, Carpenter, &
and closer to the normative corpus values (χ2 (1) = 8.6, p <         Woolley, 1982) with a comprehension question presented af-
0.005). This may indicate that on the web, high frequency            ter each sentence.
phrases are ones that are contaminated by spam and other dis-
tributional oddities, but in print, high frequency phrases are       Participants 38 students students from UC San Diego par-
ones that participants genuinely have more experience with,          ticipated for course credit. All were monolingual English
and that they are able to use this experience to make better         speakers.
predictions.
                                                                     Results and Discussion
   For word properties, there is evidence that participants pre-
                                                                     Comprehension questions All subjects performed signif-
fer to respond to words which are familiar, concrete, have
                                                                     icantly above chance on comprehension question accuracy
high contextual diversity, and are primed by words in the stem
                                                                     (minimum 77%, median 92%).
(e.g., this may explain the winter and fall responses; in ordi-
nary usage people would usually say fall and winter, which           Reading times We analyzed the total reading time for a re-
makes fall an unlikely continuation according to the corpus;         gion consisting of the target word plus the following word
but in any case fall is primed by winter). They may avoid            (to capture spillover; see Fig. 3). After removing sentences
                                                                 1640

with incorrect comprehension question answers or outlier             only a fraction of our data (roughly 75%), and adding these
reading times in the critical region (reading times <80 ms,          additional controls reduces our usable data still more, making
>1500 ms, or >4 sd above participant-specific means, 1.9%            this a worst case for statistical analysis. However, further data
of data removed), reading times were entered into a multi-           collection should resolve this issue in one way or the other.
level mixed-effects regression model using lme4 (Bates &
Maechler, 2010). Our first question was whether cloze or cor-                 Conclusion and Future Directions
pus probabilities better explained reading times, so both were
log-transformed (Smith & Levy, 2008 demonstrated that the            Provisionally, at least, we can conclude that not only is cloze
empirical relationship between word predictability and read-         systematically biased relative to corpus probability, but that at
ing time is in fact log-linear) and entered into the regression.     least some portion of these biases are also reflected in com-
In addition, for controls, we entered the log-frequency (from        prehender’s subjective probability estimates (i.e., arise along
Brysbaert & New, 2009), word length, log-frequency/word-             the a→e or a→b arrows in Fig. 1).
length interaction, and log-corpus probability for three differ-        The next challenge for future work is to break down the
ent words: the word preceding the target, the target itself, and     different biases we have observed, and further identify their
the word following the target. Because our stimuli were not          locus of effect. For instance, it could be the case that of the
optimized to produce corpus estimates of the probability of          effects observed in Experiment 1, the age of acquisition bias
the word following the target, this probability was estimated        arises from strategic effects in the cloze task (b→c), while
using only a two-word context (i.e., an unsmoothed trigram           simultaneously the familiarity effect is caused by biases in
model). As random effects, we allowed the intercept to vary          subjective probability estimation (a→b), and then in addition
by stem, and the intercept, slope of the cloze effect, and slope     corpus sampling problems (a→e) are making our corpus es-
of the target word corpus probability effect to vary by subject      timates more noisy across the board.
(this structure selected by model comparison). Significance             Fortunately, testing such hypotheses is possible with the
was assessed by assuming calculated t values were distributed        tools we have described. Our proposed strategy would be to
as standard normal under the null hypothesis (Baayen, David-         use the log-linear modeling approach from Experiment 1 to
son, & Bates, 2008).                                                 estimate and then correct for different biases — that is, to es-
                                                                     timate what cloze would look like if different biases didn’t
   We found that cloze was significant after controlling for
                                                                     exist. In the situation described in the previous paragraph,
corpus probability and the other factors described (web: t =
                                                                     we would predict that removing the age of acquisition bias
−2.87, p < 0.005, books: t = −2.32, p < 0.03), but that after
                                                                     should produce a measure that explains reading times even
controlling for cloze and these other factors, corpus probabil-
                                                                     better than cloze itself does, while removing the familiar-
ity was not significant (web: t = 1.83, n.s., books: t = 1.13,
                                                                     ity bias should reduce our ability to explain reading times.
n.s.; both trends in the wrong direction). This would suggest
                                                                     Since these tests would be comparing cloze-based measures
that some of the the biases we observe in cloze probability are
                                                                     against each other, they reduce the possibility of artifacts
also present in readers’ subjective probabilities — but before
                                                                     caused by corpus sampling problems. However, if such prob-
we conclude this, there is another possible interpretation we
                                                                     lems do exist, we suspect that they should make the corpus
must consider. It might be that readers have accurate knowl-
                                                                     estimates more noisy but without producing any systematic
edge of word predictability, but that their reading times are
                                                                     errors. Therefore, we can test for their presence by removing
independently sensitive to some of the properties listed in Ta-
                                                                     all of the known, systematic biases from cloze, and testing
ble 2. In that case, cloze might outperform corpus probability
                                                                     whether this ‘unbiased’ cloze continues to outperform corpus
simply because cloze is able to account for two factors that
                                                                     probability.
affect reading times, while corpus probability can account for
only one.                                                               Thus, while the differences between cloze and corpus prob-
                                                                     ability continue to raise worrisome questions about current
   To rule out this possibility, we re-ran the regression de-        methodology (it remains possible that many or most of the
scribed above, this time adding the properties from Table 2          biases we found are artifactual), the way is clear to not only
as additional controls; if cloze is only performing well be-         resolve this issue, but also shed new light onto the mecha-
cause of its partial confounding with these other measures,          nisms underlying linguistic prediction in general.
then including them directly should cancel out its effect.
On the contrary, however, our results were essentially un-                               Acknowledgments
changed in the web corpus — cloze remains significant (t =
−2.33, p < 0.02), while corpus probability remains insignif-         We thank Erin Bennett, Tiffany Chiou, Megha Ram, Maria
icant (t = 1.75, n.s.). For the book corpus, cloze drops to          Sokolov, and Daphne Tan for assistance in collecting data.
insignificance (t = −1.6, n.s.) while corpus probability re-         This research was partially supported by NIH Training Grant
mains insignificant (t = 0.59, n.s.). However, as the general        T32-DC000041 to the Center for Research in Language at
trend remains the same, we suspect this is simply a conse-           UC San Diego to NJS, NSF grant 0953870 to RL, and fund-
quence of our reduced statistical power when working with            ing from the Army Research Laboratory’s Cognition & Neu-
this corpus. We start out with corpus probability estimates for      roergonomics Collaborative Technology Alliance.
                                                                 1641

                         References                                   University of South Florida word association, rhyme, and
                                                                      word fragment norms. Available from http://www.usf
Baayen, R. H., Davidson, D. J., & Bates, D. M. (2008,
                                                                      .edu/FreeAssociation/
  November). Mixed-effects modeling with crossed random
                                                                    Smith, N. J., & Levy, R. (2008). Optimal processing times
  effects for subjects and items. Journal of Memory and Lan-
                                                                      in reading: a formal model and empirical investigation. In
  guage, 59(4), 390–412.
                                                                      B. C. Love, K. McRae, & V. M. Sloutsky (Eds.), Proceed-
Bar, M. (Ed.). (2009). Predictions in the brain: using our past
                                                                      ings of the thirtieth annual conference of the Cognitive Sci-
  to prepare for the future [theme issue]. Phil. Trans. R. Soc.
                                                                      ence Society (pp. 595–600). Austin, TX: Cognitive Science
  B, 364.
                                                                      Society.
Bates, D., & Maechler, M. (2010). lme4: Linear mixed-
                                                                    Stadthagen-Gonzalez, H., & Davis, C. J. (2006). The bristol
  effects models using s4 classes [Computer software man-
                                                                      norms for age of acquisition, imageability and familiarity.
  ual]. Available from http://CRAN.R-project.org/
                                                                      Behavior Research Methods, 68, 598–605.
  package=lme4 (R package version 0.999375-37)
                                                                    Taylor, W. L. (1953). “Cloze procedure”: A new tool for
Brants, T., & Franz, A. (2006). Web 1T 5-gram version 1.1,
                                                                      measuring readability. Journalism Quarterly, 30, 415–
  LDC2006T13. Philadelphia, PA: Linguistic Data Consor-
                                                                      433.
  tium.
                                                                    Wilson, M. D. (1988). The MRC psycholinguistic database:
The British National Corpus, version 3 (BNC XML edi-
                                                                      Machine Readable Dictionary, version 2. Behavioural Re-
  tion). (2007). Available from http://www.natcorp.ox
                                                                      search Methods, Instruments and Computers, 20(1), 6–11.
  .ac.uk/ (Distributed by Oxford University Computing
  Services on behalf of the BNC Consortium)
Brysbaert, M., & New, B. (2009). Moving beyond Kuc̆era
  and Francis: A critical evaluation of current word fre-
  quency norms and the introduction of a new and improved
  word frequency measure for American English. Behavior
  Research Methods, 41, 977–990.
Demberg, V., & Keller, F. (2008). Data from eye-tracking cor-
  pora as evidence for theories of syntactic processing com-
  plexity. Cognition, 109(2), 193–210.
Ehrlich, S. F., & Rayner, K. (1981). Contextual effects on
  word perception and eye movements during reading. Jour-
  nal of Verbal Learning and Verbal Behavior, 20(6), 641–
  655.
Griffiths, T. L., & Tenenbaum, J. B. (2006). Optimal predic-
  tions in everyday cognition. Psychological Science, 17(9),
  767–773.
Hale, J. (2001). A probabilistic Earley parser as a psycholin-
  guistic model. In Proceedings of NAACL-2001 (pp. 159–
  166).
Just, M. A., Carpenter, P. A., & Woolley, J. D. (1982).
  Paradigms and processes in reading comprehension. Jour-
  nal of Experimental Psychology: General, 111(2), 228–
  238.
Kutas, M., & Hillyard, S. A. (1984). Brain potentials reflect
  word expectancy and semantic association during reading.
  Nature, 307, 161–163.
Levy, R. (2008). Expectation-based syntactic comprehen-
  sion. Cognition, 106, 1126–1177.
McDonald, S. A., & Shillcock, R. C. (2003). Low-level
  predictive inference in reading: The influence of transi-
  tional probabilities on eye movements. Vision Research,
  43, 1735–1751.
Michel, J., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K.,
  Team, T. G. B., et al. (2011, January). Quantitative anal-
  ysis of culture using millions of digitized books. Science,
  331(6014), 176–182.
Nelson, D. L., McEvoy, C. L., & Schreiber, T. A. (1998). The
                                                                1642

