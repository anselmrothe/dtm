UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Towards Computational Guessing of Unknown Word Meanings: The Ontological Semantic
Approach
Permalink
https://escholarship.org/uc/item/6wf1h6bc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Taylor, Julia M.
Raskin, Victor
Hempelmann, Christian F.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                  Towards Computational Guessing of Unknown Word Meanings:
                                              The Ontological Semantic Approach
                                             Julia M. Taylor (jtaylor1@purdue.edu)
                                            CERIAS, Purdue University & RiverGlass, Inc
                                          West Lafayette, IN 47907 & Champaign, IL 61820
                                              Victor Raskin (vraskin@purdue.edu)
                                              Linguistics & CERIAS, Purdue University
                                                         West Lafayette, IN 47907
                                     Christian F. Hempelmann (chempelm@purdue.edu)
                                           Linguistics, Purdue University & RiverGlass, Inc
                                          West Lafayette, IN 47907 & Champaign, IL 61820
                               Abstract                               leopards, are from millions of years ago.), or by providing
   The paper describes a computational approach for guessing
                                                                      the presumably known opposites for comparison through
   the meanings of previously unaccounted words in an                 words like but, rather then, not (It is frigid outside, rather
   implemented system for natural language processing.                than warm and comfortable like yesterday.).
   Interested in comparing the results to what is known about            Both in the case of human acquisition of new vocabulary
   human guessing, it reviews a largely educational approach,         and the machine attempt at guessing its meaning, these
   partially based on cognitive psychology, to teaching humans,       somewhat trivial instances, where the meaning of a new
   mostly children, to acquire new vocabulary from contextual         word is immediately explained, either by giving its
   clues, as well as the lexicographic efforts to account for
   neologisms. It then goes over the previous NLP efforts in          definition or by examples, present no particular interest for
   processing new words and establishes the difference—mostly,        us here. Besides, such cases are rather rare in regular
   much richer semantic resources—of the proposed approach.           expository texts because most writers do not bother to allow
   Finally, the results of a computer experiment that guesses the     for vocabulary deficiency with regards to words with which
   meaning of a non-existent word, placed as the direct object of     they are well familiar themselves. Thus, it is the non-trivial
   100 randomly selected verbs, from the known meanings of            cases, those without an attached explanation or description,
   these verbs, with methods of the ontological semantics
                                                                      that it is necessary to address when one is interested in
   technology, are presented and discussed. While the results are
   promising percentage-wise, ways to improve them within the         designing a computer system for natural language
   approach are briefly outlined.                                     understanding.
                                                                         On the other side of the spectrum lie words that can only
   Keywords: guessing word meaning, natural language
                                                                      be guessed through their functional description, not
   understanding, ontological semantic technology
                                                                      necessarily following the first use of an unknown word.
                                                                      These functional descriptions should be gathered throughout
                 Unknown Words in Text
                                                                      the document, or a number of documents, narrowing the
Along with ambiguity, unattested input is one of the major            original functional description, if necessary, or supplying
problems for natural language processing systems. An NLP              other facets of it. For example, They used a Tim-Tim to
system is robust only if it can deal with unknown words.              navigate their way to the cabin on the lake. It took them
Yet, to deal with such words only makes sense when the rest           almost half a day. They hadn’t checked if the maps had been
of the sentence is understood. We take an approach here               recently updated on the device, and spent hours looking for
similar to that of a human learner that encounters an                 roads that no longer existed. From the clues in the first
unfamiliar word and is able to approximate its meaning                sentence, Tim-Tim can be understood as a navigation
based on the rest of the sentence or its subsequent usages in         instrument (including an atlas or a map) through an inverse
other sentences.                                                      function of the instrument of navigation. Since no other
   There are some suggested strategies in the human                   devices are mentioned, this navigation instrument can be
acquisition and understanding of unknown words. Some                  considered the device from the third sentence whose maps
cases stand out as easy and almost self-explanatory. One of           can be periodically updated. It is essential, therefore, in
these cases is when a word is immediately explained. Such             situations of dispersed clues that co-reference (or
an explanation may be introduced by a that is phrase (To              antecedence) be established correctly—in this case, between
lose weight, one may have to follow a diet, that is, to limit         device and Tim-Tim.
the amount of food and to avoid eating certain foods.), or by            Towards the middle of the spectrum are the cases where
apposition (Computers programs follow algorithms, ordered             the description may immediately follow the first use of the
lists of instructions to perform.), or by examples (The               word but without being helpfully triggered by phrases like
earliest records of felines, for example, cats, tigers, lions, or     for example or that is (He was rather taciturn. He didn’t like
                                                                  3581

small talk, rarely spoke in society, hardly said anything          o     words that are neither in the oral vocabulary nor the
other than hello when meeting people.). The word taciturn                reading vocabulary, for which there is no concept
in the first sentence is used as a description of he. The                available, and for which a concept cannot be easily built
second sentence continues the description of the same                 Concepts are used in this kind of research in an
person. We could assume that taciturn somehow overlaps             undefined, somewhat pre-scientific sense of general ideas,
with this description, and then, at the very least, we know        possibly underlying synonyms (see, for instance,
that the unknown word refers to a person’s temperament             Wisniewski, 1996, 1997a,b, 2000), the sense that came to be
and/or communication style, and it is on the quiet side.           used in much later ontologies as means to control
   The difference between this text and the previous example       terminological usage (see Raskin et al., 2008).
is that the former consistently describes a person, to the            Metrics are created to measure the strength of contextual
point that that is could be added in (He was rather taciturn,      support, determined on the basis of raters’ judgments as to
that is, he didn’t like small talk, rarely spoke in society,       how much information the text provides about the meaning
hardly said anything other than hello when meeting                 of an unfamiliar word (see Nagy et al., 1987). Success is
people.). The latter one does not contain a focused                also seen as depending on the readability of a text, a
description but rather disperses the clues throughout the          controversial measure, usually defined in terms of sentence
short narrative, and the meaning of the word has to be             length and difficulty of words, and on the “density of hard
derived based on what function it could play in the situation.     words.”
   We are ultimately interested in the more challenging case          These parameters of human cognition pertain clearly to
of dispersed clues, where the functional details must be           the computation of learning new words. Thus, to compare
collected to identify the meaning of the unknown word. This        with the four learning situations considered in human
paper describes an experiment where functional details of a        learning above, two different types of unattested input can
single sentence determine the meaning of the word.                 be encountered in computational systems: unique unattested
                                                                   words with no other (related or unrelated) senses already
       Previous Research on Unknown Words                          known and new senses of known words. For each of those, a
                                                                   concept may be available in the existing ontology or needs
Human Vocabulary Acquisition                                       to be acquired, thus resulting in similar four situations.
The problem of understanding unknown words has been
addressed both with regard to humans, in first and second          Lexicography
language acquisition, and to computers. With humans, it is         Partially, the NLP effort concerning unknown words
known as vocabulary acquisition and enrichment. As                 overlaps with another human effort, namely, dealing with
children and second language learners increase their               neologisms, a traditional concern of lexicography, the art—
vocabulary, enabling as well as expediting this process has        rather than the science—of dictionary making. Algeo (1977,
been an important educational goal. Both methodologies for         1980, 1991, 1993) provides frequently cited information on
helping children and students guess the meanings of the            the source and typology of words entering the language (cf.
unknown words from contextual clues and the metrics for            also Barnhart, 2007; Lehrer, 2003; O’Donovan and O’Neil,
evaluating such methodologies have been discussed in detail        2008; Sheidlower, 1995; Simpson, 2007). Recent
(see, for instance, Bransford and Johnson, 1972; Gipe, 1979;       developments in electronic media have led to new spellings
McKeown, 1985; Nagy et al., 1985; Rankin and Overholser,           of known words (e. g., 4 u), their new forms or senses (to
1969—but cf. Schatz and Baldwin, 1986; and for a more              (un)friend) and new words (twitter). This has engendered a
contemporary perspective, Wagner et al., 2006).                    recent concern for the normalization of e-mail/SMS text
   There is also a number of useful anonymous websites             (Aw et al., 2006; Choudhury et al., 2007).
with exercises and helpful tips for guessing the meanings of
unknown words without looking them up in dictionaries or           Natural Language Processing
encyclopedias1.                                                    While some NLP efforts focus on determining just the part-
   When dealing with human vocabulary acquisition, four            of-speech (POS) of an unknown word (Mikheev, 1997;
types of situations are typically recognized (cf. Nagy et al.,     Murawaki and Kurohashi, 2008; Ciramita, 2002), others
1987):                                                             attempt to guess its lexical/semantic class, most prominently
o words that are in the oral vocabulary but not in the             if it is a proper name (Smarr and Manning, 2002; Bikel et
      reading vocabulary                                           al., 1999; Collins and Singer, 1999; Cucerzan and
o new meanings for words that are already in the reading           Yarowsky, 1999; Buchholz and Van Den Bosch, 2000,
      vocabulary with one or more other meanings                   Nadeau and Sekine, 2009). This results in a large-scale
o words that are in neither the oral vocabulary nor their          effort in named entity recognition, especially in languages
      reading vocabulary and for which there is no concept         like Chinese and Korean, where there is no helpful
      available but for which a concept can be easily built        capitalization, and many names, including foreign ones,
                                                                   utilize the characters for regular words, using their phonetic
   1                                                               values to approximate the foreign pronunciation. Most NLP
    English-Zone.com, www.sinclair.edu/centers/tlc/...
                                                                   work on unknown words is done in the statistical and/or
   /learning_words_from_context_clues.pdf
                                                               3582

machine learning paradigm (Weischedel et al., 1993:                   As such our approach a more mature version of work on
Campbell and Johnson, 1999: Ciaramita and Johnson, 2003;           unknown words in NLP like that of Granger (1977),
Curran, 2005), without ‘understanding’ the contextual clues.       ‘mature’ here meaning not only that our resources are much
                                                                   richer, but also that the unknown word task is only one of
  Guessing Meaning of Unknown Words With                           the many that OST undertakes in the course of its
       the Ontological Semantic Technology                         processing of text towards the representation of its meaning.
                                                                   In this paper, we are illustrating our approach by the
In this section, we will demonstrate in somewhat simplified
                                                                   inferencing of noun meanings in relation to the meaning of
non-proprietary technical detail, how the meaning of an
                                                                   the verbs of which they are direct objects, like Granger
unknown word is determined on the basis of the full under-
                                                                   does. More recent approaches in the same vein include
standing of everything else in the sentence. This will be
                                                                   Cardie (1993), Hastings and Lytinen (1994), and not least
done with the methods and resources of the Ontological
                                                                   the work by Wilks and colleagues on “lexical tuning”
Semantic Technology (OST—see Raskin et al., 2010,
                                                                   (Wilks, 1978; Wilks and Catizone, 2002), much in the spirit
Taylor et al., 2010, and Hempelmann et al., 2010).
                                                                   of very rich semantic resources underlying OST.
   At the core of OST are repositories of world and linguistic
                                                                      While other NLP approaches emphasize neologisms as
knowledge, acquired semi-automatically (Hempelmann et
                                                                   the focus of their unknown-word effort (for a very recent
al., 2010, Taylor et al., 2010) within the approach and used
                                                                   overview see Cook, 2010), we realistically expect unattested
to disambiguate the different meanings of words and
                                                                   input to contain existing words which have not yet found
sentences and to represent them. These repositories, also
                                                                   their way into our lexicon. Even with a 100,000-sense
known as the static knowledge resources, consist of the
                                                                   lexicon, only 10% or so of the lexical resources of a natural
ontology, containing language-independent concepts and
                                                                   language would be covered, and unlike native speakers who
relationships between them; one lexicon per supported
                                                                   make do with well under 50,000 words in their vocabularies
language (for example, English), containing word senses
                                                                   (cf. Nation, 2006), a contemporary NLP application will
anchored in the language-independent ontology which is
                                                                   typically go into highly specific technical terms or seek the
used to represent their meaning; the Proper Name
                                                                   explanation of a very rare word.
Dictionary (PND), which contains names of people,
countries, organizations, etc., and their description              Computational Experiment
anchoring them in ontological concepts and interlinking
them with other PND entries; and a common sense rules              To guess any word in any syntactic position is an overly
resource. A conforming lexicon and ontology, as well as            complex task. We simplified it for this paper by considering
PNDs and common sense rules, are used by the Semantic              only those words that play the role of the direct object of a
Text Analyzer (STAn), a software, developed by RiverGlass          transitive verb, as a starting point. The selection task was
Inc., that produces Text Meaning Representations (TMRs)            twofold: we needed to select a number of verbs to test, and
from the text that it reads. The format of TMRs conforms to        then we needed to select sentences that we could test these
the format and interpretation of the ontology. The processed       verbs in.
TMRs are entered into InfoStore, a dynamic knowledge                  The description of the senses in the English OST lexicon
resource of OST, from which information is used for further        contains an annotation field for the purposes of providing a
processing and reasoning.                                          human-readable definition and an example of the word
   Thus, just as in cognitive psychology, underlying some of       sense in a sentence. The annotation fields were filled in the
the pedagogical research on vocabulary acquisition, where          process of a sense acquisition, long before the experiment in
“the reader is seen as building a mental representation of the     this paper was thought of, and the acquirers did not have
textual meaning based on information contained in the text         any constraining instructions in producing the examples.
and on the activation of complementary knowledge                   The format of the definition and example is not much
resources (van Dijk & Kintsch, 1983, Johnson-Laird, 1983)”         different than that of any dictionary. It serves no purpose for
(Rieder, 2002: 54), STAn is constructing TMRs by                   the computer, which reads machine readable syn-struc, sem-
processing text with the help of the OnSe knowledge                struc and extracts the needed information from there.
resources. From this perspective, the unknown word tasks              The annotation examples are considered to be exemplars
can be seen as finding a formalized solution to a cloze test       of sentences that the software should be able to process.
(Taylor, 1953), where every nth word of a test is deleted and      Since the examples are free creations by acquirers,
the participant is asked to reconstruct these omitted words.       independent from our task, we considered them to be as
Because this “inferencing” of the meaning of an unknown            appropriate for the task as any corpus selection, and it saved
word is done by humans on the basis of context as well as          us the effort of looking for one verb match within a corpus.
language and world knowledge, OST models context as the            Thus, we selected these examples as the test sentences and
syntactic environment of the unknown word mapped onto              replaced the direct object in each example with a word zzz–
the concepts found in this environment and the constraints         an unknown word to the system. The computer’s task was to
these concepts place on the word. The concepts and their           find ontological concepts that could be an interpretation of
properties represent the world knowledge required for the          the word, based on the provided sentence.
task.
                                                               3583

   Our lexicon contains 4469 senses of transitive verbs, not           Note that the first clue of the sentence about rethinking is
including verbs that could be either transitive or intransitive.    practically useless: any object (physical, mental, or social)
From among the 4469 candidates, we randomly selected                that can serve as THEME for thinking works. She could be
verbs until we reached 100 that could be processed using the        rethinking a party, a paper topic, or curtains (the first and
example sentences that resulted in correct sense                    the third theme require the handling of ellipsis). Thus,
interpretation of the verb. We considered 189 verb senses,          without the second and the third clues, the interpretation
59 of which contained no examples, and 30 of which                  could have been left as wide as any object or event.
produced an interpretation of a verb unacceptable to a                 We consider (native-speaker-) acceptable any interpreta-
human expert. In other words, the computer misunderstood            tion that is reasonable within the context of the sentence,
the verb meaning.                                                   without any outside knowledge or emphasis. Thus, we
   The remaining 100 verbs whose example sentences                  consider it possible for a football player to be infuriated
passed the acceptability rating were considered for the             when a dog barks, for plants to be imported, and for water
unknown word test. Each direct object, defined by the syn-          vehicles to be caulked.
struc, was replaced in the example sentence with zzz. For              As demonstrated above, the task did not necessarily
example, the sentence for the verb rethink, She decided she         restrict possible interpretations to a small number of
would rethink the new curtains before buying them for the           concepts. To get a better handle on what the analyzer
whole house became She decided she would rethink zzz                offered as its guesses, we considered the top five
before buying them for the whole house.                             interpretations (TMRs) for each sentence, if the output
   We added a file to our English lexicon with the word zzz         contained at least five, and all of the interpretations when
and over 2000 senses of it, one for every event and object in       there were less than five. We then took the fraction of
our ontology. Thus, when processing the altered examples,           correct and incorrect interpretation of zzz in this sentence
STAn was able to consider every object and event as                 compared to the overall number of meanings considered
possible meanings of the unknown word zzz. To detect the            (the largest overall number could be, of course, 5). Thus, if
meaning of zzz, the system should interpret the rest of the         the system suggested 2 acceptable results and 3
sentence, according to its ontological knowledge, while             unacceptable ones, we reported 0.4 and 0.6 respectably. If
filling possible interpretations of zzz. The text-meaning           the system suggested only 1 result and it was acceptable, it
representation (TMR) of the original sentence is:                   was still counted as 1.
     (DECIDE                                                           We found that the system suggested unacceptable
        (AGENT(HUMAN(GENDER(FEMALE)))                               meanings of zzz for 34.4% of the 100 senses. Out of the
        (THEME(CONSIDER-INFO(ITERATION(MULTIPLE))
                                                                    65.6% acceptable meanings, 13% were considered to be no
              (AGENT(HUMAN(GENDER(FEMALE)
              (THEME(INFORMATION                                    worse than a human could do.
                 (HAS-TOPIC(CURTAIN(NOVELTY(HIGH))))))                 In some cases (n=5), the analyzer used the intended
              (BEFORE(BUY                                           meaning of the verb in the sample sentence, but switched
                 (THEME(CURTAIN(HAS-LOCALE(HOUSE))))))              the meaning of the verb when zzz was inserted. When the
        )))                                                         sentence made more sense with a different meaning of the
   When zzz is inserted, the TMR becomes:                           verb with the chosen interpretation of zzz (n=2), it was
     (DECIDE
        (AGENT(HUMAN(GENDER(FEMALE)))
                                                                    counted as acceptable.
        (THEME(CONSIDER-INFO(ITERATION(MULTIPLE))                      STAn generally prefers finer grain concepts to the coarser
              (AGENT(HUMAN(GENDER(FEMALE)                           grain. Such a preference achieves the selection of, for
              (THEME(???))                                          instance, a human female over a general animal female in
              (BEFORE(BUY                                           the resolution of an unreferenced usage of the pronoun she.
                 (THEME(???(HAS-LOCALE(HOUSE))))))                  Such a preference, however, usually backfires with the
            )))
                                                                    unknown word task, where it would be smarter to select the
   Looking at the above TMR, the semantic text analyzer
                                                                    most generic concept for the constraints and narrow it down
needs to find the concepts that can satisfy the following:
                                                                    further only when more details are available. It is the
   o it is something that a human can rethink or it is
                                                                    selection of fine grain concepts that dominated the category
        information about something that a human can
                                                                    of the acceptable but not preferred meanings. Thus, the
        rethink
                                                                    example sentence The constantly barking dog infuriated the
   o it is a theme of BUY
                                                                    neighbors, once the word neighbors was substituted with
   o it is located in a HOUSE
                                                                    zzz, led to ‘the constantly barking dog infuriated the wide
   Combining these clues, we have limited knowledge for
                                                                    receiver’ as the first interpretation. ‘Wide receiver’ here
determining a narrow sense of zzz—anything that fits into a
                                                                    refers to a player position in American football, and one can
house and can be bought can work here: furniture,
                                                                    imagine, in principle, a situation where that fury could be
decorative items, wall paint, china, etc. The resulting broad
                                                                    quite possible. The corresponding TMRs for the original
categories highlight the difficulty that a system faces: a
                                                                    example sentence and the zzz are shown below.
concept denoting all décor works as well as that for a                 (anger(experiencer(personal-role))
miniature.                                                                (cause(make-noise(volume(high))(pitch(low))
                                                                              (agent(dog))(iteration(multiple))
                                                                3584

   )))                                                            ontology and the lexicon as well as better grain size
   (anger(experiencer(wide-receiver))                             management within the software should improve the
       (cause(make-noise(volume(high))(pitch(low))                guessing results within a single sentence. Coreference and
            (agent(dog))(iteration(multiple))                     ellipsis resolution will facilitate bringing several sentences
   )))
                                                                  with their clues together and thus further improve the
   A perfect solution was achieved on sentences like He
                                                                  processing and interpretation of unknown words within the
shucked the corn, with the original sentence and the zzz-
                                                                  approach.
replacement interpreted by STAn as:
   (remove(theme(plant-part))
       (agent(human(gender(male))))                                                        References
       (start-location(grain)))                                   Algeo, J. (1977). Blends, a structural and systemic view.
   (remove(theme(plant-part))                                        American Speech, 52(1/2), 47–64.
       (agent(human(gender(male))))                               Algeo, J. (1980). Where do all the new words come from.
       (start-location(seed nut grain)))
                                                                     American Speech 55(4), 264–277.
   At the opposite end of spectrum lie the sentences that
                                                                  Algeo, J. (Ed.) (1991). Fifty years among the new words.
were not interpreted by STAn at a level acceptable for a
                                                                     Cambridge: Cambridge University Press.
human judgment. One such sentence was The engine
                                                                  Algeo, J. (1993). Desuetude among new words.
emitted steam and the substituted version The engine                 International Journal of Lexicography 6(4), 281–293.
emitted zzz. The unacceptable interpretation of zzz was that      Barnhart, D. K. (2007). A calculus for new words.
of shampoo, beer, wine, and yogurt. Such misinterpretations          Dictionaries, 28,132–138.
are typically caused by the unnecessarily relaxed ontological     Beck, I., McKeown, M., & McCaslin, E. (1983). All
constraints on some events. In this case, the event EXUDE
                                                                     contexts are not created equal. Elementary School
(anchoring concept of this sense of emit) has a default theme
                                                                     Journal, 83, 177-181.
of GASEOUS-MATERIAL, or LIQUID-MATERIAL, resulting in
                                                                  Bikel, D., Schwartz, R., & Weischedel, R. (1999). An
the acceptability of the above substances. On the other hand,
                                                                     algorithm that learns what’s in a name. Machine Learning
the almost 2:1 ratio of the acceptable interpretations
                                                                     34, 211–231.
suggests that most of the ontology is well constrained.
                                                                  Bransford, J., & Johnson, M. (1972). Contextual
   We also wanted to know whether direct objects could be
                                                                     prerequisities for understanding: Some investigations of
found using n-grams or other techniques that would take a            comprehension and recall. Journal of Verbal Learning
subject and a verb of the sentence as input and return a             and Verbal Behavior, 11, 717-726.
possible direct object. We randomly selected ten verbs from       Buchholz, S., & Van Den Bosch, A. (2000). Integrating
our sample and ran a subject + verb query against a database         seed names and n-grams for a named entity list and
of English concordances using the Brown, BNC written and
                                                                     classifier. Proc. of the 2nd International Conference on
BNC spoken2 corpora. Only one query out of ten produced a
                                                                     Language Resources and Evaluation Athens, Greece.
non-zero result. Reducing the query to a single word,
                                                                  Campbell, D. A., & Johnson, S. B. (1999). A technique for
indicating the verb, produced seven non-zero results.
                                                                     semantic classification of unknown words using UMLS
   A similar search on Google produced many results, thus
                                                                     resources. Proc. of the AMIA Symposium (pp. 716–720).
lowering a possibility that the selected verbs are not used in
                                                                  Cardie, C. (1993). A case-based approach to knowledge
common speech. While the small number of attempted
                                                                     acquisition for domain-specific sentence analysis. Proc. of
queries against the corpora should not be taken as a
                                                                     the Eleventh National Conference on Artificial
conclusive result, the number can be used as an indication of        Intelligence (pp. 798–803).
failure of finding appropriate words using non-conceptual         Choudhury, M., Saraf, R., Jain, V., Mukherjee, A., Sarkar,
representation (even for computational purposes). Thus, at           S., & Basu, A. (2007). Investigation and modeling of the
least in guessing unknown words, some form of conceptual             structure of texting language. International Journal of
representation and conceptual hierarchy should be used for
                                                                     Document Analysis and Recognition, 10(3/4),157–174.
an attempt of approaching human-level competence.
                                                                  Ciaramita, M. (2002). Boosting automatic lexical acquisi-
                                                                     tion with morphological information. Proceedings of the
                             Summary                                 Workshop on Unsupervised Lexical Acquisition, Philadel-
We have demonstrated on an admittedly restricted purview             phia, PA (pp. 17–25).
that a meaning-based computational system of language             Ciaramita, M., & Johnson, M. (2003) Supersense tagging of
understanding is capable of guessing the meaning of                  unknown nouns in WordNet. Proc. of EMNLP (pp. 594-
unknown words from the context, with the clues determined            602).
similarly to the way humans approach it. In the illustrated       Collins, M., & Singer, Y. (1999). Unsupervised models for
case, the context consisted primarily of the ontologically           named entity classification. In P. Fung and J. Zhou (Eds.),
defined meaning of the known words directly related to the           Proc. of EMNLP/VLC’99. College Park, MD: ACL.
target word syntactically. Further improvements in the            Cook, P. (2010). Exploiting linguistic knowledge to infer
                                                                     properties of neologisms. Ph.D. thesis, University of
   2
                                                                     Toronto.
     http://www.lextutor.ca/concordancers/concord_e.html
                                                              3585

Cucerzan, S., & Yarowsky, D. (1999). Language indepen-           Raskin, V., Buck, B., Keen, A., Hempelmann, C. F., &
  dent named entity recognition combining morphological            Triezenberg, K. E. (2008). Accessing and manipulating
  and contextual evidence. In P. Fung and J. Zhou (Eds.).          meaning of textual and data information for information
  Proc. of EMNLP/ VLC’99, College Park, MD: ACL.                   assurance and security and intelligence. In F. Sheldon
Curran, J, R. (2005). Supersense tagging of unknown nouns          (Ed.), Proc. of the Fourth Cyber Security and Information
  using semantic similarity. Proc. of ACL, Ann Arbor, MI:          Intelligence Research Workshop (CSIIRW’08), ACM
  (pp. 26–33).                                                     Digital Library.
Dijk, T. A. van & Kintsch, W. (1983). Strategies of              Raskin, V., Hempelmann, C. F., & Taylor, J. M. (2010).
  discourse comprehension. Orlando: Academic Press.                Guessing vs. knowing: The two approaches to semantics
Gipe, J. (1979). Investigating techniques for teaching word        in natural language processing. In A. E. Kibrik (Ed.),
  meanings. Reading Research Quarterly, 14, 624-644.               Proc. of Annual International Conference Dialogue.
Granger, R. H. (1977). FOUL-UP: A program that figures           Rieder, A. (2002). A cognitive view of incidental
  out the meanings of words from context. Proc. of the Fifth       vocabulary acquisition: from text meaning to word
  International Joint Conference on Artificial Intelligence,       meaning. VIEWS, 11(1-2), 53-71.
  Cambridge, MA (pp. 172–178).                                   Schatz, E., & Baldwin, R. S. (1986). Context clues are
Hastings, P. M. & Lytinen, S. L. (1994). The ups and downs         unreliable predictors of word meanings. Reading
  of lexical acquisition. Proc. of the Twelfth National            Research Quarterly, 21, 439-453.
  Conference on Artificial Intelligence (pp. 754–759).           Sheidlower, J. T. (1995). Principles for the inclusion of new
Hempelmann, C. F., Taylor, J. M., and Raskin, V. (2010).           words in college dictionaries. Dictionaries, 16, 33–44.
  Application-guided ontological engineering, H. A.              Simpson, J. (2007). Neologism: The long view.
  Arabnia, D. de la Fuente, E. B. Kozerenko, and J. A.             Dictionaries, 28:146–148.
  Olivas (Eds.), Proc. of International Conference on            Smarr, J., & Manning, C. D. (2002). Classifying unknown
  Artificial Intelligence.                                         proper noun phrases without context. Technical Report
Johnson-Laird, P. N. (1983). Mental models. Cambridge,             dbpubs/2002-46, NLP Group, Stanford, CA: Stanford
  MA: Harvard University Press.                                    University.
Lehrer, A. (2003). Understanding trendy neologisms. Italian      Taylor, J. M., Raskin, V., & Hempelmann, C. F. (2010). On
  Journal of Linguistics, 15(2), 369–382.                          an automatic acquisition toolbox for ontologies and
McKeown, M. (1985). The acquisition of word meaning                lexicons in Ontological Semantics, International
  from context by children of high and low ability. Reading        Conference on Artificial Intelligence.
  Research Quarterly, 20, 482-496.                               Taylor, W. L. (1953). Cloze procedure: A new tool for
Mikheev, A. (1997). Automatic rule induction for unknown-          measuring readability. Journalism Quarterly, 30, 415–
  word guessing. Computational Linguistics, 23(3):405–             433.
  423.                                                           Wagner, R. K, Muse, A. E., and Tannenbaum, K. R. (Eds.)
Murawaki, Y., & Kurohashi, S. (2008). Online acquisition           (2006). Vocabulary Acquisition: Implications for Read-
  of Japanese unknown morphemes using morphological                ing Comprehension. New York, NY: Guilford Press.
  constraints. Proc. of EMNLP (pp. 429–437).                     Weischedel, R. M., Schwartz, R. M., Ramshaw, L. A., &
Nadeau, D., & Sekine S. (2009). A survey of named entity           Palmucci, J. (1993). Coping with ambiguity and unknown
  recognition and classification. In S. Sekine & E.                words through probabilistic models. Computational
  Ranchhod (Eds.), Named entities: Recognition,                    Linguistics, 19(2), 359-382.
  classification and use. Amsterdam: John Benjamins.             Wilks, Y. (1978). Making preferences more active. Artificial
Nagy, W. E., Herman, P. A., & Anderson, R. C. (1985).              Intelligence, 11(3), 197–223.
  Learning words from context. Reading Research                  Wilks, Y., & Catizone, R. (2002). Lexical tuning. Proc. of
  Quarterly, 20, 233-253.                                          CICLING 2002 (pp. 106–125).
Nagy, W. E., Anderson, R. C., & Herman, P. A. (1987).            Wisniewski, E. J. (1996). Construal and similarity in
  Learning word meanings from context during normal                conceptual combination. Journal of Memory and
  reading. American Educational Research Journal, 24(2),           Language, 35, 434–453.
  237-270.                                                       Wisniewski, E. J. (1997a). When concepts combine.
Nation, I. S. P. (2006). How large a vocabulary is needed          Psychonomic Bulletin and Review, 4, 167–183.
  for reading and listening? Canadian Modern Language            Wisniewski, E. J. (1997b). Conceptual combination:
  Review, 63, 59-82.                                               Possibilities and esthetics. In T. B. Ward, S. M. Smith, &
O’Donovan, R., & O’Neil, M. (2008). A systematic                   J. Vaid (Eds.), Creative thought: An investigation of
  approach to the selection of neologisms for inclusion in a       conceptual structures and processes (pp. 51–81).
  large monolingual dictionary. Proc. of the 13th Euralex          Washington, DC: APA.
  International Congress (pp. 571–579).                          Wisniewski, E. J. (2000). Similarity, alignment, and
Rankin, E., & Overholser, B. (1969). Reaction of                   conceptual combination: Reply to Estes and Glucksberg.
  intermediate grade children to contextual clues. Journal of      Memory & Cognition, 28, 35–38.
  Reading Behavior, 2, 50-73.
                                                             3586

