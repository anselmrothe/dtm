UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Do you have to look where you go? Gaze behaviour during spatial decision making

Permalink
https://escholarship.org/uc/item/9n91h72n

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)

Authors
Wiener, Jan
De Condappa, Olivier
Holscher, Christoph

Publication Date
2011-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Do you have to look where you go?
Gaze behaviour during spatial decision making
Jan M. Wiener (jwiener@bournemouth.ac.uk)
Department of Psychology, Bournemouth University
Poole, BH12 5BB, UK

Olivier De Condappa (odcondappa@bournemouth.ac.uk)
Department of Psychology, Bournemouth University
Poole, BH12 5BB, UK

Christoph Hölscher (hoelsch@cognition.uni-freiburg.de)
Center for Cognitive Science, University of Freiburg,
Friedrichstraße 50, 79098 Freiburg, Germany
Abstract
In this paper we present an eye-tracking experiment
investigating the relation of gaze behavior, spatial decision
making and route learning strategies. In the training phase
participants were passively transported along a route
consisting of 18 intersections. Each intersection featured two
landmarks, some of which were unique while others were
non-unique. In the test phase participants were presented with
static images of the intersections and had to indicate the
direction in which the original route proceeded. We report
systematic gaze bias towards the eventually chosen movement
direction. Furthermore, we demonstrate that by dissociating
the decision relevant information from the location to which a
response is directed, these gaze bias effects can be
systematically modulated. The results provide novel insights
into how attentional processes mediate performance in a route
memory task and are related to current theories of visual
decision making.
Keywords: Spatial cognition, route learning, eye-tracking,
gaze bias, decision making

Introduction
The relation between decision making and gaze behavior
has primarily been studied in a non-spatial context. Shimojo
and colleagues (2003), for example, demonstrated that
when asked to choose the most attractive face, participants
display a gaze bias towards the eventually chosen face in the
last second before they report their decision (Shimojo,
Simion, Shimojo, & Scheier,, 2003). Glaholt & Reingold
(2009) have recently demonstrated that such gaze bias
effects are not specific to preference choices but constitute a
more general phenomenon of visual decision making. The
gaze-cascade model provides a theoretical framework for
gaze bias effects. It states that gaze does not merely reflect
preferences, but is involved in the formation of preference
in that gaze orientation towards a stimulus and preference
for that stimulus are linked in a positive feedback loop
(Shimojo et al., 2003; Simion & Shimojo, 2006).

In recent studies, Wiener, Büchner, Hölscher, and
Konieczny (2009, under review) reported similar gaze bias
effects in the context of wayfinding. In these studies,
participants were presented with static screenshots of
decision points in complex architectural environments. In
one experiment, the participants’ task was to decide between
path options in order to find an object hidden in the
environment. In a second experiment, participants were first
informed about which path option to follow as if following a
guided route. They were then presented with the same
images and had to indicate which path option they chose
during initial exposure. Both experiments revealed a robust
gaze bias towards the eventually chosen path options.
Results from these studies provide first evidence for gaze
bias effects in the context of navigation and wayfinding.
Furthermore, the fact that the temporal dynamic of the gaze
bias was influenced by the wayfinding task (Wiener et al.,
under review) suggests that the analysis of gaze behavior is
a promising mean to investigate higher-level cognitive
functions and processes involved in wayfinding behavior.
The current study aims to further investigate the
relationship between decision making and gaze bias effects
in a spatial context. Specifically, we are interested in
developing a better understanding of how the positioning of
decision-relevant information and the placement of the
actual choices relate. To the best of our knowledge, in the
studies investigating gaze bias effects so far, the information
relevant for the decision and the choice options coincided
spatially. For example, when deciding which of two faces
depicted on images is more attractive, the eventually chosen
picture holds information relevant for that choice (e.g.
Shimojo et al., 2003). Using a route-learning paradigm we
spatially dissociated the information relevant for the
movement decision from the actual path option that has to
be chosen. The following scenario best illustrates this:
Imagine learning a route through a novel environment. At a
particular intersection along the route you may retrieve the
required movement response as: “Turn right at the yellow
house”. Depending on whether the yellow house – i.e. the

1583

landmark that allows you to recognize the particular
intersection – is located at the right or left side of the
intersection, the decision relevant information either
coincides with the required movement response or is
spatially dissociated from the required movement response.
This only holds true if the intersection is approached from
the same direction as during initial exposure. The question
of how people integrate information about the local
configuration of landmarks at an intersection with route
knowledge – which would allow them to also continue a
route when approaching a place from a different direction –
is beyond the scope of the current study.
Systematically manipulating whether or not the decisionrelevant information coincides with the required response
will allow us to investigate in more detail whether gaze bias
effects reflect the intake of relevant information and the
decision making process itself, or whether gaze bias effects
are also related to the process of reporting the outcome of a
decision. In addition, this manipulation will allow us to
investigate the cognitive strategies participants employ
during route learning.
Predictions
If gaze bias effects reflect information intake and the
decision making process as suggested by the cascade model
(Shimojo et al. 2003), the gaze bias should be directed
towards the decision relevant information, independent of
whether or not this information spatially coincides with the
option that has to be chosen.
However, if gaze bias also reflects the process of
reporting a decision, a gaze bias towards the eventually
chosen path option is expected, independent of whether or
not this options spatially coincides with the decision–
relevant information.

Methods
Virtual Environment
Using virtual environment technology (Vizard 3.0 by
WorldViz) we created a route consisting of 18 intersections
that were connected by corridors 30 m in length. Each
intersection could be identified by landmarks – i.e. images
of different animals – located at either side of the
intersection (see Figure 1). During the experiment,
participants were passively transported along a route at
3m/sec; at each intersection they experienced either a left
turn or a right turn and a total of 9 left turns and 9 right
turns.
The 18 intersections could be subdivided into two classes:
six of the intersections featured two unique landmarks – i.e.
landmarks only present once in the entire environment (UU
intersections); 12 of the intersections contained a unique
landmark and a non-unique landmark (UX intersections).
The non-unique landmark was always the same image of a
pig. The UX intersections were further subdivided into six
UX+ and six UX- intersections. At UX+ intersections
participants experienced a turn in direction of the unique
animal, at UX- intersections participants experienced a turn
in direction of the non-unique animal (for a summary of the
types of intersections, see Table 1). For the purpose of this
study it is important to note that distinguishing between
different UX intersections requires attending to the unique
landmark. This is true for both UX+ and UX- intersections.

Type
6 x UU
6 x UX+
6 x UX-

Table 1: Types of intersections
Landmarks
Turn-Direction
2 unique
Towards unique
1 unique; 1 non-unique
Towards unique
1 unique; 1 non-unique
Towards nonunique

Participants
17 participants (12 women) aged 18 to 28 (M = 19.53, SD
= 2.35) took part in the experiment. They were mainly
students from Bournemouth University and received course
credit compensation for their participation.

Figure 1: intersection in the virtual environment with 2
landmarks.

Procedure
The experiment consisted of six experimental blocks;
each block consisted of a training phase and a test phase.
In the training phase, participants were passively
transported along the entire route with a movement speed of
3m/sec. They initiated the training phase by pressing the
SPACE bar and were instructed to learn the route.
In the test phase, participants were presented with
screenshots of all 18 intersections in random order. They
were informed about the random presentation order and
their task was to indicate the direction in which the original
route proceeded as quickly and as accurately as possible by
pressing either the left or the right arrow key. Performance

1584

(correct choices), response time and gaze behaviour was
measured.

main effect of the type of intersection (F(2,32)=1.79, p=.18,
partial – η² = .10), or a significant interaction
(F(10,160)=1.17, p=.31, see Figure 2).

Experimental Setup
The stimuli were displayed at a resolution of 1024 x 768
pixels on a 20“ CRT monitor. The screen refresh rate was
100 Hz. Participants sat in front of the monitor at a distance
of ~60 cm, such that the resulting visual angle of the
monitor was 37 degrees (horizontally) x 28 degrees
(vertically). Eye movements were recorded using a SR
Research Ltd. EyeLink 1000 eye tracker sampling pupil
position at 500 Hz. The participant’s head was constrained
using a chin rest. The eye-tracker was calibrated using a 9point grid. A second 9-point grid was used to calculate the
accuracy of the calibration. Fixations were defined using the
detection algorithm supplied by SR Research.
Analysis Gaze Behavior
For each stimulus two interest areas equally dividing the
image in a left part and a right part were defined. Fixations
were assigned to the different interest areas. For the time
course analyses – i.e. the analyses of the likelihood that the
eventually chosen part of the stimulus was inspected – we
removed all fixations towards the central interest area,
retaining only fixations towards the two path options.

Figure 3: Response time decreases from experimental block
to experimental block (error bars represent SEM).
Mean Response Time. Only trials in which participants
responded correctly entered this response time analysis. An
ANOVA (factors: experimental block [1-6] & type of
intersection [UU, UX+, UX-]) revealed significant main
effects of both experimental block (F(5, 85.30)=34.77,
p<.001, partial – η² = .67) and type of intersection (F(2,
36.35)=9.00, p<.001, partial – η² = .33), but no significant
interaction (p=.44) on mean response times. Specifically,
response time decreased over experimental trials. Most
importantly, Bonferroni corrected pairwise comparisons
revealed that response times for UX- trials was significantly
longer (1619msec) than for UU (1422msec; p=.001) or UX+
trials (1312msec; p<.001), while response times for UU and
UX+ trials did not differ (p=.10, see Figure 3).

Results

Figure 2: Performance (correct responses) increases over the
experimental blocks (error bars represent SEM).

Behavior
Performance. On average participants chose the correct
direction in 85% of the trials. An ANOVA (factors:
experimental block [1-6] & type of intersection [UU, UX+,
UX-]) revealed a significant main effect of experimental
block (F(5,80)=22.20, p<.001, partial – η² = .58), but no

Figure 4: Likelihood that the eventually chosen part of the
stimulus is inspected for all types of intersections (error bars
represent SEM).

1585

Figure 5: Upper panel: Example stimuli of the test phase with fixation patterns (heat maps); Movement direction is
indicated by the arrow, superimposed on the images (not part of original stimulus); Left: UU intersection with 2 unique
landmarks; Middle: UX+ intersection with the unique landmark on the left hand side (i.e. in movement direction); Right: UXintersection with the non-unique landmark on the right hand side (i.e. opposite to movement direction; Lower panel: Gaze bias
over time, from 1500msec before the decision was reported.

Eye-tracking
As seen in Figure 4, fixations during the test phase were
primarily targeted at the signs displaying the animals. Most
of the fixations towards the central area result from the
1500msec phase prior to the onset of the actual stimulus
during which participants were required to attend to a
fixation cross in the center of the screen.
Gaze Bias Analysis. In order to analyze systematic gaze
bias effects, we synchronized the eye-tracking data between
(correct) trials at the time when the decision was reported –
i.e. when participants pressed either the left or right arrow
key. Given that the average response time for the different
types of intersections ranged between ~1300msec and
1600msec, the following analyses concentrated on the last
1500msec before the response was reported. This data was
then split into 30 intervals each covering 50msec.
For each 50msec interval we calculated the likelihood that
participants’ gaze was directed towards the (eventually)
chosen part of the stimulus. This value ranged from 0 to 1.
The gaze bias analysis revealed systematic gaze bias effects.
Specifically, for all types of intersections (UU, UX+, UX-)
participants demonstrated a gaze bias in the movement
direction, reaching its maximum around the time when the
decision was reported (see Figure 4).
An ANOVA (factors: time to report decision [50msec
intervals from 1500msec before the response until the
response] & type of intersection [UU, UX+, UX-]) revealed
significant main effects of both time to report decision

(F(30, 479.1) = 28.52, p < .001 , partial – η² = .64) and type
of intersection (F(2, 32.05) = 66.76, p < .001 , partial – η² = .
81). The interaction did not reach statistical significance
(F(60, 947) = 1.33, p=.053, partial – η² = .08). The overall
gaze bias averaged over 1500msec was stronger at UX+
intersections (mean: .70) followed by UU (mean: .59) and
UX- intersections (.43). Pairwise comparisons demonstrate
significant differences between all three conditions (all
comparisons p<.001).
A closer inspection of the gaze bias data (see Figure 4)
suggests two distinct phases: In the early phase, there is
little dynamic in the gaze bias. Only in the late phase does
the likelihood that participants inspect the eventually chosen
part of the stimulus change dramatically. We therefore split
the data at -750msec and reanalyzed the early phase and the
late phase independently.
Early phase (-1500msec – -750msec): An ANOVA
(factors: time to report decision [-1500 – -750 msec] & type
of intersection [UU, UX+, UX-]) revealed s significant main
effect of the type of intersection (F(2,32.20)=22.24, p<.001,
partial – η² = .58) but neither a main effect of time to report
decision (p=.97) nor an interaction (p=.96). Specifically,
the overall gaze was stronger at UX+ intersections (mean: .
61) followed by UU (mean: .50) and UX- intersections
(.34). Pairwise comparisons demonstrate significant
differences between all three conditions (all comparisons
p<.001).
Late phase (-750msec – 0msec): An ANOVA (factors:
time to report decision [-750 – 0 msec] & type of
intersection [UU, UX+, UX-]) revealed significant main

1586

effects of both time to report decision (F(15, 240) = 96.40,
p < .001 , partial – η² = .86) and type of intersection (F(2,
32) = 79.84, p < .001 , partial – η² = .83), as well as a
significant interaction (F(30, 480) = 5.17, p<.001, partial –
η² = .24).
Separately analyzing the data for the early and late phase
demonstrates a stable offset in the likelihood that the
eventually chosen option is inspected between all the
different types of intersections, but no changes in the
temporal dynamics. Only in the late phase does the
likelihood systematically increase and differences in the
temporal dynamic are observed between conditions.
This is also apparent in Figure 5: At UU intersections,
participants distribute their gaze evenly between the two
sides of the image until ~800msec before reporting their
decision. They then display what we refer to as a positive
gaze bias – i.e. a gaze bias in direction of the eventually
chosen side. At UX+ intersections, participants display a
positive gaze in both the early phase and the late phase. At
UX- intersections, in contrast, participants show a negative
gaze bias during the early phase – i.e. they spend more time
inspecting the part of the image that they do not choose, but
that contains the distinctive information needed to identify
the current location and thus the required movement choice.
Only at the end of the late phase do participants display a
positive gaze bias.

Discussion
In this study we investigated gaze bias effects in the context
of spatial decision making. Participants were navigated
along a route consisting of left or right turns along 18
intersections. Their task was to remember the route and to
replicate the turns in a subsequent test phase in random
order. Participants’ gaze behavior was recorded while they
decided which of two path options – left or right –
corresponded to the training route. The different
intersections could be identified by landmarks – i.e. by
images of animals displayed on signs that were mounted to
the left and right side of the intersection (see Figure 1). Each
intersection featured two landmarks. Intersections of type
UU featured two unique landmarks, while intersections of
type UX+ and UX- always featured a unique and a nonunique landmark. Identifying a specific UX intersection
therefore required attending to the unique landmark. UX+
and UX- intersections differed in the response required at
these intersections. Movement was required in the direction
of the unique landmark at UX+ intersections and in the
direction of the non-unique landmark at UX- intersections.
The behavioral results clearly demonstrate that
participants could learn the route successfully. In the test
phase of the first experimental block participants already
reached average performance levels of about 65% correct
responses. In the test phase of the third experimental block
performance was close to 90%, and above 90% thereafter.
While performance increased over experimental blocks,

response times decreased from around 2500msec in the first
block to just over 1000msec in blocks five and six.
However, in contrast to performance, response times
differed significantly between types of intersections.
Average response time was similar between UU and UX+
intersections but was ~200msec longer on UXintersections. This difference in response time between
intersections is not easily explained by the main theories of
route learning. Route knowledge is often conceptualized as
a series of recognition triggered responses (Trullier, Wiener,
Berthoz, & Meyer, 1997) in which the recognition of a place
– for example by recognizing a landmark or a snapshot –
triggers a particular movement response such as ‘turn left’
(i.e. landmarks serve as associative cues).
Waller and Lippa (2007) have recently suggested another
route learning strategy in which participants simply recall
landmarks that are located in movement direction (beacon
strategy). From a memory perspective this strategy requires
no explicit learning of a potentially arbitrary association
between a landmark and the action, as the movement
direction can be derived from the landmark location. While
this strategy could, in principle, be also applied in the
current route-learning paradigm, it would fail at UXintersections at which the landmark in movement direction
is not unique to the intersection. We did, however, not find a
difference in performance between the different types of
intersections. It is conceivable that participants relied on the
beacon strategy only in cases where that was a sufficiently
safe strategy. The present study is not explicitly designed to
test strategy shifts in the Waller and Lippa (2007) paradigm.
Whether attentional shifts in the gaze behavior for UXintersections can be tied to such differences in strategies
remains an open issue for further studies.
A comparison of participants’ gaze behavior at the
different types of intersections provides a possible
explanation for the observed difference in response times.
Participants displayed a gaze bias in direction of the
eventually chosen path option – a positive gaze bias – in the
last few hundred milliseconds before reporting the response
for all types of intersections. This was expected for UU and
UX+ intersections as the landmark that was presented on the
corresponding side of the intersection was unique, allowing
participants to unambiguously identify the intersection
along with the required movement response. At UXintersections, however, the landmark in direction of
movement is a non-unique landmark. In order to identify the
intersection and retrieve the required movement response,
participants had to inspect the landmark situated opposite
the required movement direction. This is expressed in a
negative gaze bias from 1500msec until ~400msec before
responding (see Figure 4 and Figure 5). However, instead of
responding while or immediately after picking up the
decision-relevant information, participants shifted their gaze
in the direction of movement. This suggests that they in fact
had to look in direction of (intended) motion. The difference
in response time would then simply reflect the process of
shifting attention towards the intended movement direction

1587

before the response can be given. A number of earlier
studies have reported anticipatory gaze behavior in the
direction of motion (e.g. Grasso, Prevost, Ivanenko, &
Berthoz, 1998; Land & Lee, 1994). In these studies,
participants were actively moving through the environment
and anticipatory gaze behavior is assumed to be involved in
the control of locomotion or steering. In the test phase of
our study, however, participants were not actually moving
through the environment, they were inspecting static
images. So from a perspective of rational analysis of
behavior, the attentional shift was not strictly necessary.
One might speculate that participants are so used to shifting
their attention to the movement direction that they do this
automatically, even if it takes additional time. The final
attentional shift in UX- does provide the cognitive agent
with an opportunity to double-check whether counterevidence against the associated movement decision is
present in the movement direction. Yet the current study
does not include such negative cases, and furthermore such
a double-check strategy is not found in the UX+ case either.
The observation is also clearly compatible with common
coding approaches (e.g. Prinz 1997) which assert that
perception and motor actions share core processes and
representations. In that sense the final visual shift
synchronizes the perceptual input with the anticipated
movement direction. Further research is clearly needed to
appropriately untangle the reasons for the shift in gaze at
UX- intersections.
The results of this study also have implications for visual
decision making on a more general level. As mentioned in
the introduction, the gaze-cascade model for visual decision
making states that an orienting bias – i.e. a gaze bias –
effectively results in a preference decision for a particular
choice based on a positive feedback loop involving
exposure and preferential looking (Shimojo et al., 2003),
even if the visual stimulus is removed during decision
making (Simion & Shimojo, 2007). However, a first
analysis of the gaze behavior at UX- intersections in this
study demonstrates both negative and positive gaze bias
effects. A relatively steady negative bias – directed towards
the decision-relevant unique landmark – is observed from
1500msec until ~500msec before the decision. Only
afterwards do participants display a positive gaze bias. In
contrast, participants display a positive gaze bias during the
entire 1500msec period at UX+ intersections. Moreover, the
positive gaze bias at UX- intersections builds up much
quicker than at UU or UX+ intersections. These results
demonstrate that dissociating the decision relevant
information from the location to which a response is
directed can systematically modulate gaze bias effects.

Conclusion

when the relevant stimulus information mismatched the
required movement direction. Gaze analysis suggests that
the reaction time can be broken down into separate
processes of stimulus processing and action preparation that
require an attention shift in the spatially mismatched
condition. While this provides a new piece in the puzzle of
understanding landmark processing in route navigation, it
also suggests that spatially dissociating relevant stimulus
information and the required overt reaction can be used to
further scrutinize gaze bias effects and the gaze-cascade
model of Shimojo and colleagues (2003).

References
Glaholt, M. G., & Reingold, E. M. (2009). The time
course of gaze bias in visual decision tasks. Visual
Cognition, 17(8), 1228 – 1243.
Grasso, R., Prevost, P., Ivanenko, Y., & Berthoz, A.
(1998). Eye-head coordination for the steering of
locomotion in humans: an anticipatory synergy.
Neuroscience Letters, 253, 115–118.
Land, M., & Lee, D. (1994). Where we look when we
steer. Nature, 369(6483), 742-744.
Montello, D. R. (2001). Spatial cognition. In
International encyclopedia of the social & behavioral
sciences (p. 14771-14775). Oxford: Pergamon Press.
Prinz, W. (1997). Perception and action planning.
European Journal of Cognitive Psychology, 9, 129-154.
Simion, C., & Shimojo, S. (2006). Early interactions
between orienting, visual sampling and decision making in
facial preference. Vision Research., 46, 3331–3335.
Simion, C., & Shimojo, S. (2007). Interrupting the
cascade: Orienting contributed to decision making even im
the absence of visual stimulation. Perception &
Psychophysics, 69(4), 591-595.
Shimojo, S., Simion, C., Shimojo, E., & Scheier, C.
(2003). Gaze bias both reflects and influences preference.
Nature Neuroscience, 6, 1317–1322.
Trullier, O., Wiener, S. I., Berthoz, A., & Meyer, J.
(1997). Biologically based artificial navigation systems:
Review and Prospects. Progress in Neurobiology, 51, 483544.
Waller, D. & Lippa, Y. (2007). Landmarks as beacons
and associative cues: their role in route learning. Memory &
Cognition, 35, 910-924.
Wiener, J.M., Büchner, S., Hölscher, C., & Konieczny, L.
(under review). Space Perception, Spatial Decision Making,
and Wayfinding
Wiener, J.M., Büchner, S., Hölscher, C., & Konieczny, L.
(2009). How the Geometry of Space controls Visual
Attention during Spatial Decision Making. Full Paper
and Oral Presentation at CogSci

To summarize, the present study provides new insights
into how attentional processes mediate performance in a
route memory task. Reaction times for decisions increased

1588

