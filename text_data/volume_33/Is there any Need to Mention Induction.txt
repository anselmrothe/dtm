UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Is there any Need to Mention Induction?
Permalink
https://escholarship.org/uc/item/8kc5270j
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Author
Thorton, Chris
Publication Date
2011-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                              Is there any Need to Mention Induction?
                                                         Chris Thornton
                                                        COGS/Informatics
                                                       University of Sussex
                                                             Brighton
                                                            BN1 9QH
                                                               UK
                                                     c.thornton@sussex.ac.uk
                            Abstract                               references that understanding is necessarily circular. As
                                                                   Hume noted, such explanations 1 end up ‘going in a cir-
   Induction is the process by which seen data becomes the
   basis for prediction of unseen data. There has long been        cle, and taking that for granted, which is the very point
   a desire to explain the procedure in a context-free way.        in question’ (Hume, 1988/1748, p. 80).
   But Hume’s circularity problem and the no-free-lunch               In the modern era, the no-free-lunch (NFL) theo-
   theorems both seem to suggest the logical impossibility
   of any context-free mechanism. Machine Learning takes           rems (Wolpert and Macready, 1997; Wolpert, 1996),
   the position that no such mechanism exists. But an al-          and conservation law of generalization (Schaffer, 1994)
   ternative comes from Epistemology. Popper’s falsifica-          have added fuel to the fire, making it doubtful that any
   tionist theory holds that there is a general mechanism,
   but that it does not perform induction. Inductive ef-           context-free principle could exist. Any such principle
   fects arise implicitly, through pursuit of a non-inductive      should yield induction in all possible scenarios, it is ob-
   goal. Less plausibly, the mechanism is taken to be un-          served. But any principle tested in all possible scenar-
   informed exploration of hypotheses. But as the paper
   shows, Popper’s solution can be reworked using informa-         ios is found to perform at the level of random guessing
   tion theory. Increasing the informational efficiency with       on average. A completely general principle of induction
   which representations predict seen data can be shown            would then seem to have no value for induction in gen-
   to produce inductive effects. With representation op-
   timization taking the place of hypothesis-search in the         eral. Again, the implication seems to be that no such
   argument, it then becomes possible to explain induction         principle can exist.
   in a context-free way.                                             As a result of such arguments, it has become widely
   Keywords: problem of induction; no free lunch (NFL);            accepted that there can be no context-free basis for
   cognitive informatics; theoretical cognitive science            preferring one inductive model over another (Langley,
                                                                   1996; Mitchell, 1997). The outcome has been read-
                        Introduction                               ily accommodated in machine learning. The fact that
Induction has traditionally been equated with the way              multiple models can be derived in different ways from
scientists use observations to form predictions. But a             given data (Michie et al. 1994) is seen to be the reason
broader view is now taken. Induction is understood to              why each ‘learning algorithm has a different inductive
be any process by which seen data comes to be used                 bias, makes different assumptions, and defines a differ-
for prediction of unseen data. On this basis, the proce-           ent objective function’ (Alpaydin, 2010, p. 309). Nei-
dure is understood to play a key role not only in science,         ther has the problem held back theoretical work. The
but also in cognition. Embedded inductive functional-              uniformity assumption that Hume sees as problematic
ity is also observed in sensory, perceptual and adaptive           becomes a methodological requirement in computational
phenomena (Smith, 2000). We even see inductive ef-                 learning theory (Bishop, 2007). This allows induction to
fects in the behaviour of certain forms of plant life. The         be treated as the problem of sampling in a known (e.g.,
behaviour of young sunflowers, for example, exhibits he-           IID) distribution (Mackay, 2003).
liotropism (sun-tracking), involving implicit prediction              In other areas, more difficulty is encountered. Con-
of the sun’s trajectory across the sky.                            sider the effect on Epistemology. The lack of a uni-
   But recognizing that inductive functionality can be             versal principle for induction means there can be no
expressed in a number of different ways does not sig-              assumption-free basis for inductively-derived knowledge.
nificantly improve the prospects for explaining how it             All of science then seems reduced to the status of guess-
works. Understanding induction to involve use of seen              work. Russell appraises the situation in vivid terms. Our
data for predicting unseen data, we are led to envisage            inability to identify any general principle of induction
a mechanism that exploits the latter being in some way             suggests ‘there is no intellectual difference between san-
uniform with the former. Unfortunately, any attempt to             ity and insanity’, and that scientists are on an equal foot-
explain the process in these terms runs into difficulties.         ing with ‘the lunatic who believes that he is a poached
First, there is Hume’s ‘circularity’ problem. Any under-           egg’ (Russell, 1946, p. 673).
standing about uniformity between seen and unseen data                 1
                                                                         Hume referred to predictions of cause/effect relationships
must itself be inductively derived. An argument which              rather than induction in general.
                                                               3483

   The difficulties are not merely epistemological in na-          entail data compression is more general than taking it
ture, however. The idea that induction performed in                to entail hypothesis elimination, but not much. Refer-
a context-free way is necessarily unprincipled leads to            encing the principle of Occam’s Razor, we might argue
counter-intuitive conclusions. Any agent (or embedded              that data compression is what scientists are really do-
module) embarking on the process must then be making               ing when they believe they’re doing induction. But even
some kind of random choice between context-sensitive               if this proposal is accepted, the assumed complexity of
approaches. This seems at odds with what is observed               information processing seems incompatible with obser-
in the natural world.                                              vations about ways more primitive agencies behave. Ul-
   Such are some of the problems stemming from                     timately, the proposal seems to break down. The idea
the much-debated problem of induction (Sloman and                  that sunflowers produce inductive effects by means of
Lagnado, 2005). Can any viable solution for the problem            data compression, for example, seems outlandish.
be worked out? Prominent among the proposals put for-                 As the present paper argues, there is a version of this
ward is Popper’s falsification framework (Popper, 1959;            argument that can be made to work, however. Instead
Popper, 1979). This solve’s Hume’s circularity problem             of taking the inductive vehicle to be compression, we
by showing how quasi-inductive effects can arise implic-           can take it to be representation optimization. Deploying
itly, through application of a procedure that makes no             concepts of information theory (Shannon, 1948; Shan-
assumptions about uniformity. The vehicle proposed is              non and Weaver, 1949), inductive effects can be shown
falsification: systematic elimination of refuted hypothe-          to arise when there is any increase in the informational
ses. This is put forward as a fully context-free method for        efficiency with which representations predict seen data.
achieving (implicitly) inductive results without requiring         Representation optimization can then be viewed as a
any (explicitly) inductive step to be taken. There is then         principled, well-motivated but non-inductive procedure
‘no need even to mention induction’ (Popper, 1979, p.              that yields inductive effects implicitly. As such, it is able
315) in explaining how it works.                                   to take the place of falsification in Popper’s argument.
   A difficulty for Popper’s proposal is that hypothesis              The workability of representation optimization as a
falsification is not seen to be a plausible vehicle for in-        vehicle for induction can be demonstrated in two ways:
duction in general. It does not reflect the real practices         either directly, using illustrations, or indirectly, by show-
of science (Kuhn, 1962; Lakatos, 1970), is unworkable              ing the process to be an implicit compression method.
where the number of hypotheses is large (Hempel, 1945;             The improvement in descriptive plausibility is more read-
Churchland, 1986; Duhem, 1954/1914; Putnam, 1974;                  ily apparent. Agents are no longer envisaged to be
Quine, 1953)2 and seems highly inappropriate as a de-              engaging in complex forms of information processing.
scription of the behaviour of more primitive forms of              Rather, they are seen to gravitate towards more efficient
agency. On the other hand, there seems nothing wrong               representation of seen data. In then becomes possible
with Popper’s strategy. If science can be shown to be ap-          to give an account of inductive behaviour that more suc-
plying a certain procedure that yields inductive success           cessfully generalizes the activities of scientists with more
without a uniformity assumption being made, Hume’s                 primitive forms of agency.
circularity is eliminated. The problem is that falsifica-             The paper sets out the proposal in more detail.
tion doesn’t quite fit the bill. Is there a way to recon-          The following section sets out the information-theoretic
struct the argument using some other vehicle?                      model through which representation optimization is
   Turning to the machine learning literature for ideas            shown to produce inductive effects implicitly. Following
about what this vehicle could be, we find a promising              that, there is a section presenting illustrative examples.
candidate in the form of data compression. Identifica-             The paper then concludes with a brief discussion of im-
tion of compression as a vehicle for induction has long            plications.
been a key part of thinking on learning. Through the
work of researchers such as Solomonoff (1964), Watan-               Efficient reconstruction of symbolic data
abe (1969), Wolff (1980), Rissanen (1978), Chater (2003)
and many others, the idea has been developed into a                The proposal is that increasing the informational effi-
major paradigm of the field. Given the general worka-              ciency with which representations predict seen data pro-
bility of compression as a vehicle for induction, can we           duces inductive (and compressive) effects implicitly. In
strengthen Popper’s proposal by replacing falsification            order to demonstrate the effect, some basic definitions
with compression?                                                  are needed. In what follows, D will represent a partic-
   Unfortunately, this move still leaves a residue of de-          ular set of symbolic data. D is assumed to comprise
scriptive implausibility. Taking inductive behaviour to            constructs whose constituents are symbols drawn from
                                                                   an alphabet of n elements. Letting |D| denote the total
    2
      Modern Bayesian approaches to inductive confirmation         number of symbols in D, we can obtain the total in-
follow the practice of Machine Learning in use of closed-world
assumptions (Earman, 1992; Horwich, 1982; Howson and Ur-           formation content using Shannon’s logarithmic measure.
bach, 1989).                                                       Given a symbol with n possible values expresses log n
                                                               3484

bits of information,3 the total content of D is then
                                                                                                         ′
                                                                                  ¯ ′ ) = I(D) − H(D )
                                                                                  I(D                                   (6)
                      I(D) = |D| log n                  (1)                                     c(D′ )
   It is assumed that constituent symbols in constructs of        The informationally optimal reconstruction of D is
D can be indexed, and that where two or more constructs        then that reconstruction that maximizes mean informa-
have the same structure, the combination of those con-         tion. Termed the first refinement of D,4 this is denoted
structs can be referenced explicitly. Such combinations        r(D):
are named unions. If x represents a union, |x| denotes
the number of symbols it utilizes, and xi is the set repre-                        r(D) = argmax I(D¯ ′)                (7)
senting the choice of symbols for the i’th element of the
(common) structure.                                               A constraint on this is that the mean information of
   D′ is then used to denote a reconstruction of D. This       r(D) can be no less than that of D itself. Were this to
is defined to be a modification of D, in which some con-       be the case, D would be its own optimal reconstruction
structs are replaced with symbols representing unions.         by definition. Given r(D) 6= D, it must be the case that
Replacement is deemed feasible just in case the con-
struct is within the represented union. Where replace-                                ¯
                                                                                      I(r(D))     ¯
                                                                                               > I(D)
ments introduce choice (multiple symbols for the same
                                                                  which further dictates that
constituent) there is a well-defined loss of information.
The loss resulting from a replacement by union x is
                                                                                       c(r(D)) < |D|.                   (8)
                              X
                     H(x) =        log |xi |            (2)       Increasing the mean content of symbols above the level
                               i                               they have in D itself must involve reducing their num-
   Equivalently, the loss may be defined as the log of the     ber. Any reconstruction of D must therefore use a lesser
combinatorial product of x’s choices:                          number of symbols than D itself. Forming a more effi-
                                                               cient reconstruction of D thus necessarily produces the
                                   Y
                     H(x) = log        |xi |            (3)    effect of compressing D, as we would expect.
                                    i
                                                                 Inductive properties of reconstructions
   The total information lost in a reconstruction is thus
the sum of information losses of its constituent symbols:      Within the analysis, data D and all its reconstructions
                                                               are ways of predicting D under different levels of infor-
                               X                               mation loss. All represent the same content. Deriving a
                    H(D′ ) =        H(Di′ )             (4)
                                                               reconstruction of D that has higher efficiency than D it-
                                 i
                                                               self, is thus the act of increasing informational efficiency
   Here, H(Di′ ) is zero if Di′ is an original symbol, and     in representation of D’s content. By Eq. 8, this must
the information loss of the represented union otherwise.       have the effect of reducing the number of symbols in
   Where replacement of constructs has the effect of re-       use. This can only be achieved through replacement of
ducing the total number of symbols in use, the symbol          two or more constructs with a union. The informational
cost of the reconstruction must be less than |D|. The          cost of this replacement then depends on the degree to
actual value can be calculated as the number of symbols        which the replaced constructs differ in their constituent
used in the reconstruction itself, added to the total num-     symbols. The greater the similarity between constituent
ber of symbols used in referenced constructs. This cost        symbols in replaced constructs, the lower the informa-
is denoted c(D′ ):                                             tion cost, and the greater the efficiency of the resulting
                                      X                        representation.
                   c(D′ ) = |D′ | +        |x|          (5)       Unions can thus be viewed as implicit generalizations,
                                     x∈D′                      whose informational value increases with the constituent
                                                               similarity of the constructs they replace. The informa-
   Here, x ∈ D′ enumerates the set of unions referenced
                                                               tional efficiency of a representation is increased through
by D′ .
                                                               the introduction of what are, in effect, ‘similarity exploit-
   Combining the reconstruction loss with the recon-           ing’ generalizations. Putting it another way, increasing
struction cost, it is then possible to define the informa-     the efficiency of a representation has the effect of identi-
tional efficiency of a reconstruction. This is the mean        fying (more) effective ways of exploiting commonalities.
information content of symbols, i.e., the net information
content divided by symbol usage:                                   4
                                                                     Not covered in this paper is an extension of the model
                                                               to deal with recursive enhancement, a regime that typically
    3
      Logarithms are taken to base 2 in all cases.             produces a series of refinements.
                                                           3485

   We then begin to see how increasing informational ef-                  -2.0    $0 = small/large white flying/swimming swan
                                                                          -2.0    $0
ficiency produces implicit inductive effects. Any recon-                  -2.0    $0
struction embodies a certain number of unions, and every                          medium white swimming swan
union specifies a choice of symbols for each of its con-                  -2.0    $0
stituents. The reconstruction represents the content of                   -8.0    (40.0-8.0)/12 = 2.67 bits per symbol
the original D with a certain loss of information. At the
same level of loss, however, it represents the content of
any other body of data whose constructs conform to the               The reconstruction is set out schematically. The first
embodied symbol choices. This can be formulated as a             five lines represent the reconstruction itself, with the ver-
rule:                                                            tical ordering corresponding to the listing of the data.
                                                                 Where a replacement is made, we see the relevant in-
                Di′ |= Dj if ∃ Dj′ : Dj′ = Di′            (9)    formation loss on the left, followed by the symbol gen-
                                                                 erated ($0 being the only one used here) to represent
   This asserts that reconstruction      Di′ generalizes data
                                                                 the construct. In the final line of the listing, we see the
Dj just in case there is a reconstruction of Dj which
                                                                 calculation of mean symbol information. Deducting the
is identical to a reconstruction of Di . The predictive
                                                                 aggregate information loss of 8.0 bits from the original
properties of a reconstruction may be formalized in the
                                                                 content of 40.0 bits, and dividing by the 12 symbols in
same way. D′ generalizes and thus (implicitly) predicts
                                                                 use, we obtain a mean of 2.67 bits. The reconstruction
all bodies of data in the set
                                                                 increases informational efficiency (mean symbol content)
                                                                 by 0.67 bits.
                       { d | D′ |= d }                   (10)
                                                                     A still more efficient reconstruction can be obtained,
   Increasing the efficiency of a representation generates       however. Consider
reconstructions that implicitly predict unseen bodies of
                                                                    -2.58     $1 = medium/large/small white flying/swimming swan
data. Such predictions will be valid just to the extent
                                                                    -2.58     $1
that unseen data exhibit constructs that are similarly              -2.58     $1
structured. Representation enhancement thus implicitly              -2.58     $1
predicts unseen data that exhibit structural uniformity             -2.58     $1
with D.                                                             -12.9     (40.0-12.92)/9 = 3.01 bits per symbol
                Illustrative example
                                                                     This yields a mean of 3.01 bits, with the in-
The scenario long favoured for examining induction is
                                                                 troduced symbol ($1 here) representing the union
the case of ‘white swans’. In this example, observations
                                                                 ‘medium/large/small white flying/swimming swan’.
of white swans lead to the conclusion that ‘all swans are
                                                                     In both reconstructions, we see effects of implicit in-
white’. To examine the way in which this conclusion
                                                                 duction. The content of the original data is represented
might arise implicitly from representation enhancement,
                                                                 in terms of unions that create choice about (i.e., general-
we envisage data in the form of attribute vectors:
                                                                 ize) properties of size and behavior. The more efficient of
             large     white      flying      swan               the two embodies the expected generalization ‘all swans
             large     white   swimming       swan               are white’. This is predictive in the sense of predicting
             small     white      flying      swan               the observation ‘medium white flying swan’, a case not
           medium      white   swimming       swan
             small     white   swimming       swan               contained in the original set.
                                                                     Illustrating the same effect in a slightly more com-
                                                                 plex way is the example below. This takes the previous
   Each vector is placed on a separate line here. In left-       data and mixes in additional vectors representing obser-
to-right order, the attributes are size, color, behavior         vations of black ravens.
and type. The attribute of color is always ‘white’, while
other attributes vary, reflecting the observed regularity                        medium       black      flying     raven
that all observed swans are white. Taking each attribute                           large      white      flying     swan
                                                                                   small      white      flying     swan
to have four possible values, the information content of                         medium       black    perching     raven
each original symbol is 2.0 bits. This yields a total in-                          small      white    perching     swan
formation content of 40.0 bits for the data.                                       large      black      flying     raven
                                                                                   large      white    perching     swan
   Taking constructs to be complete attribute vectors,                           medium       white    perching     swan
any union must combine two or more vectors. Con-                                   small      black      flying     raven
stituents of unions are thus choices of symbols for the
four attributes. On this basis, we might build a recon-
struction as follows.                                                An efficient reconstruction in this case is
                                                             3486

    -2.58  $2 = medium/small/large black perching/flying raven     tion is broadened but the final effect remains the same.
   -2.58   $3 = medium/large/small white perching/flying swan
   -2.58   $3                                                      We now focus on inductive functionality broadly defined,
   -2.58   $2                                                      taking into account the possibility of it being expressed
   -2.58   $3                                                      in different forms of agency, and at multiple levels of
   -2.58   $2
                                                                   organization. The difficulty of explaining how such pro-
   -2.58   $3
   -2.58   $3                                                      cesses can predict unseen data is again resolved by show-
   -2.58   $2                                                      ing that prediction is not really what is happening. Such
   -23.26  (72.0-23.26)/17 = 2.87 bits per symbol                  processes are adopting efficient representations of seen
   Here, the original content comes to be represented in           data, no more. The inductive effects that loom large
terms of two unions, one of which captures the general             in our interpretation are then recognized to be implicit.
properties of white swans, and the other of which cap-             Provided the world exhibits uniformity they will be pre-
tures the general properties of black ravens. Implicitly,          dictively successful, however, giving the impression of
the result embodies two inductive generalizations: ‘all            effective induction.
swans are white’ and ‘all ravens are black’. This gener-              The strategy of explaining induction by ‘not mention-
alizes the vectors ‘medium white flying raven’ and ‘small          ing it’ thus yields a reasonable accommodation of the
black perching raven’, neither of which are in the origi-          philosophical problem of induction. Rather than being
nal data. Again, we see the effect of implicit inductive           a flaw in our understanding, it begins to seem more of
prediction.                                                        an artefactual difficulty, resulting from imposition of an-
                                                                   thropocentric interpretations. The salience that the no-
                         Discussion                                tion of prediction has within human concerns may be the
                                                                   origin of a tendency to frame interpretations of adaptive
While the difficulty of establishing a universal basis for         functionality around this particular concept. Adopting
induction has plagued Epistemology for centuries, it is            a more neutral position then has the effect of addressing
now also a source of counter-intuitive conclusions regard-         some of the difficulties arising.
ing induction, broadly defined. Referencing the NFL                   One other feature of the proposal is worth highlight-
results particularly, we have to assume context-free in-           ing. It is an implication of the framework that induc-
ductive behaviour necessarily commences with a random              tion involves compression by definition. The process is
selection of a context-sensitive bias. Referencing Hume’s          then understood to be feasible only with data which
problem of circularity, we have to assume that a univer-           can be compressed. On the Kolmogorov criterion (Li
sal principle — were it to exist — could not be given any          and Vitänyi, 1997), purely random data cannot be com-
coherent definition.                                               pressed, meaning such cases are implicitly ruled out. The
   Popper gets around Hume’s problem of circularity by             proposed method is definitionally incapable of address-
assuming induction is achieved through a non-inductive             ing them. The NFL objection, which relates to average
vehicle. But the implausibility of the vehicle as a general        performance in all possible scenarios, is then avoided. In-
description of inductive behaviour poses a difficulty. In          duction under the present proposal is understood to be
the present proposal, this is resolved by replacing falsifi-       universal, but not in the sense of accommodating ran-
cation with representation optimization. As an implicit            dom data.
form of data compression, this has reasonable inductive
credentials. As a vehicle for implicit induction, it more                                References
easily accommodates the full range of processes and be-
haviours we recognize to be involved. A non-circular               Alpaydin, E. (2010). Introduction to Machine Learn-
account is then forthcoming for a context-free inductive               ing (2nd Edition). Cambridge, Massachusetts: MIT
methodology that has the potential to accommodate all                  Press.
levels of functionality, including the behaviours of scien-
                                                                   Bishop, C. (2007). Pattern Recognition and Machine
tists and sunflowers.
                                                                       Learning. Springer.
   The original Popperian version of the explanation
refers to scientists, and our observations of how they             Chater, N. and Vitänyi, P. (2003). Simplicity: a unifying
seem to use induction to predict future (or otherwise un-              principle in cognitive science. Trends in Cognitive
seen) data. Popper resolves the worry that this process                Sciences, 7 (pp. 19-22).
can have no non-circular explanation by arguing it is not
really happening. Scientists are really engaged in sys-            Churchland, P. (1986). Neurophilosophy. Cambridge,
tematic elimination of refuted hypothesis. The inductive               MA: MIT Press.
element is pure interpretation. Induction is explained by
showing it to be an interpretation of a process that is            Duhem, P. (1954/1914). The Aim and Structure of
principled, advantageous but strictly non-inductive.                   Physical Theory (Original work published 1914).
   In the proposed revision, the scope of the explana-                 Princeton, NJ: Princeton University Press.
                                                               3487

Earman, J. (1992). Bayes or Bust? A Critical Examina-       Russell, B. (1946). History of Western Philosophy. Lon-
    tion of Bayesian Confirmation Theory. Cambridge,           don: George Allen & Unwin.
    MA: MIT Press.
                                                            Schaffer, C. (1994). Conservation law for generalization
Hempel, C. (1945). Studies in the logic of confirmation        performance. Proceedings of the International Con-
    (i.). Mind, 54 (pp. 1-26).                                 ference on Machine Learning (pp. 259-265). July
                                                               10th-13th, Rutgers University, New Brunswick, New
Horwich, P. (1982). Probability and Evidence. Cam-             Jersey.
    bridge: Cambridge University Press.
                                                            Shannon, C. (1948). A mathematical theory of com-
Howson, C. and Urbach, P. (1989). Scientific Reason-           munication. Bell System Technical Journal, 27 (pp.
    ing: The Bayesian Approach. Chicago, IL, US: Open          379-423 and 623-656).
    Court Publishing Co.
                                                            Shannon, C. and Weaver, W. (1949). The Mathematical
Hume, D. (1988/1748). An Enquiry concerning Human              Theory of Communication. Urbana, Illinois: Univer-
    Understanding. La Salle, Illinois: Open Court.             sity of Illinois Press.
Kuhn, T. (1962). The Structure of Scientific Revolu-        Sloman, S. and Lagnado, D. (2005). In K.J. Holyoak and
    tions. Chicago: University of Chicago Press.               R.G. Morrison (Eds.), The Cambridge Handbook of
Lakatos, I. (1970). Falsification and the methodology          Thinking and Reasoning (pp. 95-116). Cambridge,
    of scientific research programmes. In I. Lakatos           UK: Cambridge University Press.
    and A. Musgrave (Eds.), Criticism and the Growth        Smith, C. (2000). Biology of Sensory Systems. New
    of Knowledge (pp. 91-196). Cambridge, England:             York: John Wiley & Sons, Ltd.
    Cambridge University Press.
                                                            Solomonoff, R. (1964). A formal theory of inductive in-
Langley, P. (1996). Elements of Machine Learning. San          ference, part i. Information and Control, 7, No. 1
    Francisco: Morgan Kaufmann Publishers Inc.                 (pp. 1-22).
Li, M. and Vitänyi, P. (1997). An Introduction to Kol-     Watanabe, S. (1969). Knowing and Guessing: A Quan-
    mogorov Complexity and Its Applications: Second            titative Study of Inference and Information. New
    Edition. New York: Springer-Verlag.                        York: Wiley.
Mackay, D. (2003). Information Theory, Inference, and       Wolff, J. (1980). Data compression, generalisation and
    Learning Algorithms. Cambridge: Cambridge Uni-             overgeneralisation in an evolving theory of language
    versity Press.                                             development. Proceedings of the AISB-80 conference
                                                               on Artificial Intelligence. Amsterdam.
Michie, D., Speigelhalter, D. and Taylor, C. (Eds.)
    (1994). Machine Learning, Neural and Statistical        Wolpert, D. (1996). The lack of a priori distinctions
    Classification. Ellis Horwood.                             between learning algorithms. Neural Computation,
                                                               8, No. 7 (pp. 1341-1390).
Mitchell, T. (1997). Machine Learning. McGraw-Hill.
                                                            Wolpert, D. and Macready, W. (1997). No free lunch
Popper, K. (1959). The Logic of Scientific Discovery.
                                                               theorems for optimization. IEEE Transactions on
    London: Hutchinson.
                                                               Evolutionary Computing, 1 (pp. 67-82).
Popper, K. (1979). Objective Knowledge: An Evolution-
    ary Approach (Revised Edition). Oxford: Clarendon
    Press.
Putnam, H. (1974). The “corroboration” of theories. In
    P.A. Schilpp (Ed.), The philosophy of Karl Popper
    (Vol. I) (pp. 221-240). LaSalle, IL: Open Court.
Quine, W. (1953). Two dogmas of empiricism. In
    W.V.O. Quine (Ed.), From a Logical Point of View
    (pp. 20-46). Cambridge, MA: Harvard University
    Press.
Rissanen, J. (1978). Modeling by the shortest data de-
    scription. Automatica, 14 (pp. 465-471).
                                                        3488

