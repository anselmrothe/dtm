UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Transductionally Bounded Hierarchical Systems
Permalink
https://escholarship.org/uc/item/5dm9p66g
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Somers, Sterling
Jeanson, Francis
Publication Date
2011-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                              Transductionally Bounded Hierarchical Systems
                                       Sterling Somers (sterling@sterlingsomers.com)
                            Institute of Cognitive Science, Carleton University, 1125 Colonel By Drive
                                                          Ottawa, On., Canada
                                       Francis Jeanson (fjeanson@connect.carleton.ca)
                            Institute of Cognitive Science, Carleton University, 1125 Colonel By Drive
                                                          Ottawa, On., Canada
                             Abstract                                 scales, as the events at higher levels depend on the events of
                                                                      lower levels. On the larger time scale of the higher-level
   Using a hierarchical-systems analysis, this paper supports the
   orthodox view of the mind. We claim that the orthodox mind         subsystem, the subsystem description of the lower level
   – bounded by brains or bodies – is organized into various          becomes superfluous, provided that the high-level
   system levels, each of which is emergent from the dynamics         subsystem is decomposable. This feature of hierarchies is
   of level below it. We see the extended mind hypothesis as          what Newell refers to when he describes a `strong‟ system
   borrowing terms from a high-level system of the orthodox           level. While for Simon hierarchical systems all share the
   mind and applying it to interactions between high levels of        property of near decomposability, Newell suggests that
   separate hierarchical systems, without providing any lower
   levels on which to ground it.
                                                                      some system levels can be „weak‟. Essentially, a system
                                                                      level is weak when it does not perfectly predict behavior at
   Keywords: transduction, extended mind, levels.                     the level to which it belongs in the hierarchical system. A
                                                                      weak level may be simpler than sublevels but if it is not
                         Introduction                                 decomposable into the sublevels, one may have to recruit
   Following Herbert Simon‟s analysis of complex systems              the sublevels in order to explain certain phenomena.
(Simon, 1962) and Newell‟s related chapter on system                  Important in Newell‟s and Simon‟s analysis is that for each
levels (Newell, 1990); this paper supports the orthodox view          system-level there is also some appropriate language for
of the bounds of the mental (the mental is bounded by the             describing that level.
body) in a non-question begging way. Recognizing that                    Each subsystem (system-level) in a hierarchy is
minds (high-level systems, defined by lower-level                     characterized by the interactions of the components of that
dynamics) interact with other minds or objects in the world,          system-level. The stability of these interactions, what we
not through direct interaction of mind-level (high-level)             refer to as the dynamics of the system-level, is what allows
systems, but through a physical intermediary at a lower-              us – in physical systems at least – to identify those levels. A
level, we show that it is incorrect and misleading to                 system-level, defined by its dynamic, should always be
incorporate within our definition of mind that which extends          distinguishable via some observational measure. Consider,
with other minds or external objects. The orthodox mind is            for example, a tornado1. The dynamics of a tornado (e.g.,
grounded empirically on the levels which emerge from                  the interaction of the air molecules) can be visibly identified
some fundamental level, while no such hierarchy exists in             from the surrounding system (non-tornado air). A tornado
support of minds hypothesized to extend into other minds or           is, then, a plausible candidate of an example of a
objects. In this paper we argue that cognitive systems are            hierarchical system. At a low level there are interactions
bounded by the transduction processes that give rise to the           between air molecules which, presumably, are travelling at
dynamics upon which the hierarchies are based.                        certain speeds, and following certain paths, etc.; at an
                                                                      intermediary level we may distinguish small localized wind
                           Dynamics                                   currents which are formed by aggregates of coherent
   In Simon‟s paper on complex systems (1962), he argues              molecules; while at an even higher level there is the entire
that most (if not all) complex systems are hierarchical               tornado. The description of a tornado in hierarchical terms
systems: a system “that is composed of interrelated                   is particularly useful when high-level. Longer time scale
subsystems, each of the latter being, in turn, hierarchic in          analysis allows the identification, say, of its general location
structure until we reach some lowest level of elementary              and itinerary over time and monitor or even predict its
subsystem” (p. 468). As Simon points out, hierarchical                destructive effects. Without this high-level, low-frequency
systems share a common trait: near decomposability. Near              description it would be nearly (if not actually) impossible to
decomposability simply means that higher-level subsystems             mathematically describe the activity of the tornado
of a hierarchy are composed of lower-level subsystems. As             throughout its lifetime with a system analysis at the
a result, higher-level subsystems can be decomposed into              molecular level (of course this does not include the
the lower-level subsystems that make them up. One thing               possibility of accurate simulation). In the particular case of
that follows from this is that higher levels have longer times
                                                                         1
                                                                           Simon‟s own example is of an organelle in a cell.
                                                                  3376

tornados, however, experts have found it notoriously hard to         all keep at the back of our minds (assuming materialism),
determine from global properties alone the future location           that minds are not abstract entities but are, rather, grounded
and size of these natural phenomena. Therefore, these may            in the physical (as neurons).
not qualify for a strict high-level analysis for the reason that        While, as mentioned above, there should be languages
their higher-level qualities do not belong to a hard level. In       which describe each system-level, it seems that no such
reality, an accurate prediction as to their lifetime, precise        agreed upon language has been described in cognitive
trajectory and growth/dissipation rates may require higher-          science. While there are many candidate languages such as
frequency levels of analysis combined with simulation.               intentionality, production systems, information processing,
Indeed, it has become increasingly clear over the past               dynamical systems etc., it‟s not clear that the candidate
decades that complex (non-linear, multi-variate) lower level         languages are either complete, that they can be applied to all
phenomena that may be mathematically intractable                     system-levels of the human cognitive architecture or that
analytically can be solved via computer simulation given             they, taken together, handle all relevant levels. While we
adequate model pre-conditions. Hence the degree to which a           offer no analysis of mind-level languages, we do believe
level of analysis gives rise to a set of identifiable or             that most of these languages can play a useful role in the
ascribable features that have reliable (stable) implications         analysis of what Newell calls the Cognitive and Rational
for the description and prediction of the system‟s behaviour         band. We assume, for the purposes of this paper, that all
at that level will ultimately determine the degree to which          such languages can be equally applied to an abstract „mind-
this level is strong or weak. This suggests that determining         level‟(i.e. the point at which we begin to identify minds
the degree of strength of a level is relative to its degree of       abstracted from their physical components, or the
reliability. Notably, this comes in sharp contrast with what         interaction thereof).
we would call realism about system levels. Although we do
not dismiss the fact that hierarchical systems are physical             Problems with the Extend Mind Hypothesis
systems, we do not need to (nor want to) talk about how real            We see the Extended Mind Hypothesis (EMH) as a sort of
any level of the system is; especially given the lack of             systems theory. Like both Simon and Newell, proponents
criteria which could tease apart those layers that manifest          of EMH seem to delimit the mind by the dynamics of mind-
real phenomena from those that do not. Instead, we want to           level components. An obvious example of this is Dynamical
emphasize the sufficiency of arguing in terms of levels of           Systems Theory (DSTs) which gives temporally based
analysis within their corresponding language. In particular          mathematical descriptions of interactions that “span the
we will show how the degree of reliability that a level of           nervous system, the body, and the environment” (van
analysis offers, which determines its degree of strength, can        Gelder & Port 1995, p. 34). In a different manner Clark
be evaluated based on the types and number of errors it              and Chalmers (1998) use intentional terms to capture a
cannot address.                                                      relationship between components of the mind. They aim to
                                                                     show how, using the language of belief states, that the
                    Neurons to Minds                                 dynamics of components of the orthodox mind (the one
   Newell‟s chapter on system-levels provides a good                 bounded by brains or bodies) are functionally equivalent to
discussion of the hierarchical system of the mind. Without           dynamics of components in a mind that spans bodies and
reproducing his work here, we will provide a brief overview          objects in the world. In their famous thought experiment,
of what Newell talks about in that chapter.                          Otto, an amnesiac, relies heavily on his notebook as a
   From Unified Theories of Cognition, Newell‟s chapter,             source of information that he would have otherwise forgot.
Human Cognitive Architecture, aims to describe the system-           Clark and Chalmers show how, if the information in Otto‟s
levels of the hierarchical system of human cognition. He             notebook can be considered beliefs, the notebook is
does so by outlining the various time scales at which it is          functionally equivalent to a normal person‟s memory.
appropriate to study the system-levels of the human                     Critics of the EMH defend the orthodox view by
cognitive architecture, organizing the time scales into time         appealing to differences in types of processes, or a special
bands: biological band, cognitive band, rational band, and           form of representation involved in cognitive processing
social band. Each of these time bands are subdivided into            which does not, as a matter of contingent empirical fact
the various system-levels of the human cognitive                     exist outside the brain (Adams & Aizawa, 2010). However,
architecture: neuron, neural circuit, deliberate acts,               without appeal to these, we aim to show in the following
operations, unit tasks, and so on. Newell describes the              how interaction between what has been traditionally viewed
various system-levels in terms of the interactions of their          as different systems always occurs at a physical level and
components. For instance, Newell explains how, through               that there is no mind-level dynamics which captures both
the interaction of single neurons at a frequency of ~1ms, the        the physical-level components of separate hierarchical
emergence of neural group behavior arises (neural circuit) at        systems, as well as the physical-level components which
the frequency of ~10ms. The story goes on, further and               mediate their interaction.
further up system-levels, eventually leading to operations,
unit tasks, and tasks. (Newell, 1990) Newell‟s discussion
reminds all of us within the cognitive sciences of what we
                                                                 3377

                                                                       Figure 3 illustrates what EMH proponents seem to
                                                                    support: direct high-level interaction in which, using the
                                                                    same example, belief states in a notebook (allowing that
                                                                    such a thing makes sense) affect mental states. For
                                                                    example, „MOMA is on 53rd Street‟ as written in the
   Figure 1: A hierarchical system showing how components           notebook directly influences Otto‟s belief state. Our claim is
       of higher levels are defined by the interaction of           not that descriptions of this kind of interaction are not useful
                 components at a lower level.                       metaphorical shorthand but rather that the dynamical
                                                                    interaction they seemingly summarize does not exist. While
   As an illustration, let us first develop a picture of mind       it is convenient to talk about the contents of a notebook, say
and notebook interaction in accordance with hierarchical            the sentence, „MOMA is on 53rd Street‟; interacting in some
systems theory. Figure 1, is an illustration of a hierarchical      belief-state-to-action calculus, it is inappropriate to suggest
system and you can assume as many layers as you‟d like              that such an interaction defines an interaction between
until the top layer captures what is meant by „mind‟, such          components of a mind-notebook system.
that the components there within are whatever the                      Accepting (for the moment) that the intentional stance
components of a mind are. What those components are                 (Dennett, 1987) is a plausible candidate of a weak level of
exactly is not important for our purposes here.                     (human) cognitive agents, it is understandable why we
                                                                    might want to borrow terms like „belief‟for describing other
                                                                    interactions as well. This usage of intentional terms occurs
                                                                    perhaps most notably by Clark and Chalmers (1998) when
                                                                    they suggest that Otto‟s beliefs are contained in his
                                                                    notebook. One benefit of borrowing high-level terms, which
                                                                    describe one hierarchical system, and using them to describe
                                                                    other types of interaction at a high-level, is that we get to
                                                                    use terms for which we feel we already have a grasp of. It
    Figure 2: Two hierarchical systems interacting at a low-
                                                                    also may be that the use of such terms actually helps us
                             level.
                                                                    capture whatever it is we are trying to explain when we
                                                                    employ them. This is particularly true when the level of
   Figure 2 is an illustration of what happens when two
                                                                    description we would otherwise have to use is low-level
hierarchical systems interact. Consider, for example, what
                                                                    and/or noisy. Borrowing terms in this fashion can have
happens when an agent and a notebook interact. Let‟s
                                                                    certain informative advantages and under this interpretation
assume, for the purposes of this illustration, that intentional
                                                                    we agree that EMH can be vindicated on this informational
terms adequately capture a system-level. In the notebook
                                                                    basis. It seems, however, that proponents of EMH have
this would be some sort of information such as, „MOMA is
                                                                    never favored this interpretation explicitly but have instead
on 53rd Street‟. What we can observe with the hierarchical
                                                                    attempted to make a much stronger ontological claim
system analysis is that „MOMA is on 53rd Street‟ is a
                                                                    regarding mental extension. The reason for this may be that
product of ink markings, in a certain configuration (the
                                                                    a mere informational view of EMH significantly reduces the
down arrow). Without going too low-level, the normal way
                                                                    intellectual contribution that the hypothesis was originally
for notebook-to-mind communication to happen is that light
                                                                    attempting to achieve. As we will illustrate below, we
bounces off the paper, reflects differently when it hits the
                                                                    believe that such ontological claims are unfounded and are
ink, and then eventually enters your eye. Once they hit the
                                                                    even potentially detrimental to scientific pursuits in
eye, dynamics of the lower levels instantiate the dynamics
                                                                    Cognitive Science because they undercut the lower-levels of
of the higher levels (the up-arrow), and we can use terms
                                                                    the hierarchical system.
like „belief‟ to summarize this interaction. The accuracy of
that explanation aside, we can observe that interaction
                                                                    An example
between the notebook and hierarchical cognitive system
occurs at a low, physical level. In our view, this is the              Let‟s take as an example a digital computer. The digital
correct understanding of how systems interact.                      computer is a favourite example in Cognitive Science and
                                                                    fits perfectly for our purposes here because the digital
                                                                    computer seems to be engineered to have decomposable
                                                                    system levels in the way described by Simon. If we draw
                                                                    upon the already heavily used analogy between what Newell
                                                                    calls the Cognitive Band and might be referred to as the
                                                                    Software Band in digital computers, we can see that
                                                                    software-level descriptions (e.g., interactions between
                                                                    certain programs) subsume hardware descriptions. This
       Figure 3: Two hierarchical systems interacting at a
                                                                    works well in digital computers because the linkages
                          higher-level.
                                                                3378

between the levels have been engineered to be                             same text bolded on their screen as well. One could
decomposable. In brains, the analogous linkages are in fact               imagine that in such a cyberpunk future the language used
linking theories which attempt to describe the relationship               to describe relations between the two word processing
between higher and lower levels. Perhaps one reason                       systems would be highly correlated with the language used
Cognitive Science is slow in developing theories of the                   to describe a single system. It seems likely that one would
dynamics of the cognitive (the interactions of the cognitive              simply say that the bold button on WP A caused bolding on
machine, if you will) is that the mental is a weak level and              both WPA and WPB. Our argument is that while such
thus components of the cognitive system cannot be isolated                language would be a convenient short-hand for describing
from the interaction of components at a lower level.                      the interaction of the two systems, it also undercuts the
Regardless of whether levels are weak or strong, in order to              hierarchical system in a way that using a high-level
confirm the existence of some cognitive-level component,                  language for describing an orthodox single system does not.
one needs to both identify the lower-level components, as                 This undercutting of the hierarchical system would have
well as the linking theories between the levels.                          severe consequences in our imagined science of computing.
   With a Software Band in place, describing interactions of              To explain what we mean, let‟s carry the analogy a little
the digital computer becomes more accessible. Most                        further and try to explain what would happen if an error
computer users can at least give high-level interaction                   occurred in the interaction between WP A and WPB.
descriptions (e.g., I pressed button x and the program did y)                Suppose that whenever the name Otto appeared in a
which correspond to progressively lower-level subsystems                  sentence the bold function did not work quite as it should.
such as the programming-level, operating system level, and                Let‟s imagine that in this scenario the text on WP A‟s screen
the hardware level. Now, to push the analogy a little further,            turned bold properly but the text on WP B‟s screen did not.
let us suggest that there could be a science of computing 2.              How will our imagined science of computing explain this?
To make this analogy work, let us also pretend that we do                 We can immediately see that any supposition of an extended
not already have the linking theories of computing, that                  word processing or extended computing system would have
there is some mystery about how computers work.                           to be abandoned. Because the extended word processing
Furthermore, let‟s set this illustration sometime in the                  system hypothesis posits that the base units of an extended
cyberpunk future (as Clark might say) when word                           word processing system is realized through the dynamics of
processing programs allow for collaborative work over the                 the user-interface level across the internet (i.e. Figure 3
Internet. Could we argue that there is an extended word                   applied to our example), there exists no cross-internet layer
processor? Does it make sense to talk about word processor                below, in this view, which can account for this error. What
processes spanning the Internet? In order to answer those                 in fact is the case is that information is encoded, sent across
questions, let‟s first take a look at the hierarchical structure          the hardware of the Internet, and decoded at the other end.
of a computer system.                                                     The fact that WPB‟s sentence did not get bolded can be
   Taking the hardware layer as the base unit of analysis, an             explained either within WPA‟s or WPB‟s layers or at the
interaction of logical gates (usually transistors) form groups            lower level at which signal transmission occurs (a
of logical gates, or circuits. Interactions between circuits              transmission error). The answer to both our questions above
realize groups of circuits (circuit boards, chips, etc.). As you          is: although it may be useful to speak as if there is an
move up the layers, we eventually arrive at layers most                   extended system, such a description would be misleading
people are familiar with: software layers. We can think of                for our science of computing. Errors manifested at a high-
the software layers as composed of two layers: the                        level can only be explained through decomposition and, of
programming layer and the user-interface layer. The                       course, that can only be facilitated when there are levels
programming layer is the layer at which programs are                      below to decompose to. In this example, ontological claims
written, i.e. the code behind the user-interface layer. Again,            about extended computer systems are misleading for our
it is the interactions or dynamics at the programming layer               fictional science of computing. In the same way, ontological
that realize the user-interface layer. We may also observe                claims about extended minds can be misleading, especially
that these layers form hard layers. It is the consistency of the          in fields like Cognitive Science that aims to provide linking
programming layer that makes the user-interface layer                     theories between system levels of cognitive agents.
reliable. We can also give descriptions of the interactions               Cognitive Science seems to rely on the fact there are
from a user-interface level. For instance, in a word                      emergent levels and that these levels are decomposable (in
processor we can say that clicking the bold button caused                 the weak or strong sense) to the various levels below.
the text to become bold.                                                     But, you might object, that the two systems are extended
   Now let‟s imagine that a collaborative word processor                  across the hardware of the Internet. We admit that we
where user-interface changes on one computer (WP A) have                  would have to accept such an objection but only so far as we
parallel effects on another computer (WP B) over the                      accept that, at some low-enough level, everything is
Internet. If a user on WPA presses the bold button, turning               extended to everything else. It would remain an open
text bold on their screen, the user on WPB would have the                 question as to whether cross-Internet dynamics would scale
                                                                          up. Furthermore it‟s not clear whether our imagined science
   2
     Believe it or not, there are actually people who call themselves
computer scientists!
                                                                      3379

of computing (of computer systems or word processor                  systems, can impede on scientific pursuits as it undercuts
systems) is really after such descriptions at all.                   the structural levels which make up the separate hierarchies.
                                                                        We accept that borrowing terms can be informative by
             Transduction as a boundary                              providing a more intuitive understanding of the interaction
   Up to here we have defended the explanatory power of              between systems. We also accept the point made by
hierarchical systems. However, the identification of a               dynamical systems theorists, phenomenologists, and the
system‟s boundaries cannot be assumed a priori. Instead, it          situated cognition folks, that we have to, in our analyses
is necessary to establish an adequate delineating process by         remember that the mind is tightly coupled with its
which systems can be distinguished. For this, we propose             environment. What we reject, however, is the ontological
that signal transduction can serve at a sufficiently low             claims that the mind (as an abstract entity, grounded in the
enough level to count as a mechanism of system                       physical) is extended with objects in its environment or
boundedness. Signal transduction occurs when change in               other minds. Although a notebook may function as if it were
signal results in a change of dynamical properties. For              a belief storage device, beliefs themselves are entities of the
instance, with an electrical motor we find a change from             hierarchical system of the mind, dependent on the
electrical dynamics to mechanical dynamics. From this we             subsystems which realize them. In so far as an analogous
can make use of transduction as the fundamental level upon           entity can be realized on a piece of paper (and we suspect
which hierarchical systems can be based. With transduction           that they cannot), that entity would be a product of its own
as a boundary, we do not need other mechanisms for                   hierarchy, communicated at a low-level (as some complex
boundedness. Furthermore, transduction avoids imposing               of signals) between different systems.
limitations on the complexity of the system. Hence, errors
that appear to occur at high levels can be accounted for by                                  References
lower level interpretation as long as this level is identifiable     Adams, F., & Aizawa, K. (2010). The bounds of cognition.
via transducing boundaries.                                             Singapore: Wiley-Blackwell.
   This idea, we feel, is nothing new. We suspect that for           Chalmers, D. (2008) Forward. In A. Clark Supersizing the
most outside of the EMH debate, this idea has been                      Mind. New York, NY: Oxford University Press.
implicitly accepted. A very similar idea, curiously enough,          Clark, A., & Chalmers, D. (1998). The Extended Mind.
was presented by David Chalmers (2008), in the Forward to               Analysis, 58, 7-19.
Andy Clark‟s book on the EMH, Supersizing the Mind.                  Dennett, D. (1987). The Intentional Stance. Cambridge,
Chalmers suggests that perception and action forms the                  MA: MIT Press.
bounds of cognitive systems and are the interfaces of the            van Gelder, T., & Port, R. (1995) It‟s about time: An
mind from and to the world. This is precisely what we mean              overview of the dynamical approach to cognition. In R.
by transduction: the process that facilitates perception and            Port, & T. van Gelder (Eds.), Mind as Motion.
action.                                                                 Cambridge, MA: MIT Press.
   Transduction processes are fundamental (non-question              Newell, A. (1990). Unified Theories of Cognition.
begging) because the dynamics upon which they are based                 Cambridge, MA: Harvard University Press.
forms a plausible candidate for highest-level description of         Simon, H. (1962). The Architecture of Complexity.
the interaction between hierarchical systems. Indeed signals            Proceedings of the American Philosophical Society, 106,
of one form or another is a common currency between                     467-482.
interconnected systems and the point at which the dynamics
of these signals change, is the point at which the dynamics
of the higher-levels begin to diverge. This divergence, we
suggest, is precisely what separates systems and, following
Simon (1962), the emergence of a hierarchy is what renders
these systems intelligible.
                         Conclusion
   Our argument in the preceding has been aimed at
providing a principled account of the boundaries of the
mind. Our claim is that transduction processes form the
boundaries of minds because the dynamics which result
from these processes form the basic system levels of the
mind viewed as a hierarchical system. We have discussed
how although it may be useful to talk as if minds extend into
other minds or objects, we also warn that doing so may be
misleading. We also demonstrated how borrowing high-
level terms which describe one hierarchical system to
describe other types of interactions, i.e. those between
                                                                 3380

