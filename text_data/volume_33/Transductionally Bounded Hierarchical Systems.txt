UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Transductionally Bounded Hierarchical Systems

Permalink
https://escholarship.org/uc/item/5dm9p66g

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)

Authors
Somers, Sterling
Jeanson, Francis

Publication Date
2011-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Transductionally Bounded Hierarchical Systems
Sterling Somers (sterling@sterlingsomers.com)
Institute of Cognitive Science, Carleton University, 1125 Colonel By Drive
Ottawa, On., Canada

Francis Jeanson (fjeanson@connect.carleton.ca)
Institute of Cognitive Science, Carleton University, 1125 Colonel By Drive
Ottawa, On., Canada

Abstract
Using a hierarchical-systems analysis, this paper supports the
orthodox view of the mind. We claim that the orthodox mind
– bounded by brains or bodies – is organized into various
system levels, each of which is emergent from the dynamics
of level below it. We see the extended mind hypothesis as
borrowing terms from a high-level system of the orthodox
mind and applying it to interactions between high levels of
separate hierarchical systems, without providing any lower
levels on which to ground it.
Keywords: transduction, extended mind, levels.

Introduction
Following Herbert Simon‟s analysis of complex systems
(Simon, 1962) and Newell‟s related chapter on system
levels (Newell, 1990); this paper supports the orthodox view
of the bounds of the mental (the mental is bounded by the
body) in a non-question begging way. Recognizing that
minds (high-level systems, defined by lower-level
dynamics) interact with other minds or objects in the world,
not through direct interaction of mind-level (high-level)
systems, but through a physical intermediary at a lowerlevel, we show that it is incorrect and misleading to
incorporate within our definition of mind that which extends
with other minds or external objects. The orthodox mind is
grounded empirically on the levels which emerge from
some fundamental level, while no such hierarchy exists in
support of minds hypothesized to extend into other minds or
objects. In this paper we argue that cognitive systems are
bounded by the transduction processes that give rise to the
dynamics upon which the hierarchies are based.

Dynamics
In Simon‟s paper on complex systems (1962), he argues
that most (if not all) complex systems are hierarchical
systems: a system “that is composed of interrelated
subsystems, each of the latter being, in turn, hierarchic in
structure until we reach some lowest level of elementary
subsystem” (p. 468). As Simon points out, hierarchical
systems share a common trait: near decomposability. Near
decomposability simply means that higher-level subsystems
of a hierarchy are composed of lower-level subsystems. As
a result, higher-level subsystems can be decomposed into
the lower-level subsystems that make them up. One thing
that follows from this is that higher levels have longer times

scales, as the events at higher levels depend on the events of
lower levels. On the larger time scale of the higher-level
subsystem, the subsystem description of the lower level
becomes superfluous, provided that the high-level
subsystem is decomposable. This feature of hierarchies is
what Newell refers to when he describes a `strong‟ system
level. While for Simon hierarchical systems all share the
property of near decomposability, Newell suggests that
some system levels can be „weak‟. Essentially, a system
level is weak when it does not perfectly predict behavior at
the level to which it belongs in the hierarchical system. A
weak level may be simpler than sublevels but if it is not
decomposable into the sublevels, one may have to recruit
the sublevels in order to explain certain phenomena.
Important in Newell‟s and Simon‟s analysis is that for each
system-level there is also some appropriate language for
describing that level.
Each subsystem (system-level) in a hierarchy is
characterized by the interactions of the components of that
system-level. The stability of these interactions, what we
refer to as the dynamics of the system-level, is what allows
us – in physical systems at least – to identify those levels. A
system-level, defined by its dynamic, should always be
distinguishable via some observational measure. Consider,
for example, a tornado1. The dynamics of a tornado (e.g.,
the interaction of the air molecules) can be visibly identified
from the surrounding system (non-tornado air). A tornado
is, then, a plausible candidate of an example of a
hierarchical system. At a low level there are interactions
between air molecules which, presumably, are travelling at
certain speeds, and following certain paths, etc.; at an
intermediary level we may distinguish small localized wind
currents which are formed by aggregates of coherent
molecules; while at an even higher level there is the entire
tornado. The description of a tornado in hierarchical terms
is particularly useful when high-level. Longer time scale
analysis allows the identification, say, of its general location
and itinerary over time and monitor or even predict its
destructive effects. Without this high-level, low-frequency
description it would be nearly (if not actually) impossible to
mathematically describe the activity of the tornado
throughout its lifetime with a system analysis at the
molecular level (of course this does not include the
possibility of accurate simulation). In the particular case of
1

3376

Simon‟s own example is of an organelle in a cell.

tornados, however, experts have found it notoriously hard to
determine from global properties alone the future location
and size of these natural phenomena. Therefore, these may
not qualify for a strict high-level analysis for the reason that
their higher-level qualities do not belong to a hard level. In
reality, an accurate prediction as to their lifetime, precise
trajectory and growth/dissipation rates may require higherfrequency levels of analysis combined with simulation.
Indeed, it has become increasingly clear over the past
decades that complex (non-linear, multi-variate) lower level
phenomena that may be mathematically intractable
analytically can be solved via computer simulation given
adequate model pre-conditions. Hence the degree to which a
level of analysis gives rise to a set of identifiable or
ascribable features that have reliable (stable) implications
for the description and prediction of the system‟s behaviour
at that level will ultimately determine the degree to which
this level is strong or weak. This suggests that determining
the degree of strength of a level is relative to its degree of
reliability. Notably, this comes in sharp contrast with what
we would call realism about system levels. Although we do
not dismiss the fact that hierarchical systems are physical
systems, we do not need to (nor want to) talk about how real
any level of the system is; especially given the lack of
criteria which could tease apart those layers that manifest
real phenomena from those that do not. Instead, we want to
emphasize the sufficiency of arguing in terms of levels of
analysis within their corresponding language. In particular
we will show how the degree of reliability that a level of
analysis offers, which determines its degree of strength, can
be evaluated based on the types and number of errors it
cannot address.

Neurons to Minds
Newell‟s chapter on system-levels provides a good
discussion of the hierarchical system of the mind. Without
reproducing his work here, we will provide a brief overview
of what Newell talks about in that chapter.
From Unified Theories of Cognition, Newell‟s chapter,
Human Cognitive Architecture, aims to describe the systemlevels of the hierarchical system of human cognition. He
does so by outlining the various time scales at which it is
appropriate to study the system-levels of the human
cognitive architecture, organizing the time scales into time
bands: biological band, cognitive band, rational band, and
social band. Each of these time bands are subdivided into
the various system-levels of the human cognitive
architecture: neuron, neural circuit, deliberate acts,
operations, unit tasks, and so on. Newell describes the
various system-levels in terms of the interactions of their
components. For instance, Newell explains how, through
the interaction of single neurons at a frequency of ~1ms, the
emergence of neural group behavior arises (neural circuit) at
the frequency of ~10ms. The story goes on, further and
further up system-levels, eventually leading to operations,
unit tasks, and tasks. (Newell, 1990) Newell‟s discussion
reminds all of us within the cognitive sciences of what we

all keep at the back of our minds (assuming materialism),
that minds are not abstract entities but are, rather, grounded
in the physical (as neurons).
While, as mentioned above, there should be languages
which describe each system-level, it seems that no such
agreed upon language has been described in cognitive
science. While there are many candidate languages such as
intentionality, production systems, information processing,
dynamical systems etc., it‟s not clear that the candidate
languages are either complete, that they can be applied to all
system-levels of the human cognitive architecture or that
they, taken together, handle all relevant levels. While we
offer no analysis of mind-level languages, we do believe
that most of these languages can play a useful role in the
analysis of what Newell calls the Cognitive and Rational
band. We assume, for the purposes of this paper, that all
such languages can be equally applied to an abstract „mindlevel‟(i.e. the point at which we begin to identify minds
abstracted from their physical components, or the
interaction thereof).

Problems with the Extend Mind Hypothesis
We see the Extended Mind Hypothesis (EMH) as a sort of
systems theory. Like both Simon and Newell, proponents
of EMH seem to delimit the mind by the dynamics of mindlevel components. An obvious example of this is Dynamical
Systems Theory (DSTs) which gives temporally based
mathematical descriptions of interactions that “span the
nervous system, the body, and the environment” (van
Gelder & Port 1995, p. 34). In a different manner Clark
and Chalmers (1998) use intentional terms to capture a
relationship between components of the mind. They aim to
show how, using the language of belief states, that the
dynamics of components of the orthodox mind (the one
bounded by brains or bodies) are functionally equivalent to
dynamics of components in a mind that spans bodies and
objects in the world. In their famous thought experiment,
Otto, an amnesiac, relies heavily on his notebook as a
source of information that he would have otherwise forgot.
Clark and Chalmers show how, if the information in Otto‟s
notebook can be considered beliefs, the notebook is
functionally equivalent to a normal person‟s memory.
Critics of the EMH defend the orthodox view by
appealing to differences in types of processes, or a special
form of representation involved in cognitive processing
which does not, as a matter of contingent empirical fact
exist outside the brain (Adams & Aizawa, 2010). However,
without appeal to these, we aim to show in the following
how interaction between what has been traditionally viewed
as different systems always occurs at a physical level and
that there is no mind-level dynamics which captures both
the physical-level components of separate hierarchical
systems, as well as the physical-level components which
mediate their interaction.

3377

Figure 1: A hierarchical system showing how components
of higher levels are defined by the interaction of
components at a lower level.
As an illustration, let us first develop a picture of mind
and notebook interaction in accordance with hierarchical
systems theory. Figure 1, is an illustration of a hierarchical
system and you can assume as many layers as you‟d like
until the top layer captures what is meant by „mind‟, such
that the components there within are whatever the
components of a mind are. What those components are
exactly is not important for our purposes here.

Figure 2: Two hierarchical systems interacting at a lowlevel.
Figure 2 is an illustration of what happens when two
hierarchical systems interact. Consider, for example, what
happens when an agent and a notebook interact. Let‟s
assume, for the purposes of this illustration, that intentional
terms adequately capture a system-level. In the notebook
this would be some sort of information such as, „MOMA is
on 53rd Street‟. What we can observe with the hierarchical
system analysis is that „MOMA is on 53rd Street‟ is a
product of ink markings, in a certain configuration (the
down arrow). Without going too low-level, the normal way
for notebook-to-mind communication to happen is that light
bounces off the paper, reflects differently when it hits the
ink, and then eventually enters your eye. Once they hit the
eye, dynamics of the lower levels instantiate the dynamics
of the higher levels (the up-arrow), and we can use terms
like „belief‟ to summarize this interaction. The accuracy of
that explanation aside, we can observe that interaction
between the notebook and hierarchical cognitive system
occurs at a low, physical level. In our view, this is the
correct understanding of how systems interact.

Figure 3: Two hierarchical systems interacting at a
higher-level.

Figure 3 illustrates what EMH proponents seem to
support: direct high-level interaction in which, using the
same example, belief states in a notebook (allowing that
such a thing makes sense) affect mental states. For
example, „MOMA is on 53rd Street‟ as written in the
notebook directly influences Otto‟s belief state. Our claim is
not that descriptions of this kind of interaction are not useful
metaphorical shorthand but rather that the dynamical
interaction they seemingly summarize does not exist. While
it is convenient to talk about the contents of a notebook, say
the sentence, „MOMA is on 53rd Street‟; interacting in some
belief-state-to-action calculus, it is inappropriate to suggest
that such an interaction defines an interaction between
components of a mind-notebook system.
Accepting (for the moment) that the intentional stance
(Dennett, 1987) is a plausible candidate of a weak level of
(human) cognitive agents, it is understandable why we
might want to borrow terms like „belief‟for describing other
interactions as well. This usage of intentional terms occurs
perhaps most notably by Clark and Chalmers (1998) when
they suggest that Otto‟s beliefs are contained in his
notebook. One benefit of borrowing high-level terms, which
describe one hierarchical system, and using them to describe
other types of interaction at a high-level, is that we get to
use terms for which we feel we already have a grasp of. It
also may be that the use of such terms actually helps us
capture whatever it is we are trying to explain when we
employ them. This is particularly true when the level of
description we would otherwise have to use is low-level
and/or noisy. Borrowing terms in this fashion can have
certain informative advantages and under this interpretation
we agree that EMH can be vindicated on this informational
basis. It seems, however, that proponents of EMH have
never favored this interpretation explicitly but have instead
attempted to make a much stronger ontological claim
regarding mental extension. The reason for this may be that
a mere informational view of EMH significantly reduces the
intellectual contribution that the hypothesis was originally
attempting to achieve. As we will illustrate below, we
believe that such ontological claims are unfounded and are
even potentially detrimental to scientific pursuits in
Cognitive Science because they undercut the lower-levels of
the hierarchical system.

An example
Let‟s take as an example a digital computer. The digital
computer is a favourite example in Cognitive Science and
fits perfectly for our purposes here because the digital
computer seems to be engineered to have decomposable
system levels in the way described by Simon. If we draw
upon the already heavily used analogy between what Newell
calls the Cognitive Band and might be referred to as the
Software Band in digital computers, we can see that
software-level descriptions (e.g., interactions between
certain programs) subsume hardware descriptions. This
works well in digital computers because the linkages

3378

between the levels have been engineered to be
decomposable. In brains, the analogous linkages are in fact
linking theories which attempt to describe the relationship
between higher and lower levels. Perhaps one reason
Cognitive Science is slow in developing theories of the
dynamics of the cognitive (the interactions of the cognitive
machine, if you will) is that the mental is a weak level and
thus components of the cognitive system cannot be isolated
from the interaction of components at a lower level.
Regardless of whether levels are weak or strong, in order to
confirm the existence of some cognitive-level component,
one needs to both identify the lower-level components, as
well as the linking theories between the levels.
With a Software Band in place, describing interactions of
the digital computer becomes more accessible. Most
computer users can at least give high-level interaction
descriptions (e.g., I pressed button x and the program did y)
which correspond to progressively lower-level subsystems
such as the programming-level, operating system level, and
the hardware level. Now, to push the analogy a little further,
let us suggest that there could be a science of computing 2.
To make this analogy work, let us also pretend that we do
not already have the linking theories of computing, that
there is some mystery about how computers work.
Furthermore, let‟s set this illustration sometime in the
cyberpunk future (as Clark might say) when word
processing programs allow for collaborative work over the
Internet. Could we argue that there is an extended word
processor? Does it make sense to talk about word processor
processes spanning the Internet? In order to answer those
questions, let‟s first take a look at the hierarchical structure
of a computer system.
Taking the hardware layer as the base unit of analysis, an
interaction of logical gates (usually transistors) form groups
of logical gates, or circuits. Interactions between circuits
realize groups of circuits (circuit boards, chips, etc.). As you
move up the layers, we eventually arrive at layers most
people are familiar with: software layers. We can think of
the software layers as composed of two layers: the
programming layer and the user-interface layer. The
programming layer is the layer at which programs are
written, i.e. the code behind the user-interface layer. Again,
it is the interactions or dynamics at the programming layer
that realize the user-interface layer. We may also observe
that these layers form hard layers. It is the consistency of the
programming layer that makes the user-interface layer
reliable. We can also give descriptions of the interactions
from a user-interface level. For instance, in a word
processor we can say that clicking the bold button caused
the text to become bold.
Now let‟s imagine that a collaborative word processor
where user-interface changes on one computer (WP A) have
parallel effects on another computer (WP B) over the
Internet. If a user on WPA presses the bold button, turning
text bold on their screen, the user on WPB would have the

same text bolded on their screen as well. One could
imagine that in such a cyberpunk future the language used
to describe relations between the two word processing
systems would be highly correlated with the language used
to describe a single system. It seems likely that one would
simply say that the bold button on WP A caused bolding on
both WPA and WPB. Our argument is that while such
language would be a convenient short-hand for describing
the interaction of the two systems, it also undercuts the
hierarchical system in a way that using a high-level
language for describing an orthodox single system does not.
This undercutting of the hierarchical system would have
severe consequences in our imagined science of computing.
To explain what we mean, let‟s carry the analogy a little
further and try to explain what would happen if an error
occurred in the interaction between WP A and WPB.
Suppose that whenever the name Otto appeared in a
sentence the bold function did not work quite as it should.
Let‟s imagine that in this scenario the text on WP A‟s screen
turned bold properly but the text on WP B‟s screen did not.
How will our imagined science of computing explain this?
We can immediately see that any supposition of an extended
word processing or extended computing system would have
to be abandoned. Because the extended word processing
system hypothesis posits that the base units of an extended
word processing system is realized through the dynamics of
the user-interface level across the internet (i.e. Figure 3
applied to our example), there exists no cross-internet layer
below, in this view, which can account for this error. What
in fact is the case is that information is encoded, sent across
the hardware of the Internet, and decoded at the other end.
The fact that WPB‟s sentence did not get bolded can be
explained either within WPA‟s or WPB‟s layers or at the
lower level at which signal transmission occurs (a
transmission error). The answer to both our questions above
is: although it may be useful to speak as if there is an
extended system, such a description would be misleading
for our science of computing. Errors manifested at a highlevel can only be explained through decomposition and, of
course, that can only be facilitated when there are levels
below to decompose to. In this example, ontological claims
about extended computer systems are misleading for our
fictional science of computing. In the same way, ontological
claims about extended minds can be misleading, especially
in fields like Cognitive Science that aims to provide linking
theories between system levels of cognitive agents.
Cognitive Science seems to rely on the fact there are
emergent levels and that these levels are decomposable (in
the weak or strong sense) to the various levels below.
But, you might object, that the two systems are extended
across the hardware of the Internet. We admit that we
would have to accept such an objection but only so far as we
accept that, at some low-enough level, everything is
extended to everything else. It would remain an open
question as to whether cross-Internet dynamics would scale
up. Furthermore it‟s not clear whether our imagined science

2
Believe it or not, there are actually people who call themselves
computer scientists!

3379

of computing (of computer systems or word processor
systems) is really after such descriptions at all.

Transduction as a boundary
Up to here we have defended the explanatory power of
hierarchical systems. However, the identification of a
system‟s boundaries cannot be assumed a priori. Instead, it
is necessary to establish an adequate delineating process by
which systems can be distinguished. For this, we propose
that signal transduction can serve at a sufficiently low
enough level to count as a mechanism of system
boundedness. Signal transduction occurs when change in
signal results in a change of dynamical properties. For
instance, with an electrical motor we find a change from
electrical dynamics to mechanical dynamics. From this we
can make use of transduction as the fundamental level upon
which hierarchical systems can be based. With transduction
as a boundary, we do not need other mechanisms for
boundedness. Furthermore, transduction avoids imposing
limitations on the complexity of the system. Hence, errors
that appear to occur at high levels can be accounted for by
lower level interpretation as long as this level is identifiable
via transducing boundaries.
This idea, we feel, is nothing new. We suspect that for
most outside of the EMH debate, this idea has been
implicitly accepted. A very similar idea, curiously enough,
was presented by David Chalmers (2008), in the Forward to
Andy Clark‟s book on the EMH, Supersizing the Mind.
Chalmers suggests that perception and action forms the
bounds of cognitive systems and are the interfaces of the
mind from and to the world. This is precisely what we mean
by transduction: the process that facilitates perception and
action.
Transduction processes are fundamental (non-question
begging) because the dynamics upon which they are based
forms a plausible candidate for highest-level description of
the interaction between hierarchical systems. Indeed signals
of one form or another is a common currency between
interconnected systems and the point at which the dynamics
of these signals change, is the point at which the dynamics
of the higher-levels begin to diverge. This divergence, we
suggest, is precisely what separates systems and, following
Simon (1962), the emergence of a hierarchy is what renders
these systems intelligible.

systems, can impede on scientific pursuits as it undercuts
the structural levels which make up the separate hierarchies.
We accept that borrowing terms can be informative by
providing a more intuitive understanding of the interaction
between systems. We also accept the point made by
dynamical systems theorists, phenomenologists, and the
situated cognition folks, that we have to, in our analyses
remember that the mind is tightly coupled with its
environment. What we reject, however, is the ontological
claims that the mind (as an abstract entity, grounded in the
physical) is extended with objects in its environment or
other minds. Although a notebook may function as if it were
a belief storage device, beliefs themselves are entities of the
hierarchical system of the mind, dependent on the
subsystems which realize them. In so far as an analogous
entity can be realized on a piece of paper (and we suspect
that they cannot), that entity would be a product of its own
hierarchy, communicated at a low-level (as some complex
of signals) between different systems.

References
Adams, F., & Aizawa, K. (2010). The bounds of cognition.
Singapore: Wiley-Blackwell.
Chalmers, D. (2008) Forward. In A. Clark Supersizing the
Mind. New York, NY: Oxford University Press.
Clark, A., & Chalmers, D. (1998). The Extended Mind.
Analysis, 58, 7-19.
Dennett, D. (1987). The Intentional Stance. Cambridge,
MA: MIT Press.
van Gelder, T., & Port, R. (1995) It‟s about time: An
overview of the dynamical approach to cognition. In R.
Port, & T. van Gelder (Eds.), Mind as Motion.
Cambridge, MA: MIT Press.
Newell, A. (1990). Unified Theories of Cognition.
Cambridge, MA: Harvard University Press.
Simon, H. (1962). The Architecture of Complexity.
Proceedings of the American Philosophical Society, 106,
467-482.

Conclusion
Our argument in the preceding has been aimed at
providing a principled account of the boundaries of the
mind. Our claim is that transduction processes form the
boundaries of minds because the dynamics which result
from these processes form the basic system levels of the
mind viewed as a hierarchical system. We have discussed
how although it may be useful to talk as if minds extend into
other minds or objects, we also warn that doing so may be
misleading. We also demonstrated how borrowing highlevel terms which describe one hierarchical system to
describe other types of interactions, i.e. those between

3380

