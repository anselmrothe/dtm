UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Estimating scene typicality from human ratings and image features
Permalink
https://escholarship.org/uc/item/19n7p532
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Ehinger, Krista A.
Xiao, Jiangxiong
Torralba, Antonio
et al.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

              Estimating scene typicality from human ratings and image features
                                           Krista A. Ehinger (kehinger@mit.edu)
                             Department of Brain & Cognitive Sciences, MIT, 77 Massachusetts Ave.
                                                    Cambridge, MA 02139 USA
                                            Jianxiong Xiao (jxiao@csail.mit.edu)
                      Computer Science & Artificial Intelligence Laboratory, MIT, 77 Massachusetts Ave.
                                                    Cambridge, MA 02139 USA
                                        Antonio Torralba (torralba@csail.mit.edu)
                      Computer Science & Artificial Intelligence Laboratory, MIT, 77 Massachusetts Ave.
                                                    Cambridge, MA 02139 USA
                                                  Aude Oliva (oliva@mit.edu)
                             Department of Brain & Cognitive Sciences, MIT, 77 Massachusetts Ave.
                                                    Cambridge, MA 02139 USA
                            Abstract                                 prototypes are an average or central tendency of all category
                                                                     members. People do not need to actually encounter the
Scenes, like objects, are visual entities that can be
                                                                     prototypical example of a category in order to form a
categorized into functional and semantic groups. One of the
                                                                     concept of that category; instead, they extract the prototype
core concepts of human categorization is the idea that
                                                                     through experience with the variation that exists within the
category membership is graded: some exemplars are more
                                                                     category (Posner & Keele, 1968).
typical than others. Here, we obtain human typicality
                                                                       Environmental scenes, like objects, are visual entities that
rankings for more than 120,000 images from 706 scene
                                                                     can be organized in functional and semantic groups. Like
categories through an online rating task on Amazon
                                                                     other conceptual categories, scenes contain more and less
Mechanical Turk. We use these rankings to identify the
                                                                     typical exemplars. Tversky and Hemenway (1983)
most typical examples of each scene category. Using
                                                                     identified some typical examples of indoor and outdoor
computational models of scene classification based on
                                                                     scene categories, but the total number of scene categories
global image features, we find that images which are rated
                                                                     used in their studies was very small. Here, we extend the
as more typical examples of their category are more likely
                                                                     idea of scene typicality to a very large database containing
to be classified correctly. This indicates that the most typical
                                                                     over 700 scene categories. The goal of the current study is
scene examples contain the diagnostic visual features that
                                                                     two-fold: first, to determine the prototypical exemplars that
are relevant for their categorization. Objectless, holistic
                                                                     best represent each visual scene category; and second, to
representations of scenes might serve as a good basis for
                                                                     evaluate the performances of state-of-the-art global features
understanding how semantic categories are defined in term
                                                                     algorithms at classifying different types of exemplars.
of perceptual representations.
   Keywords: scene perception; prototypes; categorization.                                     Method
                        Introduction                                 Dataset
Most theories of categorization and concepts agree that              Stimuli were taken from the SUN Database, a collection of
category membership is graded – some items are more                  130,519 images organized into 899 categories (see Xiao,
typical examples of their category than others. For example,         Hays, Ehinger, Oliva & Torralba, 2010). This database was
both sparrows and ostriches are birds, but a sparrow is              constructed by first identifying all of the words in a
generally regarded as a much more typical bird than an               dictionary corresponding to types of places, scenes, or
ostrich. The more typical examples of a category show                environments (see Biederman, 1987, for a similar procedure
many advantages in cognitive tasks. For example, typical             with objects). Our definition of a scene or place type was
examples are more readily named than atypical examples               any concrete common noun which could reasonably
when people are asked to list examples of a category (eg.,           complete the phrase, “I am in a place,” or “Let’s go to the
furniture) and response times are faster for typical examples        place.” We included terms which referred to specific
when people are asked to verify category membership (eg.,            subtypes of scenes or sub-areas of scenes. However, we
“a chair is a piece of furniture”) (Rosch, 1975).                    excluded specific places (like MIT or New York), terms
  According to Prototype Theory, concepts are represented            which did not evoke a clear visual identity (like workplace
by their most typical examples (Rosch, 1971). These                  or territory), spaces which were too small for a human body
                                                                 2562

to navigate within (such as a desktop), and scenes with
mature content. We included views of the interiors of
vehicles (airplane cabin), but not exterior views of vehicles.
We included specific types of buildings (skyscraper, house),
because, although these can be seen as objects, they are
known to activate scene-processing-related areas in the
human brain (Epstein & Kanwisher, 1998). This procedure
yielded an initial set of about 2400 scene words, and after
combining synonyms and separating scenes with different
visual identities (such as indoor and outdoor views), we
obtained a list of about 899 unique semantic categories of
scenes and places. For each of these categories, we collected
a large set of images online, resulting in a database of about
                                                                      Figure 1: The display seen by participants in the typicality
130,000 images.
                                                                     rating task. In the experiment, images were shown in color.
   Note that there are different ways to define and categorize
“scenes,” which would generate a slightly different or more
complete database than the one used here. For example, one          category, and did not include the image which had served as
might decide that different views of the same place qualify         the target in the previous task. Images were shown at a size
as different scenes, or one might choose to subdivide scenes        of 100 x 100 pixels, but holding the mouse over any image
based on spatial layout or surface features (e.g., forests with     caused a larger 300 x 300 pixel version of that image to
or without snow). However, this work represents the first           appear. An example of this display is shown in Figure 1.
attempt at estimating typicality on a dataset that is extensive     Workers were asked to select, by clicking with the mouse,
enough to cover most of the plausible scene categories used         three images that best illustrated the scene category.
to refer to places and scenes in discourse.                            In the third part of the task, workers were shown the same
                                                                    20 images (but with their array positions shuffled) and were
Stimuli                                                             asked to select the three worst examples of the target scene
                                                                    category.
   For the typicality experiment, we used the 706 scene
categories from this database that contained at least 22            Design
exemplars. Category size ranged from 22 images in the
smallest categories to 2360 in the largest. A total of 124,901      On each trial, the set of 20 images was drawn randomly
images were used in the experiment.                                 from the set of images in the target category. These random
                                                                    draws were such that each image appeared at least 12 times,
                                                                    and no more than 15 times over the course of the
Participants
935 people participated in the experiment through                   experiment. This resulted in 77,331 experimental trials.
Amazon’s Mechanical Turk, an online service where                   Each trial was completed by a single participant.
workers are paid to complete short computational tasks              Participants could complete as many trials as they wished;
(HITs) for small amounts of money. All workers were                 the mean number of trials completed per participant was
located in the United States and had a good performance             82.7 trials (median 7 trials).
record with the service (at least 100 HITs completed with an
acceptance rate of 95% or better). Workers were paid $0.03                                       Results
per trial.                                                             Participants’ performance was judged on two measures:
                                                                    their performance on the 4AFC task, and whether they
Procedure                                                           selected different images as the best and worst examples on
Participants were told that the goal of the experiment was to       a single trial. In general, participants performed well on the
select illustrations for a dictionary. Each trial consisted of      4AFC task, with an average correct response rate of 97%
three parts.                                                        (s.d. 0.13%). Most of the incorrect responses occurred on
   First, participants were given the name of a scene               trials where one of the randomly-drawn foil images came
category from the database, a short definition of the scene         from a category similar to the target category (for example,
category, and four images. Workers were asked to select             a cathedral might be the foil image for the category
which of the four images matched the name and definition            “basilica”). Participants were also reliably able to select
(one of the four images was drawn from the target category          different images as the best and worst examples of their
and the other three were randomly selected from other               category: participants marked an image as both best and
categories). The purpose of this task was to ensure that            worst on only 2% of trials (s. d. 0.10%); the likelihood of
participants read the category name and definition before           reselecting an image by chance is 40%. However, there
proceeding to the rating task.                                      were a few participants who reselected images at about
   Next, participants were shown 20 images in a 4 x 5 array.        chance rates, which suggests that they were selecting images
These images were drawn randomly from the target                    at random with no regard for the task. We identified 19
                                                                2563

Figure 2: The five images rated most typical by participants, from the ten largest categories in
                                        the database.
                                            2564

participants who reselected the same images as both best
and worst on at least 25% of trials (2% of total participants).
Together these participants had submitted 872 trials (1.13%
of trials), which were dropped from further analysis.
   A “typicality score” was obtained for each image in the
dataset. The typicality score was calculated as the number
of times the image had been selected as the best example of
its category, minus a fraction1 of the number of times it was
selected as the worst example, divided by the number of
times the image appeared throughout the experiment.
(Taking a fraction of the worst votes allows the number of
“best” votes to be used as a tie-breaker for images that
performed similarly.) A typicality score near 1 means an
image is extremely typical (it was selected as the best
example of its category nearly every time it appeared in the
experiment), and a typicality score near -1 means an image
is extremely atypical (it was nearly always selected as a
worst example).
   Examples of the most typical images from various
categories are shown in Figure 2.
Comparison to chance                                                    Figure 3: A comparison of the typicality scores obtained by
Even if participants selected “best” and “worst” examples at              the best image in each category in the expeirment to the
random, some images in each category would emerge as                      typicality scores obtained in a simulation where images
highly typical (or atypical) due to chance. It is important to                              were rated randomly.
check that the most typical images in this experiment are
rated higher than would be expected if participants were                  However, there are some categories where the most
simply responding randomly.                                            typical image scores no higher than would be expected from
   To check this, we ran a set of 100 simulations in which             chance (46 categories have most typical image that is within
the images were rated randomly. Each image appeared in                 a standard deviation of the average best score from the
the simulation the same number of times it appeared in the             simulation). It’s not clear why participants gave chance-like
actual experiment. On each appearance, the image had a                 performance in these categories: although some of these
15% chance of being voted a “best” example, a 15% chance               categories are very unusual (e.g., rectory, cloister), many of
of being voted a “worst” example, and a 70% chance of                  them are familiar everyday categories like closet, desert
receiving no vote. The simulation assumed that participants            road, or factory. This may reflect the distribution of
never selected the same image as both “best” and “worst” in            exemplars we were able to obtain for these categories in the
a single trial, which was not actually true in the experiment.         initial image search: it’s possible that the images we
This means that the simulation actually overestimates the              collected for these categories were fairly homogeneous,
typicality scores that could be produced by random                     with no particularly good or bad exemplars.
responses.
   As shown in Figure 3, the typicality scores obtained by               Typicality and models of scene classification
the most typical images in the experiment are much higher              Do more typical exemplars of a scene category contain more
than the maximum scores that would be expected if                      of the visual features relevant to scene classification? To
participants were rating images randomly. More than half of            investigate this question, we classified scenes using the “all
the categories (401 out of 706) have a most typical image              global features” classifier described in Xiao, et al. (2010). In
that is at least 3 standard deviations higher than the average         computer vision, global features represent a class of
“most typical image” from the simulation. This indicates               algorithms that encode the spatial layout of textures in the
that participants were selecting these images according to a           image, without representing object information.
strategy (such as selecting images that best matched their
internal prototype for the scene category) and not just                Global Features
selecting images at random.                                               As in Xiao et al. (2010), the all-feature kernel combines
                                                                       several representations that have been shown to be reliable
                                                                       for scene classification tasks. The GIST descriptor computes
   1
     This fraction was arbitrarily set to 0.9, but any value in the    the output energy of 24 filters (8 orientations at 4 different
range 0.500 to 0.999 gives essentially the same results: changing      scales) averaged on a 4x4 grid (Oliva & Torralba 2001). The
this value changes the range of possible scores, but doesn’t           Dense SIFT features (Lazebnik, et al., 2006) builds a
significantly change the rank order of scores within a category
                                                                       coarse-to-fine spatial histogram pyramid of quantized
(90% of images move by less than 5 percentile points).
                                                                   2565

orientation histograms of image magnitude and orientation           is 38% (chance performance is 0.25%). What are the
values on local patches. The HOG features (Dalal & Triggs,          performances as a function of scene typicality? Figure 4
2005; Felzenszwalb, et al., 2010) count occurrences of              shows that classification of individual images varies with
gradient orientation and use overlapping local contrast for         their typicality score: the most typical images were
normalization to improve invariance to changes in                   classified correctly about 50% of the time, and the least
illumination or shadowing. While SIFT is known to be very           typical images were classified correctly only 23% of the
good at finding repeated image content, the self-similarity         time. Images were divided into four groups corresponding
descriptor (SSIM) (Shechtman & Irani, 2007) relates images          to the four quartiles of the distribution of typicality scores
using their internal layout of local self-similarities. Unlike      across the database. These groups contained 5020, 4287,
GIST, SIFT, and HOG, which are all gradient-based                   5655, and 4908 images (groups are listed in order from
approaches (measuring the density of the features), SSIM            fourth quartile -- lowest typicality – to first quartile). A one-
may provide a distinct, complementary measure of scene              way ANOVA comparing these quartile groups shows a
layout. Additionally, the “all-features” kernel includes            significant effect of image typicality quartile on
histograms for specific geometric classes as determined by          classification accuracy (F(3,19846) = 278, p < .001);
Hoiem et al. (2005), which represent aspects of a scene’s           Bonferroni-corrected post-hoc tests show that the
spatial layout.                                                     differences between each quartile are significant.
   The “all global features” classifier is built from the large        Image typicality is also related to the confidence of the
set of classifiers based on these state-of-the-art features. It     SVM classifier. The confidence reflects how well the
covers a range of features which are likely to be important         classifier believes the image matches its assigned category –
in scene recognition, including color histograms,                   scores near 1 indicate that the classifier is very confident
representations of texture and scene regions (e.g., ground vs.      that the image belongs in the category and scores near -1
sky), and information about edges and line orientations.            indicate that the classifier does not believe the image
                                                                    belongs in the category. (Due to the difficulty of the one-
Classification procedure                                            versus-all classification task, confidence was low across all
Classifiers were trained with one-versus-all support vector         classifications, and even correctly-classified images had
machines as in Xiao et al (2010). In order to have enough           average confidence scores below zero.) Figure 5 shows the
exemplars for training and testing, the following simulations       SVM confidence as a function of image typicality for
used the 397 categories that contain at least 100 exemplars.        correctly- and incorrectly-classified images. Confidence
From each category, 50 images were selected at random to            increases with increasing typicality, but this pattern is
serve as the training set, and another 50 images were               stronger in correctly-sorted images. A 4 x 2 ANOVA gives
randomly selected to serve as the test set. Since the training      significant main effects of image typicality (F(3,19842) =
and testing sets were chosen by random selection, they              79.8, p < .001) and correct vs. incorrect classification
contained a range of more and less typical exemplars.               (F(1,19842) = 6006, p < .001) and a significant interaction
   Xiao et al. (2010) found that the average performance of         between these factors (F(3,19842) = 43.5, p < .001).
the “all global features” classifier on this 397-scene dataset
Figure 4: Performance of the SVM classifier as a function of         Figure 5: Confidence of the SVM classifier as a function of
    image typicality. Images are sorted according to their               image typicality. Images are sorted according to their
   typicality score from least typical (4th quartile) to most           typicality score from least typical (4th quartile) to most
                     typical (1st quartile).                                              typical (1st quartile).
                                                                2566

                         Conclusion                             Blanz, V., Tarr, M. J., & Bülthoff, H. H. (1999). What
                                                                  object attributes determine canonical views? Perception,
   Intelligent systems, artificial and biological, face the
                                                                  28, 575-599.
problem of how to organize complex stimulus
                                                                Dalal, N., & Triggs, B. (2005). Histogram of oriented
representations. One framework for classifying scenes
                                                                  gradient object detection. In Proc. IEEE Conf. Computer
involves identifying visually informative features within a
                                                                  Vision and Pattern Recognition, 886-893.
category. Previous attempts to characterize the categorical
                                                                Epstein, R. & Kanwisher, N. (1998) A cortical
representation of environmental scenes have capitalized on
                                                                  representation of the local visual environment. Nature,
uncovering a manageable set of dimensions, features, or
                                                                  392, 598-60.
objects with which to represent environments (Oliva &
                                                                Felzenszwalb, P., Girshick, R., McAllester, D., & Ramanan,
Torralba, 2001; Renninger & Malik, 2004; Fei-Fei &
                                                                  D. (2010). Object detection with discriminatively trained
Perona, 2005; Lazebnik et al., 2006; Vogel & Schiele, 2007;
                                                                  part based models. IEEE Trans. on Pattern Analysis and
Greene & Oliva, 2009; Ross & Oliva, 2010).
                                                                  Machine Intelligence, 32, 1627-1645.
   An alternate framework for classifying visual scenes
                                                                Fei-Fei, L., & Perona, P. (2005). A bayesian hierarchical
appeals to their conceptual nature. Scenes, like individual
                                                                  model for learning natural scene categories. In
objects, are associated with specific functions and
                                                                  Proceedings of Computer Vision and Pattern
behaviors, and have a categorical structure (Tversky &
                                                                  Recognition, 524-531.
Hemenway, 1983). Here, we show that people have a
                                                                Greene, M.R., & Oliva, A. (2009). Recognition of natural
representation of a typical or “best” exemplar for a wide
                                                                  scenes from global properties: Seeing the forest without
range of scene categories. This elaborates on the scene
                                                                  representing the trees. Cognitive Psychology, 58, 137-179.
prototype work of Tversky and Hemenway, and extends
                                                                Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond bags
prototype research from the domains of objects, faces, and
                                                                  of features: Spatial pyramid matching for recognizing
abstract patterns to scenes.
                                                                  natural scene categories. In Proceedings of Computer
   Furthermore, we show that scenes which people rate as
                                                                  Vision and Pattern Recognition, 2169-2178.
more typical examples of their category are more likely to
                                                                Oliva, A., & Torralba, A. (2001). Modeling the shape of the
be correctly classified by computer vision algorithms based
                                                                  scene: A holistic representation of the spatial envelope.
on global image features. Although we cannot claim that the
                                                                  International Journal in Computer Vision, 42, 145-175.
features used in these algorithms are the same features
                                                                Posner, M. I., & Keele, S. W. (1968). On the genesis of
which humans would use to perform the same classification
                                                                  abstract ideas. Journal of Experimental Psychology, 77,
task, this nevertheless indicates that more typical examples
                                                                  353-363.
of a scene category contain more of the diagnostic visual
                                                                Renninger, L.W., & Malik, J. (2004). When is scene
features that are relevant for scene categorization.
   Finally, this study is the first to show that reliable         recognition just texture recognition? Vision Research, 44,
                                                                  2301-2311.
prototypes can be identified for a very large dataset of
                                                                Rosch, E. (1973). Natural categories. Cognitive Psychology,
environmental scene categories, by both human observers
                                                                  4, 328-350.
and state of the art vision algorithms. One of the important
distinctions between objects and scenes is that the             Rosch, E., Mervis, C., Gray, W., Johnson, D., & Boyes-
categorical boundaries between scenes are less well defined       Braem, P. (1976). Basic objects in natural categories.
than the boundaries between objects. Natural scenes in            Cognitive Psychology, 8, 382-439.
                                                                Ross, M.G., & Oliva, A. (2010). Estimating perception of
particular often lie on the boundary between two or more
categories, like forest/mountain or river/lake (Vogel &           scene layout properties from global image features.
Schiele, 2004), suggesting that typicality might be a             Journal of Vision, 10, 1-25.
particularly important concept for future progress in the       Shechtman, E., & Irani, M. (2007). Matching local self-
field of human and computational scene understanding.             similarities across images and videos. In Proc. IEEE
                                                                  Conf. Computer Vision and Pattern Recognition.
                                                                Tversky, B., & Hemenway, K. (1983). Categories of
                    Acknowledgments                               environmental scenes. Cognitive Psychology, 15, 121-
K.A.E. is funded by an NSF graduate research fellowship.          149.
This work is funded by NSF CAREER award (0546262),              Vogel, J., & Schiele, B. (2004). A semantic typicality
NSF grants (0705677 and 1016862), and a NEI grant                 measure for natural scene categorization. In Pattern
(EY02484) to A.O; NSF CAREER award (0747120) to A.T;              Recognition Symposium DAGM 2004.
and Google research awards to A.O and A.T.                      Vogel, J., & Schiele, B. (2007). Semantic model of natural
                                                                  scenes for content-based image retrieval. International
                         References                               Journal of Computer Vision, 72, 133-157.
Biederman, I. (1987). Recognition-by-Components: A              Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., & Torralba, A.
   Theory of Human Image Understanding. Psychological             (2010). SUN Database: Large scale scene recognition
   Review, 94, 115-147.                                           from abbey to zoo. In Proc. IEEE Conf. on Computer
                                                                  Vision and Pattern Recognition.
                                                            2567

