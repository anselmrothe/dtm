UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Simple Sequential Algorithm for Approximating Bayesian Inference
Permalink
https://escholarship.org/uc/item/40t6n43w
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Bonawitz, Elizabeth
Denison, Stephanie
Chen, Annie
et al.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

           A Simple Sequential Algorithm for Approximating Bayesian Inference
           Elizabeth Bonawitz, Stephanie Denison, Annie Chen, Alison Gopnik, & Thomas L. Griffiths
                                   {liz b, smdeniso, c annie, gopnik, tom griffiths}@berkeley.edu
                                              Department of Psychology, 5427 Tolman Hall
                                                        Berkeley, CA 94720 USA
                             Abstract                                  – causal learning. These algorithms need to approximate
                                                                       Bayesian inference, but also need to be computationally
   People can apparently make surprisingly sophisticated in-
   ductive inferences, despite the fact that there are constraints     tractable. One strategy that has proven effective for approx-
   on cognitive resources that would make performing exact             imating Bayesian inference in computer science and statis-
   Bayesian inference computationally intractable. What algo-          tics is using sampling-based approximations, also known as
   rithms could they be using to make this possible? We show that
   a simple sequential algorithm, Win-Stay, Lose-Shift (WSLS),         Monte Carlo methods. We introduce a new sequential sam-
   can be used to approximate Bayesian inference, and is consis-       pling algorithm based on the Win-Stay, Lose-Shift (WSLS)
   tent with human behavior on a causal learning task. This al-        principle, in which a learner maintains a particular hypothe-
   gorithm provides a new way to understand people’s judgments
   and a new efficient method for performing Bayesian inference.       sis until receiving evidence that is inconsistent with that hy-
   Keywords: Bayesian inference; algorithmic level; causal             pothesis. We show that this WSLS algorithm approximates
   learning                                                            Bayesian inference, and can do so quite efficiently.
                                                                          Previous work in cognitive psychology has shown that peo-
                         Introduction                                  ple follow a WSLS strategy in concept learning tasks (Restle,
In the last five years a growing literature has demon-                 1962; Levine, 1975). We use this as the starting point for an
strated that people often act in ways consistent with optimal          investigation of whether human behavior that approximates
Bayesian models (e.g., Griffiths & Tenenbaum, 2005; Good-              Bayesian inference in causal learning might be explained in
man, Tenenbaum, Feldman, & Griffiths, 2008). These ap-                 terms of a WSLS strategy. We compare the WSLS algo-
proaches have provided a precise framework for characteriz-            rithm to simply sampling from the posterior distribution as
ing intuitive theories and have provided an account of how             an account of human behavior in a simple causal learning
a learner should update her beliefs as evidence is acquired            task. Both algorithms predict that people should behave in
(Gopnik & Schulz, 2004; Griffiths & Tenenbaum, 2009). The              a way that is consistent with Bayesian inference, but WSLS
theory-based Bayesian approach has met with much success               also predicts that there should be a characteristic pattern of
in describing the inferences made by adults (for a review see          dependency between people’s successive responses.
Tenenbaum, Griffiths, & Kemp, 2006) and research in cog-                  The plan of the paper is as follows. First, we introduce
nitive development suggests that children can make similar             the causal learning task that will be the focus of our analysis,
inferences (Gopnik & Schulz, 2007; Gopnik et al., 2004;                and summarize how Bayesian inference can be applied in this
Gweon, Schulz, & Tenenbaum, 2010; Kushnir & Gopnik,                    task. We then introduce the idea of sequential sampling algo-
2007; Schulz, Bonawitz, & Griffiths, 2007). Taken together,            rithms, including our new WSLS algorithm. This is followed
this research strongly suggests that Bayesian statistics pro-          by a mathematical analysis of the WSLS algorithm, show-
vides a productive starting point for understanding human in-          ing that it approximates Bayesian inference. The remainder
ductive inference.                                                     of the paper focuses on an experiment in which we evaluate
   Theory-based Bayesian approaches have typically been                how well the WSLS algorithm captures people’s judgments
used to give a “computational level” (Marr, 1982) analysis             in our causal learning task.
of the inferences people make when solving inductive prob-
lems, focusing on the form of the computational problem and                   Bayesian inference and causal learning
its ideal solution. However, it need not be the case that the          While the algorithms that we present in this paper will apply
algorithms people are using to solve these problems actually           to any inductive problem with a discrete hypothesis space,
resemble exact Bayesian inference. Indeed, given the compu-            we will make our analysis concrete by focusing on a simple
tational complexity of exact Bayesian inference (Russell &             causal learning problem. In this problem, there are three cat-
Norvig, 2002) and the numerous findings that children and              egories of objects: red, green, and blue blocks. Each of these
adults alike have difficulty with explicit hypothesis testing          kinds of blocks activate a machine with different probability
(e.g., Kuhn, 1989; Klahr, Fay, & Dunbar, 1993) and some-               when they are placed on the machine. The red blocks acti-
times only slowly progress from one belief to the next (Carey,         vate the machine on five out of six trials, the green blocks on
1991; Wellman, 1990), it becomes interesting to ask how                three out of six trials, and the blue blocks on just once out of
learners might be behaving in a way that is consistent with            six trials. A new block is then presented, which has lost its
Bayesian inference.                                                    color, and needs to be classified as either a red, green, or blue
   Here we investigate the algorithms that learners might              block, based on some observations of what happens when it
be using in solving a particular kind of inductive problem             is placed on the machine over a series of trials. The question
                                                                   2463

is what people will infer about the causal properties of this                     Independent sampling is the simplest kind of Monte Carlo
block, and which class of blocks it belongs to.                                method, and is thus a parsimonious place to start in consid-
   Given this hypothesis space, we can consider how an ideal                   ering the algorithms learners might use. In particular, the
learner should update his or her beliefs in light of the evidence              problem of sequentially updating a posterior distribution in
provided by its interaction with the machine. Assume that                      light of evidence can be solved approximately using sequen-
the learner begins with a prior probability distribution over                  tial Monte Carlo methods such as particle filters (Doucet,
hypotheses, P(h), where the probability assigned to each hy-                   Freitas, & Gordon, 2001). A particle filter approximates the
pothesis reflects the degree of belief in each hypothesis being                probability distribution over hypotheses at each point in time
true before seeing any data. Given some observed data d, re-                   with a set of samples (or “particles”), and provides a scheme
flecting whether the block activates the machine on a single                   for updating this set to reflect the information provided by
trial, the learner obtains a posterior distribution over hypothe-              new evidence. The behavior of the algorithm depends on the
ses, P(h|d), via Bayes’ rule:                                                  number of particles. With a very large number of particles,
                                                                               each particle is similar to a sample from the posterior. With a
                                       P(d|h)P(h)
                       P(h|d) =                                         (1)    small number of particles, there can be strong sequential de-
                                  ∑h0 ∈H P(d|h0 )P(h0 )                        pendencies in the representation of the posterior distribution.
where P(d|h) is the likelihood, indicating the probability of                  Recent work has explored particle filters as a way to explain
observing d if h were true, and H is the hypothesis space.                     patterns of sequential dependency that arise in human induc-
   Often, Bayesian inference is performed in a sequential set-                 tive inference (Sanborn, Griffiths, & Navarro, 2006; Levy,
ting, with a series of observations being made one after an-                   Reali, & Griffiths, 2009).
other, and the posterior distribution being updated after each                    Particle filters have many degrees of freedom, with
observation. This is the case with our causal learning prob-                   many different schemes for updating particles being possible
lem, where we receive a sequence of observations of the block                  (Doucet et al., 2001). They also require learners to maintain
interacting with the machine on successive trials, rather than                 multiple hypotheses at each point in time. Here, we inves-
a single observation. Letting d1 , . . . , dn denote observations              tigate a simpler algorithm that assumes learners maintain a
after n trials, we are interested in the posterior distribution                single hypothesis, resampling from the posterior with a prob-
P(h|d1 , . . . , dn ). This can be computed via Equation 1, substi-            ability dependent on the degree to which the hypothesis is
tuting d1 , . . . , dn for d. However, it can be simpler to follow a           contradicted by data. This is similar to using a particle fil-
sequential updating rule, which allows us to compute the pos-                  ter with just a single particle, with a computationally expen-
terior after observing d1 , . . . , dn+1 from the posterior based on           sive resampling step being more likely to be carried out as
d1 , . . . , dn . Formally, this is                                            that particle becomes inconsistent with the data. Because of
                                    p(dn+1 |h)p(h|d1 , . . . , dn )            its tendency to maintain a hypothesis that makes a successful
      P(h|d1 , . . . , dn+1 ) =                                         (2)    prediction and change hypotheses when this is not the case,
                                ∑h0 p(dn+1 |h0 )p(h0 |d1 , . . . , dn )
                                                                               we call this the Win-Stay, Lose-Shift (WSLS) algorithm.
where we assume that the observations di are conditionally                        The WSLS principle has a long history both in computer
independent given h (i.e., that a block has an independent                     science, where it appears as a heuristic algorithm in rein-
chance of activating the machine on each trial, once its color                 forcement learning and game theory (Robbins, 1952; Nowak
is known).                                                                     & Sigmund, 1993), and in psychology, where it has been
                                                                               proposed as an account of human concept learning (Restle,
                 Sequential sampling algorithms
                                                                               1962). The WSLS strategy has also been shown in chil-
The Bayesian analysis presented in the previous section pro-                   dren, especially between the ages of three- to five-years-old
vides an abstract, “computational level” (Marr, 1982) char-                    (Levine, 1975). More recently, WSLS has been analyzed as
acterization of causal induction, identifying the underlying                   a simple model of learning that leads to interesting strategies
problem and how it might best be solved. We now turn to the                    in game theory (Nowak & Sigmund, 1993). In the remain-
problem of how to approximate this optimal solution. Sim-                      der of the paper, we show that this kind of strategy can yield
ply implementing Bayesian inference by listing all hypothe-                    a simple method for approximating Bayesian inference, and
ses and then updating them following Bayes’ rule quickly be-                   appears to be consistent with human behavior.
comes intractable, as it requires considering each hypothesis
after every observation. We thus consider the possibility that                   Analyzing the Win-Stay, Lose-Shift algorithm
people may be approximating Bayesian inference by follow-
ing a procedure that produces samples from the posterior dis-                  A first step towards exploring the WSLS algorithm is to show
tribution. This idea is consistent with the prevalence of Monte                that it can be used to approximate Bayesian inference. In
Carlo methods for approximating Bayesian inference in com-                     this section, we define the WSLS algorithm we will be ana-
puter science and statistics (e.g., Robert & Casella, 2004), as                lyzing, and contrast it to simply sampling from the posterior
well as with behavioral evidence that people select hypothe-                   distribution (which we will term Random Sampling, or RS).
ses in proportion to their posterior probability (Goodman et                   Random Sampling assumes that learners draw a new sample
al., 2008; Denison, Bonawitz, Gopnik, & Griffiths, 2010).                      from the posterior distribution every time they need to make
                                                                           2464

a response, which requires evaluating all hypotheses or using                          which is just p(hn+1 = h|d1 , . . . , dn+1 ). This is because
a sequential algorithm such as a particle filter with a large set                                                                                          
of particles. Taking independent samples from the posterior                                                                        p(dn+1 |hn+1 = h)
                                                                                                 E(φ) = E p(hn |d1 ,...,dn ) c
distribution has two consequences. First, when we consider                                                                         p(dn+1 |d1 , ..., dn )
the behavior of a group of people, the proportion of people                                                     ∑   p(d   n+1 |h)p(h|d  1 , ..., dn )
selecting each hypothesis will match the posterior probabil-                                            =c h                                          =c
                                                                                                                       p(dn+1 |d1 , ..., dn )
ity. Second, successive responses from an individual will be
independent of one another. We will show that WSLS shares                              This provides us a simple set of conditions under which
the first of these properties with RS, but not the second, mak-                        our criterion is satisfied, with q(hn+1 = h|d1 , ..., dn+1 ) =
ing it possible to separate these two algorithms empirically.                          p(hn+1 = h|d1 , ..., dn+1 ) for any c such that φ ∈ [0, 1].
   The simplest version of the WSLS algorithm assumes that                                There are two interesting special cases to consider. The
learners maintain their current hypothesis provided they see                           first arises when we take c = p(dn+1 |d1 , ..., dn+1 ). In this case,
data that are consistent with that hypothesis, and generate a                          φ = p(dn+1 |hn+1 = h). This results in a simple algorithm that
new hypothesis otherwise. This is the version explored by                              makes a choice to resample based on the likelihood associated
Restle (1962). It is relatively straightforward to show that this                      with the current observation, given the current h. That is,
can approximate Bayesian inference in cases where the like-                            with probability proportional to this likelihood, the learner
lihood function p(di |h) is deterministic, giving a probability                        resamples from the full posterior.
of 1 or 0 to any observation di for every h, and observations                             The second special case is the most efficient algorithm of
are independent conditioned on hypotheses. More precisely,                             this kind, in the sense that it minimizes the rate at which sam-
the marginal probability of selecting a hypothesis hn given                            pling from the posterior is required. This corresponds to tak-
data d1 , . . . , dn is the posterior probability p(hn |d1 , . . . , dn ),             ing c = max
                                                                                                  p(dn+1 |d1 ,...,dn+1 )                          p(d   |h    =h)
                                                                                                                         , resulting in φ = maxn+1p(dn+1 |h) . For
                                                                                                       h p(dn+1 |h)                                   h    n+1
provided that hypotheses are generated from the posterior dis-
                                                                                       some hypothesis spaces, it may be possible to compute φ in
tribution whenever the learner chooses to shift hypotheses.
                                                                                       advance for all possible data and hypotheses. After this single
   We now turn to a proof of the more general case, in non-
                                                                                       costly computation is complete, the learner need only look up
deterministic settings. We will do this by considering the con-
                                                                                       the values.
ditions required for an argument by induction to apply. First,
                                                                                          This proof shows that the marginal distribution over hy-
we assume that hn ∼ p(hn |d1 , . . . , dn ). We define the transi-
                                                                                       potheses after observing dn will be the same for any n. How-
tion kernel of the WSLS algorithm, q(hn+1 |hn ), to be:
                                                                                      ever, there are still important differences in what Win-Stay,
                               δ(hn )             with probability φ                   Lose-Shift predicts for the dependency between guesses for
hn+1 |hn ∼
                     p(hn+1 |d1 , . . . , dn+1 ) with probability 1 − φ                a particular individual as compared to Random Sampling.
                                                                                       Namely, there is no dependency between hn and hn+1 in RS,
where δ(h) is the distribution putting all of its mass on h,
and φ is the probability of staying, which is a function of                            but there is for WSLS: if the data are consistent with hn ,
d1 , . . . , dn+1 and hn . The distribution over hypotheses after                      then the learner will retain hn with probability proportional
observing dn+1 is given by                                                             to p(di |hn ) rather than randomly sampling hn+1 from the pos-
q(hn+1 = h|d1 , . . . , dn+1 )                                                         terior distribution. We can use this difference to attempt to
                                                                                       diagnose the algorithm that people are using when they are
   =        ∑ q(hn+1 = h|hn )p(hn |d1 , ..., dn )                                      solving a causal learning problem.
             hn
                                                            
   =        ∑    δ(hn , j)φ + (1 − φ)p(hn+1 |d1 , ..., dn+1 ) p(hn |d1 , ..., dn )           Evaluating inference strategies in people
             hn
   =        p(hn+1 = h|d1 , ..., dn+1 )(1 − E(φ)) + φp(hn+1 = h|d1 , ..., dn )         We now turn to the question of whether people’s responses
                                                                                       are well captured by the algorithms described above. Namely,
                                                                                       we might expect that if participants behave in ways consistent
where the expectation, E(φ), is with respect to                                        with the WSLS algorithm we should observe dependencies
p(hn |d1 , ..., dn ).                                                                  between their responses; specifically, participants should re-
   We now examine the conditions on φ such that                                        tain hypotheses that are consistent with the evidence, and re-
q(hn+1 |d1 , . . . , dn+1 ) = p(hn+1 |d1 , . . . , dn1 ), corresponding to             sample proportional to the likelihood, p(d|h). However, if
the conditions required for the marginal distribution under                            participants behave in ways consistent with Random Sam-
WSLS to match the posterior. If we take                                                pling, then responses will be resampled from the posterior
                 p(hn+1 = h|d1 , ..., dn+1 )        p(dn+1 |hn+1 = h)                  distribution regardless of previous guesses, such that there are
       φ=c                                      =c
                   p(hn = h|d1 , ..., dn )          p(dn+1 |d1 , ..., dn )             no dependencies between responses.
where c is a constant which can depend on d1 , . . . , dn+1 but is                     Methods
invariant over hypotheses, we obtain
                                                                                       Participants and Design Participants were 65 undergradu-
q(hn+1 = h|d1 , . . . , dn+1 )                                                         ates recruited from an introductory psychology course. The
   =         p(hn+1 = h|d1 , ..., dn+1 )(1 − c) + cp(hn+1 = h|d1 , ..., dn+1 ).participants were split into 2 conditions (N = 28 in the “On-
                                                                                   2465

First” condition; N = 32 in the “Off-First” condition; 5 par-
ticipants were excluded for not completing the experiment).
Stimuli Stimuli consisted of 13 white cubic blocks (1cm3 ).
Twelve blocks had custom-fit sleeves made from construction
paper of different colors: 4 red, 4 green, and 4 blue. An ac-
tivator bin large enough for 1 block sat on top of a [15” x
18.25” x 14”] box. Attached to this box was a helicopter toy
that lit up when activated. The activator button for the toy was
inside the box hidden from view. There was a set of On cards
that pictorially represented the toy in the on position, and a
set of Off cards that pictorially represented the toy in the off
position. Because participants were tested in large groups, a
computer slideshow that depicted the color of the blocks and
the cards was used to illustrate the evidence shown.
                                                                     Figure 1: Bayesian posterior probability and human data for
                                                                     each block, red (R), green (B), and blue (B) after observing
Procedure Participants in each condition were tested on              each new instance of evidence, using parameters estimated
separate days in two large groups. Participants were in-             from fitting the Bayesian model to the data.
structed to record responses using paper and pen and not to
change answers provided for previous questions after view-
ing subsequent evidence. Participants were told that different       the participants were told, ”It’s okay if you keep thinking it is
blocks possess different amounts of “blicketness,” a fictitious      the same color and it is also okay if you change your mind.”
property. Blocks that possess the most blicketness almost            In the On-first condition the toy turned on for the first trial,
always activate the machine, blocks with very little blick-          and did not activate on the three subsequent trials. In the Off-
netness almost never activate the machine, and blocks with           first condition the toy did not activate on the first trial, but
medium blicketness activate the machine half of the time. A          turned on for the three subsequent trials.
red block was chosen at random and placed in the activator
bin. The helicopter toy either turned on or remained in the          Results
off position. The experimenter explained that a correspond-          Comparison to Bayesian inference Responses were
ing On or Off card was placed on the table to depict the event       uniquely and unambiguously categorized as “red”, “green”,
and the computer slidehow slide showed the same evidence.            and “blue”. There was a slight bias to favor green blocks
The card remained on the table and the computer slideshow            (60%), with red (25%) and blue (15%) blocks being less fa-
remained on the screen throughout the experiment. After 5            vored.1 We determined the parameters for the prior distri-
more repetitions using the same red block for a total of 6           bution and likelihood in two ways. For the first way (“initial
demonstrations, participants were told that red blocks have          responses”) priors were determined by the participants’ initial
the most blicketness (they activated 5/6 times). The same            block color predictions and the likelihood of block activation
procedure was repeated for the blue and green blocks with            was determined by the initial observations of block activa-
the blue blocks having very little blicketness (activating 1/6       tions during the demonstration phase (5/6 red, 1/2 green, 1/6
times), and green blocks having medium blicketness (activat-         blue). For the second way (“maximized”) we searched for the
ing 3/6 times). All evidence remained visible on the computer        set of priors and the likelihood activation weights that would
slideshow. To ensure that participants were paying attention,        maximize the log-likelihood for the model.2 Using either set
they were asked to match each color to the proper degree of          of parameters, participant responses were well captured by
blicketness (most, very little, medium) by writing down their        the posterior probability (initial responses: r(22) = .76, p <
responses.                                                           .001; maximized: r(22) = .85, p < .0001, see Figure 1). The
    After the memory check, a novel white block that lost its        primary difference between the model and data is that peo-
fitted-sleeve was presented and participants were asked to           ple seem to change their beliefs more strongly than the model
write down an initial guess about what color fitted-sleeve the       predicts. This may be a consequence of pedagogical reason-
white block should have (red, green, or blue). The white             ing, a point we return to in the Discussion.
block was then placed into the activator bin four times and              1 Such a bias is consistent with people’s interest in non-
each time the participant saw whether or not the toy acti-           determinism; the green blocks were the most stochastic in that they
vated. After each demonstration, the appropriate On or Off           activated on exactly half the trials.
card was chosen and the slideshow was advanced to repre-                 2 The maximized priors were .27 red, .48 green, .25 blue; these
sent the state of the toy. Participants were asked to record         priors correspond strongly to the priors represented by participants.
                                                                     The maximized likelihood was .85 red, .5 green, .16 blue which also
their best guess about what color they believed the block to         corresponds strongly to the likelihood given by the initial activation
be after each demonstration, but before each guess was made          observations.
                                                                 2466

Comparison to WSLS and RS To compare people’s re-
sponses to the WSLS and RS algorithms, we first calculated
the “switch” probabilities under each model in the two ways
previously described: using the parameters from the initial
responses and using the previously estimated maximized pa-
rameters. Calculating switch probabilities for RS is relatively
easy: because each sample is independently drawn from the
posterior, the switch probability is simply calculated from the
posterior probability of each hypothesis after observing each
piece of evidence. Switch probabilities for WSLS were cal-
culated such that resampling is based only on the likelihood
associated with the current observation, given the current h.        Figure 2: Correlations between the probability of switching
That is, with probability equal to this likelihood, the learner      hypotheses in the models given the maximized parameters
resamples from the full posterior. Responses were much bet-          and the human data, for (a) the Win-Stay Lose-Shift algo-
ter captured by the WSLS algorithm using the maximized pa-           rithm and (b) Random Sampling.
rameters (r(15) = .81, p < .0001) and the parameters given
by participant initial responses (r(15) = .78, p < .001) as
compared to the RS algorithm (maximized: r(15) = .58, p =            independently sampling responses each time from the poste-
.02; initial responses: r(15) = .39, p = ns). See Figure 2.          rior. These results extend previous work exploring WSLS
We also computed the log-likelihood scores for both models.          strategies, showing that at least one strategy of this kind
The WSLS model better fit the data than the RS model (ini-           provides a viable way to approximate Bayesian inference,
tial responses: p(d|W SLS) = −221, p(d|RS) = −262; maxi-             demonstrating that causal induction contains problems from
mized: p(d|W SLS) = −215, p(d|RS) = −251). These results             this class, and providing evidence that WSLS is an appropri-
suggest that the pattern of dependencies between people’s re-        ate algorithm for describing people’s inferences.
sponses are better captured by the WSLS algorithm than by               Connecting the computational and algorithmic levels is a
an algorithm such as RS that produces independent samples.           significant challenge for Bayesian models of cognition and
                                                                     this is only a beginning step in understanding the psycho-
                          Discussion                                 logical processes at work in causal inference. We believe
                                                                     that there are several important directions for future research
Our results show how tracking learning at the level of the in-       in this area. First, it would be interesting to test the algo-
dividual can help us understand the specific algorithms that         rithm’s predictions across various psychological experiments
learners might be using to approximate Bayesian inference.           that have relied purely on a Bayesian inference approach;
First we introduced an algorithm, Win-Stay, Lose-Shift that          this would allow for a better assessment of the WSLS algo-
approximates Bayesian inference by maintaining a single hy-          rithm’s efficiency. Second, both algorithms can be seen as ex-
pothesis over time, and proved that the marginal distribution        treme versions of particle filters: Random Sampling in cases
over hypotheses after observing data will always be the same         where there are a large set of particles drawn from the poste-
for this algorithm as for sampling from the posterior (Random        rior and randomly drawing one member of the set at random
Sampling). That is, both algorithms return a distribution over       for each query; and, Win-Stay Lose-Shift, which is similar
responses consistent with the posterior distribution obtained        to using a single particle that is resampled from the posterior
from Bayesian inference. We provided an analysis of WSLS             when the particle becomes inconsistent with the data. There
with two special cases. The first case resulted in a simple          may be some value in exploring algorithms that lie between
algorithm that makes a choice to resample based on the like-         these extremes, with a more moderate number of particles as
lihood associated with the current observation, given the cur-       well as exploring algorithms that shift from one hypothesis
rent hypothesis. The second is the most efficient algorithm          to the next by modifying the current hypothesis in a princi-
of this kind in that it minimizes the rate at which sampling         pled and structured manner. Considering intermediate models
from the posterior is required, and may thus be of interest for      would also allow future work to examine the degree to which
approximating Bayesian inference in other settings.                  fewer or greater numbers of particles capture inference and
   Our analysis also made it clear that there are important dif-     to what degree these constraints change with age and expe-
ferences in what WSLS and RS predict for the dependency              rience. Third, we constrained our space to a modest number
between guesses, making it possible to separate these algo-          of hypotheses, but other work has begun to examine how hy-
rithms empirically. We explored the algorithms that peo-             pothesis spaces may be learned and simultaneously searched;
ple use for solving inductive inference problems through             this should be jointly developed with approaches taken here
an experiment using a simple causal learning task. In this           that explore the space of plausible algorithms that capture
experiment, people’s overall responses are consistent with           people’s causal inferences. Fourth, in this particular task,
Bayesian inference, but people show dependencies between             the aggregate distribution of adult responses shifted more dra-
responses characteristic of the WSLS algorithm, rather than          matically than the Bayesian model presented here predicted.
                                                                 2467

It is likely, given the context of showing participants a pre-            A rational analysis of rule-based concept learning. Cognitive Sci-
determined computer slideshow, that adults were making a                  ence, 32:1, 108-154.
                                                                        Gopnik, A., Glymour, C., Sobel, D., Schulz, L., Kushnir, T., &
pedagogical assumption (Shafto & Goodman, 2008) which                     Danks, D. (2004). A theory of causal learning in children: Causal
would better capture the data. Future work may investigate                maps and bayes nets. Psychological Review, 111, 1-31.
this possibility.                                                       Gopnik, A., & Meltzoff, A. N. (1997). Words, thoughts, and theo-
                                                                          ries. Cambridge, MA: MIT Press.
   Young children have particularly limited cognitive re-               Gopnik, A., & Schulz, L. (2004). Mechanisms of theory formation
sources (e.g., German & Nichols, 2003; Gerstadt, Hong, &                  in young children. Trends in Cognitive Science, 8, 371-377.
Diamond, 1994; Siegler, 1975), but are nonetheless capable              Gopnik, A., & Schulz, L. (Eds.). (2007). Causal learning: Psychol-
                                                                          ogy, philosophy, and computation. Oxford: Oxford University
of behaving in a way that is consistent with optimal Bayesian             Press.
models. Children must thus be especially adept at manag-                Griffiths, T., & Tenenbaum, J. (2009). Theory-based causal induc-
ing limited resources to approximate Bayesian inference. Ar-              tion. Psychological Review, 116, 661-716.
                                                                        Griffiths, T., & Tenenbaum, J. B. (2005). Structure and strength in
guably, many of the most interesting cases of belief revision             causal induction. Cognitive Psychology, 51, 354-384.
happen in the first few years of life (Wellman, 1990; Bul-              Gweon, H., Schulz, L., & Tenenbaum, J. (2010). Infants consider
lock, Gelman, & Baillargeon, 1982; Carey, 1985; Gopnik &                  both the sample and the sampling process in inductive generaliza-
                                                                          tion. Proceedings of the National Academy of Science, 107(20),
Meltzoff, 1997). Understanding more precisely how specific                9066-9071.
algorithms shape children’s learning may provide a potential            Klahr, D., Fay, A., & Dunbar, K. (1993). Heuristics for scientific
solution to the problem of how limited cognitive resources                experimentation: A developmental study. Cognitive Psychology,
                                                                          25, 111-146.
and Bayesian frameworks of children’s cognition can be rec-             Kuhn, D. (1989). Children and adults as intuitive scientists. Psy-
onciled. We are currently investigating these questions.                  chological Review, 96, 674-689.
                                                                        Kushnir, T., & Gopnik, A. (2007). Conditional probability versus
   While there is still important work to be done, connect-               spatial contiguity in causal learning: Preschoolers use new con-
ing the algorithmic level to the computational level is a first           tingency evidence to overcome prior spatial assumptions. Devel-
step in understanding the algorithms that learners may be                 opmental Psychology, 44, 186-196.
                                                                        Levine, M. (1975). A cognitive theory of learning: Research on
using to approximate Bayesian inference. We have demon-                   hypothesis testing. Hillsdale, NJ: Lawrence Erlbaum.
strated that the WSLS algorithm, previously provided as a               Levy, R., Reali, F., & Griffiths, T. L. (2009). Modeling the effects
model of human hypothesis testing, can be used to approxi-                of memory on human online sentence processing with particle fil-
                                                                          ters. In D. Koller, D. Schuurmans, Y. Bengio, & L. Bottou (Eds.),
mate Bayesian inference. This provides a way to perform se-               Advances in Neural Information Processing Systems 21 (pp. 937–
quential Bayesian inference while maintaining only a single               944).
hypothesis at a time, and leads to an efficient approximation           Marr, D. (1982). Vision. San Francisco, CA: W. H. Freeman.
                                                                        Nowak, M., & Sigmund, K. (1993). A strategy of win-stay, lose-
scheme that might be useful in computer science and statis-               shift that outperforms tit-for-tat in the prisoner’s dilemma game.
tics. We have also shown that a WSLS algorithm seems to                   Nature, 364, 56-58.
capture people’s judgments in a simple causal learning task.            Restle, F. (1962). The selection of strategies in cue learning. Psy-
                                                                          chological Review, 69, 329-343.
Our results add to the growing literature suggesting that even          Robbins, H. (1952). Some aspects of the sequential design of ex-
responses by an individual that may appear non-optimal may                periments. Bulletin of the American Mathematical Society, 58,
in fact represent an approximation to a rational process.                 527-535.
                                                                        Robert, C., & Casella, G. (2004). Monte Carlo statistical methods
                                                                          (2nd ed.). New York: Springer.
Acknowledgments. This research was supported by the McDon-              Russell, S. J., & Norvig, P. (2002). Artificial intelligence: A modern
nell Foundation Causal Learning Collaborative, grant IIS-0845410          approach (2nd ed.). Englewood Cliffs, NJ: Prentice Hall.
from the National Science Foundation, and grant FA-9550-10-1-           Sanborn, A. N., Griffiths, T. L., & Navarro, D. J. (2006). A more
0232 from the Air Force Office of Scientific Research.                    rational model of categorization. In Proceedings of the 28th An-
                                                                          nual Conference of the Cognitive Science Society. Mahwah, NJ:
                           References                                     Erlbaum.
                                                                        Schulz, L. E., Bonawitz, E. B., & Griffiths, T. L. (2007). Can being
Bullock, M., Gelman, R., & Baillargeon, R. (1982). The develop-           scared make your tummy ache? naive theories, ambiguous evi-
   ment of causal reasoning. In W. J. Friedman (Ed.), The develop-        dence, and preschoolers’ causal inferences. Developmental Psy-
   mental psychology of time (p. 209-254). New York: Academic             chology, 43, 1124-1139.
   Press.                                                               Shafto, P., & Goodman, N. (2008). Teaching games: Statistical
Carey, S. (1985). Conceptual change in childhood. Cambridge,              sampling assumptions for pedagogical situations. In Proceedings
   MA: MIT Press.                                                         of the 30th annual conference of the cognitive science society.
Carey, S. (1991). Knowledge acquisition: Enrichment or conceptual       Siegler, R. (1975). Defining the locus of developmental differences
   change? In S. Carey & S. Gelman (Eds.), Epigenesis of mind:            in children’s causal reasoning. Journal of Experimental Child
   Essays on biology and cognition. Hillsdale, NJ: Erlbaum.               Psychology, 20, 512-525.
Denison, S., Bonawitz, E., Gopnik, A., & Griffiths, T. (2010).          Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006). Theory-based
   Preschoolers rationally sample hypotheses. In Proceedings of the       Bayesian models of inductive learning and reasoning. Trends in
   32nd annual conference of the cognitive science society.               Cognitive Science, 10, 309-318.
Doucet, A., Freitas, N. de, & Gordon, N. (2001). Sequential Monte       Wellman, H. M. (1990). The child’s theory of mind. Cambridge,
   Carlo methods in practice. New York: Springer.                         MA: MIT Press.
German, T., & Nichols, S. (2003). Children’s inferences about long
   and short causal chains. Developmental Science, 6, 514-523.
Gerstadt, C., Hong, Y., & Diamond, A. (1994). The relationship
   between cognition and action: performance of children 3 - 7 years
   old on a stroop-like day-night test. Cognition, 53, 129-153.
Goodman, N., Tenenbaum, J., Feldman, J., & Griffiths, T. L. (2008).
                                                                    2468

