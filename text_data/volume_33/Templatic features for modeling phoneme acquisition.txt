UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Templatic features for modeling phoneme acquisition
Permalink
https://escholarship.org/uc/item/6wr7r81s
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Dupoux, Emmanuel
Beraud-Sudreau, Guillaume
Sagayama, Shigeki
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                            Templatic features for modeling phoneme acquisition
                                    Emmanuel Dupoux (emmanuel.dupoux@gmail.com)
                             Ecole des Hautes Etudes en Sciences Sociales, Ecole Normale Supérieure
                                                  29 rue d’Ulm, 75005 Paris, France
                               Guillaume Beraud-Sudreau (guillaumeberaud@gmail.com)
             Laboratoire de Sciences Cognitives et Psycholinguistique, Centre National de la Recherche Scientifique
                                                  29 rue d’Ulm, 75005 Paris, France
                                      Shigeki Sagayama (sagayama@hil.t.u-tokyo.ac.jp)
                                         Departement of Information Physics and Computing
                              University of Tokyo, 7-3-1, Hongo, Bunkyo-ku, Tokyo 113-8656 Japan
                             Abstract                                    These studies, however, did not use raw speech signals,
   We describe a model for the coding of speech sounds into a
                                                                      but rather a small number of parameters extracted by hand:
   high dimensional space. This code is obtained by computing         e.g., the frequency of the first and second formants, vowel
   the similarity between speech sounds and stored syllable-          duration, etc. (Valhabba et al 2007). This presupposes that
   sized templates. We show that this code yields a better linear     infants are equipped with fairly speech-specific perceptual
   separation of phonemes than the standard MFCC code.                abilities and, crucially, that they know how to segment the
   Additional experiments show that the code is tuned to a            continuous stream into discrete segments like consonants, or
   particular language, and is able to use temporal cues for the      vowels. This latter assumption is problematic given that
   purpose of phoneme recognition. Optimal templates seem to
   correspond to chunks of speech of around 120ms containing          such segmentation is not universal, but depends on the
   transitions between phonemes or syllables.                         phonology of the language (Dupoux et al, 1999).
                                                                         Varadarajan et al. (2008) is one of the few published
   Keywords: Early language acquisition, modeling, phonemes           paper that attempted to learn phonemes from raw speech.
                                                                      Using an optimized version of Successive State Splitting
                         Introduction                                 (SSS, Takami & Sagayama, 1992), they grew in an
   Infants spontaneously learn their ambient language at an           unsupervised fashion a large network of Hidden Markov
amazing speed. During their first year of life, they construct        Model (HMM) states. These states were shown to encode
abstract perceptual categories corresponding to the                   speech sounds with no loss of information compared to
phonemes of their language. They lose the ability to                  supervized HMMs, but there were two problems. First, the
distinguish fine phonetic variants that belong to the same            states of the HMM network did not correspond to
phoneme category, and enhance their ability to distinguish            phonemes, but rather to subphonemic units of the size of
between category contrasts (see a review in Kuhl, 2000).              acoustic events (e.g. burst, closure, transition, etc). This is
This is done without any supervision from the parents,                the oversegmentation problem. Second, even combining
before a substantial recognition lexicon has been built (12-          states into sequences did not yield phonemes, but rather,
month-olds are believed to recognize about 100 words), and            context dependant variants (contextual allophones). This is
before they can articulate correctly the phoneme categories           the contextual variability problem. Here, we address the
they recognize. How do infants achieve this? One possibility          first problem, the second problem being address in other
is that they perform some kind of unsupervised statistical            work (Peperkamp et al, 2006; Martin et al. submitted;
clustering of the ambient speech signals. Maye, Werker and            Boruta et al. 2011).
Gerken (2002) showed that 6-month-old infants perform                    The oversegmentation problem of SSS, although
such computations, using artificial languages with either a           disappointing, is not entirely surprising. State-of-the art
monomodal statistical distribution or a bimodal distribution          supervised HMMs have the same problem: segments are
of phonetic cues.                                                     typically modeled using three states, not a single state. The
   Only a limited number of studies have addressed the                reason is that HMMs represent speech as local spectral
computational mechanisms that could underlie such                     feature vectors (e.g. Mel-Frequency Cepstral Coeficients –
acquisitions. Guenther & Gjaja (1996) showed that Self-               MFCC, computed over a 15-20ms window), whereas
Organizing Maps have the potential to construct phoneme               phonemes are realized as a complex articulatory trajectory
categories in an unsupervised fashion (see also Valhabba et           spanning between 50 and 150ms, sometimes involving a
al., 2007; Gauthier, Shi & Xu, 2007). Valhabba et al (2007)           sequence of events (constriction, release, changes in the
implemented an incremental version of Expectation                     source, etc.). Since HMMs are modeling speech sounds
Maximization on a Gaussian Mixture model, and showed                  through Gaussians distributions (which are local), the only
that both the number of vowels and their statistical                  way to model phonemes accurately is to segment them into
distributions can be inferred from the signal in an                   subparts. This problem is not limited to MFCC features, but
unsupervised fashion.                                                 would also apply to any local feature, like for instance
                                                                  219

Figure 1. Outline of the High Dimensional Template Matching model. It is composed of an instance-based bank of reference
templates, and has two processing modes: during early language experience (dotted lines), the templates are segmented out of
the speech stream, during subsequent development and in adults, the signal is matched to the bank of templates (solid lines).
wavelet type functions (Smith & Lewicki, 2006), or features        segmenting and storing of syllable-sized templates, which
derived from auditory models (Chi et al, 2005).                    are the basis for discovering the smaller and more abstract
   To solve the oversegmentation problem, we propose to            phonemes, which can in turn be used to recover the even
explore the feasibility of replacing low dimensional, low-         more abstract linguistic features. In this paper, we explore a
level features with high dimensional, holistic or coarse           quantitative assessment of this last approach.
grained features. We review some existing proposals.
                                                                                         The algorithm
Holistic/templatic features                                        The idea of using examples from the problem set as the
Research into the human visual system has revealed that the        basis for representing further examples is at the core of
brain analyzes shapes and objects in a series of hierarchical      Support Vector Machine models (Cortes & Vapnik, 1995).
stages in which stimulus features of increasing complexity         The present proposal is inspired by the idea that large units
and size are extracted. Ullman et al. (2002) argued that the       like syllables are natural perceptual units for infants and
maximally informative features for the purpose of object           adults. For instance, Bertoncini & Mehler (1981) showed
classification are not local features, but rather features of      that neonates can count the number of syllables in a speech
intermediate complexity, that correspond to fragments of           stream, before they have learned the phonemes of their
images or objects. The brain would store such fragments            language. The proposal is that, during their first year of life,
which form a high dimensional code adapted to a particular         infants build a large base of syllable-like templates, and at a
domain of object perception. Along a similar line, Edelman         later stage, compute the similarity between the incoming
(1996) proposed that the brain represents a shape through its      signal and the stored templates. The High Dimensional
similarity to a number of reference shapes, that are stored as     Template Matching model (HD-TMatch) presented in
patterns of elementary features. Familiar and novel objects        Figure 1 assumes that all sounds (templates and signal) are
are then represented as points in a shape space computed           first coded in terms of low level features (Step 0). During
from similarities to a set of reference objects.                   the early acquisition phase, (Step 1), the model segments out
Such proposals are only starting to be applied to speech.          chunk of speech of a given size and stores them as templates
Liquid State Machines (Maas et al, 2002), or Echo State            in an instance-based memory system. After the templates
Systems (Jaeger, 2002) use recurrent networks or dynamic           become fixed, speech sounds are matched to the templates
systems to recode a time-varying low dimensional signal            (Step 2), and a similarity between each template and the
into a high dimensional one which incorporates information         signal is computed (Step 3). This model translates a time
spanning the recent past of the system. Such codes are more        varying trajectory in acoustic space into a point in similarity
robust to noise than low-level featural approaches                 space. As such, it has the potential to solve part of the
(Skowronski & Harris, 2007), however it is unclear how to          oversegmentation problem since it matches whole
optimize such representations. Coath & Denham (2005),              trajectories instead of just a slice of time. It also has the
proposed a model storing templates consisting of 100ms-            potential to address convergence towards the native sounds
200ms speech sounds, which are used as convolution filters.        since the stored templates belong to the native language.
They argue that the high dimensional code obtained is more         Note that the model is not committed to templates being
robust to variation due to time compression and speaker            exactly aligned to linguistically defined syllables; they could
variation than classical features. Dupoux (2004) has               as well correspond to diphones, triphones, or acoustic
proposed a similar approach based on the psycholinguistics         chunks of syllable size.
of human infants, whereby processing is based on the
                                                               220

- Step 0: Coding. The input was coded in terms of a frame          derivatives of the MFCC coefficients are a standard way to
every 5 ms consisting of 13 MFCC coefficients (Mermel-             improve on local featural codes to capture some of the
stein, 1976) computed on overlapping 15 ms windows.                dynamic properties of speech.
- Step 1. Templates Segmentation. The template base can               The algorithms were tested on two pseudo-languages,
vary according to three independent parameters: (a) number         which we constructed with carefully balanced phoneme and
of templates. To be effective, templates have to be numerous       syllable sets. Utterances of each pseudo-language were
enough to cover the range of possible sound combinations           recorded by a male talker in a quiet and non reverberating
in the language. However, too many templates may hamper            environment, digitized, and converted into MFCC coef-
learning. (b) template duration: a template has to be long         ficients. All stimuli were hand labeled for the purpose of
enough to contain significant dynamic properties, but not          performing the linear separation test. The stimuli were then
too long, otherwise the number of templates required for           distributed into three sets. The first set was used to generate
total language coverage explodes. (c) template boundaries:         the templates, the second one for training the perceptron and
Templates can either be temporally aligned to structural           a third one for generalization. Each test was performed 10
properties of speech (syllable boundaries, peak of vowel           times, with a different random assignment of sets, in order
nucleus, etc), or randomly segmented. Even though these 3          to derive standard deviations for the error rates.
variables may interact, in the present study, we manipulate           The Monosyllabic language contained 6 vowels /a e i u o
one while keeping the other two constant in artificial             y / and 6 consonants /R m s p t k/. These phonemes were
languages.                                                         combined to create 36 Consonant-Vowels (CV) syllables.
- Step 2. Template Matching. In the model, each template is        The syllables were pronounced in isolation (as if they were
matched to the signal in a parallel and independent way, as        monosyllabic words). Each syllable was recorded 54 times.
if each template were as an autonomous recognizer, looping         The template set contained between 4 and 12 exemplars of
through the signal in the attempt to recognizing itself. We        each syllables, the training set contained 34 exemplars and
use Dynamic Time Warping (DTW) (Myers and Rabiner,                 the generalization set contained 8 exemplars.
1981) to find the optimal alignment of the template and the           The Polysyllabic language contained trisyllabic CVCV-
signal, hence obtaining a warping function for each                CV words, composed of 8 phonemes /R ∫ d m e a i u/. These
template. Multiple passes through the templates are allowed        phonemes are arranged following the same CV structure
if the signal is long enough.                                      than in the previous sets. The set was built in such a way
- Step 3. Similarity. The warping function is used to extract      that all the phonemes consonants or vowels were produced
two different types of signal: spectral similarity, and            the same number of times, in every position. The template
temporal distortion. Spectral similarity is based on the           set contained 12 exemplars of each syllables (192
readout of the Euclidian distance between the MFCC                 templates), the training set contained 34 exemplars and the
coefficients of the signal and the aligned template for each       generalization set contained 8 exemplars.
frame (see Appendix). Temporal distortion is based on the
local slope of the warping function: any deviation from a                                     Results
slope of 1 in the warping function is giving a cost in the
temporal distortion between the template and signal. The           Assessing the templatic code
output representation for a bank of N templates is hence a
set of 2N time series, sampled every 5 ms.                         We used the monosyllabic language for these experiments.
                                                                   The linear classification performance of MFCC and
                        Methodology                                MFCC+Delta2 are used as baseline (Figure 2a). As seen in
                                                                   Figure 2b, template features using whole syllables as
   The aim of this paper is to compare the efficiency of           templates     yields     systematically     better    phoneme
templatic features compared to low-level ones for the              classification performance than the baselines. This shows
purpose of phoneme classification. We assessed this using a        that templatic features are both more informative than the
linear separation test: a perceptron was trained on a set of       MFCCs on which it is based, and outperform the MFCC
labeled examples using the RPROP algorithm (Riedmiller,            time derivatives. Increasing the number of templates from 4
and Braun, 1993), and the performance of the classification        per syllable types to 12 per syllable types improves slightly
was measured both on the training set and on a novel               the performance, but as the overall dimensionality grows
generalization set. Training and recognition of the phonetic       from 144 to 432 dimensions, one can start to see evidence of
categories was computed frame by frame using human                 over-fitting (i.e. a growing gap between training and
labels, and the error rate was the percentage of misclassified     generalization). Adding time distortion coding increases
frames over the training or generalization sets. This              more the performance than adding more templates,
performance was compared to the results obtained with two          suggesting that the temporal distortion adds a new and
baseline low-level featural codes. One is the raw MFCC (13         useful type of information. This is interesting, because
dimensions) used as input to the model. The second baseline        temporal alignment parameters are typically thrown away in
is MFCC + Delta2 code, which corresponds to MFCC                   classical speech recognition systems (more on this below).
coefficients plus their first and second order derivatives (39     In Figure 2c, we show that the gain in performance obtained
dimensions). This is a useful comparison since time                by template coding is not due to high dimensionality alone.
                                                               221

Figure 2. Percent error in a phoneme classification test using linear separation, for the training and the generalization sets, as
a function of type of input code, using the Monosyllabic Language. Bars show one standard deviation over and above the
mean. a. Baseline scores for the MFCC (12 dimensions) and MFCC+Delta2 (39 dimensions) codes. b. Scores for templatic
codes. We used as templates 4 exemplars of each of the 36 syllable types (left) or 12 exemplars (right). The ‘-time’ bars show
the scores with spectral similarity only and the ‘+time’ shows the score where time distortion has been added. c. Scores for
compressed templatic codes. We projected a code using 8*36 templates (spectral+temporal) onto the first 39 principal
components (left). We quantizated the spectral similarity of a code with 12*36 templates onto a binary code (0 or 1) (right).
   Indeed, projecting the code obtained with 8 templates per       sublanguage used the minimally distinct consonants /p t k/
syllable type (plus time distortion) onto the first 39 PCA         and vowels /o u y/. Each sublanguage had only 9 syllable
dimensions still yields better performance than                    types. We used as a template set the syllables from one
MFCC+Delta 2 despite the fact that the number of                   sublanguage, and tested either on new exemplars of the
dimensions is the same. Finally, we quantized each                 same language (appropriate templates) or exemplars from
dimension onto a binary code by using a threshold set at one       the other language (inappropriate templates). As shown in
standard deviation above the mean (the means and standard          Table 1, using the inappropriate language for the template
deviations are computed across the dimensions, for each            set yields a large drop in performance, and this both for the
time frame separately). This was done for a code using 12          easy and hard sublanguage. An ANOVA ran across 10
templates per syllable, and the result was undistinguishable       simulations on the log probability of error for the
from that obtained using non quantized version, suggesting         generalization set showed a significant effect of
that the high dimension templatic code is intrinsically a          sublanguage (F(1,36)=537, p<.0001), and appropriateness
(sparse) binary code.                                              (F(1,36)=410, p<.0001), but no interaction between these
                                                                   two factors (p>.05). Appropriate templates were better than
Language specificity                                               the MFCC+delta2 baseline (F(1,36)=266; p<.001), and
   How well does the template code capture language-               inappropriate templates were worse score than baseline
specific properties? If this code was only increasing              (F(1,36)=41, p<.0001). In brief, template features are
performance because of its high dimensionality, the                optimally tuned to the language from which they are
particular set of templates used should be irrelevant. Here,       extracted; they are very good for the segments that belong to
we split the monosyllabic language into two disjoint               that language, and poor for ‘foreign’ segments. This
sublanguages. The “easy” sublanguage used the maximally            mimicks the tuning process to native sounds which take
distinct consonants /R m s/ and vowels /a e i/. The “hard”         place during early language acquisition (Kuhl, 2000).
Table 1. Percent error in phoneme classification (and standard error across simulations) in two sublanguages, easy and hard,
as a function of the code used to represent the signals: the language-independent MFCC+Delta 2 code, and the templatic
codes based on the appropriate or inappropriate sublanguage.
    Code                                        Easy Language                           Hard Language
    Baseline                               Training         General.           Training                General.
    MFCC + Delta 2                        8.1% (0.3)       8.9% (0.7)        11.8% (0.3)             12.9% (0.7)
    Appropriate Templates                 4.8% (0.2)       5.2% (0.5)         8.2% (0.5)             10.2% (0.7)
    Inappropriate Templates               8.0% (0.2)       9.4% (1.0)        14.0% (0.7)             16.6% (1.0)
                                                               222

   Figure 4. Effect of the size of the template on phoneme classification using a linear separation test. The
language used was the Polysyllabic Language. The number of templates is fixed in all simulations.
                                                                      randomly segmented templates of a fixed duration. We
Temporal cues and vowel duration                                      found that randomly segmented templates can do almost as
   We found above that temporal distortion was useful even            well as syllables, as long as they have a duration of around
in an artificial language in which duration cues a priori             120ms. This duration corresponds to a unit whose size is
carries little linguistic information. The usefulness of              intermediate between syllables and phonemes. These two
temporal distortion should be even more apparent in a                 findings are compatible with the hypothesis that templates
language where such cues are used, like in Japanese, where            are optimal when they capture the transition parts between
vowel length is contrastive. We introduced a contrast in              phonemes. The 120ms is also compatible with the optimal
vowel duration in the “easy” sublanguage. It had 6 vowels             unit found by Coath and Denham (2005).
/a e i a: e: i:/, the latter three being obtained by doubling the
duration of the vowels in the original recording using pitch                                  Conclusion
synchronous resynthesis. The results are shown in Table 2;               We have found that coding the speech signal in a high
evidently, template codes, especially with time distortion            dimensional space of template similarity yields a significant
fare better than the MFCC controls.                                   improvement over standard MFCC features, even when
   Table 2. Percent error in phoneme classification (and              temporal derivatives are used. In addition, time distortion
standard error across simulations) in a language using                derived from the DWT alignment process adds useful
contrastive vowel duration. The score is given for the                information over and above spectral similarity. This is
training and generalization sets, and for a specific short            especially true when the language makes use of contrastive
versus long vowel contrast (generalization only).                     durational cues. We found that the improvement of the
   Code                            Training           Gener.          templatic code is limited to the particular language used to
   MFCC                          32.3% (0.3)       32.5% (0.9)        make up the template sets. Templates of one language are ill
   MFCC + Delta 2                26.0% (0.4)       26.1% (0.8)        suited to classify phonemes belonging to a different
                                                                      language. Finally, optimal templates seem to correspond to
   8*36 – time                   20.3% (0.3)       23.9% (0.8)        units around 100-200ms, containing at least the transitions
   8*36 + time                   11.9% (0.2)       15.1% (0.7)        between two adjacent phonemes.
                                                                         Of course, all of these conclusions are limited by the
Size and nature of the templates                                      experimental approach we used, which is to test our system
                                                                      on miniature languages, with restricted phoneme and
What is the optimal size of the templates? We used the                syllable inventories. It remains to be shown whether such
polysyllabic language and tested structurally defined tem-            coding and conclusions scale up to real-sized languages,
plates, syllables, phonemes and antiphonemes, segmented               more coarticulated inputs such as spontaneous speech, and
using human labels. Antiphonemes were defined as the final            psychologically realistic learning procedures, such as
50% part of one phoneme followed by the initial 50% part              incremental unsupervised clustering. Another point worth
of the next. As shown in figure 4, syllabic templates yielded         mentioning is that, because of the multiple DTWs, the
the best performance, but somewhat counter-intuitively,               complexity of the algorithm is in o(n.l²), where n is the
antiphonemes were better than phonemes. Second, we tested
                                                                  223

number of templates, and l is the utterance length. More             Dupoux, E., (2004). The Acquisition of Discrete Segmental
work remains to be done to optimize this algorithm.                    Categories: Data and Model In Proceedings of the 18th
Moreover, the usability of the code is limited by tractability         International Congress of Acoustics, Kyoto, April 4-9.
issues regarding clustering algorithms in high dimensions.           Edelman, S. (1998). Representation is Representation of
Potentially useful is the fact that templatic features can be          Similarities, Behavioral and Brain Sciences 21,449-498.
reduced to binary vectors at little or no cost.                      Gauthier, B., Shi, R., & Xu, Y., (2007). Learning phonetic
   Overall, this supports the interest of coarse graine features       categories by tracking movements, Cognition,103, 80-106
for modeling speech perception (Coath & Denham, 2005;                Guenther, F.H., & Gjaja, M.N. (1996). The perceptual
Skowronski & Harris, 2007), but more research is needed to             magnet effect as an emergent property of neural map
add biological constraints to such models and derive new               formation, JASA, 100(2), 1111–1121.
predictions for early language acquisition.                          Jaeger, H. (2002). Adaptive nonlinear system identification
                                                                       with echo state networks. In S.T.S. Becker, & K.
                    Acknowledgments                                    Obermayer (Eds.), Advances in neural information
We thank Sharon Peperkamp, Peter Dayan and Paul                        processing systems (pp. 593–600). Cambridge: MIT
Smolensky for very useful discussion and comments.                     Press.
                                                                     Kuhl, P.K. (2000). A new view of language acquisition.
                                                                       PNAS, 97, 11 850–11 857.
                          Appendix                                   Maass, W., Natschlager, T., & Markram, H. (2002). Real-
   A given stimulus S is aligned to a template T using DTW,            time computing without stable states: A new framework
and the two time axes are related through the warping                  for neural computation based on perturbations. Neural
function warp(t). We can read out DS,T(t), the Euclidian               Computation, 14, 2531–2560.
distance between the signal and warped template:                     Maye, J., Werker, J.F., & Gerken L. (2002). Infant
                                                                       sensitivity to distributional information can affect
where si(t)and ti(t) are the MFCC coefficients of S and T at           phonetic discrimination. Cognition, 82(3), B101-B111.
time t, respectively. We then define Sim(t), a time-                 Mehler, J. & Bertoncini, J. (1981). Syllables as Units in
dependant measure of template similarity:                              Infant Perception, Infant Behav. and Devel., 4, 271-284.
                                                                     Myers, C.S., & Rabiner, L. R. (1981). A comparative study
                                                                       of several dynamic time-warping algorithms for
where α is a constant used to avoid infinite values for a              connected word recognition. The Bell System Technical
distance of zero (α =10-3.). Finally, we define a time-                Journal, 607, 1389-1409.
dependant measure of temporal distortion:                            Peperkamp, S., Le Calvez, R., Nadal, J.P. & Dupoux, E.
                                                                       (2006). The acquisition of allophonic rules. Cognition,
where warp’(t) is the smoothed slope of the warping                    101, B31-B41.
function at time t, computed with a regression on 5 adjacent         Riedmiller, M. & Braun, H. (1993), A Direct Adaptive
frames, and truncated to fit the interval [10-3, 10+3].                Method for Faster Backpropagation Learning: The
                                                                       RPROP Algorithm, Proceedings of the IEEE International
                                                                       Conference on Neural Networks, San Francisco: IEEE.
                         References                                  Skowronski, M.D. & Harris, J.G. (2007). Automatic speech
Boruta, L., Peperkamp, S., Crabbé, B., & Dupoux, E.                    recognition using a predictive echo state network
   (2011). Testing the robustness of online word                       classifier. Neural Networks, 20, 414–423.
   segmentation: effects of linguistic diversity and phonetic        Smith E.C., Lewicki, M.S., (2006). Efficient auditory
   variation. Proceedings of CMCL, ACL, Portland, Oregon.              coding, Nature, 439, 978–982.
Chi, T., Ru, P., & Shamma, S.A. (2005). Multiresolution              Takami, J. & Sagayama, S. (1992). A successive state
   spectrotemporal analysis of complex sounds, JASA, 118,              splitting algorithm. In ICASSP, 66.6, 573–576.
   887-906.                                                          Ullman, S., Vidal-Naquet, M. & Sali, E. (2002). Visual
Coath M., & Denham, S.L., (2005). Robust sounds                        features of intermediate complexity and their use in
   classifications through the representations of similarity           classification. Nature Neuroscience, 5(7), 682-687.
   using response fields derived from stimuli during early           Vallabha, G. K., McClelland, J. L., Pons, F., Werker, J. &
   experience, Biological Cybernetics, 93(1), 22-30.                   Amano, S. (2007) Unsupervised learning of vowel
Cortes, C. & Vapnik, V. (1995). Support-Vector Networks,               categories from infant-directed speech, PNAS, 104:33,
   Machine Learning, 20.                                               13273–13278.
de Boer, B. & Kuhl, P.K. (2003) Investigating the role of            Varadarajan, B., Khudanpur, S., & Dupoux, E. (2008).
   infant-directed speech with a computer model. ARLO 4,               Unsupervised Learning of Acoustic Subword Units. In
   129–134.                                                            Proceedings of ACL-08: HLT, 165-168.
Dupoux, E., Kakehi, K., Hirose, Y., Pallier, C., & Mehler, J.        Werker, J.F., & Tees, R.C., (1984). Cross-language speech
   (1999). Epenthetic vowels in Japanese: A perceptual                 perception: Evidence for perceptual reorganization during
   illusion? Journal of Experimental Psychology: Human                 the first year of life, Infant Behav. and Devel., 7, 49–63.
   Perception and Performance, 25(6), 1568--1578.
                                                                 224

