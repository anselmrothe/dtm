UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The mirage of morphological complexity
Permalink
https://escholarship.org/uc/item/7nb4r0d7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Author
Moscoso del Prado, Fermin
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                             The Mirage of Morphological Complexity
                          Fermı́n Moscoso del Prado Martı́n (fermosc@gmail.com)
                   Laboratoire Dynamique du Langage, CNRS & Université de Lyon, Lyon, France &
                            Institut Rhône-Alpin des Systèmes Complexes (IXXI), Lyon, France
                           Abstract                              1. Inflectional morphology expresses semantic content
   I present a study on the morphological complexity of              that is required for successful communication.
   six European languages. A theory-free measure of the
   complexity of a language’s inflectional morphology, is        2. The presence of inflectional categories and markers re-
   derived from Gell-Mann’s concept of Effective Complex-
   ity. Using a parallel corpus, I show that disconsidering          duces the overall complexity of a language.
   word order information results in the classical gradation
   of inflectional complexity: Languages in which words           What is morphological complexity?
   have more inflected variants seem to be more complex
   than languages with fewer variants. It also appears that       Notice that the discussion above assumes that some ob-
   the presence of the inflectional system increases the com-     jective quantification of the complexity of a morpho-
   plexity of languages. However, when word order infor-
   mation is explicitly considered, the apparent gradation        logical system is available. However, no definition or
   in complexity across languages vanishes. Furthermore,          quantification of the complexity of a linguistic system
   it becomes clear that the presence of inflections reduces      is widely accepted. The concept of complexity has cen-
   the overall complexity of languages. In sum, inflection
   is used whenever its presence simplifies a language’s de-      tral importance for theories of language structure and
   scription. Inflectional morphology is not a capricious         processing. For instance, McWhorter (2001) raised an
   feature, as some argue, but rather an effective tool for       interesting controversy over his claim that pidgin and
   complexity reduction.
                                                                  creole languages are intrinsically less complex than other
   Keywords: Effective Complexity, Entropy, Morphol-
   ogy                                                            languages, reflecting the very early stage in their devel-
                                                                  opment. However –despite its frequent invocation– lin-
                       Introduction                               guistic complexity remains a rather ill-defined concept.
Words are usually accepted as being the fundamental               Indeed, much previous research has been aimed at pro-
units of syntax. However, in many (or most) languages,            viding a concise and workable definition of linguistic –
words can be related to each other by (quasi-)regular re-         and morphological– complexity (e.g., Goldsmith, 2001;
lations between their orthographical forms, their gram-           Greenberg, 1954; Juola, 1998, 2007; Malvern, Richards,
matical functions, and their meanings. These intra-               Chipere, & Durán, 2004; McWhorter, 2001; Nichols,
lexical regularities are referred to as morphological struc-      2010; Shosted, 2006; Siegel, 2004; Vulanović, 2007; Xan-
ture, and constitute one of the main areas in the study           thos & Gillis, 2010). Many of these studies define com-
of human languages, both from a linguistic and a psy-             plexity in terms of either counting forms, or detecting
chological perspective.                                           the presence or absence of certain morphological phe-
                                                                  nomena or structures that are chosen as representative
Morphology: ‘By itself ’ ?                                        of complexity based on some theoretical premises, an ap-
The classical structuralist position on inflectional sys-         proach which was also used by Lupyan and Dale (2010).
tems is that they are largely random classifications that         However, such theory-bound measures can result in con-
need not serve any useful function within the overall lin-        siderable vagueness and subjectivity with regard to what
guistic system. This is still a common view both among            is more or less complex (see, e.g., McWhorter, 2001;
linguists (e.g., Aronoff, 1994; Bloomfield, 1933) and psy-        Nichols, 2010; Shosted, 2006, for debate on this issue).
chologists (e.g., Maratsos, 1979). For instance, Aronoff          To obtain more objective estimates, some have measured
(1994) argues that morphology must be considered ‘by              the complexity of inflectional paradigms as the num-
itself’, not merely from the perspective of its interface         ber of inflected variants that can be formed for a sin-
with syntax, phonology, or semantics, and that linguis-           gle word (e.g. Malvern et al., 2004; Xanthos & Gillis,
tic theory should therefore consider a separate and au-           2010). This simple approach presents the problem that
tonomous morphological component. In contrast, sup-               it ignores that there are sometimes very regular relations
porters of ‘functionalist’ theories of grammar would ar-          between different inflected forms (e.g., the plural of an
gue that the presence of inflectional systems must be             English word is in most cases the same as the singular
useful from a communicative perspective. Otherwise, in            plus an ‘s’), and not much information is required to
line with Zipf (1949)’s ‘Principle of Least Effort’, the          encode this (what some term the Paradigm Cell Filling
cognitive system would have discarded them for requir-            Problem; cf., Ackerman, Blevins, & Malouf, 2009). To
ing unnecessary efforts. From this perspective, the pres-         avoid this, some have taken a theory-free approach in-
ence of inflectional morphology is only justified if one or       vestigating to what extent linguistic utterances or struc-
both of the following conditions apply:                           tures can be compressed (Goldsmith, 2001; Juola, 1998,
                                                              3524

2007). The irreducible information that is left after com-         be the size of the shortest grammar that could fully de-
pression provides a useful index of the informational con-         scribe it (Goldsmith, 2001; McWhorter, 2001). As a
tent of the signal; its Algorithmic Information Content            starting point, let us assume that the regularities that
(AIC; Chaitin, 1987; Kolmogorov, 1965), and can thus               need to be accounted for correspond to all the sentences
be taken as a theory-free measure of complexity.1 Notice           that appear in an arbitrarily large reference corpus of
that, by themselves, AIC approaches based on the com-              a language. This is to say, the definition of grammati-
pressibility of a corpus (such as Juola, 1998, 2007) do not        cal complexity rests on a reference corpus containing N
in fact constitute measures of complexity; it is easy to see       characters (or phonemes, etc.), and corresponds to the
that AIC would rate an incompressible totally random               length of the shortest possible grammar that can gener-
corpus as being more complex than the complete works               ate all the sentences in that corpus (i.e., it is complete)
of Shakespeare, as the latter can be compressed to some            and only those (i.e., it does not over-generate). Notice
degree. In contrast, the approach of Goldsmith (2001)              this definition leaves open the grammatical theory or for-
overcomes this limitation. It does not measure the com-            malism in which the grammar is expressed: Any formal
pressibility of the corpus, but rather, the compressibil-          mechanism that is able to generate the sentences in a
ity of its formal regularities (a random text would have           language is valid candidate. Let us denote the length of
no regularities to compress). However, by summariz-                that optimal grammar as G(N ). In parallel to the defini-
ing paradigms as sets of forms and affixes, Goldsmith              tion above, one can also consider the AIC of the reference
(2001)’s approach overlooks the crucial role that can be           corpus, that is, the length of the shortest possible algo-
played by the actual functions served by each inflected            rithmic description enabling its full reconstruction and
variant, as these relate to the complexity of the system,          denote the length of that optimal compression by H(N ).
and have been shown to be of crucial importance for the               On the one hand, the grammar that determines G(N )
cognitive system (Kostić, Marković, & Baucal, 2003).             needs to be able to generate all sentences in the corpus.
                                                                   On the other hand, the compressed version of the cor-
Outline                                                            pus also needs to generate all those very sentences, with
In what follows, I start by using the definition of the            only the additional burden of having to reconstruct their
Effective Complexity of language (Moscoso del Prado                actual ordering and frequencies of occurrence. As both
Martı́n, submitted) to derive a measure of inflectional            H(N ) and G(N ) are defined in terms of ideal ‘optimal’
complexity. This is followed by a corpus-based analysis            methods that do not waste any space, one can decom-
of the inflectional complexity of six European languages,          pose
investigating the variation in morphological complexity                              H(N ) = G(N ) + Hs (N ),              (1)
according to different measures. I conclude by discussing
the theoretical implications of the results.                       where Hs (N ) ≥ 0 denotes the additional information
                                                                   that needs to be coded in the AIC. It is useful to think
 Formulation & Computational Methods                               of G(N ) and H(N ) in terms of per-character rates; their
                                                                   relation to the size of the reference corpus:
Effective Complexity of Language
As in Moscoso del Prado Martı́n (submitted), the defini-                         1
                                                                       g(N ) =     G(N ) =
tion of linguistic complexity is derived from Gell-Mann                          N
(1995)’s general definition of Effective Complexity:                             1
                                                                              =     [H(N ) − Hs (N )] = h(N ) − hs (N ).   (2)
                                                                                 N
   A measure that corresponds [. . . ] to [. . . ] complex-
   ity [. . . ] refers not to the length of the most concise       If a finite grammar for a language does exist, then, for
   description of an entity (which is roughly what AIC             increasingly large corpora, the grammar should come
   is), but to the length of a concise description of a            closer to being ‘complete’. That is to say, from some
   set of the entity’s regularities. Thus something al-            large N onwards, G(N ) should grow much more slowly
   most entirely random, with practically no regulari-             than N itself. One can now take the idealization a step
   ties, would have effective complexity near zero. So             further, and require that the ideal grammar be able to
   would something completely regular, such as a bit               generate all sentences (and only those) that could even-
   string consisting entirely of zeroes.                           tually happen in the language (i.e., they have non-zero
                                                                   probabilities of occurrence). This is equivalent to tak-
For human languages, such descriptions of the system’s             ing the limit of an infinite corpus size. In this way, one
regularities are grammars. Indeed, many have advocated             defines the grammatical complexity of the language as
that the best measure of a language’s complexity would
                                                                                        G = lim G(N ).                     (3)
    1                                                                                        N →∞
      The approach advocated by Goldsmith (2001) is not com-
pletely theory-free, as it requires the parsing of words into
stems and affixes, which is in itself a strong theoretical com-    At this extreme, the finite grammar should be complete,
mitment.                                                           hence its size would become negligible compared to that
                                                               3525

of the corpus. Defining the grammatical density of the             where Ls denotes the mean length of a sentence in
language as the infinite corpus size limit of g(N ),               whichever units (characters, phonemes, . . . ) g was com-
                                                                   puted. This new measure provides a finite index of
                                          G(N )                    the complexity of a language, precisely measuring how
               g = lim g(N ) = lim               .          (4)
                     N →∞          N →∞     N                      much grammatical knowledge is provided by each new
If G is finite, one should find that                               observed sentence.
                                G(N )                              Inflectional complexity
                     g = lim           = 0.                 (5)
                          N →∞    N                                Assuming that one can somehow separate the different
That is to say, for a language to have a finite grammar,           contributions to the per-sentence grammatical density
its grammatical density should be zero so that the limits          that are provided by the different components of the
in Eqs. 3–5 converge. One can also extend this limiting            grammar (below the sentence level) one could decom-
condition to the compressibility measures, and write               pose gs into something like
  g = lim g(N ) = lim [h(N ) − hs (N )] = h − hs = 0.                gs = gslexicon + g inflection + gsderivation + gssyntax + . . . . (8)
        N →∞           N →∞
                                                            (6)    Consider now that one could erase from the corpus all
On the one hand, h reflects the rate of compressibility of         inflectional information without disturbing any of the
the original corpus, taken to the limit of infinite corpus         other levels. In this case, one would obtain a new ver-
size. On the other hand, hs is the rate of compressibil-           sion of the grammatical densities that would discount all
ity of a manipulated version of the corpus, where the              inflectional information,
sentence identities, frequencies, and orderings are main-
tained, but their actual internal structure is lost. For                    gs0 = gs0lexicon + gs0derivation + gs0syntax + . . . ,     (9)
stationary2 ergodic sources, it is guaranteed that h and
hs are actually the source entropies (Shannon, 1948) of            such that one can write,
the original and modified versions of the corpus (Chaitin,
                                                                                           gsinflection = gs − gs0 .                  (10)
1987; Kolmogorov, 1965).
   Moscoso del Prado Martı́n (submitted) found that, for           I refer to gsinflection as the inflectional complexity of a lan-
human languages g > 0, and therefore no finite gram-               guage, and it measures the average amount of informa-
mar can ever be found that fully accounts for all sen-             tion required to describe the new inflectional structures
tences in the language without generating impossible               contained by a newly observed sentence. This measure
sentences. In other words, the grammatical complex-                is readily computable, and it enables direct comparison
ity G of languages is not finite, but rather keeps growing         of the inflectional systems of different languages. Notice
for a growing reference corpus. In contrast, the gram-             also that its actual values are themselves meaningful. A
matical density measure g provides a finite value that             positive gsinflection indicates that the presence of the mor-
can be compared across languages, corresponding to the             phological system increases the size of the grammar that
average information that is provided by each new char-             is required to describe the language. On the other hand,
acter or phoneme in the large corpus size limit. This              were gsinflection to be negative, it would indicate that the
is to say, even if the actual grammatical complexity is            presence of the inflectional structures in fact simplifies
not finite, one can compare the speed at which the com-            the grammar of the language; the grammar would be
plexity increases with increasing corpus size. Different           more complex if the inflectional structures were absent.
languages can be encoded with more or less transparent
orthographies, or may make use of longer, but very pre-            Computations
dictable expressions. Therefore, it is important to con-           The discussion above assumes that the entropy measures
sider the grammatical density, not in terms of characters          h and hS can be accurately estimated from corpora.
or phonemes, but using instead the basic unit that is be-          Here, I provide a summary of the methods used for esti-
ing considered for the grammar, that is, the sentence.             mating these magnitudes, see Moscoso del Prado Martı́n
We can therefore define the per-sentence grammatical               (submitted) for a more detailed discussion of these meth-
density of the language as:                                        ods and their accuracy.
                          gs = Ls · g,                      (7)    Estimation of h: Asymptotic Lempel-Ziv com-
    2                                                              plexity The source entropy h indexes the compressibil-
      Stationarity is not a property of linguistic corpora, but
it can be enforced by simply randomizing the order of the          ity of the corpus. Although knowing what is the shortest
sentences in the corpus (Moscoso del Prado Martı́n, submit-        possible (the most compressed) version of a sequence is
ted). This discards all supra-sentential information, but we       impossible unless one explicitly knows the dynamics of
are assuming that the grammar must produce all well-formed
sentences, and information beyond the sentence is thus not         the process that generated it, some specific lossless com-
relevant here.                                                     pression algorithms are guaranteed to converge to this
                                                               3526

maximum in the long length limit. Of these, a partic-           of the sentence frequencies, and the obtained estimator
ularly simple choice is to use the Lempel-Ziv algorithm         is further adjusted to account for the yet unseen sen-
(LZ76; Lempel & Ziv, 1976). For very long sequences,            tences. Although the Chao-Shen estimate converges rel-
the Lempel-Ziv complexity almost surely converges to            atively rapidly to the true entropy value, in this case
the entropy of the source (Ziv & Lempel, 1978). Hence,          it still tends to underestimate the true value of the en-
the LZ76 algorithm can be used for estimating the source        tropy. However, Moscoso del Prado Martı́n (submitted)
entropies h. However, one should take into account that,        found that an asymptotic correction of the Schürmann
although the LZ complexity asymptotically approaches            and Grassberger (1996) type is very accurate in this case
h, estimates of h based directly on some large corpus           as well.
are still subject to finite sample effects. To a large de-
gree however, the finite sample error in estimation can                          Materials and Methods
be corrected for by modelling the convergence of the
Lempel-Ziv measure. In this respect, Schürmann and             I used the Dutch, English, French, German, Italian, and
Grassberger (1996) found that the convergence of the            Spanish sections of the Europarl Corpus (Koehn, 2005),
Lempel-Ziv measure (L[n]) as a function of the corpus           version 5. This corpus contains transcripts of sessions of
size n can be very accurately modelled by a three para-         the European Parliament between 1998 and 2009, tran-
mater power-law expression, and one of these parameters         scribed (some partly) in eleven of the official languages
corresponds to the asymptotic value of the entropy h.           of the E.U. For each language, the sessions of the Parlia-
These three parameters can be fitted from data. Given a         ment were collapsed into eleven files, each correspond-
finite size corpus, one can compute the Lempel-Ziv com-         ing to one individual year, and all computations were
plexity using different subsets of the corpus of increasing     repeated for each year, as to provide multiple estimates
length, and fit a simple non-linear regression provides         for each language, all equivalent across the six languages.
the estimate of h. This approximation has been found            Within each of the eleven files in each language, the order
to perform extremely well (Moscoso del Prado Martı́n,           of the sentences was randomized to ensure stationarity.
submitted; Schürmann & Grassberger, 1996).                        In order to provide versions of the corpora that pre-
                                                                served all linguistic information except for that corre-
Estimation of hs : Chao-Shen estimator The other                sponding to inflectional structures, I also created new
entropy rate factor, hS corresponds to the per-character        versions of each corpora that had been tokenized, that
entropy rate of a sequence which preserves the identity         is, all inflected forms were replaced by their base forms
and ordering of the sentences, but lacks their internal         (i.e., “cars” was replaced by “car”, “ate” was replaced
structure. A simple way to estimate this is to consider         by “eat”, etc.). Notice that this manipulation preserves
the corpus as a sequence of Ns symbols S1 S2 . . . SNs ,        all orthographic, lexical, syntactic, and semantic struc-
each of which corresponds to a full sentence. Note here         ture, but lacks only the inflectional details. The to-
that the alphabet on which this sequence is defined is          kenizing was performed using the TreeTagger software
unknown and possibly not even finite. Having such a             (Schmid, 1994) with the available scripts for each of the
large, perhaps infinite alphabet, makes the use of the          languages.3
Lempel-Ziv method inappropriate. For the LZ76 algo-
rithm to begin to converge towards the source entropy,             Finally, to investigate the effect that functional infor-
one needs to have observed at least a great proportion          mation about syntax and semantics has on inflectional
of the symbols in the original alphabet. However, even          complexity, additional versions of all corpora (original
in extremely large corpora, a great majority of the sen-        and tokenized) were created by randomizing the order
tences will occur exactly once, and a great many more           of the words (including sentence breaks) in each corpus.
possible sentences will not be in the corpus at all. On the     This produces versions of the corpora which retain most
other hand, as the sequence S1 S2 . . . SNS originates from     lexical and ortho/phonological information as well as the
a corpus in which the order of the sentences has been           same diversity and probability distribution of inflectional
randomized, it is clear that the sequence is an ensem-          forms that was present in the originals. The measures of
ble of NS independent and identically distributed ran-          inflectional complexity obtained from these randomized
dom variables. To solve such type of problems, Chao             corpora therefore consider the information about diver-
and Shen (2003) developed an estimator that takes into          sity of inflectional forms, the regularity of their forma-
account the contribution of unseen items to the over-           tion, and the homogeneity of their probability distribu-
all entropy. The Chao-Shen estimator performs remark-           tions. They do not consider any information regarding
ably well in sequences originating from infinite alpha-         the functions that each inflectional form serves in the
bets (cf., Moscoso del Prado Martı́n, submitted). The           text, or the particular cell in the inflectional paradigms
estimator relies on a Good-Turing (Good, 1953) adjust-          they occupy.
ment of the frequencies of occurrence of the sentences.
                                                                    3
The entropy is then computed on the adjusted values                   http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
                                                            3527

              Results and Discussion                             ence of inflectional morphology does not seem to be in
                                                                 any way capricious, but instead it reflects a delicately
Figure 1 summarizes the resulting inflectional complex-
                                                                 optimized system.
ity values that were obtained for the different corpora.
The upper panel plots the distribution of complexity val-                          Acknowledgments
ues for the corpora in which the word order was left in-
tact. Notice that, in this case, there was no significant        I am indebted to A. Boussidan for proof-reading this
heterogeneity in the distributions of complexity across          manuscript.
the six languages compared. All complexity values seem
to arise from a single distribution. More importantly,                                 References
the inflectional complexity values tended to be negative         Ackerman, F., Blevins, J. P., & Malouf, R. (2009).
for all languages but Dutch, for which it was not signif-          Parts and wholes: Implicative patterns in inflectional
icantly different than zero. In other words, considering           paradigms. In J. P. Blevins & J. Blevins (Eds.), Anal-
word order information, the inflectional morphology sys-           ogy in grammar: Form and acquisition (pp. 54–81).
tems in fact simplify the grammar of the languages.                Oxford, UK: Oxford University Press.
   In contrast, the lower panel plots the estimated distri-      Aronoff, M. (1994). Morphology by itself: Stems and
butions of inflectional complexity when the word order             inflectional classes. Cambridge, MA: MIT Press.
information has been disregarded. In this case, there            Bloomfield, L. (1933). Language. New York, NY: Holt.
is indeed significant heterogeneity in the complexity of         Chaitin, J. G. (1987). Algorithmic information theory.
the different languages, and this variation fits well with         Cambridge, England: Cambridge University Press.
the intuitions that one would obtain from form-counting          Chao, A., & Shen, T.-J. (2003). Nonparametric esti-
measures of inflectional complexity: English ≤ Dutch <             mation of Shannon’s index of diversity when there are
German ≤ Romance Languages. Whereas English and                    unseen species. Environmental and Ecological Statis-
Dutch have very limited inflectional systems, German               tics, 10 , 429–443.
makes use of a relatively complex nominal declension             Gell-Mann, M. (1995). What is complexity? Complex-
system, and Romance languages have rather complex                  ity, 1 , 16-19.
verbal conjugation mechanisms including tens of forms.           Goldsmith, J. A. (2001). Unsupervised learning of the
Furthermore, if word order is disconsidered, the com-              morphology of a natural language. Computational Lin-
plexity values are in all cases significantly positive, this       guistics, 27 , 153–198.
is to say, the inclusion of the inflectional system makes        Good, I. J. (1953). The population frequencies of
the description of the languages more complex. Notice,             species and the estimation of population parameters.
however, that this differences in morphological complex-           Biometrika, 40 , 237–264.
ity are but a mirage produced by not considering the             Greenberg, J. (1954). A quantitative approach to mor-
actual functions served by each form.                              phological typology of language. In R. Spencer (Ed.),
   I have introduced an objective and theoretically moti-          Method and perspective in anthropology (pp. 192–195).
vated measure of inflectional complexity. This new mea-            Minneapolis, MN: University of Minnesota Press.
sure overcomes all the problems that were detailed in            Juola, P. (1998). Measuring linguistic complexity: The
the introduction: it considers the diversity of forms, the         morphological tier. Journal of Quantitive Linguistics,
regularity of their relations, the position that each form         5 , 206–213.
occupies in the paradigm, the frequency distribution of          Juola, P. (2007). Assessing linguistic complexity. In
forms, and the function that each forms serves. Fur-               M. Miestamo, K. Sinnemäki, & F. Karlsson (Eds.),
thermore, the measure relies on an adequate concept of             Language complexity: Typology, contact, change (pp.
complexity, in contrast with the plain randomness that             89–108). Amsterdam, The Netherlands: John Ben-
is reflected by raw measures of corpus compressibility.            jamins.
Finally, by extrapolating to infinite corpus size, one at-       Koehn, P. (2005). Europarl: A parallel corpus for statis-
tenuates the innacuracies that are introduced by making            tical machine translation. In Proceedings of MT sum-
inferences on finite-sized corpora.                                mit X (pp. 79–86). Phuket, Thailand: Asia-Pacific
   These results highlight the strong degree of mutual             Association for Machine Translation. Available from
dependence between morphological and syntactic infor-              http://www.statmt.org/europarl/
mation. In line with the behavioral results of Kostić           Kolmogorov, A. N. (1965). Three approaches to the
et al. (2003), the structure of the morphological system           quantitative definition of information. Problems in In-
cannot be considered independently of the actual func-             formation Transmission, 1 , 4–7.
tions that the different forms serve. In fact, considering       Kostić, A., Marković, T., & Baucal, A. (2003). In-
the presence of these functions, inflectional morphology           flectional morphology and word meaning: orthogo-
serves a role in reduction of uncertainty, simplifying the         nal or co-implicative domains? In R. H. Baayen &
description of the whole grammar. Therefore, the pres-             R. Schreuder (Eds.), Morphological structure in lan-
                                                             3528

                    10
                     0
                    -10                                                                                  n
                    -20
                                                                                                             lang
                    -30                                                                                             en
                                                                                                                    de
                x                                                                                                   es
                                                                                                                    it
                    20
                                                                                                                    nl
                                                                                                                    fr
                    15
                                                                                                         r
                    10
                                 en               de     es            it          nl           fr
                                                              lang
Figure 1: Summary of results. The upper panel plots the distribution of inflectional complexity (in nats/sentence)
values obtained for each language in the original word order corpora. The lower panel plots the same results for the
corpora in which the word order was randomized.
  guage processing (pp. 1–44). Berlin, Germany: Mou-                   using decision trees. In Proceedings of the Interna-
  ton de Gruyter.                                                      tional Conference on New Methods in Language Pro-
Lempel, A., & Ziv, J. (1976). On the complexity of finite              cessing. Manchester, UK.
  sequences. IEEE Transactions in Information Theory,                Schürmann, T., & Grassberger, P. (1996). Entropy es-
  22 , 75–81.                                                          timation of symbol sequences. Chaos, 6 , 414–427.
Lupyan, G., & Dale, R. (2010). Language structure is                 Shannon, C. E. (1948). A mathematical theory of com-
  partly determined by social structure. PLoS ONE , 5 ,                munication. Bell System Technical Journal , 27 , 379-
  e8559.                                                               423, 623-656.
Malvern, D., Richards, B. J., Chipere, N., & Durán, P.              Shosted, R. (2006). Correlating complexity: A typolog-
  (2004). Lexical diversity and language development:                  ical approach. Linguistic Typology, 10 , 1–40.
  Quantification and assessment. Palgrave Macmillan.                 Siegel, J. (2004). Morphological simplicity in pidgins
Maratsos, M. (1979). Learning how and when to use                      and creoles. Journal of Pidgin and Creole Languages,
  pronouns and determiners. In P. Fletcher & M. Gar-                   19 , 139–162.
  man (Eds.), Language acquisition (pp. 225–240). Cam-               Vulanović, R. (2007). On measuring language complex-
  bridge, UK: Cambridge University Press.                              ity as relative to the conveyed linguistic information.
McWhorter, J. H. (2001). The world’s simplest gram-                    SKY Journal of Linguistics, 20 , 399–427.
  mars are creole grammars. Linguistic Typology, 5 ,                 Xanthos, A., & Gillis, S. (2010). Quantifying the devel-
  125–166. (With replies and rejoinder)                                opment of inflectional diversity. First Language, 30 ,
Moscoso del Prado Martı́n, F. (submitted). The                         175 –198.
  Effective Complexity of language: English requires                 Zipf, G. K. (1949). Human behavior and the princi-
  at least an infinite grammar.         Available from                 ple of least effort: An introduction to human ecology.
  http://moscosodelprado.net/docs/complexity-cognt.pdf
                                                                       Cambridge, MA: Addison Wesley.
Nichols, J. (2010). Linguistic complexity: a comprehen-              Ziv, J., & Lempel, A. (1978). Compression of individual
  sive definition and survey. In G. B. Sampson, D. Gil,                sequences via variable length coding. IEEE Transac-
  & P. Trudgill (Eds.), Language complexity as an evolv-               tions in Information Theory, 24 , 530–536.
  ing variable (pp. 109–124). Oxford, England: Oxford
  University Press.
Schmid, H. (1994). Probabilistic part-of-speech tagging
                                                                3529

