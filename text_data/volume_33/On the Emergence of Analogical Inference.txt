UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
On the Emergence of Analogical Inference
Permalink
https://escholarship.org/uc/item/3rw3r9zp
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Thibodeau, Paul
Flusberg, Stephen
Glick, Jeremy
et al.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                    On the Emergence of Analogical Inference
                                         Paul H. Thibodeau (pthibod1@stanford.edu)
                                           Stephen J. Flusberg (sflus@stanford.edu)
                                            Jeremy J. Glick (jjglick@stanford.edu)
                                       Daniel A. Sternberg (sternberg@stanford.edu)
                                     Department of Psychology, 450 Serra Mall, Bldg. 420
                                                      Stanford, CA 94305 USA
                                                                       Markman & Gentner, 1997; Hummel & Holyoak, 1997), it
                              Abstract                                 is unclear how this analogy-specific machinery comes to
      What processes and mechanisms underlie analogical                exist in the brain over the course of development. Even
   reasoning? In recent years, several computational models of         developmentally-oriented models such as DORA (Doumas,
   analogy have been implemented to explore this question.             Hummel, & Sandhofer, 2008), which attempts to learn the
   One feature of many of these models is the assumption that          structure used by LISA, assume a great deal of analogy-
   humans possess dedicated analogy-specific cognitive                 specific cognitive machinery without specifying how this
   machinery – for instance, a mapping or binding engine. In           machinery comes to exist in the first place.
   this paper, we question whether it is necessary to assume the          Here, we address this issue by proposing that some forms
   existence of such machinery. We find that at least for some         of analogical processing may emerge gradually over the
   types of analogy, it is not. Instead, some forms of analogical
   processing emerge naturally and spontaneously from
                                                                       course of development through the operation of low-level
   relatively simple, low-level learning mechanisms. We argue          domain general learning mechanisms (Flusberg, Thibodeau,
   that this perspective is consistent with empirical findings         Sternberg, & Glick, 2010; Leech, Mareschal, & Cooper,
   from the developmental literature and with recent advances          2008). In support of this view we describe a set of
   in cognitive neuroscience.                                          simulations carried out using the Rumelhart network
      Keywords: Analogy; metaphor; relational reasoning;
                                                                       (Rumelhart, 1990), a neurally inspired model that has
   development; connectionism; computational model.                    succeeded in capturing many results from the literature on
                                                                       semantic development in children (e.g. Rogers &
                          Introduction                                 McClelland, 2004) and whose variants have been used to
                                                                       understand the deterioration of conceptual knowledge in
In the past three decades, there has been a growing                    semantic dementia (e.g. Dilkina, McClelland, & Plaut,
appreciation for the possibility that analogy lies at the core         2008).
of human cognition (Gentner, 1983; Hofstadter, 2001;
Holyoak, Gentner, & Kokinov, 2001; Penn, Holyoak, &                                           Simulations
Povinelli, 2008). On this view, it is our ability to
understand, produce, and reason with analogies that allows             Our learning task is inspired by Hinton’s (1986) family tree
us to create the wonderfully rich and sophisticated                    model, one of the first attempts to address relational
intellectual and cultural worlds we inhabit.                           learning in a connectionist network. The task of the model
  In an attempt to illuminate the cognitive mechanisms that            is to learn “statements” that are true about the various
underlie      analogical       processing,     several     detailed    members of a family, including identity information,
computational models have been developed that capture                  perceptual features, and relations between family members.
key components of the analogical reasoning process (see                   Input to the model consists of activating a Subject unit,
French, 2002 for a review). Among the most influential of              corresponding to a particular family member, and a
these models are the Structure Mapping Engine (SME:                    Relation unit. The Relation units correspond to the
Falkenhainer, Forbus, & Gentner, 1989), and Learning and               different kinds of relationships that can hold between
Inference with Schemas and Analogies (LISA: Hummel &                   subjects and objects (e.g. “is_named”, “parent_of”). The
Holyoak, 1997). These models vary drastically in many                  network is wired up in a strictly feed-forward fashion, as
ways; however, they share a fundamental commitment to                  shown in Figure 1, such that the input propagates forward
explicitly structured symbolic or hybrid representations               through the internal layers, resulting in a set of predictions
(e.g. of objects and relations), together with the existence of        over the Object layer.
a dedicated analogical mapping or binding mechanism that                  Over the course of training, the network’s weights
operates over these representations. Indeed, proponents of             change (via backpropagation of the cross-entropy error on
these approaches argue that analogical inference is beyond             the output units) in order to better predict which Object
the reach of models that lack these properties, including              outputs correspond to each combination of Subject and
fully distributed connectionist models (e.g. Gentner &                 Relation inputs. As the model also contains intervening
Markman, 1993; Holyoak & Hummel, 2000).                                layers of units between the input and output layers, it is
  While the structured approach has successfully captured              forced to re-represent the inputs as a distributed pattern of
adult behavior in numerous analogical reasoning tasks (e.g.            activation over these internal layers.
                                                                   2904

   The underlying model parameters were identical in all of
the simulations that we present. In all cases, the learning
rate was .005 and the network was trained for 10,000
epochs. Results were averaged over 10 runs of each
network in order to provide statistical tests. The hidden
layers were identical in each case: 6 Subject Representation
units and 16 Integration units. In all presented simulations,
error on the training patterns was very low by the end of
training (average cross-entropy error < .35).
                                                                    Figure 2: An illustration of the Stripe and Solid families,
                                                                         which served as the source and target domain.
                                                                    We can contrast two major predictions. Naively, one
                                                                 might think that the network runs on raw association. As
                                                                 the Solids’ dog is most similar to the Stripes’ dog, the
                                                                 network would therefore conclude that the Stripes’
                                                                 daughter walks the Solids’ dog! Alternatively, we might
                                                                 expect that the network will encode the relational structure
                                                                 between the two families, and so will correctly conclude
                                                                 that the person in the appropriate position within the Solid
               Figure 1: The network architecture.
                                                                 family -- namely, the daughter -- will be the one who walks
                                                                 their dog. In fact, the latter is the case: the network decides
The Basic Model
                                                                 that within the Solid family, the daughter walks the dog. A
In the first simulation, the network learns about the Stripes    paired t-test contrasting the activation levels of the Stripes’
and the Solids – two families with isomorphic relational         daughter with the Solids’ daughter was highly significant,
structure (detailed in Table 1 and pictured in Figure 2) –       t[9] = 7.75, p < .001 (see Figure 3).
with a single fact omitted about the Solid family. While the        To ensure that the network used the relational similarity
network knows that the daughter of the Solids owns their         between the two families in making this inference, we ran a
dog, it receives no information about who walks their dog.       second simulation, in which the model was trained only on
This network does a good job of learning the facts on which      the Solid family, with no information about the Stripe
it is trained, but the question of interest is whether it can    family. In this network, the model does not conclude that
extend its knowledge to answer a question on which it            the daughter walks the dog. Instead, the network decides
received no training: who walks the Solids’ dog?                 that the dog walks itself! A paired t-test contrasting the
                                                                 activation levels of the Solids’ dog with the Solids’
                                                                 daughter was highly significant, t[9] = 5.61, p < .001 (see
                                                                 Figure 3).
                                                                    Simulations 1 and 2 do not, however, distinguish another
                                                                 set of predictions. It is possible that the network has
                                                                 learned to align the two families, either with respect to their
                                                                 relational structure or shared perceptual features, but only
                                                                 in an exact way. On this account, the model may have
                                                                 placed both mothers, both daughters, and both dogs in
                                                                 correspondence.
                                                                    On the other hand, perhaps the network has learned the
                                                                 details of the family relations within each family as well as
                                                                 across families. In this case, it could learn a regularity like
                                                                 “whoever owns the dog, walks the dog,” which is driven
                                                                 neither by perfect, global structural alignment nor by
                                                                 associations between surface features. This kind of
         Table 1: A subset of the information that the           relational binding is closely related to those tasks that
          network learns about each family member.               previous researchers have argued can only be done using a
                                                                 distinct mapping mechanism operating over explicit
                                                                 symbols (e.g. Gentner & Markman, 1993; Holyoak &
                                                             2905

                           Figure 3: Activation levels for the target units in the first three simulations.
Hummel, 2000). Therefore, it would be a surprising and               which the son of one family owns and walks the dog, and
exciting finding if this network were able to succeed in             the task of the model is to infer that the daughter of the
such an abstract relational mapping task.                            other family, who owns the dog, also walks it.
   In order to distinguish between these hypotheses, we ran             Inexact Match – Can the model align non-isomorphic
a third simulation, very similar to the first, except that in        structures? We can investigate the extent to which the
the Stripe family, the son, not the daughter, both owns and          network relies on perfectly overlaying the two families by
walks the dog. In this case, the network can only succeed            making the family structures only approximately match. In
in inferring that the Solids’ daughter walks the dog if it           the fourth simulation, the Stripes have three children, two
learns the details of the relational structure, and in               sons and a daughter, and one of the sons again owns (and
particular the regularity between owning a dog and walking           walks) their dog. The Solid family still has two children,
it. This is precisely what occurs. Separate tests contrasting        one son and one daughter, and their daughter owns the dog.
the activation level of the Solids’ daughter with the                Despite these changes, the model continues to make the
activation level of the Stripes’ son, t[9] = 2.58, p < .05, the      inference that she probably walks the dog as well. A paired
Solids’ son, t[9] = 2.95, p < .05, and the dog, t[18] = 3.35, p      t-test contrasting the activation levels of the Solids’
< .01, are all significant (see Figure 3).                           daughter with the Stripes’ son was highly significant, t[9] =
   This demonstrates that raw co-occurrence, or other                4.28, p < .01. This demonstrates that the network can learn
simple associative processes which are often believed to             to draw inferences over structures, like many of those in
underlie the performance of error-driven learning models             previous work (such as Falkenhainer et al., 1989), which
(e.g., Hummel, 2010 in reply to Ramscar, Yarlett, Dye,               are only partially alignable.
Denny, & Thorpe, 2010), is not the key to learning in this              Distributed Inputs – Does the model rely on
model. It is, however, interesting to notice that the Stripes’       implementing symbols? We have claimed that the success
son is the model’s choice early in training, suggesting that         of this network depends on its development of distributed,
the network first tends to make judgments predominately              subsymbolic representations, with which it can integrate the
based on surface similarity, but over time shifts towards            perceptual and the relational information about the family
judgments based on relational similarity. This “relational           members within a high-dimensional representational space.
shift” has been widely observed in the literature on the             Others might argue instead that the network is simply
development of analogical reasoning abilities (e.g.                  implementing symbols, and succeeds by performing some
Goswami, 1992). Intriguingly, this pattern is observed               syntax-like transformation on those symbols. Such an
throughout the various simulations presented in this paper.          argument may point to the localist input units representing
                                                                     the family members. We argue that the localist inputs are a
Extending The Model                                                  useful simplification, but that focusing on them is a
We have shown a basic set of simulations that succeed in             distraction, as the network can never directly exploit these
performing analogical inference from a family that is fully          localist units. Instead, it is required to re-represent each
described to one that is less fully described. In the                item as a pattern of activation over a hidden layer, as
simulations below, we will extend the basic model in                 described above.
several directions, addressing possible objections to our               To make this point more clearly, we ran a fifth
claim that it is in fact succeeding at analogical inference.         simulation that used distributed input representations for
Each of these models will extend the third simulation, in            the family members. Following a model by Rogers and
                                                                     McClelland (2004), these were simply chosen to be each
                                                                2906

family member’s corresponding perceptual features. This                                     Discussion
should not assist the network in acquiring the relational
                                                                   To summarize the results of the above simulations, we have
structure; if anything, it should bias the network towards
                                                                   demonstrated that analogical reasoning can emerge from a
the surface-level perceptual features for generalization.
                                                                   general, neurally inspired connectionist model of semantic
Nevertheless, the network still infers that the owner of the
                                                                   learning and reasoning. Critically, this analogical inference:
dog walks it, transferring from the Stripes’ son to the
                                                                   (1) is driven by generalization from a source domain to a
Solids’ daughter. A paired t-test contrasting the activation
                                                                   target domain; (2) relies on abstract relational structure, not
levels of the Solids’ daughter with the Stripes’ son was
                                                                   surface-level similarities or direct featural associations or
highly significant, t[9] = 6.05, p < .001.
                                                                   co-occurrences; (3) parallels important features of the
   Non-overlapping Outputs – Does the model require
                                                                   development of analogy in children; (4) can operate over
perceptual overlap? On the other hand, one might argue
                                                                   structures which only approximately match, or which are
that the architecture is biased in the opposite direction: the
                                                                   only partially alignable; (5) exploits structural similarity
more direct overlap between the two families at the feature
                                                                   even in the absence of explicit overlap, allowing the
level (that is, at the Output layer), the less work the model
                                                                   possibility of cross-domain analogical inference in guiding
needs to do to align their structures. What if only the
                                                                   learning; and (6) scales up to more complex training sets.
relational similarity is available, as might be the case when
                                                                      How is it that a connectionist model can succeed at this
constructing analogical mappings across very different
                                                                   kind of analogical inference task? As we have
domains of knowledge? This kind of analogy may be
                                                                   demonstrated in several variations of the model, it is not
critical for explaining how analogy can subserve cognition
                                                                   due to any direct co-occurrence of feature, nor is it due to
and reasoning more generally.
                                                                   any kind of surface-level similarity between the items.
   To test this, we constructed a sixth simulation that had
                                                                   Instead, we argue that part of the answer involves the
completely non-overlapping output units. The network
                                                                   progressive differentiation of its representations over the
essentially had two copies of each output property, so that
                                                                   course of development. Initially, all the weights are set to
each family’s target representations were totally distinct.
                                                                   very small random values, so the network essentially treats
To succeed in generalizing the relation between the two
                                                                   every family member, and every relation, as being the
families, the network would need to align the structures
                                                                   same. Over the course of training, the model learns to “pull
even in the absence of any surface-level similarity between
                                                                   apart” those representations that must be differentiated in
the two families. And this is precisely what it did. Again,
                                                                   order to produce the right answers. However, it only does
when the network is told that, in the Stripe family, the son
                                                                   so in response to erroneous predictions. This biases the
owns and walks the dog, it concludes that for the Solids, the
                                                                   network to reuse as much representational structure as it
owner of the dog -- the daughter -- must also walk it. A
                                                                   can get away with.
paired t-test contrasting the activation levels of the Solids’
                                                                      In this particular network, the families share a great deal
daughter with the Stripes’ son was significant, t[9] = 3.58,
                                                                   of structure. As a result, the network’s representations of
p < .01.
                                                                   the families become aligned over the course of training –
   Scaling up – Can the model make inferences when
                                                                   since this allows the network to learn more efficiently (i.e.,
given more than two families? Finally, it remains to be
                                                                   to reduce error more quickly). The side effect of this
shown that the ability of the model to make analogies does
                                                                   representational overlap is that when the network learns a
not depend on it living in a world with only two different
                                                                   fact about one family (e.g. one dog’s owner walks it), the
structures. Is it able to extend its learning to multiple
                                                                   representations of the members of the other family (e.g.
families?
                                                                   between that dog and its owner) get to come along for the
   In this final simulation, the network learned about four
                                                                   ride. This is not to say that the model is stuck with its first
rather than two distinct families (adding the Dash family
                                                                   guess about the structure of the world. As we indicated in
and the Dot family). In this training set, a different person
                                                                   the description of Simulation 3, and as is visible in other
walks the dog in each family. Additionally, two of the
                                                                   simulations, the model undergoes a developmental shift
families have slightly different structures: one has only a
                                                                   from predominantly perceptual to predominantly relational
son, another has two sons and a daughter. Despite this
                                                                   inference, when the environment warrants such a shift.
added complexity, the network infers that in the target
                                                                      We can observe the process of progressive differentiation
family, the daughter must also walk the dog. A within-
                                                                   in this network by looking at a clustering diagram of
subjects ANOVA using a planned contrast comparing the
                                                                   activation patterns along the Subject Representation layer at
activation values of the Solids’ daughter with the Stripes’
                                                                   different points in time for simulation 3 (see Figure 4).
son, the Dashes’ mother, and the Dots’ father (each a dog
                                                                   Early in training, the network groups items essentially at
walker in their respective family) was significant, F[1,36] =
                                                                   random, since the weights were initialized to very small
42.40, p < 0.01. Paired t-tests contrasting the activation
                                                                   values. Later in training, the network’s representations
levels of the Solids’ daughter with the dog walkers in each
                                                                   capture both the surface similarities and the relational
of the other families including the Solids’ son (t [9]=7.39, p
                                                                   similarities between items. Progressive differentiation in
< .001), the Dashes’ mother (t[9]=7.37, p < .01), and the
                                                                   semantic networks has been explored more extensively in
Dots’ father (t[9]=7.31, p < .001) were also significant.
                                                                   previous work (Flusberg et al., 2010; Rogers &
                                                                   McClelland, 2004).
                                                               2907

                                                                     development (Gentner, Simms, & Flusberg, 2009;
                                                                     Loewenstein & Gentner, 2005). Therefore, this approach
                                                                     views relational labels as another set of environmental
                                                                     regularities, serving the function of augmenting the
                                                                     statistical structure of the environment in ways that
                                                                     facilitate learning analogical representations (rather than as
                                                                     explicitly symbolic representations in the brain).
                                                                        In one sense, then, our model supports the view that
                                                                     analogy is a special component of cognition, because it
                                                                     allows us to draw inferences about things we haven’t
                                                                     directly experienced. In another sense, however, analogy is
                                                                     not special, in that we do not posit a separate set of
                                                                     cognitive machinery in order to accomplish analogical
                                                                     inference. Instead, these inferences emerge as a byproduct
                                                                     of learning to predict outcomes in an environment that
                                                                     contains relevant relational structure.
                                                                        Previous work has highlighted the difficulties of pursuing
                                                                     subsymbolic accounts of analogy (e.g. Gentner &
                                                                     Markman, 1993; Holyoak & Hummel, 2000). In part
                                                                     because of the lack of progress in this direction, some
                                                                     researchers have gone so far as to claim that analogical
                                                                     reasoning requires at least some explicitly symbolic
                                                                     representations, or even that a subsymbolic account is
                                                                     impossible in principle. Our model is, of course, not the
                                                                     first to counter these claims (see, e.g., Leech et al. 2008).
                                                                     On the other hand, it may be the first to demonstrate that a
                                                                     model equipped with subsymbolic representations can
                                                                     make novel analogical inferences. Leech and colleagues
                                                                     (2008) pointed to a possible reframing similar to our own
                                                                     (and to the principle of coherent covariation described in
                                                                     Rogers & McClelland, 2004), suggesting that, “analogical
                                                                     inferences might best be understood as novel
                                                                     generalizations governed by the distributional information
                                                                     about which input features and relations co-vary across the
    Figure 4: The hierarchical clusters above illustrate the         base and target domains” (p. 403).
similarity structure of the learned Subject representations in          This is not to say that this kind of semantic network can
     Simulation 3. Early in training (the upper panel), the          account for all of human cognition. Far from it! We do not
  network does not group individuals by family or relation.          believe that these models can even explain all of human
    Later in training, at 1,300 epochs (the lower panel), the        analogy. Many of the analogy tasks used in previous work,
      network has aligned the families according to their            which models like SME and LISA can capture so well, rely
                     relational similarity.                          on cognitive processes which we do not even attempt to
                                                                     model (e.g. Markman & Gentner, 1997; Morrison et al.,
   It is also important to clarify what aspects of the               2004). In particular, we would agree that some of these
environment we believe are encoded in our training                   tasks may rely on strong working memory and cognitive
patterns. Many of these patterns, such as those representing         control processes, one-shot learning and episodic memory,
the visual features of the family members, might be thought          and much richer linguistic abilities than we implement in
of as arising from perception. However, others, particularly         this model. In our model, we treat relational language as a
those representing familial relations such as “mother_of”            simple environmental cue, encoding a certain kind of
and “owner_of”, are much more likely to be encoded                   statistical structure that is then used to shape semantic
linguistically than visually. That is, part of our story is that     representations. While this is one important role of
learners hear language describing the people and things              language in analogical reasoning, it is not the only one; the
around them at the same time as they experience them                 ability to verbally re-describe a situation to oneself, for
directly, and these different sources of information are             example, is an important tool in many higher-level
integrated whenever (as we think is almost always the case)          reasoning tasks (Williams & Lombrozo, 2010).
there is some coherent covariation of information between               Therefore, we would like to suggest that one major
the several sources (Rogers & McClelland, 2004). This is             unsolved problem is the integration of the kind of slow-
consistent with a great deal of empirical work                       learning semantic cognition model described in this paper
demonstrating that relational language facilitates analogical        with the online, structurally explicit models already in
inference and drives the relational shift in analogical              place. The extensive and valuable work on models such as
                                                                 2908

SME and LISA over the past twenty years, no less than the          Conference of the Cognitive Science Society, volume 1,
connectionist models we have implemented, must be used             page 12. Amherst, MA.
to guide future research into analogical processing across            Holyoak, K., Gentner, D., and Kokinov, B. (2001). The
development, in behavior, and in the brain.                        place of analogy in cognition. In Gentner, D., Holyoak, K.,
                                                                   and Kokinov, B., editors, The analogical mind:
                    Acknowledgments                                Perspectives from cognitive science, pages 1–19. MIT
   The authors would like to thank Jay McClelland and Lera         Press, Cambridge, MA.
Boroditsky for inspiration and helpful, critical discussion of        Holyoak, K. and Hummel, J. (2000). The proper
the content of this paper and the issues it addresses. This        treatment of symbols in a connectionist architecture. In
material is based on work supported under a National               Dietrich, E. and Markman, A., editors, Cognitive dynamics:
Science Foundation Graduate Research Fellowship and a              Conceptual and representational change in humans and
Stanford Graduate Fellowship.                                      machines, pages 229–264. Lawrence Erlbaum Associates,
                                                                   Hillsdale, NJ.
                                                                      Hummel, J. and Holyoak, K. (1997). Distributed
                         References                                representations of structure: A theory of analogical access
   Bowdle, B. and Gentner, D. (1997). Informativity and            and mapping. Psychological Review, 104(3):427–466.
asymmetry in comparisons. Cognitive Psychology,                       Hummel, J. E. (2010). Symbolic versus associative
34(3):244–286.                                                     learning. Cognitive Science, 34(6):958–965.
   Dilkina, K., McClelland, J., and Plaut, D. (2008). A               Leech, R., Mareschal, D., and Cooper, R. (2008).
single-system account of semantic and lexical deficits in          Analogy as relational priming: A developmental and
five      semantic       dementia    patients.      Cognitive      computational perspective on the origins of a complex
Neuropsychology, 25(2):136–164.                                    cognitive skill. Behavioral and Brain Sciences,
   Doumas, L., Hummel, J., and Sandhofer, C. (2008). A             31(04):357–378.
theory of the discovery and predication of relational                 Loewenstein, J. and Gentner, D. (2005). Relational
concepts. Psychological Review, 115(1):1–43.                       language and the development of relational mapping.
   Falkenhainer, B., K.D., F., and Gentner, D. (1989). The         Cognitive Psychology, 50(4):315–353.
structure-mapping engine: Algorithm and examples.                     Markman, A. and Gentner, D. (1997). The effects of
Artificial Intelligence, 41(1):1–63.                               alignability on memory. Psychological Science, 8(5):363–
   Flusberg, S. J., Thibodeau, P. H., Sternberg, D. A., and        367.
Glick, J. J. (2010). A connectionist approach to embodied             Morrison, R., Krawczyk, D., Holyoak, K., Hummel, J.,
conceptual metaphor. Frontiers in Psychology, 1(0):12.             Chow, T., Miller, B., and Knowlton, B. (2004). A
   French, R. (2002). The computational modeling of                neurocomputational model of analogical reasoning and its
analogy-making. Trends in Cognitive Sciences, 6(5):200–            breakdown in frontotemporal lobar degeneration. Journal
205.                                                               of Cognitive Neuroscience, 16(2):260–271.
   Gentner, D. (1983). Structure-mapping: A theoretical               Penn, D., Holyoak, K., and Povinelli, D. (2008).
framework for analogy. Cognitive Science, 7(2):155–170.            Darwin’s mistake: Explaining the discontinuity between
   Gentner, D. (2010). Bootstrapping the mind: Analogical          human and nonhuman minds. Behavioral and Brain
processes and symbol systems. Cognitive Science,                   Sciences, 31(02):109–130.
34(5):752–775.                                                        Ramscar, M., Yarlett, D., Dye, M., Denny, K., and
   Gentner, D. and Markman, A. B. (1993). Analogy -                Thorpe, K. (2010). The effects of feature-label-order and
watershed or waterloo? structural alignment and the                their implications for symbolic learning. Cognitive Science,
development of connectionist models of cognition. In               34(6):909–957.
Advances in Neural Information Processing Systems 5,                  Rogers, T. T. and McClelland, J. L. (2004). Semantic
[NIPS Conference], pages 855–862, San Francisco, CA,               Cognition. MIT Press, Cambridge, MA.
USA. Morgan Kaufmann Publishers Inc.                                  Rumelhart, D. (1990). Brain style computation: Learning
   Gentner, D., Simms, N., and Flusberg, S. (2009).                and generalization. In Zornetzer, S., Davis, J. L., and Lau,
Relational language helps children reason analogically. In         C., editors, An introduction to neural and electronic
Taatgen, N. A. and van Rijn, H., editors, Proceedings of the       networks, pages 405–420. Academic Press.
31th Annual Conference of the Cognitive Science Society.              Williams, J. and Lombrozo, T. (2010). The role of
   Goswami, U. (1992). Analogical reasoning in children.           explanation in discovery and generalization: evidence from
Psychology Press.                                                  category learning. Cognitive Science, 34(5):776–806.
   Hinton, G. (1986). Learning distributed representations
of concepts. In Proceedings of the Eighth Annual
                                                               2909

