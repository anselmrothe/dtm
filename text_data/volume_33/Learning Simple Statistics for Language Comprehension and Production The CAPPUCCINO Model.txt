UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Simple Statistics for Language Comprehension and Production: The CAPPUCCINO
Model
Permalink
https://escholarship.org/uc/item/97g4d9pc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
McCauley, Stewart M.
Christiansen, Morten H.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

          Learning Simple Statistics for Language Comprehension and Production:
                                                The CAPPUCCINO Model
                                           Stewart M. McCauley (smm424@cornell.edu)
                                      Morten H. Christiansen (christiansen@cornell.edu)
                                Department of Psychology, Cornell University, Ithaca, NY 14853 USA
                              Abstract                                  that grammatical knowledge develops gradually through
   Whether the input available to children is sufficient to explain     abstraction over multi-word utterances (e.g., Tomasello,
   their ability to use language has been the subject of much           2003), which are assumed to be stored as multi-word
   theoretical debate in cognitive science. Here, we present a          ‘chunks.’ Testing this latter assumption, Bannard and
   simple, developmentally motivated computational model that           Matthews (2008) showed not only that non-idiomatic chunk
   learns to comprehend and produce language when exposed to            storage takes place, but also that storing such units actively
   child-directed speech. The model uses backward transitional
   probabilities to create an inventory of ‘chunks’ consisting of
                                                                        facilitates processing: young children repeated multi-word
   one or more words. Language comprehension is                         sequences faster, and with greater accuracy, when they
   approximated in terms of shallow parsing of adult speech and         formed a frequent chunk. Moreover, Arnon and Snider
   production as the reconstruction of the child’s actual               (2010) extended these results, demonstrating an adult
   utterances. The model functions in a fully incremental, on-          processing advantage for frequent phrases. The existence of
   line fashion, has broad cross-linguistic coverage, and is able       such chunks is problematic for generative approaches that
   to fit child data from Saffran’s (2002) statistical learning
                                                                        have traditionally clung to a words-and-rules perspective, in
   study. Moreover, word-based distributional information is
   found to be more useful than statistics over word classes.           which memory-based learning and processing are restricted
   Together, these results suggest that much of children’s early        to the level of individual words (e.g., Pinker 1999).
   linguistic behavior can be accounted for in a usage-based               One remaining challenge for usage-based approaches is to
   manner using distributional statistics.                              provide an explicit computational account of language
   Keywords:Language Learning; Computational Modeling;                  comprehension and production based on multi-word chunks.
   Corpora; Chunking; Shallow Parsing; Usage-Based Approach             Although Bayesian modeling has shown that chunk-based
                                                                        grammars are in principle sufficient for the acquisition of
                          Introduction                                  linguistic productivity (Bannard, Lieven, & Tomasello,
                                                                        2009), no full-scale computational model has been
The ability to produce and understand a seemingly
                                                                        forthcoming (though models of specific aspects of
unbounded number of different utterances has long been
                                                                        acquisition do exist, such as the optional infinitive stage;
hailed as a hallmark of human language acquisition. But
                                                                        Freudenthal, Pine & Gobet, 2009). The scope of the
how is such open-endedness possible, given the much more
                                                                        computational challenge facing usage-based approaches
limited nature of other animal communication systems? And
                                                                        becomes even more formidable when considering the
how can a child acquire such productivity, given input that
                                                                        success with which the generativist principles of words and
is both noisy and necessarily finite in nature? For nearly half
                                                                        rules have been applied in computational linguistics. In this
a century, generativists have argued that human linguistic
                                                                        paper, we take an initial step towards answering this
productivity can only be explained by positing a system of
                                                                        challenge by presenting the ‘Comprehension And
abstract grammatical rules working over word classes and
                                                                        Production      Performed      Using     Chunks     Computed
scaffolded by considerable innate language-specific
                                                                        Incrementally, Non-categorically, and On-line’ (or
knowledge (e.g., Pinker, 1999). Recently, however, an
                                                                        CAPPUCCINO) model of language acquisition.
alternative theoretical perspective on linguistic productivity
                                                                           The aim of the CAPPUCCINO model is to provide a test
has emerged in the form of usage-based approaches to
                                                                        of the usage-based assumption that children’s language use
language (e.g., Tomasello, 2003). This perspective is
                                                                        may be explained in terms of stored chunks. To this end, the
motivated by analyses of child-directed speech, showing
                                                                        model gradually builds up an inventory of chunks consisting
that there is considerably more information available in the
                                                                        of one or more words—a ‘chunkatory’—used for both
input than previously assumed. For example, distributional
                                                                        language comprehension and production. The model was
and phonological information can provide reliable cues for
                                                                        further designed with several key psychological and
learning about lexical categories and phrase structure (for a
                                                                        computational properties in mind: a) incremental learning:
review, see Monaghan & Christiansen, 2008). Behavioral
                                                                        at any given point in time, the model can only rely on the
studies have shown that children can use such information
                                                                        input seen so far (no batch learning); b) on-line processing:
in an item-based manner (Tomasello, 2003).
                                                                        input is processed word-by-word as it is encountered; c)
   A key difference between generative and usage-based
                                                                        simple statistics: learning is based on computing backward
approaches pertains to the granularity of the linguistic units
                                                                        transitional probabilities (which 8-month-olds can track;
necessary to account for the productivity of human
                                                                        Pelucchi, Hay, & Saffran, 2009); d) comprehension: the
language. At the heart of usage-based theory lies the idea
                                                                    1619

model segments the input into chunks comparable to the              child corpora, in order to assess what could be learned from
output of a shallow parser; e) production: the model                the input available to individual children. All corpora
reproduces the child’s actual utterances; f) naturalistic           involved interactions between a child and one or more
input: the model learns from child-directed speech; g) cross-       adults. The average age of the target child at the beginnings
linguistic coverage: the model is exposed to a typologically        of the corpora was 1;8, and 3;6 at the ends. The average
diverse set of languages (including Sesotho, Tamil,                 number of words in each corpus was 168,204.
Estonian, and Indonesian).
   In what follows, we first describe the basic workings of                        Table 1: Natural Language Corpora
the CAPPUCCINO model, its comprehension performance                   Language       Genus           Family          Word Ord.
across English, German, and French, and its production                English        Germanic        Indo-European      SVO
ability across 13 different languages. Next, we demonstrate           German         Germanic        Indo-European       n.d.
that the model is capable of closely fitting child data from a        French         Romance         Indo-European      SVO
statistical learning study (Saffran, 2002). Finally, we discuss       Irish          Celtic          Indo-European      VSO
the limitations of the current model.                                 Croatian       Slavic          Indo-European      SVO
                                                                      Estonian       Finnic          Uralic             SVO
  Simulation 1: Modeling Comprehension and                            Hungarian      Ugric           Uralic              n.d.
          Production in Natural Languages                             Hebrew         Semitic         Afro-Asiatic       SVO
                                                                      Sesotho        Bantoid         Niger-Congo        SVO
The CAPPUCCINO model performed two tasks:                             Tamil          Dravidian       Dravidian          SOV
comprehension of child-directed speech through the                    Indonesian     Sundic          Austronesian       SVO
discovery and use of chunks, and sentence production                  Cantonese      Chinese         Sino-Tibetan       SVO
through the use of the same chunks and statistics as in               Japanese       Japanese        Japanese           SOV
comprehension. Comprehension was approximated in terms
of the model’s ability to segment a corpus into phrasal units,         The selected languages differed syntactically in a number
and production in terms of the model’s ability to reconstruct       of ways (see Table 1). Four word orders were represented:
utterances produced by the child in the corpus. Thus, the           SVO, VSO, SOV, and no dominant order (n.d.; Haspelmath
model sought to 1) build an inventory of chunks—a                   et al., 2005). The languages varied widely in morphological
chunkatory—and use it to segment out phrases, and 2) use            complexity, falling across the isolating/synthetic spectrum:
the chunks to reproduce child utterances. We hypothesized           while some languages had a relatively low morpheme-to-
that both problems could, to a large extent, be solved by           word ratio (e.g., Cantonese), others had a much higher ratio
attending to a single statistic: transitional probability (TP).     (e.g., Hungarian), and others had ratios falling between the
   TP has been proposed as a cue to phrase structure in the         two (e.g., Sesotho; Chang, Lieven, & Tomasello, 2008).
statistical learning literature: peaks in TP can be used to            Corpus Preparation Each corpus was submitted to the
group words together, whereas dips in TP can be used to             same automated procedure whereby punctuation (including
find phrase boundaries (e.g., Thompson & Newport, 2007).            apostrophes: e.g., it’s→its), codes, and tags were removed,
The view put forth in such studies is that TP is useful for         leaving only speaker identifiers and the original sequence of
discovering phrase structure when computed over form                words. Hash tags (#) were added to the beginning of each
classes rather than words themselves. We hypothesized,              line to signal the start of the utterance.
instead, that distributional information tied to individual
words provides richer cues to syntactic structure than has          Comprehension Task
been assumed previously. Because we adopted this item-              Child language comprehension was approximated in terms
based approach, we decided to examine backward                      of the model’s ability to segment the corpus into phrasal
transitional probability (BTP) as well as forward transitional      units. The model's performance was evaluated against a
probability (FTP). If learners compute statistics over              shallow parser, a tool (widely used in the field of natural
individual words rather than form classes, the FTP between          language processing) which identifies and segments out
the words in phrases like the cat will always be low, given         non-embedded phrases in a text. The shallow parsing
the sheer number of nouns that may follow any given                 method was chosen because it is consistent with the
determiner. BTPs provide a way around this issue: given the         relatively underspecified nature of human sentence
word cat, the probability that the determiner the                   comprehension (Sanford & Sturt, 2002) and provides a
immediately precedes it is quite high.                              reasonable approximation of the item-based way in which
                                                                    children process sentences (cf. Tomasello, 2003).
Corpora                                                                For reasons explained above, we focused on BTP as a cue
Thirteen corpora were selected from the CHILDES database            to phrasal units. The model discovered chunks by tracking
(MacWhinney, 2000) to cover a typologically diverse set of          the peaks and dips in BTP between words, using high BTPs
languages, representing 12 genera from 8 different language         to group words into phrases and low BTPs to identify phrase
families (Haspelmath, Dryer, Gil, & Comrie, 2005). For              boundaries. Chunks learned in this way were then used to
each language, the largest available corpus for a single child      help process and learn from subsequent input. We tested the
was chosen rather than aggregating data across multiple
                                                                1620

model on the corpora for which an automated scoring               was calculated by hits / (hits + false alarms), and
method was available: English, German, and French.                completeness by hits / (hits + misses).
   Model The model discovered its first chunks through               Alternate Distributional Models As previous work in
simple sequential statistics. Processing utterances on a          the statistical learning literature has focused on FTP as a cue
word-by-word basis, the model learned frequency                   to phrase structure (e.g., Thompson & Newport, 2007), an
information for words and word-pairs, which was used on-          alternate model was created to compare the usefulness of
line to track the BTP between words and maintain a running        this cue against the BTPs used by CAPPUCCINO. This
average BTP for previously encountered pairs. When the            model was identical to the original model, but used FTPs in
model calculated a BTP that was greater than expected,            place of the BTPs. We refer to this as the FTP-chunk model.
based on the running average, it grouped the word-pair            To assess the usefulness of variable-sized chunks, two
together such that it would form part (or all) of a chunk;        additional alternate models were created which lacked
when the calculated BTP met or fell below the running             chunkatories, relying instead on either FTPs or BTPs
average, a boundary was placed and the chunk thereby              computed over stored trigrams (in the case of the former, if
created (consisting of one or more words to the left) was         the FTP between the first bigram and the final unigram of a
added to the chunkatory.                                          trigram fell below the average, a boundary was inserted).
   Once the model discovered its first chunk, it began using      We refer to these models as the FTP-3G and BTP-3G
its chunkatory to assist in processing the input on the same      alternates, respectively.
word-to-word basis as before. The model continued learning           Word Class Corpora A great deal of work in
the same low-level distributional information and                 computational linguistics has assumed that statistics
calculating BTPs, but also used the chunkatory to make on-        computed over form classes are superior to word-based
line predictions as to which words would form a chunk,            approaches for learning about syntax (hence the widespread
based on previously learned chunks. When a word-pair was          use of tagged corpora). This assumption is also present
encountered, it was checked against the chunkatory; if it had     throughout the statistical learning literature (e.g., Thompson
occurred at least twice as a complete chunk or as part of a       & Newport, 2007; Saffran, 2002), but is at odds with the
larger chunk, the words were grouped together and the             present model, which relies on statistics computed over
model moved on to the next word. If the word-pair was not         individual words rather than classes. To evaluate the
represented strongly enough in the chunkatory, the BTP was        usefulness of word-based transitional probabilities against
compared to the running average, with the same                    those calculated over word classes, we ran the model and
consequences as before. Thus, there were no a priori limits       alternates on separate versions of each of the three corpora,
on the number or size of chunks that could be learned.            in which words were replaced by the names of their lexical
   As an example, consider the following scenario in which        categories. For English, this process was automatically
the model encounters the phrase the blue doll for the first       carried out using the tags in the original corpus. The
time and its chunkatory includes the blue car and blue doll       untagged French and German corpora were tagged using
(with counts greater than 2). When processing the and blue,       TreeTagger (Schmid, 1994) before undergoing the same
the model will not place a boundary between these two             process. Across all three corpora, the same 13 categories
words because the word-pair is already strongly represented       were used (noun, verb, adjective, numeral, adverb,
in the chunkatory (as in the blue car). The model therefore       determiner, pronoun, preposition, conjunction, interjection,
predicts that this bigram will form part of a chunk. Next,        abbreviation, infinitive marker, and proper name). Unknown
when processing blue and doll, the model reacts similarly,        words (e.g., transcribed babbling) were marked as such.
as this bigram is also represented in the chunkatory. The            Results and Discussion The results are displayed in
model thereby combines its knowledge of two chunks to             Figure 1. Chi-square tests were performed separately for
discover a new, third chunk, the blue doll, which is added to     accuracy and completeness on each language/model pair,
the chunkatory. As a consequence, the (sub)chunk, the blue,       contrasting BTP vs. FTP, chunks vs. 3G, and words vs.
becomes even more strongly represented in the chunkatory,         classes. All differences observable in the graph were highly
as there are now two chunks in which it appears.                  significant (p<.001), with the exceptions of non-significant
   Scoring The model was scored against shallow parsers:          differences in accuracy when using words vs. classes for the
the Illinois Chunker (Punyakanok & Roth, 2001) was used           FTP-chunk model (German) and both 3G models (English).
for English, and TreeTagger (Schmid, 1994) was used for              When exposed to words, CAPPUCCINO offered the best
French and German. After shallow parsing the corpora,             combination of accuracy and completeness; for each
phrase labels (VP, NP, etc.) were removed and replaced            language, it scored highest on both measures, with the
with boundary markers of the sort produced by the model.          exception of a 1%-point accuracy difference from the
   Each boundary marker placed by the model was scored as         French class-based FTP-3G alternate (note, however, that
a hit if it corresponded to a boundary marker created by the      CAPPUCCINO had a better completeness score by 23%-
shallow parser, and as a false alarm otherwise. Each              points). The 3G alternates displayed higher accuracy when
boundary placed by the shallow parser but which was not           exposed to classes rather than words (with the exceptions of
placed by the model was scored as a miss. Thus, accuracy          FTP-3G for English and BTP-3G for German), but lower
                                                                  completeness; in all cases, completeness was far lower for
                                                              1621

                      Fig. 1: Accuracy and completeness scores for CAPPUCCINO and the FTP-chunk/3G alternates
class-based models (with the exception of the 3G-BTP                  from the utterance and placed in the bag as a single chunk.
alternate for English). More generally, the best performance          Next, chased a cat is checked, unsuccessfully, followed by
was achieved using BTP-based chunks for all three                     chased a, also without success. The word chased is placed
languages, despite syntactic differences between them;                in the bag. Then a cat is checked, and so on. Crucially,
though two of the languages were SVO, the third (German)              however, this procedure was only used to find chunks that
had no dominant word order.                                           the model already knew (i.e., that were in the chunkatory)
   Thus, CAPPUCCINO was able to approximate the                       and would be likely to use as such (e.g., the dog). Once in
performance of a shallow parser by learning in an on-line,            the bag, the order of chunks was randomized.
incremental fashion from a single distributional cue. This               During production, the model had to reproduce the child’s
was only the case, however, when the model was exposed to             utterance using the unordered chunks in the bag. We
individual words rather than lexical categories. In addition          modeled this as an incremental, chunk-to-chunk process
to highlighting the wealth of distributional information              rather than one of whole-sentence optimization. Thus, the
available in the input, these results suggest that item-              model began by removing from the bag the chunk with the
specific knowledge of phrase structure may be more useful             highest BTP given the # tag (which marked the beginning of
to early learners than abstract knowledge.                            each utterance in the corpus), and producing it as the start of
                                                                      its new utterance. The chunk was removed from the bag
Sentence Production Task                                              before the model selected and produced its next chunk, the
The production task was inspired by the bag-of-words                  one with the highest BTP given the most recently produced
incremental generation task used by Chang et al. (2008),              chunk. In this manner, the model used chunk-to-chunk
which offers a method for automatically evaluating                    BTPs to incrementally produce the utterance, adding chunks
syntactic learners on any language corpus. In our task, the           one-by-one until the bag was empty. In rare cases where
model made its way through the corpus incrementally,                  two or more units in the bag-of-words were tied for the
collecting statistics and discovering chunks in the service of        highest BTP, one of them was chosen at random.
comprehension (as described above). Each time the model                  Scoring Method For each utterance the model produced
encountered a multi-word child utterance, however, it was             correctly, it received a score of 1; if the utterance did not
required to recreate the utterance using only chunk                   match the corresponding child utterance completely, a score
information discovered in the previously encountered input.           of 0 was assigned. The overall percentage of correctly
   Model We began with the assumption that the overall                produced utterances was then used as a measure of sentence
message which the child wanted to convey could be                     production performance for a given corpus.
approximated by treating the utterance as a randomly-                    Alternate Models The alternate FTP-chunk and 3G
ordered set of words: a ‘bag-of-words.’ The task for the              models used in the comprehension task were again used as
model, then, was to place these words in the correct order            baselines. The FTP-chunk model performed production in
(as originally produced by the child). Following usage-               an identical manner to CAPPUCCINO (but used FTPs). As
based approaches, the model utilized its chunkatory to                the 3G alternates lacked chunk inventories, they relied on
reconstruct the child’s utterances. In order to model retrieval       TPs between unigrams and the start-of-utterance marker to
of stored chunks during production, the bag-of-words was              select the first word in an utterance before using TPs based
filled by comparing parts of the child’s utterance against the        on trigram statistics for every subsequent word.
chunkatory. E.g., consider a scenario in which the model                 Results and Discussion The average sentence production
encounters the child utterance the dog chased a cat and the           score for all 13 corpora (see Figure 2) was 59.8% for
largest chunk in the chunkatory consists of 3 words. To               CAPPUCCINO, compared to 52.3%, 49.6%, and 54.2% for
begin, the first 3 words are checked for storage as a single          the FTP-chunk, FTP-3G, and BTP-3G alternates,
chunk. As this is not found in the chunkatory, the dog is             respectively. The model scored higher than the alternates on
checked. This check succeeds, so the words are removed                all corpora. A 2 (Unit Type: Chunk vs. 3G) x 2 (Direction:
                                                                  1622

                Fig. 2: Sentence production performance for CAPPUCCINO and the FTP-chunk/3G alternates
FTP vs. BTP) ANOVA confirmed the observable model
differences in Figure 2, yielding main effects of Unit Type          Method
(F(1,12)=26.2, p<.001) and Direction (F(1,12)=14.9,                  The model was identical to that used with natural languages.
p<.01), and a Unit Type x Direction interaction                      Importantly, this meant that the model continued learning
(F(1,12)=24.6, p<.001).                                              during exposure to test items. For each language, 15
   These results offer substantial, cross-linguistic support for     simulations were performed, corresponding to the 30 child
CAPPUCCINO, and, more broadly, for the view that simple              subjects from Saffran (2002). The model received the same
distributional statistics can capture a considerable part of         amount of exposure to the exact same stimuli as the human
early linguistic behavior. A single distributional cue, BTP,         subjects did (for each language, 50 sentences repeated 8
was used by the model to discover chunks as well as                  times for a total of 400 training items, followed by 24 test
combine them to construct over half of the child utterances          item pairs). Each test item pair consisted of one sentence
in each corpus (in one case, over 75%). Importantly, this            that was grammatical, and one that was ungrammatical.
approach was effective across the isolating/synthetic                Saffran’s languages were created such that the same set of
spectrum, yielding high performance despite the                      test items could be used for both language exposure
morphological complexity of the language learned by the              conditions. We hypothesized that the human responses in
model. This was true for all four word orders represented.           this study were primarily based on sensitivity to the phrase-
   The results also serve to demonstrate that a single source        like structure of the test stimuli. The model was therefore
of information is sometimes useful for learning about                evaluated against a version of the test items that contained
structure at multiple levels: the same distributional statistic      the correct phrase boundaries, as defined by the rewrite
(BTP) can be used to segment words when calculated over              rules used to generate the sentences in Saffran’s study.
syllables (e.g., Pelucchi et al., 2009), to discover phrase          Boundaries were placed between phrases in a non-
structure when calculated over words (as in the                      embedded fashion that emulated the shallow parsing
comprehension task), and to construct utterances when                technique used to evaluate the model’s performance on
calculated over multi-word chunks.                                   natural languages in Simulation 1.
                                                                        To further contrast the usefulness of item-specific vs.
         Simulation 2: Modeling Child AGL                            class-based distributional information, a separate set of
Artificial grammar learning (AGL) studies provide a means            simulations was performed after each word had been
to study learning from language-like stimuli in a controlled         replaced by the corresponding class symbol from the rewrite
setting. As such, they provide a rich source of                      rules in the original Saffran (2002) study.
psycholinguistic data for constraining computational                    Scoring To model the two-alternative forced choice
accounts of learning from distributional information. We             (2AFC) task from Saffran (2002), each item in a given test
therefore tested CAPPUCCINO’s ability to model data from             pair was scored according to the number of correctly placed
a child AGL experiment (Saffran, 2002).                              phrase boundaries. The item with the highest score was then
   This particular study was chosen because it demonstrated          selected as the model’s response. If the model produced the
the use of predictive dependencies on the part of the learner        same number of hits (including zero) for both items, a
to group words into phrases. Subjects (aged 7;6 to 9;8) were         choice was made at random, allowing individual differences
trained on nonsense sentences generated by one of two                to appear across simulations.
artificial grammars. Each grammar consisted of a set of
rewrite rules used to generate an artificial language: one           Results and Discussion
incorporated predictive dependencies between words within            The child subjects in Saffran (2002) had overall correct
phrases (Language P), while the other lacked this cue                response rates of 71.8% for Language P and 58.3% for
(Language N). When tested on grammatical/ungrammatical               Language N. The model provided a close quantitative fit,
item pairs, children exposed to Language P outperformed              with overall correct response rates of 70.5% for Language P
those exposed to Language N.                                         and 57.5% for Language N. When the model was given
                                                                     information on word classes instead of concrete words,
                                                                 1623

                                                                    a child may discover its earliest chunks before segmentation
                                                                    of the component words has taken place. Finally, the
                                                                    unitization account offered by the model is oversimplified;
                                                                    psycholinguistic work suggests that there is no frequency
                                                                    ‘threshold’ beyond which collocations are unitized, but
                                                                    instead that the processing advantage for chunks increases
                                                                    as a function of frequency (Arnon & Snider, 2010). In future
                                                                    work, we will thus aim to extend CAPPUCCINO by
                                                                    exposing it to unsegmented corpora, by making its chunk
                                                                    processing more graded, and by applying it to specific
                                                                    patterns of language acquisition.
  Fig. 3: 2AFC task accuracy (%) for subjects and CAPPUCCINO
                                                                                         Acknowledgments
however, it provided a poor fit to the child data, with 81.1%       Thanks to Jen Misyak and Rick Dale for helpful comments.
accuracy for Language P and 39.7% for Language N.
   Thus, the model provides a close fit to these                                                References
psycholinguistic data, suggesting that the ability to group         Arnon, I., & Snider, N. (2010). More than words: Frequency
words into larger units can indeed account for subject                 effects for multi-word phrases. Journal of Memory and
performance in the original study. While predictive                    Language, 62, 67-82.
dependencies between word classes were a potentially                Bannard, C., Lieven, E.V., & Tomasello, M. (2009). Modeling
useful cue, the calculation of statistics over classes in the          children’s early grammatical knowledge. Proceedings of the
present model could not account for subject performance as             National Academy of Sciences, 106, 17284-17289.
well as the word-based approach. This resonates with the            Bannard, C., & Matthews, D. (2008). Stored word sequences in
superior performance of the model on natural languages                 language learning. Psychological Science, 19, 241-248.
                                                                    Chang, F., Lieven, E., & Tomasello, M. (2008). Automatic
when working with words as opposed to lexical categories.
                                                                       evaluation of syntactic learners in typologically-different
                                                                       languages. Cognitive Systems Research, 9, 198-213.
                    General Discussion                              Freudenthal, D. Pine, J.M. & Gobet, F. (2009). Simulating the
Our CAPPUCCINO model has demonstrated that                             referential properties of Dutch, German, and English Root
incremental learning and on-line processing based on a                 Infinitives in MOSAIC. Language Learning and Development,
single distributional cue, BTP, can capture a considerable             5, 1-29.
                                                                    Haspelmath, M., Dryer, M.S., Gil, D., & Comrie, B. (2005). The
part of children’s early linguistic behavior. In addition to           world atlas of language structures. Oxford: OUP
approximating the performance of a shallow parser with              MacWhinney, B. (2000). The CHILDES project: Tools for
high accuracy and completeness, the model was able to                  analyzing talk, Vol. II: The database. Mahwah, NJ: LEA.
reproduce the majority of the child utterances in each of a         Monaghan, P. & Christiansen, M.H. (2008). Integration of multiple
typologically diverse set of 13 corpora, and closely fit AGL           probabilistic cues in syntax acquisition. In H. Behrens (Ed.),
data from child subjects. In line with usage-based                     Trends in corpus research: Finding structure in data (pp. 139-
approaches to language (e.g., Tomasello, 2003), the model’s            163). Amsterdam: J. Benjamins.
superior comprehension performance and ability to fit child         Pelucchi, B., Hay, J.F., & Saffran, J.R. (2009). Learning in reverse:
AGL data when exposed to words as opposed to lexical                   Eight-month-old       infants   track    backward     transitional
                                                                       probabilities. Cognition, 113, 244-247.
categories suggests that knowledge of concrete words and            Pinker, S. (1999). Words and rules. New York: Basic Books.
chunks may be more important to early language acquisition          Punyakanok, V., & Roth, D. (2001). The use of classifiers in
than abstract rules operating over word classes. Of course,            sequential inference. In Dietterich, G., Becker, S., &
there is more to comprehension than shallow parsing—e.g.,              Ghahramani, Z. (Eds.), Proceedings of the Conference on
meaning is not taken into account—but it is encouraging to             Advances in Neural Information Processing Systems (pp. 995-
see just how well the model can reconstruct children’s                 1001). Cambridge, MA: MIT Press
utterances based on distributional information alone.               Saffran, J. R. (2002). Constraints on statistical language learning.
   As an initial step towards a chunk-based account of                 Journal of Memory and Language, 47, 172-196.
                                                                    Sanford, A.J. & Sturt, P. (2002). Depth of processing in language
children’s comprehension and production of language,
                                                                       comprehension: Not noticing the evidence. Trends in Cognitive
CAPPUCCINO is not without limitations. Firstly, although               Sciences, 6, 382-386.
it closely fits child data from an artificial language learning     Schmid, H. (1995, March). Improvements in part-of-speech
study, it is important to determine whether our model can              tagging with an application to German. Paper presented at the
also account for specific patterns of natural language                 EACL-SIGDAT Workshop, Dublin, Ireland.
acquisition (e.g., similar to MOSAIC’s match with cross-            Thompson, S.P., & Newport, E.L. (2007). Statistical learning of
linguistic data regarding the optional infinitive stage;               syntax: The role of transitional probability. Language Learning
Freudenthal et al., 2009). Secondly, our model learns from             and Development, 3, 1-42.
already segmented speech and does not address the ways in           Tomasello, M. (2003). Constructing a language: A usage-based
                                                                       theory of language acquisition. Cambridge, MA: HUP.
which word segmentation may impact on chunk discovery;
                                                                1624

