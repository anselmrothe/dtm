UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Comparison of Human and Agent Reinforcement Learning in Partially Observable Domains
Permalink
https://escholarship.org/uc/item/1s82r2tf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Doshi-Velez, Finale
Ghahramani, Zoubin
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                   A Comparison of Human and Agent Reinforcement Learning
                                           in Partially Observable Domains
                              Finale Doshi-Velez                                        Zoubin Ghahramani
                    Massachusetts Institute of Technology                             University of Cambridge
                          Cambridge, Massachusetts                                        United Kingdom
                            finale@alum.mit.edu                                      zoubin@eng.cam.ac.uk
                             Abstract                                   for decision making under uncertainty than the simpler (fully-
                                                                        observable) Markov decision processes, since they assume
   It is commonly stated that reinforcement learning (RL) algo-         that the state of the world is known, but inferred from noisy
   rithms require more samples to learn than humans. In this
   work, we investigate this claim using two standard problems          observations. In this setting, we show that, surprisingly, when
   from the RL literature. We compare the performance of human          put in an identical setup on standard decision making prob-
   subjects to RL techniques. We find that context—the meaning-         lems, RL methods often learn faster and achieve better solu-
   fulness of the observations—plays a significant role in the rate
   of human RL. Moreover, without contextual information, hu-           tions than humans. Even more surprisingly, while human per-
   mans often fare much worse than classic algorithms. Compar-          formance does improve when subjects are given contextual
   ing the detailed responses of humans and RL algorithms, we           information about the problem, their average performance of-
   also find that humans appear to employ rather different strate-
   gies from standard algorithms, even in cases where they had          ten still does not match RL methods. Our work has inter-
   indistinguishable performance to them.                               esting implications for our understanding of both human and
   Keywords: sequential decision-making; reinforcement learn-           machine decision making. Without contextual information,
   ing; computational models; machine learning                          humans may require more experience than RL algorithms to
                                                                        perform well even on simple problems. However, making use
   The ability of humans to make sequential decisions under             of context is one of the important open problems for machine
uncertainty has been widely studied in psychology and neu-              learning.
roscience. The field of reinforcement learning (RL) studies
the theoretical formulation and algorithmic implementation                                      Experiment
of artificial agents that make sequential decisions to maxi-            We tested two hypotheses: first, that human subjects would
mize their expected reward (Sutton & Barto, 1998). While RL             perform significantly better if given contextual observations,
algorithms often provide theoretical guarantees on the qual-            and second, that human subjects would outperform RL algo-
ity of the agent’s long-term behaviour, the common lore in              rithms. Performance was evaluated as the sum of rewards ob-
the RL community (Singh, 2009; Peters, Bagnell, & Schaal,               tained during the last tenth of a learning trial. We also exam-
2006; Morimoto & Doya, 2005) is these approaches are                    ined which RL algorithms’ behaviour most closely matched
painfully slow, requiring thousands of trials to learn to act           human behaviour.
in, what seem to humans, relatively simple domains.
   While RL has been applied as a theoretical tool for un-              Task Descriptions
derstanding human decision making behaviour (Samejima &                 The tasks consisted of two common problems in the RL lit-
Doya, 2007; Daw, O’Doherty, Dayan, Seymour, & Dolan,                    erature, both formulated as POMDPs. Playing the role of
2006; Kakade & Dayan, 2002; Daw, Courville, & Tourtezky,                the agent, the human subject—who had no initial knowledge
2006; Yoshida & Ishii, 2003; Acuña & Schrater, 2008; Dayan             about the structure of the problem—selected actions to take.
& Daw, 2008), the supposed “slowness” of RL methods has                 The problem returned an observation, displayed on a com-
not been experimentally tested against human learning per-              puter screen, which depended on the underlying state of the
formance. Are these RL algorithms actually slower to learn              environment, and an immediate reward. The subject’s goal
than humans? To what extent is this lore biased by the fact             was to maximize their cumulative rewards.
that humans bring structural knowledge from previous expe-                 Each task could be presented to the subject in two differ-
riences to new problems? For example, when entering a new               ent versions. In the with-context version C+, the domain’s
building, a human will probably assume that he cannot walk              observations had meaning in the context of the task. In the
through its walls, whereas RL problems would typically have             context-free version C−, observations had no meaning; the
to relearn this fact for each new location. Humans also tend to         C− version of the problem was meant to simulate what a RL
assume near-deterministic worlds, whereas RL algorithms are             algorithm might “see,” as a computer system cannot attribute
often initialized as believing all possible outcomes are equally        meaning or significance to particular observations.
likely.                                                                    In the first problem, the tigerworld task (Kaelbling,
   In this paper, we focus on the approaches humans take on             Littman, & Cassandra, 1995), players were confronted with
problems where aspects of the environment cannot be fully-              two doors (see figure ?? for an illustration). Behind one door
observed (formally partially observable Markov decision pro-            was a tiger (reward = −100); behind the other was a prize
cesses (POMDPs)). POMDPs offer a more realistic scenario                (reward = +10). At every iteration, players had three options:
                                                                    2703

they could open one of the two doors, or they could “listen”
for more information. Each listen attempt had an 85% chance
of being accurate and an associated reward of −1. In the C+                                   +10
version, the observations were images of a tiger on the left                                  −100
or the right of the image. In the without-context version, the
image of a tiger on the right was replaced with an image of
an apple, and the image of a tiger on the left was replaced by               (a) Illustration of the (b) Sample C+ Ob- (c) Sample C− Ob-
an image of a banana. The text on the actions (“listen,” “open               Gridworld Problem servation                 servation
left,” “open right”) was also replaced by numbers (“1,” “2,”
“3”). Opening either door reset the tiger and the prize to ran-             Figure 2: The gridworld task rewarded players for reaching the top-
                                                                            right corner of a 4x3 grid. The right images show the observation
dom positions.1 Understanding that listening provided useful                for the bottom-right corner.
but noisy information was the key learning challenge in the
tigerworld task.
                                                                            Methods
      value       value                                                     Procedure To test the first hypothesis, each subject played
       =?          =?                                                       every task-version pair (tigerworld, with and without con-
                                                                            text; gridworld, with and without context). Subjects were in-
                                                                            formed they were learning four different tasks. Each subject
                                                                            played 500 iterations in tigerworld and 750 iterations in the
   (a) Illustration of the (b) Sample C+ Ob- (c) Sample C−                  gridworld. The simpler tigerworld problem was always pre-
   Tigerworld Problem servation                   Observation               sented before gridworld. The length of the experiment and
                                                                            the decision to present the simpler problem first were decided
Figure 1: The tigerworld problem consisted of two doors. One door           from an initial pilot test.
had a tiger behind it, the other a prize. Players could open a door
or “listen” for the tiger’s location. The right two images show a              The ordering of the two versions was counterbalanced be-
possible result of the “listen” action in the C+ and C− versions,           tween the subjects: half the subjects received the C+ tasks
respectively.                                                               first; half received the C− tasks first. Subjects playing the C+
                                                                            versions first had slightly better overall performance than sub-
    The second problem, the gridworld task, players had to                  jects playing the C− version first (t(31) = 2.32, p < 0.05). To
navigate from a random starting place on a 4x3 grid (Russell                check if subjects were using learned effects of actions from
& Norvig, 2010) to reach the prize in the top-right cor-                    one task version to the next, the labels associated with the ac-
ner (see figure 2 for the map). Reaching either the prize                   tions in the C− tasks were either ordered identically as the
(reward = 10) or the penalty (reward = -100) square reset                   C+ versions or permuted. For example, if the C+ version
the player to an arbitrary position on the board. Unlike in                 had buttons ‘left,’ ‘right,’ ‘listen,’ then the numbers ‘0,’ ‘1,’
the tigerworld task, the observations in the gridworld task                 ‘2’ could either map to ‘left,’ ‘right,’ ‘listen’ (same order) or
were deterministic—players always saw the walls immedi-                     ‘right,’ ‘listen,’ ‘left’ (permuted order). Subjects were split
ately around them. However, actions had stochastic effects:                 evenly between these two versions; we found that changing
80% of the time the action would execute as expected; 20%                   the action mapping had no significant effect on performance
of the time the player would find themselves moved in a per-                (t(31) = 0.88, p > 0.10).
pendicular direction. Reaching the prize square while navi-                    After signing a consent form, subjects were shown the in-
gating around the penalty square was the key learning chal-                 terfaces and given a chance to familiarise themselves with it.
lenge in the gridworld task. In the contextual version of the               They were also told the following information:
problem, the subjects saw gridcells with walls and arrows as
                                                                            • Each task was unique and unrelated to the other tasks.
observations (figure 2(b)) for normal cells and a happy or sad
emoticon for the two reward cells. Action buttons were la-                  • Actions could have stochastic effects, but there were no
beled with the compass directions; subjects reported no trou-                  adversarial effects.
ble making the association between the compass directions on                • Past (especially recent) observations could be important.
the action buttons and the arrows indicating free directions to             • They could take notes or use a calculator if they wished.
move in the observations. In the C− version, each unique ob-                • There was no time limit.
servation was mapped to a specific fruit. The action buttons                • The trials would be long enough that they should feel free
were also numbered instead of labeled with compass direc-                      to spend time exploring.
tions. Rewards in both tasks were deterministic functions of
the underlying hidden state.                                                After all trials were complete, subjects were interviewed on
                                                                            how they approached the problem. They were encouraged
    1 Opening   a door in the original version of tigerworld results in     to explain any sketches or computations they had made. Fi-
a random observation. In pilot trials, subjects found this version
very hard to learn; therefore, we augmented tigerworld with a third         nally, subjects were asked if they had realised that the tasks
“reset” observation that always followed an open-door action.               were paired (3 of 16 subjects did). Each version took subjects
                                                                        2704

15-20 minutes to complete; the entire set of tasks took most          tual observations, but only one inferred the map when fruit
subjects 60-90 minutes. Subjects were allowed to take breaks          images were substituted for the wall images.
between tasks.
                                                                                                   Performance on Tiger                                               Performance on Gridworld
   To test our second hypothesis, we collected a fresh group of                      0                                                                  0
subjects. Each subject played one version (with/without con-                       −5                                                                  −1
text) of tigerworld for 3000 iterations and one version of grid-
                                                                                   −10                                                                 −2
world for 2000 iterations. The trial lengths were chosen based
on pilots showing that human subjects varied greatly in their             Reward                                                              Reward
                                                                                   −15                                                                 −3
of learning rates and “inspiration” moments. Half the subjects                     −20                                                                 −4
played the C+ tigerworld scenario and the C− gridworld sce-
                                                                                   −25                                                                 −5
nario; the other half played the C− tigerworld scenario and                                                      With Context (C+)
                                                                                                                 Without Context (C−)
                                                                                                                                                                                      With Context (C+)
                                                                                                                                                                                      Without Context (C−)
the C+ gridworld. Subjects were given the same instructions                        −30
                                                                                         0   100       200       300        400         500
                                                                                                                                                       −6
                                                                                                                                                            0   100   200   300      400      500   600   700   800
                                                                                                         Iteration                                                                Iteration
as in the first experiment. These longer trials lasted 90-150
minutes; subjects were encouraged to take breaks whenever             Figure 3: Reward in each phase of the trial. Blocks consisted of 50
they wished to avoid fatigue.                                         iterations for both problems. Shaded regions show the standard error
                                                                      of the mean.
Aparatus The subjects participated in the study by using a
mouse to click buttons displayed on a computer screen. The
display had three elements. A large central pane showed the           Comparison to Reinforcement Learning Algorithms
current observation (updated after each action). Above the            We next compared human performance to three approaches
observation pane was a panel that showed subjects their im-           from RL: RMAX (Brafman & Tennenholtz, 2002), u-
mediate reward after each action (cumulative rewards were             tree (McCallum, 1995), and iPOMDP (Doshi-Velez, 2009).
not shown). Finally, a set of action-selection buttons were           The first, RMAX, builds a model of the world’s dynamics,
located below the observation window.                                 choosing optimistic rewards for parts of the world it has not
   Subjects could not access prior histories of actions, obser-       seen, and then uses the model to make decisions. RMAX
vations, or rewards; however, they were provided pen and pa-          is designed for fully-observable problems, that is, problems
per. Subjects could also use of a calculator (none did).              with no hidden information. To apply RMAX to our domains,
Participants To test the first hypothesis, that context had           we use a history of recent observations as a proxy for the state,
a significant effect in human learning, 16 subjects (13 male,         a technique often used for tackling partially-observable prob-
3 female) were recruited from the University of Cambridge             lems (Breslow, 1996; Lin & Mitchell, 1992).
Engineering Department. To test the second hypothesis,                   Specifying how much past history to consider adds an ad-
eight additional subjects were recruited from the University          ditional parameter to RMAX; the u-tree algorithm tries to dy-
of Cambridge Engineering Department. Finally, three addi-             namically learn the window size: it uses a series of statistical
tional subjects (2 male, 1 female) participated in a pre-trial        tests to increase the number of past observations considered
pilot. Participants were compensated for their participation; a       if it enables the agent to improve its overall rewards. Like
prize was also offered for the highest score.                         RMAX, u-tree builds a model using each of these (now vari-
                                                                      able length) past histories as states and solves the model to
                           Results                                    select actions. Finally, iPOMDP also builds a model of the
                                                                      world first, but it does not assume that the world is fully-
Effect of Context                                                     observable; indeed, it assumes that the number of hidden
We had hypothesized that subjects would perform better in             states could be potentially unbounded. While iPOMDP cor-
C+ versions of each problem. Performance was evaluated                rectly models the true partially-observable nature of the prob-
based on the sum of all immediate rewards received during             lems, it must search over a much larger class of models.
a trial. Subjects performed significantly better with context            We had hypothesized that subjects would perform better
than without, paired t(31) = 2.99, p < 0.005, with a mean             than the RL algorithms when given context and worse when
benefit of 1, 243 points in the final cumulative reward.              not given context. We tested both performance on the last
   The total reward gained over time is shown in Fig. 3. The          tenth of the data as well as compared the performance of
trials are broken into blocks of 50 iterations, and the shaded        the subjects and the algorithms for each block of 50 inter-
regions show the standard error of the mean. The upward               actions during the trial. On the tigerworld problem, the al-
trends in all curves indicates learning occurred during the           gorithms outperform the subjects without context both in the
course of the task. In the tigerworld problem, the C− case            last tenth of the trial, t(1602) = −4.82, p < 0.005 and in each
started with a low initial reward, but by the end, the human          block of the learning process, t(59) = −12.79, p < 0.005.
subjects were performing as well with context as without (al-         More surprisingly, the algorithms also outperformed sub-
though still suboptimally). In contrast, the human subjects on        jects when they had context both in the last tenth of the
the C− version of gridworld never matched the C+ perfor-              trial, t(3003) = −5.76, p < 0.005 and throughout the learning
mance: many subjects inferred the grid when given contex-             process, t(59) = −10.92, p < 0.005. These results directly
                                                                   2705

contradict conventional wisdom that while an RL algorithm                                                                                       are shown in Fig. 5. In the tigerworld problem, the small
might eventually produce a superior solution than a human,                                                                                      window sizes yield similar (suboptimal) performance levels
they generally learn more slowly.                                                                                                               as the human subjects, but much more quickly. The longer
   The left pane of Fig. 4 compares the performance of the                                                                                      window sizes result in slower learning, but they eventually
three RL algorithms to human subjects without context on                                                                                        out-perform the human subjects regardless of whether the
the tigerworld problem. As before, the shaded regions show                                                                                      subjects had context. RMAX’s learning rate is even more
the standard error of the mean, and averages are computed for                                                                                   striking in the gridworld problem (right panel of Fig. 5).
blocks of 50 iterations; the expected optimal performance—                                                                                      The longer window sizes, with a large number of parame-
computed by applying value iteration to each domain—for                                                                                         ters (O(S2 ), where S is the number of states), are very slow
an agent that knew the domain is given by the dashed line                                                                                       to learn, but building a model reduces the need for long win-
(note that the expected optimal performance is the average                                                                                      dows: the small-window learners quickly surpass human per-
performance an optimal agent would gain over many runs;                                                                                         formance. In post-experiment interviews, most human sub-
individual runs can exceed this value). What is striking is                                                                                     jects also showed maps that they had built as they played.
how quickly RMAX and iPOMDP algorithms achieve near-                                                                                            What then distinguished RMAX? We hypothesize the crucial
optimal performance; u-tree, testing variable window lengths,                                                                                   difference was RMAX’s optimistic approach to filling in un-
learns slower but also ultimately bests the human subjects.                                                                                     known parts of the model, which lead it to explore all aspects
   Recall that the key challenge in the tigerworld problem was                                                                                  of the problem. In contrast, humans in post-experiment in-
learning that the observations of where the tiger was located                                                                                   terviews claimed they behaved much more cautiously after
were noisy: repeated measurements were needed to ascer-                                                                                         discovering a −100 penalty.
tain the tiger’s location to a reasonable degree of accuracy.                                                                                                                   Tiger: RMAX Comparison                                                   Gridworld: RMAX Comparison
The gridworld problem tested a different challenge: building                                                                                                            5                                                                        2
a map of a domain where actions sometimes “slipped” or had                                                                                                              0                                                                        0
unexpected results. As seen in the right pane of Fig. 4, the                                                                                                          −5                                                                       −2
difference in performance in the gridworld problem is much
                                                                                                                                                     Blocked Reward                                                           Blocked Reward
less clear. We found no significant difference between the RL                                                                                                         −10                                                                      −4
algorithms and the human subjects: still, it is interesting to                                                                                                        −15                                                                      −6
                                                                                                                                                                                                                                                                                 With Context
note that even when given the context of the walls and corri-                                                                                                         −20                                                                      −8
                                                                                                                                                                                                                                                                                 Without Context
                                                                                                                                                                                                                                                                                 Optimal
                                                                                                                                                                                                                                                                            RMAXdata4
                                                                                                                                                                                                                                                                                 versions:
dors, the human subjects did not outperform the algorithms,                                                                                                                                                                                                                     window = 1
                                                                                                                                                                                                                                                                                 window = 2
                                                                                                                                                                      −25                                                                      −10
which did not have access to this information. The difference                                                                                                                                                                                                                    window = 3
                                                                                                                                                                                                                                                                                 window = 4
in the RMAX algorithm’s performance through the learning                                                                                                              −30
                                                                                                                                                                            0   500   1000     1500      2000   2500   3000
                                                                                                                                                                                                                                               −12
                                                                                                                                                                                                                                                     0       500     1000         1500             2000
                                                                                                                                                                                             Iteration                                                             Iteration
process (again measured as the performance in each block
of 50 interactions) was significantly greater than the human
                                                                                                                                                Figure 5: Comparison with RMAX over several window sizes. The
subjects’ performance, t(39) = −3.81, p < 0.005. Finally, it                                                                                    dip in the human performance curve in gridworld was due to a single
is interesting to note that the iPOMDP algorithm performs the                                                                                   subject’s performance.
most poorly in this domain. The extra complexity of having to
explicitly consider the partial observability in these relatively                                                                                  The previous analysis found that the RL algorithms of-
simple domains results in a much slower learner.                                                                                                ten outperform average human performance, both with and
                                                                                                                                                without context. When compared to the best human subject
                     5
                              Tiger: Human, RL Comparison
                                                                                         1
                                                                                                 Gridworld: Human, RL Comparison                (as measured by cumulative performance on the last tenth of
                     0                                                                   0
                                                                                                                                                the trial), we find that the best subject outperformed the al-
                   −5                                                                   −1                                                      gorithms on gridworld when given context: the best subject
  Blocked Reward
                                                                                                                                                scored 33 times more points in the last tenth of the trial than
                                                                       Blocked Reward
                   −10                                                                  −2
                   −15                                                                  −3                                                      RMAX, the best algorithm. However, the best human did not
                   −20                                       Human
                                                             Optimal
                                                                                        −4                                      Human
                                                                                                                                Optimal
                                                                                                                                                outperform the best algorithm in tigerworld—RMAX scored
                   −25                                       Random
                                                             RMAX
                                                                                        −5                                      Random
                                                                                                                                RMAX
                                                                                                                                                6 times as many points as the best human. The plots of the
                   −30                                       UTREE
                                                             iPOMDP
                                                                                        −6                                      UTREE
                                                                                                                                iPOMDP
                                                                                                                                                best human subject’s performance are shown alongside the
                   −35
                         0   500   1000     1500
                                          Iteration
                                                      2000   2500   3000
                                                                                        −7
                                                                                             0       500       1000
                                                                                                            Iteration
                                                                                                                         1500         2000      best algorithms in Fig. 6. Interestingly, the best human sub-
                                                                                                                                                ject appears to learn a (suboptimal) solution the tigerworld
Figure 4: Reward for each trial block of 50 iterations. Shaded re-                                                                              problem slightly faster than the algorithms, but the gridworld
gions show the standard error of the mean. For RMAX, the window                                                                                 problem takes longer to learn (though the performance is
size was 2.                                                                                                                                     near-optimal in the end).
   Recall that RMAX uses the most recent window of obser-                                                                                       Algorithms Matching Human Behaviour
vations as a proxy for the hidden state. We tested the algo-                                                                                    Finally, we examined which RL methods most closely mod-
rithms with windows ranging from only the most recent ob-                                                                                       elled human behaviour. To evaluate how well these RL pro-
servation to the last four observations. The results for RMAX                                                                                   cedures predicted human subjects’ behaviour, we played each
                                                                                                                                             2706

                             Tiger: Human, RL Comparison                                               Gridworld: Human, RL Comparison
                    5                                                                    2                                                                             solutions instead of seeking better solutions. In contrast, the
                    0
                                                                                         0                                                                             RL algorithms were more persistent; in general they not only
                  −5
                  −10                                                                  −2
                                                                                                                                                                       learned as quickly or quicker than human subjects, but they
 Blocked Reward                                                       Blocked Reward
                  −15
                                                                                       −4
                                                                                                                                                                       also refined their solutions more than human subjects. Thus,
                  −20                                                                                                                                                  we find that contrary to conventional wisdom about these sim-
                                                                                       −6
                  −25
                                                                                                                                                                       ple algorithms—that they learn slowly—these algorithms of-
                  −30                                       Human                      −8                                                       Human
                  −35
                                                            Optimal                                                                             Optimal                ten learn significantly faster than human subjects.
                                                            RMAX                                                                                RMAX
                                                                                       −10
                                                            UTREE                                                                               UTREE
                  −40
                                                            iPOMDP                                                                              iPOMDP
                                                                                                                                                                          The quantitative performance curves matched post-
                  −45                                                                  −12
                        0   500   1000     1500
                                         Iteration
                                                     2000   2500   3000                      0   200    400   600   800   1000
                                                                                                                      Iteration
                                                                                                                                 1200   1400   1600   1800   2000
                                                                                                                                                                       experiment interviews in which the subject (like many oth-
                                                                                                                                                                       ers) produced an accurate map of the gridworld—despite the
Figure 6: Performance of best human subject (with context). In both                                                                                                    transition uncertainty and location ambiguity—but found it
domains, the subject learns a near-optimal solution.                                                                                                                   very difficult to reason about the observation uncertainty in
                                                                                                                                                                       tigerworld. The algorithms treated both of these forms of
                                                                                                                                                                       uncertainty equivalently; thus they learned in proportion to
subject’s history of actions and observations forward to the
                                                                                                                                                                       the overall level of uncertainty. We can conclude that either
RL agent. At each iteration, the RL algorithm updated its in-
                                                                                                                                                                       humans require more experience to learn than supposed, RL
ternal state given the current history of the human subject’s
                                                                                                                                                                       algorithms are faster learners, or both.
actions. Based on this history, the algorithm decided which
action it would select next. Similarity between the algo-                                                                                                                 Our work is consistent with studies showing humans have
rithm’s and the human subject’s behaviour was assessed along                                                                                                           difficulty planning under uncertainty, though none directly
two criteria: (1) whether the agent’s selected action matched                                                                                                          compare human and algorithm performance in multi-state
the subject’s selected action,2 and (2) the algorithm’s regret                                                                                                         partially-observable domains. For example, handling loca-
given the human subject’s action choice. Formally, regret is                                                                                                           tion ambiguity was found to be the primary bottleneck for hu-
the value the agent thought the human subject lost by his or                                                                                                           mans trying to perform spatial navigation tasks (Stankiewicz,
her choice of action (as computed from the algorithm’s inter-                                                                                                          McCabe, & Legge, 2004). Gureckis and Love (2009) found
nal value function). Lower regrets imply a greater similarity                                                                                                          slightly noisy rewards encouraged exploration, but humans
between the algorithms’ and the human subjects’ choices.                                                                                                               are generally poor at handling randomness, even in fully-
   The results in Fig. 7 show that none of the algorithms                                                                                                              observable settings. Finally, Acuña and Schrater (2008)
matched human behaviour very often, regardless of whether                                                                                                              hypothesised that humans may learn slowly on bandit-type
the subjects had context. The algorithms matched the sub-                                                                                                              problems because they consider a wider set of underlying
jects slightly better in the tigerworld domain than the grid-                                                                                                          structures, even when they are told that the problem has a
world; the low-match rates—almost always below 50%—                                                                                                                    particular form. They showed that human learning rates on
suggest that the humans and the algorithms were employing                                                                                                              a 1-state partially-observable problem are slower than an ap-
rather different strategies, even when they had indistinguish-                                                                                                         proach that leverages the structure of the problem (also given
able performance (as in gridworld). As expected, the RMAX                                                                                                              to the human subjects) but similar to an approach that makes
learner had the highest regrets; its optimistic initialization                                                                                                         fewer structural assumptions. Their results are similar to
made it believe that humans often under-explored problems.                                                                                                             the differences we observed between the RMAX algorithm—
The u-tree algorithm had the lowest regrets, in part because it                                                                                                        which learned quickly due to its simple model assumptions—
tended to be less certain about the correct action at any time.                                                                                                        and the iPOMDP or u-tree—which learned more slowly.
   Finally, we note that the stochasticity of the problems (seen                                                                                                          The findings in this work are based on two standard prob-
in the individual problem traces in Fig. 6) resulted in high                                                                                                           lems in RL, with relatively small state spaces. We conjecture
reward variances and that different under-exploring policies                                                                                                           that without context, the advantage of RL methods over hu-
could also result in large reward variations (seen in the high                                                                                                         mans will persist for larger state spaces. For example, given
standard errors in Fig. 4. The analysis in this section shows                                                                                                          no context, a larger gridworld is even more baffling for the
that there are differences in how humans and RL agents ex-                                                                                                             human subject who was already—on average—confused by
plore given these high variances.                                                                                                                                      a 4 × 3 grid. However, in larger more structured state spaces,
                                                                                                                                                                       the human subject’s ability to generalise and make use of con-
                                                            Discussion                                                                                                 text would probably give them significant advantages. For
A significant advantage that RL algorithms have over humans                                                                                                            example, human subjects may infer that “stacking” actions
is that they do not get bored, fatigued, or disheartened. In a                                                                                                         put one object on top of another, while a simple agent may
long series of experiments in which subjects may accrue large                                                                                                          have to learn the result of a “stack” for each pair of objects.
costs before ultimately learning a good strategy, these factors                                                                                                        Similarly, humans may use patterns of grammar to analyse
often caused humans to settle for sub-optimal or reasonable                                                                                                            dialogues, whereas an agent might have to learn each part
   2 The results evaluating action-selection similarity based on a                                                                                                     of a conversation separately. It remains an interesting open
softmax action-selection criterion were nearly identical to action-                                                                                                    question as to how the learning rates of human subjects and
matching and are omitted for brevity.                                                                                                                                  RL agents compare on these more structured and hierarchi-
                                                                                                                                                                    2707

                                             Tiger : Context                                   Tiger : No Context                                     Gridworld : Context                                  Gridworld : No Context
            Proportion Same Action                             Proportion Same Action                                 Proportion Same Action                                Proportion Same Action
                                      1                                                  1                                                       1                                                    1
                                     0.5                                                0.5                                                    0.5                                                   0.5
                                      0                                                  0                                                       0                                                    0
                                           RMAX UTREEiPOMDP                                   RMAX UTREEiPOMDP                                       RMAX UTREEiPOMDP                                      RMAX UTREEiPOMDP
                                             Tiger : Context                                   Tiger : No Context                                     Gridworld : Context                                  Gridworld : No Context
                                     15                                                 15                                                      15                                                   15
                  Perceived Regret                                   Perceived Regret                                       Perceived Regret                                      Perceived Regret
                                     10                                                 10                                                      10                                                   10
                                      5                                                  5                                                       5                                                    5
                                      0                                                  0                                                       0                                                    0
                                           RMAX UTREEiPOMDP                                   RMAX UTREEiPOMDP                                       RMAX UTREEiPOMDP                                      RMAX UTREEiPOMDP
Figure 7: Proportion of same actions and perceived regret of the yoked learners. A higher proportion of same actions indicates a greater
similarity between the human’s and agent’s decisions; likewise lower regrets indicate that the agent valued actions similarly to the human
subjects. Means are shown with 95% confidence intervals.
cal learning domains. The importance of context in human                                                                                         ing of object categories. Pattern Analysis and Machine In-
learning also suggests that for work trying to build more data-                                                                                  telligence, IEEE Transactions on, 28(4), 594–611.
efficient artificial agents (Fei-Fei, Fergus, & Perona, 2006),                                                                                 Gureckis, T. M., & Love, B. C. (2009). Learning in noise:
learning and leveraging contextual information may be key                                                                                        Dynamic decision-making in a variable environment. Jour-
factor to achieving better learning performance. An exciting                                                                                     nal of Mathematical Psychology, 53(3), 180–193.
avenue for future work would be to better understand how                                                                                       Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1995).
humans leverage context when learning a task, rather than fo-                                                                                    Planning and acting in partially observable stochastic do-
cusing simply on their rates of learning.                                                                                                        mains. Artificial Intelligence, 101, 99–134.
                                                                                                                                               Kakade, S., & Dayan, P. (2002). Dopamine: generalization
                                            Acknowledgments                                                                                      and bonuses. Neural Networks, 15, 549-559.
Finale Doshi-Velez was funded by the Marshall Scholarship.                                                                                     Lin, L.-J., & Mitchell, T. M. (1992). Memory approaches to
Additional funds were provided by the Computational and                                                                                          reinforcement learning in non-markovian domains (teknisk
Biological Learning Laboratory at Cambridge University.                                                                                          rapport). Carnegie Mellon University.
                                                                                                                                               McCallum, A. (1995). Instance-based utile distinctions for
                                                References                                                                                       reinforcement learning with hidden state. I Icml.
                                                                                                                                               Morimoto, J., & Doya, K. (2005). Robust reinforcement
Acuña, D., & Schrater, P. R. (2008). Structure learning in
                                                                                                                                                 learning. Neural Computation, 17(2), 335-359.
  human sequential decision-making. I NIPS.
                                                                                                                                               Peters, J., Bagnell, D., & Schaal, S. (2006). Towards a new
Brafman, R. I., & Tennenholtz, M. (2002). R-max – a general
                                                                                                                                                 reinforcement learning?
  polynomial time algorithm for near-optimal reinforcement
                                                                                                                                               Russell, S., & Norvig, P. (2010). Artificial intelligence: A
  learning. Journal of Machine Learning Research, 3.
                                                                                                                                                 modern approach. Prentice Hall.
Breslow, L. (1996). Greedy utile suffix memory for reinforce-
                                                                                                                                               Samejima, K., & Doya, K. (2007, April). Multiple Represen-
  ment learning with perceptually-aliased states (teknisk
                                                                                                                                                 tations of Belief States and Action Values in Corticobasal
  rapport). Navy Research Center Laboratory.
                                                                                                                                                 Ganglia Loops. New York Academy Sciences Annals, 1104.
Daw, N. D., Courville, A. C., & Tourtezky, D. S. (2006). Rep-                                                                                  Singh, S.       (2009).      RL is slow.      Available from
  resentation and timing in theories of the dopamine system.                                                                                     http://umichrl.pbworks.com/RL-is-slow
  Neural Comput., 18(7), 1637–1677.                                                                                                            Stankiewicz, B. J., McCabe, M., & Legge, G. (2004). Study-
Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B., &                                                                                          ing human spatial navigation processes using pomdps. I
  Dolan, R. J. (2006). Cortical substrates for exploratory                                                                                       AAAI workshop on learning and planning in markov pro-
  decisions in humans. Nature, 441(7095), 876–879.                                                                                               cesses: Advances and challenges (s. 97-102).
Dayan, P., & Daw, N. D. (2008). Decision theory, reinforce-                                                                                    Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning:
  ment learning, and the brain. Cognitive, Affective, and Be-                                                                                    An introduction. MIT Press.
  havioral Neuroscience.                                                                                                                       Yoshida, W., & Ishii, S. (2003). A model-based reinforce-
Doshi-Velez, F. (2009). The infinite partially observable                                                                                        ment learning: a computational model and an fmri study. I
  Markov decision process. I Nips.                                                                                                               ESANN (s. 313-318).
Fei-Fei, L., Fergus, R., & Perona, P. (2006). One-shot learn-
                                                                                                                    2708

