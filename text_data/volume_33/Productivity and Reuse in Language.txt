UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Productivity and Reuse in Language
Permalink
https://escholarship.org/uc/item/4t2312gv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
O'Donnell, Timothy
Snedeker, Jesse
Tenenbaum, Joshua
et al.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                Productivity and Reuse in Language
            Timothy J. O’Donnell (timo@wjh.harvard.edu)             Jesse Snedeker (snedeker@wjh.harvard.edu)
            Harvard University Department of Psychology         Harvard University Department of Psychology
                 Joshua B. Tenenbaum (jbt@mit.edu)                Noah D. Goodman (ngoodman@stanford.edu)
                  MIT, Brain and Cognitive Science              Stanford University Department of Psychology
                           Abstract                              esizes reusability at the cost of generalization, and vice
   We present a Bayesian model of the mirror image               versa. The model advocated in this paper, Fragment
   problems of linguistic productivity and reuse. The            Grammars (FG), can be seen as optimizing this tradeoff
   model, known as Fragment Grammar, is evaluated                for a given dataset. What patterns in the data signal
   against several morphological datasets; its performance
   is compared to competing theoretical accounts including       that some structure is likely to be reused again in the
   full–parsing, full–listing, and exemplar–based models.        future? What patterns suggest that some class of struc-
   The model is able to learn the correct patterns of            tures will exhibit novelty and variability and therefore
   productivity and reuse for two very different systems:
   the English past tense which is characterized by a            should be computed on the fly?
   sharp dichotomy in productivity between regular and              We examine two morphological systems: the English
   irregular forms and English derivational morphol-
   ogy which is characterized by a graded cline from             past tense and English derivational morphology. Mor-
   very productive (-ness) to very unproductive (-th).           phology provides a domain in which questions of com-
   Keywords:Productivity;Reuse;Storage;Computation;              putation and storage have been intensely studied for
   Bayesian Model;Past Tense;Derivational Morphology
                                                                 decades (see, O’Donnell, 2011, for a review). The two
                       Introduction                              datasets examined here are of special interest because of
                                                                 their very different patterns of productivity and reuse.
Perhaps the most celebrated aspect of human lan-
                                                                 Derivational morphology is characterized by a broad
guage is its creativity. Language allows us to produce,
                                                                 cline of affixes of differing levels of productivity. This
comprehend—and perhaps even think—an unbounded
                                                                 sort of gradience is exactly the kind of structure we
number of thoughts. Creativity in language is made
                                                                 might expect probabilistic models to excel at capturing
possible by computation. Novel expressions can be pro-
                                                                 (Hay & Baayen, 2005). In contrast, the English past
duced and understood because the linguistic system pro-
                                                                 tense’s regular +ed rule (e.g., walk/walked) is highly pro-
vides productive computational processes for generating
                                                                 ductive, while the various irregular form classes (e.g.,
linguistic structures. Productive processes such as syn-
                                                                 sing/sang, sleep/slept, etc.) generalize only very rarely
tactic and morphological rules operate via the combina-
                                                                 (e.g., Prasada & Pinker, 1993). Thus, the English past
tion of large numbers of stored, reusable units such as
                                                                 tense provides an important counterpoint to the gradient
words, morphemes, and idioms. However, not all gen-
                                                                 structure of derivational morphology. A model of pro-
eralizations that are consistent with the input data are
                                                                 ductivity and reuse must be able to handle both kinds of
productive, nor is storage and reuse limited to a single
                                                                 linguistic systems: those with widely varying mixtures of
kind of unit such as words, but rather both productivity
                                                                 productive computation and reuse, and those with sharp,
and reuse cut across levels of linguistic structure (Di Sci-
                                                                 nearly deterministic, dichotomies between the two.
ullo & Williams, 1987). Therefore, a fundamental prob-
lem for linguistic and psycholinguistic theory, as well as
                                                                        Informal Overview of the Models
for the language learner, is understanding which patterns
in linguistic data should give rise to productive general-       Over the years researchers have proposed many theories
izations and which should give rise to stored, reusable          of productivity and storage (see O’Donnell, 2011, for a
structures.                                                      review). In addition to our proposal that productivity
   We present a model of productivity and reuse which            and reuse should be considered a probabilistic inference,
addresses the problem as an inference in a Bayesian              we also consider three other approaches. Each of these
framework. A productive computation is one which can             models has been chosen to formalize a specific histori-
give rise to novel forms. In a probabilistic setting, if a       cal proposal about productivity and reuse, while keeping
system hypothesizes that some (sub)computation is pro-           other dimensions maximally similar to one another.
ductive, it must reserve probability for hitherto unseen            All of the models considered here start from a un-
structures. On the other hand, if a probabilistic sys-           derlying system which defines the space of possible
tem hypothesizes that some sequence of computations              computations—here we assume that this starting sys-
will be frequently reused together, it must reserve prob-        tem provides the ability to generate a set of tree–shaped
ability for that particular sequence as a whole. Since           computations like those shown in Figure 1. Each of
there is only a finite budget of probability, this necessar-     the approaches has been implemented as a probabilis-
ily leads to a tradeoff: A probabilistic system hypoth-          tic model which is defined over this same space of trees;
                                                             1613

the models differ, however, in the strategies they use                                       N                 N                 N               N
to determine which subcomputations (subtrees) can be                                    Adj     -ity     Adj     -ness     Adj      -ity    Adj     -ity
stored and reused, and which parts of the system can
                                                                                     V    -able        V   -able         V    -able       V   -able
productively generate novel forms.
                                                                                   agree             agree             count            agree
Full–parsing: In this model, all structures are the re-                             Figure 3: Exemplar–based Inference: This figure
sult of computation using minimal–sized units. No larger                            shows exemplar–based inference. Here every possible
items are stored in memory (Figure 1). In such a set-                               subtree consistent with the data is stored. Note that
                                                                                    this leads to many overlapping analyses for each item.
ting, each primitive is highly reusable. However, any
computation will involve choosing many small, abstract
primitives. Such an approach to reuse will be most ef-                           Productivity as an Inference: The model advo-
fective when there is large amount of combinatoriality,                          cated in this paper, Fragment Grammars (FG), treats
variability, and novelty in the data.                                            as an inference the problem of which subcomputations
                                                                                 are productive and which should be stored for later
             N                   N                 N                 N           reuse.1 Like the full–parsing approach, FG is able to
                                                                                 store abstract structures. Like the full–listing approach,
       Adj       -ity     Adj      -ness     Adj      -ity     Adj      -ity
                                                                                 it can store specific structures, and furthermore, like the
    V     -able         V    -able         V    -able        V    -able          exemplar–based approach, it can store all intermediate
  agree               agree              count             agree                 structures. However, unlike the previous approaches, the
                                                                                 particular solutions it finds to productivity and reuse
    Figure 1: Full–parsing: In this model, small, reusable                       are determined by the data itself. For each datapoint
    subcomputations can be shared across many forms, as
    shown by the highlighting in the same color. However,                        to which it is exposed, the model hypothesizes whether
    generating each form requires many random choices.                           it is optimal to account for the structure using reusable
                                                                                 stored items, productive computation, or some mixture
                                                                                 of both (Figure 4).
Full–listing: In this model, the first time a structure
is built, it is stored in its entirety. Thus, this system can                                N                 N                 N               N
account for productive generalization, but is nevertheless                              Adj     -ity     Adj     -ness     Adj      -ity    Adj     -ity
very conservative—preferring to reuse previously built                               V    -able        V   -able                          V   -able
                                                                                                                         V    -able
structures whenever possible (Figure 2). Under such a
storage strategy, each stored item is extremely specific,                          agree             agree             count            agree
and, therefore, can only be reused in limited contexts.                             Figure 4: Productivity and Reuse as an Infer-
Such an approach to reuse will be most effective when                               ence: This figure shows the consequences of inferring
the language consists of a small number of specific, but                            the set of subcomputations that best account for the
                                                                                    data. In this example, more sharing is allowed on aver-
frequently reused, forms.                                                           age than full–parsing with less computation on average
                                                                                    than full–listing.
            N                   N                  N                 N
      Adj       -ity      Adj      -ness     Adj      -ity     Adj      -ity                 Formalization of the Models
   V     -able         V    -able                           V    -able
                                           V    -able                            In this section, we formalize each of the approaches to
 agree               agree                                agree                  productivity and reuse introduced above. More detailed
                                         count
                                                                                 treatments can be found in O’Donnell et al. (2009) and
    Figure 2: Full–listing: This figure shows the conse-                         O’Donnell (2011).
    quences of the full–listing model. Stored subcomputa-
    tions are very specific. Their substructures cannot be
    shared with other forms; however, they can be reused in                      Full–Parsing: We formalize the full–parsing model
    their entirety with high probability.
                                                                                 using Multinomial–Dirichlet Probabilistic Context–Free
                                                                                 Grammars (MDPCFG) (Johnson et al., 2007b). An MDPCFG
                                                                                 is a 5-tuple, G = (N, T, P, S, Π), where N is the set of
Exemplar–based Inference: This model stores all                                  nonterminal symbols, T is the set of terminal symbols, R
structures which are consistent with the data, both small                        is the set of production rules of the form A −→ γ where
and abstract, and large and specific (and all in between).                       A ∈ N and γ ∈ (N ∪ T )∗ , S ∈ N is the distinguished
This model differs from Fragment Grammars in that it
                                                                                      1
does not commit to a single analysis of each data point,                                Note that because all of these models are probabilistic,
                                                                                 they are all inferential in a certain sense. However, only the
but rather hedges across many different levels of abstrac-                       Fragment Grammar model does inference both over the set
tion.                                                                            of stored subcomputations and points of productivity.
                                                                             1614

start symbol, and Π is a set of vectors of pseudocounts                          be defined as shown below. Note that since the recursion
for the production rules. Define `(t) to be the function                         GA always returns a full tree down to terminal symbols,
which returns the label at the root of tree t. By con-                           Adaptor Grammars can only store and reuse complete
vention we will label the k immediate children of a top                          tree fragments as shown in Figure 2.
node of t as t̂i , ..., t̂k . The distribution over trees defined
by a MDPCFG can be expressed by the simple recurrence                                          (                           k
below.
                                                                                                         X                Y
                                                                                                                       θr     mem{G`(t̂ ) }(t̂i )`(t) = A ∈ N
                                                                                     GA (t) =    r:A→`(t̂i )...`(t̂k )    i=1
                                                                                                                                          i
                                                                                                 1                                                `(t) = A ∈ T
                  (                            k
                              X               Y
                                           θr     G`(t̂ ) (t̂i )`(t) = A ∈ N
       GA (t) =      r:A→`(t̂i )...`(t̂k )    i=1
                                                       i
                                                                                                       θ~A ∼ dirichlet(π~A )
                     1                                          `(t) = A ∈ T
                                                                                                 mem{GA } ∼ pyp(aA , bA , GA )
                       θ~A ∼ dirichlet(π~A )
   These equations state that the probability of a tree t                        Productivity as an Inference: Adaptor grammars
given by an MDPCFG G is just the product of the proba-                           allow the reuse of complete tree fragments. However, a
bility of the rules used to build that tree from depth–one                       complete tree–fragment cannot be used for further pro-
subtrees. Note that this corresponds to Figure 1 where                           ductive computation; it does not allow for novelty or
each form is composed of minimal fragments of structure.                         variability. To allow the reuse of productive structures,
This model puts a Dirichlet prior on the probabilities of                        Fragment Grammars generalize Adaptor Grammars via
fragments. We set all Dirichlet pseudocounts (π’s) to 1.                         the technique of stochastically lazy evaluation. Stochasti-
                                                                                 cally lazy evaluation allows the system to return delayed
Full–Listing: To formalize the full–listing model we                             or partially computed items which can remain unspeci-
choose Adaptor Grammars (AG) (Johnson et al., 2007a).                            fied, allowing productivity and novelty. These structures
Adaptor Grammars add reuse of entire subcomputations                             can then be reused via memoization, allowing the system
to the MDPCFG formalism. This is achieved via stochas-                           to infer which structures will exhibit productivity and
tic memoization. Memoization refers to the widely–used                           future novelty.
technique of storing and reusing the results of function                            Define the function prefix to enumerate all tree pre-
application. Stochastic memoization generalizes this                             fixes of a given tree—all fully connected subtrees of the
idea by probabilistically mixing the reuse of previously                         given tree which include the root node. We will write the
                                                                                                                 0
computed results with new calls to the function.                                 n leaves of a tree t as ti , ..., t0n . The probability of a tree
   Following Johnson et al. (2007a), we use a distribu-                          t under a fragment grammar is defined by the following
tion for stochastic memoization known as the Pitman-                             recursive probability mass functions.
Yor Process (PYP). Let mem{f } be a PYP memoized
version of some function f . The behavior of a PYP mem-                                        (
                                                                                                    X                         Yn             0
oized function can be described as follows. The first time                          GA (t) =
                                                                                                              mem{LA }(s)         G    0 (s )
                                                                                                                                   `(s ) i
                                                                                                                                       i
                                                                                                                                                  `(t) = A ∈ N
                                                                                                 s∈prefix(t)                  i=1
we invoke mem{f } a new value will be computed using                                             1                                                `(t) = A ∈ T
f . On subsequent invocations, we choose an old value i
                          −a
with probability nNi +b       , where N is the number of values                                    X                  Y k h                                    i
sampled so far, ni is the number of times that value i                            LA (t) =                        θr         νri G`(t̂i ) (t̂i ) + (1 − νri )1
has been used in the past, and 0 ≥ a ≥ 1 and b > −a                                         r:A→`(t̂i )...`(t̂k )     i=1
are parameters of the model. Alternatively, we sam-
                                                                                                  νri ∼ beta(ψcontinue , ψdelay )
ple a new value with probability aK+b              N +b , where K is the
number of times a new value was sampled in the past                                                    θ~A ∼ dirichlet(π~A )
from the underlying function. Notice that this process
induces a rich-get-richer scheme for sampling from the                                           mem{LA } ∼ pyp(aA , bA , LA )
memoizer. The more a particular value has been reused,
                                                                                    The stochastically lazy recurrence, LA , generates par-
the more likely it is to be reused in the future. However,
                                                                                 tially evaluated tree fragments, stopping the recur-
this rich–get–richer dynamic is tempered by a compet-
                                                                                 sion with probability νri at each internal node of a
ing bias which favors new values when many new values
                                                                                 tree. These partially evaluated tree fragments are then
have been sampled in the past.
                                                                                 stochastically memoized. Thus, the system is able to
   Application of stochastic memoization to the ba-
                                                                                 learn arbitrary mixtures of reuse and productive compu-
sic MDPCFG recurrence leads to the Adaptor Grammars
                                                                                 tation like those shown in Figure 4.
model—which was the first to use this technique (John-
son et al., 2007a).1 The Adaptor Grammars model can                              “stochastic memoization” in describing their model, this no-
                                                                                 tion has its origins in that work, and was first named as such
    1
      While Johnson et al. (2007a) do not use the term                           and developed in N. D. Goodman et al. (2008).
                                                                             1615

Exemplar–Based                  Inference: We               formalize    This model differs from the present one in that Taat-
exemplar–based models using two different vari-                          gen & Anderson (2002) optimize an objective function
ants of the Data–Oriented Parsing (DOP) formalism                        involving processing costs, in addition to prediction ac-
for tree–substitution grammar estimation (Bod et al.,                    curacy (which is the only quantity optimized by Frag-
2003). All DOP models treat the probability of a tree                    ment Grammars). They achieve excellent performance
as a sum over all possible subtrees.                                     on the past–tense domain, and we take this as converg-
                (                      n
                                                                         ing evidence in favor of an optimization perspective in
                     X                Y           0
      GA (t) =
                              prob(s)     G   0 (s )
                                           `(s ) i
                                                     `(t) = A ∈ N        this domain.
                  s∈prefix(t)         i=1     i
                  1                                  `(t) = A ∈ T
There are many different techniques which fall under the                 Simulations and Inference: For the full–parsing,
DOP umbrella. They differ primarily in how they esti-                    full–listing, and exemplar–based models, it was possi-
mate the expression PROB in the recurrence above—                        ble to directly compute the maximum a posteriori gram-
how they assign probability mass to subtrees. We will                    mar from the training corpus. Fragment Grammar re-
use two variants of DOP. The first, known as DOP1, sets                  sults were computed via selective model averaging over
the probability of a subtree proportional to its token                   a large number of runs of a Markov chain Monte Carlo
frequency in the training corpus. Let FC (s) be the fre-                 inference algorithm similar to that found in Johnson et
quency of subtree s in corpus C. Then DOP1 uses the                      al. (2007a).1
following estimator: probDOP1 (s) ∝ FC (s).
   A well–known problem with DOP1 is that, because it                                   The English Past Tense
counts all subtrees equally, it tends to overweight train-               As discussed in the introduction, the main interest of
ing data nodes which appear higher and in larger trees                   the English past tense system for present purposes is
(e.g., J. Goodman, 2003). For this reason, we also ex-                   the wide difference in productivity between the regu-
plore a second DOP estimator. This estimator assigns                     lar rule and the irregular inflectional classes. The En-
equal weight to each training data node (Bod, 2003;                      glish regular, +ed, past tense rule (e.g., walk/walked) is
J. Goodman, 2003). We will call this the Equal Node                      rampantly productive. A large number of studies have
DOP (ENDOP).                                                             shown generalization of the regular rule to novel stems
   There are many other variants of DOP. However, in                     in both production and rating tasks (e.g., Prasada &
its classical form DOP is committed to the storage of all                Pinker, 1993). The regular rule can be applied to forms
subtrees. It is this idea, in particular, which we wished to             which are phonotactically odd for English, to stems de-
explore with our exemplar–based models, and this was                     rived from other morphological processes, and in a num-
our main motivation in choosing the DOP1 and ENDOP                       ber of other rare and marginal cases (e.g., He out-Bached
estimators.                                                              Bach. Kim et al., 1991). On the other hand, while a
                                                                         number of studies have shown that irregular classes can
Other Models: There are several models in the liter-                     be generalized (e.g., bring/brang), this generalization is
ature which can be seen as explicitly adopting the idea                  very rare and much more dependent on the phonological
that productivity and reuse should be treated as an (op-                 structure of the stem (e.g., Prasada & Pinker, 1993).
timal) inference, like the present model. Zuidema (2007)                    The simulations were performed on data extracted
introduced Parsimonious Data–Oriented Parsing, a ver-                    from the annotated version of the SwitchBoard corpus
sion of DOP which explicitly eschews the all–subtree ap-                 included in the Penn TreeBank (Godfrey et al., 1992;
proach in favor of finding a set of subtrees which best                  Marcus et al., 1999). All verbs excluding forms of be,
explains the data. Two other models, developed simul-                    have and do were extracted from the corpus, lemma-
taneously with the current framework (Cohn et al., 2009;                 tized and paired with the appropriate inflection for the
Post & Gildea, 2009), make use of similar Bayesian non–                  stem.2 The model was trained on the full English verbal
parametrics to define generative models over sets of sub-                paradigm (i.e., all tenses). Figure 5 shows the simple
trees. The models of (Cohn et al., 2009) and Post &                      input representation used for all models. Note that this
Gildea (2009) differ from Fragment Grammars in that                      representation merely pairs stems with their correct in-
they do not define the distribution over stored struc-                   flectional information, it does not explicitly encode any
tures recursively—a new stored item cannot be built out                  phonological or semantic selectional restrictions. Thus,
of other stored items. The three models just discussed
                                                                             1
have been evaluated primarily on syntactic datasets, and                       Details of the inference engine can be found in O’Donnell
                                                                         (2011); O’Donnell et al. (2009).
primarily in a natural language processing, rather than                      2
                                                                               These verbs were excluded to reduce the size of the train-
psychological setting.                                                   ing corpus and, therefore, improve inference run time. There
   Taatgen & Anderson (2002) present a psychologically–                  is no theoretical reason that exclusion should have an effect
                                                                         on results for other verbs, and a large pilot study where they
motivated model of the past tense which also adopts                      were present achieved similar results to the present simula-
an optimization perspective (in the ACT–R framework).                    tions.
                                                                     1616

any success that any of the models has in learning con-                                                           very unproductive (e.g., -th). 2. Ordering: Only a small
tingencies between stems and inflections is the result of                                                         fraction of the suffix combinations which are possible in
distributional information in the input.                                                                          principle are attested in actual words. One theory which
                                                                                                                  accounts for this fact is complexity-based ordering (CBO;
                                        V                                   V
                                                                                                                  Hay, 2002; Plag & Baayen, 2009). CBO provides a di-
                     Stem                     Inflection
                                                                 Stem            Inflection                       rect link between productivity and ordering by proposing
                                                                 SING                                             that on average more productive suffixes should appear
                         GO                                               /I/ → /æ/         PAST
                                   WENT-SUPPLETION       PAST                                                     outside of less productive suffixes.
         Figure 5: Example Trees for Past Tense: Exam-                                                               The input data for our derivational morphology sim-
         ples of trees used as inputs to the past tense simulations.                                              ulations was derived from the CELEX database which
         These trees represent the past tense forms went and sang                                                 contains a large sample of English words derived from
         respectively. Note that past tense inflectional classes are
         identified using their phonological structure.                                                           dictionaries and newswire (Baayen et al., 1993). Be-
                                                                                                                  cause they undersample low–frequency words, dictionar-
   Figure 6 shows the log odds that a past tense or past                                                          ies tend to underrepresent high–productivity affixes (Ev-
participle form sampled at random from the posterior                                                              ert & Lüdeling, 2001). For this reason, the basic set
generative model will be correctly inflected. These scores                                                        of morphological parses provided by CELEX was sup-
are broken down into regular (walk/walked) and irregular                                                          plemented by applying a heuristic parsing algorithm to
(sing/sang) forms that were in the training sample and                                                            the remaining unparsed forms in the database. CELEX
a set of novel (wug/wugged) test cases.1                                                                          parses do not segment cases where one of the segmented
   Although the full–listing model, AG, is able to per-                                                           parts is not a word, as is the case, for example, with
form well on attested forms, its generalization perfor-                                                           bound stems. To expose this additional structure and
mance is more limited compared to FG. By contrast,                                                                correct errors due to automatic parsing, we selected ap-
the DOP1 exemplar–based model performs well on the                                                                proximately 10,000 of the combined set of forms from
regular forms and wug–generalization cases but fails to                                                           CELEX and automatic parsing and corrected them by
learn the irregular forms. Fragment Grammars provide                                                              hand. The resulting data set contained 338 suffixes, over
the best simultaneous performance across the three test                                                           25,000 word types, and over 7.2 million word tokens.
sets—performing well on novel items as well as correctly                                                          The input to the simulation consisted of trees like those
learning attested regulars and irregulars.                                                                        shown in Figure 7.
                                 Irregular         Regular          Novel                                                                       N                              N
                    8     �                    �                �
                                                                                                                                       Adj             -ity             V            -ion
                    6
                                                                                                                                V            -able               V          -ate
Log Odds Correct
                                                                                                                               agree                           affirm
                    4
                                                                                                                      Figure 7: Example Trees for Derivational Mor-
                    2                                                            MD    Full-Parsing (MDPCFG)          phology: This figure shows examples of the trees used
                                                                                  AG      Full-Listing (AG)
                                                                                                                      as inputs to the derivational morphology simulations.
                    0
                                                                                  D1     Exemplar (DOP1)
                              MDAG D1 ED FG    MD AG D1 ED FG   MD AG D1 ED FG
                                                                                  ED    Exemplar (ENDOP)
                                                                                                                     We first consider the productivity of suffixes as in-
                    −2
                                                                                  FG   Inference-Based (FG)
                                                                                                                  ferred by the various models. There is no gold standard
                                                                                                                  measure of productivity against which the models can
                    −4
                                                                                                                  be evaluated. However, two widely used empirical pro-
         Figure 6: Performance of Models on Past Tense                                                            ductivity statistics are Baayen’s corpus–based measures:
         Dataset: The log odds that a form randomly sampled                                                       P and P ∗ (e.g., Baayen, 1992). The former can be un-
         from each trained model will be inflected correctly for
         the three test sets.                                                                                     derstood as an estimate of the probability that a par-
                                                                                                                  ticular suffix will be used to produce a novel form (i.e.,
                                                                                                                  P [NOVEL|SUFFIX]). The latter can be understood as
                    English Derivational Morphology                                                               an estimate of the probability that a novel form will use
Having established that the inference–based model is                                                              a particular suffix (i.e., P [SUFFIX|NOVEL]).
able to account for the sharp differences in productiv-
ity in the English past tense system, we turn to the                                                                  Model       FG                 MDPCFG        AG              DOP1     ENDOP
richer and more gradient domain of derivational mor-                                                                  P         0.907                -0.0003      0.692            0.346    0.143
phology. We focus here on two aspects of the system.                                                                  P∗        0.662                 0.480       0.568            0.402    0.500
1. Gradient Productivity: Derivational suffixes reside on                                                             Table 1: Correlation with Productivity Mea-
a productivity cline from very productive (e.g., -ness) to                                                            sures: The correlation between quantities computed
                                                                                                                      from the trained models and empirical estimates of
               1
                   Regular inflection was considered correct for wug–tests.                                           Baayen’s P and P ∗ given in Hay & Baayen (2002).
                                                                                                               1617

   Table 1 shows the Pearson correlation be-                      to specialize for commonly reencountered situations.
tween the (log) quantities P [NOVEL|SUFFIX] and
P [SUFFIX|NOVEL] computed from the various (poste-                                     Acknowledgments
rior) models and (log) empirical estimates of P and P ∗           We would like to thank Marjorie Freedman, Manizeh
given in Hay & Baayen (2002). Fragment Grammars                   Khan, and Joshua Hartshorne for detailed feedback on
provide the best fit to these quantities.                         this paper.
   Complexity–based ordering predicts a direct link be-
tween productivity and suffix ordering—more productive
                                                                                             References
                                                                  Baayen, R. H. (1992). Quantitative aspects of morphological produc-
suffixes should tend to appear outside of less produc-               tivity. In G. Booij & J. van Marle (Eds.), Yearbook of morphology
                                                                     1991 (pp. 109–149). Dordrecht, The Netherlands: Kluwer Academic
tive suffixes. Plag & Baayen (2009) provide an empiri-               Publishers.
cal measure of suffix ordering, based on graph theoretic          Baayen, R. H., Piepenbrock, R., & Rijn, H. van. (1993). The CELEX
                                                                     lexical database. Linguistic Data Consortium, University of Penn-
tools, which gives an estimate of the mean rank of a suf-            sylvania.
                                                                  Bod, R. (2003). An efficient implementation of a new DOP model.
fix. The mean rank can be understood as a measure                    In Proceedings of the 10th conference of the European chapter of
of how easily a particular suffix appears after other suf-           the association for computational linguistics (Vol. 1, pp. 19–26).
                                                                     Budapest, Hungary.
fixes in complex words. To generate predictions from              Bod, R., Scha, R., & Sima’an, K. (Eds.). (2003). Data–oriented pars-
                                                                     ing. Stanford, CA: CSLI.
the model with regard to suffix ordering, we considered           Cohn, T., Goldwater, S., & Blunson, P. (2009). Inducing compact but
the subset of forms with exactly two suffixes (i.e., stem            accurate tree–substitution grammars. In Proceedings of the North
                                                                     American conference on computational linguistics.
-suffix -suffix) and computed the marginal probability            Di Sciullo, A. M., & Williams, E. (1987). On the definition of word.
                                                                     Cambridge, MA: MIT Press.
that each suffix occurred first or second in such forms.          Evert, S., & Lüdeling, A. (2001). Measuring morphological produc-
Table 2 gives the Spearman rank correlation between the              tivity: Is automatic preprocessing sufficient? In Proceedings of the
                                                                     corpus linguistics 2001 conference.
(log) ratio of the probability of appearing second to the         Frauenfelder, U. H., & Schreuder, R. (1992). Constraining psycholin-
                                                                     guistic models of morphological processing and representation: The
probability of appearing first with the mean rank statis-            role of productivity. In Yearbook of morphology 1991 (pp. 165–
tic reported in Plag & Baayen (2009). As was the case                183). Dordrecht, The Netherlands: Springer.
with measures of productivity, Fragment Grammars are              Godfrey, J., Holliman, E., & McDaniel, J. (1992). Switchboard: Tele-
                                                                     phone speech corpus for research and development. IEEE ICASSP,
best able to account for suffix ordering restrictions.               517-520.
                                                                  Goodman, J. (2003). Efficient parsing of DOP with PCFG–reductions.
                                                                     In Data-oriented parsing. Stanford, CA: CSLI Publications.
   Model               FG       MDPCFG      AG    DOP1     ENDOP  Goodman, N. D., Mansinghka, V. K., Roy, D., Bonawitz, K., & Tenen-
                                                                     baum, J. B. (2008). Church: A language for generative models.
  Mean Rank          0.568       0.275    0.424   0.452    0.431     In Uncertainty in artificial intelligence. Helsinki, Finland: AUAI
                                                                     Press.
                                                                  Hay, J. (2002, September). From speech perception to morphology:
   Table 2: Correlation of Suffix Ordering Probabil-                 Affix ordering revisited. Language, 78 (3), 527–555.
   ities and Ranks: The rank correlation between the log          Hay, J., & Baayen, R. H. (2002). Parsing and productivity. In Year-
   odds that a suffix will appear second and the mean rank           book of morphology 2001 (Vol. 35, pp. 203–236). Dordrecht, The
   statistic for the suffix given in Plag & Baayen (2009).           Netherlands: Springer.
                                                                  Hay, J., & Baayen, R. H. (2005). Shifting paradigms: Gradient struc-
                                                                     ture in morphology. Trends in Cognitive Sciences, 9 (7), 342–348.
                                                                  Johnson, M., Griffiths, T. L., & Goldwater, S. (2007a). Adaptor
                         Conclusion                                  Grammars: A framework for specifying compositional nonparamet-
                                                                     ric Bayesian models. In Advances in neural information processing
We have presented a model where productivity and reuse               systems 19. Cambridge, MA: MIT Press.
                                                                  Johnson, M., Griffiths, T. L., & Goldwater, S. (2007b). Bayesian
are viewed as inferences in a Bayesian framework and                 inference for PCFGs via Markov chain Monte Carlo. In Proceedings
                                                                     of the North American conference on computational linguistics.
have systematically compared this model with four oth-               Rochester, New York.
ers which represent formalizations of theoretical pro-            Kim, J. J., Pinker, S., Prince, A., & Prasada, S. (1991). Why no mere
                                                                     mortal has ever flown out to center field. Cognitive Science, 15 ,
posals from the literature. The inference–based model,               173–218.
                                                                  Marcus, M. P., Santorini, B., Marcinkiewicz, M. A., & Taylor, A.
Fragment Grammars, is best able to capture the pat-                  (1999). Treebank–3 (Tech. Rep.). Philadelphia: Linguistic Data
terns of productivity and reuse in two very different                Consortium.
                                                                  O’Donnell, T. J. (2011). Productivity and reuse in language. Unpub-
sub–systems of English morphology: the past tense and                lished doctoral dissertation, Harvard University.
                                                                  O’Donnell, T. J., Goodman, N. D., & Tenenbaum, J. B. (2009). Frag-
derivational morphology.                                             ment grammars: Exploring computation and reuse in language
   In the literature, the problem of balancing produc-               (Tech. Rep. No. MIT-CSAIL-TR-2009-013). Cambridge, Ma: MIT
                                                                     Computer Science and Artificial Intelligence Laboratory Technical
tivity and reuse in sometimes discussed in terms of a                Report Series.
                                                                  Plag, I., & Baayen, R. H. (2009, March). Suffix ordering and morpho-
tradeoff between the cost of computation (in time) and               logical processing. Language, 85 (1), 109–152.
the cost of storage (in space; see, e.g., Frauenfelder &          Post, M., & Gildea, D. (2009). Bayesian learning of a tree substitution
                                                                     grammar. In Proceedings of the joint conference of the 47th annual
Schreuder, 1992). Here we have provided a very differ-               meeting of the ACL and the 4th international joint conference on
                                                                     natural language processing of the AFNLP.
ent perspective. The tradeoff being optimized here is not         Prasada, S., & Pinker, S. (1993). Generalisation of regular and irreg-
between two kinds of machine–level resources, but rather             ular morphological patterns. Language and Cognitive Processes,
                                                                     8 (1), 1-56.
between the ability to predict future novelty versus future       Taatgen, N. A., & Anderson, J. R. (2002). Why do children learn to
reuse. We believe that this perspective follows naturally            say “Broke”? A model of learning the past tense without feedback.
                                                                     Cognition, 86 (2), 123–155.
from the underlying design of the linguistic system. It           Zuidema, W. (2007). Parsimonious data-oriented parsing. In Proceed-
is a system which provides the flexibility to produce and            ings of the 2007 joint conference on empirical methods in natural
                                                                     language processing and computational natural language learning
comprehend new thoughts while maintaining the ability                (EMNLP–CoNLL 2007).
                                                              1618

