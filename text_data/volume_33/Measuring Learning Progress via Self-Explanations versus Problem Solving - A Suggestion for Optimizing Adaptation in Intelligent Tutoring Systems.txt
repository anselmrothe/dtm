UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Measuring Learning Progress via Self-Explanations versus Problem Solving - A Suggestion
for Optimizing Adaptation in Intelligent Tutoring Systems
Permalink
https://escholarship.org/uc/item/27j4x4r8
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33)
Authors
Otieno, Christine
Schwonke, Rolf
Renkl, Alexander
et al.
Publication Date
2011-01-01
Peer reviewed
 eScholarship.org                                Powered by the California Digital Library
                                                                 University of California

    Measuring Learning Progress via Self-Explanations versus Problem Solving - A
             Suggestion for Optimizing Adaptation in Intelligent Tutoring Systems
                                      Christine Otieno (otieno@psychologie.uni-freiburg.de)
                           Department of Educational and Developmental Psychology, University of Freiburg
                                              Engelbergerstr. 41, D-79085 Freiburg, Germany
                                     Rolf Schwonke (schwonke@psychologie.uni-freiburg.de)
                           Department of Educational and Developmental Psychology, University of Freiburg
                                              Engelbergerstr. 41, D-79085 Freiburg, Germany
                                      Alexander Renkl (renkl@psychologie.uni-freiburg.de)
                           Department of Educational and Developmental Psychology, University of Freiburg
                                              Engelbergerstr. 41, D-79085 Freiburg, Germany
                                                 Vincent Aleven (aleven@cs.cmu.edu)
                   Human-Computer Interaction Institute, School of Computer Science, Carnegie Mellon University
                                                5000 Forbes Ave, Pittsburgh, PA 15213 USA
                                                    Ron Salden (rons@cs.cmu.edu)
                   Human-Computer Interaction Institute, School of Computer Science, Carnegie Mellon University
                                                 5000 Forbes Ave, Pittsburgh, PA 15213 USA
                                Abstract                                overview, see Koedinger & Corbett, 2006). Cognitive
                                                                        Tutors are used in more than 2600 schools across the United
  Prior studies have shown that learning by problem solving in          States as part of the regular curriculum. Based on an online
  an intelligent tutoring system such as the Cognitive Tutor can        assessment of students’ learning, Cognitive Tutors provide
  be even more effective when worked examples are added in              individualized support for guided learning by problem
  comparison to problem solving alone. Introducing self-
  explanation prompts additionally improves learning.
                                                                        solving (doing). Specifically, the Tutor selects appropriate
  Furthermore, recent findings indicate that fading out worked          problems, gives just-in-time feedback, and provides hints.
  examples according to students’ performance during learning              Introducing self-explanation prompts to the Cognitive
  (i.e., adaptive fading) is even more beneficial than fading           Tutor made the Tutor even more effective (Aleven &
  worked examples in a predefined procedure (i.e., fixed                Koedinger, 2002). However, from a cognitive load
  fading). In this contribution we investigate the relationship         perspective the introduction of self-explanation activities in
  between potential indicators for learning progress, which can         addition to problem solving places fairly high demands on
  be used for adapting fading and, thereby, fostering learning
  outcome. We found a stronger relationship of learning                 students’ limited cognitive capacity, particularly in the early
  outcomes to self-explanation performance than to problem-             stages of skill acquisition (e.g., Sweller, van Merriënboer, &
  solving performance during learning. Additionally, self-              Paas, 1998), notwithstanding the load reducing aspects of
  explanation performance is a stronger predictor for learning          Cognitive Tutors, such as making subgoals and intermediate
  outcome than prior knowledge. Hence, adaptation, not only of          steps explicit. The additional load posed by self-
  the example fading procedure but potentially of other aspects         explanations can be reduced by scaffolding the learning
  of student learning (e.g., individualized problem selection)
                                                                        process with worked examples (e.g., Salden, Aleven, Renkl,
  might better be based on self-explanation performance and
  not, or at least not only, on problem-solving performance, as         & Schwonke, 2009). Meanwhile, there is ample empirical
  it is typical of intelligent tutoring systems.                        evidence showing that learning from worked examples leads
                                                                        to superior learning outcomes as compared to problem
  Keywords: Scaffolding, Worked Examples, Intelligent                   solving (for an overview, see Renkl, 2011).
  Tutoring Systems, Adaptive Fading
                                                                           Although studying worked examples has proven to be
                                                                        effective, this is true only during early stages of skill
                           Introduction                                 acquisition (e.g., Kalyuga, Chandler, Tuovinen, & Sweller,
Nowadays individualized instruction is a catchphrase that is            2001). During that phase, scaffolding with worked examples
becoming more and more important. Cognitive Tutors and                  reduces the cognitive demands of problem solving and
other intelligent tutoring systems have proven to be very               allows the learner to focus on understanding domain
effective in supporting individual students’ learning in a              principles. As expertise increases, worked examples not
variety of domains such as mathematics or genetics (for an              only lose their effectiveness but can even impede learning
                                                                       84

progress (expertise reversal effect; Kalyuga, Ayres,            approach, the adaptation (here: of fading) in our experiment
Chandler, & Sweller, 2003). The more students know about        could not be based on problem-solving performance while
a subject matter the more emphasis should be put on             working in the Cognitive Tutor, because problem-solving
problem-solving activities which lead to an increase of         steps were worked-out in the beginning. Hence, we used
speed and higher accuracy of performance (Renkl &               self-explanation performance, that is, a type of meta-
Atkinson, 2003). Therefore, Renkl and Atkinson (2003)           cognitive performance (Aleven & Koedinger, 2002), for
proposed a fading procedure in which problem-solving            adaptation. Against this background, the questions arise
elements are successively integrated into example study         whether it is sensible at all to rely on self-explanation
until the students are able to solve problems on their own,     performance or whether this might be even the better
that is, scaffolding is reduced according to students´          indicator for learning progress. The finding of Salden et al.
expertise.                                                      (2009) on the superiority of adaptive fading suggests that
   In a number of experiments, Renkl and colleagues             self-explanation performance is a sensible indicator for
provided empirical evidence for the effectiveness of a          learning progress that can be used for adaptation, even if
smooth transition from example study to problem solving         these self-explanations are prompted and supported by
(e.g., Atkinson, Renkl, & Merrill, 2003; Renkl, Atkinson, &     menus. However, in order to gain deeper insight in the
Große, 2004). The specifics of the sequence in which            potential of self-explanation performance as an indicator for
worked examples are faded are crucial for the optimization      adaptation and in the potential of different indicators, we
of learning. Although these studies indicate that worked        performed a re-analysis of our laboratory study.
examples faded in a fixed procedure were superior to               We assumed, that students who have difficulties in
example-problem pairs, similar like in classical research       gaining deeper understanding make more mistakes while
about scaffolding (e.g. Wood, Bruner, & Ross, 1976),            working with the Tutor (e.g., Aleven, McLaren, &
fading worked examples adaptively to the individual             Koedinger, 2006). Higher proportions of incorrect entries
learner’s progress should be even more successful. By           for both numerical entries (answers) and self-explanation
assessing the learning progress one can avoid the negative      activities (reasons) should therefore be associated with
effects of worked examples, also known as the reverse           inferior learning outcomes (in terms of transfer
worked example effect (Kalyuga et al., 2001). The               performance). This (negative) relationship should be
Cognitive Tutor with its online assessment provides an          especially strong for self-explanation (i.e., reason) steps as
appropriate framework for fading worked examples                we assume that they reflect a deeper understanding.
adaptively.                                                     Therefore, self-explanation performance in addition to the
   Evidence for the effectiveness of adaptively fading          traditionally used problem-solving performance should be
worked examples was first found in one of our previous          predictive of learning outcomes. More specifically, we
experiments (Salden et al., 2009). In this laboratory study,    addressed the following hypotheses:
we compared three conditions: a problem-solving condition,         (1) There is a negative relationship between
a fixed-fading condition, and an adaptive-fading condition,               performance (i.e., incorrect entries) on problem-
in order to test effects of faded worked examples over                    solving (i.e., answer) and self-explanation (i.e.,
problem-solving alone and adaptive fading over fixed                      reason) steps while working with the Tutor and
fading of worked examples (see also Method section in this                learning outcomes.
paper). As expected, Salden et al. (2009) found a monotonic        (2) The negative relationship is stronger for
trend of adaptive fading over fixed fading over problem                   performance on self-explanations steps.
solving. Effects were found in both posttest (Z = 2.03, p <        (3) Performance on problem-solving and self-
.05) and delayed posttest (Z = 1.83, p < .05). Additionally,              explanation steps is both predictive of learning
contrasts calculated to compare the adaptive-fading                       outcomes.
condition with the non-adaptive conditions (fixed fading           (4) Performance on self-explanations steps is a
and problem solving) revealed a significant superiority of                predictor of learning outcomes, beyond the
the adaptive-fading condition in both immediate posttest                  predictive power of performance on problem-
(t(54) = 1.74, p < .05, d = .49) and delayed posttest (t(49) =            solving steps and prior knowledge.
2.04, p < .05, d = .59). These findings could be largely
replicated in a field experiment (Salden et al., 2009). Taken                              Method
together, these results indicate that not only are faded
examples superior to example-problem pairs, as already          Sample and Design
found in earlier studies (e.g., Schwonke, Renkl, Krieg,
Wittwer, Aleven, & Salden, 2009), but also adapting the         We recruited 57 students (19 in 9th grade and 38 in 10th
                                                                grade) from a German “Realschule”, which is equivalent to
fading procedure according to students’ performance is
superior to a fixed sequence.                                   an American high school. The participants (age: M = 15.63,
   Typically, Cognitive Tutors adapt instruction based on the   SD = .84) were randomly assigned to one of the three
learner’s     problem-solving      performance       (Corbett,  conditions with 19 participants each. In two conditions
                                                                students were given worked examples for problem-solving
McLaughlin, & Scarpinatto, 2000). Unlike this widely used
                                                                (i.e., answer) steps which were either faded out according to
                                                               85

a fixed procedure (fixed fading condition) or according to      “30” on the answer step and “complementary angles” on the
the student’s individual skill level and self-explanation       reason step.
performance (adaptive fading condition). The third                 In the two example conditions, students were asked to
condition did not receive any worked examples (problem          study a sequence of worked steps corresponding to the
condition) and served as a control condition. Students in all   answer steps in the problem condition. Worked examples
conditions had to provide prompted self-explanations (i.e.,     provided the numerical solutions of a problem step and
reasons) for all problem-solving steps and all students had     necessary calculations. Students were then asked to provide
to solve at least some problem steps (Aleven & Koedinger,       a reason for the answers provided by the worked examples.
2002). As the aim of our reanalysis was to investigate          The worked examples were gradually faded out according to
relationships between performance on problem and reason         either a fixed fading scheme or adaptively according to
steps while working with the Tutor and performance on           students’ performance on self-explanation steps. Self-
posttest independent of condition, the following results refer  explanation activities were held constant across the three
to all 57 participants of the study.                            experimental conditions.
Learning Environment – The Cognitive Tutor                      Instruments
In order to provide feedback and adapt to students’ skill
acquisition, Cognitive Tutors are based on so called            Pretest The pretest was implemented in the Cognitive Tutor
production rule models. Different production rules for          and consisted of four geometry problems related to the
knowledge components can be learned independently. In the       lessons taught later during the learning phase with the
present case, a knowledge component represents specific         program. All Cognitive Tutor help facilities (e.g., hints)
ways of applying principles, for example, angle addition,       were disabled during pretest. All participants completed the
that are to be learned by the student.                          same pretest.
   The assistance in the Geometry Cognitive Tutor is based
on two algorithms: model tracing and knowledge tracing          Posttest A posttest that consisted of the same problems as
both of which are grounded in the idea of knowledge             the pretest was implemented in the Tutor. Additionally, all
components in the production rule model. This model             participants were asked to complete a paper-pencil test
enables the tutor to simulate the problem solving process, to   immediately after working with the Tutor and one week
decide whether a student’s action is right or wrong and to      later (delayed posttest). Both posttests were identical. The
provide intelligent feedback (model tracing) as well as to      items in the paper-pencil tests differentiated between near-
estimate the student’s learning progress on the level of        transfer and far-transfer problems (four items for near
knowledge components (knowledge tracing; Koedinger &            transfer and four items for far transfer). Near-transfer
Corbett, 2006). On this basis, the Cognitive Tutor can adapt    problems were isomorphic to the problems in the Tutor; far-
the assistance given to students to their learning progress.    transfer problems were structurally different but based on
Hence, we were also able to fade out worked examples            the same concepts. As in the example shown in Figure 1,
adaptively in the adaptive fading condition. The type of        students were given a structurally similar figure like in the
problems that were presented in our study was held constant     Tutor for near-transfer items. They were then asked (in this
across conditions.                                              example) to calculate angle IGT and angle TGH. Figure 2
                                                                shows an example for a far-transfer item. In this item
Learning Materials Students were asked to work on fifteen       students were given a cover story of a sailor who is
problems in a Cognitive Tutor lesson on geometry, together      navigating by the stars, in this case, the Southern Cross.
covering four geometry principles. The first eight problems     Then they were asked to calculate angle CXD given that
required the application of only one geometry principle. The    angle AXD is 45 degrees to find out in which angle the
last seven problems combined different principles and were      destination island was to the sailor’s ship.
therefore more complex. In the problem condition, solving a
problem required students (a) to enter numerical values such
as the measure of an angle (i.e., the answer) and (b) to self-
explain each given numerical answer (i.e., the reason). The
self-explanation consisted of entering the name of the
principle applied into a text entry field. The principle could
be entered either by typing the name of the relevant
principle or by selecting the principle from a glossary that
contained a list of all principles used in the unit. For
example, if angles AB and AC are complementary angles
and the measure of angle AB is 60 degrees, then the
measure of angle AC is 30 degrees, because the sum of the
measures of complementary angles is equal to 90 degrees.
The student would be required to either enter “90-60” or
                                                                        Figure 1: Example for a Near-Transfer Problem
                                                               86

                                                                  To test hypotheses 1 and 2 we determined Pearson´s
                                                               correlations between the proportion of incorrect answers in
                                                               relation to all answer steps executed (i.e., problem-solving)
                                                               as well as the proportion of incorrect reasons (i.e., self-
                                                               explanation) on the one hand and immediate as well as
                                                               delayed posttest scores on the other hand (Table 1). Posttest
                                                               scores reflect the percentage of points students received for
                                                               their posttest of the total of points to be achieved. The
                                                               performance on reason steps was significantly and
                                                               substantially related to scores on near and far transfers items
                                                               on both immediate and delayed posttest. In contrast, the
                                                               performance on answer steps was only reliably related to
                                                               scores on near transfer items in the immediate posttest. In
                                                               fact, the relationships to learning outcomes were
                                                               significantly stronger for reason steps than for answer steps
                                                               as corresponding comparisons (Olkin) shows, z (near
                                                               transfer) = 2.69, z (far transfer) = 2.29, z (delayed near
         Figure 2: Example for a Far-Transfer Problem          transfer) = 1.87, z (delayed far transfer) = 2.81 (with zcrit =
                                                               1.65 for a one-tailed significance test with α= 5%). In
                                                               summary, the pattern of results only partly confirmed
Procedure                                                      hypotheses 1: Performance on reason steps (i.e., self-
The experimental sessions lasted, on average, 90 minutes       explanation), as indicator of deep understanding, was
and were divided into three parts: pretest and introduction,   significantly related to posttest scores (i.e., learning
tutoring, and posttest. First, students´ general prior         outcomes), while performance on answer steps (i.e.,
knowledge was assessed by their mathematics grade              problem solving) were not significantly related to posttest
together with some additional demographic data such as age     scores (except for near transfer on immediate posttest).
and gender. Then they received a brief introduction on how     Hypothesis 2 was confirmed: The negative relationship
to use the Cognitive Tutor followed by a short pretest         between performance on reason steps and learning outcome
implemented in the Tutor measuring their prior knowledge.      was significantly stronger than that for performance on
After completing this pretest, students read an instructional  answer steps and learning outcome.
text providing information about the rules and principles         Although, the performance on answer steps and on reason
that were later addressed in the Cognitive Tutor. In the       steps differed substantially in their predictive power with
tutoring part, students worked with their respective version   respect to the posttest measures, we found a medium
of the Cognitive Tutor. This learning phase was followed by    correlation between them (r = .45, p < .001). This
the posttests.                                                 association can be expected because answer steps and
                                                               reason steps are not independent but rather measure
                           Results                             understanding on different levels. Moreover, performance
                                                               on reason steps might be influenced by the Tutor’s support
Table 1: H1 & 2: Relationship of Performance on Answer
                                                               received on the respective answer step.
and Reason Steps and Learning Outcomes
                                                               Table 2: H3: Performance on Reason Steps as Predictor for
                              Answers       Reasons
                                                               Learning Outcomes (Final Regression Model)
                                 r             r         N
                                                                                                      B       SE     β
  Posttest     Near                                                                                           B
                              -.34*         -.65***      57
               Transfer                                          Posttest      Near        Reasons -.24       .03    -.73***
                                                                            Transfer
               Far
                                                                               Far         Reasons -.18       .04 -.50***
                              -.18          -.48***      57
               Transfer                                                     Transfer
                                                                 Delayed       Near        Reasons -.12       .04 -.41**
  Delayed      Near
                                                                 Posttest Transfer
                              -.16          -.41**       52
  Posttest     Transfer                                                        Far         Reasons -.19       .05 -.49***
                                                                            Transfer
               Far
                                                               Note. Posttest, Near Transfer: R² = .54; Posttest, Far
                              -.12          -.49***      52
               Transfer                                        Transfer: R² = .25; Delayed Posttest, Near Transfer: R² =
                                                               .17; Delayed Posttest, Far Transfer: R² = .24.
Note. * p < .05, ** p < .01, and *** p < .001 (two-tailed).
                                                               ** p < .01 and *** p < .001.
                                                              87

  To test hypotheses 3 and to decide if problem-solving or       close to 1 for all predictors). Additionally, collinearity
self-explanation activities or both in combination are           diagnostics showed that all predictors included in the
presumably best for adapting support in intelligent tutoring     models loaded highly on different dimensions.
systems, we calculated a stepwise linear regression with            Results indicate that specific prior knowledge as
performance on reason and answer steps as predictors. As         measured with the pretest did not yield additional
the correlations from Table 1 suggest, the predictive power      explanatory power. However, general prior knowledge as
of error rate on answer steps (i.e., problem-solving) was        measured with mathematics grade added predictive value to
very low. Accordingly, regression models including only          self-explanation activities in all models and even served as
significant predictors omitted error rate on answer steps        best sole predictor for near transfer in the delayed posttest
(Table 2). Hence, Hypothesis 3 was not confirmed, that is,       (Table 3). These findings are in accordance with findings of
only performance on reason steps but not on answer steps         strong influences of (general) prior knowledge on further
had significant predictive power for learning outcome.           learning (for an overview, see Dochy, Segers, & Buehl,
                                                                 1999; Shapiro, 2004). On the whole, Hypothesis 4 is
Table 3: H4: Prior Knowledge and Performance on Answer           confirmed in that self-explanation performance has
and Reason Steps as Predictors for Learning Outcomes             predictive power for learning outcomes beyond prior
                                                                 knowledge and problem-solving performance. Only in the
                                      B         SE     β         case of the delayed near transfer, the hypothesis did not
                                                B                hold.
  Posttest     Near      Step 1
             Transfer     Reasons -.20          .03    -.65***                            Discussion
                         Step 2
                                                                 Contrary to the widely used approach to base adaptation of
                          Reasons -.18          .03    -.59***   instruction in intelligent tutoring systems on problem
                          Math.       -.04      .02    -.26*     solving performance (i.e., answer steps), in the study by
                          Grade
                                                                 Salden et al. (2009) adaptation was based on self-
               Far       Step 1                                  explanation performance (i.e., reason steps). The superior
             Transfer     Reasons -.17          .05    -.44**    learning outcomes of the adaptive fading condition shows
                         Step 2                                  that adapting on the basis of self-explanation is a feasible
                          Reasons -.14          .05    -.37**    alternative. Our present findings indicate that it may even be
                          Math.       -.05      .02    -.27*     the better alternative: Learning outcomes were better
                          Grade                                  predicted by performance on reason steps (i.e., self-
  Delayed      Near      Step 1                                  explanation) than by performance on answer steps (i.e.,
  Posttest Transfer       Math.       -.08      .02    -.53***   problem solving). In addition, regression models´ predictive
                          Grade                                  power for learning outcome was not increased by including
               Far       Step 1                                  performance on answer steps. Again, given that traditionally
             Transfer     Reasons -.24          .05    -.55***   adaptation is based on problem-solving activities this is a
                         Step 2                                  very "provocative" finding: Did we use only a sub-optimal
                          Reasons -.21          .05    -.49***   indicator for students´ learning progress up until now?
                          Math.       -.05      .02    -.27*        Some students were able to write down mathematical
                          Grade                                  values but failed to provide correct self-explanations. A
Note. Posttest, Near Transfer: R² = .42 for Step 1, ΔR² = .06    similar discrepancy was observed by Siegler and Stern
for Step 2 (p < .05); Posttest, Far Transfer: R² = .19 for Step  (1998) in strategy discovery and by Aleven, Koedinger,
1, ΔR² = .07 for Step 2 (p < .05); Delayed Posttest, Near        Sinclair, and Snyder (1998) for problem solving in the
Transfer: R² = .28; Delayed Posttest, Far Transfer: R² = .30     Geometry Tutor. It indicated that (in spite of the correct
for Step 1, ΔR² = .07 for Step 2 (p < .05).                      problem-solving performance) a full understanding of the
* p < .05, ** p < .01, and *** p < .001.                         problem-solving step is still lacking and still needs to be
                                                                 developed. Against this background, self-explanation
   To test Hypothesis 4 we calculated stepwise linear            performance might actually be a particularly sensitive
regressions with general prior knowledge measured by             indicator as to whether a student has actually understood a
mathematics grade, specific prior knowledge measured by          problem-solving step and should therefore be confronted
the pretest, performance on answer steps (i.e., problem-         with a higher problem-solving demand. In addition, the
solving), and performance on reason steps (i.e., self-           present findings suggest that a step should not be faded out
explanation) as potential predictors for learning outcome.       before a "complete" understanding is achieved, that is, a
Distributional assumptions were met by all dependent             student can also provide a correct self-explanation (i.e.,
variables, that is, residuals could be assumed to be             reason) for a problem-solving step.
independent and distributed normally. Furthermore,                  Our findings have also shown that general domain
heteroscedasticity could be assumed. (Multi-)Collinearity        knowledge could be worth considering as a basis for initial
among predictors was not an issue, given tolerance values        adaptation. With respect to the finding that specific prior
well above .2 and VIF values well below 10 (VIF values
                                                                88

knowledge was less important, one should consider that the           problems: Combining fading with prompting fosters
pretest used in this study was rather short. Using a more            learning. Journal of Educational Psychology, 95, 774-
elaborate pretest might lead to different results.                   783.
Additionally, one can assume that prior knowledge and self-        Corbett, A., McLaughlin, M., & Scarpinatto, C. K. (2000).
explanation performance are not independent; prior                   Modeling student knowledge: Cognitive Tutors in high
knowledge can influence the quality of self-explanations. It         school and college. User Modeling and User-Adapted
might be argued that self-explanation performance is a more          Interaction, 10(2), 81-108.
"proximal" indicator of specific knowledge than a pretest.         Dochy, F., Segers, M., & Buehl, M. M. (1999). The relation
Further studies have to clarify this issue.                          between assessment practices and outcomes of studies:
  The main message of this paper is that traditional                 The case of research on prior knowledge. Review of
adaptation procedures that are based solely on problem               Educational Research, 69, 145–186.
solving performance are presumably sub-optimal and that            Kalyuga, S., Ayres, P., Chandler, P., & Sweller, J. (2003).
including self-explanation performance is likely to improve          The expertise reversal effect. Educational Psychologist,
adaptation. However, the present findings need, without              38, 23-31.
doubt, corroboration by further research that tests more           Kalyuga, S., Chandler, P., Tuovinen, J., & Sweller, J.
directly the effects of different adaptation procedures. We          (2001). When problem solving is superior to studying
suggest a comparison of at least three conditions in future          worked examples. Journal of Educational Psychology,
studies: one in which online assessment and adaptation are           93, 579-588.
based on self-explanation performance and one in which             Koedinger, K. R., & Corbett, A. T. (2006). Cognitive tutors:
online assessment and adaption are based on problem-                 Technology bringing learning sciences to the classroom.
solving performance. A third group combining the two                 In R. K. Sawyer (Ed.), The Cambridge handbook of the
should be added to test if self-explanation as a single              learning sciences. New York, NY: Cambridge University
indicator is as good or even better compared to a                    Press.
combination of self-explanation and problem solving.               Renkl, A. & Atkinson, R. K. (2003). Structuring the
Considering our results as well as those of many other               transition from example study to problem solving in
studies, prior knowledge, especially general domain                  cognitive skills acquisition: A cognitive load perspective.
knowledge, should be taken into account as well.                     Educational Psychologist, 38, 15-22.
  Our results might also have important implications for           Renkl, A. (2011). Instruction based on examples. In R. E.
classroom settings. Contrary to widely used methods of               Mayer & P. A. Alexander (Eds.), Handbook of research
measuring students´ understanding by examining if they are           on learning and instruction (pp. 272-295). New York,
able to solve problems correctly, it might be reasonable to          NY: Routledge.
test for students´ ability to explain their solutions rather than  Renkl, A., Atkinson, R. K., & Große, C. S. (2004). How
focusing on correct solution steps only.                             fading worked solution steps works – A cognitive load
                                                                     perspective. Instructional Science, 32, 59-82.
                    Acknowledgments                                Salden, R., Aleven, V., Renkl, A., & Schwonke, R. (2009).
  This work was supported by the Pittsburgh Science of               Worked examples and tutored problem solving:
Learning Center which is funded by the National Science              Redundant or synergistic forms of support? Topics of
Foundation; award number SBE-0354420.                                Cognitive Science, 1, 203-213.
                                                                   Schwonke, R., Renkl, A., Krieg, C., Wittwer, J., Aleven, V.,
                          References                                 & Salden, R. J. C. M. (2009). The Worked-example
                                                                     Effect: Not an Artefact of Lousy Control Conditions.
Aleven, V., McLaren, B. M., & Koedinger, K. R. (2006).               Computers in Human Behavior, 25, 258-266.
  Towards computer-based tutoring of help-seeking skills.          Shapiro, A. M. (2004). How including prior knowledge as a
  In S. Karabenick & R. Newman (Eds.), Help Seeking in               subject variable may change outcomes of learning
  Academic Settings: Goals, Groups, and Contexts (pp.                research. American Educational Research Journal, 41,
  259-296). Mahwah, NJ: Erlbaum.                                     159-189.
Aleven, V., & Koedinger, K. R. (2002). An effective meta-          Siegler, R. S. & Stern, E. (1998). Conscious and
  cognitive strategy: Learning by doing and explaining with          unconscious strategy discoveries: A micro-genetic
  a computer-based cognitive tutor. Cognitive Science, 26,           analysis. Journal of Experimental Psychology: General,
  147-179.                                                           127, 377-397.
Aleven, V., Koedinger, K. R., Sinclair, H. C., & Snyder, J.        Sweller, J., van Merriënboer, J. J. G., & Paas, F. (1998).
  (1998). Combatting shallow learning in a tutor for                 Cognitive architecture and instructional design.
  geometry problem solving. In B. P. Goettl, H. M. Halff,            Educational Psychology Review, 10, 251-296.
  C. L. Redfield, & V. J. Shute (Eds.), Intelligent Tutoring       Wood, D., Bruner, J. S., & Ross, G. (1976). The role of
  Systems, Fourth International Conference, ITS '98 (pp.             tutoring and problem solving. Journal of Child
  364-373). Berlin: Springer Verlag.                                 Psychology and Psychiatry, 17, 89-100.
Atkinson, R. K., Renkl, A., & Merrill, M. M. (2003).
  Transitioning from studying examples to solving
                                                                  89

