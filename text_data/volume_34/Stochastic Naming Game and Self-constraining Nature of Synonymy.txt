UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Stochastic Naming Game and Self-constraining Nature of Synonymy
Permalink
https://escholarship.org/uc/item/1jg9g7wr
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Eryilmaz, Kerem
Bozsahin, Cem
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

             Lexical Redundancy, Naming Game and Self-constrained Synonymy
                                             Kerem Eryılmaz (kerem@ii.metu.edu.tr)
                        Cognitive Science, Middle East Technical University (METU), Ankara 06800, Turkey
                                              Cem Bozşahin (bozsahin@metu.edu.tr)
                                            Cognitive Science, METU, Ankara 06800, Turkey
                              Abstract                                     Another avenue is that of Steels, who also sees language as
                                                                        a complex adaptive system (Steels, 2000). His work started
   Language games are tools to model some aspects of the social         as an investigation of semiotic dynamics in simple language
   aspects of language and communication. Our approach aims
   to cover the ground between the elementary naming game and           games played among artificial autonomous agents, and even-
   the complex models for social use, for the growth of possibly        tually led to an elaborate construction grammar formalism
   redundant community and personal lexicons. It uses weighted          called Fluid Construction Grammar (FCG) for simulating a
   lists of words for the personal lexicon, probabilistic choice as
   a selection mechanism and lateral inhibition as the weight up-       population of language users who bootstrap their own lan-
   date scheme. The results demonstrate that the model is a gen-        guage (Steels & De Beule, 2006).
   eralization of the elementary naming game, and it provides a
   good picture of how big a lexicon agents use for the task, how          Steels’s original models were kept elementary to make
   this size can be controlled using the model parameters and a         them easier to analyze. FCG, however, is quite comprehen-
   possible way of explaining how synonymy is kept under con-           sive. We feel that there is much to be discovered at vari-
   trol.
                                                                        ous levels of complexity, from the trivial, original “naming
   Keywords: emergence; stochastic naming game; synonymy;               games” to the full-blown grammatical complexity of FCG.
   language game; semiotic dynamics
                                                                        For example, one feature, the negotiation for names of ob-
                                                                        jects, remains simple: if two agents agree on a name, they
                          Introduction                                  agree to adopt that one and discard the alternatives, there-
Language is a social phenomenon. Although there are uses                fore their lexicons are nonredundant if and when conver-
to language that are not social, and there are modes of so-             gence arises. Since complex adaptive systems are very un-
cial interaction other than language use, a very clear and              predictable in terms of how things might change if we change
salient function of language is collaboration and communi-              the structure of interactions, we think it is important that we
cation among individuals in a population. However, com-                 explore much of the degrees of freedom as we can, trying to
putational studies on language until mid-90s had been more              characterize the emerging structures and phenomena at each
focused on modelling the individual capacities in language              level of complexity added.
acquisition and performance, and did not provide much in-                  In that spirit this paper presents an investigation of a nam-
sight into what language might look like from the viewpoint             ing game among artificial agents with a slightly more com-
of a population of language users.                                      plex, weight-based lexicon scheme rather than the original,
   A new generation of research addresses this problem by               and a consequent probabilistic word selection scheme. Our
investigating language as it is produced, used and propagated           focus here is on the tension between lexicon growth and syn-
in a community of language users. A fruitful approach has               onymy in the population.The paper first describes the original
been to see language as a complex adaptive system. What                 naming game. Our extension is described next. Our experi-
this basically implies is that only at the population level can         ments and results are then presented.
we adequately characterize language use, and only by tak-
ing into account that language is constantly reshaped by its                           The Original Naming Game
users. How an individual uses language and how the popu-                The naming game focuses on vocabulary formation and
lation generally uses language can and do affect one another.           agreement in a population. The agents try to bootstrap a vo-
In short, the claim is that some features of language do not            cabulary of (proper) nouns that they associate with the objects
stem directly from linguistic or cognitive capacity but from a          they try to name (De Vylder & Tuyls, 2006). It is assumed
society of interacting agents. One such feature is the lexicon          that the agents already know how to send and receive signals,
and its emergence as a common vocabulary of a community.                and possess the motivation to do so. It is further assumed that
   This new branch of research proved to be quite fertile and           the objects are uniquely identifiable by all agents, so feature
produced a large body of work. One path of this research                sets as in the discrimination game are not employed.1 The fi-
is exemplified by Kirby, who approaches language as “the re-            nal assumption is that the agents have an independent channel
sult of an interaction between three complex adaptive systems           of communication (such as pointing) through which they can
that operate on different timescales: the timescale of biolog-          reveal their word-object pairings to each other.
ical evolution (phylogeny), the timescale of individual learn-
ing (ontogeny) and the timescale of language change (glos-                  1 Note that it is perfectly possible to combine the two games, as
sogeny)” (Kirby, 2002).                                                 done by Steels (2003).
                                                                    1530

     Formally, the game involves a set of objects O =                          Basically, this is just a lookup for the weight associated
 {o1 , ..., om }, and a set of agents A = {α1 , ..., αn }. Each agent          with the word in an agent’s lexicon. By adding this, the
 a j possesses a lexicon La j = {Ea j ,1 , ..., Ea j ,k } where k ≤ m,         characterization of an agent is a tuple of the lexicon and
 and each entry in the lexicon consists of a list of words associ-             αi , instead of only the lexicon as in the original game:
 ated with the object oi , Ea j ,i = {wi,1 , ..., wi,q }. All agents also
 possess a function that maps from the global set of objects to                                            αi :< Lαi , θαi >
 the agent’s repertoire of words (φa : O 7→ Ea j ). 2 Therefore,
 an agent is characterized only by its lexicon. A game consists             2. Word selection: The word selection scheme in the orig-
 of these steps:                                                               inal model simply picks a word from the set of words
                                                                               present in the lexicon. It does not specify how to pick the
1. Two agents are chosen, one as the hearer (αh ) and one as                   word. Although there are some suggestions for schemes
     the speaker (αs ).                                                        that optimize convergence (Baronchelli, DallAsta, Barrat,
2. The speaker chooses an object (oi ) to refer to, and points to              & Loreto, 2005), there is no set practice. The proposed
     it (i.e. makes his choice explicit without using the system               model has a specific scheme that makes a weighted, prob-
     we try to bootstrap)                                                      abilistic choice of the word to use at each round. This in-
                                                                               troduces a number of advantages. First, it introduces some
3. The speaker chooses a word in the lexicon for the object                    noise by not guaranteeing the leading word to be chosen at
     (φ(oi )), or creates one if necessary.                                    every round. Some level of noise is often beneficial to con-
                                                                               vergence in dynamical systems. Second, it is a more realis-
4. The hearer tries to decode the word into the object being
                                                                               tic scheme, especially when top words have similar scores.
     referred to (wk, j ∈ Eh,k ), and uses the indepedent channel
                                                                               Third, it makes the system more fault tolerant by mini-
     to signal this to the other (e.g. points to it).
                                                                               mizing the impact of successful rounds caused by words
5. The speaker agent assesses if the response complies with                    that are ultimately going to fail and of unsuccessful rounds
     its lexicon, and makes the hearer aware of this assessment.               caused by words that are ultimately going to succeed.
     Agents update their lexicons accordingly.3                                Formally, the function φαi is changed to return a word by
6. If all agents have identical lexicons, the game stops.                      a weighted random choice. To this end, we first define a
                                                                               probability distribution P where:
     Each game results in success (∃wi,n | wi,n = φs (oi ); wi,n ∈
 Eh,k ) or failure (6 ∃wi,n | wi,n = φs (oi ); wi,n ∈ Eh,k ). Upon suc-                                              θαi (wk,q )
                                                                                                      P(wk,q ) =                             (1)
 cess, both agents purge their entries for that object of all but                                                  ∑y θαi (wk,y )
 the successful word. Otherwise, the hearer adds the new word
 to its entry of the object, or creates one if necessary. There                Subsequently, the word φαi ’s returns can be characterized
 is no intermediary between a word exchange being success-                     as a random variable X with distribution P.
 ful and a word dominating an agent’s lexical inventory for an
                                                                                                             φαi (ok ) = X                   (2)
 object; it operates on an all-or-nothing basis.
                The Stochastic Naming Game                                     for which:
                                                                                                           X ∼ P; X ∈ Eαi ,k                 (3)
 The current proposal has four key differences from the origi-
 nal model:                                                                 3. Parameters: As a consequence of the update scheme,
1. Lexical entries: The lexical entries in the original model                  there are more parameters in this version of the game than
     are simple lists of words. The proposed model implements                  the original one. In particular, three δ-values (δsuccess ,
     lexical entries as weighted lists of words, updated upon in-              δfailure and δinhibition ) are added for use in updating the lex-
     teractions. This allows a graded behaviour in which words                 icon, whose precise roles are elaborated in the following
     are preferred, and constitutes a more realistic situation in              paragraphs. Additionally, two θ-values (θmax and θmin ) are
     which convergence should be achieved, compared to plain                   added as the maximum and minimum values for any score
     lists.                                                                    in the lexicon.
     More formally, for each agent αi , an additional value func-           4. Update scheme: The agents in the proposed model no
     tion θαi is added to retrieve the weight:                                 longer discard the competing synonyms (i.e. the other
                                                                               words in the lexical entry) upon a successful interaction.
                              θαi : wk,q 7→ R
                                                                               Instead, the agents update the weights of their lexical en-
      2 The function φ returns E which is a list of all the words used
                       a          a                                            tries for the object upon every interaction.
 for an object from the perspective on agent a, and not the most suc-
 cessful word.                                                                 A function ω is added to each agent which returns a new,
      3 Note that this assessment does not require global knowledge;           updated weight function after an interaction:
 it is a local decision determined solely by agent’s lexicon and its
 assessment of the interaction with another agent.                                                          ω : θαi 7→ θ0αi
                                                                         1531

   This function ω adds or subtracts from scores some pre-                        which is chosen as 500,000 for this study. This is the product
   defined δsuccess , δfailure and δinhibition , based on lateral inhibi-         of the number of agents and the number of objects, making it
   tion (Lenaerts, Jansen, Tuyls, & De Vylder, 2005). Upon a                      very likely that all agents will have taken part in at least one
   successful interaction with word wk,p , this function returns                  interaction regarding each object, making the success window
   a new function θ0αi and optionally modifies the lexicon of                     more meaningful. Also, note that our concept of “conver-
   the agent. The modification is that if the resulting score                     gence” is different from what is used in the literature. It does
   for a word is less than a predefined value θmin , that word is                 not mean that all lexicons are identical, it simply means that
   removed from the lexical item for that object. Also, there                     all lexicons are “similar enough” for the intended purpose,
   is a set limit θmax on how large the weight may grow, at                       “similar enough” defined as above. This way, it is possible
   which point no weight is added. More formally, ω returns                       to see if synonyms can exist at the point where the success of
   the following upon success:                                                    any given communication is almost certain.
                                                                                     The model was run with various δ parameters. The method
                                                                                 of choosing them was fixing a set of ratios in the form
                          min(θαi (wk,q ) + δsuccess , θmax )      ifq = p        δfailure :δsuccess and δinhibition :δfailure , and then producing the ac-
   θ0αi (wk,q ) =
                          θαi (wk,q ) − δinhibition                i f q 6= p     tual δ values by choosing a value for δsuccess and calculating
                                                                           (4)    the rest using that chosen value.
   where                                                                             There were five values for δsuccess denoted by the set
                                                                                  {1.0, 3.0, 5.0, 8.0, 10.0}. For the ratio δfailure :δsuccess , the ra-
                                                                                  tios picked were 0.0:1.0, 0.5:1.0, 1.0:1.0, 1.5:1.0 and 2.0:1.0.
                           ω(θαi )(wk,q ) = θ0αi (wk,q )                   (5)
                                                                                  The ratios used for δinhibition :δfailure were 0.0:1.0, 0.5:1.0,
   and the following upon failure:                                                1.0:1.0 and 1.5:1.0. If both δfailure and δinhibition are 0.0 for
                                                                                  a case, it is not possible to calculate δinhibition from the ratio,
         θ0αi (wk,q ) = θαi (wk,q ) − δ f ailure
                          
                                                           ifq = p         (6)    so for those cases δinhibition was set to δmin to provide some
                                                                                  negative feedback to the model so that it can converge.
   where                                                                             The cases in which δinhibition > 1.25 × δ f ailure are excluded
                                                                                  since this corresponds to the vicinity of the original model
                                                                                  and our aim is at exploring different areas of the parameter
                           ω(θαi )(wk,q ) = θ0αi (wk,q )                   (7)
                                                                                  space. Only the case represented by the tuple (10.0, 0.0, 10.0)
   It then modifies the lexicon as follows:                                       is included for comparison since it exactly corresponds to the
                                                                                  behaviour of the original model.
                            La0 = (La /Eαi ,k ) ∪ Eα0 i ,k                 (8)       The values δmax and δmin were fixed at 10.0 and 0.1, re-
                                                                                  spectively.
   where
                                                                                                                    Results
                 Eα0 i ,k = {w|θ0αi (w) ≥ θmin ; ∀w ∈ Eαi ,k }             (9)
                                                                                  The results make it clear that choosing the original model pa-
                                                                                  rameters is not the only viable option for our model. In fact,
   With this scheme, it is possible to mimick the original
                                                                                  of the total of 70 parameter sets, only 16 performed worse
model of Steels by using δsuccess , δfailure and δinhibition ,values of
                                                                                  than the original model parameters in terms of time of con-
10.0, 0.0, 10.0 respectively. Informally, this makes sure that
                                                                                  vergence.
success always maximizes the weight of a word and always
                                                                                     In the following, we will present the results on how model
eliminates other synonyms, and that failure does not have an
                                                                                  parameters interact. We are not going to present all the results
impact on the weights. This, in effect, is the behaviour of the
                                                                                  because of space considerations. The analysis will be made
original model.
                                                                                  both in terms of time of convergence, relative convergence
                               Methodology                                        rate and lexicon size. Relative convergence rate is defined as
                                                                                  the time it takes for the system to converge once the average
Each parameter set, that is, a tuple of (δsuccess , δfailure ,
                                                                                  size of the lexicons are maximized (this time point of max-
δinhibition ) was considered a unique case, and the simulation
                                                                                  imum lexicon size is represented as tmax in the simulation).
was run 50 times for each case, using 50 agents and 2 ob-
                                                                                  Time of convergence is the total number of rounds from the
jects.4 The model is considered to have reached convergence
                                                                                  start of the simulation to converge. Lexicon size is the total
when there is 100% success over a success window of 100
                                                                                  number of words in an agent’s lexicon.
rounds or when it reaches the limit for number of rounds,
                                                                                     The results confirm that δinhibition functions as a way to
    4 Originally, up to 5 objects were going to be tested but as far              shrink the lexicon to have a greater relative convergence rate,
as we could tell from the pilots, this did not provide any interesting            that is, to reduce the time it takes for the convergence to be
insights that 2 objects did not provide for our intentions. Since more
objects dramatically increased the simulation time and analysis, the              reached once tmax is reached. The functions of δsuccess and
simulations were run using just 2 objects.                                        δfailure are straightforward as positive and negative feedback,
                                                                              1532

respectively.                                                            tem returns to moving towards the word it originally had been
                                                                         converging to.
Interaction of δsuccess and δfailure                                        In contrast, a big lexicon at tmax means there are many al-
The interaction of δsuccess and δfailure is easier to explain.           ternatives, and a disturbance need not be fully counteracted;
These are directly counteracting forces, and therefore if                the system might just converge to another word-object pair-
δfailure is greater than δsuccess , the system fails to converge save    ing that is salient in the population. Accordingly, our re-
a few exceptional cases. This is not surprising since the sys-           sults show that a relatively large δfailure combined with a small
tem, especially before tmax , mostly learns by failing the ex-           δinhibition produces the fastest convergences, with a large lexi-
changes therefore learning new words. If the impact of failure           con at tmax since δinhibition does not get a chance to shrink the
is higher than that of success, then it becomes really difficult         lexicons as much. After that, applications of δinhibition mostly
to disseminate some words to all agents so that later they can           help convergence to the pairings that will ultimately domi-
converge to that word. Basically, they all get discarded before          nate.
their commonality causes them to become successful across
many interactions.                                                       Lexicon Size
   This effect is further compounded by non-zero δinhibition             Lexicon size can be used as an indicator of game dynamics.
values, which, upon occasional initial success, acts effec-              Since it is not a parameter but a quantity that manifests itself
tively as a failure penalty for all non-successful words, further        during the game, it is difficult to test. Nonetheless, there are
decreasing the number of common words. However, there are                some clear tendencies that are important.
a few cases where models with δsuccess ≤ δfailure actually con-
verge to a common vocabulary. In a closer look, there are two
conditions under which convergence occurs. One is when all
δ values are quite small, so that success can accumulate to
save at least a couple of alternatives for a word from being
discarded. In contrast, larger values mean that one or two
failures guarantee discarding of a word, and the number of
successes have little impact on this since the scores stop in-
creasing once they hit the upper limit of θmax . In other words,
small δ values give the agents some room to keep a greater
amount of interaction history, and therefore they become bet-
ter at evolving their lexicon in tandem with the population
trends.
   The other case is where δinhibition = 0. This leaves
only δsuccess and δfailure to battle each other, and occasionally
leaves room for convergence unless δfailure < 1.25 × δsuccess
or δfailure > 0.75 × θmax . This is equivalent to saying that
δfailure should leave room for at least two failures until a pre-
viously successful word is discarded, i.e. the word is not dis-
carded right away upon failure. Since there is no other mecha-           Figure 1: A log-log plot of time of convergence versus aver-
nism to decrease the weights, this allows some dissemination             age lexicon size for all convergent parameter sets.
with a bit of luck.
                                                                            The first such tendency is between the time of convergence
Interplay of δfailure and δinhibition                                    and average lexicon size at tmax . It turns out that there is a
The key to the relationship between δfailure and δinhibition is          direct proportion between the logarithm of average lexicon
that they can replace one another with slightly different ef-            size and the logarithm of the time of convergence. Roughly,
fects. δinhibition is a stronger form of δfailure which affects          this means the bigger the lexicon size at tmax , the longer the
not only one but almost all (save the successful one for                 convergence takes. This indicates that the time of conver-
the round) words associated with an object each round. In                gence and relative convergence rate are not necessarily corre-
fact, this is why the original model, with the parameter set             lated. Although parameter sets that produce bigger lexicons
(10.0,0.0,10.0), can function without δfailure .                         converge fast after tmax , they also take longer to put together
   This great impact of δinhibition effectively shrinks the lexi-        (i.e. to reach tmax ) and therefore do not necessarily represent
con, and this makes the tmax smaller but also reduces the rel-           the sets that allow convergence in the minimum number of
ative convergence rate. The reason is that at tmax there are             rounds.
less alternatives that the system may converge to for an ob-                The second tendency is best represented by a plot of rounds
ject. This means that any perturbation in the system, such               (i.e. the time series) versus average lexicon size (see Figure
as an interaction where the speaker agent prefers a not-to-              2). The plots fork into two fairly distinct groups, and all of
be-successful word, needs to be counteracted so that the sys-            the plots with bigger lexicon size belong to parameter sets in
                                                                     1533

                                                                            2) can conceivably make use of more memory than available
                                                                            here. This indicates that δinhibition is a way of controlling the
                                                                            memory use of the population for the task.
                                                                                                       Conclusion
                                                                            We believe that this line of research is quite relevant to cog-
                                                                            nitive science and to study of linguistics in general. Although
                                                                            the communicative capacities of our agents are very limited
                                                                            in that they do not contain any capabilities for syntax, dis-
                                                                            course etc., our results are somewhat reminiscent of Elman’s
                                                                            work on importance of starting with a small working memory
                                                                            in language acquisition (1993). Our model shows that using
                                                                            a reasonable δinhibition actually facilitates learning although it
                                                                            does reduce the amount of memory used for the task. In our
                                                                            model, it is not even about the availability of more memory;
                                                                            the memory simply does not need to be larger.
                                                                               This would not make sense for learning static knowledge
Figure 2: A log-log plot of time of convergence versus aver-                where having as many samples as possible at any point in
age lexicon size for all convergent parameter sets.                         time is the goal. But such a finding is arguably not as sur-
                                                                            prising for learning population-generated knowledge which
                                                                            may change from one point in time to the other. The memory
which δinhibition ≤ 0.1. This demonstrates that δinhibition has the         constraint comes not from the learners but from the nature of
function of shrinking the lexicon. In the figure, it is apparent            what is learned, how it changes and the relationship between
that for all cases, there is a peak (whose position in time we              the learners and what is learned. The constraint is not on any
call tmax ) after which convergence is achieved (which is also              given specific lexicon but on the “lexicon of the population”.
a finding encountered in the original naming game literature).              In other words, this effect does not really stem from the agents
For very small values of δinhibition , this peak is more or less            themselves but from the fact that what they are trying to agree
identical to the time point at which convergence is achieved.               on is malleable by this very process of negotiation.
In other words, δinhibition shrunk the lexicon so little that the              It also confirms the previous findings in the semiotic dy-
system converged with a larger average lexicon size. In other               namics literature that the sudden popularity of a word upon
words, on average, at least some of the objects have more                   coinage is an illusion. The conclusion from this literature,
than one alternative word associated with them although the                 including this study, is that there are often many competing
success rate climbed to 100% over a window of 100 rounds.                   synonyms for the same concept, and, after a word hits a pop-
This may hold an insight into how synonymy may be con-                      ularity threshold, the system falls into a spiral of making the
trolled in human languages.5 In our model, synonymy does                    most popular word even more popular. This is even true in
not necessarily hinder mutual communication. In more com-                   a model such as ours where there is a net negative feedback,
plex models, if they have analogous tendencies at all, it might             δfailure , and varying ratios of the strength of other mechanisms
be beneficial for large populations to be able to communicate               of pressure, unlike other models in the literature. There are
very successfully without a full agreement on which object is               grey areas where words are neither in disuse nor popular, and
referred to by which word.                                                  they need not become either unused or popular for conver-
   This also suggests that memory requirement for this task is              gence to occur.
not necessarily determined by the amount of memory avail-
                                                                               Finally, these simulations can give us an idea about con-
able to the agents but by the type of learning a population
                                                                            trolling synonymy. Our model does not necessarily have
adopts (or is born with), and the fact that the task is under-
                                                                            much pressure towards eliminating synonymy; it just works
taken by a population. Even if they had very big memory
                                                                            by looking at how successful communications are. The only
capacities (i.e. more than they need to use), part of this ca-
                                                                            pressure on synonymy is controlled by δinhibition . Such inhibi-
pacity would be redundant because of the decay in weights
                                                                            tion effectively means that success of one word for an object
of the unsuccessful or non-successful words would get them
                                                                            is the failure of others for that same object. However, un-
discarded from the lexicon.6 However, parameter sets with
                                                                            like simpler models in the literature, synonymy can be pre-
small δinhibition (similar to those in the top partition of Figure
                                                                            served while still achieving very high communicative suc-
    5 Of course, these are results from a single study with very strict     cess, though only when using low δinhibition values and with
and narrow boundaries in terms of what it deems possible “lan-              the side effect of having a larger vocabulary to hold the syn-
guage”; hence the verb inspiration instead of insight.
    6 Non-successful words are those whose weights are decremented          onyms. The fact that the model also converges with mul-
not because they failed, as unsuccessful words’ are, but because            tiple synonyms for an object both when δinhibition 6= 0 and
some other word succeeded and δinhibition was nonzero.                      δinhibition = 0 demonstrates that this ability to maintain syn-
                                                                        1534

onymy is not about the existence but the magnitude of the
pressure towards no synonymy, as exerted by δinhibition .
    The main lesson to learn from this is that synonyms are
eliminated only if they seriously hinder communicative suc-
cess, and they are eliminated more or less globally when they
do. It would be more feasible to use synonyms if the agents
had contextual cues as humans do to constrain the search
space, but synonymy can be preserved even without this addi-
tional information. If the ambiguity they bring is manageable
within the task we are trying to achieve with our language
(and these models demonstrate that there really exist such
tasks), they need not be eliminated. This is analogous to the
difference between colloquial text and legislation in terms of
ambiguity, which arguably has something to do with what the
population of language users intend to use the language for.
What we are trying to achieve in such tasks is not necessarily
identical lexicons but communicative success, and what this
means has to be defined on a per-case basis.
    Figure 2 shows that we pay the price for synonymy if we
let it loose: it causes either late convergence or no conver-
gence. The natural equivalent of these results are open to dis-
cussion. Our suggestion is that they seem to indicate keeping
it low among a community while not completely eliminating
it seems to be a way of balancing expressivity and commu-
nicative success. If either aspect begins to dominate, it will
be equivalent to widespread synonymy and no synonymy, re-
spectively. The conclusion that these may be counteracting
forces are shown by synonym’s relation to number of itera-
tions, rather than assumed cognitively.
                           References
Baronchelli, A., DallAsta, L., Barrat, A., & Loreto, V. (2005).
   Strategies for fast convergence in semiotic dynamics. Word
   Journal Of The International Linguistic Association, 6.
De Vylder, B., & Tuyls, K. (2006). How to reach linguistic
   consensus: A proof of convergence for the naming game.
   Journal of theoretical biology, 242(4), 818–831.
Elman, J. (1993). Learning and development in neural net-
   works: The importance of starting small. Cognition.
Kirby, S. (2002, April). Natural Language From Artificial
   Life. Artificial Life, 8(2), 185–215.
Lenaerts, T., Jansen, B., Tuyls, K., & De Vylder, B. (2005).
   The evolutionary language game: An orthogonal approach.
   Journal of Theoretical Biology, 235(4), 566–582.
Steels, L. (2000, December). language as a complex adaptive
   system. In M. Schoenauer et al. (Eds.), Parallel problem
   solving from nature-ppsn vi (Vol. 6).
Steels, L. (2003, July). Evolving grounded communication
   for robots. Trends in Cognitive Sciences, 7(7), 308–312.
Steels, L., & De Beule, J. (2006). A (very) brief introduc-
   tion to fluid construction grammar. In Proceedings of the
   third workshop on scalable natural language understand-
   ing scanalu 06 (p. 73). Association for Computational Lin-
   guistics.
                                                                1535

