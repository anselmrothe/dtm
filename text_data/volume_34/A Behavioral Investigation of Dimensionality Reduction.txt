UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Behavioral Investigation of Dimensionality Reduction
Permalink
https://escholarship.org/uc/item/0ss3m55c
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Lewis, Joshua
Van der Maaten, Laurens
de Sa, Virginia
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                        A Behavioral Investigation of Dimensionality Reduction
               Joshua M. Lewis                         Laurens van der Maaten                            Virginia R. de Sa
            josh@cogsci.ucsd.edu                         lvdmaaten@gmail.com                          desa@cogsci.ucsd.edu
       Department of Cognitive Science Pattern Recognition & Bio-informatics Lab Department of Cognitive Science
      University of California, San Diego             Delft University of Technology            University of California, San Diego
                             Abstract                                   wide range of techniques?1 To answer this question, we need
                                                                        to evaluate whether humans are good at evaluating embed-
   A cornucopia of dimensionality reduction techniques have             dings. As there is no external authority we can appeal to, this
   emerged over the past decade, leaving data analysts with a           is a daunting task. However, it is relatively easy to find out
   wide variety of choices for reducing their data. Means of eval-
   uating and comparing low-dimensional embeddings useful for           whether human data analysts are at least consistent in their
   visualization, however, are very limited. When proposing a           evaluations, which is the first aim of this study. Consistency,
   new technique it is common to simply show rival embeddings           across individuals and across a wide range of inputs, is a rea-
   side-by-side and let human judgment determine which embed-
   ding is superior. This study investigates whether such human         sonable prerequisite for evaluation.
   embedding evaluations are reliable, i.e., whether humans tend           Beyond investigating whether human data analysts are con-
   to agree on the quality of an embedding. We also investigate         sistent when they evaluate embeddings, the second aim of this
   what types of embedding structures humans appreciate a pri-
   ori. Our results reveal that, although experts are reasonably        study is to investigate what humans are doing when they eval-
   consistent in their evaluation of embeddings, novices gener-         uate embeddings. Such information could be useful for deter-
   ally disagree on the quality of an embedding. We discuss the         mining whether humans are appropriate for an evaluation task
   impact of this result on the way dimensionality reduction re-
   searchers should present their results, and on applicability of      with a known structure (e.g. if they naturally prefer embed-
   dimensionality reduction outside of machine learning.                ding characteristics appropriate to the structure), or for devel-
   Keywords: dimensionality reduction; unsupervised machine             oping techniques that are tailored towards producing results
   learning; psychophysics                                              that humans will find helpful (e.g. algorithms that selectively
                                                                        emphasize informative data structure). We can to some extent
                                                                        infer human strategies from the algorithms humans prefer, but
                          Introduction                                  we can also investigate those strategies by correlating embed-
There is an evaluative vacuum in the dimensionality reduc-              ding characteristics with human evaluations.
tion literature. In many other unsupervised machine learn-                 Motivated by the two aims described above, we solicit em-
ing fields, such as density modeling, evaluation may be per-            bedding quality judgments from both novice and expert sub-
formed by measuring likelihoods of held-out test data. Al-              jects in an effort to determine whether they are consistent in
ternatively, in domains such as topic modeling, human com-              their ratings, and which embedding characteristics they find
putation (Ahn, Maurer, McMillen, Abraham, & Blum, 2008)                 appealing. For the novice subjects, we manipulate dataset
resources such as Amazon’s Mechanical Turk may be em-                   knowledge—half read a description and see samples from
ployed to exploit the fact that humans are phenoms in evaluat-          each dataset, and half do not. We hypothesize that provid-
ing semantic structure (Chang, Boyd-Graber, Gerrish, Wang,              ing dataset information will increase consistency, as it should
& Blei, 2009). Human evaluations have also been used to                 if the evaluative process is principled. The study consists of
assess image segmentation techniques (Martin, Fowlkes, Tal,             two experiments. The first presents subjects with a selection
& Malik, 2001). The field of dimensionality reduction, how-             of embeddings derived from nine distinct dimensionality re-
ever, lacks a standard evaluation measure (Venna, Peltonen,             duction algorithms; the second uses embeddings from a sin-
Nybo, Aidos, & Kaski, 2010), and is not as obvious a target             gle algorithm with several different parameter settings for a
for human intuition. Two or three dimensional embeddings                more controlled comparison between “clustered” and “grad-
can be visualized as scatter plots, but on what intuitive basis         ual” embeddings.
can we judge a 200 to 2-dimensional reduction to be good? In
addition, Gestalt effects or simple rotations may bias human                    Dimensionality reduction techniques
evaluations of scatter plots. Nevertheless, with no broadly
agreed upon embedding quality measure (though a few have                Dimensionality reduction techniques can be subdivided into
been proposed, see below), human judgment is often explic-              several categories: linear or non-linear, convex or non-
itly and implicitly solicited in the literature. The most com-          convex, parametric or non-parametric, etc. (Lee & Verley-
mon form of this solicitation consists of placing a scatter plot        sen, 2007). Whilst many new techniques have been proposed
of the preferred embedding next to those of rival embeddings            over the last decade, data analysts still often resort to linear,
and inviting the reader to conclude that the preferred embed-           convex, parametric techniques such as PCA to visualize their
ding is superior (e.g., (Maaten & Hinton, 2008)). If one is                 1 Moreover, one should note that dimensionality reduction com-
interested in applying a dimensionality reduction algorithm             prises only a small part of the “visualization zoo” (Heer, Bostock, &
to visualize a dataset, is this a valid way to select from the          Ogievetsky, 2010).
                                                                   671

data. Non-linear, convex, non-parametric manifold learn-             by minimizing the divergence between two distributions over
ers such as Isomap (Tenenbaum, Silva, & Langford, 2000),             pairs of points, in which the probability of picking a pair of
LLE (Roweis & Saul, 2000), and MVU (Weinberger, Sha,                 points decreases with their pairwise distance (Maaten & Hin-
Zhu, & Saul, 2007) are also frequently used for visualiza-           ton, 2008).
tion purposes (Jain & Saul, 2004; Lewis, Hull, Weinberger,
& Saul, 2008; Mahecha, Martı́nez, Lischeid, & Beck, 2007),                                  Experimental setup
even though it is unclear whether these techniques produce           We perform two experiments with our human subjects. The
superior results (Maaten & Hinton, 2008).                            first experiment uses stimuli generated from the dimen-
   As described in the introduction, one of the key prob-            sionality reduction algorithms described above to determine
lems of dimensionality reduction is that it lacks a widely           whether humans are consistent in their evaluations when the
agreed upon evaluation measure (Venna et al., 2010). In              embeddings are fairly distinct (the first aim of the study). The
fact, it is very unlikely that there will ever be such an eval-      second experiment uses stimuli that are all generated by t-
uation measure, as it would imply the existence of a free            SNE, but with different parameter settings that affect how
lunch (Wolpert, 1996): the “optimal” dimensionality reduc-           clustered the resulting embedding appears. This helps us de-
tion technique would be the technique that optimizes the mea-        termine what type of structure humans generally prefer in em-
sure. Also, there is a lot of debate within the field on what a      beddings (the second aim of our study).
good objective for dimensionality reduction is: for instance,
                                                                     Experiment 1
a latent variable model approach to dimensionality reduction
suggests one should focus on preserving global data struc-           In the first experiment, we divided subjects into (1) an expert
ture (Lawrence, 2005), whereas a manifold learning view-             group with detailed knowledge of dimensionality reduction
point deems preservation of local data structure more impor-         and information on the underlying datasets presented in writ-
tant (Roweis & Saul, 2000). The lack of an evaluation mea-           ten and pictorial form, (2) a novice group with no knowledge
sure and the ongoing debate within the field motivate the use        of dimensionality reduction and no information on the visu-
of a more anthropocentric approach.                                  alized data, and (3) a group of similar novices but with the
                                                                     same information on the underlying datasets given to the ex-
   In our study, we investigate nine dimensionality reduction
                                                                     perts. The dataset information we presented to groups 1 and
techniques, selected for their importance in the literature: (1)
                                                                     3 constituted of an intuitive description of the data, as well as
PCA, (2) projection pursuit, (3) random projection, (4) Sam-
                                                                     images representing the underlying data (e.g., the Swiss roll,
mon mapping, (5) Isomap, (6) MVU, (7) LLE, (8) Laplacian
                                                                     or handwritten character images).
Eigenmaps, and (9) t-SNE. PCA and projection pursuit find
                                                                        Thirty one undergraduate human subjects were recruited
a subspace of the original data space that has some desired
                                                                     for this study as the novice group, 16 female and 15 male,
characteristic. For PCA, this subspace is the one that maxi-
                                                                     with an average age of 19.1 years. None of the novice sub-
mizes the variance of the projected data. For projection pur-
                                                                     jects had any specific knowledge of dimensionality reduction
suit (Friedman & Tukey, 1974), the subspace maximizes the
                                                                     techniques. Our expert group consisted of five subjects—
non-Gaussianity of the projected data. Random projections
                                                                     three graduate students and two faculty members. The ex-
are linear mappings that mostly preserve pairwise distances in
                                                                     pert subjects were drawn from the same institution and rep-
the data due to the Johnson-Lindenstrauss lemma (Bingham
                                                                     resent two different departments. Amongst the five expert
& Mannila, 2001). Sammon mapping constructs an em-
                                                                     subjects there are four distinct academic backgrounds at the
bedding that minimizes a weighted sum of squared pair-
                                                                     graduate level. The informed novice group had 15 subjects
wise distance errors, where distances are weighted in in-
                                                                     and the uninformed novice group 16. We generated two-
verse proportion to their magnitude (Sammon, 1969). Isomap
                                                                     dimensional point-light stimuli (see Figure 1 for a visualiza-
constructs an embedding by performing classical scaling on
                                                                     tion of all the stimuli) by running the nine dimensionality re-
a geodesic distance matrix that is obtained by running a
                                                                     duction techniques discussed in Section on seven different
shortest-path algorithm on the nearest neighbor graph of the
                                                                     high-dimensional datasets, comprising a variety of domains.
data (Tenenbaum et al., 2000). MVU learns an embedding
                                                                     We ran each technique for a reasonable range of parameter
that maximizes data variance, while preserving the pairwise
                                                                     settings, and we selected the embedding that was best in terms
distances between each data point and its k nearest neighbors,
                                                                     of the trustworthiness2 (Venna & Kaski, 2006) for presenta-
by solving a semidefinite program (Weinberger et al., 2007).
                                                                     tion to the subjects.
LLE constructs an embedding that preserves local data struc-
                                                                        Each trial consisted of nine different embeddings of the
ture by minimizing a sum of squared errors between each map
                                                                     same dataset arranged randomly per trial in a 3 × 3 grid. The
point and its reconstruction from its k nearest neighbors in
                                                                     datasets were shown as scatter plots with white points on a
the original data (Roweis & Saul, 2000). Laplacian Eigen-
                                                                     black background to reduce brightness-related eye fatigue.
maps try to minimize the squared Euclidean distances be-
                                                                     For novice subjects, trials were organized into three blocks
tween each points corresponding to its k nearest neighbors in
the original data, while enforcing a covariance constraint on            2 The trustworthiness measures the ratio of k nearest neighbors in
the solution (Belkin & Niyogi, 2002). t-SNE embeds points            the data that is still among the k nearest neighbors in the maps.
                                                                 672

                        Figure 1: All stimuli from experiment 1. Methods are in rows; datasets are in columns.
of seven, where each dataset appeared once per block and                   1971) and include neutral responses. Fleiss’ κ measures the
the order of the datasets within each block was randomized.                deviation between observed agreement and the agreement at-
Expert subjects were tested on one block. We instructed sub-               tributable to chance given the relative frequency of ratings,
jects to choose the two most useful displays and the one least             and normalizes for the number of raters. Neutral ratings
useful display from the nine available on every trial. After               are twice as frequent as non-neutral, and positive ratings are
describing what a scatter plot is and emphasizing that each                twice as frequent as negative ratings, so the compensation
set of nine plots is a different perspective on the same dataset,          for relative frequency in Fleiss’ κ makes it well-suited to our
we gave subjects the following instructions: For each trial,               data.
please examine all the scatter plots and choose the two that
you find most useful and the one that you find least useful. The              We also measured the following six characteristics of our
task in the second part of this experiment will be much faster             embedding stimuli: (1) variance, (2) skewness, (3) kurtosis,
and easier if you choose useful scatter plots. Do the best                 (4) clusteredness, (5) visual span, and (6) Gaussianity. The
you can to choose useful plots based on whatever criteria you              variance, skewness, and kurtosis were measured per dimen-
deem appropriate. We intentionally left the task ambiguous                 sion in a map that was scale-normalized such that yi ∈ [0, 1]d
so as not to bias subjects towards particular evaluation crite-            (preserving the aspect ratio of the maps), and averaged over
ria3 , and the fictitious “second part” of the experiment existed          the d dimensions of the map. We measured clusteredness
solely for increasing subject motivation.                                  by constructing k-nearest neighbor graphs in the map with
   We analyzed our novice subjects for internal consistency                k = 3, . . . , 12, and measuring the maximum clustering coeffi-
of their positive and negative ratings across blocks and found             cient of the resulting graphs (Watts & Strogatz, 1998). The
that even our least consistent subject was more consistent than            clustering coefficient computes the ratio of connections be-
expected by chance. Hence, we did not exclude any subjects                 tween the adjacent vertices of map point i, averaged over all
due to internal inconsistency. To analyze consistency across               map points. The visual span of each map was measured by fit-
subjects (the first aim of this study) we use Fleiss’ κ (Fleiss,           ting a Parzen kernel density estimator with Gaussian kernels
                                                                           on the map (the variance σ of the Gaussians was optimized on
    3 For instance, defining a classification task would bias subjects     a small validation set). Subsequently, we measure the ratio of
to embeddings that show separated clusters.                                the map surface that has a density of at least 10% of the max-
                                                                       673

imum density of the density estimate. The Gaussianity of the
maps was determined by averaging the results of Lilliefors
tests (Lilliefors, 1967) performed on 5, 000 one-dimensional
random projections of the map4 . We analyze the relationships
between novice informed ratings, novice uninformed ratings,
expert ratings, and the six quantitative measures by calcu-
lating the Pearson’s correlation coefficient ρ between ratings
and measures (after normalization within trial).
                                                                               Figure 3: Sample stimuli from experiment 2. Parameter val-
                                                                               ues are in rows; datasets are in columns.
                                                                               exhibited poor agreement. Hence, the consistency measures
                                                                               reveal that, whereas experts tend to agree with each other on
                                                                               the quality of an embedding, novices strongly disagree with
                                                                               each other in their evaluations (they disagree more than if the
                                                                               evaluation was done randomly). Surprisingly, novices who
                                                                               received information on the underlying data disagree more
                                                                               strongly with each other than novices who had no informa-
                                                                               tion about the underlying data (counter to our hypothesis but
Figure 2: Human responses to the embeddings in experiment                      interpretable, see below).
1. Positive responses in the first row, negative in the second
row. Experts (left), novices (center) and informed novices
(right) by column. Algorithm and dataset ordering are the                      Table 1: Correlation coefficients between human responses
same as in Figure 1 within each block.                                         and dataset characteristics. Text in bold if p < .0036 af-
                                                                               ter Bonferroni correction for n = 14 comparisons per subject
                                                                               group and α = .05.
Experiment 2
                                                                                                                                                                                Trustworthiness
The second experiment was run directly following experi-
                                                                                                                                                               Clusteredness
ment 1 on the same subject pool using the same methods, save
                                                                                                                                                Visual Span
stimulus design. In experiment 2, the nine stimuli in each trial
                                                                                               Lilliefors    Skewness    Kurtosis    Variance
are obtained by running t-SNE with nine different degrees
of freedom v (viz. v = 0.5, 0.8, 1.0, 1.2, 1.5, 2.0, 2.5, 3.0, 4.0)
on the seven datasets. The degrees of freedom in t-                                      ρ
SNE determine to what extent the visualizations are “clus-                        Ex. Pos.    .26           -.01        -.19        .34         .17           .22              .41
tered” (Maaten, 2009). Sample stimuli are shown in Figure 3.                     Ex. Neg.     -.08          .17         .19         -.14        -.17          .08              -.08
                                                                                Nov. Pos.     .07           -.03        .50         -.18        -.29          .01              -.08
                             Results                                            Nov. Neg.     .00           .17         -.10        .22         .10           -.03             .24
Experiment 1                                                                      Inf. Pos.   -.02          -.16        -.10        -.11        .44           -.45             -.09
                                                                                 Inf. Neg.    .03           .31         .19         .10         -.19          .20              .19
For the first experiment, the Fleiss’ kappa consistency mea-
sure κ for experts is 0.39, for uninformed novices is −0.28,
and for informed novices is −0.40. Fleiss’ kappa κ ranges                         In Figure 2, we depict the raw ratings (averaged over each
from −1 to +1, with −1 representing complete disagreement,                     group) as a collection of Hinton diagrams. In the figure, a
+1 representing complete agreement and 0 representing the                      large square indicates that a relatively large number of sub-
amount of agreement expected by chance. Though there is                        jects gave a positive or negative evaluation of the embedding
no standard significance test for Fleiss’ kappa, based on the                  of the corresponding dataset, constructed by the correspond-
Landis and Koch scale (Landis & Koch, 1977), experts exhib-                    ing technique. The top row of diagrams represent positive
ited fair to moderate agreement, while both groups of novices                  responses and the bottom negative, so if subjects are in dis-
   4 Note that if the distribution of points in the embedding is Gaus-         agreement about a stimulus, there will be a large box in its
sian, the point distribution in each of the random projections has to          corresponding location in both rows. The diagrams reveal
be Gaussian as well.                                                           that informed novices exploit dataset knowledge in specific
                                                                         674

instances to differ significantly from uninformed novices. For       some sense after closer consideration. Comparing the Hin-
example, the t-SNE embedding of the Swiss roll dataset (a            ton diagrams between novices and informed novices, one can
relatively clustered embedding) is rated much more nega-             plainly see that informed novices are converging on a smaller
tively by novices when they know that the data are gradual.          selection of techniques for both positive and negative ratings.
Experts tend to rate t-SNE positively or negatively depend-          The issue for the informed novices, however, is that they are
ing on the dataset and show a relatively consistent rating for       not sure whether these stimuli should be rated as positive or
Isomap. Informed novices consistently rated Sammon map-              negative. As a result, there is often energy for the same cell
ping and projection pursuit positively while generally rating        in both diagrams. Since the base rate of positive and negative
manifold learners such as Isomap and LLE negatively. Un-             ratings is low compared to the neutral ratings, the κ consis-
informed novices are all over the map with the exception of          tency measure interprets this as substantial disagreement and
(like all other subjects) rating MVU as not notable in either a      thus the negative numbers. Importantly, the informed novice
positive or negative sense.                                          κ is further from chance level than the novice κ. In Experi-
   Table 1 shows correlation coefficients between the six em-        ment 2, uninformed novices actually differ more from chance
bedding characteristics and the evaluations by the three hu-         but the effect is about half the size, and experts remain con-
man groups. We also present the correlation between the              sistent.
evaluations and the trustworthiness, which gives an indica-             Expert ratings are laudable in that they correlate in the
tion of the quality of the embedding (in terms of local neigh-       correct direction with trustworthiness and have a context-
borhood preservations). The significant correlations are in          dependent appreciation of clusteredness. Both novice groups
bold type, after a Bonferroni correction for multiple compar-        rate clusteredness negatively regardless of context and are
isons (14 comparisons per subject group, α = .05). Notably,          more influenced by elementary characteristics such as visual
expert positive ratings are the only ratings that correlate sig-     span. The substantial difference in strategy between novices
nificantly and in the correct direction with trustworthiness.        and experts indicates that novices could really benefit from
Another correlation that stands out is visual span: it appears       training on the task of evaluating embeddings (unlike eval-
to play a substantial role in informed novice ratings (they ap-      uating results from topic modeling, image segmentation, or
parently surmise an embedding should fill up the available           object recognition).
space), whereas it plays little role in expert ratings.
                                                                                             Conclusion
Experiment 2
For the second experiment, the consistency measure κ for ex-         With respect to the first aim of our study (determining
perts is 0.35, for uninformed novices is −0.32, and for in-          whether humans are consistent in rating embeddings), we
formed novices is −0.26. The results of the second experi-           conclude that dimensionality reduction experts are indeed
ment thus reveal a similar trend: experts have fair agreement        reasonably consistent judges of embedding quality. This sup-
on the quality of embeddings, whereas novices give ratings           ports the practice of soliciting expert judgment for embed-
have poor agreement. The ratings reveal that, whereas ex-            ding evaluations, as nowadays is common in the literature on
perts selectively rate more clustered or more continuous em-         dimensionality reduction. However, we also conclude that
beddings positively depending on the dataset, novices over-          novices are very inconsistent with one another in terms of
whelmingly rate the more clustered embeddings as negative.           their rating of an embedding, even when they have detailed
On the other hand, for positive ratings the novices tend to          information on the dataset the embedding is visualizing. In
choose embeddings at either end of the spectrum while avoid-         fact, novices even correlate negatively with a measure of em-
ing the moderate values of v. Moderate values of v might be          bedding quality.
avoided since subjects want to classify displays closest to the         With respect to the second aim of our study (determining
prototypical clustered or graded display (Rosch, 1975). Us-          what types of structure humans appreciate in embeddings),
ing the same set of correlations from Experiment 1 we find           we conclude that humans do not appear to have overwhelm-
that experts ratings do not strongly correlate with any of the       ingly strong a priori preferences, although novices appear to
characteristics (including trustworthiness), but both groups of      appreciate gradual embeddings that span a large portion of
novices show a correlation between negative ratings and those        the space. Experts can adapt their preference for gradual vs.
stimuli with low kurtosis and high clusteredness.                    clustered depending on the dataset.
                                                                        Overall, our results discourage free-form solicitation of
                          Discussion                                 human computation approaches á la (Chang et al., 2009)
In both experiments, experts show themselves to be more              and (Martin et al., 2001) to the evaluation of dimensional-
consistent than chance and much more consistent than                 ity reduction techniques. Moreover, the novices’ lack of con-
novices in either condition. This is reassuring, and indicates       sistency lends worry to the prospect of naı̈ve dimensionality
that the idea of having experts evaluating embeddings is not         reduction-based analysis. Most data analysts seeking to ap-
flawed to begin with. In the first experiment, novice subjects       ply dimensionality reduction are not very familiar with the
actually get less consistent with each other if they are in-         field. As a result, they are likely to be susceptible to the fa-
formed. While this seems troubling at first, it actually makes       vorable visualizations presented in many dimensionality re-
                                                                 675

duction papers. To ensure that dimensionality reduction tech-         Lee, J., & Verleysen, M. (2007). Nonlinear dimensionality
niques are applied wisely, authors should strive to explicate           reduction. New York, NY, USA: Springer.
the dataset characteristics that favor their algorithms (e.g., t-     Lewis, J., Hull, P. M., Weinberger, K., & Saul, L. (2008).
SNE is useful if the data is expected to have cluster struc-            Mapping uncharted waters: exploratory analysis, visualiza-
ture, Isomap if the data lie on a convex manifold). Authors             tion, and clustering of oceanographic data. In Proceedings
could also cover usage scenarios appropriate to their algo-             of the international conference on machine learning and
rithm (e.g., if a researcher is interested only in visualizing          applications (pp. 388–395).
points that are most different then PCA would suffice and             Lilliefors, H. (1967). On the kolmogorov-smirnov test for
other techniques would be overkill), including guidelines for           normality with mean and variance unknown. Journal of
interpreting the relationship between the high and low dimen-           the American Statistical Association, 62, 399–402.
sional spaces (sometimes this relationship will be very clear,        Maaten, L. van der. (2009). Learning a parametric embed-
as in PCA; other times, as in MVU, there is not a clear re-             ding by preserving local structure. In Proceedings of the
lationship). In addition, data analysts should be encouraged            12th international conference on artificial intelligence and
to use sanity checks such as the trustworthiness measure in             statistics (pp. 384–391).
order to prevent them from basing analysis on interesting, but        Maaten, L. van der, & Hinton, G. (2008). Visualizing data us-
flawed, embeddings.                                                     ing t-SNE. Journal of Machine Learning Research, 9(Nov),
                                                                        2431–2456.
                    Acknowledgments                                   Mahecha, M., Martı́nez, A., Lischeid, G., & Beck, E. (2007).
This work is funded by NSF Grant #SES-0963071, Divvy:                   Nonlinear dimensionality reduction: Alternative ordination
Robust and Interactive Cluster Analysis (PI Virginia de Sa).            approaches for extracting and visualizing biodiversity pat-
                                                                        terns in tropical montane forest vegetation data. Ecological
                         References                                     Informatics, 2, 138–149.
                                                                      Martin, D., Fowlkes, C., Tal, D., & Malik, J. (2001, July).
Ahn, L. von, Maurer, B., McMillen, C., Abraham, D., &                   A database of human segmented natural images and its ap-
   Blum, M. (2008). reCAPTCHA: Human-Based Char-                        plication to evaluating segmentation algorithms and mea-
   acter Recognition via Web Security Measures. Science,                suring ecological statistics. In Proceedings of the 8th inter-
   321(5895), 1465-1468.                                                national conference on computer vision (Vol. 2, pp. 416–
Belkin, M., & Niyogi, P. (2002). Laplacian Eigenmaps and                423).
   spectral techniques for embedding and clustering. In Ad-           Rosch, E. (1975). Cognitive reference points. Cognitive
   vances in neural information processing systems (Vol. 14,            Psychology, 7(4), 532 - 547.
   pp. 585–591). Cambridge, MA, USA: The MIT Press.                   Roweis, S., & Saul, L. (2000). Nonlinear dimensionality re-
Bingham, E., & Mannila, H. (2001). Random projection in                 duction by Locally Linear Embedding. Science, 290(5500),
   dimensionality reduction: applications to image and text             2323–2326.
   data. In Proceedings of the 7th acm sigkdd (pp. 245–250).          Sammon, J. (1969). A nonlinear mapping for data structure
Chang, J., Boyd-Graber, J., Gerrish, S., Wang, C., & Blei, D.           analysis. IEEE Transactions on Computers, 18(5), 401–
   (2009). Reading tea leaves: How humans interpret topic               409.
   models. In Advances in neural information processing sys-          Tenenbaum, J., Silva, V. de, & Langford, J. (2000). A global
   tems (Vol. 21).                                                      geometric framework for nonlinear dimensionality reduc-
Fleiss, J. (1971). Measuring nominal scale agreement among              tion. Science, 290(5500), 2319–2323.
   many raters. Psychological Bulletin, 76(5), 378-382.               Venna, J., & Kaski, S. (2006). Visualizing gene interaction
Friedman, J., & Tukey, J. (1974). A projection pursuit algo-            graphs with local multidimensional scaling. In Proceed-
   rithm for exploratory data analysis. IEEE Transactions on            ings of the 14th european symposium on artificial neural
   Computers, 23, 881-890.                                              networks (pp. 557–562).
Heer, J., Bostock, M., & Ogievetsky, V. (2010). A tour                Venna, J., Peltonen, J., Nybo, K., Aidos, H., & Kaski, S.
   through the visualization zoo. ACM Queue, 8(5).                      (2010). Information retrieval perspective to nonlinear di-
Jain, V., & Saul, L. (2004). Exploratory analysis and visu-             mensionality reduction for data visualization. Journal of
   alization of speech and music by locally linear embedding.           Machine Learning Research, 11(Feb), 451–490.
   In Proceedings of the international conference of speech,          Watts, D., & Strogatz, S. (1998). Collective dynamics of
   acoustics, and signal processing (Vol. 3, pp. 984–987).              small-world networks. Nature, 393, 440–442.
Landis, J. R., & Koch, G. G. (1977, March). The measure-              Weinberger, K., Sha, F., Zhu, Q., & Saul, L. (2007). Graph
   ment of observer agreement for categorical data. Biomet-             Laplacian regularization for large-scale semidefinite pro-
   rics, 33(1), 159–174.                                                gramming. In Advances in neural information processing
Lawrence, N. (2005). Probabilistic non-linear principal com-            systems (Vol. 19).
   ponent analysis with Gaussian process latent variable mod-         Wolpert, D. (1996). The lack of a priori distinctions between
   els. Journal of Machine Learning Research, 6(Nov), 1783–             learning algorithms. Neural Computation, 8, 1341-1390.
   1816.
                                                                  676

