UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Comparing the inductive biases of simple neural networks and Bayesian models
Permalink
https://escholarship.org/uc/item/2zc5f43q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Griffiths, Thomas
Austerweil, Joseph
Berthiaume, Vincent
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

   Comparing the inductive biases of simple neural networks and Bayesian models
                                         Thomas L. Griffiths (tom griffiths@berkeley.edu)
                                       Joseph L. Austerweil (joseph.austerweil@gmail.com)
                                       Vincent G. Berthiaume (vberthiaume@berkeley.edu)
                              Department of Psychology, University of California, Berkeley, CA 94720 USA
                                Abstract                                  to be found by training neural networks by gradient descent
   Understanding the relationship between connectionist and               comparable to those produced by Bayesian inference (that is,
   probabilistic models is important for evaluating the compati-          are the inductive biases of these approaches related)? Finally,
   bility of these approaches. We use mathematical analyses and           how compatible are the inductive biases of neural networks
   computer simulations to show that a linear neural network can
   approximate the generalization performance of a probabilis-            with those of structured probabilistic models? We provide
   tic model of property induction, and that training this network        positive answers to the first two questions, showing that a
   by gradient descent with early stopping results in similar per-        simple neural network can always approximate a probabilis-
   formance to Bayesian inference with a particular prior. How-
   ever, this prior differs from distributions defined using discrete     tic model of property induction, and that training this network
   structure, suggesting that neural networks have inductive bi-          using a gradient descent algorithm is similar to Bayesian in-
   ases that can be differentiated from probabilistic models with         ference with a particular prior distribution. However, we also
   structured representations.
                                                                          show that there remains a significant difference between this
   Keywords: Bayesian modeling, connectionism, inductive bi-
   ases, property induction                                               prior and distributions based on discrete representations.
                            Introduction                                                      Mathematical analysis
Cognitive scientists use different mathematical formalisms                Our mathematical analysis focuses on comparing the model
to model human cognition. Understanding the relationships                 of property induction used by Kemp and Tenenbaum (2009;
between these approaches is critical to resolving questions               henceforth KT09) with a linear neural network.
about the nature of the mind. Recently, researchers have de-              Setting up the problem
bated whether probabilistic or connectionist models of cog-
nition provide better prospects for making progress in cogni-             The KT09 model assumes that we want to capture the joint
tive science (Griffiths, Chater, Kemp, Perfors, & Tenenbaum,              distribution of the elements of continuous N-dimensional
2010; McClelland et al., 2010). One of the key issues in                  vectors x indicating the value of a single property for N ob-
this debate is that many probabilistic models are defined in              jects.1 This distribution, p(x), results from a diffusion pro-
terms of structured, discrete representations, while connec-              cess on a graph. The diffusion process induces a multivariate
tionist models use continuous, graded representations that can            Gaussian distribution on x with mean zero and covariance
mimic discrete structure when needed. A possible resolution                                                              −1
                                                                                                                      1
would be to view probabilistic and connectionist models as                                      Σdiscrete = ∆ + 2 I                                (1)
                                                                                                                     σ
lying at different levels of analysis (Marr, 1982), with neural
networks a continuous approximation to Bayesian inference                 where ∆ is the Laplacian of the graph, being G − I for a graph
over discrete representations. However, this requires estab-              with adjacency matrix G, and I is the identity matrix.
lishing whether such an approximation is possible.                            Now consider a linear neural network model.2 This model
   To explore this issue, we use the problem of property in-              represents an observed N × M matrix (the values of M prop-
duction as a case study for investigating the relationship be-            erties for N objects) as the product
tween probabilistic models of cognition and neural networks.
Property induction – inferring the properties of a novel ob-                                                X = YZ                                 (2)
ject based on the properties of other objects – has played a
                                                                          where X is the N × M matrix of observed objects, Y is an
key role in the debate between probabilistic and connection-
                                                                          N × K matrix, and Z is a K × M matrix. In this model, Z is
ist models. An influential probabilistic model explains hu-
                                                                          the representation of the set of properties on a hidden layer
man property induction in terms of Bayesian inference over
                                                                          with K units (as might be encoded in the weights from an
discrete representations such as graphs and trees (Kemp &
Tenenbaum, 2009), whereas a successful connectionist model                     1 This formulation is a little counter-intuitive, as the set of objects
explains people’s inferences via continuous representations               is fixed but the set of properties is left open (ie. new properties tend
                                                                          to be observed, rather than new objects). This differs from the most
learned by gradient descent (Rogers & McClelland, 2004).                  intuitive way of thinking about the problem for a neural network,
   We use a combination of mathematical analysis and com-                 in which the network is trained to predict the properties that objects
puter simulations to address three questions. First, can a prob-          have, with the set of properties fixed and the set of objects left open.
                                                                               2 Neural network models typically use non-linear activation func-
abilistic model with a discrete representation for a set of ob-
                                                                          tions at the hidden layer. This complicates the analysis, but we hope
jects be approximated by a neural network model with con-                 to explore the consequences of such non-linearities in future work.
tinuous representations? Second, are the solutions that tend              We return to this point in the Discussion.
                                                                      402

input layer to the hidden layer, with localist coding of proper-              approximate this outcome, with the approximation improv-
ties at the input layer) and Y encodes the relation of properties             ing as the number of samples M increases. Thus, the answer
over objects on the hidden layer.3 A single property vector is                to our first question is that the probabilistic model can be ap-
generated by multiplying the weight matrix, Y, by the vector                  proximated arbitrarily well by a neural network.
representing the property, z, to obtain x = Yz. The model is                     Establishing that our simple neural network with continu-
trained by finding weights Y and representations Z that min-                  ous representations can potentially approximate the general-
imize the error in reconstructing X.                                          ization performance of a probabilistic model using a discrete
                                                                              representation raises a different question: Will these models
Approximating generalization                                                  also perform similarly when learning those representations
It should be clear that the linear neural network can perfectly               from data? That is, if we train a neural network model on a
reproduce any observed matrix X, provided K is greater than                   finite number of samples from p(x), will it behave similarly
or equal to the rank of X. This follows simply by thinking                    to a probabilistic model that infers a discrete representation
about Equation 2 as a set of equations for the entries in X                   from the same data via Bayesian inference? This is a question
where the entries in Y and Z are free parameters – we can re-                 about the inductive biases of these two different approaches
produce X if we have enough free parameters to construct its                  to learning – those factors that lead a learning algorithm to
linearly independent columns. The more interesting question                   favor one solution over another. In the context of the prop-
is thus how the network will generalize. That is, what does it                erty induction problem, this question reduces to whether the
predict for a new property based on what it has learned from                  predictions produced by the neural network after training will
the observed properties?                                                      be similar to those resulting from Bayesian inference with a
    Analyzing generalization requires making assumptions                      particular prior distribution.
about the nature of the z vector for a novel property. If
we assume that z follows a multivariate Gaussian distribu-                    Gradient descent and Bayesian inference
tion with mean zero and covariance σ2z I, we can obtain some                  Gradient descent is a standard approach to training a neural
results that provide connections between the neural network                   network, where the weights are assigned small random values
and Bayesian approaches. This is a reasonable assumption                      and then modified in the direction indicated by the gradient of
if the weights from the localist node from an unobserved                      the error repeatedly for a fixed number of training iterations.
property to the hidden layer are assumed to be independently                  In this section, we summarize results showing that this learn-
drawn from a Gaussian distribution. This will be true if the                  ing algorithm behaves similarly to Bayesian inference with a
initial weights are drawn from a Gaussian, but as we show                     Wishart prior on covariance matrices.
below it is also consistent with the implicit prior assumed by                   For simplicity, we start by considering the problem of up-
gradient descent algorithms.                                                  dating z for a single property, keeping Y fixed. In this case
    We can determine the prediction the neural network will                   the goal is to find the z such that Yz minimizes the squared
make for a new property by asking how x is distributed given                  error in reconstructing the corresponding property vector x.
Y. Using standard Gaussian identities, x will be multivariate                 We can write the objective function as (x − Yz)T (x − Yz).
Gaussian with mean zero and covariance                                        Differentiating, we obtain the weight update rule
                         Σcontinuous = σ2z YYT                        (3)                             ∆z = ηYT (x − Yz)                       (4)
                                                                              where η is a learning rate (assuming simultaneous updates).
since x is a linear function of a Gaussian random variable.
                                                                                 For comparison with performing Bayesian inference, we
    Characterizing the distribution on x implied by this model
                                                                              can derive the estimate for z that we would obtain by as-
makes it straightforward to construct a condition under which
                                                                              suming a Gaussian prior and finding the posterior mean (or
the model produces the same joint distribution as a probabilis-
                                                                              the maximum a posteriori value, as they are the same in this
tic model based on any discrete graph structure: This will oc-
                                                                              case). The Bayesian estimate is
cur when Σdiscrete = Σcontinuous . This can be used to establish
a direct connection between the neural network’s representa-                                                     σ2x −1 T
tions for the objects and the graph Laplacian ∆. In particular,                                   ẑ = (YT Y +       I) Y x                   (5)
                                                                                                                 σ2z
Y can be obtained from the eigenvectors of ∆. If the network
is trained from a matrix X of property values sampled from                    where σ2x is the assumed noise variance in x.
p(x), then any learning algorithm that produces a represen-                      Inspecting these two equations, we can see that they use
tation corresponding to the principal components of X will                    two different forms of regularization – approaches to control-
     3 Since the model is linear, this interpretation can be “transposed”     ling the complexity of the solution found by learning. Neural
to give another interpretation, where Y is the hidden layer represen-         network training typically starts with weights close to zero, so
tation of the objects and Z the weights for the properties. This is a         weights grow over successive passes through the data. Stop-
more intuitive way of formulating the model and is also more con-             ping early keeps weights smaller. In the Bayesian solution,
sistent with connectionist models of these phenomena, as advocated
by Rogers and McClelland (2004). However, this interpretation is a            the ratio of σ2x to σ2z controls the size of the weights: If σ2z is
little harder to use to get intuitions about the results shown below.         small relative to σ2x (i.e., we are more confident in our prior
                                                                          403

beliefs than the observed data), the corresponding term can           each graph. The distributions on graphs we considered were
dominate the matrix that is inverted, reducing the weights            stochastic graph grammars that generate trees, chains, grids,
proportionally. Despite this difference in regularization style,      and partitions (Nagl, 1986; Kemp & Tenenbaum, 2008) and
there are cases where they will produce similar results: If z is      Erdös-Rényi random graphs (Erdös & Rényi, 1959).
close to zero and YT Y is close to cI, then ẑ will equal z after        Our analysis proceeded as follows. For each prior distri-
one pass of gradient descent with η = 1/(c + σσx2 ).
                                                  2
                                                                      bution over covariance matrices, we generated T samples of
                                                  z
                                                                      N × N covariance matrices Σ1 , . . . , ΣT . From each covariance
   More generally, it is possible to show that the solution pro-
                                                                      matrix, we sampled a N × M matrix X containing the values
duced by a linear neural network trained by gradient descent                                                                                   iid
with early stopping is equivalent to generating a Bayesian es-        of M features for each of the N objects (X = [x1 , . . . , xM ], xi ∼
timate with a Gaussian prior (Fleming, 1990; Santos, 1996).           N(0, Σ)). We then computed the marginal probability of these
When applied to Equation 4, these results indicate that fol-          samples under a Wishart distribution, integrating over its pa-
lowing this learning rule is equivalent to assuming a Gaussian        rameters. This let us determine how closely different priors
prior on z with mean zero and a covariance determined by Y            relate to the Wishart distribution.
and the number of iterations of learning.                                To compare the different approaches to learning, we ap-
   While the analysis presented so far has focused on Z, the          plied the neural network and Bayesian models to all of the
linearity of the network means that learning Y can be ana-            samples of X we had produced. We found YYT at differ-
lyzed in the same way. A Gaussian prior on Y implies that the         ent stopping points and compared this to the true covariance
implicit prior on YYT assumed by a neural network trained by          matrix for data generated from each of the different priors.
gradient descent with early stopping is a Wishart distribution,       The goal of this first analysis was to evaluate whether the
the distribution obtained by taking the product of two matri-         neural network performed best with data whose covariance
ces drawn from a multivariate Gaussian (Muirhead, 1982).              matrix was Wishart distributed. For the second analysis, we
                                                                      also obtained an estimate of the covariance matrix from each
Summary of mathematical results                                       sample using Bayesian inference with each of the different
The key results of the mathematical analyses presented in this        prior distributions and calculated the distance between these
section are that the generalization performance of the KT09           covariance matrices and YYT . This allowed us to examine
model can be approximated by a linear neural network model            how the distance between the solutions produced by the neu-
with continuous representations, and that the inductive bias          ral network and Bayesian inference was related to the extent
induced by training the neural network by gradient descent            to which the priors were similar to a Wishart distribution.
with early stopping should be similar to that of Bayesian in-         Calculating marginal Wishart probabilities
ference with a Wishart prior on covariance matrices. These
results make two clear predictions: Neural networks should            To perform our analysis, we must be able to calculate how
perform best when learning from data whose covariance ma-             close a distribution is to a Wishart. We did this using the
trices are Wishart distributed, and we should expect them to          marginal probability of a set of covariance matrices under a
perform more similarly to Bayesian models that use a Wishart          Wishart, integrating over the parameters of the distribution.
prior than to models with other priors.                               The result is a measure of the “Wishartiness” of the covari-
   These results also raise a question: How similar is the            ance matrices, which can be applied to samples from different
Wishart distribution to distributions that are based on discrete      distributions in order to evaluate their similarity to a Wishart.
representations? If the distributions are similar, then the in-          Assume we have a Wishart distribution with degrees of
ductive biases of neural networks and probabilistic models            freedom b and covariance center S, and that S is drawn from
with discrete representations will also be similar, meaning           an inverse-Wishart distribution with parameters a and Ψ. We
that these approaches need not be seen as lying in opposition         draw covariance matrices Σ1 , . . . , ΣT from this distribution.
to one another. If the distributions are different, then there        The marginal probability of Σ1 , . . . , ΣT given a, b, and Ψ is
are opportunities to empirically separate these accounts and                                              Z                  T
we cannot view simple neural networks as a scheme for ap-                    p(Σ1 , . . . , ΣT ) =            dS p(S|a, Ψ) ∏ p(Σt |b, S)
proximating the solutions identified by probabilistic models.                                                               t=1
                         Simulations                                  which yields
We explored the issues raised by our mathematical analy-
ses through simulations comparing the performance of neural                                        ΓN ( 21 (a + bT ))|Ψ|a/2 ∏t=1
                                                                                                                             T |Σ |(b−N−1)/2
                                                                                                                                  t
                                                                        p(Σ1 , . . . , ΣT )  =                                        (a+bT )/2
networks and Bayesian models with different prior distribu-                                       ΓN (a/2)(ΓN (b/2))T Ψ + ∑t=1  T Σ
                                                                                                                                    t
tions. The set of priors that we used included the Wishart
distribution as well as several distributions based on dis-          where ΓN (·) is the multivariate gamma function,
crete structures. Following the KT09 model, we included
                                                                                                                N
distributions on covariance matrices by defining a distribu-                            ΓN (x) = πN(N−1)/4 ∏ Γ(x + (1 − j)/2)
tion on graphs G and then deriving a covariance matrix for                                                     j=1
                                                                  404

                  Average distance of YYT from the true covariance matrix                and the probability of generating the proposed state from the
           250
                                                                                         current state and vice versa (Metropolis, Rosenbluth, Rosen-
           200                                                                           bluth, Teller, & Teller, 1953). This probability was annealed
                                                                                         by raising the probability to the power 1/τ, with τ decreasing
           150                                                                           according to a logarithmic schedule.
Distance                                                                                 Graph grammar priors. We used four priors based on
           100
                                                                                         graph grammars, defining distributions on graphs that corre-
            50                                                                           pond to trees, grids, chains, and partitions (Nagl, 1986; Kemp
                                                                                         & Tenenbaum, 2008). These random graph grammars are
             0                                                                           generative processes that start with a single node and then re-
              0             500             1000          1500              2000
                                      Training epochs                                    place a random node in the current graph with two nodes J
                                                                                         times, where J ∼ Geom(θ). Different graph structures result
Figure 1: Average distance between the true covariance ma-                               from using different rules for connecting the parents and chil-
trix and the covariance matrix learned by the neural network.                            dren of the old node to the new nodes (for the tree grammar,
                                                                                         there is also a latent node that cannot contain any objects),
                                                                                         and different rules for connecting the new nodes result in dif-
and Γ(x) is the generalized factorial function (Boas, 1983).                             ferent generated graph structures.4 Afterwards, the objects
This is the ratio of the normalization constants for a Wishart                           are assigned to nodes uniformly at random (except not to la-
and an inverse-Wishart distribution, due to conjugacy.                                   tent nodes). For example, if the rule for node replacement
                                                                                         does not create any edges, then the random graph grammar
Neural network learning
                                                                                         generates random partitions of the objects.
The linear neural network is defined by two matrices: a N ×K
                                                                                            To convert the graph to a covariance matrix, we follow
matrix Y that maps the properties into the latent space and a
                                                                                         Kemp and Tenenbaum (2008) by first forming an “entity”
K × M matrix Z that maps the latent space to the objects. We
                                                                                         graph containing N + L nodes, where the first N nodes rep-
trained the neural network by gradient descent on error, with
                                                                                         resent each object and are only directly connected with an
                              ∆y    = η(x − yZ)ZT                           (6)          edge to their assigned node. Second, we complete the “entity”
                                                                                         graph by connecting the last L nodes to each other accord-
and Equation 4 as the weight update rules. K was set to one                              ing to the result of the previous graph replacement process.
more than the rank of the object matrix X. The weights were                              Next, we form a N + L × N + L adjacency matrix W, where
initialized to normally distributed random values with mean                              1/wi j ∼ Exp(β) if there is an edge between nodes i and j (rep-
0 and variance 0.05. We used a learning rate η of 0.0025 and                             resenting how close nodes i and j are). Otherwise, wi j = 0.
2000 training epochs (full passes through the data), which                               This specifies a N + L × N + L covariance matrix for the mul-
were determined by pilot simulations. At each possible stop-                             tivariate Gaussian   distribution
                                                                                                                    −1 over the latent and observed
ping point (epoch), we recorded YYT . Figure 1 shows the
                                                                                                    
                                                                                                                 1
                                                                                         variables, E − W + σ2 I         where E is a N + K × N + K di-
average distance between YYT and the true covariance ma-
                                                                                         agonal matrix with eii = ∑ j wi j and I is the N + K × N + K
trix as a function of epoch, which initially decreases and then
                                                                                         identity matrix. The hidden nodes can be marginalized out
rises again due to overfitting.
                                                                                         analytically, resulting in the N objects being normally dis-
Priors and Bayesian inference                                                            tributed with covariance matrix given by the first N × N ele-
                                                                                         ments of the original covariance matrix.5 Bayesian inference
We considered eight different prior distributions, requiring us
                                                                                         was performed with code from http://charleskemp.com,
to use three different algorithms for Bayesian inference.
                                                                                         which uses stochastic search to find an estimated maximum a
Wishart prior. The first Bayesian model used a Wishart                                   posteriori covariance matrix for a given set of data.6
prior with covariance center I and degrees of freedom b =                                Erdös-Rényi priors. In addition to the four random graph
1000. Unfortunately the Wishart is not conjugate to the mul-                             generators from Kemp and Tenenbaum (2008), we used a
tivariate Gaussian, so we found an estimate of the covariance                            standard random graph generator: the Erdös-Rényi random
matrix under this prior using stochastic search with simu-                               graph (Erdös & Rényi, 1959). Each object is represented by
lated annealing. The state of the search (a covariance matrix)                           a node. Unlike the node replacement grammars, we gener-
was initialized to a random draw from the posterior distribu-
tion using an inverse-Wishart prior (for details, see Gelman,
                                                                                            4 For   simplicity, we assumed the graph structures are undirected.
Carlin, Stern, & Rubin, 1995). A new proposed state was
                                                                                            5 It is important to note that this is not equivalent to the first N ×N
then drawn from a Wishart distribution centered at the current
                                                                                         elements of the inverse covariance matrix.
state with b + N degrees of freedom. A Metropolis-Hastings                                   6 The parameters were set to β = 0.4 (edge length parameter),
acceptance rule was used to decide whether to replace the                                σ = 0.4 (covariance matrix regularization parameter), and θ = 1 −
                                                                                           2
current state with the proposed state, based on the product                              e−3 (simplicity bias), which are similar to the values used by Kemp
of two ratios of their (unnormalized) posterior probabilities                            and Tenenbaum (2008). We used the “45” speed setting.
                                                                                   405

ate random graphs by directly connect pairs of objects with                                  Average distance YYT from Bayesian estimators with different priors
an edge with probability p. Once the graph is generated,                                    55
the implied covariance matrix is found by the same proce-                                        ER (p = 0.9)
                                                                                            50        ER (p = 0.1)
dure as before (except we do not need to perform the ad-
ditional marginalization step as the initial covariance matrix                                   ER (p = 0.9)
                                                                        Bayes−NN distance
                                                                                            45
is already N × N). We considered priors corresponding to
p ∈ {0.1, 0.5, 0.9}. Covariance matrices with these priors                                  40
were estimated using stochastic search by simulated anneal-                                                                           Partition Grid
ing. The covariance matrix was initialized to a random Erdös-                              35                                                Tree Chain
Rényi covariance matrix and proposals were generated from
the current state by removing or deleting a random number of                                30
edges (such that the number of edges in the proposals were
binomially distributed). The search procedure and annealing                                 25
                                                                                                                                                           Wishart
schedule were otherwise the same as for the Wishart prior.
                                                                                            20
The distance between covariance matrices                                                    −3000      −2500      −2000      −1500         −1000       −500
                                                                                                                Log Wishart marginal likelihood
To analyze the results produced by the neural network and
Bayesian models, we needed a measure of the similarity
                                                                        Figure 2: Average (smallest possible) distance of YYT from
of two matrices. We used a distance metric between posi-
                                                                        the Bayesian estimates of the covariance matrix, plotted as a
tive definite matrices (valid covariance matrices) defined by
                                                                        function of the logarithm of the Wishart marginal likelihood
Förstner and Moonen (1999)
                            s                                           for the corresponding prior.
                                n
               d(Σ1 , Σ2 ) =   ∑ ln2 λi (Σ1 , Σ2 ),        (7)
                               i=1
                                                                        computed the distance between YYT and the Bayesian es-
where λi (Σ1 , Σ2 ) are the generalized eigenvalues of Σ1 and           timates for each object set, then averaged this quantity across
Σ2 , being the roots of |λΣ1 − Σ2 | = 0. When computing these           all object sets. The results are shown in the third row of
distances, we used the best stopping point for the neural net-          Table 1. As predicted, we found a negative correlation be-
work (the one resulting in minimal distance). Looking across            tween the distance between estimates and the extent to which
epochs, we found the value of YYT with the minimal distance             the corresponding prior is consistent with a Wishart distri-
to the true covariance matrix and to the eight covariance ma-           bution (as reflected by the marginal probabilities in the first
trices estimated by Bayesian models with different priors.              row of Table 1) with r = −0.92 and r = −0.83 for Pearson’s
                                                                        product-moment and Spearman’s rank-order correlation, re-
Simulation procedure and results                                        spectively.7 A scatterplot showing the relationship between
For each prior, we generated 101 data sets that each con-               these two quantities is shown in Figure 2.
sisted of T = 100 covariance matrices. From each matrix,                   The variation in how well the neural network approximated
we sampled the values of M = 100 features for N = 10 ob-                the Bayesian estimates with different prior distributions is
jects. We then computed the marginal probability of the co-             informative about the inductive biases of neural networks
variance matrices generated by each prior under the assump-             and structured probabilistic models. The neural network was
tion they were drawn from a Wishart distribution, with the              closest in performance to Bayesian inference with a Wishart
median result shown in the top row of Table 1. As expected,             prior, which is purely continuous. All priors based on dis-
the Wishart prior was the most compatible with a Wishart dis-           crete structure, in the form of an underlying graph, resulted
tribution. The discrete priors produced results that were rea-          in statistically significantly worse performance. Within these
sonably consistent with the Wishart distribution, while the the         discrete priors, those based on graph grammars were better
Erdös-Rényi generative processes produced results that were           approximated than the Erdös-Rényi priors. This pattern of
poorly characterized as Wishart. We used the data set with              results is interesting from the perspective of the debate be-
the median Wishart value for the subsequent analyses.                   tween probabilistic and connectionist accounts of property in-
   Next, we trained neural networks on the object set X gen-            duction, which has focused on discriminating the predictions
erated from each covariance matrix sampled from each of the             of probabilistic models using representations based on graph
eight priors, and computed the distance between YYT and the             grammars from neural networks. Our results suggest that this
true covariance matrix. The results are shown in the second             may be harder than discriminating probabilistic models that
row of Table 1. Performance was statistically significantly             assume arbitrary discrete structure, as in the Erdös-Rényi pri-
better when the true covariance matrices were drawn from                ors, from neural networks.
the Wishart, consistent with our mathematical analysis.
   Finally, we found Bayesian estimates of the covariance ma-               7 We confirmed that this correlation could not be fully explained
trix for each object set X using all eight priors. Stochas-             by the norm of the matrices, but plan on running further simulations
tic search was run for 20000 iterations in each case. We                to rule out other possible alternative explanations for our results.
                                                                  406

                 Table 1: Properties of different priors and comparison of gradient descent and Bayesian learning
                                                        Graph grammar priors                               Erdös-Rényi priors
                               Wishart          Grid       Chain       Tree       Partition          p = 0.1      p = 0.5      p = 0.9
     Marginal probability
          under Wishart        -567.87        -867.68     -884.95   -946.22       -1073.10          -2678.04     -2940.98     -2919.44
    Distance of  YYT   from
         true covariance        14.15a         33.31b      34.18b    32.36b        33.97b            33.93b       31.73b       33.35b
    Distance of YYT from
       Bayesian estimate        23.53a         36.04b      36.07b    36.03b        36.19b            50.21c       46.29d       53.23e
      Note: In each row, different superscripts indicate statistically significant differences in scores (Bonferroni p < .05).
                          Discussion                                  els may require going beyond simple neural networks that use
                                                                      general-purpose learning algorithms.
Our analysis of the relationship between probabilistic and            Acknowledgments. We thank Noah Goodman, Surya Ganguli, and
connectionist models in the context of property induction has         Jay McClelland for discussions and grant number FA-9550-10-1-
produced several interesting results. First, the generalization       0232 from the Air Force Office of Scientific Research and a fellow-
                                                                      ship from the Fonds de Recherche du Québec to VGB for funding.
performance of a probabilistic model with a discrete repre-
sentation can be approximated by an appropriately configured
linear neural network with continuous representations. Sec-                                        References
ond, training such a network by gradient descent with early           Boas, M. L. (1983). Mathematical methods in the physical sciences
                                                                         (2nd ed.). New York: Wiley.
stopping is similar to performing Bayesian inference over co-         Erdös, P., & Rényi, A. (1959). On random graphs, I. Publicationes
variance matrices with a Wishart prior. Finally, prior distribu-         Mathematicae, 6, 290-297.
tions that assume discrete structure vary in the extent to which      Fahlman, S. E., & Lebiere, C. (1990). The cascade-correlation learn-
                                                                         ing architecture. In Advances in Neural Information Processing
they resemble a Wishart prior, and this variation predicts how           Systems 2.
well Bayesian inference using those prior distributions is ap-        Fleming, H. E. (1990). Equivalence of regularization and truncated
proximated by a neural network. However, all prior distribu-             iteration in the solution of ill-posed image reconstruction prob-
                                                                         lems. Linear Algebra and its Applications, 130, 133-150.
tions using discrete structure that we considered resulted in         Förstner, W., & Moonen, B. (1999). A metric for covariance matri-
worse approximations than that given with a Wishart prior.               ces. In F. Krumm & V. S. Schwarze (Eds.), Qua vadis geodisa...?
                                                                         festschrift for Erik W. Grafarend on the occasion of his 60th birth-
   There are limitations in the analyses presented here that we          day (p. 113-128). Stuttgart, Germany: Stuttgart University.
hope to address in future work. As noted earlier, the assump-         Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995).
                                                                         Bayesian data analysis. New York: Chapman & Hall.
tion of linearity in the neural network deviates from stan-           Griffiths, T. L., Chater, N., Kemp, C., Perfors, A., & Tenenbaum,
dard practice in connectionist modeling. While we do not                 J. B. (2010). Probabilistic models of cognition: exploring rep-
expect that this will substantially change our results (given            resentations and inductive biases. Trends in Cognitive Sciences,
                                                                         14(8), 357-364.
that early stopping enforces small weights, effects of the non-       Kemp, C., & Tenenbaum, J. B. (2008). The discovery of structural
linearity should be minimized), further simulations should be            form. Proceedings of the National Academy of Sciences, USA,
conducted to confirm that this is the case. We would also like           105, 10687-10692.
                                                                      Kemp, C., & Tenenbaum, J. B. (2009). Structured statistical models
to explore more sophisticated learning algorithms, such as               of inductive reasoning. Psychological Review, 116(1), 20-58.
cascade correlation (Fahlman & Lebiere, 1990), which may              Marr, D. (1982). Vision. San Francisco, CA: W. H. Freeman.
result in different inductive biases.                                 McClelland, J. L., Botvinick, M. M., Noelle, D. C., Plaut, D. C.,
                                                                         Rogers, T. T., Seidenberg, M. S., et al. (2010). Letting struc-
   Returning to the questions that motivated our investigation,          ture emerge: connectionist and dynamical systems approaches to
our results provide a mixed set of answers as to the potential           cognition. Trends in Cognitive Sciences, 14(8), 348-356.
                                                                      Metropolis, A. W., Rosenbluth, A. W., Rosenbluth, M. N., Teller,
for neural networks to be viewed as a continuous approxi-                A. H., & Teller, E. (1953). Equations of state calculations by fast
mation to Bayesian inference over discrete representations.              computing machines. Journal of Chemical Physics, 21, 1087-
While specific neural networks can always be constructed that            1092.
                                                                      Muirhead, R. J. (1982). Aspects of multivariate statistical theory.
emulate the generalization performance of probabilistic mod-             New York: John Wiley & Sons.
els using discrete representations and the inductive biases of        Nagl, M. (1986). Set theoretic approaches to graph grammars.
neural networks can be expressed in a form that is consistent            In Proceedings of the 3rd international workshop on graph-
                                                                         grammars and their application to computer science (p. 41-54).
with Bayesian inference, these inductive biases are quite dif-           London, UK: Springer.
ferent from those of Bayesian models using priors defined on          Rogers, T., & McClelland, J. (2004). Semantic cognition: A parallel
discrete objects. Our results suggest that there is room to em-          distributed processing approach. Cambridge, MA: MIT Press.
                                                                      Santos, R. J. (1996). Equivalence of regularization and truncated
pirically separate these two approaches, and that identifying            iteration for general ill-posed problems. Linear Algebra and its
neural systems that can approximate arbitrary Bayesian mod-              Applications, 236, 25-33.
                                                                  407

