UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Interaction of Word Learning and Semantic Category Formation in Late Talking
Permalink
https://escholarship.org/uc/item/8rf426sw
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Nematzadeh, Aida
Fazly, Afsaneh
Stevenson, Suzanne
Publication Date
2012-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

  Interaction of Word Learning and Semantic Category Formation in Late Talking
                                    Aida Nematzadeh, Afsaneh Fazly, and Suzanne Stevenson
                                                         Department of Computer Science
                                                               University of Toronto
                                                    {aida,afsaneh,suzanne}@cs.toronto.edu
                               Abstract                                     of children also form different abstract knowledge of cate-
                                                                            gories, which causes differences in their word learning (as
   Late talkers (LTs) — children who show a marked delay in
   vocabulary learning — have also been shown to differ from                suggested by Jones & Smith, 2005; Colunga & Sims, 2011).
   normally-developing (ND) children with respect to the seman-                We investigate this possibility by examining within a com-
   tic organization of their learned vocabulary. We use a compu-            putational model the precise interaction between early word
   tational model of word learning to study how individual dif-
   ferences between LTs and NDs give rise to differences in ab-             learning and knowledge of semantic categories of words. We
   stract knowledge of categories emerging from learned words,              do so by extending an existing model of cross-situational
   and how this affects their subsequent word learning. Our re-             word learning (Fazly, Alishahi, & Stevenson, 2010). As in
   sults suggest that the vocabulary composition of LTs and NDs
   differ at least partially due to a deficit in the attentional abili-     Nematzadeh, Fazly, and Stevenson (2011), we simulate the
   ties of LTs, which also results in the learning of weaker abstract       difference between ND and LT learners as a difference in
   knowledge of semantic categories of words.                               the ability of the cross-situational learning mechanism to at-
                                                                            tend to appropriate semantic features for a word. Within this
                            Introduction                                    framework, we propose a new model that forms clusters of
Late talkers (LTs) are children with a marked delay in word                 words according to their learned semantic properties, and that
learning at an early age, some of whom go on to exhibit spe-                uses this knowledge in guiding the future associations be-
cific language impairment (SLI). Early identification of LTs                tween words and meanings. We show that the semantic clus-
at risk for SLI is especially important, since early intervention           ters of words are qualitatively very different for our ND and
can produce significant changes in the language development                 LT models; moreover, the two learners exhibit striking differ-
of these children (Desmarais, Sylvestre, Meyer, Bairati, &                  ences in terms of the usefulness of their learned clusters for
Rouleau, 2008). Many psycholinguistic studies have thus fo-                 subsequent word learning. Through computational modeling,
cused on understanding signs of late talking, as well as factors            we thus suggest an interaction between the impaired ability
contributing to it (Paul & Elwood, 1991; Thal, Bates, Good-                 of LTs to form informative abstract semantic categories, and
man, & Jahn-Samilo, 1997; Rescorla & Merrin, 1998; Ellis                    the observed delay in their vocabulary acquisition.
Weismer & Evans, 2002; Rowe, 2008; Stokes & Klee, 2009).
   An important observation about late-talking children is that                             The Computational Model
they seem to not only learn more slowly than their normally-                Overview of the Word Learning Model
developing (ND) peers, but also to learn differently. For
example, the vocabulary composition of LTs shows greater                    The model of Fazly et al. (2010) is a cross-situational learner
variability, e.g., in terms of how consistently certain proper-             that incrementally forms probabilistic associations between
ties, such as shape, are associated with particular categories,             words and their semantic properties. The input to a child is
such as solid objects (Jones & Smith, 2005; Colunga & Sims,                 simulated as a sequence of utterances (a set of words), each
2011). More generally, the vocabulary of LTs has been shown                 paired with a scene representation (a set of semantic features,
to exhibit less semantic connectivity than that of NDs (Sheng               representing what is perceived when the words are heard):
& McGregor, 2010; Beckage, Smith, & Hills, 2010). The                            Utterance: { she, drinks, milk }
greater variability and the weaker connectivity in the vocab-                    Scene: { ANIMATE, PERSON, FEMALE, CONSUME, DRINK,
                                                                                         SUBSTANCE, FOOD , DAIRY- PRODUCT }
ulary of LTs call for further investigation since they might be
reflective of underlying cognitive deficits in these children.              Given such an input pair, the model adjusts its probabilis-
   Psycholinguistic evidence suggests that children’s word                  tic representation of the meaning of each word. First, the
learning improves when they form some abstract knowledge                    model determines, based on its current probabilistic knowl-
about what kinds of semantic properties are relevant to what                edge of word–meaning associations, which semantic features
kinds of categories (Jones, Smith, & Landau, 1991; Colunga                  in the scene are more and less likely to be associated with
& Smith, 2005; Colunga & Sims, 2011). This abstract knowl-                  each word in the utterance. Using that assessment of word–
edge is argued to emerge by generalizing over the learned                   feature alignment in the current input, the model then updates
words. Stated otherwise, words that have been learned con-                  its probabilistic representation of the meaning of each word.
tribute to generalized abstract knowledge about word mean-                     In this way, the model uses cross-situational evidence to
ings and semantic categories, which then guide subsequent                   gradually improve its representation of the meaning of each
word learning. It is possible that because of the differences in            word w as a probability distribution, p(.|w), over all semantic
the vocabulary composition of LTs and NDs, the two groups                   features: i.e., p( f |w) is the probability of feature f being part
                                                                        2085

of the meaning of word w. At the heart of this process are             Learning Semantic Categories of Words
two calculations which we briefly summarize here (see Fazly            We extend the word learning model above by incorporating
et al., 2010, for more detail). The alignment probability deter-       the ability to form clusters of words based on their learned se-
mines how strongly a word w and a feature f are associated in          mantics, and to use the resulting semantic categories in subse-
the current (multi-word) utterance U at time t, in proportion          quent word learning. 2 These abilities represent a first step in
to the model’s current hypothesis of how likely the feature is         integrating the model’s word learning with formation of con-
part of the meaning of the word:                                       ceptual categories. These extensions to the model are key to
                                                                       further examination of the cognitive mechanisms that might
                   (t)           p(t−1) (f |w)                         underlie the weaker semantic connectivity observed in the vo-
                  aw (w|f ) =                                   (1)
                               ∑    p(t−1) (f |w0 )                    cabulary of LTs. Specifically, while Nematzadeh et al. (2011)
                               0
                              w ∈Ut                                    showed that learned words of their ND learner had greater se-
                                                                       mantic coherence than those in the LT learner, the model did
In order to collect this knowledge across all cross-situational
                                                                       not actually form semantic clusters of words, nor use seman-
uses of the word and feature, the model maintains an incre-
                                                                       tic relations among words to help in word learning.
mentally accumulated sum of these alignments that captures
                                                                          Our new model, at certain points in time (depending on the
the overall strength of the association between w and f :
                                                                       simulation), groups the words it has observed into clusters
                                                    (t)                based on the similarity among their learned meanings. Given
        assoc(t) (w, f ) = assoc(t−1) (w, f ) + aw (w| f )      (2)
                                                                       two words w and w0 , we determine their degree of semantic
   The second key formula to the operation of the model is the         similarity by treating their learned probability distributions
meaning probability that uses the association scores to update         over the semantic features, p(.|w) and p(.|w0 ), as input vec-
the meaning of each word after processing an input pair:               tors to the cosine function. These cosine values guide the
                                                                       grouping of words using a standard unsupervised hierarchi-
                            assoc(t) (f , w) + λ(t)                    cal clustering method. The clusters of semantically related
         p(t) (f |w) =                                          (3)    words can then be analyzed to see how the factors that simu-
                          ∑  assoc(t) (f 0 , w) + β · λ(t)
                         0
                                                                       late ND and LT learners in the model contribute to different
                        f ∈M
                                                                       quality of semantic categorization, as observed by Sheng and
where β is the number of expected distinct features, M is              McGregor (2010) and Beckage et al. (2010), among others.
the subset of those features that have been observed, and λ(t)            Moreover, the semantic clusters enable us to build further
is a smoothing function which we formulate in a way that               on the explanation of late talking as arising from attentional
captures the developing ability of the model to attend to input,       differences in learners (as proposed in Nematzadeh et al.,
as follows.                                                            2011). Specifically, we assume that learned semantic cate-
   Research has shown that children’s ability to attend to rel-        gories enable children to generalize their knowledge of re-
evant features of a perceived scene improve over time (e.g.,           lated words, which can help focus subsequent word learn-
Mundy et al., 2007). Moreover, LTs have been observed to               ing on relevant semantic features in the input. In our model,
show difficulty with the communicative abilities that enable           knowledge about the semantic category of a word can be used
children to direct appropriate attention on relevant aspects           as an additional source of information about which seman-
of a scene (e.g., Rescorla & Merrin, 1998). In recent work             tic features are more likely to be aligned with the word in a
(Nematzadeh et al., 2011), we demonstrated that we can use             given input. For example, features such as EDIBLE and FOOD
the λ(t) function to simulate how quickly or slowly the atten-         should be more strongly aligned to a word referring to a kind
tional abilities of a learner develop over time. Specifically,         of fruit than to a word referring to a kind of vehicle.
the λ(t) function determines how much weight is given to un-              We achieve this in our model by aligning a word w and a
observed word–feature pairs, with greater weight reflecting            feature f in an input utterance–scene pair according to both
immature attentional skills in which the learner fails to focus        word-level and category-level information, the latter drawing
on the observed (appropriate) meaning features. In the model,          on the incrementally created semantic clusters. We adopt the
λ(t) is designed to decrease over time, to simulate gradually          formulation used by Alishahi and Fazly (2010) to combine
improving attentional processes that can appropriately focus           word and category information in the alignment probability: 3
on the observed word–feature pairs. We modeled the differ-                                        (t)                    (t)
ence between ND and LT learners by having a λ(t) function                     a(t) (w| f ) = ω · aw (w| f ) + (1 − ω) · ac (w| f )        (4)
for the latter that decreases much more slowly, corresponding              2 We continue to refer to the clusters that our model learns both
to delayed development of appropriate attention to the input.          as clusters, to emphasize that they are learned in an unsupervised
Here we adopt that same formulation,1 but extend the model             manner, and as semantic categories, to emphasize their connection
                                                                       to children’s knowledge of abstract categories.
as follows to consider the role of attention and its interaction           3 The approach of Alishahi and Fazly (2010) differs from ours:
with semantic category formation in word learning.                     (1) They examine the role of syntactic categories (e.g., noun or verb)
                                                                       in word learning while we look at semantic categories. (2) They use
    1 Our ND and LT simulations here use the same settings for λ(t)    predefined correct assignments of words to such parts of speech, but
as what we referred to as ND and LT.5 in our previous work.            our clustering is based on the model’s learned semantic knowledge.
                                                                   2086

          apple: { FOOD:1, SOLID:.72, · · · , PLANT- PART:.22,             psycholinguistically-plausible set of features for this purpose
                  PHYSICAL - ENTITY :.17, WHOLE :.06, · · · }
                                                                           (Howell et al., 2005); however, they were only available for
 Figure 1: Sample true meaning features & their scores for apple.          a limited number of nouns. Here we develop an improved
                                                       (t)
The first component of the above formula, aw (w| f ) is the                semantic representation for nouns that enables a more exten-
word-based alignment, given in Eqn. (1) above. The second                  sive test of our clustering method and associated processing
                  (t)                                                      involving semantic relatedness among words.
component, ac (w| f ), is an analogous category-based align-
ment (described below). The ω term is a weight (between                        We construct the lexical entry t(w) for each noun w draw-
0 and 1) that determines the relative contribution of the two              ing on WordNet5 as follows. For each synset in WordNet,
alignments; here we use a balanced weighting of 0.5.                       we select one member word to serve as the semantic feature
   Where the word-based alignment captures the association                 representing that synset. The initial representation of t(w)
between a feature and a single word, the category-based                    consists of the set of such features from each ancestor (hyper-
                (t)
alignment, ac (w| f ), assesses the overall association between            nym) of the word’s first sense in WordNet.6 We use the same
the feature f and the words in cluster(w), the cluster assign-             features as in previous work to initialize t(w) for other parts
ment determined by the model for word w. This alignment                    of speech (Nematzadeh et al., 2011; Alishahi & Fazly, 2010).
is calculated by replacing occurrences of p( f |w) in Eqn. (1)                 To complete the representation of t(w), we need a score for
with p( f |cluster(w)). We again follow Alishahi and Fa-                   each feature which can be used in the probabilistic generation
zly (2010) in defining p( f |cluster(w)) as the average of the             of a scene for an utterance containing w. We assume that gen-
meaning probabilities of the words in the cluster:                         eral features such as ENTITY, that appear with many words,
                                                                           are less informative than specific features such as FOOD, that
                                  1                                        appear with fewer words. Hence, we aim for a score that gives
   p(t) ( f |cluster(w)) =                      ∑ p(t) ( f |w) (5)
                            |cluster(w)| w∈cluster(w)                      a higher value to the more specific features, so that more in-
                                                                           formative features are generated more frequently.
where |cluster(w)| is the number of words in the cluster.                      We formulate such a score by forming semantic groups
                                                                           of words, and determining for each group the strength and
         Semantic Representation in the Model
                                                                           specificity of each feature within that group; multiplying these
The Representation of a Scene                                              components gives the desired assessment of the feature’s in-
The input data for our model consists of a set of utterances               formativeness to that group of words. 7
paired with their scene representations. As in Nematzadeh                      First, we form noun groups by using the labels provided
et al. (2011), the utterances are bags of lemmatized words,                in WordNet that indicate the semantic category of the sense;
taken from the child-directed speech (CDS) portion of the                  e.g., the first sense of apple is in category noun.food. (For
Manchester corpus (Theakston et al., 2001, from CHILDES                    words other than nouns, we form single-member groups con-
MacWhinney, 2000). The corpus is transcripts of conversa-                  taining that word only.) Next, for each feature f in t(w) for
tions with 12 British children, ages 1;8 to 3;0. We use half the           a word w in group g, the score is calculated by multiplying
data as the development set, and the rest for final evaluations.           strength( f , g) and specificity( f ):
   The corresponding scene representation for each utterance
must be artificially generated, since no semantic annotation                                                      count( f , g)
                                                                                         strength( f , g) =
of the contextual scene exists for any large corpus of CDS.                                                   ∑ count( f 0 , g)
First, we create an input-generation lexicon containing the                                                  f 0 ∈g
“true” meaning t(w) for each word w in our corpus: t(w) is                                                               |G|
a vector over a set of semantic features, each associated with                              specificity( f ) = log
                                                                                                                     |g : f ∈ g|
a score. An example lexical entry is given in Figure 1; the
creation of this lexicon is described below.4 Next, to generate            where |G| is the total number of groups, and |g : f ∈ g| is the
the scene S for an utterance U, we probabilistically sample                number of groups that f appears in; strength( f , g) captures
an observed subset of features from the full set of features in            how important feature f is within group g (its relative fre-
t(w) for each word w ∈ U. This imperfect sampling allows us                quency among features within g); specificity( f ) reflects how
to simulate the noise and uncertainty in the input, as well as             specific a feature is to a group or small number of groups,
the uncertainty of a child in determining the relevant mean-               with larger values indicating a more distinctive feature. For
ing elements in a scene. The scene S is the union of all the               each word w, each feature f in t(w) is associated with the
features sampled for all the words in the utterance.                       score for f and g (where w ∈ g); the resulting scores are then
                                                                               5 http://wordnet.princeton.edu
The Representation of Word Meaning
                                                                               6 A native speaker of English annotated a sample of 500 nouns
We focus on the semantics of nouns, since they are central                 with their most relevant sense in our CDS corpus, revealing that the
to work on the role of category knowledge in word learn-                   first WordNet sense was appropriate for 80% of the nouns. One
ing. Our previous work (Nematzadeh et al., 2011) used a                    regular exception was nouns with both ‘plant’ and ‘food’ senses,
                                                                           such as broccoli, which were predominantly referring to food. For
    4 It should be emphasized that the input-generation lexicon is not     these, we always use the ’food’ sense.
used for learning by the model; it is used only to create the input.           7 Our score is inspired by the tf-idf score in information retrieval.
                                                                       2087

re-scaled so that the maximum score is 1, to be appropriate                      TRUE                 ND                  LT
for the probabilistic generation of the input scenes.                        P      R     F     P      R       F     P     R      F
                                                                            .77    .71 .66     .79    .53     .51   .88   .19    .24
                    Experimental Results
In our previous work (Nematzadeh et al., 2011), we showed              Table 1: Average P, R, and F scores (shown in boldface), for the
                                                                       TRUE, LT and ND clusters after processing 15K input pairs.
in computational simulations that LT learners not only learn
fewer words than an ND learner, but that the LTs also have a           R scores, compared to the TRUE clusters. By contrast, the LT
less semantically-connected vocabulary, a result in line with          clusters have a very low F score. These results confirm that,
the findings of Beckage et al. (2010). Here, using our ex-             in contrast to the ND learner, the LT learner is unable to use
tended model with its improved semantic representation, we             its learned knowledge of word meanings to form reasonable
first analyze the learned clusters of words for our two learn-         categories of words, confirming that nouns in the vocabulary
ers, to confirm that the semantic category knowledge of the            of the LT learner have less semantic coherence than those of
LT learner is of substantially poorer quality. We also inves-          our ND learner. Moreover, the unusual nature of the clus-
tigate the differential effects of the learned clusters for the        ters formed by the LT learner (in contrast with ND) is further
two learners in subsequent word learning. It is known that             confirmed by its very high P and very low R scores compared
word learning in children is boosted by their knowledge of             to the TRUE clusters. Detailed examination of the clusters
word categories (Jones et al., 1991). Here, we interleave the          reveals that LT has learned a large number of small clusters
two processes of semantic clustering and word learning in              (leading to high precision), but also a few large semantically-
our model, and examine the patterns of word learning over              incoherent clusters (leading to very low recall).
time, for the two learners, with and without category knowl-
edge. Our hypothesis is that the ND learner not only forms             Incorporating Categories in Word Learning
higher quality semantic clusters of words compared to the LT
learner, but that its (more coherent) category knowledge con-          Here we investigate the role of category formation in a natu-
tributes to improved word learning over time.                          ralistic word learning setting. Specifically, we interleave the
                                                                       two processes by allowing the model to use its semantic clus-
Analysis of the Learned Clusters                                       ters in word learning. To simulate the simultaneous learning
We examine the quality of the semantic clusters formed by              of categories and word meanings, the model builds clusters
each learner (ND and LT). We train the learners on 15K                 from its learned noun meanings after processing every 1000
utterance–scene pairs, and perform a hierarchical clustering           input utterance–scene pairs. It then uses these clusters when
on the resulting learned meanings of all the observed nouns.           processing the next 1000 pairs (at which point a new set of
To provide a realistic upperbound as a point of comparison             clusters is learned). After the first 1000 input pairs, the model
for the two learners, we also cluster (using the same clus-            calculates the alignment probabilities using both word-based
tering algorithm and similarity measure) the true meanings             and category-based knowledge, as in Eqn. (4).
of the nouns. These “TRUE” clusters indicate how well the                 For each noun in an utterance, if it has been observed prior
nouns can be categorized by the clustering method on the ba-           to the last clustering point, the model uses the cluster con-
sis of their true (in contrast to learned) meanings. In all cases,     taining the noun to calculate the category-based alignment.
we set the number of clusters to 20, which is the approximate          But a novel (previously unobserved) noun has not yet been
number of the actual WordNet categories for nouns.                     assigned to a cluster. However, it is recognized that children
   To measure the overall goodness of each of the three sets           can use contextual linguistic cues to infer the general seman-
of clusters (TRUE, ND, and LT), we compare the clustering              tic properties of a verbal argument (Nation et al., 2003). For
to the actual WordNet category labels for the nouns, as fol-           example, a child/learner knowing the verb eat might be able
lows. (The WordNet category labels reflect human judgments             to infer that the novel word dax in “she is eating a dax” is
of semantic categories, since they are provided by manual an-          likely referring to some ‘edible thing’. We assume here that
notation.) We first label each cluster c with the most frequent        a learner can use the context of a novel noun to identify its
category assigned by WordNet to the words in that cluster,             general semantic category. In our model, we simulate this
called label(c). We then measure P(recision), R(ecall), and            inference process by giving the model access to the Word-
their harmonic mean, F(-score), for each cluster, and average          Net category label of the novel word. Recall that each noun
these over all clusters in a set. Given a cluster c, P measures        sense in WordNet is assigned a category label that provides
the fraction of nouns in c whose WordNet category matches              information about its general semantics. The model can then
the cluster label; R is the fraction of all nouns whose WordNet        choose a learned cluster for the novel noun by identifying the
category is label(c) that are also in c. We report the average P,      cluster whose assigned label matches the WordNet category
R, and F scores for the TRUE, LT, and ND clusters in Table 1.          of the noun. If more than one cluster has the same label as
   As expected, the F score is the highest for the TRUE clus-          the category of the novel word, the cluster with the highest
ters, which result from the same clustering algorithm but ap-          precision is selected. If the learner does not have a matching
plied to noise-free semantic representations. In comparison,           cluster, no category information is used for the novel word.
the ND learner has somewhat lower F scores, as well as P and              We process 15K input pairs overall, and look at the aver-
                                                                   2088

Figure 2: Change in the average Acq score of all nouns over time;            Figure 3: Changes in the novel word learning over time
ND-CAT and LT-CAT use category formulation during learning.
age acquisition score (Acq, defined below) of nouns for each          be the unknown animal.) We predict a substantial benefit of
learner, with and without category knowledge, as a function           category knowledge when observing a word for the first time,
of time (the number of input pairs processed); see Figure 2.          since this is when there’s the least cross-situational informa-
The Acq score for a word w shows how similar its learned              tion available to a learner about the particular word and its
meaning l(w) is to its true meaning t(w):                             features. Here we examine the effect of category knowledge
                                                                      on the learning of novel words over time, within the naturalis-
                Acq(w) = sim(l(w),t(w))                       (6)     tic setting of the utterance–scene pairs of our corpus, focusing
                                                                      on those inputs that include previously unseen words.
where sim is the cosine similarity between the two vectors.              We train the model on 15K input pairs, but restrict eval-
   A comparison of the curves in Figure 2 reveals several in-         uation to the learning of novel words. Specifically, we look
teresting patterns. First, the use of category knowledge sub-         at the difference in the Acq score of words at their first expo-
stantially improves the word learning performance of ND,              sure, for the ND and LT learners, each with and without using
whereas it has no effect at all on the (poorer) performance           category knowledge. To do this, we look at utterances con-
of the LT learner. These results further elaborate the findings       taining at least two nouns, at least one of which is novel.8 For
of our analysis of the learned clusters: the clusters learned         each such input utterance, we record the resulting Acq score
by the ND are a better match than those of the LT with the            of all novel words in the utterance, and take their average. For
manually-annotated categories provided by WordNet; more-              each learner, we also examine the pattern of change in these
over, they are able to contribute helpful information to word         average scores over time, as shown in Figure 3.
learning, where the LT clusters are not.                                 The results show that after 2K input utterances, there is no
   Thus, the LT clusters are not only in principle of lesser          difference between using and not using categories for each
quality, they are in practice less useful. Also, the positive ef-     of the learners (i.e., comparing ND-CAT and LT-CAT to ND
fect of category knowledge for ND increases over time, sug-           and LT, respectively). This is because none of the learners
gesting that the quality of its clusters improves as the model        has formed sufficiently good categories yet. After 8K utter-
is exposed to more input. This mutually reinforcing effect of         ances, ND-CAT performs much better than ND, showing the
semantic category formation with word learning underscores            benefit of using category knowledge in learning novel words
the importance of studying the interaction of the two.                in an ambiguous setting. By contrast, for the LT learner, the
Category Knowledge in Novel Word Learning                             Acq score of the novel nouns does not increase when using
                                                                      category information (LT-CAT) even with additional expo-
Results of the previous section suggest that the ability of           sure to the input. Another interesting pattern is that for the
a learner to form reliable categories of semantically-similar         ND learner, the average Acq score does not increase between
words may be closely tied to its word learning performance.           8K and 15K input utterances. However, when using cate-
In particular, we expect category knowledge to increase the           gories (ND-CAT), this score increases over time. Although
likelihood of associating a word with its relevant semantic           the ND model has learned additional words after 15K inputs,
features when there is ambiguity and uncertainty in the cross-        knowledge of more words alone does not result in improved
situational evidence. For example, when a child hears “The            learning of novel words. By contrast, the increasing seman-
wug will drink the dax” while observing an unknown animal             tic category knowledge in ND-CAT over time leads to greater
and a bowl of liquid in the scene, the child must rely on in-         improvements in learning the meaning of novel nouns.
formation sources other than the cross-situational evidence to
infer the possible meanings of the two novel words. (That is,             8 If the utterance only has 1 novel noun, the task is too easy be-
the child must infer that wug as a drinker is more likely to          cause the features of nouns and other parts of speech do not overlap.
                                                                  2089

                         Conclusions                                  expectations about kinds: A role for associative learning.
                                                                      Psychological Review, 112(2), 347–382.
One possible explanation for the language deficiencies of
                                                                    Desmarais, C., Sylvestre, A., Meyer, F., Bairati, I., &
late-talking children is inadequacies in their attentional and        Rouleau, N. (2008). Systematic review of the literature on
categorization abilities (Jones & Smith, 2005; Colunga &              characteristics of late-talking toddlers. Int’l J. of Language
Sims, 2011). In this paper, we have investigated (through             and Communication Disorders, 43(4), 361–389.
computational modeling) two interrelated issues: (1) how            Ellis Weismer, S., & Evans, J. L. (2002). The role of process-
variations in the development of attentional abilities in             ing limitations in early identification of specific language
normally-developing (ND) and late-talking (LT) children               impairment. Topics in Language Disorders, 22(3), 15–29.
may interact with their categorization skills, and (2) how dif-     Fazly, A., Alishahi, A., & Stevenson, S. (2010). A probabilis-
ferences in semantic category formation could affect word             tic computational model of cross-situational word learning.
learning. We have extended a model of word learning that in-          Cognitive Science, 34(6), 1017–1063.
corporates an attention mechanism (Nematzadeh et al., 2011)         Howell, S. R., Jankowicz, D., & Becker, S. (2005). A model
to incrementally cluster words, and to use these semantic             of grounded language acquisition: Sensorimotor features
clusters in subsequent word learning.                                 improve lexical and grammatical learning. J. of Memory
   Psycholinguistic findings have noted that the vocabulary of        and Language, 53, 258–276.
LTs shows both a lack of appropriate generalization (Jones &        Jones, S., & Smith, L. B. (2005). Object name learning and
Smith, 2005; Colunga & Sims, 2011), and less semantic con-            object perception: a deficit in late talkers. J. of Child Lan-
nectivity (Beckage et al., 2010; Sheng & McGregor, 2010).             guage, 32, 223–240.
We find here that the clusters formed by our LT model indeed        Jones, S., Smith, L. B., & Landau, B. (1991). Object proper-
show more inconsistency and less coherence compared to our            ties and knowledge in early lexical learning. Child Devel-
ND learner. In addition, unlike our LT learner, our ND model          opment, 62(3), 499–516.
can use its learned knowledge of word meanings to form              MacWhinney, B. (2000). The CHILDES project: Tools for
semantically-coherent and informative categories, which in            analyzing talk (3rd ed., Vol. 2: The Database). Erlbaum.
turn contribute to an improvement in subsequent word learn-         Mundy, P., Block, J., Delgado, C., Pomares, Y., Hecke,
ing. Moreover, the LT learner has particular difficulties in          A. V. V., & Parlade, M. V. (2007). Individual differences
learning novel words, while the ND learner gets increasingly          and the development of joint attention in infancy. Child
                                                                      Development, 78(3), 938–954.
better over time when it draws on category knowledge. The
                                                                    Nation, K., Marshall, C. M., & Altmann, G. T. (2003). Inves-
inability of an LT learner to form reasonable semantic clus-
                                                                      tigating individual differences in children’s real-time sen-
ters limits its ability to generalize its knowledge of learned
                                                                      tence comprehension using language-mediated eye move-
words to new words. This could be a substantial factor in the
                                                                      ments. J. Experimental Child Psychology, 86, 314–329.
LT’s delayed vocabulary acquisition.
                                                                    Nematzadeh, A., Fazly, A., & Stevenson, S. (2011). A com-
   The model presented here treats semantic category learn-           putational study of late talknig in word-meaning acquisi-
ing and word learning as two interacting but independent pro-         tion. In Proc. of CogSci’11.
cesses. In particular, the mechanism for incorporating cate-        Paul, R., & Elwood, T. J. (1991). Maternal linguistic input to
gory knowledge into word learning simply adds this knowl-             toddlers with slow expressive language development. J. of
edge as another factor in guiding the formation of word–              Speech, Lang., & Hearing Research, 34, 982–988.
feature associations. Our ongoing work is exploring a unified       Rescorla, L., & Merrin, L. (1998). Communicative intent in
mechanism in which category knowledge is integrated into              late-talking toddlers. Applied Psycholing., 19, 398–414.
the attentional mechanism of the word learning model. Such          Rowe, M. L. (2008). Child-directed speech: relation to so-
an approach will enable us to further explore how specific cor-       cioeconomic status, knowledge of child development and
relations between semantic properties and abstract categories         child vocabulary skill. J. of Child Language, 35, 185–205.
(such as shape–solid object) emerge from the input (for LTs         Sheng, L., & McGregor, K. K. (2010). Lexical–semantic
and NDs), and how these affect subsequent word learning.              organization in children with specific language impairment.
                                                                      J. of Speech, Lang., & Hearing Research, 53, 146–159.
                          References                                Stokes, S. F., & Klee, T. (2009). Factors that influence vo-
                                                                      cabulary development in two-year-old children. J. of Child
Alishahi, A., & Fazly, A. (2010). Integrating syntactic knowl-
                                                                      Psychology, 50(4), 498–505.
   edge into a model of cross-situational word learning. In
                                                                    Thal, D. J., Bates, E., Goodman, J., & Jahn-Samilo, J. (1997).
   Proc. of CogSci’10.
                                                                      Continuity of language abilities: An exploratory study of
Beckage, N., Smith, L. B., & Hills, T. (2010). Semantic net-
                                                                      late- and early-talking toddlers. Developmental Neuropsy-
   work connectivity is related to vocabulary growth in chil-
                                                                      chology, 13(3), 239–273.
   dren. In Proc. of CogSci’10.
                                                                    Theakston, A. L., Lieven, E. V., Pine, J. M., & Rowland, C. F.
Colunga, E., & Sims, C. (2011). Early talkers and late talkers
                                                                      (2001). The role of performance limitations in the acquisi-
   know nouns that license different word learning biases. In
                                                                      tion of verb–argument structure: An alternative account. J.
   Proc. of CogSci’11.
                                                                      of Child Language, 28, 127–152.
Colunga, E., & Smith, L. B. (2005). From the lexicon to
                                                                2090

