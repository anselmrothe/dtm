UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Using listener gaze to augment speech generation in a virtual 3D environment
Permalink
https://escholarship.org/uc/item/7vn7t95m
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Staudte, Maria
Koller, Alexander
Garoufi, Konstantina
et al.
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

     Using listener gaze to augment speech generation in a virtual 3D environment
         Maria Staudte                      Alexander Koller                  Konstantina Garoufi                Matthew Crocker
     Saarland University                  University of Potsdam              University of Potsdam              Saarland University
                              Abstract                                   ments. A natural language generation (NLG) system, that ex-
   Listeners tend to gaze at objects to which they resolve referring     ploited this information to provide direct feedback, commu-
   expressions. We show that this remains true even when these           nicated its intended referent to the listener more effectively
   objects are presented in a virtual 3D environment in which lis-       than similar systems that did not draw on listener gaze. Gaze-
   teners can move freely. We further show that an automated
   speech generation system that uses eyetracking information            based feedback was further shown to increase listener atten-
   to monitor listener’s understanding of referring expressions          tion to potential target objects in a scene, indicating a gen-
   outperforms comparable systems that do not draw on listener           erally more focused and task-oriented listener behavior. This
   gaze.
                                                                         system is, to our knowledge, the first NLG system that adjusts
                          Introduction                                   its referring expressions to listener gaze.
In situated spoken interaction, there is evidence that the gaze          Related work
of interlocutors can augment both language comprehension
and production processes. For example, speaker gaze to ob-               Previous research has shown that listeners align with speak-
jects that are about to be mentioned (Griffin & Bock, 2000)              ers by visually attending to mentioned objects (Tanenhaus
has been shown to benefit listener comprehension by direct-              et al., 1995) and, if possible, to what the speaker attends to
ing listener gaze to the intended visual referents (Hanna &              (Richardson & Dale, 2005; Hanna & Brennan, 2007; Staudte
Brennan, 2007; Staudte & Crocker, 2011; Kreysa & Knoe-                   & Crocker, 2011). Little is known, however, about speaker
ferle, 2011). Even when speaker gaze is not visible to the               adaptation to the listener’s (gaze) behavior, in particular when
listener, however, listeners are known to rapidly attend to              this occurs in dynamic and goal-oriented situations. Typi-
mentioned objects (Tanenhaus, Spivey-Knowlton, Eberhard,                 cally, Visual World experiments have used simple and static
& Sedivy, 1995). This gaze behavior on the part of listeners             visual scenes and disembodied utterances and have analyzed
potentially provides speakers with useful feedback regarding             the recorded listener gaze off-line (e.g., Altmann & Kamide,
the communicative success of their utterances: By monitor-               1999; Knoeferle, Crocker, Pickering, & Scheepers, 2005).
ing listener gaze to objects in the environment, the speaker             Although studies involving an embodied speaker inherently
can determine whether or not a referring expression (RE) they            include some dynamics in their stimuli, this is normally con-
have just produced was correctly understood or not, and po-              strained to speaker head and eye movements (Hanna & Bren-
tentially use this information to adjust subsequent production.          nan, 2007; Staudte & Crocker, 2011). Besides simplifying
   In this paper we investigate the hypothesis that speaker              the physical environment to a static visual scene, none of
use of listener gaze can potentially enhance interaction, even           these approaches can capture the reciprocal nature of interac-
when situated in complex and dynamic scenes that simulate                tion. That is, they do not take into account that the listeners’
physical environments. In order to examine this hypothesis in            eye movements may, as a signal of referential understanding
a controlled and consistent manner, we monitor listener per-             to the speaker, change the speaker’s behavior and utterances
formance in the context of a computer system that generates              on-line and, as such, affect the listener again.
spoken instructions to direct the listener through a 3D virtual             One study that emphasized interactive communication in
environment with the goal of finding a trophy. Successful                a dynamic environment was conducted by Clark and Krych
completion of the task requires listeners to press specific but-         (2004). In this experiment, two partners assembled Lego
tons. Our experiment manipulated whether or not the com-                 models: The directing participant advised the building par-
puter system could follow up its original RE with feedback               ticipant on how to achieve that goal. It was manipulated
based on the listener’s gaze or movement behavior, with the              whether or not the director could see the builder’s workspace
aim of shedding light on the following two questions:                    and, thus, use the builder’s visual attention as feedback for
                                                                         directions. Clark and Krych found, for instance, that the vis-
• Do listener eye movements provide a consistent and useful              ibility of the listener’s workspace led to significantly more
   indication of referential understanding, on a per-utterance           deictic expressions by the speaker and to shorter task com-
   basis, and when embedded in a dynamic and complex,                    pletion times. However, the experimental setting introduced
   goal-driven scenario?                                                 large variability in the dependent and independent variables,
                                                                         making controlled manipulation and fine-grained observa-
• What effect does gaze-based feedback have on listeners’
                                                                         tions difficult. In fact, we are not aware of any previ-
   (gaze-)behavior and does it increase the more general ef-
                                                                         ous work that has successfully integrated features of natu-
   fectiveness of an interaction?
                                                                         ral environments—realistic, complex and dynamic scenes in
   We show that the listeners’ eye movements are a reliable              which the visual salience of objects can change as a result of
predictor of referential understanding in our virtual environ-           the listener’s moves in the environment—with the reciprocal
                                                                     1007

nature of listener-speaker-adaptation while also being able to
carefully control and measure relevant behavioral data.
   Recently, researchers have examined eye gaze of speakers
and listeners in the scenes of Tangram puzzle simulations on
computer screens (Kuriyama et al., 2011; Iida, Yasuhara, &
Tokunaga, 2011). In these experiments, eye gaze features are
found to be useful for a machine learning model of reference
resolution. However, this setting is restricted in its dynam-
ics, as it does not embed the objects into physical scenes or
involve any updates to the spatial and visual context of the
objects in the scenes. In contrast, by generating REs and ask-
ing the subjects to resolve them, rather than resolving human-
produced REs itself, the system we propose here can provide
more control over the language that is used in the interaction.
                                                                     Figure 1: A screenshot of one of the virtual 3D environments.
   Computational models of gaze behavior are frequently im-
plemented in embodied conversational agents as part of non-
verbal behavior that aims at improving the human-computer            hidden in a wall safe. They must press certain buttons in the
interaction (see e.g. Foster, 2007). Such agents do not typi-        correct sequence in order to open the safe; since they do not
cally employ listener gaze tracking for the generation of ap-        have prior knowledge of which buttons to press, they rely on
propriate REs, though. One work that focuses on situated RE          instructions and REs generated by the NLG system in order to
generation is Denis (2010), which takes the visual focus of          carry out the task. A room may contain several buttons other
objects into account for the gradual discrimination of refer-        than the target, which is the button that the user must press
ents from distractors in a series of utterances. However, vi-        next. These other buttons are called distractors and are there
sual focus in Denis’ work is modeled by visibility of objects        to make the RE resolution task more challenging. Rooms
on screen rather than eye gaze. To our knowledge, there exists       also contain a number of landmark objects, such as chairs
no prior RE generation algorithm that is informed directly by        and plants, which cannot be interacted with, but may be used
listener gaze.                                                       in REs to nearby targets. For our experiment we use three
   Finally, gaze as a modality of interaction has been inves-        different virtual environments designed by Gargett, Garoufi,
tigated in virtual reality games before, e.g. by Hülsmann,          Koller, and Striegnitz (2010), which differ in what objects
Dankert, and Pfeiffer (2011). However, most such settings            they contain and where they are located.
do not use language as a further modality. One virtual game-
like setting which focuses on language is the recent Challenge       Generation systems
on Generating Instructions in Virtual Environments (GIVE;
Koller et al., 2010), which evaluates NLG systems that pro-          We implemented three different NLG systems for generating
duce natural-language instructions in virtual environments. In       instructions in these virtual environments. All systems gen-
this work we use the freely available open-source software in-       erate navigation instructions, which guide the user to a spe-
frastructure provided by GIVE1 to set up our experiment.             cific location, as well as object manipulation instructions such
                                                                     as “press the blue button” containing REs such as “the blue
                          Methods                                    button”. The generated instructions are converted to speech
                                                                     by the MARY text-to-speech system (Schröder & Trouvain,
In the GIVE setting (Koller et al., 2010; Striegnitz et al.,
                                                                     2003) and presented via loudspeaker. At any point, the user
2011), a human user can move about freely in a virtual in-
                                                                     may press the ‘H’ key on their keyboard to indicate that they
door environment featuring several interconnected corridors
                                                                     are confused. This will cause the NLG system to generate a
and rooms. A 3D view of the environment is displayed on a
                                                                     clarified instruction. All three systems operate on the same
computer screen as in Fig. 1, and the user can walk forward
                                                                     codebase for the generation of simple yet effective naviga-
and backward, and turn left and right, using the cursor keys.
                                                                     tion instructions (e.g. “go through the doorway”), but differ
They can also navigate to buttons and, once they have ap-
                                                                     in their RE generation strategies.
proached them closely enough, click on them with the mouse
to press them. In Fig. 1 the object currently under inspection          Our baseline system generates REs that are optimized for
by the user is the rightmost button on the wall, marked with         being easy for the listener to understand, according to a
a large white circle. The trace of the fixation’s coordinates is     corpus-based model of understandability (Garoufi & Koller,
rendered by smaller white circles. These gaze markings do            2011). Crucially, this system does not monitor whether the
not appear on the user’s screen during the experiment.               listener understood an RE. It never gives any (positive or neg-
                                                                     ative) feedback, and will only generate a follow-up RE if the
   The user interacts with an NLG system in the context of a
                                                                     user either asks for help (‘H’ key) or presses the wrong but-
treasure-hunt game, where the user’s task is to find a trophy
                                                                     ton. Therefore we call this system the no-feedback system.
    1
      http://www.give-challenge.org/research                            The movement system extends the no-feedback system by
                                                                 1008

monitoring the user’s movements in the game after it has ut-           their English skills as fluent, and all were able to complete
tered an RE, and attempting to predict whether they will press         the tasks. Their mean age was 27.6.
the button it described or not. This system does nothing un-
til only a single button in the current room is visible to the         Task and procedure
user; then it tracks the user’s distance from this button, where       A faceLAB eyetracking system2 remotely monitored partici-
“distance” is a weighted sum of walking distance to the but-           pants’ eye movements on a 24-inch monitor. Before the ex-
ton and the angle the user must turn to face the button. If,           periment, participants received written instructions that de-
after hearing the RE, the user has decreased the distance by           scribed the task and explained that they would be given in-
more than a given threshold, the system concludes that the             structions by an NLG system. They were encouraged to re-
hearer has resolved the RE as this button. If it is the button         quest additional help anytime they felt that the instructions
the system intended to refer to, it utters the positive feedback       were not sufficient (by pressing the ‘H’ key).
“yes, that one!” For incorrect buttons, it utters the negative            The eye-tracker was calibrated using a nine-point fixation
feedback “no, not that one.”                                           stimulus. We disguised the importance of gaze from the par-
   Finally, the eyetracking generation system attempts to pre-         ticipants by telling them that we videotaped them and that the
dict whether the user will press the correct button or not by          camera needed calibration. Participants then started with a
monitoring their gaze. At intervals of approximately 15 ms,            short practice session to familiarize themselves with the game
the system samples the (x,y) position on the screen that the           controls and to clarify remaining questions, before playing
user is looking at. It then resolves this (x,y) screen position to     three full games (each with a different virtual environment
an object in the 3D scene. If the user fixates the same object         and generation system). The order of games was alternated
for more than 300 ms, the system counts this as an inspec-             according to the Latin square design. Finally, each partici-
tion of that object; interruptions of the inspection of less than      pant received a questionnaire which aimed to assess whether
150 ms are ignored. Once it has detected an inspection to a            participants noticed that they were eye-tracked and that one
button in the room, the eyetracking system generates positive          of the generation systems made use of that. The entire exper-
or negative feedback utterances in exactly the same way as             iment lasted approximately 30 minutes.
the movement system does.
                                                                       Analysis
   The system maps the screen positions reported by the eye-
tracker to 3D objects as follows: When the 3D engine renders           Firstly, we determined whether the participant pressed the
the 3D scene onto the 2D screen, it assumes a certain position         correct button (without having to ask for help by pressing the
of the “camera” in the 3D environment; this roughly corre-             ‘H’-key) by comparing each button the participant pressed
sponds to the position of the user’s eyes. For each object that        with the target referent of the most recent first-mention RE.
is currently visible, the system computes its bounding box,            REs that did not lead to a button press (e.g. because the par-
i.e. the smallest box that completely contains the object. It          ticipant navigated away to another room, causing the system
determines the minimum angle α between the ray from the                to switch to navigation instructions) were considered unsuc-
camera position to some corner of the bounding box and the             cessful. This served as a dependent variable but also as a
ray from the camera position to the center of the bounding             means for subdividing data according to un-/successful trial
box. Intuitively, α represents the size of the object on the           completion. Secondly, inspections recorded on a button in the
screen. The system also determines the angle β between the             player’s room, i.e., on the target or a distractor, during the crit-
ray from the camera position to the (x,y) position in the screen       ical time region were registered in all conditions (not just the
plane reported by the eyetracker and the center of the bound-          eyetracking NLG system) and analyzed as a main dependent
ing box. Small values of β represent situations in which the           variable. Further total trial time, i.e., the time taken from the
user looks directly at the center of an object. An object is a         onset of an RE to the button press, as well as the onset time
candidate for being fixated if one of β/α or β − α is below a          of system feedback (when provided) were recorded. Finally,
certain threshold. Among all candidates (if there are any), the        we considered the frequency with which participants asked
system then finally chooses the object with the smallest β.            for help by pressing the ‘H’ key as a measure of confusion.
   Both the movement-based and the eyetracking-based                      To control for external factors, we discarded individual
model withhold their feedback until a first full description of        scenes in which the systems rephrased their first-mention REs
the referent (a first-mention RE) was spoken. Additionally,            (e.g. by adding further attributes), as well as a few scenes
they only provide feedback on newly approached or inspected            which the participants had to go through a second time due to
buttons and will not repeat this feedback unless the listener          technical glitches. To remove errors in eyetracker calibration,
has approached or inspected another button in the meantime.            we included interactions with the eyetracking NLG system in
We call the time between the onset of the first-mention RE             the analysis only when we were able to record inspections (to
and the next button press in a scene, the critical time region.        the referent or any distractor) in at least 80% of all referen-
                                                                       tial scenes. This filtered out 9 interactions out of the 93 we
Participants                                                           collected.
Thirty-one students, enrolled at Saarland University, were                 2
                                                                             http://www.seeingmachines.com/product/
paid to take part in this study (12 females). All reported             facelab
                                                                   1009

   Inferential statistics on this data were carried out using
                                                                     Table 1: Mean inspection durations for target and distrac-
mixed-effect models from the lme4 package in R (Baayen,
                                                                     tor buttons and the total trial time in milliseconds, for suc-
Davidson, & Bates, 2008). Specifically, we used logistic re-
                                                                     cessful and unsuccessful button presses separately. (ET =
gression for modeling binary data such as referential success
                                                                     eyetracking-based system, MOV = movement-based system,
rates, Poisson regression for count variables (e.g., ‘H’-key
                                                                     NO = no-feedback system.) Differences to ET are significant
strokes) and linear regression for inspection durations. Fur-
                                                                     at: *** p < 0.001, ** p < 0.01, * p < 0.05, # p < 0.1.
ther, main effects and interactions were determined through
model reduction, which assesses the contribution of a pre-
dictor or interaction to a fitting model by running a χ2 -             System (# Trials)     Target       Distractor     Trial Total
comparison between models with and without the particular              Successful:
predictor(s).                                                            ET (182)            2111.6       720.5          8096
                                                                         MOV (258)           1493.8**     260.5***       7418
                            Results                                      NO (237)            1492.0***    185.7***       6877**
The post-task questionnaires, revealed no differences in par-          Unsuccessful:
ticipants’ preferences for any particular NLG system. Similar            ET (16)             752.1        3378.9         10892
numbers of participants chose each of the systems on ques-               MOV (37)            602.6        2113.1         10343
tions such as “which system did you prefer”. When asked                  NO (47)             619.5        1891.7         9130
for differences between the systems in free-form questions,
no subject mentioned eye gaze. We take this to mean that the
participants did not realize they were being eyetracked.
Eye movements                                                        However, this is most likely due to the low amount of unsuc-
                                                                     cessful scenes.
We recorded and analyzed inspections to target and distractor           Finally, we considered only cases in which feedback was
buttons in all conditions. Mean inspection durations during          indeed given in order to more precisely assess the influence
the critical time region (reference onset until button press)        of effective feedback (types) on participant inspections dur-
were correlated with the success in pressing the correct button      ing reference resolution. Table 2 shows this data further
and are provided in Table 1.                                         subdivided into scenes with initially positive feedback and
   To investigate our first hypothesis, namely that listener eye     scenes with initially negative feedback (the eyetracking sys-
movements provide a consistent and useful indication of ref-         tem is used as intercept for comparisons between both sys-
erential understanding even when embedded in a dynamic,              tems). This is to explore the effect of positive and confirm-
complex and goal-driven scenario, we first consider our base-        ing feedback given by each system and the possibly differ-
line condition, the no-feedback system, separately: Model re-        ent effect of negative feedback which unspecifically re-directs
duction revealed that both inspection duration on the target         the participant to other buttons. We observed that positive
and inspection duration on the distractors indeed predict suc-       feedback of both systems leads to a similar increase of tar-
cess (χ2 (1) = 28.87, p < .001 and χ2 (1) = 96.24, p < .001,         get and decrease of distractor inspections (cf. before and af-
respectively). While target inspection duration positively pre-      ter columns in Table 2). However, eyetracking-based feed-
dicts success (Coeff. = 0.00110, SE = 0.00024, Wald’s Z              back was given earlier (Coeff. = 573.6, SE = 240.2, t =
= 4.53 , p < .001), distractor inspections negatively pre-           2.39, p(MCMC) < 0.05) and led to overall longer inspec-
dict success (Coeff. = −0.00178, SE = 0.00027, Wald’s Z              tions of the target and distractor buttons relative to the trial
= −6.71, p < .001).                                                  duration. That is, participants spent significantly more time
   Further, to assess the influence of gaze-based feedback           of a trial (34.1%) looking at potential target buttons than with
back on listeners’ gaze behavior, we investigated whether the        movement-based feedback (25.5%, Coeff. = −0.0552, SE =
type of system used for generating REs did in fact influence         0.0178, t = −3.11, p(mcmc) < 0.01). This effect was even
inspection durations (as given in Table 1). We fitted mod-           larger with negative feedback where the difference in feed-
els to target inspection duration and distractor inspection du-      back onset was even greater (Coeff. = 1237.8, SE = 378.1,
ration using system as predictor, for successful and unsuc-          t = 3.27, p(MCMC) < 0.01) and the relative button inspec-
cessful scenes separately. Model reductions revealed a main          tion time was also longer (Coeff. = −0.1818, SE = 0.0283,
effect of system (target: χ2 (2) = 12.79, p < .01, distrac-          t = −6.43, p(MCMC) < 0.001). Possibly because of this
tor: χ2 (2) = 47.10, p < .001) on both inspection variables,         large difference in feedback onset, we also found (marginally)
but only in successful scenes. That is, with the eyetracking-        longer inspections to the buttons after feedback onset.
based feedback system, participants inspected both the target
and distractor buttons longer than with the other two systems.       Interaction Effectiveness
An average trial also lasted longer with this system than with       To evaluate our second hypothesis, namely that gaze-based
the no-feedback system. In unsuccessful scenes no signifi-           feedback potentially sustains a more effective interaction than
cant differences between inspection durations were observed.         other or no feedback, we considered several indicators for in-
                                                                 1010

Table 2: Mean values for initial positive and negative feedback separately: inspection durations for target and distractor buttons
(before and after feedback onset), feedback onset times, total trial durations, proportion of time spent fixating buttons during
trials, and referential success rates. Differences to ET are significant at: *** p < 0.001, ** p < 0.01, * p < 0.05, # p < 0.1.
                                          Target           Distractor        Feedback     Trial    Button Fix.    Success
                                                                             Onset        Total    Proportion
                                    Before    After      Before    After
            Positive Feedback:
                ET                     513    1389          111    67        4115         6511     34.1           97.6
                MOV                    465    1123          196    30        4688*        7051*    25.5**         97.0
            Negative Feedback:
                ET                     109    2155          733    1596      3987         11888    39.5           84.0
                MOV                    120    926***      484#     802#      5225**       11319    20.1***        68.0*
teraction effectiveness. As a first measure, we looked at the        mance of the eyetracking NLG system, which outperforms
frequency with which participants pressed the ‘H’ key to in-         the no-feedback baseline on listener confusion and on RE
dicate their confusion. The overall average of ‘H’ keystrokes        success rate. If gaze was not a reliable indicator of RE inter-
per game was 1.14 for the eyetracking generation system,             pretation, this system would frequently give misleading feed-
1.77 for the movement system was employed, and 2.26 for the          back and therefore perform worse. Together with the find-
no-feedback system. A model fitted to the key stroke distri-         ing that positive gaze-based feedback leads to shorter trial
bution per system shows significant differences both between         times than positive movement-based feedback, while nega-
the eyetracking and the no-feedback system (Coeff. = 0.703,          tive gaze-based feedback leads to better success rates than
SE = 0.233, Wald’s Z = 3.012, p < .01) and between the eye-          negative movement-based feedback, this confirms our second
tracking and the movement-based system (Coeff. = 0.475, SE           hypothesis. That is, the eyetracking system (positively) influ-
= 0.241, Wald’s Z = 1.967, p = .05).                                 ences interaction effectiveness.
   A second measure of interaction quality is the ratio of
                                                                        One observation from the games in the experiment is that
all REs that the participants resolved correctly. Mean suc-
                                                                     listeners tend to rapidly look back and forth between differ-
cess rates for trials with feedback only are further reported
                                                                     ent buttons when they are confused. However, it needs to be
in the final column of Table 2. Logistic mixed-effects mod-
                                                                     still worked out, how to interpret such signals more gener-
els revealed a significant difference in success rates (Coeff. =
                                                                     ally. A further issue is that all objects in the 3D world shift
−0.918, SE = 0.461, Wald’s Z = −1.990, p < .05) for nega-
                                                                     on the screen when the user turns or moves in the virtual en-
tive feedback while the success rates were similar for positive
                                                                     vironment. The user’s eyes will typically follow the object
feedback. Additionally, total trial time is significantly short-
                                                                     they are currently inspecting, but lag behind until the screen
ened by positive (but not negative) eyetracking-based feed-
                                                                     comes to a stop again. One topic for future work would be to
back (Coeff. = 713.7, SE = 311.4, t = 2.29, p < .05). Thus,
                                                                     remove such noise from the eyetracking signal.
when positive feedback was given, the eyetracking system
had shorter trial times (along with earlier feedback), while            Finally, the negative feedback our systems gave was very
having similar success rates as the movement system. Con-            unspecific (“no, not that one”, even when there were other
versely, negative feedback led to similar trial times but with       distractors) and given earlier and numerically also more fre-
higher success rates by the eyetracking system.                      quently by the eyetracking system. This could explain the
                                                                     different effects of positive and negative feedback on inspec-
                          Discussion                                 tion behavior and the time-accuracy trade-off for each sys-
Concerning our first hypothesis—that gaze reflects on-               tem: Longer trial times but better success rates for negative
line referential understanding even in dynamic 3D                    gaze-based (compared to movement-based) feedback. We
environments—we find that participants indeed tend to                used negative feedback to keep the experimental situation
rapidly fixate the object described by the system. Appro-            more controlled but the performance of the feedback systems
priate feedback by the eyetracking system, in turn, elicits          could possibly be improved by giving more specific feedback
longer inspection durations on potential targets, showing            (“no, the BLUE button”). Another avenue for future research
more focused, task-oriented listener attention.                      is to examine whether listener gaze could also be useful for
   This positive finding is further supported by the perfor-         other NLG or dialog tasks apart from RE generation.
                                                                 1011

Conclusion                                                                 conversation. Journal of Memory and Language, 57,
We reported on an experiment in which an NLG system used                   596–615.
listener gaze to track the listener’s understanding of REs and      Hülsmann, F., Dankert, T., & Pfeiffer, T. (2011). Compar-
provide positive or negative feedback when needed. This                    ing gaze-based and manual interaction in a fast-paced
shows that listener gaze provides consistent and useful feed-              gaming task in virtual reality. In Virtuelle & Erweiterte
back about the listener’s interpretation process, and that NLG             Realität, 8. Workshop der GI-Fachgruppe VR/AR.
systems can be improved by tracking this interpretation pro-        Iida, R., Yasuhara, M., & Tokunaga, T. (2011). Multi-modal
cess in real time.                                                         reference resolution in situated dialogue by integrat-
   These findings have consequences both for psycholinguis-                ing linguistic and extra-linguistic clues. In Proceed-
tics and for computational linguistics. On the psycholinguis-              ings of 5th International Joint Conference on Natural
tic side, they open the way for eyetracking experiments that               Language Processing.
are set in a more natural and dynamic, and importantly, truly       Knoeferle, P., Crocker, M. W., Pickering, M., & Scheepers,
interactive, environment than traditional Visual World exper-              C. (2005). The influence of the immediate visual con-
iments. On the computational side, they offer a testbed for                text on incremental thematic role-assignment: evidence
interactive NLG and dialogue systems; even though eyetrack-                from eye-movements in depicted events. Cognition, 95,
ing devices are not yet commonplace as computer peripherals                95–127.
they can still allow us to implement and test theories of how       Koller, A., Striegnitz, K., Byron, D., Cassell, J., Dale, R.,
to effectively track the comprehension process of the user.                Moore, J., et al. (2010). The First Challenge on Gener-
                                                                           ating Instructions in Virtual Environments. In E. Krah-
                    Acknowledgments                                        mer & M. Theune (Eds.), Empirical Methods in Natu-
                                                                           ral Language Generation (pp. 337–361). Springer.
The research reported of in this paper was partly supported         Kreysa, H., & Knoeferle, P. (2011). Peripheral speaker
by the “Multimodal Computing and Interaction” Cluster of                   gaze facilitates spoken language comprehension: syn-
Excellence at Saarland University. We thank Irena Dotcheva                 tactic structuring and thematic role assignment in Ger-
for help with data collection as well as Alexandre Denis and               man. In B. Kokinov, A. Karmiloff-Smith, & N. Ners-
Christoph Clodo for software support.                                      essian (Eds.), Proceedings of the European Conference
                                                                           on Cognitive Science 2011.
                         References
                                                                    Kuriyama, N., Terai, A., Yasuhara, M., Tokunaga, T., Yam-
Altmann, G., & Kamide, Y. (1999). Incremental interpre-                    agishi, K., & Kusumi, T. (2011). Gaze matching of
       tation at verbs: restricting the domain of subsequent               referring expressions in collaborative problem solving.
       reference. Cognition, 73(3), 247–264.                               In Proceedings of International Workshop on Dual Eye
Baayen, R., Davidson, D., & Bates, D. (2008). Mixed-effects                Tracking in CSCW (DUET).
       modeling with crossed random effects for subjects and        Richardson, D. C., & Dale, R. (2005). Looking to under-
       items. Journal of Memory and Language, 59, 390–                     stand: The coupling between speakers’ and listeners’
       412.                                                                eye movements and its relationship to discourse com-
Clark, H. H., & Krych, M. A. (2004). Speaking while mon-                   prehension. Cognitive Science, 29(6), 1045–1060.
       itoring addressees for understanding. Journal of Mem-        Schröder, M., & Trouvain, J. (2003). The German Text-to-
       ory and Language, 50(1), 62–81.                                     Speech Synthesis System MARY: A Tool for Research,
Denis, A. (2010). Generating referring expressions with ref-               Development and Teaching. International Journal of
       erence domain theory. In Proceedings of the 6th Inter-              Speech Technology, 6, 365–377.
       national Natural Language Generation Conference.             Staudte, M., & Crocker, M. W. (2011). Investigating joint at-
Foster, M. E. (2007). Enhancing human-computer interaction                 tention mechanisms through human-robot interaction.
       with embodied conversational agents. In Proceedings                 Cognition, 120(2), 268–291.
       of HCI International 2007.                                   Striegnitz, K., Denis, A., Gargett, A., Garoufi, K., Koller, A.,
Gargett, A., Garoufi, K., Koller, A., & Striegnitz, K. (2010).             & Theune, M. (2011). Report on the Second Second
       The GIVE-2 Corpus of Giving Instructions in Virtual                 Challenge on Generating Instructions in Virtual Envi-
       Environments. In Proceedings of the 7th Conference                  ronments (GIVE-2.5). In Proceedings of the Gener-
       on International Language Resources and Evaluation.                 ation Challenges Session at the 13th European Work-
Garoufi, K., & Koller, A. (2011). The Potsdam NLG sys-                     shop on Natural Language Generation.
       tems at the GIVE-2.5 Challenge. In Proceedings of the        Tanenhaus, M. K., Spivey-Knowlton, M., Eberhard, K., &
       Generation Challenges Session at the 13th European                  Sedivy, J. (1995). Integration of visual and linguistic
       Workshop on Natural Language Generation (ENLG).                     information in spoken language comprehension. Sci-
Griffin, Z. M., & Bock, K. (2000). What the eyes say about                 ence, 268, 1632–1634.
       speaking. Psychological Science, 11, 274–279.
Hanna, J., & Brennan, S. (2007). Speakers’ eye gaze disam-
       biguates referring expressions early during face-to-face
                                                                1012

