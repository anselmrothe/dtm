UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A time-invariant connectionist model of spoken word recognition
Permalink
https://escholarship.org/uc/item/7wr3d4hv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Hannagan, Thomas
Magnusson, James
Grainger, Jonathan
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                 A time-invariant connectionist model of spoken word recognition
                                      Thomas Hannagan (thom.hannagan@gmail.com)
                                                   CNRS & Aix-Marseille University
                                            3, place Victor Hugo, 13331 Marseille, France
                                     James S. Magnuson (james.magnuson@uconn.edu)
      Department of Psychology, University of Connecticut, 406 Babbidge Road, Unit 1020, Storrs, CT 06269-1020 USA
                               and Haskins Laboratories, 300 George St., New Haven, CT 06511 USA
                                    Jonathan Grainger (i.jonathan.grainger@gmail.com)
                                                   CNRS & Aix-Marseille University
                                            3, place Victor Hugo, 13331 Marseille, France
                             Abstract                                cochlear nerve, the other in parallel through the optic nerve.
                                                                     In their own dedicated primary cortical regions, however,
   One of the largest remaining unsolved mysteries in cognitive
   science is how the rapid input of spoken language is mapped       both arrive at spatial representations – tonotopic for the
   onto phonological and lexical representations over time.          auditory system, retinotopic for the visual system. What
   Attempts at psychologically-tractable computational models        happens next, according to computational models of visual
   of spoken word recognition tend either to ignore time or to       and spoken word recognition, further hints at some possible
   transform the temporal input into a spatial representation.       unification.
   This is the approach taken in TRACE (McClelland & Elman,
   1986), the model of spoken word recognition that has the
   broadest and deepest coverage of phenomena in speech
                                                                     Modeling spoken and visual word recognition:
   perception, spoken word recognition, and lexical parsing of       TRACE and IA
   multi-word sequences. TRACE reduplicates featural,                From a psycholinguistic point of view, two early models of
   phonemic, and lexical inputs at every time step in a              word recognition based on the same computational
   potentially very large memory trace, and has rich
   interconnections (excitatory forward and backward
                                                                     framework have been enormously successful. In the visual
   connections between levels and inhibitory links within            domain, the Interactive Activation (IA) model and its
   levels). This leads to a rather extreme proliferation of units    extensions (McClelland & Rumelhart, 1981; Grainger &
   and connections that grows dramatically as the lexicon or the     Jacobs, 1996) can account for a large number of robust and
   memory trace grows. Our starting point is the observation that    sometimes counterintuitive behavioral findings, in a simple
   models of visual object recognition – including visual word       and elegant hierarchical structure where units at any level
   recognition – have long grappled with the fundamental             compete to represent the stimulus, and engage in "lobbying"
   problem of how to model spatial invariance in human object
   recognition. We introduce a model that combines one aspect        up and down in the hierarchy. In the auditory domain,
   of TRACE – time-specific phoneme representations – and            TRACE (an extension of the IA framework for speech;
   higher-level representations that have been used in visual        McClelland & Elman, 1986) continues to produce new
   word recognition – spatially- (here, temporally-) independent     insights into human behavior, including close fits to fine-
   diphone and lexical units. This reduces the number of units       grained estimates of the time course of spoken word
   and connections required by several orders of magnitude           recognition from the visual world paradigm (Allopenna et
   relative to TRACE. In this first report, we demonstrate that
                                                                     al., 1998; Dahan, Magnuson, Tanenhaus, & Hogan, 2001);
   the model (dubbed TISK, for Time-Invariant String Kernel)
   achieves reasonable accuracy for the basic TRACE lexicon          Dahan, Magnuson, & Tanenhaus, 2001).1
   and successfully models the time course of phonological              One probably superficial difference between the two
   activation and competition. We close with a discussion of         models is that between-level connections in IA models of
   phenomena that the model does not yet successfully simulate       reading typically include both inhibitory and excitatory
   (and why), and with novel predictions that follow from this       connections, whereas between-level connections in TRACE
   architecture.
                                                                        1
   Keywords: Keywords: Spoken Word Recognition; Time                      It is important to note that current, psychologically tractable
   invariance ; Computational models; TRACE.                         models of spoken word recognition do not take real speech as their
                                                                     input. While Grossberg & Myers (2000) have modeled aspects of
                          Background                                 speech and word processing using real speech inputs, these efforts
                                                                     have not yet yielded a model that can handle speech input and a
   Could it be that despite very salient differences, the            broad range of phenomena in spoken word recognition. In order to
auditory and visual systems actually rely on the same                be able to address complex issues in word recognition without first
mechanisms in order to recognize words? One signal has a             solving all fundamental problems in speech perception, TRACE’s
temporal dimension and is carried by transient sound waves,          inputs (for example) are "pseudo-spectral" acoustic-phonetic
the other is spatially extended and travels at the speed of          features that ramp on and off over time, with temporal overlap
light. One signal travels sequentially (over time) through the       between adjacent phonemes providing a coarse analog of
                                                                     coarticulation.
                                                                   1638

are only excitatory. The evidence that this is superficial         report first results on a model that achieves decent
comes from demonstrations in visual letter identification          performance using many fewer nodes and connections than
that performance is at least as good without inhibitory            TRACE. With a 2 second layer of time-invariant input
connections between levels (Rey et al., 2009). A much more         nodes and TRACE's 14 phonemes and 212 words, TISK
serious difference, however, is that the IA model can only         requires 9.7 thousand units and 62 thousand connections.
recognize words at one location on the retina, whereas             This represents a 9-fold improvement over TRACE for
TRACE has to recognize words at any point in time.                 units, and 2 orders of magnitude for connections. Critically,
   But this impressive ability of TRACE is only achieved at        the orders of magnitude in improvement turn out to be
the price of duplicating each unit for as many time slices as      proportional to lexicon size: with 20,000 words and 40
needed in the simulation. That is, the processing units in         phonemes, TISK would require 48 thousand units (TRACE
TRACE form a large memory, with units aligned with time            requires 84 times more) and 3.3 million connections
’slices’. Essentially, there is a copy of every feature,           (TRACE requires 24 thousand times more).
phoneme, and word unit at every time slice (the complete
details are more complex – for example, words are only             String kernels
duplicated every 3 time slices; see McClelland & Elman,            In the machine learning literature, one computational
1986, for details). When input begins, the first instant of the    technique that has been very successful at representing
input aligns with and activates units in the first time slice in   sequences of symbols independently of their position goes
memory. As the input continues, it activates nodes aligned         under the name of string kernels (Hofmann et al., 2007).
with specific time slices. Those units can become and              Symbols can be amino-acids, nucleotides, or letters in a
remain active for a considerable time after the input has          webpage: in every case the gist of string kernels is to
continued on. Conceptually, this is like marks on a page left      represent strings (such as "TIME”) as points in a high-
by a seismograph – the memory banks contain a trace of the         dimensional space of symbol combinations (for instance as
input that has come along. But these are not passive traces,       a vector where each component stands for a combination of
since unit activations flux as a function of excitatory and        two symbols, and only the components for “TI”, “TM”,
inhibitory input from other units, and a decay parameter.          “TE”, “IM”, “IE”, “ME” would be non-zero). It is known
   Having reduplicated units allows TRACE to solve the             that this space is propitious to linear pattern separations and
temporal alignment problem by brute force; given the input         yet can also capture the (domain-dependent) similarities
/dad/, it can tell that the phoneme /d/ should be activated        between them. Although it has been argued in the visual
twice and how far apart in time the two occurrences are –          modality that string kernels can account for masked priming
because the two instances of /d/ are encoded by completely         effects and are thus likely involved in the early stages of
independent /d/ detectors aligned with different points in         processing, there has been very little investigation of String
time. The same applies at the word level; TRACE can tell           kernels in the auditory domain (Gales, 2009, being a yet
that /dag/ (the TRACE representation of DOG) occurs twice          unpublished exception).
in /dagitsdag/ (DOG EATS DOG) because the two instances               Given the demonstrated versatility of the technique, there
are encoded by independent /dag/ detectors aligned with            is every reason to suspect that string kernels could also work
different points in time.                                          in spoken word recognition, where symbols would then be
   But this comes at a cost. Consider the number of units per      discrete and time-specific phonemes, which would be turned
slice: 63 x 3 features, 14 phonemes, and, in the basic             into vectors in the space of time-invariant phoneme
TRACE lexicon, 212 words, for 415 units. If we ballpark            combinations. This would entail that the same type of
the number of connections by assuming an average of 8              representations are in fact at work in spoken and visual
featural connections per phoneme, and 3 phonemes per               word recognition. However, while one can find some appeal
word, and allowing for connections between units at                in this unification (this would for instance pave the way to
adjacent time slices, we would have approximately 47,000           establishing connections between sublexical orthography
connections per time slice with a 200-word lexicon. If we          and sublexical phonology), there remains the nagging
make the trace approximately 2 seconds long (the duration          problem of how to turn sequences of time-specific
of echoic memory), we need approximately 83 thousand               phonemes into time-invariant phoneme combinations – that
units and 9.4 million connections. If we increase the lexicon      is, how to compute the string kernel for spoken words.
to a more realistic size of 20,000 words and the phoneme           Thinking in the unified framework of string kernels suggests
inventory to 40, these figures reach approximately 4 million       that similar problems across modalities can receive similar
units and 80 billion connections.                                  solutions, and we now introduce our time-invariant
   One might argue that this may not be an unreasonable            alternative to the TRACE model, which handles the
scale, given the number of neurons and connections in the          transition between time-specific and time-invariant units in
brain. However, principles of parsimony (might there be a          much the same way as location-specific and location-
simpler solution?) and evolutionary pressures to minimize          invariant units are activated in the visual modality, through
energy consumption would be reasonable motivations to              the use of symmetry networks (Shawe-Taylor, 1989).
seek a less costly solution to time-invariance. Exploring
such an alternative is the purpose of this paper, and we
                                                                 1639

                            Model                                     Note that feedback serves several functions, as does
                                                                   lexical-phonemic feedback in TRACE: it provides a basis
                                                                   for lexical effects on phoneme decisions; it makes the model
Architecture and dynamics
                                                                   more efficient and robust against noise (Magnuson et al.,
The model is illustrated in Figure 1. It uses the same lexicon     2005); and it provides an implicit sensitivity to phonotactics
and basic activation dynamics as the TRACE model, but a            – the more often a phoneme or nphone occurs in lexical
radically different architecture. It is comprised of four          items, the more feedback it potentially receives. Feedback in
levels: inputs, phonemes, nphones (currently, nphones are          models of spoken word recognition is a controversial topic
single phones or diphones) and words. Inputs consist of a          (see McClelland et al., 2006; McQueen et al., 2006; Mirman
bank of time-specific feature units as in TRACE, through           et al., 2006), which we do not address here; our aim is to see
which a wave of transient activation pattern travels.              whether a model with a radically simpler computational
However, this input layer is deliberately very simplified          architecture compared to TRACE can (begin to) account for
compared to its TRACE analog, given that at any time there         a similar range of phenomena in spoken word recognition.
is always at most one input unit active – inputs do not               Units in the model are leaky integrators: at each cycle, all
overlap in time, and do not code for phonetic similarity (that     units are activated according to the net input they receive
is, the /d/ unit is equally similar to /a/ and /t/, as each unit   and to their previous activation, minus a decay term, as
can either be on or off; we will address phonetic grain in         described in equation 1:
future work). This input level sends activation forward to
the phoneme level. The time-specific phoneme level
consists of 10 banks of 14 phonemes that serve as input to
the network (the limitation to 10 is completely arbitrary, but
sufficient for single-word recognition; there are only 14
phonemes because we are using the 14 phonemes                      and where the net input of unit i at time t is given by:
implemented in TRACE). Input phonemes are introduced
one at a time and activate the time-invariant nphone level
via     feedforward       connections.     Phoneme-to-nphone          The python code for the model as well as the list of
connection weights are set according to a gradient weighting       parameters are available upon request to the first author. We
scheme that we will shortly describe. The nphone level             now describe how the connections between phonemes and
consists of 196 + 14 units, one for each phoneme and for           nphones are set in the model.
each of the 142 possible diphones that can be composed
with the set of phonemes. Units at this level compete with
one another via lateral inhibition, and send activation
forward to the time invariant word level through excitatory
connections, whose weights were normalized by the number
of nphones of the destination word. The word level consists
of 212 units, one for each of the original words in the
TRACE lexicon, with lateral inhibition between words, and
feedback excitatory connections from words to nphones.
                                                                     Figure 2: A symmetry network for time-invariant nphone
                                                                        recognition that can distinguish between anaphones.
                                                                   A symmetry network for phonological string
                                                                   kernels
                                                                   The problem we are confronting here is to achieve time-
                                                                   invariant recognition while still distinguishing between
                                                                   transposed phoneme combinations. Since we must
                                                                   recognize a succession of phonemes like [/a/t, /b/t+1]
                                                                   whatever time “t” is, we need to be able to recognize each
                                                                   phoneme /a/ and /b/ at any “t”. But since each unit must
                                                                   activate at any time, how then can we activate unit /ab/
  Figure 1: The TISK model - a time-invariant architecture         rather than /ba/ at the nphone level?
                  for spoken word recognition.
                                                                 1640

   This issue of selectivity (here, between “anaphones”:          reaches 100% recognition. A consideration of the few
words with the same phonemes in different order) versus           unrecognized words, like /triti/ and /st^did/, is instructive in
invariance (here, to position-in-time) has long been              that they were often confused with their cohort candidates
identified in the fields of visual recognition and computer       (e.g. /trit/ and /st^di/), which activate exactly the same
vision, and has recently received attention in a series of        nphones but one (resp. /ti/ and /id/). This confusion can
articles investigating invariant visual word recognition          only happen in the current model when two phonemes are
(Dandurand, Grainger, & Dufau, 2010; Dandurand,                   closely repeated at the end of a relatively long word, since
Hannagan, & Grainger, 2010; Hannagan, Dandurand &                 the importance of any one nphone for recognition of a word
Grainger, 2011). The solution adopted in the present model        is currently inversely proportional to how many nphones it
is illustrated in Figure 2, and was inspired by what has been     activates. We note that a model whose nphone-to-word
learned through this recent work on the way various               weights would be set following other criteria (for instance,
backpropagation networks deal with the selectivity versus         the conditional probability of the word given the nphone)
invariance dilemma (to our knowledge this solution has not        would give more importance to diagnostic nphones and
yet been proposed in spoken word recognition models).             reach perfect accuracy.
Briefly stated, this consists of correlating phoneme-to-
nphone connection strengths with phoneme position-in-             Competitor effects: Cohort, rhyme and embedding
time, as illustrated in Figure 2 (blue excitatory connections).   Figure 3 shows the average cohort and rhyme effects in the
If the weights from phoneme units /a/1, /a/2,..., /a/T to         model (left panel) and in TRACE (right panel). The curves
diphone unit /ab/ decrease linearly from T-1 to zero, and if      were calculated by averaging across trials the activation
on the contrary the weights from phoneme units /b/1, /b/2,...,    levels of all targets (“target” curve in black), of all words
/b/T to diphone unit /ab/ increase at the same pace from zero     that started with the same phonemes as the target (“Cohorts”
to T-1, then presenting the input sequence [/a/t, /b/t+1] will    curve in red), of all words that ended with the same
always result in a constant net input for /ab/ whatever the       phonemes as the target (“Rhymes” curve in blue), of all
time “t” is, and it will result in a smaller constant net input   words contained in the target (“Embeddings” curve in
to /ba/. By setting the weights from these phoneme units to       purple) and of all other words (“Mean of all words” curve
the transposed diphone /ba/ in exactly the opposite pattern,      in grey).
and by setting once and for all a common activation
threshold for every diphone unit anywhere between these
two net inputs, one can ensure that the network can always
neatly distinguish between /ab/ and /ba/. To prevent
sequences with repeated phonemes like [/b/1, /a/2, /b/3] from
activating large sets of unwanted nphones like /bi/, /b^/), it
is however necessary to introduce gating connections (black
connections in Figure 2), whereby for instance /b/1 disables
the connection between all future /b/t >1 and diphones /*b/
(where “*” stands for any phoneme but b).
   Other architectures exist that can operate the transition
between time-specific phonemes and time-invariant
nphones, but the symmetry network we introduce within
this model builds on a solution found by the
backpropagation algorithm, and has thus arguably a
headstart in learnability. It also seems to provide a faithful          Figure 3: Average activations in the lexicon, when
implementation of the “string kernel” code recently               partitioned for each trial as Target, Cohort words, Rhyming
described by Hannagan & Grainger (2011).                                      words, embedded words and All words.
                                                                                      Left panel: TISK model.
                            Results                                                Right panel: TRACE model.
                                                                     Apart from superficial differences in zero-valued versus
Recognition rate                                                  negative resting levels, Figure 3 shows that the agreement
We presented the model with every word in its 212-word            between models is good on competitor effects. Indeed the
lexicon. A word was counted as correctly recognized if it         magnitude and ordering of the Cohorts, Rhymes and
had been the most active lexical unit for ten cycles in a row     Embeddings effects is similar in the two models, relative to
before the deadline, which was set to 100 cycles.                 the baseline Mean of all words.
Recognition performance was similar across different                 The behavior exhibited by both models also mirrors the
operational measures of recognition. With these settings, the     cohort and rhyme effects that have been reported in humans
model correctly recognizes 98% of the 216 words. We               performing for instance the so-called “visual world” task. In
consider this satisfactory for a first test of a new              a nutshell, overall candidate words that begin like the target
computational approach, although the TRACE model                  are more active early on during processing while those that
                                                                1641

end like the target are more active later one during                gyri, a VWFA homologue for speech has not yet been
processing, without ever rising to the activation level of the      detected.
target, or going below the activation level of unrelated               The second is that paradigms for testing time-invariance
words.                                                              are less easily designed than those which test location-
                                                                    invariance in the visual case. Varying on Rauschecker et al.
                                                                    (2011) however, we can propose a task that tests for the
                          Discussion                                presence of time-specific word representations, in which
The previous results tentatively suggest that a time- specific      subjects would be presented with a sequence of meaningless
model of spoken word recognition like TRACE could in                sounds where one spoken word would be embedded. By
principle be replaced by a time-invariant alternative (TISK).       manipulating the position of this word in the sequence, one
This raises the questions of whether there is indeed any kind       could then test whether a “blind” classifier could be trained
of evidence for time-invariant phonological representations         on the evoked fMRI activation patterns to discriminate
in the brain, above and beyond considerations of parsimony,         which activation patterns correspond to which positions-in-
and whether one could find predictions that would allow us          time. A clear demonstration that a classifier is unable to
to uniquely distinguish between the time-invariant and time-        “blindly” map phonological patterns to position-in-time
specific candidate models. We now address these two                 would be good evidence for the model we have introduced.
questions.                                                          In the alternative scenario, a successful blind classifier
                                                                    would be a smoking gun for this model. Following on our
Neural evidence for time invariant spoken word                      work in the visual modality, we would then need to consider
recognition?                                                        a revised version with limited and shared units that could
Researchers interested in the neural representations for            possibly achieve semi-time invariant representations.
visual words are blessed with the Visual Word Form Area, a
well-defined region in the brain that sits at the top of what is    Specific predictions
still known as the ventral visual stream, and is                    A specific prediction of this model concerns the treatment of
demonstratively the locus of our ability to read words              repeated phonemes in a word. As we have seen, the TRACE
(Gaillard et al., 2006), but critically not to hear them. Until     model deals with both cases by assigning activation to
recently, the common wisdom was that by the mere virtue of          different time-specific units, whereas the model we have
its situation in the brain – if not by its purported hierarchical   introduced must activate for instance the same “na” unit in
architecture with increasingly large receptive fields – the         “banana”at two different times. Finding evidence against
VWFA was bound to achieve complete location invariance              this central feature would plainly falsify the model.
for word stimuli. However recent fMRI studies show that,            However it is still unclear at this point how this would really
and computational modeling explains why, a significant              manifest in the model (for instance would words with
degree of sensitivity to location is present in the VWFA            repeated diphones such as “banana” get more activation
Rauschecker et al. (2011). A trained, functional model of           from the diphone level than in the TRACE model?). In fact
location invariance for visual words explains why this can          one critical test for the current model will reside in its ability
be so: in this model the conflicting requirements for location      to handle such inputs in a way that is consistent with
invariant and selectivity conspire with limited resources,          humans. If the expected differences with TRACE are indeed
and force the model to develop in a symmetry network with           obtained, experimental evidence could then be gathered
broken location symmetry on its weights. This in turn               with the “visual world paradigm” by presenting targets and
produces “semi-location invariant” distributed activity             distractors with or without repeated diphones. Similarly, one
patterns, which are more sensitive to location for more             would expect the same phenomena to be within reach of
confusable words (Hannagan, Dandurand & Grainger,                   empirical investigations for repeated words in a sentence.
2011). Thus brain studies have already been highly
informative and have helped constrain our thinking on how                                    Conclusions
the brain recognizes visual words.                                  We have presented a computational model of spoken word
   But attempts to proceed in the same way for the auditory         recognition (TISK) that achieves a close-to-perfect word
modality quickly run into at least two brick walls. The first       recognition rate (98%), while also exhibiting the ability to
is that there is no clear homologue of the VWFA for spoken          account for basic aspects of phonological competition – the
words. This might be because the speech signal varies in            time course of cohort and rhyme effects. This time-invariant
more dimensions than the visual signal corresponding to a           alternative uses vastly (orders of magnitude) less
visual object; a VWFA homologue for speech might need to            computational resources than its time-specific counterpart,
provide invariance not just in temporal alignment, but also         TRACE, the economy in number of units being inversely
across variation in rate, speaker characteristics, etc. While       proportional to the number of time steps allowed as input
there have been reports of hints of such invariance and/or          and (in TRACE) memory at all levels or (in our model) at
multidimensional sensitivity in the superior (Salvata et al.,       the phoneme level. A notable property of the model is that
in press) and medial (Chandrasekaran et al., 2011) temporal         the computational mechanisms involved – string kernels and
                                                                    symmetry networks – are exactly the same as have been
                                                                  1642

proposed in the visual word recognition literature, paving      Hannagan, T., Dandurand, F., & Grainger, J. (2011). Broken
the way to a possible unified account of word recognition         symmetries in a location invariant word recognition
across modalities. Finally we have pointed to where we            network. Neural Computation, 23 (1), 251–283.
think specific predictions of the model should arise, and we    Hofmann, T., Scho lkopf, B., & Smola, A. J. (2007).
have put forward a new task that makes the model more             Kernel methods in machine learning. Annals of Statistics,
easily falsifiable.                                               36, 1171 – 1220.
                                                                Jones, M. N., & Mewhort, D. J. K. (2007). Representing
                    Acknowledgments                               word meaning and order information in a composite
This work was conducted under the European Research               holographic lexicon. Psychological Review , 114 , 1 – 37.
Council grant #230313 awarded to Jonathan Grainger.             Magnuson, J. S., Strauss, T. J., & Harris, H. D. (2005).
                                                                  Interaction in spoken word recognition models: Feedback
                        References                                helps. In Proc. Ann. Cognitive Science Society.
                                                                McClelland, J. L., & Elman, J. L. (1986). The trace model
Allopenna, P. D., Magnuson, J. S., & Tanenhaus, M. K.             of speech perception. Cognitive Psychology, 18, 1 – 86.
  (1998). Tracking the time course of spoken word               McClelland, J. L., Mirman, D., & Holt, L. L. (2006). Are
  recognition: evidence for continuous mapping models.            there interactive processes in speech perception? Trends
  Journal of Memory and Language, 38, 419 – 439.                  in Cognitive Science, 10, 363 – 369.
Chandrasekaran, B., Chan, A.H.D., & Wong, P.C.M.                McClelland, J. L., & Rumelhart, D. E. (1981). An
  (2011). Neural processing of what and who information           interactive activation model of context effects in letter
  during spoken language processing. Journal of Cognitive         perception: Part 1. an account of basic findings.
  Neuroscience, 23(10), 2690-2700.                                Psychological Review, 88, 375 – 407.
Cohen, L., Dehaene, S., Naccache, L., Lehericy, S.,             McQueen, J., Norris, D. & Cutler A. (2006). Are there really
  Dehaene-Lambert, G., Henaff, M., et al. (2000). The             interactive processes in speech perception? Trends in
  visual word-form area: spatial and temporal                     Cognitive Sciences, 10(12), 533.
  characterization of an initial stage of reading in normal     Mirman, D., McClelland, J. L., & Holt L.L. (2006).
  subjects and posterior split-brain patients. Cognition and      Theoretical and empirical arguments support interactive
  Brain Theory, 123, 291 – 307.                                   processing. Trends in Cognitive Sciences, 10(12), 534.
Dahan, D., Magnuson, J. S., & Tanenhaus, M. K. (2001).          Rauschecker, A. M., Bowen, R. M., Parvizi, J., & Wandell,
  Time course of frequency effects in spoken-word                 B. A. (2011). Position-sensitivity in the VWFA measured
  recognition: Evidence from eye movements. Cognitive             using fMRI pattern-classification and intracranial
  Psychology, 42, 317 – 367.                                      recordings in humans. In Society for Neuroscience Proc.
Dahan, D., Magnuson, J. S., Tanenhaus, M. K., & Hogan, E.       Rey, A., Dufau, S., Massol, S., & Grainger, J. (2009).
  M. (2001). Subcategorical mismatches and the time               Testing computational models of letter perception with
  course of lexical access: Evidence for lexical competition.     item-level ERPs. Cognitive Neurospsychology, 26, 7 – 22.
  Language and Cognitive Processes, 16, 507-534.                Salvata, C., Blumstein, S. E., & Myers, E. B. (in press).
Dandurand, F., Grainger, J., & Dufau, S. (2010). Learning         Speaker invariance for phonetic information: An fMRI
  location invariant orthographic representations for printed     investigation. Language & Cognitive Processes.
  words. Connection Science, 22(1), 25 – 42.                    Shawe-Taylor, J. (1989). Building symmetries into
Dandurand, F., Hannagan, T., & Grainger, J. (2010). Neural        feedforward networks. In Proceedings of the First IEE
  networks for word recognition: Is a hidden layer                Conference on Artificial Neural Networks, London.
  necessary? In S. Ohlsson & R. Catrambone (Eds.),
  Proceedings of the 32nd Annual Conference of the
  Cognitive Science Society, 688-693.
Gaillard, R., Naccache, L., Pinel, P., Clémenceau, S., Volle,
  E., Hasboun, D., et al. (2006). Direct intracranial, fMRI,
  and lesion evidence for the causal role of left
  inferotemporal cortex in reading. Cognition and Brain
  Theory, 50(2), 191 – 204.
Gales, M. J. F. (2009). Sequence kernels for speaker and
  speech recognition. In Proc. Technology Workshop at
  Johns Hopkins University, Baltimore.
Grainger, J., & Jacobs, A. M. (1996). Orthographic
  processing in visual word recognition: A multiple readout
  model. Psychological Review, 103, 518 – 565.
Grossberg, S., & Myers, C. W. (2000). The resonant
  dynamics of speech perception: Interword integration and
  duration-dependent backward effects. Psychological
  Review, 107, 735 – 767.
                                                              1643

