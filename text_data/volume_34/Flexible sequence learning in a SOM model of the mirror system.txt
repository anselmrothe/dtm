UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Flexible sequence learning in a SOM model of the mirror system
Permalink
https://escholarship.org/uc/item/39r423bn
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Thill, Serge
Behr, Josef
Ziemke, Tom
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                 Flexible sequence learning in a SOM model of the mirror system
                                                    Serge Thill (serge.thill@his.se)
                                         Interaction Lab, School of Humanities and Informatics
                                                            University of Skövde
                                                           54128 Skövde, Sweden
                                                        Josef Behr (jbehr@uos.de)
                                          Institut für Kognitionswissenschaft, Neurokybernetik
                                                           University of Osnabrück
                                                         49069 Osnabrück, Germany
                                                  Tom Ziemke (tom.ziemke@his.se)
                                         Interaction Lab, School of Humanities and Informatics
                                                            University of Skövde
                                                           54128 Skövde, Sweden
                              Abstract                                   Sequencing via ordinal nodes and conditions of
                                                                         satisfaction
   We present initial work on a biologically and cognitively in-         The gist of the framework by Sandamirskaya & Schöner
   spired model that may allow embodied agents to autonomously
   learn sequences of action primitives (forming an overall be-          (2010) is the existence of ordinal nodes which essentially
   haviour). Specifically, we combine a flexible model of se-            count through the sequence. These nodes are implemented
   quence generation with a model of parietal mirror neuron ac-          via coupled dynamical systems (see Methods), designed so
   tivity. The main purpose is to illustrate that the approach is
   viable. Although further work is needed to improve the re-            that only one node can be active at a time. Upon comple-
   sults sketched out here, the concept is sound and relevant both       tion of the element of the sequence represented by the ac-
   to efforts in modelling mirror neuron activity and enabling ar-       tive node, activation is passed onto the next node in the se-
   tificial embodied agents to autonomously learn sequences of
   action primitives.                                                    quence. In their work (e.g. Sandamirskaya & Schöner, 2010;
                                                                         Sandamirskaya et al., 2011), the action primitives forming the
   Keywords: Behavioural sequence learning; Ordinal node                 sequence exist in the sensorimotor representation of an em-
   model; Self-organising maps; Mirror neurons
                                                                         bodied agent, implemented using techniques from Dynamic
                                                                         Field Theory (Schöner, 2009; Spencer et al., 2009). This has
                          Introduction                                   the advantage that the sensorimotor representations of these
                                                                         primitives are stable (since they are essentially stable fixed-
We are concerned with the problem of generating sequences                point attractors), which makes it particularly simple to link
of action primitives which are flexible with respect to the pre-         specific locations in the dynamic fields representing the sen-
cise time it takes to execute the different components (primi-           sorimotor space of the agent to specific ordinal nodes. Part
tives) of the same sequence at different times. A thorough dis-          of the challenge of the work presented in the present paper
cussion of the issue is given, for instance, by Sandamirskaya            is to illustrate that the ordinal node system could also be at-
& Schöner (2010). In a nutshell, part of the problem is                 tached to a representation with more noise and less stability
that one cannot simply chain together the different primi-               than dynamic fields.
tives through, for example, simple Hebbian learning. Rather,                The decision that a given action primitive has completed
mechanisms must exist for keeping track of the current loca-             is implemented a separate system (also exploiting dynamic
tion in the sequence, including ways of verifying that the cur-          fields) that checks for a Condition of Satisfaction (CoS). One
rent action has successfully completed or failed to complete.            of the open challenges here is the question of how to best
Sandamirskaya & Schöner (2010) describe a general frame-                learn the CoS for specific primitives (including identifying
work which can address these issues and we briefly sketch the            that the primitive has, for whatever reason, failed). It is not
main points in the next section.                                         the purpose of the present work to address the open issues
   Overall, the aim of the work in the present paper is to com-          regarding the CoS - rather, we focus on combining the ordinal
bine said framework with a model of parietal mirror neuron               node model with a model of mirror neuron activity discussed
activity (Thill et al., 2011) and to illustrate that such an ap-         in the next section.
proach is, in principle, viable. Importantly, since the mirror
neuron model used here autonomously organises itself, the                Mirror system sequences
work proposed here may be relevant and helpful in designing              One example of sequencing in biology is given by the hy-
artificial embodied agents that should autonomously learn se-            pothesised functioning of the mirror system. Without enter-
quences of actions and use them to predict actions of others.            ing the debate on what higher-level cognitive abilities mirror
                                                                     2423

neurons may or may not be useful/essential for (see for in-          Combining models
stance Hickok, 2008; Rizzolatti & Sinigaglia, 2010, for such         Previous models of parietal mirror neuron activation tend not
a debate), it appears that parietal mirror neurons in macaque        to address the timing aspect of the chains in much detail, fo-
monkeys organise into pools of neurons responding to spe-            cussing instead on merely linking the different pools forming
cific motion primitives (e.g. a reach or a grasp but not both;       a chain through hard-coding (Chersi et al., 2006, 2010) or, for
Fogassi et al., 2005). It has then been hypothesised (e.g.           instance, Hebbian learning (Erlhagen et al., 2007). With the
Chersi et al., 2006) that these pools of neurons can be chained      exception of Chersi et al. (2010), these models do not take
together to form sequences of simple, often-encountered ac-          into account that the pools encoding the same primitive under
tions (such as reach-grasp-bring to mouth for eating). Mod-          different goals are not entirely distinct (Fogassi et al., 2005).
els on the basis of this hypothesis have proven useful, for in-      Thill et al. (2011), whose main focus is the exact nature of
stance, in putting forward theories unifying apparently con-         this overlap between populations, do not specifically address
flicting results on interference and facilitation in action lan-     chain formation at all.
guage processing (Chersi et al., 2010).                                 The present paper therefore presents an augmented version
   A particular model that specifically addresses the devel-         of the model from Thill et al. (2011). Specifically, we now
opment of parietal mirror neurons has been previously pre-           implement the learning of chains of primitives, using the ap-
sented by some of us (Thill et al., 2011). This model uses a         proach of Sandamirskaya & Schöner (2010). This new model
self-organising map (SOM) to illustrate how a “blank” struc-         then allows us to address a number of open issues: to what ex-
ture, through the organisational principles of SOMs can au-          tent is the ability to activate the correct (and only the correct)
tonomously form an organisation whose activity resembles             sequence of events (given the first element) affected by the
that of parietal mirror neurons.                                     overlap between neural populations? When observing an ac-
                                                                     tion primitive in an unknown context, is it possible to predict
   The inputs to the model represent an arbitrary encoding           all possible chains this action could be part of?
of observed (or executed) motion primitives (e.g. based on              These issues are relevant, both for our understanding of
changes in position per time step) and contextual informa-           (in particular) sequences in mirror neuron activity and for the
tion (including, for instance, affordances in the perceived          ability to endow artificial agents with similar abilities. If one
scenery). These two components are sampled from two dis-             subscribes to the hypothesis that mirror neuron activity helps
tinct spaces (of arbitrary dimensionality) and concatenated          us understand the actions of others (see Rizzolatti & Sini-
into a single input vector as required by standard SOM im-           gaglia, 2010, for a thorough review and discussion), then the
plementations (Kohonen, 1997). The model is trained on re-           ability to predict the likely outcome of an action given the ini-
peated presentations of all combination of motion primitives         tial movement based on the resulting mirror neuron activity is
and contexts. After training, the model can be run on-line by        a desirable ability. This includes the ability to autonomously
continuously feeding it input vectors and some plasticity (al-       learn sequences of actions as well as the ability to both cor-
lowing, for instance, the learning of new primitives) can be         rectly identify a sequence if the context is clear and predict all
retained by not reducing the learning rate to 0 (albeit keeping      possible sequences if the context is ambiguous (for instance,
it at a low level, see Thill & Ziemke, 2010).                        a familiar gesture observed in a completely new context).
   The trained maps organise in a fashion remarkably simi-                                       Methods
lar to that of parietal mirror neurons (Fogassi et al., 2005):
Within the map, different areas encode different action primi-       Overall model design
tives (which could represent motions such as reaching, grasp-        The model (Fig. 1) is composed of a self-organising map
ing or bring-to-mouth, similar to e.g. Chersi et al., 2006).         which is meant to represent parietal mirror neuron activation
Within the area encoding one such primitive, some nodes are          (Thill et al., 2011) and an ordinal node model for sequence
active whenever the model input encodes that primitive. Oth-         learning (Sandamirskaya & Schöner, 2010). The activity over
ers are active only if the action input additionally encodes a       time in the SOM is used (1) to train the sequence learning
specific context in which the primitive is observed (usually         model, (2) to activate learned sequences and (3) to provide
sufficient to specify the most likely goal of the action, see        the input necessary to move from one sequence element to
Thill et al., 2011). The proportion of context-independent           the next. It therefore combines the idea of chaining pools of
nodes is a direct consequence of the way inputs are repre-           neurons (e.g. Chersi et al., 2006) with the flexible execution
sented (specifically, of the ratio between the maximal vari-         of sequences provided by the ordinal nodes model of San-
ability in encoding the primitives and contextual information        damirskaya & Schöner (2010).
respectively, called β in the model). Exploring how β (for
which values between 1 and 5 cover most aspects of interest)         Self-organising maps as a mirror system
affects the organisation of the maps revealed that, for β ≈ 3.5,     The self-organising maps used in this paper are in essence
the proportion of context-independent nodes is similar to the        identical to those used by Thill et al. (2011) and are trained
corresponding neurophysiological data observed in the pari-          in the same manner. The only difference is that the previous
etal mirror area of macaques (Fogassi et al., 2005).                 maps explicitly dedicated part of their space to the theoretical
                                                                 2424

                                     Memory nodes                                         Primitive 1   Primitive 2    Primitive 3
                                     Ordinal nodes                            Context 1
         I          IC                 CoS fulﬁlled
                                     SOM                                      Context 2
Figure 1: Overall model architecture. Activation in the
SOM feeds into the ordinal nodes, both to activate the se-              Figure 2: Sequences in the SOM. Shown are the activation of
quence (green) and to move between sequence elements (red)              three primitives (columns) seen under two different contexts
if the CoS is fulfilled (blue). Connections between the SOM             (rows) for a map with β = 3. Dark regions indicate most ac-
and the nodes are bidirectional; node activity can thus also be         tivity and are clearly in different locations for different prim-
used to activate regions in the SOM (omitted in the figure for          itives. For the same primitives in different contexts, similar
clarity).                                                               regions are activated but the overlap between the most active
                                                                        neurons in each context is limited (see Thill et al., 2011, for a
                                                                        thorough discussion).
possibility of learning motion primitives from a second limb
(see Thill & Ziemke, 2010, for details). Since that is irrele-
vant here, the present maps are trained assuming the need to
represent just one limb. The trained maps therefore represent,
as before, five motion primitives observed under two different          functioning, see Sandamirskaya & Schöner (2010) for de-
contexts. They behave as described in the introduction: input           tails), f (·) is a sigmoidal nonlinearity and the constants in
vectors consisting of a concatenation of observed/executed              the present implementation are chosen as: c0 = 7.2, c1 = 3.6,
motion encoding and contextual information are continuously             c2 = 4.8, c3 = 0.8, c4 = 4, c5 = 2, c6 = 2.6, cin = 0.1,
fed to the map. Depending on the previously discussed ratio             cCoS = 0.2, hd = −5 and hm = −2. A detailed discussion
β, some nodes of the map will be active regardless of the               of the functioning of the model is given by Sandamirskaya &
contextual information whereas others will be sensitive to the          Schöner (2010). We deviate in two minor aspects: (1) The
latter (see Thill et al., 2011, for a complete discussion of the        term cin I is added and provides an external input (obtained
definition of activity).                                                from the activity in the SOM described above). This is only
Ordinal node model                                                      used at the beginning of a sequence to activate the first ordinal
                                                                        node. (2) We simplify the Condition-of-Satisfaction (CoS)
The ordinal node model used here largely follows San-                   aspect. In the original model, this is given by an additional
damirskaya & Schöner (2010) and is described by the fol-               dynamic field which is able to “perceive” that the CoS has
lowing equations:                                                       been reached. Here, the inhibitory activation is obtained from
                                                                        the same SOM that would provide I for the activation of the
        τḋi (t)   = −di (t) + hd + c0 f (di (t))              (1)      first node, which simplifies the design of the model. Since
                                                                        the model is not actually implemented in an agent, there is
                     −c1 ∑ f (di′ (t)) + c2 f di−1m
                                                        
                                                    (t)                 also no point in devising a sophisticated ”perception” of the
                            i′ 6=i
                                                                        CoS here. Rather, the CoS is presumed fulfilled after a ran-
                       −c3 f (dim (t)) − cCoS IC (t) + cin I            domly chosen number of time-steps and the inhibitory acti-
                                                                        vation released to the ordinal model, thus moving the model
       τd˙im (t)   = −dim (t) + hm + c4 f (dim (t))            (2)      onto the next element of the sequence. This is acceptable for
                     −c5 ∑ f (di′ (t)) + c6 f (di (t))
                                                                        the present purposes since the point here is to illustrate the
                            i′ 6=i
                                                                        learning of sequences, not the ability to autonomously de-
                                                                        tect that an element of a sequence has completed (or failed
  where di refers to the activation of the ith ordinal node             to complete). An implementation of this model in an agent
(and dim is the associated memory node needed for proper                would of course need to address this aspect in more detail.
                                                                     2425

Task and learning
                                                                                    1
For each β value between 1 and 5 (in increments of .5), 100
                                                                                   0.9
maps have been generated. Each map is activated manually
with a series of input vectors which simulate a sequence of 3                      0.8
motion primitives being executed first in one context and then                     0.7
in another (see Fig. 2 for an example of two sequences). Two                       0.6
                                                                      Proportion
sets of ordinal nodes are used to learn these two sequences.
                                                                                   0.5
Learning is achieved during manual activation of the map by
                                                                                   0.4
clamping the relevant ordinal node to an active state and then
using simple Hebbian learning to train weights between this                        0.3
node and all neurons in the map (with normalised activation).                      0.2
After training, any weights below a threshold of 0.5 are set to                              Correct activation
                                                                                   0.1
0 to allow only the SOM nodes with the strongest activation                                  Correct non−activation
                                                                                    0
to connect with the relevant ordinal nodes.                                              1           2            3   4   5
   Of particular interest are the following questions: Will both                                                  β
sets of ordinal nodes correctly activate if the SOM activity is
that of the first element of their respective sequences? Also,     Figure 3: Correct activation/non-activation. Dark bars in-
will a set of ordinal nodes trained on the first sequence re-      dicate the proportion of cases in which presenting the first el-
main inactive if the SOM activity represents the first element     ement of a sequence correctly triggered the sequence. Light
of the second sequence (and vice versa)? Illustrating these        bars indicate the proportion of the cases that correctly trigger
behaviours would confirm good performance of the model             which correctly remain silent if the first element presented
given that sequences are correctly activated if and only if the    has the same motion primitive but different contextual infor-
map activity corresponds to their first element. It should be      mation.
remembered at this point that map activity is noisy and fluc-
tuates over time - the task is therefore not trivial.
   An additional interest is the behaviour of the model in case    ond sequence) becomes harder as β increases. This is indeed
of ambiguous contextual information. As discussed in the           what we find (see Fig. 3). Specifically, it is possible, in most
introduction, this could correspond to observing a familiar        cases, to correctly activate a sequence by presenting its first
primitive in an unfamiliar context and predicting what the         element in map activation (although it does fail on occasion,
likely outcome of the action could be. It is of course a matter    likely due to the noisy map activity). Importantly, this is in-
of debate what the exact behaviour of the model should be          dependent of β, which is expected. The light bars in Fig. 3
in this case; one could for instance argue that it should de-      then show how many (proportionally) of the sequences cor-
pend on how similar the unfamiliar context is to previously        rectly activated by their own first element also remain silent
encountered ones. Here, we simply investigate the behaviour        when the first element of the second possible sequence is pre-
if the vector encoding contextual information is truly am-         sented instead. As expected, this number decreases over time
biguous, namely by corresponding to the point in the input         but remains over .5 in all cases.
space whose coordinates are equidistant from the subspaces            However, this measure iterates over sequences that are cor-
encoding all known contexts. In other words, the ambigu-           rectly activated (or not); it does not measure the number of
ous context encoding vector cannot be uniquely assigned to         maps for which both sequences are correctly activated (or
any previously encountered case. We simply postulate that,         not). The evolution of this proportion is shown in Fig. 4
in the absence of any information that could favour either of      (black bars) and is decreasing more dramatically as β in-
the chains, the desirable behaviour of the model is to activate    creases. At the same time, it should be noted that for e.g.
both, essentially predicting that both behaviours are equally      β = 4, ≈ 60% of nodes in the SOM encoding a given primitive
likely.                                                            are active independent of context (leaving only 20% capable
                                                                   of uniquely identifying each of the contexts).
                           Results
Correct activation/non-activation                                  Correct behaviour under ambiguous context
For each value of β, 100 sets of 2 sequences have been             The second interesting question was whether both sequences
learned. Per set, the sequences differ only in the context in      would be activated by the first motion primitive shown in a
which they have been executed. As β increases, the propor-         perfectly ambiguous context. Considered independently of
tion of neurons active in one but not both of the contexts de-     the performance on the previous task, we find that a large
creases (Thill et al., 2011). It can therefore be expected that    number of models indeed activate both sequences given an
the basic task of correctly activating a sequence if the map       ambiguous context In particular, we find that this proportion
activity corresponds to its first element (and not activating      increases with β (from0.4 to > 0.9), likely due to the increas-
said sequence if the contextual information is that of the sec-    ing number of neurons which are active irrespective of con-
                                                               2426

                                                                                                 Discussion
                 1
                                      Fully correct on trained          Insights from the model
                0.9                   Correct prediction
                0.8                                                     Most of the results shown have their main purpose in illustrat-
                                                                        ing that the model works as expected, including the increas-
                0.7
                                                                        ing difficulty in obtaining “perfect” models as the β values
                0.6
   Proportion
                                                                        of the underlying maps increase. The exact extent of this in-
                0.5                                                     crease in difficulty is hard to judge from the work presented
                0.4                                                     here as several aspects can be improved. First and foremost,
                                                                        the parameters for the ordinal nodes model are set indepen-
                0.3
                                                                        dently of β, even though β has a rather significant effect on
                0.2                                                     the input into the ordinal nodes and therefore the behaviour
                0.1                                                     of the sequences. The fact that it was possible at all to create
                 0                                                      successful models across the entire range of β values under-
                      1   2      3           4            5
                                 β                                      lines the potential of the approach. In future work, however,
                                                                        the focus would have to be on β values around 3 − 3.5, since
                                                                        these are the values for which the activity in the SOM most
Figure 4: “Perfect” models. Black bars indicate proportion              resembles that observed by Fogassi et al. (2005) in parietal
of cases in which the same model activates the correct se-              mirror neurons (Thill et al., 2011).
quence for each of the two learned sequences if presented                  The self-organising maps themselves are randomly gener-
with the first element of each sequence and does not activate           ated; nonetheless it seems that some are more suited for a
the wrong sequence. Light bars indicate the proportion of               combination with an ordinal nodes model than others (since
cases fulfilling the first condition which also correctly activate      some “perfect” models were found even for β = 5, although
both sequences if the contextual information is ambiguous               the number was very low) and more work would be needed
.                                                                       to investigate what features of these maps, if any, facilitate
                                                                        the task. Insights into this question could prove very valu-
                                                                        able in more general future work combining the ordinal node
text.                                                                   model with sequences that are generated in systems which do
                                                                        not offer the “nice and clean” activation patterns of dynamic
   This illustrates the expected effect of β: as the distinction        fields.
between contexts diminishes, activation of both sequences is               The connections between the activity in the map and the or-
facilitated. However, this measure can again be seen as be-             dinal nodes are learned with a simple general Hebbian learn-
ing a bit too general since there is not necessarily anything           ing approach and the only transformation of the map activity
special about activating both sequences if the same model               consisted of a simple normalisation. Again, this is about the
failed to not activate a sequence when primed with the first            simplest approach imaginable and it is likely that improve-
element (including the contextual information) of the sec-              ments, including possible non-linear transformations of map
ond sequence. The more interesting question is therefore                activity, can lead to a higher proportion of “perfect” models
simply how many of the models that correctly behave given               for larger values of β (in particular of course for β ∈ [3, 3.5]).
the learned sequences (black bars in Fig. 4 also behave as                 The most interesting result in the present paper was that the
expected given the first primitive under an ambiguous con-              largest difficulty resided in finding models which perform as
text (within the context of this work, we can call these mod-           expected when started with a first element from either learned
els “perfect”, since they fulfil all the expectations set out to        sequence and not, as one might have expected, in finding such
them). Surprisingly, this proportion appears to be indepen-             a model that also perform correctly on the prediction task.
dent of β (light bars in Fig. 4), although it has to be kept in         This is encouraging as it illustrates that the concepts of using
mind that for β ≥ 3, the number of models which fulfill the             a combination of our previous SOM models of mirror neurons
first condition is rather low.                                          and the ordinal node model has potential, not just for generat-
   In other words, if a model is capable of correctly activating        ing the sequences one wishes to generate but also for predict-
the relevant sequence (and only that sequence) given a full             ing what sequences observed actions can be part of; this both
first element of that sequence, it is likely to also activate both      in the case where the contextual information strongly favours
sequences if given the first primitive under an ambiguous con-          one of the learned sequences and when the contextual infor-
text. This is the most significant result in the present paper:         mation is perfectly ambiguous.
although it is increasingly difficult to find a model which will           Again, there is a need for future work in this aspect. It
correctly activate its sequences given the first element as β in-       seems reasonable (for the purposes of predicting likely se-
creases, it is then much easier to find a model which can also          quences an observed primitive could belong to) to expect that
activate both sequences in the case of perfectly ambiguous              a perfectly ambiguous context should activate all candidates
contextual information.                                                 but it is less clear - and beyond the scope of what can be
                                                                     2427

achieved in this space - what should happen if the context is          motor chains.         Frontiers in Neurorobotics, 4(4),
merely ambiguous but closer in input space to some known               DOI:10.3389/fnbot.2010.00004.
contexts than others. Should the model simply activate the           Erlhagen, W., Mukovskiy, A., Chersi, F., & Bicho, E. (2007).
most likely sequence or would one prefer a mechanism that              On the development of intention understanding for joint
could attach a confidence value - indicated for instance by the        action tasks. In Proceedings of the 6th ieee international
time it takes the first ordinal nodes of all candidate sequences       conference on development and learning (p. 140-145). Im-
to activate - to indicate most and least likely sequences?             perial College London.
Overall relevance                                                    Fogassi, L., Ferrari, P. F., Gesierich, B., Rozzi, S., Chersi,
                                                                       F., & Rizzolatti, G. (2005). Parietal lobe: from action
The work presented here is relevant for at least two areas.
                                                                       organization to intention understanding. Science, 308, 662-
First is the modelling of mirror neurons as it is one of the
                                                                       667.
first attempts to explicitly include the idea that executing the
same action primitive at different points in time can lead to        Hickok, G. (2008). Eight problems for the mirror neuron
different durations, thus going beyond simple Hebbian-type             theory of action understanding in monkeys and humans.
associations directly between the primitives forming an over-          Journal of Cognitive Neuroscience, 21(7), 1229-1243.
all action sequence (e.g. Chersi et al., 2010). Second, by           Kohonen, T. (1997). Self-organizing maps. Heidelberg:
modelling the specific organisation of parietal mirror neurons         Springer.
(which can develop autonomously, see Thill et al., 2011) and         Rizzolatti, G., & Sinigaglia, C. (2010). The functional role of
using that as an input to the ordinal node system, the model           the parieto-frontal mirror circuit: interpretations and mis-
may provide a way for an artificial agent to learn sequences           interpretations. Nature Reviews Neuroscience, 11(4), 264-
of primitives online and autonomously, which is still an open          274.
challenge (Sandamirskaya & Schöner, 2010).                          Sandamirskaya, Y., Richter, M., & Schöner, G. (2011). A
   The practical future applications are thus primarily in the         neural-dynamic architecture for behavioral organization of
design of future artificial cognitive systems; however all as-         an embodied agent. In Ieee 10th international conference
pects of the model are inspired by biology; any implementa-            on development and learning (icdl), frankfurt.
tion of the model could thus also be relevant to improve our         Sandamirskaya, Y., & Schöner, G. (2010). An embodied
understanding of the analogous biological systems.                     account of serial order: how instabilities drive sequence
                                                                       generation. Neural Networks, 23, 1164-179.
                          Conclusion
                                                                     Schöner, G. (2009). Toward a unified theory of develop-
We presented an initial implementation of a mirror system ac-
                                                                       ment. In J. P. Spencer, M. S. C. Thomas, & J. L. McClel-
tivity model augmented with a framework for generating se-
                                                                       land (Eds.), (p. 25-48). Oxford.
quences. The main purpose was that it is in principle feasible
to use the ordinal node framework to this effect. Although           Spencer, J. P., Perone, S., & Johnson, J. S. (2009). To-
further work is needed to improve the quality, it was possi-           ward a unified theory of development. In J. P. Spencer,
ble to show that the model can learn sequences based on the            M. S. C. Thomas, & J. L. McClelland (Eds.), (p. 86-118).
noisy SOM activity as well as correctly predict the likely se-         Oxford.
quence an observed initial primitive can belong to (including        Thill, S., Svensson, H., & Ziemke, T. (2011). Modeling the
predicting both if both are equally likely). Since the SOM             development of goal-specificity in mirror neurons. Cogni-
autonomously organises, the model presented here may be a              tive Computation, 3(4), 525-538.
viable candidate for autonomous sequence learning using the          Thill, S., & Ziemke, T. (2010). Learning new motion primi-
ordinal node framework (Sandamirskaya & Schöner, 2010).               tives in the mirror neuron system: A self-organising com-
                                                                       putational model. In S Doncieux et al (Ed.), Sab 2010, lnai
                     Acknowledgments                                   6226 (p. 413-423). Heidelberg: Springer.
This work was supported by the European Commission
FP7 project NeuralDynamics, (A neuro-dynamic framework
for cognitive robotics: scene representations, behavioral se-
quences, and learning), Grant agreement no. 270247.
                          References
Chersi, F., Mukovskiy, A., Fogassi, L., Ferrari, P. F., & Erlha-
   gen, W. (2006). A model of intention understanding based
   on learned chains of motor acts in the parietal lobe. In Pro-
   ceedings of the 15th annual computational neuroscience
   meeting. Edinburgh, UK.
Chersi, F., Thill, S., Ziemke, T., & Borghi, A. M.
   (2010).      Sentence processing: linking language to
                                                                 2428

