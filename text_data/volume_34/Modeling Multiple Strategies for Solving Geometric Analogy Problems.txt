UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Multiple Strategies for Solving Geometric Analogy Problems

Permalink
https://escholarship.org/uc/item/7xr7b0dx

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)

Authors
Lovett, Andrew
Forbus, Kenneth

Publication Date
2012-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Modeling Multiple Strategies for Solving Geometric Analogy Problems
Andrew Lovett (andrew-lovett@cs.northwestern.edu)
Kenneth Forbus (forbus@northwestern.edu)
Qualitative Reasoning Group, EECS Department, 2133 Sheridan Road
Evanston, IL 60208USA

In the following section, we provide some background on
the geometric analogy task. We then present our
computational model, which utilizes two strategies for
performing the task. Afterwards, we compare the model
against human performance and discuss the results. We
close with related work and conclusions.

Abstract
We present an improved computational model for performing
geometric analogy. The model combines two previously
modeled strategies and makes explicit claims about when
people will use one strategy or the other. We compare the
model to human performance on a classic problem set. The
model’s strategy shifts, along with working memory load,
account for most of the variance in human reaction times.

Background

Keywords: geometric analogy; visual problem-solving;
structure-mapping

Geometric analogy involves comparing images to identify
differences. However, researchers disagree on which
comparisons people make. The debate encompasses two
competing strategies. The first involves inserting each
possible answer into the analogy to evaluate it. Consider
Figure 1A. The strategy proceeds as follows:
1) Compare A and B to get Δ(A,B), the differences
between A and B. Here there is a change from two
overlapping objects to one object inside the other.
2) For each possible answer i, compare C to i to get
Δ(C,i), the differences between C and that answer. Then,
perform a second-order comparison: measure the similarity
of Δ(A,B) to Δ(C,i). Whichever answer produces the most
similar set of differences is chosen. Here answer 3 produces
an identical set of differences to Δ(A,B), so it is chosen.
The second strategy solves for the answer directly:
I) Compare A and B to get Δ(A,B).
II) Compare A and C to get the corresponding objects. In
Figure 1A, the large leftmost shapes correspond, and the
small rightmost shapes correspond.
III) Apply the differences in Δ(A,B) to the corresponding
objects in C to infer D’, a representation of the answer.
Here the small rectangle in C would move inside of the
larger shape. Pick the answer most similar to D’.
Mulholland, Pellegrino, and Glaser (1980) call the first
strategy infer-infer-compare and the second strategy infermap-apply. However, this assumes that different processes
are used to compare images in steps 1), 2), and II). We
believe structure-mapping can determine differences,
identify correspondences, and measure similarity.
Therefore, we instead call the strategies second-order
comparison and visual inference.
Sternber (1977) argued that people use visual inference to
perform geometric analogy. However, Mulholland,
Pellegrino, and Glaser (1980) found evidence that secondorder comparison was being used. Bethell-Fox, Lohman,
and Snow (1984) suggested that individuals may adjust their
strategy, depending on problem difficulty. Their eyetracking data demonstrated that people typically use visual
inference, solving directly for the answer. However, as
problems become more difficult, people may abandon this

Introduction
Visual problem-solving has long been a popular tool for
evaluating people’s cognitive abilities (Raven, Raven, &
Court, 1998; Dehaene et al., 2006). Problem-solving tasks
frequently involve a sequence of images (e.g., Figure 1).
Individuals must compare the images, identifying some
pattern across them. They must then apply this pattern,
finding the answer that best completes (or violates) it.
Visual problem-solving depends on a comparison process
for identifying commonalities and differences in images.
We have previously argued that structure mapping (Gentner,
1983), a theory of analogical comparison, may also explain
concrete visual comparison in humans (Markman &
Gentner, 1996; Lovett et al., 2009a; Sagi, Gentner, &
Lovett, in press). According to structure mapping, people
compare stimuli by aligning the common relational structure
in symbolic, qualitative representations. We have posited
that structure mapping may play a ubiquitous role,
identifying commonalities and differences and estimating
similarity. Based on this hypothesis, we have built models
of three visual problem-solving tasks: geometric analogy
(Lovett et al., 2009b), Raven’s Progressive Matrices
(Lovett, Forbus, & Usher, 2010), and the oddity task (Lovett
& Forbus, 2011a).
Here, we complement our model of visual comparison
with a model of visual inference. Visual inference explains
how individuals apply a set of differences to one image to
create a novel image representation. It plays a key role in
tasks such as geometric analogy (Figure 1), where
participants are asked “A is to B as C is to…?” We show
how this leads to a new model for geometric analogy.
Rather than assuming participants always infer the correct
answer, the model makes explicit claims about when visual
inference will succeed and when it will fail, requiring a shift
to an alternate strategy. The model’s strategic shifts
correlate well with human reaction times on a classic
geometric analogy problem set (Evans, 1968).

701

A)

B)

C)

D)

E)

F)

Figure 1: Geometric analogy problems with average response times and percent selecting each answer (Lovett et al., 2009b).
approach, instead trying out each possible answer in the
analogy.
One might conclude that people use visual inference when
problems are easy and second-order comparison when
problems are relatively difficult. However, this is not the
full story. Sternberg (1977) found that participants used
visual inference on multiple choice problems, where they
had to consider several possible answers. Mulholland,
Pellegrino, and Glaser (1980) found that participants used
second-order comparison on true/false problems, where they
saw a completed analogy and simply judged whether it was
correct. The Sternberg problems appear more difficult—at
the very least, they put more load on working memory, as
there are multiple answers to consider. Why, then, would
people use second-order comparison on the easier
Mulholland et al. problems?
We believe the answer lies in the algorithmic complexity
of the two strategies. In the analysis below, we compute
each strategy’s complexity by counting the number of
comparisons necessary to solve a problem. We concede that
this may be a simplification; for example, second-order
comparisons may require more time and effort than others.

However, our model predicts that a common process
(structure-mapping) is used for all comparisons. Thus the
number of times this process repeats should provide a
reasonable approximation of a strategy’s complexity.
Consider second-order comparison. If there are n
possible answers, then the number of comparisons is 1 (A to
B) + n (C to each answer) + n (Δ(A,B) to Δ(C,n) for each
answer) = 2n + 1.
Now consider visual inference. The number is 1 (A to B)
+ 1 (A to C) + n (D’ to each answer) = n + 2. However, this
strategy also requires a non-comparison operation: inferring
D’ by applying Δ(A,B) to the corresponding objects in C.
Suppose we have a true/false problem. Then the number
of answers n = 1. The number of comparisons is 3 for both
strategies. In this case, one might prefer second-order
comparison, as it doesn’t requiring inferring D’. This
explains why Mulholland et al. found that participants used
second-order comparison.
Suppose we have a multiple-choice problem. The number
of answers n > 1. Now, there will be fewer comparisons for
visual inference, and this should be the preferred strategy, as
Sternberg found. However, if a problem is particularly

702

complex, participants may be unable to perform the
inference operation. In this case, they may fall back on the
second-order comparison strategy, which requires only
comparison operations.
Our model, described below, attempts to solve problems
via visual inference. When it fails, either because it cannot
perform the inference or because the inferred image doesn’t
match any of the answers, the model reverts to second-order
comparison. Thus, the model can help explain why some
problems take longer to solve than others: certain problems
require a shift from the default visual inference strategy to a
less efficient second-order comparison strategy.

SME computes up to 3 mappings between the compared
cases. Each mapping contains: A) a similarity score based
on the breadth and depth of aligned structure; B)
correspondences between the entities and expressions in the
two cases; and C) candidate inferences (CIs), inferences
based on expressions in one case that failed to align with the
other. For example, consider the A/B comparison in Figure
1A. A forward CI (A->B) would indicate that the two shapes
no longer partially overlap. A backward CI (B->A) would
indicate that one shape no longer contains the other. CIs are
useful for identifying differences between the cases.
The model compares two images via the following steps:
1)
Compare the qualitative image representations
using SME. This produces a mapping indicating the
corresponding objects, commonalities, and differences.
2)
For each pair of corresponding objects, compare
the objects’ shapes to identify a shape transformation. This
is done by comparing the objects’ edge-level representations
to get corresponding edges, and then using those
correspondences to compute a transformation, such as a
rotation or reflection (see Lovett & Forbus, 2011b).
Sometimes there are multiple valid transformations. For
example, in Figure 1C, there is both a rotation and a
reflection between the ‘B’ shapes. In such cases, the model
picks the simplest transformation, according to the
following rankings: identity, reflection, rotation. Objects
can also change scale, becoming larger or smaller.
3)
Compute the similarity between images. This is
primarily based on SME’s similarity score, but it is updated
according to the shape comparisons: if two objects are
identical, the images will be rated more similar.
4)
Compute a qualitative, structural representation of
the differences between the images. This describes
differences of the following types:
A) Spatial relation addition/removals, based on the CIs.
B) Reversals of spatial relations. This is a special case of
A) where two objects swap places in a relation. In Figure
1D, the dot and triangle swap places in an above relation.
C) Object additions/removals, where objects are added or
removed between images (e.g., Figure 1B).
D) Object transformations, where there is a shape
transformation between corresponding objects.

Model
Our model depends on three core processes: perception,
visual comparison, and visual inference. Below, we describe
each process and then show how the model combines these
processes to implement different problem-solving strategies.

Perception
Our model uses the CogSketch sketch understanding system
(Forbus et al., 2011) to generate a qualitative representation
for each image. Qualitative, or categorical, representations
are abstract, describing features like relative position (right
of) or relative orientation (parallel), rather than exact
numerical sizes and orientations. There is abundant
psychological evidence that people are sensitive to such
features (e.g., Huttenlocher, Hedges, & Duncan, 1991;
Rosielle & Cooper, 2001).
CogSketch performs sketch understanding, rather than
full vision. It requires users to draw separate line drawings
of each object in a visual scene (e.g., the rectangle and
triangle in image A of Figure 1A). Given these objects,
CogSketch automatically computes spatial relations between
objects and attributes for individual objects.
Our model takes the process a step further. It can
automatically segment an object into edges and build an
edge-level representation, describing qualitative spatial
relations between the edges. Alternatively, it can group
several objects together based on similarity to build a grouplevel representation. See Lovett and Forbus (2011b) for
details on this process, as well as the full vocabulary of
qualitative terms at the edge, object, and group levels.

Visual Inference
The visual inference operation applies a set of differences to
an image to produce a novel image representation. In
geometric analogy, the A/B differences are applied to C to
produce D’, a representation of the answer image. Consider
Figure 1A. Inference proceeds as follows:
1)
Compare image A to image C to get the
corresponding objects.
2)
Apply the A/B differences to the corresponding
objects in C to produce a new qualitative representation:
A) Add or remove spatial relations.
B) Reverse the arguments of spatial relations.
C) Add or remove objects. If an object is added, create a
new object in CogSketch, basing it off some existing object.

Visual Comparison
We model visual comparison using the Structure-Mapping
Engine (SME) (Falkenhainer, Forbus, & Gentner, 1989), a
computational model based on Genter’s (1983) structuremapping theory. Given two cases described in predicate
calculus, it computes a mapping between them by aligning
their common relational structure. SME is biased to prefer
aligning deep structure. For example, at the edge level, a
first-order relation might indicate that there is a convex
corner between two edges. A second-order relation might
indicate that two convex corners are adjacent. These
higher-order relations, which take other relations as their
arguments, receive priority during mapping.

703

If an object is removed, remove any spatial relations
referring to that object.
D) For all other objects, apply the appropriate shape
transformation to the object in C to create a new object for
D’. This might mean leaving the shape unchanged
(identity), rotating it, reflecting it, scaling it, etc.
E) Compute shape attributes for all the newly created
objects, and add them to the D’ representation.
Note that D’ contains: a) a qualitative, structural image
description; and b) a set of concrete, quantitative objects.
Thus, it contains just enough to support visual comparisons
between D’ and other images. However, D’ is not a
concrete image: the model lacks exact, quantitative
locations for each object.
There are several ways that visual inference can fail:
A) When a spatial relation cannot be added because the
objects it describes are not found in C (there are no
corresponding objects).
B) When a spatial relation cannot be reversed. In Figure
1D, there is a reversal of above in the A/B differences.
However, there is no above in C to reverse.
C) When an object cannot be removed or transformed
because the object is not found in C.
D) When transforming an object doesn’t produce the
desired effect. On Figure 1E, the model reflects the ‘B’
shape over the x-axis. However, when it compares the
result to the original ‘B’ shape, they appear identical (recall
that identity is ranked before reflection). The model treats
this as a failure to transform.
The model is focused on generation: adding expressions
to C’s representation to produce D’. Thus, visual inference
fails when a spatial relation cannot be added to C or
reversed in C, but it does not fail when a spatial relation
cannot be removed from C. For example, in Figure 1B, the
A/B differences include removing a rightOf relation. There
is no such relation in C to be removed. Visual inference
succeeds here, whereas it fails in 1D, where there is no
above relation in C to be reversed. Thus, the model explains
why 1D is a harder problem (compare the reaction times).

sensitivity analysis shows that our results would be the same
for values ranging from .67 to .87. If multiple answers tie
for the best score, this is treated as a failure.
Note that when SME compares Δ’s for second-order
comparison, it is possible to find a perfect match even for
non-identical Δ’s. SME supports tiered identicality
(Falkenhainer, 1990), where non-identical predicates can
align when they are members of a common category. For
example, in Figure 1D, Δ(A,B) and Δ(C,3) each involve
reversal of a positional relation (above and rightOf). Thus,
Figure 1D is not solvable by visual inference, but it is easily
solvable by second-order comparison.
Strategic Shifts The model first attempts to solve a problem
via visual inference. This can fail in two ways: either the
inference operation may fail (Figures 1D, 1E), or D’ may
fail to match any of the answers. For example, in Figure 1F,
the A/B differences show the inner shape being removed.
The model applies these differences to C to infer an image
with a large circle, which matches none of the answers.
If visual inference fails, the model reverts to second-order
comparison. When the model utilizes this strategy, it must
make two other strategic decisions: the comparison mode
when comparing A to B, and the comparison mode when
comparing C to each answer i. The comparison modes are:
A) Normal: Images are compared as described above.
B) Reflection: Instead of preferring identity during shape
comparison, the model prefers reflection. In Figure 1E, the
C/3 comparison will find a y-axis reflection between the ‘B’
shapes, instead of treating them as identical.
C) Rotation: Instead of preferring identity during shape
comparison, the model prefers rotation.
D) Alternate: The model looks for an alternate mapping
between the images. In Figure 1F, an alternate A/B
mapping aligns the small triangle in A with the large
triangle in B.
The model only considers an alternate mapping when
SME finds more than one mapping between the images. It
only considers the Reflection/Rotation modes when the
images each contain a single object, allowing the model to
focus on different ways of comparing that one object.
The model independently varies the mapping mode for
A/B and C/i comparisons, beginning with Normal for each.
It terminates when it identifies a sufficient answer. If no
such answer is found, it picks the highest-scoring answer.

Geometric Analogy
Our new geometric analogy model solves problems via two
strategies: visual inference and second-order comparison.
For visual inference, the model compares A and B to get
Δ(A,B), the differences between them. It applies Δ(A,B) to
C to get D’, a representation of the answer image. It
compares D’ to each possible answer. If an answer is
sufficiently similar, it selects that answer.
For second-order comparison, the model compares A and
B to get Δ(A,B). For each answer i, it compares C and i to
get Δ(C,i) and then compares Δ(C,i) to Δ(A,B) (again, using
SME). If an answer’s Δ(C,i) is sufficiently similar to
Δ(A,B), it selects that answer.
In each case, an answer is sufficiently close if either a)
SME detects no differences; or b) the SME similarity score
lies above a similarity threshold. We use a similarity
threshold of 0.8 (where 1.0 is a perfect match). However, a

Experiment
We evaluated our model on 20 geometric analogy problems
from Evans (1968). We recreated each problem in
PowerPoint and then imported the problems into
CogSketch. This required us to manually segment each
problem into images (image A, image B, etc) and segment
each image into objects (each object was drawn as a
separate shape in PowerPoint). Beyond this, the model
automatically segmented each object into edges and
generated representations at the edge and object levels—this
problem set did not contain any groups of objects.

704

Our prior behavioral study (Lovett et al., 2009b) provides
data on human performance which our simulation models,
so we summarize it next.

elements and transformations increased. They suggested this
was because at some point the problem exceeds people’s
working memory capacity, requiring a shift in strategy.
We coded for working memory load by counting the
number of elements in Δ(A,B), the differences between
images A and B. This is a key representation for both visual
inference and second-order comparison. Because
Mulholland et al. found a non-linear effect of working
memory, we discounted the first two elements. Thus, if
Δ(A,B) was one or two, the WM Load was coded as zero.
We ran a linear regression to identify the effect of the
above factors on human reaction times. Table 1 shows the
results. Overall, this model achieves an R2 of .95 (.93
adjusted), meaning it explains almost all the variance in
human reaction times. The grayed cells indicate which
factors made a significant contribution to the model (p <
.01). The intercept of 6.4 indicates that the easiest problems
took around 6.4 s, while the various factors increased the
time to complete a problem.
Note that with correlations, extreme values can result in
an overestimation of the explained variance (the R2 value).
In this case, participants took far longer to solve the two
problems requiring the Alt-Mapping shift (e.g., Figure 1F).
If we remove these data points and rerun the analysis, AltMapping ceases to be a factor, and R2 drops to .80 (.76
adjusted). Thus, even discounting these difficult problems,
the regression explains most of the variance in performance.

Behavioral Study
The Evans problems were shown to 34 adult participants.
They were given a description of the geometric analogy task
followed by two simple example problems (without
feedback) before they saw the 20 problems. Both the
ordering of the problems and the ordering of the five
possible answers were randomized across participants.1
Before each problem, participants clicked on a fixation
point in the screen’s center to indicate readiness. After the
problem was presented, participants clicked on the picture
that best completed the analogy. Participants were instructed
to be as quick as possible without sacrificing accuracy. The
two measures of interest were the answer chosen and the
reaction time, i.e., the time taken to solve the problem.
Results The results show a high degree of consistency
across participants. All participants chose the same answer
for 9 of the 20 problems, while over 90% chose the same
answer for 7 additional problems. The greatest disagreement
was on Figure 1F, where only 56% chose the same answer.
Henceforth, we refer to the answer chosen by the majority
as the preferred answer. In reporting and analyzing reaction
times (including Figure 1), we consider only responses with
the preferred answer, filtering out minority responses.

Table 1. Linear model for human reaction times on
geometric analogy (grayed cells are significant factors).

Simulation & Analysis
The model chose the preferred answer on all 20 problems.
This indicates that our approach—qualitative representation,
comparison via structure mapping, and visual inference—is
sufficient for matching human performance on the task.
We next asked whether people take longer to solve
problems where our model must make a strategy shift. We
coded each problem for three factors: Alt-Strategy, AltMapping, and Alt-Transform. Alt-Strategy indicates that our
model reverts to second-order comparison to solve the
problem. Alt-Mapping indicates that it uses the Alternate
image mapping mode. Alt-Transform indicates that it uses
the Reflection or Rotation mapping modes. We group these
mapping modes together, as our model uses the same
mechanism for computing both transformation types.
We also coded each problem for working memory load.
Previous research has shown that geometric analogy
problems get harder as either the number of elements or the
number of transformations increases (Mulholland,
Pellegrino, & Glaser, 1980; Bethell-Fox, Lohman, & Snow,
1984). Mulholland et al. found that this effect was nonlinear: there was a higher cost when the numbers of both

Intercept
6.4 s

WM
Load
5.7 s

AltStrategy
4.4 s

AltTransform
- 0.7 s

AltMapping
10.5 s

The only factor that did not contribute significantly was
Alt-Transform. Alt-Transform refers to problems like
Figure 1E, where the model must switch to a Reflection
mode to identify a reflection between the identical ‘B’
shapes. The analysis suggests there is no increased cost for
Alt-Transform problems. However, this does not mean such
problems are easy; they are difficult in that the model must
make the Alt-Strategy shift to solve them, changing to
second-order comparison. Once this strategy shift has been
made, there is no additional cost for the Alt-Transform shift.

Related Work
Evans’ ANALOGY (1968) was the first computational
model of analogy. A ground-breaking system, it solved the
same 20 geometric analogy problems as our model using
second-order comparison. However, its brute-force
comparison processes do not align well with human
cognition (see Lovett et al., 2009b for a discussion).
Our own previous model (Lovett et al., 2009b) also
solved problems via second-order comparison. The present
approach builds on that model by implementing visual
inference as a complementary strategy. The previous model
explained .56 of the human variance on the Evans problems,

1

Due to experimenter error, some participants received the same
random orderings. As many as five received one ordering, but on
average only 1.5 received the same ordering. When we randomly
selected one instance of each ordering, the participant number
dropped to 22, and the pattern of results remained the same.

705

whereas the current model explains .95 of the variance.
However, the previous analysis did not consider multiple
factors or filter out reaction times for minority responses.
Several other approaches have utilized visual inference
strategies, but these suffer from important limitations.
Some (Schwering et al., 2009; O’Donoghue, Bohan, &
Keane, 2006) use hand-coded symbolic inputs, rather than
automatically generating representations. This means the
models are unable to reason about quantitative spatial
information, e.g., shape transformations. Others (Ragni,
Schleipen, & Steffenhagen, 2007) are unclear on their
comparison processes. Finally, because these models have
not been systematically evaluated on a pre-existing problem
set, it is unclear how well they match human performance.

Falkenhainer, B., Forbus, K., & Gentner, D. (1989). The
structure
mapping
engine:
Algorithm
and
examples. Artificial Intelligence, 41, 1-63.
Forbus, K., Usher, J., Lovett, A., Lockwood, K., & Wetzel,
J. (2011). CogSketch: Sketch understanding for cognitive
science research and for education. Topics in Cognitive
Science, 3(4). 648-666.
Gentner, D. (1983). Structure-mapping: A theoretical
framework for analogy. Cognitive Science, 7, 155-170.
Huttenlocher, J., Hedges, L. V., & Duncan, S. (1991).
Categories and particulars: Prototype effects in estimating
spatial location. Psychological Review, 98(3), 352-376.
Lovett, A., & Forbus, K. (in preparation). A model of
strategic comparison during visual problem-solving.
Lovett, A., & Forbus, K. (2011a). Cultural commonalities
and differences in spatial problem-solving: A
computational analysis. Cognition, 121(2), 281-287.
Lovett, A., & Forbus, K. (2011b). Organizing and
representing
space
for
visual
problemsolving. Proceedings of Qualitative Reasoning Workshop.
Lovett, A., Forbus, K., & Usher, J. (2010). A structuremapping
model
of
Raven's
Progressive
Matrices. Proceedings of CogSci ’10.
Lovett, A., Gentner, D., Forbus, K., & Sagi, E. (2009a).
Using analogical mapping to simulate time-course
phenomena in perceptual similarity. Cognitive Systems
Research 10(3), 216-228.
Lovett, A., Tomai, E., Forbus, K., & Usher, J. (2009b).
Solving geometric analogy problems through two-stage
analogical mapping. Cognitive Science, 33(7), 1192-1231.
Markman, A. B., & Gentner, D. (1996). Commonalities and
differences in similarity comparisons. Memory &
Cognition, 24(2), 235-249.
Mulholland, T. M., Pellegrino, J. W., & Glaser, R. (1980).
Components of geometric analogy solution. Cognitive
Psychology, 12, 252-284.
O’Donoghue, D. P., Bohan, A., & Keane, M. T. (2006).
Seeing things - Inventive reasoning with geometric
analogies and topographic maps. New Generation
Computing, 24(3), 267-288.
Ragni, M., Schleipen, S., & Steffenhagen, F. (2007).
Solving proportional analogies: A computational model.
Workshop on Analogies: Integrating Multiple Cognitive
Abilities, CogSci ’07.
Raven, J., Raven, J. C., & Court, J. H. (1998). Manual for
Raven’s Progressive Matrices and Vocabulary Scales.
Oxford: Oxford Psychologists Press.
Rosielle, L. J., & Cooper, E. E. (2001). Categorical
perception of relative orientation in visual object
recognition. Memory & Cognition, 29(1), 68-82.
Sagi, E., Gentner, D., & Lovett, A. (in press). What
difference reveals about similarity. Cognitive Science.
Schwering, A., Gust, H., Kühnberger, K., & Krumnack, U.
(2009). Solving geometric proportional analogies with the
analogy model HDTP. Proceedings of CogSci 2009.
Sternberg, R. J. (1977). Intelligence, Information Processing
and Analogical Reasoning. Hillsdale, NJ: Erlbaum.

Conclusions
We believe our model is the first to combine two established
problem-solving strategies: visual inference and secondorder comparison. Beyond utilizing both strategies, the
model makes explicit claims about when people will
abandon visual inference and fall back on second-order
comparison. Our analysis shows that these claims help
explain human reaction times on the 20 Evans problems:
people take longer to solve problems where the model
reverts to second-order comparison, and they take even
longer when the model must find an alternate mapping.
Importantly, our two problem-solving strategies are not
unique to geometric analogy. We recently (Lovett & Forbus,
in prep) integrated these strategies into a new model of
Raven’s Progressive Matrices, a more complex task that is
popularly used to evaluate general intelligence. As that
model and the present model show, successful problemsolving requires flexibly moving between different
comparison strategies. These models, along with our oddity
task model (Lovett & Forbus, 2011a), also demonstrate the
utility of structural alignment across qualitative
representations. In the future, we plan to evaluate the
generality of our approach on new problem-solving tasks.

Acknowledgments
This work was supported by NSF SLC Grant SBE-0541957,
the Spatial Intelligence and Learning Center (SILC).

References
Bethell-Fox, C. E., Lohman, D. F., & Snow, R. E. (1984).
Adaptive reasoning: Componential and eye movement
analysis of geometric analogy performance. Intelligence,
8, 205-238.
Dehaene, S., Izard, V., Pica, P., & Spelke, E. (2006). Core
knowledge of geometry in an Amazonian indigene group.
Science, 311, 381-384.
Evans, T. (1968). A program for the solution of geometricanalogy intelligence test questions. In M. Minsky (Ed.),
Semantic Information Processing. Cambridge: MIT Press.
Falkenhainer, B. (1990). Analogical interpretation in
context. Proceedings of CogSci ‘90.

706

