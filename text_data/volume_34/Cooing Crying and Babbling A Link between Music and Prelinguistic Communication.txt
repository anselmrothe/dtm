UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Cooing, Crying, and Babbling: A Link between Music and Prelinguistic Communication
Permalink
https://escholarship.org/uc/item/510653gs
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Byrd, Michael
Bowman, Casady
Yamauchi, Takashi
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                        Cooing, Crying, and Babbling: A Link between Music and
                                               Prelinguistic Communication
                                    Michael Byrd, Casady Bowman, and Takashi Yamauchi
                   (mybrd@neo.tamu.edu, casadyb@neo.tamu.edu, takashi-yamauchi@tamu.edu)
                                               Department of Psychology, Mail Stop 4235
                                        Texas A&M University, College Station, TX 77843 USA
                              Abstract                                and music followed by an overview of the experiment. After
                                                                      discussing our timbre extraction and sound creation method,
   Like language, the human capacity to create music is one of
   the most salient and unique markers that differentiates            we introduce one experiment that investigates the
   humans from other species (Cross, 2005). In the following          connection between music and prelinguistic communication.
   study, the authors show that people’s ability to perceive
   emotions in infants’ vocalizations (e.g., cooing and babbling)     Related Work
   is linked to the ability to perceive timbres of musical
   instruments. In one experiment, 180 “synthetic baby sounds”
                                                                      Infants begin life with the ability to make different sounds—
   were created by rearranging spectral frequencies of cooing,        first cooing and crying, then babbling. Next they form one
   babbling, crying, and laughing made by 6 to 9-month-old            word, and then two, followed by full sentences and speech.
   infants. Undergraduate participants (N=145) listened to each       In the first ten months, infants progress from simple sounds
   sound one at a time and rated the emotional quality of the         that are not expressed in the phonetic alphabet, to babbling,
   “synthetic baby sounds.” The results of the experiment             which is an important step in infants learning how to speak
   showed that five acoustic components of musical timbre (e.g.,      (Gros-Louis, West, Goldstein, & King, 2006; Oller, 2000).
   roll off, mel-frequency cepstral coefficient, attack time and
   attack slope) could account for nearly 50% of the variation of        Musical instruments and infants’ vocalizations both elicit
   the emotion ratings made by undergraduate students. The            emotional responses, while conveying little information on
   results suggest that the same mental processes are probably        what the sender is trying to express. Music can have a very
   applied for the perception of musical timbres and that of          powerful effect on its listeners, as we all have a piece of
   infants’ prelinguistic vocalization.                               music that will bring back emotions. Music can convey at
   Keywords: Emotion; Language; Music                                 least three universal emotions, happiness, sadness and fear
                                                                      (Fritz et al., 2009). These emotions are similar to the
                           Introduction                               emotions expressed by infants with their limited sounds
                                                                      (Dessureau, Kurowski, & Thompson, 1998; Zeifman, 2001;
Infants use a variety of vocal sounds, such as cooing,                Zeskind & Marshall, 1998). Both infants and music convey
babbling, crying, and laughing, to express their emotions.            meaning without the use of words. Infants rely on their
Infants’ prelinguistic vocal communications are highly                voices and non-verbal/non-word sounds to communicate
affective in the sense that they evoke specific emotions—             and it is these sounds that inform the listener of how
happiness, frustration, anger, hunger, and/or joy—without             important and of what type of danger the infant is facing,
conveying concrete ideas. In this sense, infants’ vocal               such as being too cold, hungry or of being left alone
communication parallels music. Music is highly affective;             (Dessureau et al., 1998; Zeifman 2001; Zeskind & Marshall,
yet it is conceptually limited (Cross, 2005; Ross, 2009).             1998).
   The interaction between music and language has attracted              Across cultures, songs sung while playing with babies are
much attention recently (Chen-Haffteck, 2011; Cross, 2001;            fast, high in pitch, and contain exaggerated rhythmic
Masataka, 2007). However, despite their similarities, little          accents, whereas lullabies are lower, slower and softer.
attention has been paid to the relationship between music             Infants will use cues in both music and language to learn the
and prelinguistic vocalizations (Chen-Haffteck, 2011; Cross,          rules of a culture. Motherese, a form of speech used by
2001; He, Hotson, & Trainor, 2007; Masataka, 2007). If                adults in interacting with infants, often consists of singing to
music and language are highly related, what is the                    infants using a musical, sing-song voice, that mimics
relationship between infants’ vocal communications such as            babies’ cooing by using a higher pitch. An infant’s caregiver
babbling, and music?                                                  will use higher pitch when speaking to an infant, as it helps
   In the study described below, we analyze acoustic cues of          the infant learn and also draws their attention (Fernald
infants’ vocalization and demonstrate that emotions created           1989).
by prelinguistic vocalization can be explained to a large                In summary, research shows that there is a close link
extent by the acoustic cues of sound that differentiate               between infants’ vocal communication and music. This link
timbres of musical instruments, potentially implicating that          is demonstrated through the babbling and cooing sounds
the same mental processes are applied for the perception of           used by infants’ to communicate, and also by mothers’ use
musical timbres and that of infants’ vocalizations.                   of motherese to assist infant’s learning of language in a
   The paper is organized as follows: we review related               sing-song manner. Infants are able to use the same cues
work examining the link between prelinguistic vocalization
                                                                  1392

from both music and language to facilitate learning in both           Below, we briefly describe our timbre extraction method
domains. Given these close connections, it is likely that the      and the method of creating “synthetic baby sounds.”
same mental processes are involved for the perception of
instrumental sounds and the perception of infants’                 Timbre Extraction
vocalizations. The beginning stages of this idea are                  This section describes acoustic cues relating to timbre in
investigated in one experiment by examining the emotion            detail, as well as the computational procedure of extracting
perception of synthetic baby sounds.                               these cues. The purpose of using these acoustic cues is to act
                                                                   as predictors in regression analyses that can explain
Overview of the Study                                              perceived emotions of our “synthetic baby sounds.” The
In the Emotion Rating Experiment described below, we               acoustic cues were chosen based on their use in musical
tested the general hypothesis that the same mental process is      timbre (see Lartillot & Toiviainen, 2007).
involved for the perception of infants’ vocalization and that         Eight acoustic properties of timbre: attack time, attack
of timbres of musical instruments. More specifically, we           slope, zero-cross, roll off, brightness, mel-frequency
hypothesize that the acoustic components of timbre will be         cepstral coefficients, roughness, and irregularity were
significant predictors of emotion. If this is true, then there     extracted from all sound stimuli using MIRToolbox in
should be a plausible link between musical timbre and              Matlab (Lartillot, Toiviainen, & Eerola, 2008). These
prelinguistic vocal timbre, also indicating a link for mental      acoustic properties are known to contribute to the perception
processing in the two domains. We employed an audio                of timbre in music independently of melody and other
synthesizer program and created 180 different “synthetic           musical cues (Hailstone et al., 2009). The acoustic features
baby sounds” by combining spectral frequencies of real             were extracted from synthesized sounds rated in the
baby sounds. In the experiment, our undergraduate                  Emotion Rating Experiment.
participants (N=145) listened to the “synthetic baby sounds”          Attack time is the time in seconds it takes for a sound to
one at a time and rated affective qualities of these sounds.       travel from amplitude of zero, to the maximum amplitude of
Later, we extracted “musical timbres” from the synthetic           a given sound signal, or more simply the temporal duration.
baby sounds, and examined the extent to which the                  Some features of timbre such as attack time contribute to the
emotional ratings made by our undergraduate students were          perception of emotion in music (Gabrielsson & Juslin, 1996;
accounted for by the timbres of the synthetic baby sounds.         Juslin, 2000; Loughran, Walker, O’Neill & O’Farrell,
   Timbre is an important perceptual feature of both music         2001); which suggests that features of timbre can at least in
and speech. Timbre is defined as the “acoustic property that       part determine the emotional content of music (Hailstone et
distinguishes two sounds”—for example, those of the flute          al., 2009).
and the piano—“of identical pitch, duration, and intensity”
(Hailstone et al., 2009; McAdams & Cunible, 1992). The                                                Attack time is computed
classic definition of timbre states that two different timbres                                     using the equation of a line, y
result from the sound of different amplitudes (of harmonic                                         = mx + b, it is part of a
components) of a complex tone in a steady state”                                                   sounds amplitude envelope
(Helmholtz, 1885). Timbre is a sound quality that                                                  where m is the slope of the
encompasses the aspect of a sound that is used to distinguish                                      line and b is the point where
it from other sounds of the same pitch, duration, and                                              the line crosses the vertical
loudness.                                                                                          axis (t=0). Figure 1 gives an
   The timbre properties of attack time, attack slope, zero-                                       illustration of attack time.
cross, roll off, brightness, mel-frequency cepstral                Figure 1. Attack times of       The horizontal segments
coefficients, roughness, and irregularity are well known in        an audio file. A through d      below the x-axis indicate the
music perception research as the main acoustic cues that           are separate attack times;      time it takes in seconds to
correlate with the perception of timbre of musical                 indicateb by the distance       achieve the maximum peak
instruments (Hailstone et al., 2009). Our assumption is that       from the black line, to the     of each frame for which the
if infants’ vocal sounds are perceived in the same manner as       the red line.                   attack time was calculated.
the timbres of musical instruments are perceived, these same          Attack slope is the attack phase of the amplitude envelope
acoustic properties can account for the perception of              of a sound, also interpreted as the average slope leading to
emotions in infants’ vocalization.                                 the attack time. This can also be calculated using the
   Using principal components analysis (PCA), we                   equation of a line y = mx +b, where m is the slope of the
summarized emotional ratings made by our undergraduate             line and b is the point where the line crosses the vertical axis
participants into two principal dimensions, to reduce the          (t=0), see Figure 2. The red line in Figure 2 indicates the
data, and applied a stepwise regression to evaluate the            slope of the attack.
extent to which our predictors—the acoustic timbre
components—accounted for emotion ratings for synthesized
baby sounds.
                                                               1393

                                         Zero-cross is the number         components, and g (fcb) is a ‘standard curve.’ This was first
                                      of times a sound signal             proposed by (Plomp & Levelt, 1965).
                                      crosses the x-axis, this
                                      accounts for noisiness in a                                     n
                                                                                                           a j ak g ( f cb )
                                      signal and is calculated                                        j ,k
                                                                                                                 n
                                      using the following equation
                                                                                                                 j
                                                                                                                   a 2j
                                      where sign is 1 for positive
                                      arguments and 0 for
                                                                             Following extraction of the value for roughness from the
                                      negative arguments. X[n] is
                                                                          sound stimuli, principal components analysis was used to
                                      the time domain signal for
                                                                          reduce the dimensions of the roughness data.
Figure 2. Attack slope of a           frame t.
audio file. The red arrow                                                    Mel-frequency Cepstral Coefficients (mfcc) represent the
indicates     the      duration                                           power spectrum of a sound. This power spectrum is based
(attack time) for which the                                               on a linear transformation from actual frequency to the Mel-
                                                                          scale of frequency. The Mel scale is based on a mapping
attack slope is calculated.
                                                                          between actual frequency and perceived pitch as the human
                    1 N                                                   auditory system does not perceive pitch in a linear manner.
            Zt           | sign ( x[ n ]) sign ( s[ n ])
                    2n1                                                   Mel-frequency cepstral coefficients are the dominant
      Roll off is the amount of high frequencies in a signal,             features used in speech recognition as well as some music
 which is specified by a cut-off point. The roll-off frequency            modeling (Logan, 2001). Frequencies in the Mel scale are
 is defined as the frequency where response is reduced by -3              equally spaced, and approximate the human auditory system
  dB. This is calculated using the following equation where               more closely than a linearly spaced frequency bands used in
 Mt is the magnitude of the Fourier transform at frame t and              a normal cepstrum. Due to large data output, prior to
   frequency bin n. Rt is the cutoff frequency, see Figure 3.             analyses mfcc data were reduced using principal
                      Rt                    Rt                            components analyses to create a workable set of data. A
                         M t [n ]  0.85 *      M t [n ]                   cutoff criterion of 80% was used to represent the variability
                     n 1                   n 1
                                                                          in the original mfcc data. Figure 4 shows the numerical Mel-
Brightness is the amount of energy above a specified
                                                                          frequency cepstral coefficient rank values for the 13 mfcc
frequency, typically set at 1500 Hz – this is related to
                                                                          components returned. Thirteen components are returned due
spectral centroid. The term brightness is also used in
                                                                          to the concentration of the signal information in only a few
discussions of sound timbres, in a rough analogy with visual
                                                                          low-frequency components.
brightness. Timbre researchers consider brightness to be one
of the strongest perceptual distinctions between sounds.
Acoustically it is an indication of the amount of high-
frequency content in a sound, and uses a measure such as
the spectral centroid, see Figure 3.
                                             Roughness is sensory
                                          dissonance, the perceived
                                          harshness of a sound;
                                          this is the opposite of
                                          consonance (harmony)
                                          within music or even a         Figure 4. Mel-frequency cepstral coefficients (mfcc) of an
                                          single tone harmonics.         audio file. This figure shows the acoustic component mfcc.
                                          Both consonance and            Each bar represents the numerical (rank coefficient) value
                                                                         computed for the thirteen components returned.
                                          dissonance are relevant
                                          to emotion perception
                                          (Koelsch,             2005).       Irregularity of a spectrum is the degree of variation
Figure 3. Brightness of an                                                between peaks of a spectrum (Lartillot et al., 2008). This is
audio file. To the right of the           Roughness      is calculated
                                          by computing the peaks          calculated using the following equation where irregularity is
red dashed line is the amount                                             the sum of the square of the difference in amplitude between
of energy above 1500 Hz, or               within        a     sound’s
                                          spectrum and measuring          adjoining partials in a sound.
the brightness of the sound.
                                          the distance between                                      N
peaks, dissonant sounds have irregularly placed spectral                                                (ak     ak 1 ) 2
peaks as compared to consonant sounds with evenly spaced                                           k 1
                                                                                                            N
spectral peaks.                                                                                                ak2
   Formally, roughness is calculated using the following                                                   k 1
equation where aj and ak are the amplitudes of the
                                                                      1394

Creating Synthetic Baby Sounds                                    adjusted and normalized. Participants were instructed to
We created 180 short, 2 second, synthetic baby sounds from        listen to sound stimuli, and rate each sound on five emotion
ten real infant sounds: five males and five females ranging       categories, happy, sad, angry, fearful, and disgusting
from ages 6 to 9 months making screaming, laughing,               (Ekman, 1992; Johnson-Laird & Oatley, 1989). Each scale
crying, cooing and babbling sounds. These sounds were             ranged from 1 to 7—1 being strongly disagree (the degree
chosen to create novel stimuli emulating human                    to which the stimuli, sounded like one of the five emotions),
prelinguistic sounds. Among these sounds, four (one               and 7 being strongly agree. Stimuli were presented in a
screaming boy, one crying boy, one screaming girl and one         random order.
crying girl) were audio-recorded directly from two volunteer
infants in Nacogdoches, Texas using an Olympic Digital            Results
Voice WS-400S recorder. The sounds of babbling and                This section starts with descriptive statistics of emotion
cooing boys and girls were taken from audio-files                 ratings followed by the results from stepwise regression
downloaded       from      a    sound      effects    website     analysis, which examined the extent to which emotion
(http://www.freesounds.org), and the sounds of laughing           ratings given to the synthetic baby sounds were explained
boy and girl were taken from files downloaded from                by their timbre properties. For the regression analysis,
YouTube (http://www.youtube.com).                                 average emotion scores were calculated for individual
   These infant sounds were decomposed by four laboratory         synthetic sounds by collapsing over individual participants,
assistants into amplitude and spectral frequency components       yielding a 179 sounds x 5 emotion dimension matrix. By
by applying fast Fourier transform using a sound editing          applying principal component analysis (PCA), this matrix
software program (SPEAR, Klingbeil, 2005). Arbitrarily            was summarized in a 179 x 2 matrix with the two columns
chosen spectral frequencies of one sound (e.g., a babbling        corresponding to two principal components identified by the
sound of a boy) were mixed with arbitrarily chosen spectral       PCA procedure. The first two orthogonal components
frequencies of another sound (e.g., cooing girl) and then         explained 88.1% and 7.1% of the variance of the emotion
modified by means of amplitude, or shifting frequencies, to       rating data, respectively.
convey one of the basic emotions, happy, sad, anger, or fear         Descriptive Statistics. Behavioral data, Figure 5, shows
(Ekman, 2002).                                                    overall observations for each emotion from the emotion
   For each sound pair, four sounds were created to sound         rating data. From the whiskers of the box plot for the
happy, sad, angry, and fearful. In this manner, each sound        emotion data, it is apparent that there is variation within the
pair (45 pairs in total, all possible pairs of the 10 real        data. The highest rating for the emotion data did not exceed
sounds), was used to create four affective sounds, which          a value of 6, on the scale of 1-7. The median of the ratings
was decided subjectively by the laboratory assistants. The        for emotion varied between approximately 2.5 and 4.75
total 180 sound stimuli were normalized and white noise           within the emotion rating data.
was taken out prior to and after creation of each sound              For all 179 sounds rated, most were rated as angry,
stimulus.                                                         indicated by the median of the data for anger. The sounds
                                                                  were rated least like the emotion happy, as the median for
             Emotion Rating Experiment                            this emotion was the lowest for all sounds rated on the five
The goal of the experiment was to obtain empirical ratings        emotions.
of college students examining the emotional quality of the
synthetic baby sounds that we created. To analyze the link
between emotion ratings and acoustic cues, a stepwise
regression analysis was employed.
Participants. A total of 145 undergraduate students (73
males, 73 females) participated in this experiment for course
credit. Participants were randomly assigned to one of two
groups that listened to 90 or 89 sounds of 179 total sounds.
Stimuli were randomly assigned to one of two groups; no
participants were in both groups.
Materials. Stimuli were taken from the 180 synthetic baby         Figure 5. Box plot of observations for emotion ratings.
sounds that were created from a group of a total of ten           Each box in the figure indicates one emotion rated by
recorded real infants’ sounds (see the “Creating Synthetic        participants. The median is indicated by the red line in the
Baby Sounds” section for the details of the sound creation).      center of each box, and the edges indicate the 25th and 75th
                                                                  percentiles, the whiskers of each plot indicate the extreme
Procedure. Participants were presented with 90/89 sounds          data points, and outliers are plotted outside of the whiskers.
using customized Visual Basic software through JVC Flats
stereo headphones. Each stimulus’s maximum volume was
                                                              1395

   Regression analysis. A step-wise regression analysis was      (a)                                 (b)
used to analyze the collected rating data and timbre
components, to determine which component could best
explain the emotion rating data. Seventeen total predictors
were used in the stepwise regression to analyze the emotion
ratings made by participants. These were attack time, attack
slope, zero-cross, roll off, brightness, mel-frequency
cepstral coefficients 1-6, roughness 1-4, and irregularity.
Due to large data output, mfcc data were reduced using
principal components analyses to create a workable set of
data. There were originally 13 numerical Mel-frequency
cepstral coefficient rank values returned. These 13 rank
values were reduced to 6, accounting for 78% of the total         Figure 6. R-squared. Emotion judgment principal
mfcc data. Roughness was also reduced in the same way             components 1 (PCA 1) and 2 (PCA 2). This figure shows the
using PCA, from 79 components to four components that             proportion of R-squared contributed for each addition of a
described 80% of the original roughness data. These               predictor to the model for principal component I and II from
predictors were used to analyze the emotion ratings made by       the emotion judgments.
participants.
   The results of the regression for the first principal             Figure 6 shows the proportion of R-squared contributed
component (PCA1) indicated four acoustic features                 for each addition of a predictor to the model for PCA 1 – (a)
significantly predicted emotion ratings; roll off (β = -.386,     and PCA 2 – (b). Looking at the values of R-squared, it is
p< .001), mfcc 6 (β =. 218, p< .001), attack time (β =. 248,      apparent that roll-off was best able to describe the emotion
p< .001), and mfcc 3 (β = -.202, p< .002), and attack slope       ratings, accounting for 30% of the emotion ratings for PCA
(β = .034, p< .034), see Table 1 for percent explained by         1. The second principal component does show several
principal component 1.                                            significant acoustic cues that predict emotion; however,
                                                                  none are as strong as in the first principal component.
 Table 1: Significant acoustic components for emotion PCA
                          1and PCA 2                                                 General Discussion
                                                                  Music and language are perhaps two of the most cognitively
   Predictors            PCA 1                PCA 2
                                                                  complex and emotionally expressive sounds invented by
   % explained           88%                  7.1%
                                                                  humans. Recently, the evolutionary origins of music and
   Attack time           .23***               .31***
                                                                  language have attracted much attention in researchers of a
   Attack slope          .12*
                                                                  broad spectrum (Cross, 2001, 2005; Hauser et al., 2002;
   Irregularity                               -.16*
                                                                  Kirby, 2007). The present study, examining the relationship
   Mfcc 1                                     -.24**
                                                                  between infants’ vocalizations—cooing, babbling, crying
   Mfcc 3                -.19**
                                                                  and screaming—and the perception of musical timbres,
   Mfcc 6                .22***
                                                                  suggests that the link between music and language can go
   Roughness                                  .21**
                                                                  even further back to the prelinguistic level of development.
   Zero-cross                                 .25**
                                                                     Our Emotion Rating Experiment indicates that nearly
   Roll off              -.41***
                                                                  50% of emotions created by synthetically produced infant
            * p <.05, ** p < .01, and *** p < .001.               sounds can be explained by a small number of acoustic cues
                                                                  pertaining to musical timbres. Among those, roll off, which
   The second principal component (PCA 2) showed that             quantifies the amount of high frequencies in a signal, turned
five acoustic features significantly predicted emotion            out to be the most important cue. The second most
ratings; mfcc 1 (β = -.244, p< .001), zero cross (β = .250, p<    important property, mfcc (mel-frequency cepstral
.002), attack time (β = .305, p< .000), roughness 2 (β = .208,    coefficients), corresponds to perceived pitch in the human
p< .006), and irregularity (β = -.159, p<. 024), (Table 1).       auditory system, and are the dominant features used in
                                                                  speech recognition and music modeling (Logan, 2001).
                                                                  Given these findings, we conjecture that high-frequency
                                                                  sounds are probably taken as the robust cue of emotion
                                                                  attribution, and more fine-grained distinctions of emotion
                                                                  are made by extracting speech-related cues.
                                                                     The ability to discriminate sounds is said to be present
                                                                  even in primitive animals such as carp (Chase, 2001),
                                                                  implying that this ability evolved early in history. Some
                                                                  animals have sounds and or calls that can convey the
                                                                  emotions of finding something of interest or of fear (Hauser,
                                                              1396

Chomsky, & Fitch, 2002). Such abilities were probably                 Emotion, 3, 81-123.
present even before music was fully developed in the                Juslin, P. N. (2000). Cue utilization in communication of
current form.                                                         emoting in music performance: Relating performance to
                                                                      perception. Journal of Experimental Psychology: Human
                    Acknowledgments                                   Perception and Performance, 26, 1797-1813.
We would like to thank Na Yung Yu and Ricardo Gutierrez-            Kirby, S. (2007). The evolution of language. In R. Dunbar
Osuna for their valuable comments. The first two authors,             & L. Barrett (Eds.), Oxford handbook of evolutionary
MB and CB, contributed to this study an equal amount and              psychology (pp. 669-681). Oxford: Oxford University
the order of their authorship was determined by a coin toss.          Press.
                                                                    Klingbeil, M. (2005). Software for spectral analysis, editing,
                                                                      and synthesis Proceeding of the ICMC (pp. 107-110).
                         References                                   Barcelona Spain.
Chase, A. R. (2001). Music Discriminations by carp                  Koelsch, S. (2005). Neural substrates of processing syntax
  (Cyprinus carpio). Animal Learning and Behaviour, 29,               and semantic in music. Current Opinion in Neurobiology,
  336-353.                                                            15, 207-212.
Cross, I. (2001). Music, mind and evolution. Psychology of          Lartillot, O., Toiviainen, P., Eerola, T. (2008) A Matlab
  Music, 29, 95-102.                                                  Toolbox for Music Information Retrieval. In, C. Preisach,
Cross, I. (2005). Music and meaning, ambiguity and                    H. Burkhardt, L. Schmidt-Thieme, and R. Decker (eds.),
  evolution. In D. Miell, R. MacDonald, D. Hargreaves                 Data Analysis, Machine Learning and Applications,
  (Ed.), Musical Communication (pp. 27-43). New York:                 Studies in Classification, Data Analysis, and Knowledge
  Oxford University Press.                                            Organization, (pp 261-268). New York: Springer.
Dessureau, B. K., Kurowski, C. O., & Thompson, N. S.                Logan, B., & Robinson, T. (2001). Adaptive model-based
  (1998). A reassessment of the role of pitch and duration in         speech enhancement. Speech Communication, 34, 351-
  adults' responses to infant crying. Infant Behavior and             368.
  Development, 21, 367-371.                                         Loughran, R., Walker, J., O’Neill, M., O’Farrell, M.
Ekman, P. (1992). Are there basic emotions? Psychological             (2001). The Use of Mel-frequency Cepstral Coefficients
  Review, 99, 550-553.                                                in Musical Instrument Identification. in Proc. of the 6th
Fernald, A. (1989). Intonation and Communicative Intent in            International Conference on Music Information Retrieval
  Mothers' Speech to Infants: Is the Melody the Message?              (ISMIR), 2005, Finland, (pp. 1825–1828).
  Child Development, 60, 1497-1510.                                 Masataka, N. (2007). Music, evolution and language.
Fritz, T., Jenschke, S., Gosselin, N., Sammler, D., Peretz, I.,       Developmental Science, 10, 35-39. McAdams, S., &
  Turner, R., Koelsch, S. (2009). Universal Recognition of            Cunible, J. C. (1992). Perception of timbral analogies.
  Three Basic Emotions in Music. Current Biology, 19,                 Philosophical Transactions: Biological Sciences, 9, 336-
  573-576.                                                            383.
Gabrielsson, A., & Juslin, P. N. (1996). Emotional                  Olivier Lartillot, Petri Toiviainen, “A Matlab Toolbox for
  expression in music performance between the performer’s             Musical Feature Extraction From Audio”, International
  intention and the listener’s experience. Psychology of              Conference on Digital Audio Effects, Bordeaux, 2007.
  Music, 24, 68-91.                                                 Oller, D. K. (2000). The emergence of the speech capacity.
Gros-Louis, J., West, M. J., Goldstein, M. H., & King, A. P.          Mahwah, NJ: Lawrence Erlbaum
  (2006). Mothers provide differential feedback to infants'         Plomp, R., & Levelt, W. J. M. (1965). Tonal consonance
  prelinguistic sounds. International Journal of Behavioral           and critical bandwidth. Soesterberg: Institute for
  Development, 30, 112-119.                                           Perception RVO-TNO, National Defense Research
Hailstone, J. C., Omar, R., Henley, S., Frost, C., Kenward,           Council T.N.O.
  M., & Warren, J. D. (2009). It's not what you play, it's          Ross, B. (2009). Challenges facing theories of music and
  how you play it: Timbre affects perception of emotion in            language co-evolution. Journal of the Musical Arts in
  music. Quarterly Journal of Experimental Psychology,                Africa, 6, 61-76.
  62, 2141-2155.                                                    Zeskind, P., S, & Marshall, T., R. (1988). The Relation
Hauser, M. D., Chomsky, N., & Fitch, W. T. (2002). The                between Variations in Pitch and Maternal Perceptions of
  Faculty of Language: What Is It, Who Has It, and How                Infant Crying. Child Development, 59, 193-196.
  Did It Evolve? Science, 22, 1569-1580.
He, C., Hotson, L., & Trainor, L. J. (2007). Mismatch
  Responses to Pitch Changes in Early Infancy. Journal of
  Cognitive Neuroscience, 19, 878-892.
Helmholtz, H. v. (2005). On the Sensations of Tone as a
  Physiological Basis for the Theory of Music. London:
  Longmans, Green, and Co.
Johnson-laird, P. N., & Oatley, K. (1989). The language of
  emotions: An analysis of a semantic field. Cognition &
                                                                1397

