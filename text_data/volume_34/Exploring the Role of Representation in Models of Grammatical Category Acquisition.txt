UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Exploring the Role of Representation in Models of Grammatical Category Acquisition
Permalink
https://escholarship.org/uc/item/55z990w6
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Qian, Ting
Reeder, Patricia
Aslin, Richard
et al.
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

        Exploring the Role of Representation in Models of Grammatical Category
                                                              Acquisition
                                               Ting Qian (tqian@bcs.rochester.edu)1
                                        Patricia A. Reeder (preeder@bcs.rochester.edu)1
                                           Richard N. Aslin (aslin@cvs.rochester.edu)1
                                                Josh B. Tenenbaum (jbt@mit.edu)2
                                        Elissa L. Newport (newport@bcs.rochester.edu)1
                                 1 Department   of Brain & Cognitive Sciences, University of Rochester
                                            2 Department   of Brain & Cognitive Sciences, MIT
                             Abstract                                   tween linguistic elements. Although studies have shown that
                                                                        human learners and computational models can successfully
   One major aspect of successful language acquisition is the abil-
   ity to generalize from properties of experienced items to novel      learn grammatical categories when only these cues are avail-
   items. We present a computational study of artificial language       able, the question of representation still remains poorly un-
   learning, where the generalization patterns of three generative      derstood. How do learners represent the knowledge of pre-
   models are compared to those of human learners across 10 ex-
   periments. Results suggest that an explicit representation of        viously encountered linguistic items in order to generalize to
   word categories is the best model for capturing the generaliza-      novel ones?
   tion patterns of human learners across a wide range of learning
   environments. We discuss the representational assumptions               The aim of the present work is to ask what types of repre-
   implied by these models.                                             sentations are used by human learners in an artificial grammar
                                                                        learning (AGL) task that includes many of the distributional
                          Introduction                                  properties of spoken language. We focus on how learners in-
Learning the grammar of a language consists of at least two             duce grammatical categories and assign words to them. Our
important tasks. First, learners must discover the cues in the          approach involves computational modeling, comparing the
linguistic input that are useful for constructing the grammar           simulated learning outcome of three different models, each of
of the language. Second, learners must represent their knowl-           which makes a different assumption about how learners rep-
edge of the grammar in a form that makes it possible to assess          resent the learned grammar. We assess the models by com-
the grammaticality of future input. With an appropriate repre-          paring the generalization patterns of each model and those of
sentation of the grammar, learners can generalize from prop-            human learners. Our experimental data come from our pre-
erties of the small set of experienced items to predicted prop-         vious findings across 10 AGL experiments (Reeder et al., in
erties of novel items. This ability for generalization is crucial       review; Schuler et al., in prep). In the next section, we first
for language acquisition, as the input for learning is naturally        provide a brief summary of these results. Importantly, the
limited. Such generalization should extend to only the novel            goal of our modeling work is not to mirror every detail of hu-
items that are actually licensed by the language, no more               man behavior in AGL experiments: to do so, one must con-
(over-generalization) and no less (under-generalization).               sider psychological variables such as memory and attention,
   Previous research has offered several hypotheses regarding           which are currently not included in our models. Instead, we
the cues that learners use and the representations of gram-             are interested in exploring the representational assumptions
mar they form. In the realm of syntactic category acquisition,          that human learners have adopted in our experiments.
one hypothesis is that the categories (but not their contents)
                                                                        Background on Behavioral Results
are innately specified prior to receiving any linguistic input,
with the assignment of words to categories accomplished with            The behavioral data come from a series of 10 experiments
minimal exposure (e.g. McNeill, 1966). On this view, both               with adult participants in which we created an artificial gram-
the cues and the representations are predefined and indepen-            mar with the structure (Q)AXB(R). Each letter represents a
dent of linguistic input. A contrasting view states that gram-          category of nonsense words. Q and R words served as op-
matical categories are learned, though different hypotheses             tional categories that made sentences of the language vary in
appeal to the importance of different cues or cue combina-              length from 3 to 5 words and made words of the language
tions during the learning process (such as semantic cues, e.g.,         observe patterning in terms of relative order but not fixed po-
Bowerman, 1973). Within this class of non-nativist hypothe-             sition. The sizes of the categories varied across experiments,
ses, several studies have suggested that distributional cues            leading to different numbers of possible sentences in the lan-
may be sufficient for extracting the grammar of the input               guage. For ease of presentation, we will number the experi-
language (e.g., Braine, 1987; Maratsos & Chalkley, 1980;                ments. In Experiments 1-4 (Reeder et al., 2009), there were
Mintz et al., 2002). Distributional cues are defined over pat-          108 possible sentences that could be created from this gram-
terns in the linguistic input, such as token frequencies, co-           mar; in Experiment 5 (Reeder et al., 2009), there were 576
occurrence statistics, and latent structural dependencies be-           possible sentences; in Experiments 6-10 (Reeder et al., 2010;
                                                                    881

Schuler et al., in prep), there were 144 possible sentences.
                                                                      Table 1: Descriptions of the sampling bias in each experiment
   Participants in these experiments were first exposed to a
                                                                        Experiment     Sampling bias
carefully selected subset of the possible sentences of the
grammar. The exposure strings were chosen to test whether               Expt 1         Uniformly Distributed Gaps, Dense Sampling
                                                                                       (1/3 withheld): Every X-word heard with every
specific distributional cues enabled learners to form a cate-                          A- and B-word
gory of lexical items and generalize to novel words, or to al-          Expt 2         Uniformly Distributed Gaps, Sparse Sampling
low exceptions that maintain lexical specificity. In particular,                       (2/3 withheld): Every X-word still heard with
different experiments tested learners sensitivity to the con-                          every A- and B-word
texts of individual words and their individual frequencies, the         Expt 3         Systematic Gaps, Sparse Sampling: Each X-
sparsity of sampling the language, the overlaps among con-                             word heard with a subset of possible A- and B-
                                                                                       words
texts across words, the non-overlap of contexts (or systematic
gaps in information), and the size of the exposure set. In each         Expt 4         Extended Exposure to Systematic Gaps: Same
                                                                                       as Experiment 3, but exposure was tripled
experiment, a portion of the possible strings was withheld in
order to create different kinds of “gaps” in the input to partic-       Expt 5         Subcategorization: Gaps were inserted such
                                                                                       that a clear divide segregated X-words and con-
ipants.                                                                                texts words into two subcategories
   After exposure, subjects completed a grammaticality rating           Expts 6-9      Same as Experiments 1-4, but included a very
task, where they rated strings on a scale from 1-5, with larger                        minimally overlapping X-word (X4); X4 seen
                                                                                       in just one sentence frame in each condition
values indicating higher grammaticality. The test was com-
prised of three types of test strings: familiar AXB sentences           Expt 10        Same as Experiment 3, but bigram statistics are
                                                                                       not balanced because words varied in frequency
(presented during exposure), novel AXB sentences (withheld
from the exposure set), and ungrammatical strings that vio-
lated the AXB word order (i.e., “A1X1A2” or “A1A2B3”).                Learners rated novel grammatical sentences as high as fa-
Importantly, in order to understand how learners generalized          miliar grammatical strings in Experiments 1 and 2, show-
from training sentences and the type of knowledge represen-           ing a strong tendency to generalize across the words within
tations suggested by their generalization behaviors, we var-          a category. In Experiments 3 and 10, where a systematically-
ied the way the presentation set and the gaps occurred in each        gapped training set was presented (balanced or not), learn-
experimental condition, as summarized in Table 1. In Ex-              ers became more conservative and treated novel grammatical
periments 1-2, we varied the sparseness of sampling the lan-          sentences as somewhat less grammatical than familiar ones
guage, but learners heard all AX and XB bigrams. In Ex-               (but still more grammatical than ungrammatical ones). Gen-
periments 3-4, we varied the overlap of contexts across X             eralization was further reduced in Experiment 4, when the
words: each X was heard with only 2 of the 3 As and Bs.               exposure to systematic gaps was increased. In the subcate-
Experiments 6-9 included a new X word (called “X4”) that              gorization experiment (Experiment 5), learners did not fully
appeared in only one sentence frame in the training subset.           generalize across the gaps created by the subcategory struc-
The purpose was to test whether learners would generalize to          ture, indicating that they used the distributional information
X4 as one of the X words (and therefore able to occur in all          to learn that there were two subcategories within the X cate-
X-word contexts), despite its own extremely limited exposure          gory. Lastly, the results of Experiment 6 showed that when
and minimal overlap with the other X-word contexts. Exper-            learners were given a dense sampling of a language with al-
iment 5 created subcategories in the language, with distinct          most complete overlap of contexts for several words in the X
occurrence privileges for X words and contexts words: half            category, learners generalized a novel word (X4) to the full
of the X words only occurred with half of the A words and             range of grammatical contexts of the other X-words, even
half of the B words, while the remaining X words occurred             when they heard X4 in only one of those contexts. When con-
with the remaining As and Bs.                                         texts were more sparse (Experiment 7) and there were signifi-
   In experiments 1-9, the bigram statistics were carefully bal-      cantly more systematic gaps in the input (Experiments 8 & 9),
anced: all grammatical bigrams were presented equally often           learners did not fully transfer their knowledge of X-category
(with the exception of the X4 bigrams in Experiments 6-9).            structure to the minimally overlapping X4 word. In all ex-
Under this balanced design, one possible strategy for judg-           periments, ungrammatical sentences were rated significantly
ing grammaticality could be simply to keep track of bigram            lower than any novel grammatical test string.
statistics. To examine this, we ran Experiment 10, where the
bigram statistics were not balanced.                                                               Models
   By definition, the generative grammar used in all these ex-        We use a generative model-based framework to develop our
periments is the same: (Q)AXB(R). However, our distribu-              three models. The structures of these generative models make
tional manipulations across all of these experiments led hu-          explicit the assumptions about knowledge representations.
man subjects under certain circumstances to restrict gener-           The goal of our modeling effort is to understand what ele-
alization to be maximally compatible with the input, while            ments must be included in the representation of the QAXBR
in other circumstances to generalize to the full grammar.             grammar so that the models’ generalization behavior will be
                                                                  882

most compatible with human behavior across all 10 experi-             Word Bigram Mixture Model
ments. The answer to this question is related to the types of         The word bigram model implies that there is one single rep-
distributional cues that human learners attend to in the exper-       resentation that corresponds to the grammaticality of a sen-
iments. For the models reported in this paper, we make the            tence. Natural languages, however, are usually more flexi-
simplifying assumption that learners only attend to local bi-         ble: a sentence can have many different types of grammati-
gram information in the input, the bare minimum to capture            cality (or ungrammaticality), such as Noun-Verb agreement,
the sequential dependencies within QAXBR sentences (al-               as well as lexical restrictions, such as transitive/intransitive
though our models can easily be extended to use other distri-         verbs. We address this problem by developing a word bi-
butional cues). A successful model should assign high proba-          gram mixture model, where multiple patterns of grammati-
bilities to grammatical sentences and low probabilities to un-        cality can be modeled simutaneously. Each component in the
grammatical ones. Crucially, a successful model should as-            mixture is a word bigram model. A grammatical sentence is
sign probabilities to novel grammatical sentences that match          generated from a component grammar, which is in turn gen-
the ratings of human learners.                                        erated from a stochastic process (the model can be viewed as
                                                                      a Dirichlet process mixture model; Ferguson, 1973). We can
Word Bigram Model                                                     describe the process of generating a sentence s in two steps:
The first model is the word bigram model: the probability                                                                             nk
of a sentence is simply the product of the probabilities of its       • (a) p(s is generated by an existing component k) =           n+α
                                                                                                                                   α
ordered word pairs, where the probability of each word wi is             (b) p(s is generated by a new component) = n+α
conditioned on the preceding word wi−1 :                              • If (a), p(s = w1 , . . . , wm ) = p(w1 , . . . , wm |Bk )
                                                                         If (b), p(s = w1 , . . . , wm ) = p(w1 , . . . , wm |Bnew )
                       p(s) = ∏ p(wi |wi−1 )                  (1)
                              wi ∈s
                                                                      where nk is the number of sentences that have been generated
   Equation (1) can be interpreted to suggest that a word bi-         as instances of component grammar k, n is the number of sen-
gram model represents the grammar with a set of multino-              tences that have already been generated, α is a free parameter
mial distributions. Each distribution specifies the probabili-        of the model (a larger α leading to more new clusters), and B
ties that a word will be followed by any other words in the           refers to the parameters of a component bigram model (as de-
vocabulary. The parameters of these distributions are typi-           scribed in the previous section). Combining these two steps,
cally estimated from training data with maximum likelihood            the probability that s will be generated by a word bigram mix-
estimation (MLE). However, the standard MLE algorithm is              ture model is
insensitive to sample size, which is a crucial variable of in-
terest in several experiments. When comparing Experiments                                                ∑k p(s|Bk )nk + p(s|Bnew )α
                                                                              p(s = w1 , . . . , wn ) =                                  (3)
3 and 4, for example, our subjects exhibited different gener-                                                         n+α
alization patterns as a result of the change in the amount of
exposure to the same set of training data (i.e., a change in          Simulation Procedures Equation (3) describes a genera-
sample size only). Therefore, we adopt a Bayesian approach            tive model, with which we can assess the probability that
that is sensitive to sample size. The fully derived form for          a sentence is generated by an existing representation of the
estimating the probability of a word is:                              grammar. However, the learner faces the opposite problem:
                                                                      they must infer the representation given the observed sen-
                                             nwi−1 ,wi + β            tences. We used the Gibbs sampling method to infer these
    p(wi |wi−1 , all previous bigrams) =                      (2)     parameters (the exact details are not described due to space
                                          ∑k nwi−1 ,wk + vβ
                                                                      limits). We run the model on the training data used in the ex-
                                                                      periments. The first 500 samples of each run are discarded
where v is the vocabulary size, nwi−1 ,wi is the frequency of bi-
                                                                      (which may be biased towards initial values). Due to the
gram (wi−1 , wi ), and β is a free parameter. The β parameter
                                                                      small scale of our artificial language, the sampler converges
determines whether certain parameter settings of the multino-
                                                                      quickly, well within the discarded 500 samples. Each of the
mial distributions are favored. Here, we report results with β
                                                                      remaining posterior samples is considered as a candidate rep-
set to 1, which is a non-biased prior.
                                                                      resentation of the grammar. For experiments with longer ex-
Simulation Procedure In each experiment, the word bi-                 posure, we also run the sampler longer to approximate the
gram model first estimates its model parameters according             effect. The average probability that a sentence is generated
to the training sentences. In experiments where the length            by these posterior representations is taken as a measure of the
of exposure is a predictor of interest (Expts. 3, 4, 8 & 9),          grammaticality of the sentence.
we duplicate the training data to simulate the effect of ex-
tended exposure. Unlike human subjects, however, the model            Category Bigram Mixture Model
is given information regarding the size of the vocabulary, and        A notable feature of the two models presented so far is the
does not have memory limitations.                                     lack of explicit representation for grammatical categories.
                                                                  883

   Model Comparison in R2                                                                       Word Bigram         Word Bigram Mixture          Category Bigram Mixture
   0.8                                                                    0.83
                                                                                                                                                        0.72
   0.6                                                                                           0.67      0.67                          0.66                          0.63
                                                              0.51
   0.4          0.47 0.47 0.47                                      0.46                                                       0.44
                                                                                                      0.42      0.4       0.43                0.41 0.38      0.42
                                                         0.34                    0.34                                0.34           0.35                          0.35
                                                                                      0.31 0.31
   0.2                         0.27 0.26 0.27  0.26
                                                    0.22
   0.0
                    Expt 1         Expt 2          Expt 3          Expt 4            Expt 5          Expt 6         Expt 7         Expt 8         Expt 9        Expt 10
Figure 1: Grammaticality predictions made by the category bigram mixture model best approximate subject ratings in most
experiments. R2 is calculated by regressing subject ratings against model predictions.
In both models, bigram statistics are based on word tokens.                                  parameter (a larger value leading to more category bigrams).
However, a crucial component of language acquisition in-                                     Finally, the probability that each word is generated from its
volves organizing words into grammatical categories and dis-                                 category cI , conditioned on the category of its preceding word
covering relations between them. To investigate whether                                      ci−1 , is modeled as a multinomial distribution.
human learners were in fact organizing the words into cat-
                                                                                             Relation to other models The problem of discovering cat-
egories, we also developed the category bigram mixture
                                                                                             egories for word tokens in a language is analogous to the
model. The category bigram mixture model preserves the
                                                                                             problem of part-of-speech tagging in computational linguis-
notion of multiple component grammars and introduces a
                                                                                             tics, which has been under active research for several decades.
bigram-based word category discovery process nested within
                                                                                             Our category bigram mixture model is most similar to the
each component grammar. In other words, the component
                                                                                             Bayesian unsupervised tagging algorithm developed by Gold-
grammars in the category bigram mixture model are them-
                                                                                             water & Griffiths (2007). While our approach is not fully
selves infinite mixtures of bigram models on categories.
                                                                                             Bayesian (in the sense that hyper-parameters are treated as
Therefore, generating a sentence under the grammar is a two-
                                                                                             free parameters), it has the flexibility of discovering multiple
step process with the second step containing another two-step
                                                                                             part-of-speech sequence patterns (i.e. component grammars)
process:
                                                                                             and creating as many part-of-speech tags as needed (due to
• (a) p(s is generated by an existing component k) =                          nk             the nested Dirichlet Process).
                                                                             n+α
                                                                   α
   (b) p(s is generated by a new component) = n+α                                            Simulation Procedure As in the case of the word bigram
                                                                                             mixture model, Gibbs sampling is applied to the inference
• If (a), for the category of each word, ci :
                                                                                             problem to find samples of the posterior distribution. Each
                                                                        nkl                  of the remaining posterior samples is considered as a candi-
   – (i) p(ci−1 , ci belongs to existing bigram l) =                 nk +α0
                                            α0                                               date representation of the grammar under the category bigram
       (ii) p(ci−1 , ci is novel) =       nk +α0                                             mixture model. The average probability that a test sentence
   – If (i), p(wi ) ∼ MultiNomial(ci |ci−1 )                                                 will be generated by these representations is taken as a mea-
       If (ii), p(wi ) ∼ MultiNomial(cnew |ci−1 )                                            sure of the grammaticality of the sentence.
   If (b), for the category of each word, ci :
                                                                                                                  Results and Discussion
                                                                       nnew
                                                                        l
   – (i) p(ci−1 , ci belongs to existing bigram l) =                 nk +α0                  Model predictions are in the format of probability estimates.
                                            α0
       (ii) p(ci−1 , ci is novel) =       nk +α0                                             A higher probability estimate means that a sentence is more
   – If (i), p(wi ) ∼ MultiNomial(ci |ci−1 )                                                 grammatical. The quality of model predictions is determined
       If (ii), p(wi ) ∼ MultiNomial(cnew |ci−1 )                                            by examining how well they correlate with subject ratings. To
                                                                                             ensure that subject ratings are maximally comparable with
where in the top-level process, nk is the number of sentences                                model predictions, we transformed discrete ratings into z-
that have been generated by component grammar k, n is the                                    scores within each subject and experiment, so that the ratings
total number of generated sentences, α is the free parameter                                 of subjects with consistent biases (consistently high or con-
(as in the word mixture model) influencing the tendency of                                   sistently low ratings) were normalized. We computed the R2
creating more component grammars. For clarity, we write                                      metric for each group using a linear regression where model
ci−1 , ci as the bigram label that each bigram l is associated                               predictions were used to predict subject ratings. A model
with in the nested process: nlk is the frequency of category                                 with a high R2 indicates that the particular model explains
bigram l in component grammar k; nk is the total number of                                   a significant amount of variance in subject ratings (see Fig 1).
category bigrams in component grammar k, and α0 is a free                                    Overall, the category bigram mixture model best captures hu-
                                                                                       884

                                                                         Normalized Grammaticality Rating (y-axis)
man behavior across all 10 experiments combined (R2 Word                 Expt 4                                   Grammatical Familiar
Bigram = 0.4, R2 Word Bigram Mixture = 0.35, R2 Category                                                          Grammatical Novel
                                                                                                                  Ungrammatical
Bigram Mixture = 0.47).                                                  1 .5
   The general advantage of the category bigram mixture                  1 .0
                                                                         0.5
model suggests that our human learners may have acquired
                                                                         0.0
a representation of an X category, and not just a set of sim-
                                                                        −0.5
ple word co-occurrences. In X4-related experiments (Expts.
6-9), we asked whether learners could extend their knowl-                Expt 9                                   Grammatical Familiar X4
edge of a target category to a very infrequently presented                                                        Grammatical Novel X4
                                                                                                                  Ungrammatical X4
word for which they only had minimal context information.                1 .5
We found that there was a point in learning where hearing                1 .0
just one context for the minimally overlapping X4 word was               0.5
                                                                         0.0
enough to generalize full category privileges for that word
                                                                        −0.5
(Expt 6). Simple word co-occurrence and bigram counts
will not achieve this outcome. The category bigram mixture
                                                                         Expt 10                                  Grammatical Familiar
model, however, has the appropriate representation for sup-                                                       Grammatical Novel
                                                                                                                  Ungrammatical
porting such a learning outcome. Indeed, in Experiment 6,                1 .5
X4 gets assigned to the same category as all other X-words,              1 .0
thus enabling the generalization to novel X4 sentences (the              0.5
effect of X4 sentences on overall R2 is reduced by the ex-               0.0
tremely small number of X4 sentences in the testing phase).             −0.5
                                                                                Word Bigram Word Bigram Category Bigram Subjects
Limitations of the category bigram mixture model                                              Mixture       Mixture
While the category bigram mixture model best approximates
                                                                     Figure 2: Z-score normalized grammaticality ratings by mod-
human generalization patterns across the 10 experiments, it
                                                                     els and human subjects in 3 experiments where the Category
does no better than the other two models in capturing hu-
                                                                     Bigram Mixture model had highest R2 . Error bars = SE. Re-
man performance in the subcategorization experiment. In-
                                                                     garding the difference between familiar and novel ratings,
deed, the two mixture models acquire the subcategory struc-
                                                                     Expt 4 shows overgeneralization by the category bigram mix-
ture, but fit human performance no better than the simplest
                                                                     ture model, Expt 9 shows overly conservative behavior of the
word bigram model. This paradoxical result is due to the ex-
                                                                     category bigram mixture model, and Expt 10 shows the cat-
perimental design: all bigrams in the training subset conform
                                                                     egory bigram mixture model best capturing human behavior,
to the subcategory boundaries and are presented equally of-
                                                                     despite systematic gaps and variable bigram frequencies.
ten. At test, novel subcategory-conforming items are rated
as high as familiar ones because they contain only bigrams
that have been presented (thus indistinguishable from famil-
iar ones). Test strings violating the subcategory structure are          In the word-based models, the reduced generalization is
rated low by the word bigram model simply because they con-          captured because an increase in the probabilities of observed
tain one or two bigrams which are never seen in the training         word bigrams necessarily leads to a decrease in the probabil-
data. The balanced presentation of all within-subcategory bi-        ities of unobserved ones, thus producing restricted general-
grams enables the word bigram model to distinguish between           ization. However, this effect is much weaker in the category
subcategory conforming and violation items without learning          bigram mixture model, where repeated exposure to training
the existence of two subcategories. As a result, even though         sentences only strengthens the category bigram dependen-
the two mixture models successfully discover the existence of        cies. When a novel grammatical sentence is presented to this
two subcategories, the additional advantage of such discover-        model, its category bigrams have been observed many times
ies is minimal.                                                      during training and the sentence will receive a relatively high
   The category bigram mixture model also tends to over-             rating as a result (despite this, R2 is relatively higher in the
generalize in experiments with systematic gaps. This is most         category bigram mixture model because its predictions are
clearly demonstrated in Experiments 4 and 9 (see Fig 2).             qualitatively closer to subject ratings; see Fig 2). This is an
In those experiments, subjects were exposed to a language            indication that learners have slightly different constraints on
with frequent systematic gaps in the input. Human learners           learning and/or a slightly different strategy from the category
gave novel grammatical sentences a significantly lower rating        bigram mixture model. We are currently exploring other pos-
than familiar grammatical strings, especially when the train-        sible models that build on the idea of an underlying category
ing materials were presented multiple times. We view this re-        representation, but incorporate learning constraints that more
striction of generalization as a rational behavior that prevents     closely mimic human learning (e.g. incremental models of
human learners from over-generalizing when systematic and            learning) and lead to the construction of a more restrictive
persistent gaps occur in the input.                                  grammar that is still compatible with the input.
                                                                 885

   This pattern can be contrasted with model performance on           generative component that asks how likely it is that a string
Expt 10, where not all grammatical bigrams are seen equally           is absent given a random sampling process. If that probabil-
often during exposure. Results from this experiment make              ity is low, then it would penalizes the probability even fur-
clear that the category bigram mixture model is the most ro-          ther by downweighting it in the grammar. We are explor-
bust to manipulations of the bigram distribution (see Fig 2).         ing this direction, in conjunction with other mixture models
The other two models rate grammatical novel sentences al-             that may more closely mirror the constrained learning envi-
most as low as ungrammatical sentences, since novel gram-             ronment that human learners face during natural grammatical
matical sentences contain novel and low-frequency bigrams.            category acquisition.
By definition, these are less grammatical to the word-based
models due to having a lower probability. The category mix-                                Acknowledgments
ture model, on the other hand, is not negatively influenced           This research was supported by NIH Grants HD037082 to
by the unbalanced design due to the abstraction of word cate-         RNA and DC00167 to ELN, and by an ONR Grant to D.
gories.                                                               Bavelier at the University of Rochester.
                     General Discussion                                                         References
                                                                      Bowerman, M. (1973). Structural relationships in childrens
Across 10 experiments, we compared the grammaticality pre-               utterances: Syntactic or semantic? In T. Moore (Ed.), Cog-
dictions of three different models to human subject ratings.             nitive development and the acquisition of language. Har-
Our primary interest was to find the representational elements           vard University Press.
of the grammar that are most compatible with the generaliza-
                                                                      Braine, M. (1987). What is learned in acquiring word classes
tion behaviors displayed by the learners. Generalization de-
                                                                         a step toward an acquisition theory. In B. MacWhin-
pends on the ability to abstract over categories, which is fun-
                                                                         ney (Ed.), Mechanisms of language acquisition (p. 65-87).
damental to linguistic productivity. A number of researchers
                                                                         Lawrence Erlbaum Associates.
have asked whether there is adequate distributional infor-
mation in the input to form linguistic categories. Previous           Ferguson, S. (1973). A bayesian analysis of some nonpara-
work uses hierarchical clustering and a computational learn-             metric problems. Annals of Statistics, 1, 209-230.
ing mechanism to attempt to deduce grammatical categories             Goldwater, S., & Griffiths, T. (2007). A fully bayesian ap-
from corpora of child directed speech based solely on distri-            proach to unsupervised part-of-speech tagging. In ACL.
butional analyses of the input (e.g. Mintz et al., 2002; Red-         Maratsos, M., & Chalkley, M. A. (1980). The internal lan-
ington et al., 1998). These models have been able to use co-             guage of childrens syntax: The ontogenesis and representa-
occurrence statistics among words to achieve relatively good             tion of syntactic categories. In K. Nelson (Ed.), Childrens
categorization performance for frequent target words, indicat-           language (Vol. 2). Gardner Press.
ing the utility of these types of distributional cues for catego-     McNeill, D. (1966). Developmental psycholinguistics. In The
rization.                                                                genesis of language: A psycholinguistics approach (p. 69-
   The behavioral experiments that this work is built upon               73). MIT Press.
suggest that the patterning of word tokens in a substantial cor-      Mintz, T. H., Newport, E. L., & Bever, T. G. (2002). The
pus of linguistic input appears to be sufficient to extract the          distributional structure of grammatical categories in speech
underlying structural categories in a natural language, given            to young children. Cognitive Science, 26, 393-425.
an appropriately capable learner. Our modeling results have           Redington, M., Chater, N., & Finch, S. (1998). Distribu-
further explicated the representational assumptions for ex-              tional information: A powerful cue for acquiring syntactic
tracting the knowledge of a grammar. Of the three models,                categories. Cognitive Science, 22, 435-469.
two models are based on simple word bigrams collected from            Reeder, P. A., Newport, E. L., & Aslin, R. N. (2009). The
training data. While word bigrams are useful for capturing               role of distributional information in linguistic category for-
the lexical dependencies of the grammar, they cannot explain             mation. In CogSci 2009 (p. 2564-2569).
how human learners could generalize from experienced ex-
                                                                      Reeder, P. A., Newport, E. L., & Aslin, R. N. (2010). Novel
amples to novel items, especially when the prior experience
                                                                         words in novel contexts: The role of distributional infor-
is minimal (i.e., Expts 6-9). Such rapid and automatic gener-
                                                                         mation in form-class category learning. In CogSci 2010
alization behavior calls for a richer representation, in which
                                                                         (p. 2063-2068).
the grammar of the artificial language is organized around
potential categories of vocabulary words. The category bi-            Reeder, P. A., Newport, E. L., & Aslin, R. N. (in review).
gram mixture model introduces the notion of categories as a              From shared contexts to syntactic categories: The role
representational assumption, which led to model predictions              of distributional information in learning linguistic form-
that better approximated the behavior of human learners in               classes.
almost all experiments. A limitation of the category bigram           Schuler, K., Reeder, P. A., Newport, E. L., & Aslin, R. N.
mixture model, however, is that it overgeneralizes compared              (in prep). The effects of uneven frequency information in
to human performance. A fourth type of model could add a                 linguistic category formation.
                                                                  886

