UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Superspace extrapolation reveals inductive biases in function learning
Permalink
https://escholarship.org/uc/item/4k527342
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Lucas, Christopher
Sterling, Douglas
Kemp, Charels
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                     University of California

           Superspace extrapolation reveals inductive biases in function learning
           Christopher G. Lucas                              Douglas Sterling                           Charles Kemp
             cglucas@cmu.edu                  douglas.sterling@bccn-berlin.de                         ckemp@cmu.edu
           Department of Psychology                Bernstein Center for Computational             Department of Psychology
          Carnegie Mellon University                       Neuroscience, Berlin                  Carnegie Mellon University
                             Abstract                                  similarity-based approach. The rule-based approach proposes
   We introduce a new approach for exploring how humans learn          that humans rely on a set of parametric functions that have
   and represent functional relationships based on limited obser-      explicit mental representations, including linear functions,
   vations. We focus on a problem called superspace extrapo-           polynomial functions, and others (Carroll, 1963; Brehmer,
   lation, where learners observe training examples drawn from
   an n-dimensional space and must extrapolate to an n + 1-            1974; Koh & Meyer, 1991; Koh, 1993; Bott & Heit, 2004).
   dimensional superspace of the training examples. Many exist-        The similarity-based approach proposes that humans remem-
   ing psychological models predict that superspace extrapolation      ber specific examples encountered during training, and make
   should be fundamentally underdetermined, but we show that
   humans are able to extrapolate both linear and non-linear func-     predictions about test points based on similarity to the train-
   tions under these conditions. We also show that a Bayesian          ing points (Busemeyer, Myung, & McDaniel, 1993; Kelley
   model can account for our results given a hypothesis space that     & Busemeyer, 2008). Similarity-based approaches have tra-
   includes families of simple functional relationships.
                                                                       ditionally struggled to account for extrapolation, and super-
                          Introduction                                 space extrapolation is especially challenging for these ap-
People regularly face situations where they must reason about          proaches. We show that humans are able to learn several
functions defined over continuous variables. For example,              different functions in a superspace extrapolation paradigm,
consider a truck driver who wants to predict how quickly his           which supports the idea that people can formulate and use ex-
truck can accelerate based on the mass of his cargo. If the            plicit representations of both linear and nonlinear functions.
driver has transported similar masses in the past, he can gen-            The hybrid approach to function learning proposes that
erate an accurate prediction by recalling the accelerations ob-        humans can make both rule-based and similarity-based in-
served on these previous occasions. The real test of whether           ferences. We show that this approach can account for our
and how he has learned the function is how he extrapolates             data by evaluating a hybrid model that builds on the Gaus-
from past examples and makes predictions about loads that              sian process account of Griffiths, Lucas, Williams, and Kalish
are much lighter or heavier than those he has seen in the past.        (2009). Other models of function learning are prominent in
Figure 1a, for example, shows how a learner might use linear           the literature, and here we mention two representative ex-
extrapolation to generalize on the basis of two examples.              amples. The Population of Linear Experts (POLE) model
   In any function learning setting, extrapolation judgments           (Kalish, Lewandowsky, & Kruschke, 2004) proposes that hu-
are shaped by the examples observed and the assumptions or             mans learn functions that are piecewise linear (in the 1D case)
inductive bias that the human brings to the problem. Min-              or piecewise planar (in the 2D case). Since the training ex-
imizing the information carried by the training examples               amples in a superspace extrapolation task are collinear, any
makes the role of the inductive bias especially apparent. Here         given piecewise planar function belongs to an infinite family
we explore how humans learn functions from impoverished                of piecewise planar functions that make very different extrap-
training data, and focus in particular on the problem of su-           olation predictions but fit the training examples equally well.
perspace extrapolation. Given training examples that fall              For example, Figures 1c and 1d show two different extrapola-
within an n-dimensional space, we study how learners ex-               tion functions that account perfectly for the same set of train-
trapolate to an n + 1-dimensional superspace that encloses             ing examples. As a result, models that rely exclusively on lin-
the training examples. If the underlying function is one-              ear extrapolation suggest that superspace extrapolation prob-
dimensional, superspace extrapolation requires the learner to          lems are fundamentally underdetermined and are unlikely to
generalize on the basis of a single training example (Fig-             lead to consistent patterns of human responses. The Sigma
ure 1b). We focus on the corresponding problem in two                  model (Juslin, Karlsson, & Olsson, 2008) is an alternative
dimensions, where the learner observes training examples               approach which proposes that humans can acquire explicit
drawn from a one-dimensional space and must generalize to              representations of linear functions, but that knowledge about
the full two-dimensional space (Figures 1c-f).                         non-linear functions is “carried implicitly by memory for ex-
   Superspace extrapolation is an interesting problem in its           emplars.” We show that people are successfully able to ex-
own right, but also provides a way to distinguish between              trapolate non-linear functions in a superspace extrapolation
competing accounts of function learning. The psychologi-               paradigm, which suggests that the rule-based component of
cal literature on function learning includes two prominent ap-         a hybrid approach should include room for non-linear func-
proaches that we will call the rule-based approach and the             tions.
                                                                   713

                                                                      learners’ representational toolkit, we might predict that their
                                                                      extrapolations would resemble Figure 1c.
                                                                         If extrapolation in cases like Figure 1c does depend on ex-
                                                                      plicit rules, then extrapolations might vary dramatically if the
                                                                      positions of the training points are rotated. For example, the
                                                                      function f (x, y) = |x − y| turns into the function |x − 1 + y|
                                                                      when rotated by π/2 around the line (0.5, 0.5, t) which passes
                                                                      through (0.5, 0.5) in the xy-plane and is perpendicular to the
                                                                      z-axis. It seems plausible that participants rely on a hypoth-
                                                                      esis space of rules that can accommodate the original but not
                                                                      the rotated function. We therefore compare each extrapola-
                                                                      tion problem to a rotated variant where the training points are
                                                                      rotated around the line (0.5, 0.5, t), and predict that partici-
                                                                      pants will be able to learn the unrotated but not the rotated
Figure 1: Examples of superspace extrapolation in one and             version of each function. Linear extrapolation is equally pos-
two dimensions. Black dots are training points. (a) Standard          sible in the rotated and unrotated cases, and a linear extrapo-
function learning with one cue dimension; (b) Extrapolating           lation account therefore predicts no qualitative difference be-
from cues in a zero-dimensional subspace; (c) Superspace              tween these two versions of the problem. Many similarity-
extrapolation in two dimensions, where f (x, y) = |x − y|;            based approaches also predict that the rotated and unrotated
(d) A second example of superspace extrapolation applied to           versions should lead to similar results, since similarity met-
|x − y|, assuming a difference piecewise linear function; (e)         rics (e.g. Euclidean distance) are often rotation-invariant.
Superspace extrapolation where f (x, y) = xy; (f) A second
example using xy, with similarity-based extrapolation.                Methods
                                                                      Participants. 33 participants were recruited from Carnegie
                          Experiment                                  Mellon’s participant pool and the local community and re-
                                                                      ceived course credit or ten dollars for participating.
We developed a behavioral experiment with two goals in                Materials. Cues were presented using adjacent horizontal
mind. The first and most basic goal is to find out whether            bars and participants made predictions by adjusting a third
superspace extrapolation is possible at all. Expecting partici-       horizontal bar centered under the midpoint between the cue
pants to make generalizations about a function given a single         bars. Each bar had a bounding box, so the range of valid
training point seems unreasonable (Figure 1b), and it is possi-       values—which we denote with [0, 1] for simplicity—was ev-
ble that participants will find the two dimensional version of        ident to participants. No numerical information was provided
superspace extrapolation equally underdetermined. If super-           about any of the variables. Feedback presentations took the
space extrapolation turns out to be possible, our second goal         form of a green bar overlaid on the prediction bar.
is to understand how this kind of extrapolation is achieved. In       Procedure. Participants were told that they would have
particular, we aimed for a task that could address whether par-       to learn several cause-effect relationships through trial-and-
ticipants use explicit rules to make inferences that go beyond        error. Each participant was presented with the five distinct
linear and similarity-based extrapolation.                            functions listed in Table 1 in random order, in either ro-
   We hypothesized that participants could learn a range of           tated or unrotated form. For a given unrotated function
two-dimensional functions and chose to focus on five specific         f (x, y) and a rotation angle θr , we define a rotated func-
functions that are relatively simple and qualitatively differ-        tion g(x, y) = f (x0 , y 0 ) where (x0 , y 0 ) is the result of rotat-
ent from one another. These functions are shown in Table 1            ing (x, y) around the point (0.5, 0.5) by θr . Table 1 contains
and plotted in Figure 2. Note that the family of functions            explicit definitions of the unrotated and rotated versions of
includes both linear and non-linear functions. Consider one           all functions. For each function, participants saw a training
such function, the absolute difference function plotted in Fig-       phase followed by a test phase. Both phases consisted of a
ure 1c. Suppose that a learner observes the training points           series of trials in which participants were presented with cues
shown in black, which happen to fall along a line. There are          (x, y) and asked to predict f (x, y).
many possible ways to extrapolate from the training points to            The training phase included 40 randomly-ordered exam-
the entire space—for example, Figure 1d shows an extrapola-           ples that fell along a single line. Specifically, training exam-
tion to an axis-aligned function that is especially simple in the     ples fell at equal intervals along a line segment with length
sense that it is invariant with respect to one of the dimensions.     0.9 centered at (0.5, 0.5), making an angle of θl (see Table 1)
If people extrapolate by fitting a piecewise linear (i.e. planar)     with the x-axis. After each training prediction, participants
function to the training points, then there seems to be no rea-       who gave guesses within 0.04 of the true value moved to the
son to prefer the extrapolation in Figure 1c to Figure 1d or the      next example point, while inaccurate guesses were followed
infinitely many alternative extrapolations that fit two planes to     by feedback in which the correct value of f (x, y) was pre-
the training points. On the other hand, if |x − y| is in human        sented and participants had to adjust their prediction to match
                                                                  714

                                                                                   Function    MAEu      MAEr           p
   Name          Unrot. f (.)    θr       Rot. f (.)        θl
                                                                                       x        0.039     0.055       0.51
                                 1                          1
   Projection    x               2π       1−y               8π
                                                                                  (x + y)/2     0.071     0.151     0.00088
                  1
                                                                                      xy        0.092     0.140       0.042
   Average        2 (x + y)      − 12 π   1
                                          2 (x + 1 − y)     3
                                                            8π                      |x − y|     0.123     0.247      0.0015
                                 1
                                          x(1 − y)          5                     max(x, y)     0.046     0.130     0.0077
   Product       xy              2π                         8π
                                 1                          5
   Difference    |x − y|         2π       |x + y − 1|       8π        Table 2: Mean absolute error for test points in learning ro-
                                                                      tated MAEr and unrotated functions (MAEu ). p-values were
   Max           max(x, y)       − 12 π   max(x, 1 − y)     7
                                                            8π        obtained using a two-tailed permutation test, using 200,000
                                                                      samples per test.
Table 1: List of functions that participants learned. θr refers
to the relative angles of the original and rotated functions, and
                                                                      are sensitive to complements in some cases, responses for the
θl denotes the angle of the original line.
                                                                      remaining rotated functions suggest that participants find it
                                                                      difficult to learn simple functions defined in terms of comple-
that value in order to continue.                                      ments.
   In the subsequent test phase, participants received no feed-          The descriptions provided by individual participants indi-
back and were presented with 10 equidistant points along the          cated that many had acquired explicit representations of the
original training line, 10 within-space points that fell beyond       unrotated functions. Five examples of these descriptions are:
the extrema of the original line, and 36 superspace extrapo-          “effect was identical to cause B” (projection); “roughly the
lation points in a uniform 6-by-6 grid over the [0, 1] × [0, 1]       average of the two causes” (average); “fraction multiplica-
range. After each test phase, participants were prompted to           tion” (product); “difference of the causes” (difference); “the
describe what they thought the function was before moving             larger of the two values” (max). Responses for the rotated
on to the next function. The bars corresponding to variables          functions sometimes indicated complex hypotheses, but more
x and y were selected randomly.                                       often indicated confusion or uncertainty about the nature of
                                                                      the function. These descriptions indicate that some individ-
Experimental Results
                                                                      uals had clearly learned the functions. To further explore
We excluded one participant who did not attempt to learn the          responses at the individual level, we looked at the extent to
functions, indicated by a mean absolute error exceeding 0.25.         which individual participants’ judgments fit the true functions
Ten of the 32 participants who remained did not complete all          versus several alternatives, for both unrotated and rotated
of the functions in the allotted hour, but each version (rotated      functions, shown in Figure 3. The space of candidate func-
or unrotated) of each function was completed by at least 11           tions included all true unrotated and rotated functions, along
participants. The side on which x and y were presented had            with a set of simple alternatives shown in the caption to Fig-
no significant influence on performance, so the two orienta-          ure 3. The alternatives include a function that captures com-
tions were grouped together.                                          plete uncertainty (f (x, y) = 0.5), floor and ceiling responses
   The five panels labeled (iii) in Figure 2 show average hu-         (f (x, y) = 1 and f (x, y) = 0), and some simple linear com-
man responses for the five unrotated functions. The black             binations of x and y. For all of the unrotated functions, most
dots show responses for the training points, and the surfaces         participants’ extrapolation judgments were best fit by the true
show responses for the extrapolation points. In all cases, par-       function. For the rotated versions, the modal judgment only
ticipants were able to learn the function values for the train-       matched the true function for rotations of f (x, y) = x and
ing points, and their extrapolation judgments were qualita-           f (x, y) = max(x, y).
tively similar in all cases to the true functions. Note that su-         Additional evidence that individual participants often
perspace extrapolation was possible even for the three non-           learned the true functions relatively well is provided by ex-
linear functions in the set. The panels labeled (iv) in Figure 2      amining the mean absolute error with respect to the true
show average responses for the rotated functions. Partici-            function. Performance for the projection function was near-
pants appeared to learn the rotated projection function, but          ceiling for both unrotated and rotated versions, but in all
extrapolation judgments for the four other rotated functions          other cases participants had lower mean absolute error for
appear qualitatively different from the true functions. Table 1       the unrotated functions than the rotated functions. Table 2
shows that the rotated version of the projection function is          shows mean absolute error for the extrapolation points, and
f (x, y) = 1 − y. Recall that the cues were presented using           a similar pattern held for the training points, with signifi-
sliders on horizontal bars, and that the value of each cue cor-       cantly better performance in the unrotated cases for f (x, y) ∈
responds visually to the proportion of the bar to the left of the     {x, |x − y|, xy, max(x, y)} at α = 0.05.
slider. The rotated projection function can be learned by pay-           Previous experiments have explored the relative learnabil-
ing attention to the complement of the y cue, or the propor-          ity of one-dimensional functions, and our results provide
tion of the y-bar to the right of the slider. Although responses      some initial evidence about a learnability ordering for two
for the rotated projection function suggested that participants       dimensional functions. The results in Table 2 suggest that the
                                                                  715

  Figure 2: True functions, average judgments, and model
  predictions for five different functions. Subfigures (a-e) re-
  fer to the five distinct functions we selected, each labeled
  accordingly. The black dots represent the training examples
  and the surfaces represent extrapolation points. Each sub-
  figure is divided as follows: (i) Training and ground truth
  for the unrotated functions; (ii) Training and extrapolation
  for the rotated functions; (iii) Average judgments for the
  unrotated functions; (iv) Average judgments for the rotated
  functions; (v-vi) Model predictions and correlations with
  human extrapolation judgments, for unrotated and rotated
  functions, respectively. Note that axes differ in the rotated
  cases, allowing straightforward comparison with the unro-
  tated cases.
716

                                                                                                 True function (un−rotated)                                                                         True function (after rotation)
                                                                          x            (x+y)/2             |x−y|                  xy            max(x,y)                        1-y        (x+1-y)/2          |x+y-1|            x(1-y)       max(x,1-y)
                                                                 x                                                                                             x                                                                                           x
                                  Un-rotated functions
                                                            (x+y)/2                                                                                         (x+y)/2                                                                                        (x+y)/2
                                                              |x−y|                                                                                          |x−y|                                                                                         |x−y|
                                                                xy                                                                                            xy                                                                                           xy
              Best−fit function
                                                           max(x,y)                                                                                         max(x,y)                                                                                       max(x,y)
                                                               1−y                                                                                            1−y                                                                                          1−y
                                  Rotated functions
                                                          (x+1−y)/2                                                                                        (x+1−y)/2                                                                                       (x+1−y)/2
                                                            |x+y−1|                                                                                         |x+y−1|                                                                                        |x+y−1|
                                                             x(1−y)                                                                                          x(1−y)                                                                                        x(1−y)
                                                         max(x,1−y)                                                                                        max(x,1−y)                                                                                      max(x,1−y)
                                                              other                                                                                          other                                                                                         other
                                                                  0   5       10   0    5   10       0 5 10            0      5        10   0    5   10                 0   5     10   0    5   10      0 5 10            0     5    10   0     5   10
                                                                                                  Number of participants                                                                             Number of participants
Figure 3: Trained functions versus functions that best fit participants’ judgments. The other group includes f (x, y) ∈
{1, 0.5, 0, 0.5x, 0.5y, 0.75x + 0.25y, 0.25x + 0.75y}. Best-fitting functions were those that minimized mean squared error.
projection and maximum functions are easiest to learn, and                                                                                                              Function family                         Prior                    Mean function
that the product and difference functions are most difficult to                                                                                                            β[1 x y]T                             0.5                  µ0 = 0, µ1 = µ2 = 1
learn.                                                                                                                                                                     β[1 x y]T                              0.4                µ0 = 1, µ1 = µ2 = −1
   Taken together, our data provide strong evidence that hu-                                                                                                            β[1 x y x2 y 2 ]T                        0.09               µ0 = µ1 = µ2 = µ3 = 0
mans are capable of superspace extrapolation, and can learn                                                                                                                β1 |x − y|                           0.001                        µ1 = 1
both linear and non-linear functions under this paradigm. The                                                                                                            β1 max(x, y)                           0.001                        µ1 = 1
lower performance for the rotated functions suggests that the                                                                                                                β1 xy                              0.001                        µ1 = 1
space of learnable functions is restricted. Both results are                                                                                                           Smooth functions                          0.01                     f (x, y) = 0
compatible with the idea that people can acquire explicit rep-
resentations of rules, but raise challenges for approaches that                                                                                               Table 3: Hypothesis space captured by the Gaussian pro-
focus on similarity-based computations alone. Our data, how-                                                                                                  cess model. Un-normalized prior probabilities are given for
ever, are compatible with a hybrid approach, and the next sec-                                                                                                each function family for readability. For the first five fami-
tion describes one such approach that accounts for our data                                                                                                   lies, coefficients βi are distributed normally around µi with a
relatively well.                                                                                                                                              common variance for each coefficient. The difference, max-
                                                                                                                                                              imum, and product families are not described by Griffiths et
        Modeling superspace extrapolation                                                                                                                     al. (2009), but the prior probabilities on all remaining families
The hybrid approach to function learning is motivated by the                                                                                                  and the µi values for these families are drawn from Griffiths
idea that humans readily learn certain rules but fall back on                                                                                                 et al. (2009).
similarity-based computations when no simple rule is consis-
tent with the observed examples. The rule-based component                                                                                                     capture both rule-based and similarity-based function learn-
of this approach can potentially explain how humans carry out                                                                                                 ing. As originally presented, this model takes kernel func-
superspace extrapolation when learning the unrotated func-                                                                                                    tions that express linear and quadratic rules as well as a
tions in our experiment, and the similarity-based component                                                                                                   standard similarity-based kernel function for which the co-
may help to explain responses for the rotated functions.                                                                                                      variance between any two points x and x0 is K(x, x0 ) =
   To demonstrate that the hybrid approach can account for                                                                                                    θ1 exp(− θ12 ||x − x0 ||2 ), where θ1 and θ2 determine the
                                                                                                                                                                          2
our data, we developed a computational model which as-                                                                                                        smoothness of the function. Intuitively, this last kernel ex-
sumes that humans make use of a hypothesis space that con-                                                                                                    presses the assumption that functions are locally smooth, and
tains several families of functions. Some of these function                                                                                                   was used to produce the extrapolations in Figure 1f. The
families correspond to simple rules, and others are more                                                                                                      model generates predictions by integrating over all possible
generic and include all smooth functions. The first column of                                                                                                 functions for all function types, integrating out all applicable
Table 3 shows one such hypothesis space that includes linear,                                                                                                 parameters. For a more detailed description, see Griffiths et
quadratic, difference, maximum and product functions, along                                                                                                   al. (2009).
with one generic family of smooth functions. Given this hy-                                                                                                      Our extension to the original model was to add a kernel
pothesis space and a set of training examples, extrapolation                                                                                                  capturing each of the three non-linear rules in our experiment,
judgments can be made by using the posterior distribution                                                                                                     which are equivalent to Bayesian regression models of the
over the space of functions.                                                                                                                                  form β|x − y|, βmax(x, y), and βxy, where β is a coefficient
   The model we implemented builds on the hybrid approach                                                                                                     distributed normally around one. We assigned each new ker-
of Griffiths et al. (2009), which uses Gaussian processes to                                                                                                  nel a prior probability of 0.001, or one tenth that of the least-
                                                                                                                                                       717

probable kernel in the original model, before renormalizing           hybrid approach to function learning that accommodates both
kernel probabilities. See Table 3 for a summary of all of the         explicit rules and similarity-based inferences.
kernels in the model and their corresponding probabilities.              Superspace extrapolation requires learners to go beyond
   Model predictions are shown in Figures 2e and f, along             the available data in a fundamental way, and other prob-
with correlations with human extrapolation judgments. In              lems where humans make inferences based on limited data
most cases the predictions of this model closely matched par-         have also provided important evidence about human induc-
ticipants’ superspace extrapolations for both unrotated and           tive biases (Shepard, 1994; Feldman, 1997). Psychologists
rotated functions. The latter result is the more striking of the      sometimes study what can be learned from textual corpora
two, as the predictions arise from averaging over several ker-        and other massive data sets, but exploring what humans learn
nels rather than choosing suitable ones in advance.                   from highly constrained data sets can be equally valuable.
   The one major discrepancy between model predictions and
human judgments occurs for the rotated version of |x − y|,            Acknowledgments. This work was supported by the James
where the extrapolation judgments predicted by the model are          S. McDonnell Foundation Causal Learning Collaborative Ini-
substantially more extreme than the human responses. This             tiative and by NSF award CDI-0835797.
result is driven by the fact that the family of difference func-                                 References
tions in Table 3 can perfectly account for the rotated training
                                                                      Bott, L., & Heit, E. (2004). Nonmonotonic extrapolation in func-
points if β1 takes a value larger than 1. Unlike the model, hu-
                                                                         tion learning. Journal of Experimental Psychology: Learning,
mans may be unable to learn weighted versions of the differ-
                                                                         Memory, and Cognition, 30(1).
ence function in Table 3, which could be captured by setting          Brehmer, B. (1974). Hypotheses about relations between scaled
the coefficient β1 for this family to 1. The model represents            variables in the learning of probabilistic inference tasks. Orga-
the simplest possible extension of the Gaussian process ac-              nizational Behavior and Human Decision Processes, 11, 1-27.
count of Griffiths et al. (2009), but adjusting the priors on the     Busemeyer, J., Myung, I. J., & McDaniel, M. (1993). Cue com-
coefficients may result in a more accurate model of human                petition effects: Theoretical implications for adaptive network
learning.                                                                learning models. Psychological Science, 4(3), 196.
                                                                      Carroll, J. (1963). Functional learning: The learning of continuous
Alternative models                                                       functional mappings relating stimulus and response continua.
Several recent models that address extrapolation in function          DeLosh, E. L., Busemeyer, J. R., & McDaniel, M. A. (1997). Ex-
learning and multiple cue judgment, including POLE (Kalish               trapolation: The sine qua non of abstraction in function learn-
et al., 2004), EXAM (DeLosh, Busemeyer, & McDaniel,                      ing. Journal of Experimental Psychology: Learning, Memory,
1997), and Sigma (Juslin et al., 2008), all suggest that humans          and Cognition, 23, 968-986.
extrapolate according to linear functions. In their present           Feldman, J. (1997). The structure of perceptual categories. Jour-
forms, none of these models appear to account for our re-                nal of Mathematical Psychology, 41, 145-170.
sults. We fit the POLE model to our data and found that ex-           Griffiths, T. L., Lucas, C. G., Williams, J. J., & Kalish, M. L.
trapolations were consistently piecewise linear in one cue di-           (2009). Modeling human function learning with Gaussian pro-
                                                                         cesses. Advances in Neural Information Processing Systems,
mension, and invariant to the other, taking a form like that in
                                                                         21.
Figure 1c. This approach to superspace extrapolation seems
                                                                      Juslin, P., Karlsson, L., & Olsson, H. (2008). Information integra-
plausible a priori but it does not reflect the behavior of our           tion in multiple cue judgment: A division of labor hypothesis.
participants. The EXAM model makes extrapolation predic-                 Cognition, 106(1), 259–298.
tions using the nearest past examples to a new point, imply-          Kalish, M., Lewandowsky, S., & Kruschke, J. (2004). Population
ing that peoples’ judgments are invariant to the rotation of a           of linear experts: Knowledge partitioning and function learning.
given function, which is inconsistent with our data. Finally,            Psychological Review, 111, 1072-1099.
the Sigma model (Juslin et al., 2008) proposes that humans            Kelley, H., & Busemeyer, J. (2008). A comparison of models for
can acquire explicit representations of linear functions but             learning how to dynamically integrate multiple cues in order to
that extrapolation of non-linear functions relies on similarity-         forecast continuous criteria. Journal of Mathematical Psychol-
based generalization. The Sigma model is therefore incon-                ogy, 52(4), 218–240.
sistent with our finding that people were able to learn several       Koh, K. (1993). Induction of combination rules in two-
non-linear functions.                                                    dimensional function learning. Memory & cognition, 21(5),
                                                                         573–590.
                          Conclusion                                  Koh, K., & Meyer, D. (1991). Function learning: Induction of
                                                                         continuous stimulus-response relations. Journal of Experimen-
We introduced the problem of superspace extrapolation,
                                                                         tal Psychology: Learning, Memory, and Cognition, 17(5), 811–
which provides a new way to explore the inductive biases that            836.
people bring to the task of function learning. Our data sug-          Shepard, R. N. (1994). Perceptual-cognitive universals as reflec-
gest that these inductive biases include a toolkit of linear and         tions of the world. Psychonomic Bulletin and Review, 1, 2-29.
non-linear rules that can be compared against the available
data. Our results challenge several popular accounts of func-
tion learning, but we showed that they are compatible with a
                                                                  718

