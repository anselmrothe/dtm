UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Concept learning as motor program induction: A large-scale empirical study
Permalink
https://escholarship.org/uc/item/0005t82w
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Lake, Brenden
Salakhutdinov, Ruslan
Tenenbaum, Joshua
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

      Concept learning as motor program induction: A large-scale empirical study
               Brenden M. Lake                              Ruslan Salakhutdinov                      Joshua B. Tenenbaum
Department of Brain and Cognitive Sciences                   Department of Statistics       Department of Brain and Cognitive Sciences
    Massachusetts Institute of Technology                     University of Toronto             Massachusetts Institute of Technology
                               Abstract                                    more structured representations that can generalize in deeper
   Human concept learning is particularly impressive in two re-            and more flexible ways. Concepts have been characterized
   spects: the internal structure of concepts can be representation-       in terms of “intuitive theories,” which are mental explana-
   ally rich, and yet the very same concepts can also be learned           tions that underly a concept (e.g., Murphy & Medin, 1985),
   from just a few examples. Several decades of research have
   dramatically advanced our understanding of these two aspects            or “structural description” models, which are compositional
   of concepts. While the richness and speed of concept learn-             representations based on parts and relations (e.g., Winston,
   ing are most often studied in isolation, the power of human             1975; Hummel & Biederman, 1992). In the latter framework,
   concepts may be best explained through their synthesis. This
   paper presents a large-scale empirical study of one-shot con-           the concept “Segway” might be represented as two wheels
   cept learning, suggesting that rich generative knowledge in the         connected by a platform, which supports a motor, etc. Most
   form of a motor program can be induced from just a single               recently, research in AI and cognitive science has empha-
   example of a novel concept. Participants were asked to draw
   novel handwritten characters given a reference form, and we             sized rich generative representations. Concepts like “house”
   recorded the motor data used for production. Multiple drawers           can vary in both the number and configuration of their parts
   of the same character not only produced visually similar draw-          (windows, doors, balconies, etc.), much like the variable syn-
   ings, but they also showed a striking correspondence in their
   strokes, as measured by their number, shape, order, and direc-          tactic structure of language. This has lead researchers to
   tion. This suggests that participants can infer a rich motor-           model objects and scenes using generative grammars (Wang
   based concept from a single example. We also show that the              et al., 2006; Savova, Jakel, & Tenenbaum, 2009; Zhu, Chen,
   motor programs induced by individual subjects provide a pow-
   erful basis for one-shot classification, yielding far higher accu-      & Yuille, 2009) or programs (Stuhlmuller, Tenenbaum, &
   racy than state-of-the-art pattern recognition methods based on         Goodman, 2010).
   just the visual form.                                                      A different tradition has focused more on rapid learning
   Keywords: concept learning; one-shot learning; structured               and less on conceptual richness. People can acquire a concept
   representations; program induction
                                                                           from as little as one positive example, contrasting with early
   The power of human thought derives from the power of                    work in psychology and standard machine learning that has
our concepts. With the concept “car,” we can classify or even              focused on learning from many positive and negative exam-
imagine new instances, infer missing or occluded parts, parse              ples. Bayesian analyses have shown how one-shot learning
an object into its main components (wheels, windows, etc.),                can be explained with appropriately constrained hypothesis
reason about a familiar thing in an unfamiliar situation (a car            spaces and priors (Shepard, 1987; Tenenbaum & Griffiths,
underwater), and even create new compositions of concepts (a               2001), but where do these constraints come from? For sim-
car-plane). These abilities to generalize flexibly, to go beyond           ple prototype-based representations of concepts, rapid gen-
the data given, suggest that human concepts must be represen-              eralization can occur by just sharpening particular dimen-
tationally rich. Yet it is remarkable how little data is required          sions or features, as described in theories of attentional learn-
to learn a new concept. From just one or a handful of exam-                ing (Smith, Jones, Landau, Gershkoff-Stowe, & Samuelson,
ples, a child can learn a new word and use it appropriately                2002) and overhypotheses in hierarchical Bayesian models
(Carey & Bartlett, 1978; Markman, 1989; Bloom, 2000; Xu                    (Kemp, Perfors, & Tenenbaum, 2007). From this perspective,
& Tenenbaum, 2007). Likewise, after seeing a single “Seg-                  prior experience with various object concepts may highlight
way” or “iPad,” an adult can grasp the meaning of the word,                the most relevant dimensions for whole classes of concepts,
an ability called “one-shot learning.” A central challenge is              like the “shape bias” in learning object names (as opposed to
thus to explain these two remarkable capacities: what kinds of             a “color” or “material bias”). It is also possible to learn new
representations can support such flexible generalizations, and             features over the course of learning the concepts (Schyns,
what kinds of learning mechanisms can acquire a new con-                   Goldstone, & Thibaut, 1998), and recent work has combined
cept so quickly? The greater puzzle is putting them together:              dimensional sharpening with sophisticated methods for fea-
how can such flexible representations be learned from only                 ture learning (Salakhutdinov, Tenenbaum, & Torralba, 2011).
one or a few examples?                                                        Despite these different avenues of progress, we are still far
   Over the last couple of decades, the cognitive science of               from a satisfying unified account. The models that explain
concepts has divided into different traditions, focused largely            how people learn to perform one-shot learning are restricted
on either the richness of concepts or on learning from sparse              to the simplest prototype- or feature-based representations;
data. In contrast to the simple representations popular in                 they have not been developed for more sophisticated repre-
early cognitive models (e.g., prototypes; Rosch, Simpson, &                sentations of concepts such as structural descriptions, gram-
Miller, 1976) or conventional machine learning (e.g., sup-                 mars, or programs. There are also reasons to suspect that
port vector machines), one tradition has worked to develop                 these richer representations would be difficult if not impos-
                                                                       659

sible to learn from very sparse data. In linguistics, for in-
stance, grammar induction is typically studied in the limit as
the number of examples goes to infinity; why should we ex-
pect learning a grammar that describes instances of houses, or
cars, to be possible from just one example? Theoretical argu-
ments (e.g., the bias/variance tradeoff; Geman, Bienenstock,
& Doursat, 1992) imply that representationally rich concepts            Figure 1: The top row shows example characters from our dataset,
should generally require more data to learn, not less. The              in the original printed form. Below are three example drawings from
                                                                        participants.
work of Winston (1975) and Lovett, Dehghani, and Forbus
(2007) might be the closest to human-level concept learning,            online study where participants drew novel character con-
where they learned relational schemata for simplified notions           cepts after seeing just a single example. We refer to this
of “arches,” “houses,” “stoves,” and “fireplaces” from short            task as “one-shot category production,” drawing inspiration
sequences of examples. But a fully human-like, one-shot                 from numerous studies that have used the generation of cat-
learning ability was beyond their scope.                                egory exemplars as a window into conceptual representation
                                                                        (e.g., Battig & Montague, 1969; Rosch et al., 1976; Feld-
   Even with these gaps in our understanding, we believe that
                                                                        man, 1997). We see one-shot category production as a spe-
the power of human concepts will be best explained by bring-
                                                                        cial case of “one-shot learning,” which includes classification
ing these two traditions together. By doing so, we hope to ex-
                                                                        and other types of generalization from just one example. Our
plore the extent to which people can learn representationally
                                                                        large-scale study produced about 32,000 images of charac-
rich concepts from very sparse data, and we also hope to ex-
                                                                        ters across a set of 1,600 concepts, and the on-line drawing
plain this ability in computational terms. These are the long-
                                                                        trajectories were recorded for each image. From the produc-
term goals of our work. Here we take a first step with a large-
                                                                        tion data, we analyzed the extent to which people can infer
scale empirical study of one-shot concept learning, using a
                                                                        a robust motor program representation from a single exam-
domain of handwritten characters from the world’s alphabets
                                                                        ple. We also compared humans and multiple computational
(see Figure 1). These objects are not nearly as complex as
                                                                        approaches on a one-shot classification task, using methods
many object concepts such as “house,” “dog,” or “Segway,”
                                                                        based on either the motor data or just the visual forms.
but they still offer a vast number of novel, high-dimensional,
and cognitively natural categories with important relational            Category production experiment
structure. They are much richer than the highly oversimpli-
                                                                        The 1,600 character concepts were collected from 50 alpha-
fied artificial stimuli used in previous laboratory studies of
                                                                        bets, including current or historic scripts (e.g., Bengali, San-
one-shot learning (Feldman, 1997; Kemp & Jern, 2009). Yet
                                                                        skrit, and Tagalog) and invented scripts for purposes like sci-fi
characters are still simple enough for us to hope that tractable
                                                                        novels. The characters were taken from www.omniglot.com
computational models can represent all the structure people
                                                                        in printed fonts, and several originals and their subsequently
see in them – unlike natural images.
                                                                        drawn images are shown in Fig. 1. This dataset was pre-
   What is the right structural representation for these simple         viously used to compare models of one-shot classification
visual concepts? The generative process for any handwrit-               (Lake, Salakhutdinov, Gross, & Tenenbaum, 2011).
ten character is a motor program, which is a set of instruc-               The drawing experiment was run through Amazon Me-
tions, in the mind of the drawer, that can be sent to the mo-           chanical Turk, and participants were asked to draw at least
tor effectors such as an arm or a hand. These programs are              one entire alphabet. For each template image, they were
complex compositions of pen strokes (the “parts” or the “sub-           asked to “draw each character as accurately as you can.” An
routines” of the program), which might vary in their number,            alphabet’s printed characters were displayed in rows on a
order, and style across drawers. Despite these various de-              webpage, with an associated drawing pad below each image.
grees of freedom, human drawing is noted for its regularity,            Participants could draw by holding down a mouse button and
which has been likened to a “grammar of action” (Goodnow                moving the mouse, and we also included “forward,” “back,”
& Levine, 1973). Thus it seems fruitful to explore a genera-            and “clear” buttons. Some participants made minor image ad-
tive approach based on motor programs, especially since peo-            justments with small mouse movements, and we tried to mit-
ple have the generative capacity for drawing. There are also            igate this inconsistency by excluding strokes that were very
well-developed, feature-based alternatives from psychology              short in time and space from the analysis.
(Grainger, Rey, & Dufau, 2008) and machine learning, espe-
cially “deep learning” models which have achieved some of                       The structure of the motor programs
the best results on handwritten digit classification (0, 1, 2, ...,     When people perceive a new character, in what sense do they
9) (e.g., Salakhutdinov & Hinton, 2009). Thus it will be im-            infer a new concept? While this mental representation might
portant to compare multiple computational approaches, with              be just a bundle of features, the concept might also include
the goal of better understanding the psychological processes            richer structure in the space of motor programs. To investi-
and also improving one-shot learning in machines.                       gate this possibility, we analyzed how multiple drawers pro-
   To begin exploring these questions, we ran a large-scale             duced a particular concept during the drawing task. We rea-
                                                                    660

                              Human drawers                                     Simple drawers                                             The shape of the parts
                canonical
                             11     2         1     2
                                                                            1          1    1
                                                                                             3
                                                                                                            1
                                                                                                            2      3               1
                                                                                                                                           The 1parse of a character into parts (strokes) is at the core of
1            1 1                                           1       3               3 2                                          3        3
                                           1     2            2                                             3         3
  2
          3
              2 2
                       33
                                2
                                        3
                                                    3            3   2              2            1                     1
                                                                                                                        2                  each
                                                                                                                                           2
                                                                                                                                                      drawing. When people look at a new concept, do they
                                      3               3                                                      2                   2
                    5             5.15            5.1
                                                5.3          5.3                      6.5               246.5             17 24            perceive
                                                                                                                                             17           the same parts? This is difficult to analyze, since the
                                                                         1                1                 2                  12          number 1      and length of the strokes can differ between images.
1            1 12 2            1 22              22            2                3      2 2          3      2
                                                                                                           3                   3 2
                                                                                                                                          3A similarity measure should also be invariant to the order and
       2                                                                 2                                                                          2
                                                      3                            1                   1
                               1      3     1 1            1                                        1                 3 1
                                                   3             3
          3           33                3                                   3               3
      6.2           6.2
                  6.2               6.2
                                  7.1             7.1
                                                7.4          7.4        20             1120             10 11             23 10
                                                                                                                                           direction
                                                                                                                                             23
                                                                                                                                                           of the strokes. Despite these challenges, we found
                                                                                                                                           that it was possible to analyze consistency in the shape of the
                          1                 1     2                                                              3             2     3
               11                12                                               3                   3 2
1       3             32
                                    33
                                         2                         1     3      11
                                                                                       2
                                                                                          3        11                      1               strokes, and we discuss our method in the section below.
                  3                                   3                2
          2             2                                                              2                    2
      8.2           8.2
                  8.4             128.4            12                   17             1617             20 16                20            Shape-based distance in motor space. Since most draw-
                                                                                                                                           ers (66%) used the modal number of strokes, we restrict
                canonical
 12    3      1122 3        121 2 33       12 3
                                            12 3          12  3    1   2      4   11 22        4 1    1 2         4   2 1
                                                                                                                       1              4
                                                                                                                                           this and subsequent analyses to only these modal drawings.
                                                                                                                                          2 1
                    3                                                                                   2                    2
    4           44             44           44             4                                                          4                  4
                                                                                                                                           With this simplification, the strokes in two images can be
                                                                        3         3 43               33 4                 33               matched
                                                                                                                                             3             in correspondence (one-to-one and onto). Our ap-
                   2.4            3.4
                                   2.4          4.4
                                                 3.4         4.4                      23               16 23              2316               23
 12 3        1 12
                2                   3      12 2 3 3           3      4   1       1 4 21           341                1 342
                                                                                                                                           proach
                                                                                                                                             2
                                                                                                                                                         also  matches the sub-structure within two strokes,
                    3
                    3      1 22      3
                                        1               1 12                                                2
                                                                                                               1                   1    1
   4           44             44            44             4         3              3                                      4               finding
                                                                                                                                              4          an  alignment   between the points in the two trajec-
                                                                              2                2         2                         3
                                                                                 3      4            3      4
                                                                                                                              2
                                                                                                                                           tories3 (onto but not one-to-one). Given an optimal matching
      4.6          5.3
                    4.6           6.1           6.7          6.7        26           9.926             21                 1521
                                   5.3           6.1                                                     9.9                               at15both levels, the overall shape distance is roughly the mean
  3
  1      2      1 24 2
                3                                                         4         4     4                                     1
                           12 324                                   1                                  24
  4            34
                              4
                                3
                                            12 43 2
                                           31
                                              4
                                                          31 4 2
                                                                                1
                                                                                   1
                                                                                                   1
                                                                                                                4         32
                                                                                                                             4
                                                                                                                                    4
                                                                                                                                           distance
                                                                                                                                              3
                                                                                                                                                   1
                                                                                                                                                          between all of the aligned trajectory points. Before
                                                                                2                          3     1                              4
                                                                                                   2                           3     1
             1                                                      2
                                                                              3
                                                                                   2
                                                                                     3         3         3
                                                                                                                          2
                                                                                                                                           computing
                                                                                                                                              2              distance, characters were also transformed to be
                            1
      7.4          11
                    7.4           1311          15
                                                 13          15         20            1320             27 13              3027             translation
                                                                                                                                             30              and scale invariant.1 Examples of the distance are
            Figure 2: For two concepts (out of the 1600 total), each box shows                                                             illustrated in Fig. 2, where the number below each drawing is
            the motor data produced by human drawers (left) or simple draw-
            ers (right). “Canonical” drawers are in the dotted boxes, and their                                                            the distance to the drawing in the dotted box.
            distances (Eq. 1) to the other examples are the numbers below each                                                                   The details of the distance measure are as follows.
            frame. Stroke color shows correspondence to the canonical, circles                                                             Consider two drawings S1 , ..., Sk and R1 , ..., Rk with k
            indicate the beginning of each stroke, and numbers inside circles
            denote stroke order.                                                                                                           strokes each. Each stroke is a sequence of positions Si =
                                                                                                                                           [Si1 , ..., Sin ] with arbitrary length, where Sij ∈ R2 . The over-
            soned that in order to do this task, participants must infer                                                                   all distance between the characters is defined as
            a novel motor program, which will be reflected in the time
            course of drawing. Consistency in the structure of these draw-                                                                                  k
                                                                                                                                                        1X
            ings would provide evidence for two interlinked claims: peo-                                                                         min           min [dtw(Si , Rπ(i) ), dtw(Si , F (Rπ(i) ))],     (1)
                                                                                                                                                     π  k
            ple seem to grasp the same underlying concept from one ex-                                                                                     i=1
            ample, and this concept includes a highly structured genera-
                                                                                                                                           where π(·) is a permutation on the stroke indices 1, ..., k (a
            tive program. To measure consistency for a particular charac-
                                                                                                                                           bijective function from the set {1, ..., k} to {1, ..., k}), and the
            ter, we quantitatively analyzed the number, shape, direction,
                                                                                                                                           flip function F (Si ) = [Sin , ..., Si1 ] reverses the stroke direc-
            and order of the parts (strokes) in the motor data.
                                                                                                                                           tion to provide direction invariance. The distance dtw(·, ·) be-
            The number of parts                                                                                                            tween two trajectories is calculated by Dynamic Time Warp-
                                                                                                                                           ing (DTW; Sakoe & Chiba, 1978), which fits a non-linear
            This analysis (and subsequent ones) used just 20 of the alpha-                                                                 warp such that each point in one trajectory is aligned with a
            bets in the dataset, excluding the six most common as deter-                                                                   point in the other. The DTW distance is then the mean Eu-
            mined by Google hits. The remaining alphabets were needed                                                                      clidean distance across all pairs of aligned points.
            to train the alternative models in the later classification ex-
            periment. The simplest statistic to analyze was the number of                                                                  The simple drawer model. Upon visual inspection of the
            parts. For each character, we investigated whether the draw-                                                                   stroke matches π(·) chosen by the outer minimization in Eq.
            ers clustered around a common number of parts (the mode                                                                        1, there is a striking consistency across drawers in the inferred
            number across participants). Aggregating across each draw-                                                                     parts for a character. We show two characters in Fig. 2, where
            ing in the dataset, the histogram in Fig. 3A (red) shows the                                                                   color denotes the stroke matches (left panels). The plots for
            absolute difference between the actual number of strokes and                                                                   the entire dataset are available online.2 While this qualita-
            the mode number of strokes from all of the drawings of that                                                                    tive correspondence may reflect richly structured motor pro-
            character. Although this distribution is guaranteed to peak at                                                                 cesses, there could be a more simplistic explanation. The
            zero, a strikingly large percentage of drawers used exactly the                                                                consistency could be a consequence of selection bias, since
            modal number (66%). As a control, a null dataset was created                                                                   we selected drawers that used the modal number of strokes,
            by replacing each number of strokes by a uniform draw (1 to                                                                           1
                                                                                                                                                    This transformation subtracts the center of gravity and rescales,
            6 here, but other values are similar). This distribution was not                                                               such that the range of the largest dimension is 105.
                                                                                                                                                  2
            nearly as peaked around the mode (Fig. 3A blue).                                                                                        http://web.mit.edu/brenden/www/consistency.html
                                                                                                                                  661

             A) Number of strokes                     C) Stroke
                                                           1,2,3,4,5direction
                                                                     strokes             D)2Stroke
                                                                                               strokes
                                                                                                    2 strokes
                                                                                                         order                3 strokes
                                                                                                                                     3 strokes                    4 stro
                                                                                                                                                            4 strokes
                                                          {1,2,3,4,5} strokes   human drawing data  2 strokes                   3 strokes
                                                                                simple drawing data
                  0 2     2 2 strokes
                              3 4 5
                      1 strokes                     0.5 3 strokes
                                                                3 0.75
                                                                  strokes       1       0.254 0.25
                                                                                                strokes0.5 0.75
                                                                                                0.54 strokes
                                                                                                       0.75    1    1            0.55 strokes
                                                                                                                          0.255 0.25
                                                                                                                                 strokes0.75
                                                                                                                                        0.5 0.751  1    0.25 0.25    0.5
                                                                                                                                                               0.5 0.7
      Difference  from mode
              Difference       number
                            from   modalof strokes      Probability of modal
              number of strokes                         stroke direction                            4 strokes                   5 strokes
             B) Stroke      shape
                         1 stroke
                   1 stroke                               2,3,4,5
                                                    2,3,4,5       strokes
                                                            strokes
                           1 stroke                {2,3,4,5} strokes
                         0.5 0.5
                  0.25 0.25     0.75 0.75
                                        1     1              0.5 0.5
                                                      0.25 0.25     0.75 0.75
                                                                            1    1       0.25 0.25
                                                                                                 0.5 0.50.75 0.75
                                                                                                                1   1     0.25 0.25
                                                                                                                                  0.5 0.50.75 0.75
                                                                                                                                                 1 1
                                                                                                  Probability of modal stroke order
                                                                                      humandrawing
                                                                                      human     drawers  data
                 10    1020     2030    30           10    1020     2030    30        simple    drawers
                                                                                      simpledrawing      data
                       Mean distance between drawers
Figure 3: A histogram analysis of the consistency in the motor data, comparing human drawers (red) with a parallel dataset of simple drawers
(blue) designed as a null hypothesis. Humans are strikingly consistent across a range of statistics compared to the simple model. As labeled,
some histograms pool     0 data
                             1 from
                                 2 3 characters
                                         4 5        with different numbers of strokes (e.g., {2,3} includes 2- and 3-stroke characters).
             Difference from mode number of strokes
and there will be fewer degrees of freedom available to a k-                       problem, we optimized using simulated annealing with alter-
stroke drawer for any given k. In the special case of k disjoint                   nating Metropolis-Hasting node swaps and Gibbs sampling
segments (like in Braille), there may only be one production                       (Rubinstein & Kroese, 2008).
option. To explore the degrees of freedom and to provide a
                                                                                   Results. The simple drawer was used to re-sketch each im-
baseline for the observed consistency, we devised a “simple
                                                                                   age, creating an entire parallel dataset for comparative anal-
drawer” model that is likely to mimic human drawers when
                                                                                   ysis. The shape-based consistency of a character is the mean
the space is highly constrained, but otherwise it more freely
                                                                                   distance (Eq. 1) between each pair of drawings of that char-
explores the potential motor space.
                                                                                   acter. Fig. 3B shows histograms of this consistency measure
    The simple drawer is given access to the same set of points
                                                                                   for the human drawers (red) and the simple drawers (blue).
a real drawer traversed in the motor data, but without the se-
                                                                                   The aggregate histogram (right) for characters with two to
quential information. It then tries to draw the same character
                                                                                   five strokes shows a large difference in the consistency of
as efficiently as possible using the same number of strokes.
                                                                                   the parts. The histogram for characters with one stroke (left)
It must visit every point exactly once, while minimizing the
                                                                                   shows a closer correspondence between participants and the
distance traveled while ink is flowing. Given a real draw-
                                                                                   simple drawer, due to the limited degrees of freedom.3 These
ing with strokes S1 , ..., Sk , the simple drawer’s interpretation
                                                                                   results suggest that people inferred motor programs that were
Q1 , ..., Qk is defined by the problem
                                                                                   based on a characteristic set of strokes.
                             k |QX i |−1
                argmin
                           X
                                         ||Qij − Qi(j+1) ||2 ,             (2)     The direction of the parts
               Q1 ,...,Qk i=1 j=1
                                                                                   Do different drawers infer the same stroke directions? For
                                                                                   each character, a single canonical drawer was chosen to mini-
where | · | is the number of points in the stroke sequene, and
                                                                                   mize the sum shape-based distance across all other drawers of
|| · ||2 is Euclidean distance. Each point Sij in the original
                                                                                   that character (Eq. 1). Example canonical drawers are shown
drawing is equal to exactly one point Qab in the new draw-
                                                                                   in the dotted boxes in Fig. 2 (left). For each person’s drawing
ing. This formulation encourages smooth strokes, but it also
                                                                                   compared to the canonical drawing, the chosen value of the
leads to creative parses (Fig. 2 right panels), in part be-
                                                                                   inner minimization in Eq. 1 indicates whether each stroke,
cause there are multiple optima. A drawback of the model
                                                                                   or that stroke in reverse direction (F (·)), is a better match to
is that it sometimes draws paths where no ink exists. To
                                                                                   the corresponding stroke in the canonical drawer. Aggregat-
reduce this problem, the simpler drawer is not allowed to
                                                                                   ing across each stroke in the dataset, Fig. 3C (red) displays
travel large distances between adjacent points, where the up-
                                                                                   the proportion of times the modal stroke direction was picked,
per bound is the maximal adjacent distance in the corre-
                                                                                   using the canonical drawer as the reference point. The dataset
sponding real drawing. For optimization, we can reformulate
the problem as the well-known traveling salesman problem                               3
                                                                                         Some single stroke characters can still be drawn in a number of
(TSP) by inserting k cost-free “points” to indicate the stroke                     ways, such as choosing the starting location of an “O.” People tend
breaks. Inspired by efficient approximate solvers for the TSP                      to start at the top, while the simpler drawer is agnostic.
                                                                               662

of simple drawers (blue) provides a direction-agnostic base-                                             One shot classification       human drawers
                                                                                           100
                                                                                                                                       simple drawers
line. By comparison, people’s inferred programs clearly have                                90
                                                                                            80                                         inferred parts
preferred directions.
                                                                         Percent correct
                                                                                            70                                         features
                                                                                            60                                         pixels
The order of the parts                                                                      50                                         human−level
Is stroke order also consistent across drawers? As in the                                   40
                                                                                            30
analysis of direction, and the canonical drawers were used                                  20
as the reference points, from which stroke order was defined.                               10
For any person’s drawing compared to the canonical draw-                                     0
                                                                                                 1   2       3         4           5
ing of that character, the chosen permutation π(·) from the                                                Number of strokes
outer minimization in Eq. 1 defines a relative ordering of the           Figure 4: Classification performance based on one example of 20
strokes. Aggregating across each drawing, Fig. 3D shows                  different characters. Test instances were compared to each class,
                                                                         and the best match was selected.
the proportion of times the modal stroke order was picked.
Like the other statistics, stroke order was also highly consis-          not improve significantly. Finally, human one-shot classifi-
tent across characters. Unsurprisingly, consistency was less             cation performance was 96%, as measured behaviorally in a
pronounced as the number strokes increased.                              20-way classification task (“human-level” in Fig. 4; see foot-
                                                                         note for experimental setup).4 Overall, the motor data was by
                One-shot classification                                  far the most effective means for one-shot classification.
The previous analyses suggest that people can infer rich
motor-based concepts from just a single example. If the same
                                                                                                           Discussion
perceptual inferences occurred in the context of categoriza-             Our category production experiment produced over 32,000
tion, would these representations prove useful for one-shot              images of handwritten characters. Each of the roughly 1,600
classification? We investigated this question by using the mo-           characters was drawn by 20 different participants, and we
tor data to classify characters by type, based on the shape-             found a strong correspondence in the structure of their in-
based distance measure (Eq. 1). The model received 20 ran-               ferred motor programs. On the whole, the number, shape,
dom characters with just one example each. Test examples                 order, and direction of the parts (strokes) was highly consis-
(2 per class) were classified as the best fitting category. All          tent across participants. Also the motor data provided a pow-
20 categories used the same (modal) number of strokes. This              erful basis for one-shot classification. These results suggest
classification task was repeated 20 times with different char-           that when people look at a new character, they can infer a
acters, and the mean percent correct is shown in Fig. 4.                 richly structured motor program. This motor program is ca-
   We used several baselines for comparison. The simplest                pable of both synthesizing new examples and classifying new
method picked the closest image in pixel space, using Eu-                instances with high discriminative accuracy.
clidean distance. We also tested Deep Boltzmann Machines                    How can these motor programs be learned from just one
(DBMs; Salakhutdinov & Hinton, 2009) as a representative                 example? This ability clearly depends on prior experience,
feature-based model. DBMs learn a hierarchy of distributed               but how does this translate into constraints on the formation
feature representations for the raw pixels, without using a pri-         of these programs? There are various possibilities. Prior
ori knowledge about the geometry of images. DBMs have                    knowledge might come in the form of shared sub-programs
obtained state-of-the-art performance on handwritten digit               or shared strokes, like our preliminary model in Lake et al.
recognition when trained with thousands of digits, and we                (2011). From their general writing and drawing experience,
pre-trained it on the 30 alphabets that were not used for clas-          people might learn sub-routines like “circles, diagonal lines,
sification (about 19,000 images). For one-shot classification,           or S-shapes,” and then they could parse novel characters into
new items were represented in feature space and classified               this rich set of parts. But prior knowledge could come in
based on cosine similarity across all hidden layers (two with            many other forms, including more general constraints and bi-
1000 units each). We also tested a model that infers latent              ases (learned or otherwise) in human drawing and motor ca-
stroke-like parts from the raw images (Lake et al., 2011), as            pabilities. Researchers have found a number of rules that use-
well as the simple drawing model, which uses the same motor              fully characterize drawing: start drawing at the top-left, draw
data but without the strong structural consistency.                      horizontal strokes left-to-right, draw vertical strokes top-to-
   Performance was measured across a range of different                  bottom, and minimize the number of strokes (Goodnow &
numbers of strokes (Fig. 4). Chance performance is 5% cor-               Levine, 1973; Van Sommers, 1984). In a preliminary anal-
rect, and pixel distance performed at 20% correct on average             ysis, we have observed strong versions of these effects in
(“pixels” in Fig. 4). Next was the DBM at 37% (“features”),              our dataset of natural alphabets. Thus, it is possible that
the inferred parts model at 48%, and then the simple drawer                  4
                                                                               This study was run on Amazon Mechanical Turk with 15 par-
at 50%. The real stroke data was far better than all of the              ticipants and 50 trials. Each trial consisted of a single test image,
other methods, with an average performance of 83% correct.               and participants were asked to pick one of the 20 other images that
                                                                         looked the most similar. This was the same task that the models
We also tried to include stroke order and direction informa-             performed, except that characters with different numbers of strokes
tion in the classification cost function, but performance did            were intermixed and a different set of alphabets was used.
                                                                   663

some of the richness of these newly acquired concepts (in-               Kemp, C., & Jern, A. (2009). Abstraction and relational learning.
cluding shape, direction, and order) is a consequence of rela-             In Advances in Neural Information Processing Systems 22.
                                                                         Kemp, C., Perfors, A., & Tenenbaum, J. B. (2007). Learning over-
tively simple, low-level principles. But it is also unclear how            hypotheses with hierarchical Bayesian models. Developmental
these directives should be combined when they conflict, or                 Science, 10(3), 307–321.
how they might interact with other forms of prior knowledge.             Lake, B. M., Salakhutdinov, R., Gross, J., & Tenenbaum, J. B.
                                                                           (2011). One shot learning of simple visual concepts. In Pro-
Computational models are well-suited to help answer these                  ceedings of the 33rd Annual Cognitive Science Conference.
questions, and we hope that future work will clarify how prior           Liberman, A. M., Cooper, F. S., Shankweiler, D. P., & Studdert-
knowledge can support such rapid program induction.                        Kennedy, M. (1967). Perception of the speech code. Psychologi-
                                                                           cal Review, 74(6), 431–461.
   Finally, although our work has focused on handwritten                 Lovett, A., Dehghani, M., & Forbus, K. (2007). Incremental Learn-
characters, we expect that similar phenomena and computa-                  ing of Perceptual Categories for Open-Domain Sketch Recogni-
                                                                           tion Kenneth Forbus Comparisons and Generalization. In Pro-
tional accounts are relevant more broadly. Characters share                ceedings of the International Joint Conference on Artificial Intel-
interesting structure with other kinds of symbols used for                 ligence.
communication, including spoken words and gestures. Char-                Markman, E. M. (1989). Categorization and Naming in Children.
                                                                           Cambridge, MA: MIT Press.
acters are produced by a sequence of strokes, and likewise,              Murphy, G. L., & Medin, D. L. (1985). The role of theories in
spoken words are produced as a sequence of phonemes. Char-                 conceptual coherence. Psychological Review, 92(3), 289–316.
acters, spoken words, and gestures are also “embodied,” since            Rosch, E., Simpson, C., & Miller, R. S. (1976). Structural bases of
                                                                           typicality effects. Journal of Experimental Psychology: Human
the mind and body can both generate and perceive concepts                  Perception and Performance, 2(4), 491–502.
in these domains (e.g., Liberman, Cooper, Shankweiler, &                 Rubinstein, R. Y., & Kroese, D. P. (2008). Simulation and the Monte
Studdert-Kennedy, 1967; Freyd, 1983). All of these concepts                Carlo method (Second ed.). Hoboken, New Jersey: John Wiley
                                                                           & Sons.
must also be learnable from one or a few examples, in the                Sakoe, H., & Chiba, S. (1978, February). Dynamic programming
context of efficient communication and social learning. One-               algorithm optimization for spoken word recognition. IEEE Trans-
shot program induction may also be possible in learning very               actions on Acoustics, Speech, and Signal Processing, 26(1), 43–
                                                                           49.
different kinds of natural concepts, such as trees or ferns that         Salakhutdinov, R., & Hinton, G. E. (2009). Deep Boltzmann Ma-
have distinctive branching patterns and unique leaf shapes.                chines. In 12th Internationcal Conference on Artificial Intelli-
One-shot learning could be possible here for a different rea-              gence and Statistics (AISTATS).
                                                                         Salakhutdinov, R., Tenenbaum, J. B., & Torralba, A. (2011). Hierar-
son: not because of the strong priors imposed by motor con-                chical deep models for one-shot learning. In Neural Information
straints or previous learning, but because a single example is             Processing Systems (NIPS).
highly complex and contains extensive repeated structure. We             Savova, V., Jakel, F., & Tenenbaum, J. B. (2009). Grammar-based
                                                                           object representations in a scene parsing task. In Proceedings of
hope that future work will explore a fuller range of rich rep-             the 31st Annual Conference of the Cognitive Science Society.
resentations for concepts, while explaining how these same               Schyns, P. G., Goldstone, R. L., & Thibaut, J.-P. (1998). The de-
concepts can be learned from just one or a few examples.                   velopment of features in object concepts. Behavioral and Brain
                                                                           Sciences, 21, 1–54.
Acknowledgements We gratefully acknowledge Jason                         Shepard, R. N. (1987). Toward a Universal Law of Generalization
Gross for developing the experimental interface and for col-               for Psychological Science. Science, 237(4820), 1317–1323.
                                                                         Smith, L. B., Jones, S. S., Landau, B., Gershkoff-Stowe, L., &
lecting the data. We also thank John McCoy for helpful com-                Samuelson, L. (2002). Object Name Learning Provides On-the-
ments, and Dan Ellis for making his DTW code available.                    Job Training for Attention. Psychological Science, 13, 13–19.
                                                                         Stuhlmuller, A., Tenenbaum, J. B., & Goodman, N. D. (2010).
                                                                           Learning Structured Generative Concepts. In Proceedings of the
                           References                                      Thirty-Second Annual Conference of the Cognitive Science Soci-
                                                                           ety.
Battig, W. F., & Montague, W. E. (1969). Category norms for              Tenenbaum, J. B., & Griffiths, T. L. (2001, August). Generaliza-
   verbal items in 56 categories: A replication and extension of the       tion, similarity, and Bayesian inference. Behavioral and Brain
   Connecticut category norms. Jounral of Experimental Psychology          Sciences, 24(4), 629–40.
   Monograph, 80(3).                                                     Van Sommers, P. (1984). Drawing and Cognition. Cambridge Uni-
Bloom, P. (2000). How Children Learn the Meanings of Words.                versity Press.
   Cambridge, MA: MIT Press.                                             Wang, W., Pollak, I., Wong, T.-S., Bouman, C., Harper, M., &
Carey, S., & Bartlett, E. (1978). Acquiring a single new word.             Siskind, J. (2006, October). Hierarchical Stochastic Image Gram-
   Papers and Reports on Child Language Development, 15, 17–29.            mars for Classification and Segmentation. IEEE Transactions on
Feldman, J. (1997). The structure of perceptual categories. Journal        Image Processing, 15(10), 3033–3052.
   of Mathematical Psychology, 41, 145–170.                              Winston, P. H. (1975). Learning structural descriptions from exam-
Freyd, J. (1983). Representing the dynamics of a static form. Mem-         ples. In P. H. Winston (Ed.), The psychology of computer vision.
   ory and Cognition, 11(4), 342–346.                                      New York: McGraw-Hill.
Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural Networks        Xu, F., & Tenenbaum, J. B. (2007). Word Learning as Bayesian
   and the Bias/Variance Dilemma. Neural Computation, 4, 1–58.             Inference. Psychological Review, 114(2), 245–272.
Goodnow, J. J., & Levine, R. A. (1973). “The Grammar of Action”:         Zhu, L., Chen, Y., & Yuille, A. (2009, January). Unsupervised
   Sequence and syntax in childen’s copying. Cognitive Psychology,
   4(1), 82–98.                                                            learning of Probabilistic Grammar-Markov Models for object cat-
Grainger, J., Rey, A., & Dufau, S. (2008). Letter perception: from         egories. IEEE Transactions on Pattern Analysis and Machine In-
   pixels to pandemonium. Trends in Cognitive Sciences, 12(10),            telligence, 31(1), 114–28.
   381–387.
Hummel, J. E., & Biederman, I. (1992, July). Dynamic binding
   in a neural network for shape recognition. Psychological Review,
   99(3), 480–517.
                                                                     664

