UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Corpus-based metrics for assessing communal common ground
Permalink
https://escholarship.org/uc/item/80g5g3kk
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Kutlak, Roman
Van Deemter, Kees
Mellish, Chris
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                 Corpus-based metrics for assessing communal common ground
                                                Roman Kutlak (r04rk9@abdn.ac.uk)
                                           Kees van Deemter (k.vdeemter@abdn.ac.uk)
                                                Chris Mellish (c.mellish@abdn.ac.uk)
                                         Computing Science Department, University of Aberdeen
                                                    Aberdeen AB24 3UE, Scotland, UK
                              Abstract                                    To the best of our knowledge, no general computational
   This article presents the first attempt to construct a computa-     models exist for assessing what knowledge is likely to be
   tional model of common ground. Four corpus-based metrics            known. In this paper, we examine a corpus-based strategy
   are presented that estimate what facts are likely to be in com-     for building such a computational model. But, before we go
   mon ground. The proposed metrics were evaluated in an ex-
   periment with human participants, focussing on a domain of          into the details of our approach, there are some terminologi-
   famous people. The results are encouraging: two of the pro-         cal and conceptual issues to be clarified. Common and mutual
   posed metrics achieved a large positive correlation between the     knowledge have been defined in different ways. In this paper,
   estimates of how widely known a property of a famous person
   is and the percentage of participants who knew the correspond-      we shall follow the terminology of Vanderschraaf and Sillari,
   ing property.                                                       which the authors clarified with the following example.
   Keywords: Common Ground; Common Knowledge; Mutual
   Knowledge; Evaluation with human subjects; Web as corpus               Suppose each student arrives for a class meeting know-
                                                                          ing that the instructor will be late. That the instructor
                          Introduction                                    will be late is mutual knowledge, but each student might
Assessing other people’s knowledge is crucial in many sit-                think only she knows the instructor will be late. How-
uations. Teachers, for example, do well to highlight infor-               ever, if one of the students says openly, “Peter told me he
mation that their pupils do not know. Examples in other ar-               will be late again”, then the mutually known fact is now
eas abound. Suppose, for example, we want to persuade you                 commonly known. Vanderschraaf and Sillari (2009)
to reduce your intake of butter. We might do this by telling
you ”butter gives you high cholesterol”. This argument only            Thus, mutual knowledge is knowledge shared by a group of
works if you, the hearer, know that cholesterol is bad for you,        people. Common knowledge might be informally charac-
as is often assumed, for instance because it raises the likeli-        terised as knowledge that is publicly shared by a group of
hood of heart disease. The (presumed) fact that cholesterol            people. Slightly more precisely, A and B have mutual knowl-
is bad for you happens to be well publicised, and this might           edge of p if and only if A knows p and B knows p. They have
be what lies behind our assumption that you know it. Similar           common knowledge of p if they have mutual knowledge of p,
examples obtain in advertising, where companies might per-             and A knows that B knows p, and B knows that A knows p,
suade you to buy a toothpaste by saying it contains fluoride,          and A knows that B knows that A knows p, and so on, ad
because they assume that many viewers know that fluoride               infinitum (Lewis, 1969). Logicians and game theorists have
is good for your teeth. It is often important to distinguish be-       proposed various precise definitions of common knowledge
tween knowledge and belief, but we will focus on cases where           (including cases with more than two knowers), typically cast
the distinction is less than crucial.                                  in epistemic logic, which formalise the “ad infinitum” (above)
   The difference between information assumed to be “given”            in different ways (Vanderschraaf and Sillari (2009)). For rea-
(i.e., known by the hearer) and “new” (i.e., privileged infor-         sons that will become clear later, we use a third term that is of-
mation of the speaker) is crucial to philosophers, logicians           ten used in this connection, common ground, in a loose sense,
and linguists (Frege (1892 (1952)); Strawson (1952); Van Ei-           when the distinction between mutual and common knowledge
jck (1993), to mention but a few) and it is highly relevant to         is irrelevant.
computational linguists working on Natural Language Gener-                The psychologists Clark and Marshall observed that, in
ation (NLG) programs (Reiter & Dale, 2000), whose output               simple situations, common knowledge is enforced by “triple
is meant to mimic human language use. A central example                co-presence”, where the speaker, the hearers and entities are
is the generation of referring expressions, which has been             physically present and the speaker believes that the hearers
studied extensively over the last 20 years (Krahmer & van              attend to the entities (Clark and Marshall (1981)). They con-
Deemter, 2012). For example, an NLG program that aims to               trast this simple situation (which they call personal common
identify a person would do well to express properties that are         ground) with communal common ground, which arises not
likely to be known by the reader. For example, the expres-             from physical co-presence but from being in a shared com-
sion “the former member of Led Zeppelin” would not be very             munity (e.g., people living in Paris). Speakers are frequently
informative to a hearer who has never heard of Led Zeppelin.           able to distinguish between knowledge that is available to
                                                                   1834

members of such communities or to outsiders (Jucks, Becker,           be utilised, and we have chosen corpora as such a source of
& Bromme, 2008; Nickerson, Baddeley, & Freeman, 1987).                knowledge. Corpora in combination with measures of associ-
Personal common ground comes from joint personal experi-              ation were previously used in distributional models of seman-
ence of the agents; communal common ground derives from               tic representation, where the main assumption is that if words
a range of sources, including the likelihood of common ex-            appear in a similar context they have a similar meaning (Firth,
perience. For example, suppose Paris residents see the Eiffel         1957). Riordan and Jones (2011) examined in detail several
Tower as they travel around their city. As they have no reason        distributional and feature-based models and concluded that
to believe that others do not see the same Eiffel Tower, they         they performed similarly well. Jurafsky (2003) argued that
can exploit the knowledge shared by the residents of Paris            probabilistic modelling, as used by computational linguists,
namely, that the Eiffel Tower is in Paris. They can use this          can effectively model some of the phenomena observed by
mutual knowledge as a shared basis for the communal com-              psychologists. Given the past success of probabilistic mod-
mon ground. In this case, the community are the Paris resi-           els applied to various tasks including distributional semantics
dents.                                                                (Baroni & Lenci, 2010), we believed that the use of these
   Our purpose was not to design a new theory of common               techniques as a tool for estimating common ground was at
ground, but to estimate what atomic facts are likely to be in         least promising.
(communal) common ground, using a corpus-based method.
The method is based on metrics that use the frequency of in-          Measures of association
formation in a corpus to predict how widely known the in-             Below, we list some of the main metrics that have been
formation is. If it is successful, the method could be used to        proposed for measuring the strength of association between
model different communities by studying different corpora.            words. These metrics assume that a context for the words has
In other words, we offer a parametrised model, that has a             been defined. The context is frequently defined as a limited
corpus (or, equivalently, a community) as its parameter.              number of words before or after the target word or a short
   We focus in this paper on reference to famous people, be-          frame such as a paragraph in which the target word occurs.
cause famous people are a prime instance of something peo-            These contexts are not suitable for our purpose, because a
ple actually have common knowledge about. The proposed                fact about a person can be mentioned further away from the
metrics will be used as one of a number of heuristics for se-         person’s name, especially if the name is pronominalised in
lecting the content of a description of a famous person.              consequent paragraphs. Instead, we will use an article as a
                                                                      context for our search. This can be, for example, a news arti-
             Estimating Common Ground                                 cle or a Wikipedia article.
The proposed heuristic simplistically hypothesises that facts         Frequency The simplest measure of association between a
that often co-occur in a document are more likely to be               person and a property (a fact about a person) is the frequency
known because of frequency and repetition effects on mem-             of occurrence of the name and the property together in a cor-
ory (Atkinson & Shiffrin, 1968). Additionally, if a fact oc-          pus. Taking a collection of documents as a corpus, frequency
curs frequently in a very large corpus (such as the world-wide        corresponds to the count of articles that contain the name and
web), which has many authors, then this implies that many             the property. This association is then the value of count(n, p)
people (i.e., many authors) know this fact. It seems plausible        where n stands for the name of an entity and p is the property
that people write about what they themselves know but, to             in question.
our knowledge, no one attempted to examine how much can               Conditional Probability A more sophisticated measure is
shallow corpus methods (i.e., methods not involving seman-            conditional probability calculated as (1) and (2), where (1)
tic analysis) tell us about mutual knowledge (Vanderschraaf           measures the probability of the name given a property, and
& Sillari, 2009).                                                     (2) measures the probability of a property given the name of
   Assessing mutual knowledge is of substantial interest, and         the person. While the former measure normalises the results
practical use, in its own right. So, how about common knowl-          by the frequency of the property, the later measure takes into
edge? If a fact occurs frequently in a a corpus, is this evidence     account how famous each person is.
for common knowledge (as opposed to just mutual knowl-
edge)? Clark (1996) essentially answered this question in the                                                  count(n, p)
affirmative. He suggested that instead of thinking directly in                   assoc prob (n, p) = P(n|p) =                     (1)
                                                                                                                 count(p)
terms of the knowledge of others, people use evidence as a
basis for common ground (as in our example of the Eiffel                                                       count(p, n)
Tower). Nickerson et al. (1987) and a series of studies per-                     assoc prob (p, n) = P(p|n) =                     (2)
                                                                                                                 count(n)
formed by R. Krauss and S. Fussell (Fussell & Krauss, 1991;
Krauss & Fussell, 1991) showed that people often use their            Pointwise Mutual Information (PMI) (Fano, 1961) is a
own knowledge to estimate what others know. Given that                measure that compares how often two events x and y occur to-
it would not make much sense for computers to use “their              gether. PMI exploits the fact that if two terms appear together
own” knowledge, an alternative source of knowledge has to             often their joint probability (P(n, p)) will be higher than if
                                                                  1835

they were independent (P(n)P(p)). The value of PMI is pos-             Another problem that comes with the usage of search en-
itive for terms that co-occur and negative otherwise.               gines is the fact that we do not know the number of searched
                                                                    documents. This is necessary for calculating the probabili-
                                        P(n, p)                     ties used by the PMI metric. We have chosen a large con-
                assocPMI (n, p) = log2                       (3)
                                       P(n)P(p)                     stant N = 1.0e12 for normalising the counts (the number of
                                                                    search results for the word the is about 25 billion on Google
   One problem with PMI is that infrequent words that only          and about 10 billion on AltaVista and Bing and 2.4 billion on
appear together achieve a disproportionately high score. In         Google custom search).
order for a property to be in common ground, it also has to
be frequently mentioned. To mitigate the problem, (Hodges,                                   Experiment
Yie, Reighart, & Boggess, 1996) suggest multiplying each            We performed an experiment to evaluate how well the differ-
PMI score by count(n, p). To reduce the big difference be-          ent heuristics perform. Given a person and a set of properties,
tween the numbers of documents and to take into consider-           our heuristics produce a set of hproperty : scorei pairs, where
ation the association as measured by PMI as opposed to the          the score for each property is calculated by one of the de-
mere count, we multiply the PMI scores by the square root           scribed measures of association. Our goal is to assign scores
of the count. The final formula used for calculating the asso-      so that they reflect the commonality of a particular property
ciation is given by (4). Our pilot experiment showed better         with regard to the name. This means that properties that are
results with the adjusted PMI metric and any subsequent ref-        often associated with a name (e.g., Isaac Newton was a physi-
erence to PMI refers to (4).                                        cist) should get a higher score than properties that are less
                                                                    frequently associated with the name (e.g., Isaac Newton was
                          p                      P(n, p)            the warden of the Royal Mint).
        assocPMI (n, p) =    count(n, p) ∗ log2              (4)       We used hearers’ individual knowledge to asses how well
                                                P(n)P(p)
                                                                    the proposed heuristics perform. More specifically, the par-
                                                                    ticipants viewed statements such as “Andy Warhol was Amer-
Search Engine as Corpus
                                                                    ican” and “Ernest Hemingway is the author of For whom the
How do we acquire the frequencies n and p in the metrics            bell tolls” and were asked to select one of the following state-
above? Turney (2001) successfully used the AltaVista search         ments: true, false or don’t know. Our hypothesis is that when
engine to measure association between words using a varia-          a metric assigned a property higher score, a higher pro-
tion of PMI called PMI-IR, where he used numbers of hits            portion of participants should give an affirmative answer
returned by the search engine instead of real corpus proba-         (i.e., state that the sentence involving the property is true).
bilities. The number of hits corresponds to the number of           The success of the metric is measured as a Spearman correla-
documents on the Internet that contain the search term. The         tion between the output of the metric and the percentages of
functionality of providing the number of hits is available from     affirmative answers assigned to the individual statements by
other search engines. Google does not respond to queries            the participants.
from programs other than web browsers but offers Google
Custom Search which allows programmers to achieve the               Heuristic Options and Pilot
same functionality upon registration. Note that each of the         Aside from the choice of metric, several other choices had to
search engines only searches a subset of all the documents          be made. The first choice was which search engine to use.
available on the Internet and these subsets can differ substan-     As we had no reason to believe that a particular search engine
tially. This is also the case for Google accessed from a web        will perform better than others, our pilot tested the metrics on
browser and from a Google Custom Search.                            the three major search engines: AltaVista (Yahoo), Bing and
   There has been a debate as to whether to use search engines      Google.
for research purposes (Kilgarriff, 2007; Pedersen, 2008). One          The second choice is what search terms to choose. Most
of the arguments against using search engines was that the          properties can be expressed as a combination of the attribute
queries are optimised by performing morphological adjust-           and a value extracted from sentences such as “Alfred Nobel
ments such as stemming and by looking up related words or           was born in Stockholm.” Choosing the value only would lead
synonyms. While these issues are pertinent to lexicography,         to a loss of information, because there would be no difference
they seem to be of less importance when it comes to estab-          between properties such as hbornIn : Stockholmi and hdiedIn
lishing an association between a person and a property. A           : Stockholmi (since in both cases we would only search for
property can be described by different words and so such op-        Stockholm). On the other hand, attributes such as actedIn can
timisations can in fact be very useful. There can be a problem      be expressed by many similar expressions (e.g., starred). In
with morphological changes to names but many search en-             such case, using both the attribute and the value might be too
gines also offer search for exact phrases that are not morpho-      restrictive. As only empirical testing can show which option
logically or semantically manipulated and so we can avoid           is better, we tested both. In the following tables, V stands
optimisations at places where they are undesirable. This is         for value only (e.g., “Stockholm”) and AV stands for attribute
usually achieved by embedding the searched string in quotes.        and value (e.g., “born in Stockholm”).
                                                                1836

   Thirdly, there is the question what to do with synonyms.           Historical Figures page 1 . Based on the pilot, we attempted
While sometimes it might help to let a metric count all syn-          to select the 10 in such a way that they varied maximally (i.e.,
onyms of a word, as people remember concepts rather than              spaced evenly) in terms of how well known they are. We cre-
exact words, sometimes we would prefer to look for an ex-             ated sentences of the appropriate form from facts concerning
act phrase. This is especially the case when the value of the         these people mentioned in Wikipedia and the BBC Histori-
property is a proper name. This means that we had the option          cal Figures page. We also added properties that did not hold
to quote the searched term to force the used search engine to         true of the person in question to keep our participants more
look for an exact match. Again, our pilot tested both options         focused and to make it less likely that a participant answered
(i.e., quoting and no quoting).                                       true to each statement without using their knowledge. Only
   The choices described above left us with a large number of         the true statements were used in the analysis. The false state-
combinations. To minimise the likelihood of type II errors,           ments were used as a measure of participant’s effort. Partici-
we first performed a pilot experiment. The pilot uses a differ-       pants who answered true to more than 4 false statements were
ent set of stimuli than the real experiment. Based on the pilot,      discarded. We used 7 true properties and 5 false (control)
we then selected the most promising combinations. The setup           properties for each person. This resulted in total of 120 state-
and the procedure used in the pilot experiment were similar           ments. To make the task shorter, the statements were ordered
to the actual experiment (which is described in the following         alphabetically and then split into 5 groups of 24 statements
sections).                                                            (14 true, 10 false, 2 or 3 properties of each person in a group).
                                                                      Participants were randomly assigned to judge the statements
                                                                      in one of the groups. Figure 1 shows the names that were
Table 1: Results of the pilot study: Spearman correlation be-         chosen for the evaluation and table 2 shows a sample of the
tween the heuristics and knowledge of hearers.                        properties that were judged by the participants along with the
        SE + Opt      Frequency    P(n | p) P(p | n)  PMI             percentage of agreement answers.
        AltaVista V      0.27       0.25     0.30     0.32
        Bing V           0.25       0.20     0.26     0.29            •  Admiral Nelson                   •  Ernest Hemingway
        Google V         0.47       0.14     0.37     0.51            •  Alfred Nobel                     •  Florence Nightingale
        Google AV        0.60       0.23     0.50     0.64            •  Andy Warhol                      •  Heinrich Himmler
                                                                      •  Duke of Wellington               •  Louis Pasteur
   Table 1 shows the Spearman correlations between the re-            •  Emperor Hirohito                 •  Plato
sults of the individual metrics (unquoted option) and people’s
judgement. The options with quoted properties proved less
useful so our final evaluation used unquoted properties. The           Figure 1: Famous people used in the evaluation experiment.
best results were achieved by using Google and expressing
properties as attribute and value. Field (2009) treats values         Procedure
around 0.1 as indicating small effects, values around 0.3 as
                                                                      In order to find a large number of participants, the experi-
medium effects and values around 0.5 as large effects. This
                                                                      ment was conducted online using the Amazon Mechanical
standard terminology gives our PMI and Frequency based
                                                                      Turk (MTurk). The use of MTurk can have some draw-
metrics a large (positive) correlation, and our P(p | n) metric a
                                                                      backs, because it lets participants work from home, which
medium (positive) correlation. To validate our results, we se-
                                                                      makes it difficult to ensure that they are fully dedicated to
lected Frequency, P(p | n) and PMI and evaluated them on
                                                                      the task; even worse, computer programs have occasionally
a different set of properties using the Google search engine
                                                                      been known to perform the task (instead of real people). Re-
(table 1 shows the relevant numbers from the pilot study).
                                                                      sponses collected during the pilot experiment showed a large
The results of the final evaluation can be found in the section
                                                                      variability in the participants’ effort, the amount of time taken
Results and Discussion.
                                                                      to complete the experiment, and a large proportion of par-
Participants                                                          ticipants from non-English speaking countries. To mitigate
                                                                      some of these problems, to ensure a reasonable level of pro-
71 English speakers participated in our main experiment. 5
                                                                      ficiency in English, and to avoid automatic responses gen-
participants were discarded because they have not finished the
                                                                      erated by computers, participants had to successfully pass a
experiment and further 5 participants were removed because
                                                                      cloze test which amounted to a very strict test of their En-
the number of errors they made was more than 4 (mean +
                                                                      glish proficiency (Stubbs & Tucker, 1974). (Only native or
2 * std. dev). The total number of participants was 61; 30
                                                                      highly fluent speakers tend to pass.) Furthermore, the final
females, 29 males and 2 unspecified.
                                                                      evaluation was advertised only to the US and UK popula-
Materials                                                             tion of the MTurk. In this way, we focussed on a particu-
                                                                      lar cultural-linguistic community; the choice seemed natural
Ten people were selected for the experiment, each of whom
was famous enough that their names occurred on the BBC                    1 http://www.bbc.co.uk/history/historic        figures/
                                                                  1837

Table 2: List of properties of Ernest Hemingway, corresponding condition and the percentage of affirmative answers. Rank AV
and Rank V show how the corresponding properties ranked according to the PMI metric using Google with unquoted properties.
                   Property                                                          Condition   Percentage   Rank AV   Rank V
                   Ernest Hemingway was a writer.                                        true      100.0         1         2
                   Ernest Hemingway was American.                                        true      100.0         2         1
                   Ernest Hemingway received the Nobel Prize in Literature.              true       63.6         4         4
                   Ernest Hemingway is the author of For whom the bell tolls.            true       54.5         3         3
                   Ernest Hemingway committed a suicide.                                 true       50.0         6         5
                   Ernest Hemingway was British.                                        false       27.3         -         -
                   Ernest Hemingway was born in Oak Park.                                true       25.0         5         6
                   Ernest Hemingway received the Italian Silver Medal of Bravery.        true       20.0         7         7
                   Ernest Hemingway is the author of A tale of two cities.              false       13.3         -         -
                   Ernest Hemingway invented dynamite.                                  false        0.0         -         -
                   Ernest Hemingway died in a plane crash.                              false        0.0         -         -
                   Ernest Hemingway was born in Paris.                                  false        0.0         -         -
given that the searched pages must have been written in En-
                                                                              Table 3: Spearman correlation between the heuristics and the
glish in order to contain the searched terms. The inclusion of
                                                                              knowledge of hearers. All correlations were significant at p
the pre-requisites (cloze test and country restrictions) greatly
                                                                              < 0.001.
improved the results (e.g., less variation in the time taken to
complete the experiment and fewer number of participants                                    Option       Frequency    P(p | n) PMI
who made errors).                                                                           Google AV       0.639      0.437   0.664
   The first page showed the instructions on how to answer                                  Google V        0.632      0.475   0.662
and how to navigate the website and also urged the partic-
ipants to rely on their own knowledge and avoid using the
Internet to answer the questions. The participants were then                  P(p | n). This suggests that a heuristic for common ground
asked to fill in some information such as sex, age group and                  that employs either the Frequency or the PMI metric, to large
interests. The participants then viewed one statement at a                    extent, agrees with the knowledge of general public.
time and were asked to select one of the three provided op-                      The presented heuristic seems to work relatively well with
tions (true, don’t know, false). The participants could also                  the kinds of facts that appear in natural language generation
provide a comment for each statement. After finishing the                     systems. A natural question is whether the heuristic can give
experiment they were given an opportunity to provide addi-                    good results for facts that are so widely known that they are
tional open comments.                                                         not explicitly stated. There are two kinds of these facts.
   The search engine queries were performed over December                        The first kind are facts such as “a person has a stomach.”
2011 and January 2012. To ensure replicability of the experi-                 While it seems improbable that such a fact would be explic-
ment, we saved all the queries and the corresponding numbers                  itly mentioned in a corpus, there are ways of implying it by
of hits returned by the search engines. These files are avail-                statements such as “a person can get a stomach flu” or “a per-
able on our website 2 .                                                       son can increase the risk of getting stomach cancer...” These
Results and Discussion                                                        statements include the words “person” and “stomach” and the
                                                                              heuristic can pick up these words without doing any semantic
Table 2 contains a sample of statements that were shown to                    analysis. We tested the heuristic with a few of such state-
the participants. Condition true means that it was a true state-              ments and it seems to place them in to the well known part
ment and false means it was a false (control) statement. As                   of common ground (i.e., assigns high scores to such facts). A
previously mentioned, only the true statements were used in                   proper evaluation would be needed to confirm this trend.
the analysis. The percentages of affirmative answers were                        The second kind of facts are facts such as “Einstein had a
correlated with the output of the metrics using Spearman cor-                 stomach.” This kind of facts requires inference, e.g. Einstein
relation. All calculations were performed using the R statis-                 is a person and people have a stomach therefore Einstein had
tical package (R Development Core Team, 2010).                                a stomach. As our heuristic works on the surface level of the
   Table 3 shows the final results of our experiment. We used                 text, it will not produce the expected results for such facts.
the Google search engine and tested expressing properties as
attribute and value (condition AV) and as value only (condi-                                  Conclusion and further work
tion V). The properties were unquoted in both cases.
   Our results show a large positive correlation between the                  We set out to find a computation estimation of common
PMI and the Frequency based metrics, and the knowledge                        ground, starting with mutual knowledge (in the sense of
of people and a medium positive correlation achieved by the                   Vanderschraaf and Sillari (2009)). We hypothesised that stan-
                                                                              dard co-occurrence measures could be used as an approx-
    2 http://www.abdn.ac.uk/                                                  imation of a solution to the problem and tested several of
                                ˜r04rk9/cge.zip
                                                                         1838

these measures against the knowledge of people in a par-            Fussell, S. R., & Krauss, R. M. (1991). Accuracy and bias
ticular community (cf. Clark and Marshall (1981)), as ac-             in estimates of others’ knowledge. European Journal of
quired in a new experiment with human participants. We                Social Psychology, 21(5), 445–454.
consider these results to be highly encouraging. They sug-          Hodges, J., Yie, S., Reighart, R., & Boggess, L. (1996). An
gest that the proposed heuristic (based on either Frequency or        automated system that assists in the generation of docu-
PMI, combined with Google search) are on the right track, at          ment indexes. Natural Language Engineering, 2(02), 137-
least in terms of estimating how widely known an atomic fact          160.
is (i.e., mutual knowledge); in section Estimating Common           Jucks, R., Becker, B.-M., & Bromme, R. (2008). Lexi-
Ground we argued that this also makes it plausible that these         cal entrainment in written discourse: Is experts’ word use
heuristics could offer a reasonable approximation of common           adapted to the addressee? Discourse Processes, 45(6), 497-
knowledge (i.e., the facts of which everyone in the commu-            518.
nity knows that everyone in the community knows them), but          Jurafsky, D. (2003). Probabilistic modeling in psycholinguis-
this was not directly investigated.                                   tics: Linguistic comprehension and production. In Proba-
   The community investigated in our experiment was com-              bilistic linguistics (pp. 39–96). MIT Press.
prised of native speakers of English in the UK and the USA          Kilgarriff, A. (2007, March). Googleology is bad science.
with access to a computer. We believe it would be interesting         Comput. Linguist., 33, 147–151.
to test to what degree different communities be modelled by         Krahmer, E., & van Deemter, K. (2012, March). Compu-
different knowledge sources.                                          tational Generation of Referring Expressions: A Survey.
   Our current work focuses on combining the metrics in-              Computational Linguistics, 38(1), 173-218.
vestigated here with other heuristics (including a discrimina-      Krauss, R. M., & Fussell, S. R. (1991). Perspective-taking in
tory power heuristic for assessing the usefulness of a fact) to       communication: Representations of others’ knowledge in
improve content selection algorithms for Natural Language             reference. Social Cognition, 9(1), 2–24.
Generation (e.g., Reiter and Dale (2000)).                          Lewis, D. K. (1969). Convention: A philosophical study.
                                                                      Cambridge, Massachusetts: Harvard University Press.
                    Acknowledgements                                Nickerson, R. S., Baddeley, A., & Freeman, B. (1987). Are
We would like to thank the members of the Natural Lan-                people’s estimates of what other people know influenced
guage Generation group of the University of Aberdeen and              by what they themselves know? Acta Psychologica, 64(3),
the anonymous reviewers for their valuable comments. This             245 - 259.
research is sponsored by the Scottish Informatics and Com-          Pedersen, T. (2008, September). Empiricism is not a matter
puter Science Alliance (SICSA).                                       of faith. Comput. Linguist., 34, 465–470.
                                                                    R Development Core Team. (2010). R: A language and
                          References                                  environment for statistical computing [Computer software
Atkinson, R., & Shiffrin, R. (1968). Human memory: A pro-             manual]. Vienna, Austria. (ISBN 3-900051-07-0)
   posed system and its control processes. In K. Spence &           Reiter, E., & Dale, R. (2000). Building natural language
   J. Spence (Eds.), The psychology of learning and motiva-           generation systems. New York, NY, USA: Cambridge Uni-
   tion: Advances in research and theory (Vol. 2, pp. 89–195).        versity Press.
   Academic Press, New York.                                        Riordan, B., & Jones, M. (2011). Redundancy in percep-
Baroni, M., & Lenci, A. (2010). Distributional memory: A              tual and linguistic experience: Comparing feature-based
   general framework for corpus-based semantics. Comput.              and distributional models of semantic representation. Top-
   Linguist., 36(4), 673–721.                                         ics in Cognitive Science, 3(2), 303–345.
Clark, H. H. (1996). Using language. New York: Cambridge            Strawson, P. (1952). Introduction to logical theory.
   University Press.                                                Stubbs, J. B., & Tucker, G. R. (1974). The cloze test as
Clark, H. H., & Marshall, C. (1981). Definite reference               a measure of english proficiency. The Modern Language
   and mutual knowledge. In A. K. Joshi, B. L. Webber, &              Journal, 58(5/6), pp. 239-241.
   I. A. Sag (Eds.), Elements of discourse understanding (pp.       Turney, P. (2001). Mining the web for synonyms: PMI-IR
   10–63). New York: Cambridge University Press.                      versus LSA on TOEFL. In Proceedings of the twelfth eu-
Fano, R. M. (1961). Transmission of information: A statisti-          ropean conference on machine learning (ecml-2001).
   cal theory of communications. New York: Wiley.                   Vanderschraaf, P., & Sillari, G. (2009). Common knowledge.
Field, A. (2009). Discovering statistics using spss. SAGE             In E. N. Zalta (Ed.), The stanford encyclopedia of philos-
   publications Ltd.                                                  ophy (Spring 2009 ed.). http://plato.stanford.edu/
Firth, J. R. (1957). A synopsis of linguistic theory 1930-1955.       archives/spr2009/entries/common-knowledge/.
   In In studies in linguistic analysis. Blackwell.                 Van Eijck, J. (1993). The dynamics of description. Journal
Frege, G. (1892 (1952)). On sense and reference. In                   of Semantics, 10(3), 239-267.
   P. T. Geach & M. Black (Eds.), Translations from the
   Philosophical Writings of Gottlob Frege. Oxford: Basil
   Blackwell.
                                                                1839

