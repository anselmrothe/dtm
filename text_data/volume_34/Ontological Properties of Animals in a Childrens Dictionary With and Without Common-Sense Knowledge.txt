UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Ontological Properties of Animals in a Children’s Dictionary With and Without Common-
Sense Knowledge
Permalink
https://escholarship.org/uc/item/4pr7b749
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Taylor, Julia
Raskin, Victor
Hempelmann, Christian
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                     University of California

    Ontological Properties of Animals in a Children’s Dictionary With and Without
                                                  Common-Sense Knowledge
                                               Julia M. Taylor (jtaylor@purdue.edu)
                                Computer and Information Technology & CERIAS, Purdue University
                                         401 N. Grant Street, W. Lafayette, IN 47907-2021 USA
                                                Victor Raskin (vraskin@purdue.edu)
                                              Linguistics & CERIAS, Purdue University
                                           656 Oval Drive, W. Lafayette, IN 47907-2086 USA
                                      Christian F. Hempelmann (chempelm@purdue.edu)
                                             Linguistics and CERIAS, Purdue University
                                           656 Oval Drive, W. Lafayette, IN 47907-2086 USA
                              Abstract                                 live in water, have tails, can swim well) and are thus
   The paper applies a limited version of the resources of
                                                                       inherited, the remaining knowledge necessary to understand
   Ontological Semantic Technology to the descriptions of              such definitions remains implicit, because it is presumed to
   animals in the American Heritage First Dictionary and               be common-sense of different kinds. There is, for example,
   constructs a partial ontology from them. The explicitly             no mention of a fish’s gills or fins.
   mentioned properties in the descriptions are then                      Capturing common-sense knowledge is a daunting task.
   supplemented by common-sense knowledge that the                     We are assuming that descriptions of the (animal) world of
   descriptions assume available to their young readership, and        this dictionary requires less common-sense knowledge
   the output is compared to the previous one. The results, albeit
   modest, shed some interesting light on the most similar and         simply because this world is more restricted than that of an
   dissimilar pairs of animals, as described in text.                  adult and the dictionary was obviously designed to
                                                                       accommodate that. If that is so, then getting a grasp of that
   Keywords: common-sense knowledge; Ontological Semantic              knowledge may be more feasible than in case of a common,
   Technology; children’s dictionary; animal dataset; similarity.
                                                                       unlimited, adult-level natural language communication1.
                                                                           The goal is, then, to illustrate how much information is
                          Introduction                                 lost when common-sense knowledge is not made explicit.
   The paper explores the common-sense knowledge that is               Using the methods of computational semantics, specifically
necessary to fully understand the definitions/descriptions             our Ontological Semantic Technology, we are taking
(henceforth, just descriptions) of around 100 animals in the           advantage of the unique design of the dictionary to identify
2007 edition of the American Heritage First Dictionary                 the required common-sense knowledge for a reasonably full
(AHFD 2007) aimed at children in grades K-2 (ages 5-8). It             comprehension of its animal descriptions. In this way, we
does not address common-sense reasoning. The purpose is                aim to get a sense of its common-sense knowledge
to get a grasp on how implicit information affects the                 dependency. As a result, we also hope to clarify some issues
structure of perceived knowledge, in this case of animal               concerning the very nature of common-sense knowledge
descriptions, and similarity among entities.                           and the feasibility of its computational acquisition and use,
   The AHFD contains about 2,000 entries, claimed, almost              which is, as a matter of act, our primary and real concern.
entirely correctly, to be written with a controlled vocabulary             In Section 1, we introduce the notions of ‘hard’ and ‘soft’
so that that every description contains only words that also           common-sense knowledge and explore its relation to
have entries in the dictionary. What is challenging in this            underdetermination of reality by language and to saliency
design for our task is the possible implication of self-               and, then, to ontology and natural language meaning,
sufficiency, that is, of a much reduced dependency on the              contingency, and instantiation.. In Section 2, we will briefly
child’s knowledge of the world, unstated explicitly in the             survey pertinent prior work. Section 3 will sketch out the
natural language descriptions but (unconsciously) assumed              Ontological Semantic Technology, our research tool as we
to be present for full comprehension.                                  applied it to the material. Section 4 compares the worldview
   Most AFHD definitions for animals follow the “genus                 on the animals that the descriptions define with the one
proximum, differentia specifica” format that is common for             complemented by the common-sense knowledge necessary
dictionaries, in particular for the classification of animal           to understand them. Section 5 discusses the results,
species: “Goldfish is a kind of fish [genus]. Goldfish are
usually small and orange [differentia].” Apart from the
                                                                          1
differentiating specifics (small, orange) and the few                       The distinction between children as “novices” who know less
properties that are specifics for the genus (in the AHFD, fish         about many domains than “expert” adults is well established (e.g.,
                                                                       Carey 1985)-for better or worse.
                                                                   2393

identifies the strengths and weaknesses of our approach, and         the situation that will remain unmentioned. If two men walk
discusses the future lines of research.                              into the room, a report of that may include what they look
                                                                     like, what they wear, the speed of their movement, etc. But
        Kinds of Common-Sense Knowledge                              it will mention nothing about their places of birth, parents’
                                                                     names and occupations, what cars they drive, what they had
Hard and Soft Common-Sense Knowledge                                 for breakfast, etc.
                                                                        Now, all that knowledge exists, and common-sense
We are introducing this new pair of terms to differentiate
                                                                     knowledge includes that these people have a birth place,
between two kinds of common-sense knowledge that a
                                                                     have parents, likely drive cars (especially if they’re
reader of the AHFD must possess to fully comprehend a
                                                                     Americans), etc. What is essential, however, is that most of
description. If that reader does not understand a word in the
                                                                     the existing but implicit information is not prominent: much
description and that word has its own AHFD entry, he or
                                                                     more likely, the prominence goes with the purpose of those
she may access the required knowledge from that entry,
                                                                     people’s entrance into the room, whether there is any cause
where it is explicitly stated. So, from the point of the initial
                                                                     for alarm or displeasure, etc. The amount of prominent, or
entry, this information is implied but from the point of the
                                                                     salient common- sense knowledge is much more limited in
dictionary, it is explicitly stated. We call this information
                                                                     any situation.
the ‘soft common-sense knowledge.’ If, on the contrary,
                                                                        Unfortunately, saliency (see Giora 2003: 13-38 and
some information that is needed for a full comprehension of
                                                                     references there) is dynamic and fluctuates very rapidly. In
the entry is not stated explicitly anywhere in the dictionary,
                                                                     AHFD, however, saliency may be conveniently seen as
we refer to it as the ‘hard common-sense knowledge.’ The
                                                                     deliberately delimited by the availability of entries for
paper focuses on the latter.
                                                                     words, thus reflecting the compilers’ notion of the mental
   Starting with the randomly selected AHFD description of
                                                                     model for a five-year-old’s world.
snake, we line up (see graph in http://web.ics.purdue.edu
/~vraskin/snake_new_label.pdf) the lexical chains under-             Instantiation and contingency
lying the entry dependency: if entry E(x) uses word y in the
description of word x, and y is not previously mentioned in          In Ontological Semantic Technology (OST), the ontology
the chain, E(x) leads to E(y). If E(i) does not evoke any new        consists of concepts and relations between them that are
words in its description, it becomes a terminal in the               determined by properties. The concepts anchor lexical
dependency line. The longest, 10-node dependency line                senses that are defined in the separate lexicon. Thus, one
holds, starting with the topmost leftmost node and ending            sense of the word cat is anchored in the ontological concept
with the rightmost node at the bottom of the picture: SNAKE          CAT. In a sentence, A cat can jump from the floor to the top
is-a REPTILE is-a ANIMAL is-a-not PLANT agent-of MAKE has-           of a bookcase, CAT is what the word cat means, i.e., a
agent BEE agent-of FILL result-in FULL precondition-of-not           generic, any member of the class.
HOLD unspecified ROOM. What this perfectly representative               In the sentence, Kisa the cat can jump from the floor to
branch illustrates is that there is no consistent or predictable     the top of a six-foot tall bookcase, however, it is no longer a
semantic dependency in the chain and that the vagaries of            generic cat, but a specific instance of the concept, and the
lexicographic connection can traverse the domain of                  relationship between the meaning of the word cat and the
knowledge, common-sense and other, in all directions, with           ontological concept CAT is no longer that of generic
some connections not easily explained.                               anchoring. This instantiation makes the sentence contingent
   Altogether, the knowledge required to understand every            on a number of indices, such as the identities of the speaker
word in the description of snake as well as every word in the        and hearer, time, place, etc. (see Lewis 1972—cf. Bar
descriptions of those words, and, in turn, every word in the         Hillel’s 1954 comment on rare non-contingent sentences,
descriptions of those words, and so on to the end of the             such as, Ice floats on water).
chain, is expressed in 86 entries. Realistically speaking, no           We understand common-sense knowledge as non-
5-year-old will read all the entries: much more likely, they         contingent and involving concepts, not their instances. It is
will have the requisite knowledge of the words.                      about what exists in the world, not what we know about
Nevertheless, this information is made available by the              particular objects or events. Our common-sense knowledge
AHFD compilers, perhaps similarly to the glosses,                    includes the fact that houses may be painted in various
footnotes, and explanatory appendices in adult-level                 colors; it does not include the fact that Tom’s house is grey
materials. Its availability makes it not quite common-sense          with burgundy trim.
knowledge—so we refer to this explicit, but remote                      So the common-sense knowledge left implicit by the
information, as weak common-sense knowledge.                         AHFD is strong, non-contingent, and definitely less salient
                                                                     than the knowledge explicitly supplied by the AHFD in its
Underdetermination and Saliency                                      descriptions.
It is known that language underdetermines reality (see, for
instance, Barwise and Perry 1983; Nirenburg and Raskin
2004): no matter how fine-grained or verbose the
description of an event, there will be tons of details about
                                                                 2394

     Prior Pertinent Work on Common-Sense                          as missing, for example, size classes necessary to
                         Knowledge                                 understand spatial relations between physical objects, such
                                                                   as the understanding that a containing object should have
Distinguishing common-sense knowledge from other                   greater dimensions than the (solid) object it contains.
implicit types of knowledge has been an issue in approaches           In contrast to previous work, which addressed the
to knowledge engineering, and while it always is a central         identification and acquisition of common-sense knowledge
one, it often remains implicit. Knowledge-based NLP has            by OST for the general purpose of processing text, this
(re-)matured enough both to be able to need as well as to          paper applies an appropriately limited version of our
accommodate the type of “deep” knowledge that overlaps             resources to a very limited corpus of a specific genre in an
with the varying notions of common sense.                          attempt to compare the ontological information following
   McCarthy (1959) is often cited as the earliest mention of       from the AHFD descriptions only with the ontological
common sense in the literature, but Bar-Hillel’s (1954) well-      information arising from the descriptions supplemented by
known example, “Little John played in his pen,” is already a       the common-sense knowledge that the descriptions imply in
clear indication of the necessity and importance of the            their readership.
common-sense knowledge—in this case, about relative sizes
of objects.                                                                       Brief Introduction to OST
   Prominently, Lenat (1990) started an early large-scale
systematic project on acquisition of common-sense                  Charniak’s (1972) often (mis)cited children’s story is used
knowledge, CyC. His method was hand-coding by a large              primarily to discuss inferencing and, hence, reasoning. It is
number of research engineers, with a high turnaround and           even more suitable for exemplifying (in square brackets) the
no well-defined acquisition methodology, which affected            most common common-sense knowledge that OST has to
results and rendered them unusable for the NLP community.          deal with in order to fulfill its function of representing the
   Gordon and Schubert’s overview (2010) classifies current        meaning of natural language text accurately and
approaches to common-sense knowledge acquisition as:               comprehensively.
hand-authoring of rules, as in CyC; abstracting from clusters         Jane was invited to Jack’s birthday party. [One brings
of propositions (e.g., Van Durme 2009); and directly               presents to a birthday party. Presents are often purchased.
interpreting general statements, such as glosses in                To purchase something, one needs money.] She wondered if
dictionaries (e.g., Clark et al. 2008), akin to the approach of    he would like a kite. She went into her room and shook her
the present paper. Other researchers have used tagging,            piggy bank. [Piggy banks contains money, usually coins.
annotating, and/or generic machine learning techniques for         Coins make noise when shaken] It made no sound. [Coins
automatically extracting implied common-sense knowledge            make noise.] (either there was no money in the piggy
from explicit text on the Web, about which Lin et al. (2004)       bank or just no coins but rather bills in the former case,
have legitimate reservations, because explicit statements on       Jane may have lacked the money to buy the present)].
the Web do not necessarily express common-sense                       The italicized part is the original story; our formulation of
knowledge.                                                         the common-sense knowledge is in square brackets; the
   Finally, we need to mention the area of research on             parenthesized part following the first arrow represents our
common sense dedicated to children’s development of such           formulation of inferences in reasoning, and while definitely
knowledge, not least related to their overall linguistic-          pertinent to common-sense knowledge, it will be left out of
cognitive development. In particular, children’s knowledge         this paper. It is noteworthy that the reasoning statements are
about animals is one of the applications. Results that inform      contingent on the story while common-sense knowledge is
our present approach include that children focus on external       generic.
features rather than internal organs, on habitats, on behavior        The first and essential function of OST is to interpret the
relevant for humans (dangerous, edible) rather than cladistic      text of the story. The OST processor reads each sentence
accuracy (“Is a camel an ungulate?”), and that children’s          linearly and looks it up, word by word, in the OST English
knowledge is derived from observations as much as                  lexicon. Every sense of every (non-auxiliary, non-
instructions, parents, or media (see Prokop et al. (2007),         parametric) word in the lexicon is anchored in an
Tunnicliffe et al. (2007), Byrne et al. (2010)).                   ontological concept, with its properties and fillers, and the
   In our own previous work (Taylor et al. 2011a), we              fillers can be restricted by the sense. The OST ontology,
include in the common-sense knowledge rules of a separate          unlike its lexicons, is language-independent (see Nirenburg
resource the knowledge-of-the-world information that is not        and Raskin 2004 for the basic theory of Ontological
already contained in the ontology and lexicon (see next            Semantics, and Raskin et al. 2010, Hempelmann et al. 2010,
section). in the experiment there, we processed text with our      Taylor and Raskin 2011, Taylor et al. 2010, 2011a,b, for the
system, and as part of routine quality assurance, added the        much revised OST).
necessary common-sense knowledge wherever we failed to                To use a greatly simplified example, the sense of the
interpret the text correctly because of the unavailability of      English word invite will be anchored in the ontological
this information in our resources (after we have excluded          concept, probably also labeled “INVITE.” The label does not
other, more banal reasons for the failure, such as an error in     contain any but distinguishing information for the computer
the resources or a bug in the software). Thus, we identified
                                                               2395

and can be any ASCII combination—it is there just for the           Ontology of Descriptions and Ontology With
convenience of the human acquirer.                                                   Common-Sense Knowledge
   INVITE
          is-a               communicative event                 In general, we are interested not only in reading and
          agent              human                               understanding a text, but also in structuring information that
          beneficiary        human                               this text contains, as well as enhancing our ontology when
          theme              social-gathering                    newly acquired information requires. We are using
          purpose            entertainment                       information about animals from AHFD to see whether such
   invite                                                        task is possible.                We then check whether supplying
          Invite                                                 additional information (common-sense knowledge left
                   agent              [preceding NP]             implicit in the dictionary) would help with the task (cf.
                   beneficiary         [following NP]            Perfors et al. 2005; Kemp et al. 2006).
                   theme              [to NP]                        Typically, a hierarchy is perceived as one of the most
                   …                                             important properties in ontology construction. All animal
   And the text meaning representation (TMR) of the first        descriptions of the dictionary provide such information.
sentence of the story will result from matching the meaning      Unfortunately, sometimes a word is used that may have
of the NPs in the appropriate EVENT slots. The reality is, of    multiple senses (such as cat being a domestic cat or feline)
course, harder, with more complex syntax, ambiguity, etc.        thus creating a flawed hierarchy. One of the goals, then, is
The unenhanced-OST problem with the story is still more          to identify such descriptions.
advanced: while TMR for each sentence is not hard to                 The proposed measure is conditional on an accepted
produce, the system will not be able to relate the sentences     membership assumption. If we assume the veracity of “B is
to each other, and the text will lack cohesiveness.              A” as a reference point, which gives us a certain amount of
                                                                 knowledge about B in terms of its properties, we estimate
                                                                 the extent to which “C is B” is (dis)confirmed as
                                                                  #2   "n
                                                                          * w * hierPi (C ,B )
                                                                   i                           where w is a property weight, and
                                                                           num(i)
                                                                                  & 1, P.b " D B & P.c " D C & b = c
                                                                                  (
                                                                                  (#1, P.b " D B & P.c " D C & b $ c
                                                                  hierPi (C,B ) = '
                                                        !                         (       0.1, P " D C & P % D B
                                                                                  ()              0, otherwise
                                                                     Placement in the hierarchy as well as concepts’ properties
                                                                 (may) affect similarity between concepts. For the purposes
                                                        !        of this paper, we assumed that the properties that are taken
                  Figure 1: OST Architecture                     into account are all equally weighted. We measure similarity
   In OST, the information processed prior to computing the                                       # 2"n * w * simP (A,B )
TMR of the current sentence is used to clarify, complement,
                                                                                                                   i
                                                                 of two concepts as i                                     where simPi(A,B) is
and disambiguate the current representation process. In this                                              num(i)
case, that information would be helpless and useless             defined as:
because, other than Jane as the agent, no previous sentence                          %          1, P.a " D A & P.b " D B & a = b
in the story even mentions objects in the following                                  '
                                                                                     '         -1,  P.a " D A & P.b " D B & a # b
sentences, and it is the common objects (or events) that the         simPi (A,B ) ! =&
                                                                                     '  0.1,   P  " D A & P $ D B || P $ D A & P " D B
anaphora/coreference resolution establishes as bridges                               '(                    0, otherwise
between and among sentences. Jane will emerge from the
story, as interpreted by the unenhanced OST, as performing
three unrelated actions. It is the common-sense knowledge                                Results and Conclusion
statements in the square brackets above that have to provide
                                                           !         We first wanted to see what kind of structure we would
such common objects to make OST processing possible: the         get from the descriptions without the use of common sense.
bridge words are underlined in the story above., and the         We calculated pair-wise similarity measurements for all
common-sense knowledge enhanced text can be processed            animals with AHFD descriptions. The similarities ranged
by OST normally.                                                 from -1.25 to 0.78. It is possible for the similarity to be –N
   This is why we recently added to the OST architecture         where N is the number of properties in both descriptions and
(Figure 1 above) the common-sense knowledge resource             all properties in the descriptions match but their fillers do
(Taylor et al. 2011a) and the methodology of adding to it        not. Having calculated the mean and standard deviation, we
when the TMRs fall short of the (often hypothetical) gold        looked at the results that were at least 3 standard deviations
standard (cf. Allen et al. 2008). .                              away from the mean as most similar cases and most
                                                                 dissimilar ones. The dissimilar pairs were: ant/chicken, ant/
                                                                 crocodile, ant/pony, ant/whale, bee/chicken, beetle/chicken,
                                                             2396

bug/chicken, bug/shark, bug/whale, butterfly/chicken,                  descriptions but omitted from others, with clearly implied
caterpillar/chicken,     caterpillar/crow,      caterpillar/whale,     values, so we added that information directly to the
chicken/cricket, chicken/fly, chicken/mosquito, chicken/               ontology as it emerged from the descriptions. The addition
moth, chicken/whale, cricket/whale, crocodile/whale,                   to common-sense knowledge solved the hierarchy problem
mosquito/whale, moth/shark, moth/whale, turtle/whale.                  of animal in the previous experiment not being an animal,
   It should be noted that, with the exception of the                  and did not introduce any additional problems.
chicken/whale, turtle/whale, and crocodile/whale pairs, the               The distribution of the resulting similarity is shown in
dissimilar pairs contain insects. One member of the pair is            Figure 2. As seen there, listing results that are 3 standard
(typically) a bird or a mammal that is somehow different               deviations away from the mean proved to be impractical,
from the rest of its class, thus deserves an explicit                  although that was done in the first experiment,. Thus, the
clarification, such as a whale being a mammal. For some                results below reflect the same number of dissimilar pairs as
reason, insects also received a fairly large amount of                 the first experiment: ape/duck, ape/swan, duck/snail,
description and thus were easy to contrast with other                  crocodile/snail, chicken/snail, crow/snail, alligator/snail,
animals.                                                               ape/goose,       beaver/snail,     eagle/snail,    hawk/snail,
   The similar pairs are: ape/monkey, bear/panda, bee/moth,            goose/monkey, fox/snail, hamster/snail, duck/monkey,
beetle/butterfly, beetle/cricket, beetle/fly, bug/caterpillar,         ape/snail, deer/swan, ape/crab, goose/snail, camel/snail,
butterfly/cricket, butterfly/fly, camel/giraffe, caterpillar/          jellyfish/monkey, ape/penguin, bear/snail, goat/snail. As
cricket, caterpillar/moth, cricket/fly, donkey/zebra, eagle/           with previous results, there is a concept that is most
hawk, fox/wolf, goose/turkey, horse/pony, leopard/lion,                dissimilar to others (snail), and the dissimilarity looks
lion/tiger. Again, (an expected) a pattern can be noticed              plausible (all below 0).
here: those animals that received a lot of similar descriptions
are being selected.
    There were 7 animals or categories in the dictionary that
were used in the is-a relations other than to indicate an
offspring of an animal. These categories were: animal,
insect, bird, fish, reptile, cat, and horse. Mammal got an
entry in the dictionary but was not used in any of the
descriptions. We excluded entries that indicated a young
animal, such a kitten is a young cat. We calculated the mean
and standard deviation of each animal relative to the above                    Figure 2. Distribution of similarity of animals
7 categories using the hier metric described above. We                    The pairs that are most similar are again several insects
assumed that if an entry had a description that X is Y, and            and birds, as well as eagle/hawk, crab/lobster, hippo/
hier(X, Y) was lower than the mean for that overall                    rhinoceros, bull/cow, dolphin/whale, horse/pony, cow/
category, the definition should be questioned and should not           sheep, cow/pony, cow/sheep, frog/toad, pig/pony, lion/tiger,
be used for hierarchy construction. The following entries              mouse/rat, jellyfish/octopus, donkey/zebra, spider/worm.
were so affected: bat is-a animal, crab is-a animal, goat is-a         As expected, the similarity results look (more?) reasonable
                                                                       with common-sense knowledge (Figure 3).
animal, hippo is-a animal, sheep is-a animal, whale is-a
animal, ostrich is-a bird, tiger is-a cat, lion is-a cat.
    There are several explanations for the results: cat is
defined as a domestic animal, and thus, of course, cannot be
a parent of wild animals. Crab has more features that puts it
next to fish, and so does whale, including the description of
the habitat. Hippo is mostly described swimming in lakes
and rivers. Bat is similar in its description to a bird. Goat
and sheep created a puzzle for us. However, we considered
it to be a success to have only 2 problematic entries.
   Interestingly, contradicting the dictionary, the metric
suggests that donkey and zebra should be types of horse;                         Figure 3: Pair-wise similarity of 7 animals.
dog and hamster should be types of cat. These entries
                                                                          While the most similar and least similar results look good,
suggest that there is not enough differentiation between the
                                                                       the middle section will need to be improved. Figure 4 shows
affected animals for them to be correctly classified.
                                                                       pair-wise comparisons of perceived similarity between cow,
   We therefore wanted to see if the ratio changes when the
                                                                       dolphin, elephant, horse, mouse, rhino, seal, and squirrel.
omitted common-sense knowledge is added to the
descriptions and if some puzzling results are corrected. The           We anticipate these results to improve when weights are
common-sense knowledge consisted of a number of                        added to properties.
additional animal properties, explicitly stated in some
                                                                   2397

                          Conclusion                               Lenat, D. B. (1990) CYC: Toward programs with common
                                                                     sense, Communications of the ACM 33:8, 30–49.
   We have demonstrated, on a very limited corpus of
                                                                   Lewis, D. (1972). General semantics. In Davidson, D. and
animal descriptions intended for a very young audience, that
                                                                     Harman, G. (Eds.), Semantics of Natural Language,
it is possible to detect semantic structure in natural language
                                                                     Dordrecht - Boston: Reidel.
descriptions as well as pointing to flawed descriptions. The
                                                                   Liu, H., & Singh, P. (2004). ConceptNet—a practical
results improve with the addition of common-sense
                                                                     commonsense reasoning tool-kit, BT Technology Journal,
knowledge omitted from but implied by the descriptions.
                                                                     22:4. 211-226.
The specific material, from a children’s dictionary that was
                                                                   McCarthy, J. (1959). Programs with common sense.
designed to limit the amount of world knowledge that the
                                                                     Proceedings of the Teddington Conference on the
young reader could be counted on contributing, helped us
                                                                     Mechanization of Thought Processes, 75–91. London:
delimit the common-sense knowledge. It is clear, of course,
                                                                     Her Majesty’s Stationary Office.
that this method of defining this elusive resource is not
                                                                   Nirenburg, S., & Raskin, V. 2004. Ontological Semantics.
useful outside of this artificially restricted environment but
                                                                     Cambridge, MA: MIT Press
the convenient handle to it was too tempting to resist.
                                                                   Opfer, J. E. & Siegler, R. S. (2004). Revisiting
                                                                     preschoolers’ living things concept: A microgenetic
                          References                                 analysis of of conceptual change in basic biology.
AHFD (2007). The American Heritage First Dictionary.                 Cognitive Psychology 49, 301-332.
   Boston-New York: Houghton Mifflin.                              Osherson, D. N., Wilkie, O., Smith, E. E., López, A., and
Allen, J. F., Swift, M., & de Beaumont, W. (2008). Deep              Shafir, E.1990. Category-based induction. Psychological
   semantic analysis of text. Proc. Step ’08. Venice.                Review, vol. 97, no. 2, 185-200.
Bar-Hillel, Y. (1954). Indexical expressions. Mind 63, 359-        Perfors, A., Kemp, C., Tenenbaum, J. B. 2005. Modeling
   379. Reprinted in his Aspects of Language, Jerusalem:             the acquisition of domain structure and feature
   Magnes, 1970, 69-88.                                              understanding. Proc. CogSci-05.
Barwise, J., & Perry J. (1983). Situations and Attitudes.          Prokop, P., Prokop, M., Dale-Tunnicliffe, S., & Diran, C.
   Cambridge, MA: MIT Press-Bradford.                                (2007). Children’s ideas of animals’ internal structures.
Byrne, J., Dale‐Tunnicliffe, S., Patrick, T., & Grace, M.            Journal of Biological Education, 41-2, 62-67.
   (2010). Children's understanding of animals in their            Raskin, V., Hempelmann, C. F., & Taylor , J. M. (2010).
   everyday life in the UK and USA. Proceedings of the 7th           Guessing vs. knowing: The two approaches to semantics
   Conference of European Researchers in Didactics of                in natural language processing, Annual International
   Biology ERIDOB.                                                   Conference Dialogue 2010, 642-650, Bekasovo
Carey, S. (1985). Conceptual Change in Childhood.                    (Moscow), Russia.
   Cambridge, MA: MIT Press.                                       Taylor, J. M., & Raskin, V. (2011). Understanding the
Charniak, E. 1972. Toward a Model of Children's Story                unknown: Unattested input processing in natural
   Comprehension. Artificial Intelligence Technical Report           language, Proc. FUZZ-IEEE-11, Taipei, Taiwan.
   Number 266, Department of Computer Science,                     Taylor, J. M., Hempelmann, C. F., & Raskin, V. (2010a).
   Massachusettes Institute of Technology, Cambridge, MA.            On an automatic acquisition toolbox for ontologies and
Clark, P., Fellbaum, C., & Hobbs, J. (2008). Using and               lexicons in Ontological Semantics, International
   extending WordNet to support question answering.                  Conference on Artificial Intelligence, Las Vegas, NE.
   Proceedings of the 4th Global WordNet Conference, 111-          Taylor, J. M., Raskin, V. & Hempelmann, C. F. (2011a).
   119.                                                              From disambiguation failures to common-sense
Giora, R. (2003). On Our Mind: Salience, Context, and                knowledge acquisition: A day in the life of an ontological
   Figurative Language. New York: Oxford University                  semantic system. In Gilof, R., (Ed.), Proceedings of the
   Press.                                                            Web Intelligence and Intelligent Agent Technology
Gordon, J. M., & Schubert L. K.. (2010). “Quantificational           Conference. IEEE.
   Sharpening of Commonsense Knowledge” , in: Havasi et            Taylor, J. M., Raskin, V., & Hempelmann, C. F. (2011b).
   al., 27-32.                                                       Towards computational guessing of unknown word
Havasi, C., Lenat, D. B., & Van Durme, B. (Eds). (2010).             meanings: The Ontological Semantic approach, Proc of
   Commonsense Knowledge: Papers from the AAAI Fall                  CogSci-11, Boston, MA.
   Symposium. Menlo Park, CA: AAAI Press, 2010.                    Tunnicliffe S. D., Boulter, C., & Reiss, M. (2007). Pigeon –
Hempelmann, C. F., Taylor, J. M., & Raskin, V. (2010).               friend or foe? Children’s understanding of an everyday
   Application-guided Ontological Engineering. Proceedings           animal. Proceedings of the British Educational Research
   of the International Conference on Artificial Intelligence,       Association Annual Conference.
   Las Vegas, NE.                                                  Van Durme, B., Michalak, P., & Schubert, L.K. (2009).
Kemp, C., Tanenbaum, J. B., Griffiths, T. L., Yamada, T,             Deriving Generalized Knowledge from Corpora Using
   and Ueda, N. 2006. Learning systems of concepts with an           WordNet Abstraction. Proceedings of EACL, 808-816.
   infinite relational model. Proc AAAI-06.
                                                               2398

