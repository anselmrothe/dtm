UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
One piece at a time: Learning complex rules through self-directed sampling
Permalink
https://escholarship.org/uc/item/79c5793c
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Markant, Doug
Gureckis, Todd
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

          One piece at a time: Learning complex rules through self-directed sampling
                                                Doug Markant (doug.markant@nyu.edu)
                                              Todd M. Gureckis (todd.gureckis@nyu.edu)
                                                               New York University
                                                            Department of Psychology
                                               6 Washington Place, New York, NY 10003 USA
                                 Abstract
                                                                          How do people make information sampling decisions?
   Self-directed information sampling—the ability to collect in-
   formation that one expects to be useful—has been shown to              In light of evidence that self-directed sampling can speed
   improve the eﬃciency of concept acquisition for both human             learning, it is important to understand how people decide
   and machine learners. However, little is known about how peo-          what data to collect. Given a potential observation, what in-
   ple decide which information is worth learning about. In this
   study, we examine self-directed learning in a relatively complex       formation do people rely on to decide if it will be useful?
   rule learning task that gave participants the ability to “design          One aspect that may help explain a person’s decision to
   and test” stimuli they wanted to learn about. On a subset of           sample an item is their uncertainty in how to classify it (or
   trials we recorded participants’ uncertainty about how to clas-
   sify the item they had just designed. Analyses of these uncer-         more generally, their uncertainty about the outcome of any
   tainty judgments show that people prefer gathering informa-            test performed on the item). Intuitively, a self-directed learner
   tion about items that help re ne one rule at a time (i.e., those       should direct their attention to items that are high in uncer-
   that fall close to a pairwise category “margin”) rather than items
   that have the highest overall uncertainty across all relevant hy-      tainty while ignoring items that can already be con dently
   potheses or rules. Our results give new insight into how people        classi ed or predicted. Consistent with this strategy, the pat-
   gather information to test currently entertained hypotheses in         tern of stimuli sampled by self-directed learners in our pre-
   complex problem solving tasks.
   Keywords: self-directed learning, categorization, active learn-        vious study (see Figure 1) revealed that participants system-
   ing, information search, rule learning                                 atically directed their samples toward the category boundary
                                                                          as the task progressed. Intuitively, the learner is mostly likely
                             Introduction                                 to be uncertain about these items (e.g., most of the errors in
A cornerstone of many educational philosophies is that people             classi cation occur near the category boundary).
learn more eﬀectively when they direct or control their own                  In the current study, we examine how subjective uncer-
learning experiences (Bruner, 1961). While there are many                 tainty in how to classify an item can be used to predict whether
ways that control might in uence learning, an important fac-              or not it is sampled. We begin by presenting three psycholog-
tor is the ability to actively gather information that one con-           ically motivated proposals for how sampling decisions relate
siders potentially useful while avoiding data that is poten-              to judgments of uncertainty, and then test these models in a
tially redundant, a behavior referred to as self-directed sam-            new experiment that extends the “self-directed” classi cation
pling (Gureckis & Markant, in revision).                                  learning paradigm used in Markant and Gureckis (2010). Our
   One recent study directly examined the interaction of                  results highlight the need for models of sampling behavior that
self-directed information sampling and learning (Markant &                go beyond monolithic measures of information value to con-
Gureckis, 2010, in revision). In this study, people attempted             sider how people collect and use data during the sequential
to learn simple dichotomous categories of objects that varied             learning of concepts.
along two perceptual dimensions (circles that diﬀered in size
and the orientation of a central line segment, see Figure 1).                   ree models for relating uncertainty and
In contrast to traditional categorization training procedures,                         information sampling decisions
we allowed participants to “design” stimuli that they wanted              e following sections lay out three possible ways in which
to learn more about on each trial. Like a child asking their              uncertainty might guide information sampling decisions.
parent to label an unfamiliar object, self-directed “designing”
or “sampling” allows the learner to focus on information they             Model 1: Sampling to reduce global uncertainty
want rather than be limited by the ow of passive experience.              Prior work on how people gather information has oen fo-
   e major nding from this study was that for simple uni-                cused on diagnostic reasoning problems in which the learner
dimensional rules, self-directed learners acquired the correct            is given a set of alternatives (e.g., diﬀerent diseases) and asked
category rule faster than “passive” participants who were pro-            to sample observable features (e.g., symptoms) in order to
vided samples from an experimenter-de ned distribution. In                identify the true diagnosis (Nelson, McKenzie, Cottrell, & Se-
addition, self-directed learners out-performed a set of “yoked”           jnowski, 2010; Skov & Sherman, 1986; Trope & Bassok, 1982).
learners who viewed the same examples but did not get to                  From a computational perspective, various authors have pro-
make information sampling decisions themselves (consistent                posed sampling norms that attempt to predict information
with studies of causal learning with similar yoked compar-                sampling decisions based on a learner’s representation of the
isons, Lagnado and Sloman, 2004; Sobel and Kushnir, 2006).                task (Nelson, 2005; Nelson et al., 2010; Oaksford & Chater,
                                                                      725

                                                                                                        Stimuli sampled by
             A                                                                             B          self-directed participants
                                                                                                           (by training block)
                  Orientation
                                A B
                                                                                                  1          2         3           4
                                                     “Adjust the antenna and
                                                      click the mouse button
                                Size                    to learn the station”
                                                                                                 5           6         7           8
Figure 1: A: Abstract stimulus space used in Markant and Gureckis (2010, in review) and which is adapted for the current study. Stimuli were
circles which varied in size and orientation of a central diagonal. In Markant and Gureckis, these objects were assigned to one of two categories
(“A” or “B”). Participants “designed” a stimulus they wanted to learn about using the mouse. Clicking the mouse button reveals the category
membership of the item. B: e pattern of sampling behavior observed by self-directed learners in Markant and Gureckis (2010) across eight
training blocks. Each dot represents a single stimulus which was selected by a participant. In the rst block, participants distributed their
samples widely over the entire stimulus space but then gradually focused their choices on the region surrounding the category boundary.
                                                                                tain, might be to decompose a complex task into a series of
1994). Much of this work has focused on what we will call                       simpler problems. For example, when multiple features may
“prospective” models (e.g., probability gain, information gain,                 be related to an outcome, a learner might choose to hold one
etc.) that estimate the expected drop in uncertainty that will                  feature constant while varying the other across multiple sam-
result from making an observation (taking into account all                      ples (Rottman & Keil, 2012). Such a strategy is related to the
possible outcomes). While in many contexts these models                         “control of variables” strategy which is essential to scienti c
make similar predictions, Nelson et al. (2010) designed a di-                   reasoning. Isolating variables oen helps people to more ef-
agnostic reasoning task which found that participants’ choices                    ciently search the space of potential hypotheses (Klahr &
were best t by probability gain, which values a potential ob-                   Dunbar, 1988) and is a key part of “learning to learn” about
servation according to how much it increases the chance of                      complex concepts (Kuhn & Dean, 2005). In an experiment
classifying an item correctly.                                                  similar to that of Markant and Gureckis (2010), Avrahami et
   In the context of learning a classi cation rule, this approach               al. (1997) had participants choose samples to teach a partner
is consistent with a preference for sampling items that the                     about a single-dimensional rule and found that the “teach-
learner is least certain about how to classify. Assume that for                 ers” frequently used a strategy of isolating individual features.
a given stimulus x = (f1 ...fd ) with d observable features the                 Moreover, their students learned better from this strategy than
learner represents the probability that x is a member of each                   when given items that were closest to the category boundary.
possible category y in the distribution P (y∣x). We can then                       We can formalize the strategy of focusing on separate com-
de ne the least certain measure as:                                             ponents in a sampling model that values uncertainty about
                                                                                any individual boundary between only two categories. La-
                   LC(x) = 1 − max(P (y∣x))                        (1)          bel margin predicts that the learner will prefer instances for
for all stimuli x. Note that there are alternative norms that                   which the likelihood of any two categories is similar, inde-
make similar predictions to least certain, such as using the                    pendent of any other categories. For example, when there
Shannon entropy of the marginal distribution to calculate un-                   are three categories and the marginal distribution is de ned
certainty (see Settles, 2009 for a review). Regardless of the par-              as P (y∣x) = (p1 , p2 , p3 ):
ticular form, the important property of this approach is that
the most valuable observation is always an item that is con-                          LM (x) = 1 − min[∣p1 − p2 ∣, ∣p1 − p3 ∣, ∣p2 − p3 ∣]   (2)
sidered equally likely to belong to all possible categories. In
general, choosing items which maximize LC(x) should con-                        Critically, label margin does not preferentially select items for
vey the greatest amount of information to the learner.                          which the learner is globally uncertain. Instead, by this ap-
                                                                                proach, a learning problem is decomposed into simpler prob-
Model 2: Isolating individual rule components
                                                                                lems and items are selected which are expected to resolve un-
through margin sampling                                                         certainty about the sub-components.
While focusing on items that are the most “globally” uncer-
tain or unpredictable seems intuitively useful, there is reason                 Model 3: Seeking con rmation
to expect that it may not be the sampling strategy humans                       While the previous two models propose that people search for
use, particularly when learning in complex, multivariate en-                    uncertain items, previous work on hypothesis testing suggests
vironments. One natural strategy, not captured by least cer-                    that people may prefer items that they already know how to
                                                                         726

                  Category Feedback      Space of probability
                   (Stimulus space)              judgments                             Least Certain              Label Margin            Most Certain
      Binary             A   B         A                                     B    A                      B  A                    B   A                       B
   Classification
                    f2                (1.0, 0.0)      (0.5, 0.5)      (0.0, 1.0)
                        f1
                                                          B                                  B                         B                        B
      Ternary             B                                         (0, 0.5, 0.5)
   Classification             A
                     f2   C                         (0.3, 0.3, 0.3)
                        f1
                                       A  (1, 0, 0)                          C    A                      C  A                    C   A                       C
Figure 2: Comparing predictions of sampling norms (red = more highly valued choices, blue = less valued choices) Top: For a binary classi -
cation problem, a new observation in stimulus space will correspond to a location on the probability judgment scale, where the lemost point
re ects con dence that the observation will be classi ed “A” and the rightmost point re ects con dence it will be classi ed “B”. For the binary
problem, the predictions of least certain and label margin are identical. Bottom: In a ternary classi cation problem, an item in stimulus space
will correspond to a location in the 3-category simplex depending on the learner’s uncertainty. Here the predictions of least certain and label
margin diverge, allowing us to test which of the two models better account for sampling behavior.
classify. For example, people have a well-documented bias to-                              category is highly unlikely but the learner is uncertain about
ward seeking positive evidence of the hypothesis they are con-                             the other two (shown in Figure 2 by the high predicted value
sidering (Klayman & Ha, 1989; Wason, 1960), a strategy that                                along the radial axes of the simplex, including the midpoints
in certain conditions is aligned with the goals of maximizing                              of each edge). In short, this model predicts that samples are
uncertainty reduction (Austerweil & Griﬃths, 2011; Navarro                                 likely to be allocated close to any boundary (i.e., “margin”) be-
& Perfors, 2011; Nelson & Movellan, 2001). To quantify this                                tween two categories. In contrast, least certain predicts sam-
strategy, we de ne the most certain measure as:                                            pling close to the junction of the category boundaries, where
                                                                                           all three classes are likely.
                       M C(x) = max(P (y∣x))                                       (3)
                                                                                           Overview of the current study
e predictions of this model directly contrast those of least                              e design of our experiment capitalizes on the distinction
certain, with the highest value assigned to items that can al-                             described in the previous section by extending the paradigm
ready be classi ed with con dence. One may also think of the                               used in Markant and Gureckis (2010) to a ternary classi ca-
most certain measure as instantiating con rmation bias—it                                  tion problem. In the experiment participants collect data by
shows a preference for items for which the learner has a strong                            sampling new instances and receiving feedback about their
prediction about the category label.                                                       category membership. As shown above, using the ternary
                                                                                           classi cation problem allows us to separate the predictions of
Empirically distinguishing these alternatives                                              the three sampling models, two of which were confounded
Given these various approaches, a key question is if they are                              in our previous design. In order to obtain an estimate of the
distinguishable based on empirical data. e predictions of                                 learner’s uncertainty at any point in time, on a subset of sam-
each model are shown for a binary classi cation problem (like                              pling trials participants report how likely they believe the in-
the task used in Markant and Gureckis, 2010) in the top row of                             stance they created will belong to each of the three categories
Figure 2. Each heatmap describes the value assigned to a po-                               (before receiving feedback). e goal of our analysis is to use
tential observation depending on the learner’s uncertainty in                              these subjective judgments to test which model provides the
how to classify it. For example, an item that can be con dently                            best account of their sampling decisions.
classi ed (e.g., p(y∣x) = (1, 0)) would be assigned a high value
by most certain and a low value by least certain. Note that for                                                        Experiment
the binary classi cation problem, least certain and label mar-                             Participants Fiy-seven undergraduates at New York University
gin make identical predictions about how items will be val-                                participated in the study for course credit. e experiment was run
                                                                                           on standard Macintosh computers in a single 1-hour session.
ued (i.e., items close to the center of the space are preferred),
making it impossible to separately test these models. How-                                 Stimuli Stimuli were de ned by a two-dimensional continuous-
                                                                                           valued feature space corresponding to the size (radius) of a circle and
ever, an interesting observation made by Settles (2009) is that                            the angle of a central diameter. ese feature values were mapped
the predictions of these models strongly diverge when consid-                              to a limited range of orientations and sizes on the display. Orienta-
ering more complex categorization tasks. For example, in a                                 tion could vary over only  degrees, ensuring that a full rotation of
                                                                                           the stimulus was not possible. e two halves of the central diame-
ternary classi cation task (see bottom row of Figure 2), label                             ter were given diﬀerent colors, further reducing the perceptual sim-
margin assigns the maximum value to any items for which one                                ilarity of stimuli at the two extremes of the orientation dimension.
                                                                                       727

         Probability judgment
                                                                          Probability judgments. Half of the training trials in each block
                                                                          were randomly selected to include probability judgments. On
                                                                          these trials, aer participants had designed an antenna but before
                                                                          the category label was shown, they judged the likelihood that the
                                                                          antenna would receive each of the three channels using a sequence
                                                                          of rating scales (shown in Figure 3). e three scales were presented
                                                                          independently such that only one was visible at a time. When
                                                                          each scale appeared, the participant clicked on a location in the
                                                                          scale according to their belief that the antenna they had designed
                                                                          would receive that channel. A response was required for each scale,
                                                                          and there was no time limit for entering the response. e initial
                                                                          position of the mouse cursor within each scale was randomized,
                                                                          allowing us to record whether responses were in uenced by the
         Feedback
                                                                          starting position. Aer probability judgments were recorded,
                                                                          they were displayed alongside the category label for the same
                                                                          duration as in regular training trials. is allowed the participant to
                                                                          evaluate the accuracy of their prediction given the true category label.
                                                                          Test Trials. Each block of  training trials was followed by  test trials.
                                                                          On each test trial, a single item was presented in the center of the dis-
                                                                          play and the participant classi ed the item according to the channel
                                                                          they believed it was most likely to receive. A response was required to
                                                                          complete the trial, and participants responded at their own pace. No
                                                                          feedback was provided on individual test trials. At the end of each
                                                CH1                       block participants were told their accuracy during the block they just
                                                                          completed, as well as the number of consecutive correct responses.
                                                                          Results
Figure 3: Top: Probability judgments were entered by clicking on
a scale for each of the three categories (CH, CH, and CH). Bot-        ree participants were excluded from analysis for failing to
tom: Probability judgments were displayed alongside the category          complete the task, leaving N = 54. irty-one people reached
label during feedback.                                                    the goal of correctly classifying  items in a row. However,
                                                                          there were a number of additional people who achieved sim-
                                                                          ilarly high rates of accuracy. For each subject we computed a
                                                                          moving average of their classi cation accuracy with a window
e minimum radius and orientation were randomized so that the
boundary between the categories corresponded to a unique bound-           of  blocks, and found  people for whom this average ex-
ary in perceptual space for each participant. A total of 256 stimuli      ceeded 83% at any point in the experiment (i.e., they correctly
were sampled from a uniform grid over the feature space and used as       classi ed  of  items within any three consecutive blocks).
test items for all participants, presented in random order.
   e category label associated with each stimulus was deterministi-      Probability judgments. On half of participants’ sampling
cally de ned by a conjunctive, ternary classi cation rule of the form     trials they judged how likely the stimulus they selected be-
shown in Figure 2. In addition to the structure that is shown, three
more rules were created through diﬀerent rotations (, , and       longed to each of the three categories, resulting in three val-
degrees) of the same boundaries in stimulus space. Each participant       ues between  and . In order to verify that participants were
was randomly assigned to one of the four rules.                           not simply responding based on the position of the cursor,
Procedure Participants were instructed that the stimuli in the            for each rating we measured the diﬀerence between the initial
experiment were television “loop antennas” and that each unique           (random) position and the participant’s response. One partic-
antenna received one of three channels (CH, CH, or CH). eir
goal was to learn the diﬀerence between the three types of antennas       ipant was excluded from further analysis because the major-
so that they could correctly classify new antennas during the test        ity of their ratings (82%) did not change by more than 0.01%
blocks. Participants were told that the experiment would end when         from the initial values (for the remaining subjects, the aver-
they correctly classi ed  consecutive test items. If a participant
failed to reach that goal the experiment ended aer  blocks or at       age proportion of samples that met the same condition was
the end of an hour (whichever occurred rst).                              M = 0.04, SD = 0.05).
Training Trials. Participants “designed” antennas by adjusting the        Fitting the alternative sampling models. Our rst goal was
size and orientation and receiving feedback about which channel was       to assess the overall t of the three sampling models to each
received. ey were instructed that they should design antennas they       participant’s full set of probability judgments. For each model
thought were useful and that would help them to predict the TV
channel for other designs they had not yet tested.                        we computed the normalizing constant necessary to de ne the
   Each trial began with the presentation of a randomly generated         probability density function. Each triplet of ratings was nor-
antenna. Participants then adjusted the size and orientation by           malized so that they summed to one. We then calculated the
moving the mouse from le to right while holding either the ‘Z’ or
‘X’ key, respectively. Only one dimension could be changed at a           log-likelihood of each judgment made by a participant and
time, but participants could make any number of changes and were          summed across all trials to get an overall score for each model.
self-paced. When the stimulus was the desired size and orientation,          Classifying participants according to the model with the
they pressed the mouse button to reveal the channel received,
displayed above the stimulus for 4 seconds.                               highest log-likelihood, we found that 3 people were best- t by
                                                                          least certain, 25 people were best- t by label margin, and the
                                                                      728

 A                            Probability judgments                                      B      Overall proportion of samples by region
                                (by best fit model)
                                                                                                                               0.6                   fast
                                                                                                                               0.5                   slow
                                                                                                                               0.4
                                                                                                                  Proportion
                                                                                                                               0.3
                                                                                                  Least certain                0.2
                                                                                                  Label margin                 0.1
                                                                                                  Most certain                 0.0
                                                                                                                                     LC    LM   MC
      least certain                  label margin                   most certain
                                                                                                                                          Region
     (3 participants)              (25 participants)              (25 participants)
Figure 4: A: Probability judgments are plotted using the 3-category simplex for participants best- t by each of the three models (see Figure 2
for reference). Each point represents a single judgment aer normalization. B: Each judgment was classi ed according to the model assigning
it the highest likelihood, eﬀectively dividing the probability space into three regions. Participants were divided into two groups based on the
number blocks they required to complete the task (“fast” and “slow”), and the relative frequency of sampling in each region is shown at right.
remaining 25 people were best- t by most certain. Judgments                Discussion
made by participants, separated by the best- tting model, are
                                                                           eories of rational information acquisition propose that the
plotted using the 3-category simplex in Figure 4A, with each
                                                                           decision to make an observation is related to the amount of
point a single sample chosen by a participant. A higher density
                                                                           information it conveys (Nelson, 2005; Oaksford & Chater,
of points re ects an increased tendency (as a group) to sam-
                                                                           1994). Sampling norms such as probability gain prospectively
ple stimuli in a given region of probability space. Upon visual
                                                                           evaluate the change in uncertainty that is expected to occur
inspection, the overall pattern for each group corresponds to
                                                                           following an observation, and a rational learner should choose
the predictions of the best- tting model (Figure 2).
                                                                           the data that maximizes that measure. Our results illustrate
Relating sampling decisions to learning eﬃciency. We next                  the relative inadequacy of these models when applied to even
tested whether a participant’s overall success at learning the             a basic rule learning task. Very few of our participants were
target concept was related to the sampling behavior re ected               best t assuming they preferentially selected observations they
in their probability judgments. Our approach was to divide                 were least certain about. In addition, the heterogeneity of par-
participants into two groups based on the number of blocks                 ticipants’ sampling strategies is a noteworthy nding. For ex-
they required to complete the task. We performed a median                  ample, about 20% of samples in the rst 4 blocks were “con r-
split on the number of blocks (median = 16) to create a group              matory” (i.e., data that the learner could already classify with
of “fast” (N = 26) and “slow” (N = 27) learners. With re-                  relative con dence), and overall there was no diﬀerence in
spect to overall model ts, however, there was no diﬀerence in              the frequency of this kind of sampling between fast and slow
the proportion of participants best t by each model between                learners. Con rmatory sampling could serve a number of
groups (fast learners: NLC = 1, NLM = 12, NM C = 13; slow                  purposes, including helping to organize the representation of
learners: NLC = 2, NLM = 13, NM C = 12).                                   a rule in mind (Mathy & Feldman, 2009) or to facilitate com-
   While overall model ts provide a measure of each partic-                parisons between successive observations, but further work is
ipant’s sampling behavior in general, inspection of the data               required to understand its exact role in this task.
showed that most subjects had relatively mixed strategies. For
                                                                           Margin sampling vs. information maximization A second
example, a participant best- t by most certain may have made
                                                                           way in which participants’ behavior diverged from the “ra-
a number of judgments consistent with label margin. Given
                                                                           tional” prediction was their preference for samples that fell
this heterogeneity, we classi ed individual probability judg-
                                                                           along the category margins over items that oﬀered informa-
ments according to the model that assigned it the highest like-
                                                                           tion about all three categories (i.e., those located at the junc-
lihood, eﬀectively dividing the probability space into three re-
                                                                           tion of the boundaries). From the perspective of an ideal ob-
gions corresponding to each model (Figure 4B). Multinomial
                                                                           server (i.e., a model that can represent the full set of possible
logistic regression was used to test for diﬀerences between the
                                                                           hypotheses and use Bayesian inference to update its beliefs),
relative frequency with which fast and slow learners sampled
                                                                           the most eﬃcient strategy is to maximize the amount of infor-
in each of the three regions. Overall frequency diﬀered signif-
                                                                           mation contained in each observation. Sampling at the cate-
icantly between the two groups (χ2 = 24.7, df = 2, p < .001).
                                                                           gory margins should only decrease the eﬃciency of learning
Post-hoc tests showed that fast learners sampled somewhat
                                                                           since it will tend to rule out a smaller number of hypotheses,
more frequently in the label margin region (t(51) = 1.68,
                                                                           which raises the question of why this kind of behavior was so
p = .09) and less frequently in the least certain region over-
                                                                           common in our task.
all (t(51) = −1.91, p = 0.06), suggesting that this pattern of
                                                                              In our discussion we motivated the margin sampling model
sampling was related to success in the task.
                                                                           by noting that people might decompose a complex prob-
                                                                     729

lem into simpler pieces. e use of such a strategy may re-                any copyright annotation thereon. e views and conclusions con-
  ect a participant’s limited ability to simultaneously represent         tained herein are those of the authors and should not be interpreted
                                                                          as necessarily representing the oﬃcial policies or endorsements, ei-
all possible alternatives and to remember prior observations.             ther expressed or implied, of IARPA, DOI, or the U.S. Government.
us, margin sampling may re ect an adaptation whereby
people isolate individual components to learn in succession.                                           References
Separately testing the role of diﬀerent features is an important          Austerweil, J., & Griﬃths, T. (2011). Seeking Con rmation Is Ratio-
part of scienti c thinking in general (Klahr & Dunbar, 1988;                      nal for Deterministic Hypotheses. Cognitive Science, 35, 499–
                                                                                  526.
Kuhn & Dean, 2005), particularly when intervention is nec-                Avrahami, J, Kareev, Y, Bogot, Y, Caspi, R, Dunaevsky, S, & Lerner,
essary to remove the eﬀect of confounding variables. Impor-                       S. (1997). Teaching by examples: Implications for the process
tantly, our results do not reveal the particular cognitive pro-                   of category acquisition. e Quarterly Journal of Experimental
                                                                                  Psychology Section A, 50(3), 586–606.
cesses underlying participants’ decisions, but merely provide             Bruner, J. (1961). e act of discovery. Harvard Educational Review,
a descriptive account of their overall behavior. Nonetheless,                     31(1), 21–32.
they provide a useful constraint for theories of information              Gureckis, T., & Markant, D. (in revision). A cognitive and compu-
                                                                                  tational perspective on self-directed learning. Perspectives in
sampling, particularly when applied to more complex tasks                         Psychological Science.
that involve sequential learning and memory demands.                      Klahr, D, & Dunbar, K. (1988). Dual space search during scienti c
                                                                                  reasoning. Cognitive Science, 12, 1–48.
Measuring subjective uncertainty. It is important to con-                 Klayman, J, & Ha, Y. (1989). Hypothesis testing in rule discovery:
sider that the probability judgments we collected provide an                      strategy, structure, and content. Journal of Experimental Psy-
                                                                                  chology: Learning, 15(4), 596–604.
incomplete picture of participants’ uncertainty over the course           Kuhn, D., & Dean, D. (2005). Is developing scienti c thinking all
of the task. Although we found some evidence that fast and                        about learning to control variables? Psychological Science,
slow learners diﬀered in the kinds of samples they collected,                     16(11), 866.
                                                                          Lagnado, D. A., & Sloman, S. (2004). e advantage of timely inter-
because uncertainty judgments were assessed on only half                          vention. Journal of Experimental Psychology: Learning, Mem-
of sampling trials it is diﬃcult to draw strong conclusions                       ory, and Cognition, 30(4), 856–876.
about the impact of those samples on classi cation accuracy.              Markant, D., & Gureckis, T. (2010). Category Learning rough Ac-
                                                                                  tive Sampling. In S. Ohlsson & R. Catrambone (Eds.). Austin,
Moreover, we cannot be sure that the judgments reported by                        TX: Cognitive Science Society.
participants accurately re ected their subjective belief since            Markant, D., & Gureckis, T. (in revision). e impact of self-directed
there were no costs for failing to report accurately1 . ese is-                  information sampling on learning. Journal of Experimental
                                                                                  Psychology: General.
sues arise whenever considering sampling models based on a                Mathy, F., & Feldman, J. (2009). A rule-based presentation order
learner’s subjective uncertainty rather than objective measures                   facilitates category learning. Psychonomic bulletin & review,
of value such as information gain, and as such present an im-                     16(6), 1050–1057.
                                                                          Navarro, D., & Perfors, A. (2011). Hypothesis generation, sparse cat-
portant challenge to be addressed in future work.                                 egories, and the positive test strategy. Psychological review,
                                                                                  118(1), 120.
                             Conclusion                                   Nelson, J. (2005). Finding useful questions: On Bayesian diagnostic-
                                                                                  ity, probability, impact, and information gain. Psychological
Past accounts of information sampling have suggested that a                       Review, 114(3), 677.
single normative model might account for people’s decisions               Nelson, J, & Movellan, J. (2001). Active inference in concept learning.
                                                                                  Advances in Neural Information Processing Systems, 45–51.
across many learning problems, and that people tend to seek               Nelson, J., McKenzie, C., Cottrell, G., & Sejnowski, T. (2010). Expe-
out data that lead to the greatest reduction in uncertainty. In                   rience Matters: Information acquisition optimizes probability
contrast, we found little evidence of a single sampling norm                      gain. Psychological Science, 21(7), 960.
                                                                          Oaksford, M, & Chater, N. (1994). A rational analysis of the selection
that was consistently applied across individuals. Instead, par-                   task as optimal data selection. Psychological Review, 101(4),
ticipants’ sampling choices seem to re ect ongoing aspects of                     608–630.
constructive problem solving. Our approach highlights the                 Rottman, B., & Keil, F. (2012). Causal structure learning over time:
                                                                                  observations and interventions. Cognitive Psychology, 64(1),
need for theories of self-directed learning to move beyond in-                    93–125.
dividual measures of information value to capture interactions            Settles, B. (2009). Active Learning Literature Survey (tech. rep.
with task demands and cognitive constraints.                                      No. 1648).
                                                                          Skov, R, & Sherman, S. (1986). Information-gathering processes:
                                                                                  diagnosticity, hypothesis-con rmatory strategies, and per-
                       Acknowledgments                                            ceived hypothesis con rmation. Journal of Experimental So-
e authors wish to thank the reviewers for their helpful comments.                cial Psychology, 22(2), 93–121.
is work was supported by the Intelligence Advanced Research              Sobel, D., & Kushnir, T. (2006). e importance of decision making
Projects Activity (IARPA) via Department of the Interior (DOI) con-               in causal learning from interventions. Memory and Cognition,
tract D10PC20023. e U.S. Government is authorized to reproduce                   34(2), 411.
and distribute reprints for Governmental purposes notwithstanding         Trope, Y, & Bassok, M. (1982). Con rmatory and diagnosing strate-
                                                                                  gies in social information gathering. Journal of Personality and
    1                                                                             Social Psychology, 43(1), 22–34.
      Two aspects of our procedure may lessen the impact of this con-     Wason, P. (1960). On the failure to eliminate hypotheses in a concep-
cern. First, we were able to measure a failure to respond by randomly             tual task. e Quarterly Journal of Experimental Psychology,
initializing the cursor position before each rating, and found that               12(3), 129–140.
people changed the position in about 95% of ratings. Second, because
judgments were displayed alongside the category label feedback, par-
ticipants may have been encouraged to respond accurately in order
to facilitate processing of that feedback
                                                                      730

