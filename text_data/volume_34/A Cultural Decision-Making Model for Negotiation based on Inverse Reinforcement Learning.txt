UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Cultural Decision-Making Model for Negotiation based on Inverse Reinforcement Learning
Permalink
https://escholarship.org/uc/item/5gh1p4d5
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Nouri, Elnaz
Georgila, Kallirroi
Traum, David
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                             A Cultural Decision-Making Model for Negotiation
                                     based on Inverse Reinforcement Learning
                                                    Elnaz Nouri (nouri@ict.usc.edu)
                                             Kallirroi Georgila (kgeorgila@ict.usc.edu)
                                                   David Traum (traum@ict.usc.edu)
                                  Institute for Creative Technologies, University of Southern California
                                          12015 Waterfront Drive, Playa Vista, CA 90094, USA
                                Abstract                                 expected utility as the key to rationality. This, in effect,
                                                                         denies the third explanation above. For very simple games,
    We learn culture-specific weights for a multi-attribute model of
    decision-making in negotiation, using Inverse Reinforcement          where it is relatively easy to calculate the payoffs, the first
    Learning (IRL). The model takes into account multiple indi-          possibility seems hard to believe, thus we are left with the
    vidual and social factors for evaluating the available choices       hypothesis that differences in behavior are based on applying
    in a decision set, and attempts to account for observed be-
    havior differences across cultures by the different weights that     common utility principles to different problems. Others,
    members of those cultures place on each factor. We apply this        e.g. (Gal, Pfeffer, Marzo, & Grosz, 2004), have claimed
    model to the Ultimatum Game and show that weights learned            that there are many factors that contribute to the behavior of
    from IRL surpass both a simple baseline with random weights,
    and a high baseline considering only one factor of maximiz-          humans in social situations. This makes the third explanation
    ing gain in own wealth in accounting for the behavior of hu-         plausible, if people from different cultures have different
    man players from four different cultures. We also show that          relative weights for the different factors. But this leads to a
    the weights learned with our model for one culture outperform
    weights learned for other cultures when playing against oppo-        further question of how to determine those different weights.
    nents of the first culture. We conclude that decision-making         In (Nouri & Traum, 2011) we presented one such model
    in negotiation is a complex, culture-specific process that can-      of decision-making that culture-specific virtual agents were
    not be explained just by the notion of maximizing one’s own
    utility, but which can be learned using IRL techniques.              able to use to play the Ultimatum Game (see the following
    Keywords: cultural decision-making; negotiation; ultimatum           section) with each other or with people. The model used
    game; inverse reinforcement learning.                                Hofstede’s multi-dimensional model of culture (Hofstede,
                                                                         2001) to determine the relative weights of different factors.
                           Introduction                                  However, in that work the weights were set manually using
 Social scientists have often observed that people from dif-             our intuitions about how to apply the literature, which
 ferent cultures behave differently in interactive situations            involved a number of relatively arbitrary decisions.
 (Camerer, 2003; Roth, Prasnikar, Okuno-Fujiwara, & Zamir,                  In this paper we attempt to learn the weights using In-
 1991). There are several different possible explanations for            verse Reinforcement Learning (IRL) (Abbeel & Ng, 2004).
 this, including                                                         To our knowledge no one has used IRL before in the Ulti-
                                                                         matum Game or generally to learn patterns of behavior in
1. one culture is better than another at optimizing outcomes;            negotiation. We also perform two experiments to try to get
                                                                         at the question above of what is the best explanation for the
2. there is some kind of convention (Lewis, 1969) or equi-               observed behavioral differences across cultures. On one ac-
    librium at work, such that people behave differently be-             count, it is the different goals that lead to different behavior.
    cause the context is different, particularly their expecta-          In this case we would predict that we learn different goals for
    tions about how others will behave. For example people               different cultural patterns and that these goals would be bet-
    in Japan or England drive on the left while people from              ter at generating observed behavior than other possible goals.
    America and Europe drive on the right, because that is the           On another account, we would expect the same set of goals to
    safest, most efficient way given how other drivers will be-          be satisfactory for any population, and differences in behav-
    have, even though the goals of safety and efficiency are             ior to result from the different environments that are encoun-
    the same, and neither is innately better at achieving these          tered. Our results show that the learned weights are better
    goals;                                                               able to match observed distributions of culture-specific be-
3. the cultures have different goals, which lead to their opti-          havior than either arbitrary weights, a simple model based on
    mizing different functions.                                          economic gain, or in most cases the weights learned for other
                                                                         cultures. This suggests that cultures vary in goals, not just
    Most classical economic game-theory accounts of                      conventional circumstances but also that we can successfully
 decision-making, e.g. (Neumann & Morgenstern, 1944),                    use IRL techniques to learn population-specific goals for this
 look at a monolithic notion of utility and maximizing                   type of game.
                                                                     2097

   The structure of the paper is as follows. First we briefly          theory of justice (Rawls, 1971)). Each of these metrics can
present the Ultimatum Game and studies that show different             be given one or more valuations, choosing an optimum point
behaviors for different culture groups. Then we describe our           and scale. The agent has a vector of weights, one per valua-
decision-making model and we present an overview of Rein-              tion, indicating the relative importance of that valuation. The
forcement Learning (RL) and IRL. After that we talk about              total value for each choice is the sum of the product of values
our experimental setup and present our results. Then we dis-           and weights for each valuation as shown in equation (1):
cuss our results and propose ideas for future work, and finally                                      n
we conclude.                                                                   Value(Choicei ) =    ∑ (W j ∗V j (Choicei ))    (1)
                                                                                                    j=1
          Culture and the Ultimatum Game
We use the Ultimatum Game as a testbed for our model. The                 An advantage of this multi-valuation approach is that it can
Ultimatum Game involves two players bargaining over a cer-             model an agent who cares (possibly to different extents) about
tain amount of money (in our experiments, $100). One player,           different aspects of the situation, such as self-interest, col-
the proposer, proposes a division, and the second player, the          lective interest, and fairness. In (Nouri & Traum, 2011) we
responder, accepts or rejects it. If the responder accepts, each       also adapted this model to take into account Hofstede’s di-
player earns the amount specified in the proposal, and if the          mensions (Hofstede, 2001), i.e. Individuality (IDV), Power
responder rejects, each player earns zero. At perfect equi-            Distance (PDI), Long Term Orientation (LTO), Masculinity
librium, according to economic game theory, the proposer re-           (MAS), Uncertainty Avoidance (UAI). Thus our generalized
ceives all or almost all of the money and the responder accepts        model shown in (2) breaks down the elements of the weight
all offers made to them. This classic experimental economics           vector into one component per dimension, and thus an overall
game has received a great deal of attention since the initial          matrix of n valuations and m (=5) dimensions.
experiment by (Güth, Schmittberger, & Schwarze, 1982). Re-                                     n     UAI
sults from these studies often deviate from the predictions of            Value(Choicei ) =    ∑ ((   ∏   W j,d ) ∗V j (Choicei )) (2)
game theory (Henrich, 2000; Camerer, 2003). In fact there                                      j=1  d=IDV
is considerable variation of offers and rejection rates across
studies (Henrich, 2000; Buchan, Croson, & Johnson, 1999),                 In this paper our focus is to learn the weights of (1) but
and it has been reported that people from different cultures           our ultimate goal is also the learning of the weights of equa-
behave differently in this game. For example (Roth et al.,             tion (2) that take into account Hofstede’s dimensions (see the
1991) studied the Ultimatum Game in four countries (US,                discussion section).
Japan, Israel, and former Yugoslavia). They found that the
                                                                                    Reinforcement Learning and
offers in US and Yugoslavia were higher than the offers in
Japan which were higher than the offers in Israel. (Henrich,                      Inverse Reinforcement Learning
2000) compared the behavior of 18-30 year old Machiguenga              An agent’s policy is a function from contexts to (possibly
men of the Peruvian Amazon with UCLA students and found                probabilistic) decisions that the agent will make in those con-
significant differences, i.e. the offers of the latter were higher     texts. Reinforcement Learning (RL) is a machine learning
than the offers of the former. (Buchan et al., 1999) studied the       technique used to learn the policy of an agent (Sutton &
differences in comparable student populations in Pennsylva-            Barto, 1998). For an RL-based agent the objective is to max-
nia and Tokyo and observed that the offers of the former were          imize the reward it gets during an interaction. Because it is
lower than the offers of the latter.                                   very difficult for the agent, at any point in the interaction, to
   All the above studies clearly show that culture can play            know what will happen in the rest of the interaction, the agent
an important role in negotiation and in particular in the Ulti-        must select an action based on the average reward it has pre-
matum Game. The question however is what role: different               viously observed after having performed that action in sim-
goals, or different conventions, and whether we can learn to           ilar contexts. This average reward is called expected future
emulate culture-specific behavior.                                     reward. RL is used in the framework of Markov Decision
                                                                       Processes (MDPs). An MDP is defined as a tuple (S, A, P,
              Our Decision-Making Model                                R, γ) where S is the set of states (representing different con-
Our decision-making model presented in (Nouri & Traum,                 texts) which the agent may be in, A is the set of actions of the
2011) considers a number of different metrics for evaluating           agent, P : S × A → P(S, A) is the set of transition probabili-
a given situation, even for something as simple as division of         ties between states after taking an action, R : S × A → ℜ is
money in an economic game such as the prisoner’s dilemma               the reward function, and γ a discount factor weighting long-
(Camerer, 2003) or the Ultimatum Game (Güth et al., 1982).            term rewards. At any given time step i the agent is in a state
Each of the metrics can be calculated from a basic payoff              si ∈ S. When the agent performs an action αi ∈ A following
matrix. The metrics we considered for the Ultimatum Game               a policy π : S → A, it receives a reward ri (si , αi ) ∈ ℜ and
include: Self (the agent’s own gain); Other (the gain of an-           transitions to state si+1 according to P(si+1 |si , αi ) ∈ P. The
other); Self/Other (the relative gain of the negotiators); Min-        quality of the policy π followed by the agent is measured by
imum (lower bound of any participant - the aim of Rawls’               the expected future reward also called Q-function, Qπ : S × A
                                                                   2098

→ ℜ. Details are given in (Sutton & Barto, 1998). There are             weights that vary depending on the round. For example, for
several algorithms for estimating the Q-function and we use             round 4 we give a higher weight to the last round values and
Q-learning (Sutton & Barto, 1998). However, Q-learning re-              for round 2 a higher weight to the first round values. For each
quires thousands of interactions between the agent and the en-          culture we generate “expert” data by having the SU-proposer
vironment in order to learn the optimal policy. In the case of          interact with the SU-responder for that culture. We then apply
a multi-party interaction, such as dialogue or the Ultimatum            IRL to learn weights of different motivational factors for each
Game, the environment also needs to represent the decisions             of these cultures and roles (proposer and responder), by iter-
and actions of another participant. For this reason we need to          atively playing against the appropriate SU. We then use the
build another agent, called a simulated user (SU) (Georgila,            weights as a reward function, using RL, to learn policies for a
Henderson, & Lemon, 2006), that will behave as part of the              proposer and responder for each culture. We evaluate success
environment and will interact with the policy for thousands             of the learned policies by how closely they match the expert
of iterations to generate data in order to explore the search           data. We compare our learned policies with two baselines:
space and thus facilitate learning. Note that the SU gener-             RL models trained with either a random reward function or a
ates a variety of actions for each state based on a probability         reward function based on maximizing the wealth of the agent.
distribution but does not learn from the interaction.                   We also compare the policies learned for a particular culture
   With RL, the reward function should be defined. Design-              with the policies learned for the other cultures and the human
ing a good reward function is not trivial and not always pos-           expert data of the other cultures.
sible. There are tasks where it is not clear what constitutes              Our state definition includes information about the accu-
a good reward function. Inverse Reinforcement Learning                  mulated wealth gain of the agent (AccSelf), i.e. the wealth
(IRL) (Abbeel & Ng, 2004) aims to learn a reward func-                  gain that the agent has gathered starting from the first round of
tion (not necessarily the true reward function) from a set of           the game, the accumulated wealth gain of the SU (AccOther),
data recording interactions between the agent and the envi-             the wealth gain of the agent in the current round (Self), the
ronment. This data is called expert data. The reward function           wealth gain of the SU in the current round (Other), and also
R can be expressed as follows:                                          different representations of their relative gain (Self/Other)
                                      k                                 and the minimum gain (Min). We also take into account the
           Rw (s, α) = wT φ(s, α) = ∑ wi φi (s, α) (3)                  round of the game. There are 11 actions that the proposer can
                                     i=1                                perform (offer=0, offer=10, ..., offer=100). The initial context
                                                                        can be different for each round depending on the accumulated
   where s is the state that the agent is in and α the action that
                                                                        wealth of the agents, and the resulting reward is uncertain,
it performs in this state, and wT is a vector of weights wi for
                                                                        depending on the action of the responder. For the responder,
the feature functions φi (s, α). Note that these feature func-
                                                                        there are only two actions (accept, reject), but again there are
tions are specified manually and the weights wi are estimated
                                                                        many possible different start states to consider depending on
by IRL.
                                                                        the accumulated wealth of the agents (the reward is determin-
   In particular we use the imitation learning algorithm
                                                                        istic based on the state and action chosen).
(Abbeel & Ng, 2004). The imitation learning algorithm is an
iterative process. Initially we have a random policy πi that by            The feature functions that we use are binary, i.e. the value
interacting with the SU generates data. Then this data is com-          of the feature function φi (s, α) is 1 when φi is true for state s
pared with the expert data and the weights wi are calculated.           and action α. So to form the feature functions φi (s, α) each
Based on these weights a reward function is estimated and               feature is paired with all the available actions. Table 1 lists
RL is performed to learn a new policy πi+1 which generates a            the features that we use to represent the type of context that
new set of data by interacting with the SU. Then this new data          we consider in each state. Thus for the proposer the feature
is compared with the expert data and new weights are calcu-             function Self≥10-offer=10 is 1 when the self gain of the pro-
lated, a new reward function is computed and so forth. The              poser is ≥10 and the proposer has made an offer of 10, which
iteration stops when the distance between the data generated            means that this feature function is going to be 0 at the time
from the interaction of the latest policy with the SU and the           of the offer (because at that point Self is always 0), 1 after
expert data is lower than an empirically set threshold.                 this offer has been accepted, and 0 after this offer has been
                                                                        rejected. We also use additional features related to the accu-
                    Experimental Setup                                  mulated wealth that are not depicted in Table 1 due to space
We use data of the distribution of offers and acceptances or            constraints. In fact every possible value of AccSelf or Ac-
rejections for four different cultures (US, Japan, Israel, and          cOther can form a feature, e.g. AccSelf=150, AccOther=200,
former Yugoslavia) reported in (Roth et al., 1991). For each            etc. Thus for the proposer the feature function AccSelf=150-
culture, we generate SU-proposers and SU-responders by us-              offer=20 is goint to be 1 when the accumulated wealth of the
ing probability functions that match the reported data. (Roth           proposer is 150 and the proposer has made an offer of 20.
et al., 1991) provide this data for the first and last round of the        As we can see from the previous discussion our model is
game. In our setup the game lasts 5 rounds. For the rounds in           considerably different from the original SU model (human
between we interpolate the first and last round values using            data) that just uses a probability distribution per round. First,
                                                                    2099

                                                                    the US culture we have policy-rewardUS-trainUS, policy-
                Table 1: Features used for IRL.
                                                                    rewardJapan-trainUS, policy-rewardIsrael-trainUS, policy-
        Self≥0       Other≥0       Self/Other>2                     rewardYugoslavia-trainUS). Then we test the 4 policies
        Self≥10      Other≥10      Self/Other>1                     against SUs from the same culture that they were trained on
        Self≥20      Other≥20      Self/Other=1                     (in this case, US). If the goals for different cultures really
        Self≥30      Other≥30      Self/Other<1                     are different, then one would expect that policy-rewardUS-
        Self≥40      Other≥40      Self/Other<1/2                   trainUS would better match the expert US data than policies
        Self≥50      Other≥50      Min(Self,Other)=0                learned using weights from other cultures.
        Self≥60      Other≥60      Min(Self,Other)=10                  To measure how closely the distributions generated with
        Self≥70      Other≥70      Min(Self,Other)=20               the 3 models match the human expert data we use Kullback-
        Self≥80      Other≥80      Min(Self,Other)=30               Leibler divergence.1 The Kullback-Leibler (KL) divergence
        Self≥90      Other≥90      Min(Self,Other)=40               between two probability distributions P and Q is defined as
        Self=100     Other=100     Min(Self,Other)=50               follows:
                                                                                                  n
                                                                                                               P(i)
                                                                                  DKL (P||Q) = ∑ P(i)log2              (4)
                                                                                                 i=1           Q(i)
our model is deterministic for each state but keeps track of
additional state information, such as accumulated gains for            where n is the number of points in the distribution that we
each side. Thus we can still get a range of different offers        consider. Because KL divergence is asymmetric we calculate
and responses from our agents, depending on the learned pol-        DKL (P||Q) and DKL (Q||P) and then we take the average. The
icy for each state (including the accumulated gain) and the         lower the KL divergence the closer the distributions.
probability of those states. Second, our model for a specific
culture includes a reward function, which is specific to that                                    Results
culture distribution. Third, the reward function could poten-       In Table 2 we can see the KL divergences that we get when
tially be applied to other problems (see the discussion sec-        we compare our model and the two baselines with the human
tion), whereas we would have to collect human data to create        expert data for the proposer and responder policies of the 4
a SU for a new problem.                                             cultures. To avoid local optima or just being lucky with the
   We perform two experiments. The goal of the first exper-         random rewards, we ran both our model and the weak base-
iment is to show that the reasoning behind the actions of the       line (based on a random reward) multiple times and for each
proposer is better modelled as a complex tradeoff of multi-         run we calculated the KL divergence. In Table 2 we report
ple goals, and cannot be explained merely by learning the           the median value of all computed KL divergences. In Fig-
behavior patterns of the partners. Thus for the proposer and        ures 1 and 2 we can also see a graphical representation of our
the responder and the 4 cultures we learn 3 policies using          comparisons for the Japan proposer policy and the US respon-
RL; one based on a random reward function that assigns arbi-        der policy. As we can see in all cases our IRL-based model
trary weights (weak baseline), one where the reward function        outperforms both the weak and strong baselines. This verifies
is based only on wealth (strong baseline), and one based on         our hypothesis that decision-making is a complex process that
IRL. If only the data patterns mattered and not the reward          cannot be attributed just to reacting to data or the sole factor
function, we should see comparable performance between              of self-gain. It also shows the power of IRL for accurately
policies trained using the weak baseline reward functions and       modelling negotiation.
policies using the learned ones. Surpassing this weak baseline
would be evidence that reward functions matter. The strong
                                                                    Table 2: KL divergences for IRL and the two baselines for all
baseline follows classical economic game theory predictions.
                                                                    cultures and roles.
If everyone really does have this as a reward function and
differences in behavior are due to learned differences in con-                        Proposer                      Responder
vention rather than goals, we should see this reward function                random      wealth     IRL     random wealth           IRL
able to match the observed behavior of different populations.         US       3.95      19.82      2.84      0.61        0.37     0.10
On the other hand, if the IRL reward functions lead to better         JP       4.01       4.86      0.74      0.64        0.25     0.16
models than the strong baseline, that is evidence that multiple
                                                                      IS       3.68      16.11      1.29      0.58        0.27     0.13
factors are taken into consideration.
                                                                      YU       9.28       3.49      1.73      0.57        0.26     0.11
   The purpose of the second experiment is to show that
the weights learned with IRL really are culture-dependent,
i.e. that they work better for the culture that the weights            The next question is whether our models are really captur-
were learned from than models learned for other cultures.           ing performance of people from the cultures that they were
To show that we use IRL to learn the reward function                    1 We also looked at Cartesian distance, but in all cases the best
for the 4 cultures and then we use these reward func-               matching policy for the expert data was the same, so we report only
tions to learn policies for each culture (for example for           KL-divergence, due to space restrictions.
                                                                2100

                                                                    sponders are the best model for US data, Japanese responders
                                                                    are a good model of the Japan data (equally good to US and
                                                                    Israel responders), Israel and Yugoslav responders are a good
                                                                    model of the Israel and Yugoslavia data respectively, but not
                                                                    as good as US responders. These results are encouraging and
                                                                    show that our models do not just beat the weaker baselines
                                                                    of wealth and random rewards, but also in most cases learn
                                                                    to model a culture better than models learned with different
                                                                    cultures. As we saw there are a few cases in which the re-
                                                                    sults were not optimal. We believe that there could possibly
                                                                    be some convergence issues, even though our IRL algorithms
                                                                    ran for over 1000 iterations and our KL divergences are based
                                                                    on many runs, or perhaps, we need a larger set of features and
                                                                    constraints between features. Given that we take into account
                                                                    in our state accumulated wealth as well as rounds, our state
Figure 1: Comparison of random reward, wealth reward,               space is fairly large. These are issues for further investigation.
IRL-based reward and human data for the Japan proposer
policies tested with Japan SU-responders.
                                                                    Table 3: Cross-culture results, comparison with human data
                                                                    from different cultures (KL divergences). Best values are in
                                                                    bold (horizontally) and italics (vertically).
                                                                                      Proposer                     Responder
                                                                                    Human Data                    Human Data
                                                                              US JP          IS     YU US JP                IS    YU
                                                                         US 2.84 3.11 4.61 2.71 0.10 0.13 0.08 0.06
                                                                         JP 1.05 0.74 1.06 1.96 0.27 0.16 0.18 0.24
                                                                         IS 1.82 2.04 1.29 4.27 0.25 0.14 0.13 0.20
                                                                         YU 2.21 2.83 5.76 1.73 0.15 0.27 0.20 0.11
                                                                       In Table 4 we can see the results of experiment 2, where
                                                                    we use weights learned with one culture to learn policies by
                                                                    training on other cultures.2 The results generally verify our
                                                                    hypothesis that the learned weights are culture-specific: with
                                                                    only two exceptions, the policy based on the reward function
Figure 2: Comparison of random reward, wealth reward,               learned for that culture outperforms policies based on reward
IRL-based reward and human data for the US responder poli-          functions for all the other cultures. In the case of the US and
cies tested with US SU-proposers.                                   Japanese responders, it appears that the policy trained with
                                                                    the Israel reward performs just as well as the policy using the
                                                                    learned reward function for US and Japanese responders, re-
trained for. We examine this question in two ways. First we         spectively. However the converse does not hold: the Japan
look at the KL divergences between all learned models and all       and US reward functions do not work well for the Israel poli-
original data sets. This is shown in Table 3. We can see that       cies. These issues need to be investigated further.
most of the time the model for each culture matches the data
set from that culture better than other data sets. On the other
                                                                                                Discussion
hand there are several exceptions, for example, US proposers        Our results show clearly that there are various factors that
do better on Yugoslavia data than US data, and US responders        may affect one’s decision and these factors may vary signif-
perform well on all human data. We can also look at this ta-        icantly depending on the culture of the decider. They also
ble from a different perspective, as a way to compare various       show the power of IRL for uncovering the decision-making
models (learned from data of different cultures) with the same      mechanism of negotiators. (Turan, Dudik, Gordon, & Wein-
human data. Here we can see that in most cases the data set is      gart, 2011) argue about the potential advantages of using IRL
best modelled by the culture trained on it. However, there are      for learning the goals and motives of negotiation participants.
a few exceptions, for example, Israel proposers are a better
                                                                        2 We used only some of the many reward functions for each cul-
model of the Israel data than US and Yugoslav proposers, but
                                                                    ture to learn policies for other culture data. In this table we show
a worse model of the Israel data than Japanese proposers. US        results for the reward functions that are closest to the median values
proposers are not a very good model of the US data. US re-          reported in Table 2, but in some cases they are not identical.
                                                                2101

                                                                                         Acknowledgments
Table 4: Cross-culture results, learning policies using rewards
calculated from different cultures (KL divergences).                 This work was funded by the NSF grant IIS-1117313 and
                                                                     a MURI award through ARO grant number W911NF-08-1-
        Policy/Role              Reward functions                    0301.
                           US       JP      IS       YU
        US Proposer        2.84    7.32 14.13 11.78                                           References
        US Responder       0.08    6.77 0.08 19.10                   Abbeel, P., & Ng, A. Y. (2004). Apprenticeship learning via
        JP Proposer        7.71    1.58 4.89 14.25                     inverse reinforcement learning. In Proceedings of the 21st
        JP Responder      14.94    0.11 0.11 15.25                     International Conference on Machine Learning (ICML).
        IS Proposer        3.92    5.93 1.27 19.52                   Buchan, N. R., Croson, R. T. A., & Johnson, E. J. (1999). Un-
        IS Responder       7.62    6.95 0.10 13.08                     derstanding what’s fair: Contrasting perceptions of fairness
        YU Proposer        3.32    7.90 19.84 1.73                     in ultimatum bargaining in Japan and the United States. In
        YU Responder       7.51    8.16 0.12        0.06               Discussion paper, University of Wisconsin.
                                                                     Camerer, C. F. (2003). Behavioral game theory - Experiments
                                                                       in strategic interaction. Princeton University Press.
                                                                     Gal, Y., Pfeffer, A., Marzo, F., & Grosz, B. J. (2004). Learn-
They use scenarios from group negotiation research and dis-            ing social preferences in games. In Proceedings of the
cuss how IRL could hypothetically be applied to such scenar-           19th National Conference on Artificial Intelligence (p. 226-
ios, but they have not actually used IRL for negotiation.              231).
   With IRL we calculated the weights for a number of fea-           Georgila, K., Henderson, J., & Lemon, O. (2006). User sim-
tures (see Table 1). These weights can be used in equation             ulation for spoken dialogue systems: Learning and evalua-
(1). However, in order to use equation (2) we need to find             tion. In Proceedings of the 9th International Conference on
some kind of mapping between these weights and Hofstede’s              Spoken Language Processing (INTERSPEECH-ICSLP).
dimensions. Our ultimate future goal is once we have Hof-            Güth, W., Schmittberger, R., & Schwarze, B. (1982). An
stede’s dimensions for a culture to be able to calculate these         experimental analysis of ultimatum bargaining. Journal of
weights automatically. That would be very useful in cases              Economic Behavior & Organization, 3(4), 367-388.
where we do not have data to calculate the weights directly          Henrich, J. (2000). Does culture matter in economic behav-
from but it is indeed a very ambitious goal. Other future work         ior? Ultimatum game bargaining among the Machiguenga
involves examining whether the reward function learned for             of the Peruvian Amazon. American Economic Review, 90,
one game or role can transfer to another. We also aim to ex-           973–979.
periment with larger numbers of RL and IRL iterations and            Hofstede, G. H. (2001). Culture’s consequences: Comparing
runs, different exploration parameters, different state repre-         values, behaviors, institutions, and organizations across
sentations, and different features.                                    nations. Thousand Oaks, CA: SAGE.
                                                                     Lewis, D. K. (1969). Convention: A philosophical study.
                         Conclusion                                    Harvard University Press.
                                                                     Neumann, J. V., & Morgenstern, O. (1944). Theory of games
We used IRL to learn a model for cultural decision-making in           and economic behavior. Princeton University Press.
negotiation. This model takes into account multiple individ-         Nouri, E., & Traum, D. (2011). A cultural decision-making
ual and social factors for evaluating the available choices in         model for virtual agents playing negotiation games. In Pro-
a decision set. Our model assigns different weights to these           ceedings of the International Workshop on Culturally Mo-
factors based on the modelled culture. We applied this model           tivated Virtual Characters.
to the Ultimatum Game and we showed that weights learned             Rawls, J. (1971). A theory of justice. The Belknap Press of
from IRL surpass both a weak baseline with random weights,             Harvard University Press.
and a strong baseline that only seeks to maximize the agent’s        Roth, A. E., Prasnikar, V., Okuno-Fujiwara, M., & Zamir,
own gain. Our model outperformed both baselines by gener-              S. (1991). Bargaining and market behavior in Jerusalem,
ating behavior that was closer to the behavior of human play-          Ljubljana, Pittsburgh, and Tokyo: An experimental study.
ers of the game in 4 different cultures. We also showed that           American Economic Review, 81(5), 1068-95.
the weights learned with our model for one culture outper-           Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning:
form weights learned for other cultures when playing against           An introduction. Cambridge, MA: MIT Press.
opponents of the first culture.                                      Turan, N., Dudik, M., Gordon, G., & Weingart, L. R.
   Our results verify our hypothesis that decision-making in           (2011). Modeling group negotiation: Three computational
negotiation is a complex, culture-specific process that can-           approaches that can inform behavioral sciences. In E. A.
not be explained just by the notion of maximizing one’s own            Mannix, M. A. Neale, and J. R. Overbeck, eds., Negotiation
utility. We showed that cultures vary in goals, not just in con-       and Groups (Research on Managing Groups and Teams)
ventional circumstances but also that we can successfully use          (Vol. 14, p. 189-205).
IRL techniques to learn culture-specific goals.
                                                                 2102

