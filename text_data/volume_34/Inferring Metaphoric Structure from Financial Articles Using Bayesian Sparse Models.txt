UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Inferring Metaphoric Structure from Financial Articles Using Bayesian Sparse Models
Permalink
https://escholarship.org/uc/item/5tm983wg
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Salzle, Martin
Keane, Mark
Publication Date
2012-01-01
Peer reviewed
  eScholarship.org                                  Powered by the California Digital Library
                                                                     University of California

                         Inferring Metaphoric Structure from Financial Articles
                                             Using Bayesian Sparse Models
                                       Martin Sälzle (saelzle_martin@ceu-budapest.edu)
                                    Department of Cognitive Science, Central European University
                                           Frankel Leó útca 30-34, Budapest, 1023, Hungary
                                              Mark T. Keane (mark.keane@ucd.ie)
                                 School of Computer Science & Informatics, University College Dublin
                                                        Belfield, Dublin 4, Ireland
                               Abstract                                   Gerow and Keane (2011a-c; henceforth abbreviated as
   Drawing from a large corpus (17,000+ articles) of financial
                                                                       G&K) took such a distributional approach to understanding
   news, we perform a Bayesian sparse model analysis of the            metaphorically-structured knowledge (in hierarchies and
   argument-distributions of the UP and DOWN-verbs, used to            antonymic relationships) between “UP" and "DOWN” verbs
   describe movements in indices, stocks and shares. Previous          from a corpus of financial news reports. Lakoff and Johnson
   work, by Gerow and Keane (2011a, 2011b, 2011c), has                 (1980) have argued that metaphors are used to structure
   shown, using measures of overlap and k-means clustering,            many domains of human experience and also many abstract
   that metaphor hierarchies and antonymic relations can be
                                                                       conceptual domains (e.g., emotions). They specifically
   found in this data; for instance, UP verbs have rise as a
   superordinate organizing a distinct set of subordinate verbs        identified the use of the UP-DOWN metaphor opposition in
   (soar, jump, climb, surge, rebound, advance). This work             accounts of wealth (e.g., WEALTH-IS-UP as in high class)
   empirically realizes theories about the structuring of our          and in the rise and fall of numbers (e.g., MORE-IS-UP;
   conceptual systems with metaphors (Lakoff, 1992; Lakoff &           LESS-IS-DOWN).
   Johnson, 1980) but does so using a distributional approach to          G&K (2011a) build a corpus of 17,000+ financial articles
   meaning; namely, that words that occur in similar contexts          covering a 4-year period, about the major world stock
   have similar meanings (see Wittgenstein, 1953). However,
   Gerow and Keane’s analysis does not show the overall                indices (Dow Jones, NIKKEI, FTSE-100) from the
   structure of how these metaphors semantically relate to one         Financial Times, NY Times and BBC websites; the corpus
   another. In the present paper, we re-analyzed their data using      contained over 10M words. After parsing the corpus, G&K
   a Bayesian sparse model (Lake & Tenenbaum, 2010) in order           selected all the sentential instances of the most commonly
   to infer this metaphor space as a uniform representation,           occurring UP and DOWN verbs (see G&K, 2011a, 2011b
   based on the argument distributions. Therefore, we treated          for details). Table 1 shows some of the most commonly
   arguments as features of metaphors. Our model learned three
                                                                       used arguments found in the corpus, indicating the
   dimensional graphs in an unsupervised manner as sparse
   representations of the metaphoric structure over all argument       metaphoric usage of the selected verbs. G&K then analyzed
   distributions, in parallel. Doing so, it also successfully          the clustering in these distributions (using k-means
   indicates the metaphoric hierarchies and antonymy relations,        clustering) and the overlaps between the distributions of
   that were found by the previous models. In conclusion, we           different verbs (using the % overlap in each pair-wise
   discuss the benefits of this approach.                              comparison of verb arguments). This analysis threw up
   Keywords: Argument features; analogy; Bayesian inference;           some striking regularities.
   emergent structure; corpus analysis; metaphor hierarchies;
   semantic cognition; similarity; sparse representation; spatial           Table 1: The percentage of rise’s argument
   metaphor; structure discovery; unsupervised learning.                    distribution covered each of the 10 most frequent
                                                                            arguments.
                          Introduction
In recent years, significant progress has been made in                             Rank    Argument Word      % of Corpus
deriving meaning from statistical analyses of distributions                           1         Index             7.3
of words (e.g., Gerow & Keane, 2011a; Landauer &                                      2         Share             5.6
Dumais, 1997; Turney, 2006; Turney & Pantel, 2010;                                    3         Point             4.8
Michel et al., 2010). This distributional approach to                                 4        Percent            2.9
meaning takes the view that words that occur in similar                               5         Price             2.4
contexts tend to have similar meanings (see Wittgenstein,                             6         Stock             2.0
1953) and that by analyzing word usage we can recover                                 7         Yield             1.9
meaning. For instance, Michel et al., (2010) argue that                               8          Cent             1.3
significant insights into human culture and behavior can be                           9         Profit            0.9
derived from analyzing very large corpora, such as the                               10          Rate             0.9
GoogleBooks repository.
                                                                   2252

Metaphor Hierarchies                                             distance, cosine similarity, K-L          divergence) on the
G&K (2011b) argued that if one verb-metaphor (e.g., that         argument distributions to determine which one best
referred to by rise) was organizing another metaphoric verb      predicted the human choices. Given a set of 13 UP verbs
(e.g., soar) then the argument distribution of the former        and 15 DOWN verbs (as possible antonyms) people
should largely cover the latter, but the opposite would not      identified 114 unique antonym pairs. Of these, in 60% of
be the case. They also argued that verb-metaphors at the         cases, the cosine similarity of the argument distributions of
same level of generality (e.g., a basic level), sibling          pairs correctly identified the most preferred antonym-pair
metaphors, would have symmetrically overlapping                  from the human ratings. This figure rose to 87% if we
argument distributions. Their coverage- and cluster analysis     consider identifying the 1st or 2nd most preferred pairs (see
confirmed these types of structure. On coverage, they found      G&K, 2011c for details). Table 3 lists some results of the
that rise organized a group of metaphoric siblings (soar,        human antonymy ratings.
jump, climb, surge, rebound, advance), set off from a set of
other more outlying verb-metaphors (increase, rally,                 Table 2: Top 5 clusters in k-means analysis of UP-
recover, gain, alleviate, elevate; see Figure 1, A). In the          verbs (* rest = the remaining verbs).
clustering, they found that rise was quite separate from all
the other verbs that clustered together and that gain and            Rank     Cluster Groups                    % of Tot.
climb were quite distinct (see Table 2). A similar pattern                                                      (Freq.)
was found for fall and its subordinate-related verb-                 1        rise, rest*                       62% (1451)
metaphors (dip, retreat, sink, plunge, tumble, slide slip,           2        rise, gain, rest*                 18% (702)
slide, ease, drop, ease, plummet), with decline, lose,               3        rise, [climb, gain], rest*        4% (36)
decrease and worsen being outliers (see Figure 1, B, and             4        rise, [jump, climb, gain], rest*  3% (27)
G&K, 2011b, for detailed results).                                   5        all-verbs-as-one-group            2% (18)
                                                                     Table 3: Some examples of people’s verb antonymy
                                                                     ratings, conducted by G&K (2011c). Percentage
                                                                     measures indicate mean antonymy ratings over
                                                                     participants and sub-tasks (free generation and match
                                                                     the opposite).
                                                                                    Verb pair         Antonymy
                                                                                    rise-fall         57%
                                                                                    jump-fall         31%
                                                                                    drop-climb        13%
                                                                                    decline-rise      27%
                                                                                    slide-climb       23%
                                                                                    soar-plummet      17%
                                                                 Using Sparse Models Instead
                                                                 G&K found a number of interesting regularities for
                                                                 hierarchical and antonymic relationships between the
                                                                 argument distributions of UP and DOWN verbs. However,
                                                                 their results were based on different approaches, rather than
                                                                 a unifying model, and do not indicate the semantic structure
                                                                 of the metaphoric corpus as a whole. Arguably, it is
                                                                 essential to understand the cognitive semantics of the
    Figure 1: Argument coverage of (A) main UP-verbs             corpus, as the meaning of individual concepts must depend
    and (B) main DOWN-verbs from G&K (2011b).                    on how they relate to one another (Kemp & Tenenbaum,
                                                                 2008). Bayesian sparse models, also known as sparse graph
Metaphoric Antonyms                                              codes (MacKay, 2003), appear to be good candidates for
G&K (2011c) also analyzed verb distributions for                 this task.
antonymic relations; arguing that preferred antonyms rise-          Bayesian sparse models basically infer an emergent
fall should have more similar distributions than less            structure in a probabilistic framework (Rogers &
preferred antonyms rise-decrease or rise-lower. G&K              McClelland, 2004). Applied to semantics, they have been
performed a psychological experiment to find the preferred       shown to perform particularly well at finding regularities for
antonyms between the UP and DOWN verbs and then                  the clustering of features for very large numbers of words
formulated several different similarity measures (Euclidean      from different conceptual domains (Lake & Tenenbaum,
                                                             2253

2010). These models assume that people learn a set of               the combined set of UP and DOWN verb-metaphors (using
parameters that fit their observed data well.                       a 28 × 605 matrix).1
   Sparse models may be better at handling metaphoric
structure than other structured probabilistic models for            Method
semantic cognition (e.g., Kemp & Tenenbaum, 2008). The              Data Set A total of 28 verbs were used, 13 UP-verbs with
latter generate structures as instances of forms and discover                  386 distinct, unique arguments, 15-DOWN verbs
the structural instance of the form that best explains the                     with 456 distinct, unique arguments (based on
underlying dataset; including, structural instances based on                   those used by G&K, 2011b-c). There were 9,721
the graph grammar of trees, linear orders, multidimensional                    distinct sentence instances in the corpus (5803
spaces, rings, dominance hierarchies, cliques, and other                       sentences with UP verbs, 3918 sentences with
forms that are supposed to be the organizing principles for                    DOWN verbs).
data of different cognitive domains. In this way, these
models account for domain-specific inferences. The learned          Model Setup The code for the model we used was written
structures can then be used to model human inductive                           in MATLAB (provided by Brenden Lake,
reasoning about novel properties of objects within those                       Department of Brain and Cognitive Sciences, MIT;
domains (Kemp & Tenenbaum, 2009).                                              see Lake and Tenenbaum, 2010, for a detailed
   However, considering a dataset of metaphors, we need to                     description of the model). Formally, the undirected
take into account that contemporary cognitive linguistics                      graph W defines a multivariate Gaussian
understands conceptual metaphor not as domain-specific                         distribution p(f(k)|W) in the generative model,
inference but rather as mappings from one conceptual                           known as a Gaussian Markov Random Field
domain to another (Lakoff, 1987; Gibbs, 1994, 1996;                            (GMRF), where the n objects are the n-dimensions
Fauconnier & Turner, 1998, 2003). For example, mapping                         of the Gaussian. With a prior distribution on
the directionality of movement to changes in quantity (e.g.,                   sparsity, the model then estimates the maximum a
“prices are rising”). A cognitive model of metaphoric                          posteriori (MAP) parameters W as optimal
structure would, therefore, not necessarily need to select be-                 structure based on data. Each data set D was cast in
tween structural instances of domain specific forms. Since a                   a n × m matrix with n metaphors and m arguments.
metaphoric corpus is likely to consists of many mappings of                    Therefore, the columns of D, denoted as arguments
many different conceptual domains, it would rather need to
infer an emergent structure on the basis of a psychologically                  {f(1), ..., f(m)}, were assumed to be independent and
justified prior probability over the hypothesis space of                       identically distributed drawn from p(f(k)|W). With
possible structures. We think that sparseness would be a                       the n-dimensional Gaussian distribution, it is
useful prior for such a model, as it accounts for the                          assumed that arguments vary smoothly over the
cognitive parsimony that is needed to mentally structure                       graph. So, if two metaphors i and j happen to be
metaphors over the vast array of semantic domains (Lakoff,                     connected by a large weight (wij), they share
1992); as well as for the trade off between cognitive effect                   similar application frequencies over arguments. As
and computational effort (Wilson & Carston, 2006).                             a result of sparsity, most metaphors are not directly
                                                                               connected in the learned graph (i.e., wij=0). The
    Sparse Model Analysis of Verb-Metaphors                                    resulting weights allowed us to further apply a
                                                                               Markov Cluster Algorithm (MCA) to classify verb
Lake and Tenenbaum’s (2010) Bayesian sparse model was
                                                                               metaphors based on the covariation of their
used on G&K’s verb-metaphor corpus, involving UP and
                                                                               argument distributions. Inflation and pre-inflation
DOWN verbs, extracted from the larger finance corpus (see
                                                                               settings for the MCA were hold on standard (see
Data Set). This metaphor corpus contained 9,700+ distinct
                                                                               Freeman et al., 2007; Theocharidis, Van Dongen,
sentence instances for these two sets of verbs. The sparse
                                                                               Enright, & Freeman, 2009).
model should be able to learn and graph the structure of
these verb-metaphors by determining how they covary with            Results & Discussion
regard to the frequency of their argument features.
Graphically, the verb-metaphors are represented as nodes in         Figure 2 shows the resulting weight matrices illustrated as
a weighted graph, where the strength of the link between            sparse graphs learned for the three different datasets: (A)
two object-nodes is related to the weighted covariation of          UP verbs, (B) DOWN verbs and (C) UP-DOWN verbs
their features. The weights of the graph, denoted as the            combined (all graphs were drawn with BioLayout Express
symmetric matrix W, are learned from data by optimizing an          3D; videos of rotating versions of respective graphs should
objective function that trades off the fit to the data with the     be retrievable by clicking on them). In each graph, the
sparsity of the graph. In the present study, the sparse model       labeled nodes represent verb-metaphors (e.g., rise, fall). The
technique was used to build three different graphs: a graph         links show the connection weights and consequential
for the UP verb-metaphors and their arguments (using a 13           distances between the nodes, denoting similarity over all
x 386 matrix), a graph for the DOWN verb-metaphors and
their arguments (using a 15 x 456 matrix), and a graph of              1
                                                                         Resulting weight matrices are available from the authors.
                                                                2254

                                                  verbs graphed. The color and thickness of the links
                                                  represent the weight magnitude; red links indicate high
                                                  weights (with thickness indicating higher weights), whereas
                                                  blue links indicate low weights (between 0 and 1). Colors of
                                                  nodes denote classes of verb-metaphors found by the MCA.
                                                  Metaphoric Structure of UP Verbs Figure 2 (A) shows
                                                           the sparse graph for the UP verb-metaphors.
                                                           Overall, it literally provides a much better picture
                                                           of the semantic space of the metaphors with the
                                                           relative distances between each clearly shown,
                                                           compared to G&K’s (2011b, 2011c) analyses.
                                                           First, note that the rise node stands out as being
                                                           distinct and non-similar to most of the other nodes.
                                                           Counter-intuitively, this occurs because though
                                                           rise has arguments that cover many of the argu-
                                                           ments of most other verbs combined (also see
                                                           Figure 1, A), it has fewer arguments in common
                                                           individually with any given verb (and, therefore,
                                                           low similarity with each). Second, rise, climb and
                                                           gain cluster separate to the remaining verb-
                                                           metaphors (purple vs. green nodes). While we
                                                           know that rise has asymmetric coverage regarding
                                                           most other verbs, climb and gain have not (also see
                                                           Figure 1, A). Therefore, the latter are two highly
                                                           interconnected outliers.
                                                  Metaphoric Structure of DOWN Verbs Figure 2 (B)
                                                           shows the sparse graph for the DOWN verb-
                                                           metaphors. Here, again, the results are very similar
                                                           to what we saw for the UP verbs. Again, the sparse
                                                           model analysis provides a much better picture of
                                                           the semantic space of the metaphors with the
                                                           relative distances between each clearly shown.
                                                           First, note that now the fall node stands out as
                                                           being distinct and non-similar to most of the other
                                                           nodes; for the same reasons we advanced for rise.
                                                           However, regarding the coverage, it can just be
                                                           considered as a superordinate to some of the other
                                                           verbs (see Figure 1, B). Second, there is a marked
                                                           cluster of verbs that are all (almost) equally similar
                                                           to one another (green nodes). Third, there is
                                                           another set of verbs that are similar but distinct
                                                           (lose, drop and slip). While slip is an outlier, lose
                                                           and drop are highly similar. So, again, while these
                                                           graphs give a better picture of the space, they may
                                                           need to be supplemented by coverage measures in
                                                           defining whether nodes might be actual
                                                           superordinates or simply unrelated.
                                                  Metaphoric Verb Antonyms Figure 2 (C) shows the sparse
                                                           graphs for the combined UP and DOWN verb
                                                           corpora. These graphs are slightly different because
                                                           they deal with both categories of verbs. G&K's
Figure 2: Resulting sparse graphs for (A) UP-              (2011c) analysis for antonymy worked on the basis
metaphors, (B) DOWN-metaphors, and (C) UP-                 that the key antonyms would be highly similar,
DOWN metaphors.
                                              2255

         relative to other pairings across the two sets of       empirically in a systematic way has proven difficult. The
         verbs. Again, the sparse model shows this very          promise of the present work is that these ideas can be em-
         clearly as notable key antonym pairs appear as          pirically supported by a distributional analysis of verb
         close nodes: for instance, rise-fall, gain-drop,        arguments, with such metaphoric import. We have shown
         climb-slip, gain-lose. Further, how the verb-           that sparse models can provide a rich and informative basis
         metaphors cluster (shown by node coloration)            for relating these verb-metaphors together in a uniform
         indicates semantic similarity in how they got           metaphor space. We believe that this approach may be
         applied. However, the antonymy ratings from the         useful in modeling other cognitive tasks that rely on these
         human subject experiment of G&K (2011c)                 metaphoric spaces (e.g., language comprehension,
         correlate just weakly with the ones from the model      analogical thinking). For instance, in analogical thinking it
         (Pearson's r=0.4; see Figure 3). This might have        has long been argued that conceptual slippage (Hofstadter,
         experimental- and model related reasons: first, the     1995) and re-description (Keane, 1996; Kurtz, 2006) are
         verb-metaphors from the corpus were applied by          needed to account for human abilities: Bayesian sparse
         human speakers to describe financial changes. The       models provide a basis for allowing such slippage, assuming
         experimental data, however, are abstract antonymy       structural support for the slippage being considered.
         ratings of verbs, having neither applicational             However, our work has also indicated that the sparse
         relation to the domains relevant to conceptualize       models will still need a coverage analysis to isolate
         finance, nor to any other cognitive domain. (Future     superordinate metaphors. And, because these are important
         experiments for metaphoric antonymy would need          for conceptually structuring the metaphor space, they should
         to take this into account.) Second, the model's         be implemented in the way sparse models generate and learn
         antonymy ratings for the superordinates rise and        structure. This might be achievable by using hierarchical
         fall had to be excluded, since they were 0 to all       Bayesian sparse models (Chandrasekaran, Parrilo, &
         other metaphors, except to one another. Finally, in     Willsky, 2010) that potentially discover organizing
         the graph are also some high-similar pairings           metaphoric concepts as hidden or latent variables, and
         within the same verb set, like rally-rebound and        further increase sparsity.
         slip-ease, that are clearly “just similar” and not
         antonyms. The latter indicates that some prior                               Acknowledgments
         categorization of what-are-known-to-be broadly          This work was carried out as part of a self-funded MSc in
         opposite sets is required before such a merged          Cognitive Science at UCD by the first author. We want to
         model might be useful. Again, an additional             thank Brenden Lake for providing the code for the sparse
         coverage analysis is needed to isolate rise and fall    model, Aaron Gerow for all the data, Anne Tamm for
         as superordinates (see also Figure 1, A and B).         linguistic advice, and Máté Lengyel for technical
                                                                 suggestions.
                                                                                           References
                                                                 Chandrasekaran, V., Parrilo, P. A., & Willsky, A. S. (2010).
                                                                    Latent variable graphical model selection via convex
                                                                    optimization. In The 48th Annual Allerton Conference on
                                                                    Communication, Control, and Computing, (pp. 1610–
                                                                    1613). IEEE.
                                                                 Fauconnier, G., & Turner, M. (1998). Conceptual
                                                                    integration networks. Cognitive science, 22(2), 133–187.
                                                                 Fauconnier, G., & Turner, M. (2003). The way we think:
                                                                    Conceptual blending and the mind’s hidden complexities.
    Figure 3: Model- versus human antonymy rating of                Basic Books.
    verb-pairs in per cent. Human ratings reflect                Freeman, T. C., Goldovsky, L., Brosch, M., Van Dongen,
    perceived antonymy of verbs (see G&K, 2011c);                   S., Mazière, P.,...Enright, A. J. (2007). Construction,
    whereas model ratings reflect the computed                      visualisation, and clustering of transcription networks
    antonymy of verb-metaphors used to describe                     from microarray expression data. PLoS Computational
    financial changes. The latter are weighted entries of           Biology, 3(10).
    the model's weight matrix.                                   Gerow, A., & Keane, M. T. (2011a). Mining the Web for
                                                                    the"Voice of the Herd" to track stock market bubbles. In
                        Conclusion                                  Proceedings of the 22nd International Joint Conference
The suggestion that significant parts of our conceptual             on Artificial Intelligence: Barcelona, Spain.
systems are structured by metaphors has mainly received          Gerow, A., & Keane, M. T. (2011b). Identifying metaphor
support from linguistic and anthropological analyses (see           hierarchies in a corpus analysis of finance articles. In
Lakoff & Johnson, 1980). However, cashing out these ideas
                                                             2256

  Proceedings of the 33rd Annual Meeting of the Cognitive       Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006).
  Science Society: Boston, MA.                                    Theory-based Bayesian models of inductive learning and
Gerow, A., & Keane, M.T. (2011c). Identifying metaphoric          reasoning. Trends in Cognitive Sciences, 10(7), 309-318.
  antonyms in a corpus analysis of finance articles. In         Turney, P. D. (2006). Similarity of semantic relations.
  Proceedings of the 33rd Annual Meeting of the Cognitive         Computational Linguistics, 32(3), 379-416.
  Science Society: Boston, MA.                                  Turney, P. D., & Pantel, P. (2010). From frequency to
Gibbs, R. W. (1994). The poetics of mind: Figurative              meaning: Vector space models of semantics. Journal of
  thought, language, and understanding. Cambridge                 Artificial Intelligence Research, 37(1), 141-188.
  University Press.                                             Wilson, D., & Carston, R. (2006). Metaphor, relevance and
Gibbs, R. W. (1996). Why many concepts are metaphorical.          the ’emergent property’ issue. Mind & Language, 21(3),
  Cognition, 61, 309–319.                                         404–433.
Hofstadter, D. R. (1995). Fluid concepts and creative           Wittgenstein, L., & Anscombe, G. E. M. (1953/2001).
  analogies: Computer models of the fundamental                   Philosophical investigations: the German text, with a
  mechanisms of thought. Together with the Fluid                  revised English translation. Wiley-Blackwell.
  Analogies Research Group. NY: Basic Books.
Keane, M. T. (1996). On adaptation in analogy. Quarterly
  Journal of Experimental Psychology, 49A, 1062-1085.
Kemp, C., & Tenenbaum, J. B. (2008). The discovery of
  structural form. Proceedings of the National Academy of
  Sciences, 105(31), 10687-10692.
Kemp, C., & Tenenbaum, J. B. (2009). Structured statistical
  models of inductive reasoning. Psychological Review,
  116(1), 20-58.
Kurtz, K. J. (2005). Re-representation in comparison:
  Building an empirical case. Journal of Experimental &
  Theoretical Artificial Intelligence, 17(4), 447–459.
Lake, B., & Tenenbaum, J. (2010). Discovering structure by
  learning sparse graph. In Proceedings of the 33rd Annual
  Cognitive Science Conference: Boston, MA.
Lakoff, G. (1987). Women, fire, and dangerous things.
  What/How Categories Reveal About the Mind. Chicago:
  Chicago UP.
Lakoff, G. (1992). The contemporary theory of metaphor. In
  A. Ortony (Ed.), Metaphor and Thought 2nd Edition.
  Cambridge University Press.
Lakoff, G., & Johnson, M. (1980). Metaphors We Live By.
  Chicago, IL: University of Chicago Press.
Landauer, T. K., & Dumais, S. T. (1997). A solution to
  plato’s problem: The latent semantic analysis theory of
  acquisition, induction, and representation of knowledge.
  Psychological review, 104(2), 211.
MacKay, D. J. C. (2003). Information theory, inference, and
   learning algorithms. Cambridge, UK: Cambridge
   University Press.
Michel, J. B., Shen, Y. K., Aiden, A. P., Veres, A., Gray,
  M. K., Pickett,…Aiden, E. L. (2011). Quantitative
  analysis of culture using millions of digitized books.
  Science, 331(6014), 176.
Rogers, T. T., & McClelland, J. L. (2004). Semantic
  cognition: A parallel distributed processing approach.
  Cambridge, MA: MIT Press.
Theocharidis, A., Van Dongen, S., Enright, A. J., &
  Freeman, T. C. (2009). Network visualization and
  analysis of gene expression data using BioLayout
  Express3D. Nature protocols, 4(10), 1535–1550.
                                                            2257

