UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
When Suboptimal Behavior is Optimal and Why: Modeling the Acquisition of Noun Classes in
Tsez
Permalink
https://escholarship.org/uc/item/97b5w454
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Gagliardi, Annie
Feldman, Naomi H.
Lidz, Jeffrey
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                 When Suboptimal Behavior is Optimal and Why:
                                 Modeling the Acquisition of Noun Classes in Tsez
                                                Annie Gagliardi (acg39@umd.edu)
                                               Naomi H. Feldman (nhf@umd.edu)
                                                   Jeffrey Lidz (jlidz@umd.edu)
           Department of Linguistics, 1401 Marie Mount Hall, University of Maryland, College Park, MD 20742 USA
                                 Abstract                              behavior suggests that there is more to language acquisition
   Children acquiring languages with noun classes (grammatical
                                                                       than a simple mapping of external statistical information to
   gender) have ample statistical information available that           an internal representation of this distribution. In particular it
   characterizes the distribution of nouns into these classes, but     suggests that properties of the learner shape the statistical
   their use of this information to classify novel nouns differs       information in the input into the subset of information that is
   from the predictions made by an optimal Bayesian classifier.        used to guide inferences in language acquisition: the intake.
   We propose three models that introduce uncertainty into the         We use a Bayesian model of noun classification to probe
   optimal Bayesian classifier and find that all three provide         what underlies the difference in the measureable input and
   ways to account for the difference between children’s
   behavior and the optimal classifier. These results suggest that     the intake that children use to acquire noun classes.
   children may be classifying optimally with respect to a                As a general framework, we assume that optimal
   distribution that doesn’t match the surface distribution of         performance in an experimental task involves the following
   these statistical features.                                         four components:
   Keywords: language acquisition; noun classes; Bayesian                (1) Accumulation of knowledge of the statistical
   classification; statistical learning.                                     distribution of features relating to some phenomenon
                                                                         (2) Observation of features in a novel experimental item
Learners are surrounded by statistical information.                      (3) Knowledge of which features are relevant for the
Considerable evidence suggests that they can make use of                     statistical computation
statistics to learn about their environment. For example,                (4) Bayesian computation to determine how to generalize
when acquiring artificial languages, children track                          the phenomenon in question to the novel instance
distributional cues that allow them to discover phonetic               (1) depends on the learner’s ability to observe and encode a
categories (Maye, Werker & Gerken, 2002), word                         statistical distribution of features pertaining to some
boundaries (Saffran, Newport & Aslin, 1996), grammatical               phenomenon. (2) is similar to (1), but refers to encoding
categories (Mintz, 2003; Reeder, Newport & Aslin 2009,                 these features given a situation where the learner will be
2010), grammatical dependencies (Gomez & Maye, 2005;                   performing a computation to classify or otherwise deal with
Saffran, 2001) and phrase structure (Takahashi, 2009). This            a novel instance. (3) requires the learner to know which
leads to a commonly held belief in the language acquisition            features are relevant for a computation and is by no means
literature that children are perfect statistical learners (e.g.        trivial, as not every feature related to every phenomenon is
Elman, Bates, Johnson Karmiloff-Smith, Parisi & Plunkett               relevant to the associated computation. (4) is an assumption
1996).                                                                 that we are making about the kind of computations that
   The hypothesis that children are perfect statistical learners       learners use distributional information for. While step (4) is
predicts that when tested on their ability to generalize               often assumed to be the culprit when learners show
aspects of their native language in an experimental setting,           suboptimal performance in experimental tasks, in principle
children’s linguistic knowledge should always reflect the              steps (1) through (3) can also contribute to suboptimal
distribution of statistical information in the input. However,         performance.
this is not always the case. Work by Hudson-Kam and                       Our case study on Tsez noun classification examines how
Newport (2009), for example, suggests that children are not            each of these pieces could result in a reshaping of the
perfectly veridical learners, in that they sometimes override          statistical information in the input. We begin with an
statistical patterns in the service of amplifying some other           outline of the distributional information that characterizes
facet of the language they are acquiring. As this work has             Tsez noun classes. We then compare children’s use of this
largely focused on artificial language learning, here we               information in classification with that of a naïve Bayesian
examine another type of non-veridical statistical learning             classifier. Finally, we explore three models that introduce
involving the acquisition of noun class (grammatical                   uncertainty in levels (1)-(3) from above, in an effort to
gender) in a natural language, Tsez. We present evidence               determine what underlies the difference between children’s
showing that children exhibit behavior that is inconsistent            performance and predictions made by the Bayesian model.
with the statistical information available in the input when
assigning novel nouns to noun classes. This inconsistent
                                                                   360

                      Tsez Noun Classes                                     highly predictive of some class and an unspecified value
                                                                            that ranges over all other possible values that were not
Many languages make use of subclasses of nouns, called
                                                                            predictive.
noun classes or grammatical gender. The presence and
number of noun classes, as well as the distribution of
                                                                                               Table 3: Structure of Features
individual nouns into classes varies greatly across
languages, but several features remain constant. All noun                     Feature          Specified Values        Unspecified Value
class systems exhibit some degree of distributional
                                                                              Semantic         male, female, animate   other
information both internal and external to the noun. Noun
internal distributional information consists of commonalities                 First segment    r-, b-                  other
among the nouns in a class, such as semantic or                               Last Segment     i                       other
phonological features. Noun external distributional
information is made up of class defining information that is                   In this paper we will focus on how children use noun
separate from the noun, such as agreement morphology that                   internal distributional information. In particular we will look
is contingent on noun class. We will look at noun class                     at whether a child can make use of the predictive
acquisition in Tsez as a case study.                                        phonological and semantic information when classifying
   Tsez, a Nakh-Dagestanian language spoken by about                        novel nouns, and how they perform when a noun has two
6000 people in the Northeast Caucasus, has four noun                        features that make conflicting predictions. Returning to the
classes. These classes can be characterized based on noun                   four components of statistical learning outlined above, we
external distributional information (e.g. prefixal agreement                will be looking at
on vowel initial verbs and adjectives) (Table 1), and noun                    (1) Whether Tsez children have knowledge of the noun
internal distributional information (semantic and                                    internal distributional information
morphophonological features on the nouns themselves)                          (2) Whether they can observe these features on novel nouns
(Table 2).                                                                    (3) Whether they assume all features are relevant for
                                                                                     classification
        Table 1: Noun External Distributional Information.                    (4) We assume for the purposes of our analysis that the
                                                                                   computation they make based on this information is
  Class 1          Class 2          Class 3          Class 4                       Bayesian.
  ∅-igu uži        j-igu kid        b-igu k’et’u     r-igu čorpa
  I-good boy       II-good girl     III-good cat     IV-good soup                       Classifying Novel Nouns in Tsez
  good boy         good girl        good cat         good soup              To assess whether children can use the statistics of noun
                                                                            internal information available in their input, we compare
         Table 2: Noun Internal Distributional Information                  classification of novel nouns by Tsez acquiring children to
                              (a selection)                                 the classification behavior that is predicted by a Bayesian
                                                                            model trained on the input data from our corpus. We
  Feature         Value     Class       % class with    % nouns with        describe the experimental data and the model in turn.
                            predicted   this feature    this value in
                                        value           predicted class
  Semantic        female    2           13              100                 Classification by Tsez Children
  Semantic        animate   3           22              100                 To determine whether or not children classified novel nouns
  First Segment   r-        4           9               61
                                                                            consistently with the predictions made by the probabilities
                                                                            associated with their noun internal features, 10 native Tsez
   Gagliardi and Lidz (under review) measured noun internal
                                                                            speaking children (mean: 6yrs, range: 4-7yrs) participated in
distributional information by taking all nouns from a corpus
                                                                            a classification task. Here we give an overview of the
of Tsez child directed speech, tagging them for potentially
                                                                            experiment; for further details, including adult data, see
relevant semantic and morphophonological cues and using
                                                                            Gagliardi and Lidz (under review).
decision tree modeling to determine which features were
most predictive of class (cf. Plaster, Harizanov & Polinsky,
                                                                            Method Children were presented with unfamiliar items
in press). The features shown in Table 2 are only a selection
                                                                            labeled with novel nouns by a native Tsez speaker. They
of the most predictive features of class, with only the most
                                                                            were instructed to first tell a character to begin eating and
predictive values of these features shown.1 The full structure
                                                                            then tell the character whether or not to eat the other labeled
of each feature that we assume in our model is given below
                                                                            items. As the both the intransitive (eat) and transitive (eat it)
in Table 3. Each feature has specified values that were
                                                                            forms for eat are vowel initial in Tsez (–iš and –ac’o
                                                                            respectively), classification of the novel word could be seen
   1
     Here we talk about ‘noun classes’ to refer what is often called
                                                                            on the agreement prefix. Furthermore, intransitive verbs in
grammatical gender. One of the cues to noun class is often natural          Tsez agree with the agent (the eater) and transitive verbs
gender, but this is only one of several cues, and many other nouns          agree with the patient (the thing eaten). An example trial is
are in each class that don’t have this (or potentially any) cue             schematized in Table 4.
predicting their class.
                                                                        361

  The test items had either a single noun internal                                                                                              feature that made conflicting predictions, children relied
distributional feature from Table 2, or a combination of                                                                                        more heavily on the phonological feature [r-] than on the
these features that made conflicting predictions (e.g.                                                                                          semantic feature. This is not likely to be predicted by the
semantic = [animate] and initial = [r]). The exact feature                                                                                      distribution of these features in the input, where nouns with
combinations used in this experiment, along with the classes                                                                                    the [animate] and [female] values of the semantic feature
each feature predicts, are shown in Table 5. While these                                                                                        never occur in Class 4.2
only represent a selection of the most predictive features, we
focus on them here as they are a representative set of                                                                                          Classification by an Optimal Bayesian Classifier
predictive semantic and phonological features.                                                                                                  Given these experimental data, we can evaluate whether
                                                                                                                                                children are optimally using the statistics in their input by
                                                             Table 4: Example Experimental Trial                                                examining how a Bayesian model would classify each novel
                                                                                                                                                noun. That is, what would an ideal learner, exposed to input
          Speaker Utterance                                                                      Action/Conclusion                              with these features, do when asked to classify novel words?
          Exper- kid                                                                             Points to girl on                                Our model is shown in Equation 1. The prior probability
          imenter girl(class2)                                                                   page                                           of a class p(c) corresponds to its frequency of occurrence,
                  girl                                                                                                                          and the likelihood terms p(f|c) for each of n independent
          Child   sis, q’ano, ɬono, j-iš     Tells kid to start                                                                                 features f can be computed from feature counts in the
                  one two three CL2-eat      eating using Class 2                                                                               lexicon.
                  One two three, Eat!        prefix j / kid is in                                                                                                            p( f1 | c ) p( f 2 | c )...p( f n | c ) p(c)          (1)
                                             Class 2                                                                                               p c | f1, f 2 ... f n =               (                     )
                                                                                                                                                                           ∑ p( f1 | c i ) p( f 2 | c i )...p( f n | c i ) p(c i )
          Exper- zamil                       Points to unfamiliar                                                                                                                                                       i
          imenter novel[animate]             animal and labels it                                                                               The results of classification with this model are shown in
                                             with the novel noun                                                                                Figure 2. Just as we did with children, we tested the model
                                             zamil                                                                                              on classification with each semantic and phonological
                                                                €
          Child   zamil b-ac’xosi aanu       Says whether or not                                                                                feature from Table 2 individually, as well as cases where
                  zamilCL3-eat-pres.part neg the girl is eating the                                                                             these features were in conflict with one another. As would
                  pro isn’t eating the zamil zamil using Class 3                                                                                be expected based on the relative strength of these features
                                             prefix b / zamil is in                                                                             (Table 2), when semantic and phonological features make
                                             Class 3                                                                                            conflicting predictions the model classifies in line with the
                                                                                                                                                predictions made by the semantic feature.
                                                Table 5: Features Used in Experiment and Simulations                                                                                                      Predicted Classification by an Optimal Naïve Bayes Model
                                                                                                                                                                                        1                                                                                             "
                                                                                                                                                 234%)!#3!"5!36&6#'#%7"3(")&89"8'&44
                                                                                                                                                                                                                                                               :'&441
          Feature                                                                Value              Class Predicted                                                                                                                                            :'&44-
                                                                                                                                                                                       +,0                                                                     :'&44=
          Semantic                                                               female             2                                                                                                                                                          :'&44.
          Semantic                                                               animate            3                                                                                  +,/
          First Segment                                                          r                  4                                                                                  +,.
          Semantic & First Segment                                               female & r         2 and 4
                                                                                                                                                                                       +,-
          Semantic & First Segment                                               animate & r        3 and 4
                                                                                                                                                                                        +"
                                                                                                                                                                                             !!"#$#%#&'            ()*&')        &$#*&%)       !!"#$#%#&'"()*&') !!"#$#%#&'"&$#*&%)
                                                                                                                                                                                                                                 :;)"<75)
 2!343!%#3$"3("$35$6"&66#7$)8"%3")&9:"9'&66
                                                                  Classification of Novel Nouns by Children
                                               1
                                                                                                                           ;'&661
                                                                                                                                      "
                                                                                                                                                  Figure 2: Predicted classification of novel nouns by an
                                              +,0
                                                                                                                           ;'&66-               optimal naïve Bayesian classifier
                                                                                                                           ;'&66>
                                                                                                                           ;'&66.
                                              +,/                                                                                               The model’s classification differs from that of the children
                                                                                                                                                in that when features made conflicting predictions the model
                                              +,.
                                                                                                                                                relied on the statistically strongest cue (the semantic
                                              +,-                                                                                               feature), while the children did not rely so heavily on this.
                                                                                                                                                                                             Predicting Suboptimal Performance
                                               +"
                                                    !!"#$#%#&'     ()*&')         &$#*&%)      !!"#$#%#&'"()*&') !!"#$#%#&'"&$#*&%)
                                                                                  ;5)"<=4)
  Figure 1: Proportion of novel nouns assigned to each class                                                                                    While children roughly align with the model when
(by cue type) in the experimental task                                                                                                          classifying based on one highly predictive feature, they
                                                                                                                                                diverge when features make conflicting predictions.
Results The proportion of nouns that children assigned to                                                                                       Children appear to use phonological features out of
each class are shown in Figure 2. When nouns had no                                                                                             proportion with their statistical reliability. That is, children
conflicting features, children assigned more nouns to the
class most strongly predicted by the feature than to any                                                                                                                        2
                                                                                                                                                     For a more detailed description of the results of the
other class. However, when nouns had more than one                                                                                              experiment, see Gagliardi & Lidz (under review).
                                                                                                                                          362

appear to prefer the weaker predictions made by the                    where Nc denotes the number if nouns in the class, Nc,f=k
phonological feature to the stronger ones made by the                  denotes the number of nouns in the class for which the
semantic feature. In order to determine the source of this             feature has value k, and K is the number of possible values
asymmetry it is useful to first consider the fundamental               for the feature.
differences between semantic and phonological features that              We introduce misrepresentation of semantic features into
could lead to this kind of behavior, and then to determine             this model by manipulating the number of observations of a
where and how these factors could affect our model.                    noun with a certain feature value in each class. Since the
  There are several differences between semantic and                   semantic incompetence hypothesis posits that children
phonological features that could affect their use in noun              misrepresent semantic feature values some proportion of the
classification, but here we will focus on a fundamental                time, we reduce the count of nouns in each class that contain
difference in how reliably perceived and encoded each                  the relevant semantic features, changing them instead to the
feature type may be during early acquisition. Every time a             unspecified feature value [other]. We then compute the
word is uttered (or most of the time, allowing for noisy               posterior probability of noun class membership using these
conditions and fast speech) phonological features are                  adjusted feature counts. We can use this model to ask how
present. However, especially during the early stages of                low the counts would have to be in order for children’s
lexical acquisition, the meaning of a word, and thus the               behavior to be optimal with respect to their beliefs.
associated semantic features, is much less likely to be                  We evaluated the model by comparing its behavior to
available or apparent. Below we will consider how this sort            children’s behavior from the classification task. The model
of asymmetry could lead to a disparity in the way children             produced a close fit to the data in each condition (Figure 3).
end up using them in novel noun classification.                        Furthermore, the estimated degree of misrepresentation was
                                                                       highly consistent across all semantic features and conflicting
Three Models of Uncertainty                                            feature combinations. The best fitting level of uncertainty
The difference between semantic and phonological features              ranged from 0.96-0.91, meaning that children would be only
could affect each of the three components from the schema              using 4-9% of the semantic cues available to them. A
of noun classification in different ways. In this section we           generalized likelihood ratio test in which the level of
will model each of these to see how building the asymmetry             misrepresentation was held constant across simulations
into each level changes the classification by the model.               (0.95) demonstrates that our semantic incompetence model
                                                                       significantly outperforms the optimal naïve Bayesian
Knowledge of Noun Internal Distributional Information                  classifier (p < 0.0001).
An asymmetry in the reliability with which semantic and
                                                                                                                    Predicted Classification with Deficient Knowledge of Noun Internal Information
phonological features of nouns are perceived and encoded                                                       1                                                                                           "
                                                                        234%)!#3!"5!36&6#'#%7"3(")&89"8'&44
                                                                                                                                                                                                :'&441
during word learning could lead to a disparity in the way                                                                                                                                       :'&44-
                                                                                                              +,0
phonological and semantic features are represented as                                                                                                                                           :'&44=
                                                                                                                                                                                                :'&44.
compared with how they are distributed in the input.                                                          +,/
   In our first manipulation (the Semantic Incompetence
                                                                                                              +,.
Hypothesis) we examined how classification by the model
would be affected if the learner was misrepresenting some                                                     +,-
proportion of the semantic features that they should have
                                                                                                               +"
encoded on nouns in their lexicon. We assume that learners                                                            !!"#$#%#&'       ()*&')         &$#*&%)
                                                                                                                                                      :;)"<75)
                                                                                                                                                                    !!"#$#%#&'"()*&') !!"#$#%#&'"&$#*&%)
represented the remaining proportion of nouns as predicted               Figure 3: Classification of novel nouns as predicted by a
(accurately observing features during the experiment and               Naïve Bayes Classifier with 95% of predictive semantic
assuming that both semantic and phonological features were             features misrepresented as [other].
relevant in classification). In doing this, we assume that
learners’ beliefs about which features are predictive of                  Although this model produces a close fit to the empirical
which class is built up as they observe different feature              data, it predicts an extremely high degree of misperception.
values on words belonging to different classes. One way of             To understand why this is the case, consider that using
quantifying this is by modeling the learner’s belief about the         likelihood terms for each class that are proportional to the
likelihood terms p(f|c) from Equation 1 under the
                                                                                                                                                N c, f =k
assumption that these beliefs are derived from the counts              true empirical counts                                                              would yield optimal noun
that a learner accumulates of nouns in each class that                                                                                           Nc
contain a given feature. We assume learners use a                      classification performance, regardless of the exact
multinomial model with a uniform Dirichlet prior                       proportion of time children are misrepresenting features.
distribution to estimate the proportion of items each class c          That is, substituting β*p(f1|c) for each term p(f1|c) in
that contain a particular value k for feature f. Under this            Equation 1,€where β is a constant denoting the degree of
assumption, each likelihood term is equal to:                          misperception, does not result in any change in the posterior
                                      N c, f = k + 1                   probability distribution. This analysis suggests that changes
                   p( f = k | c ) =                        (2)
                                       Nc + K                          in model predictions under this account of feature
                                                                 363
       €

    misrepresentation occur primarily for low empirical feature                                                                                             1
                                                                                                                                                                     Predicted Classification with Misobservation of Features on Experimental Items
                                                                                                                                                                                                                                                                    "
                                                                                              234%)!#3!"5!36&6#'#%7"3(")&89"8'&44
    counts, when the model relies heavily on pseudocounts from                                                                                                                                                                                           :'&441
                                                                                                                                                                                                                                                         :'&44-
    the Dirichlet prior distribution.                                                                                                 +,0                                                                                                                :'&44=
                                                                                                                                                                                                                                                         :'&44.
                                                                                                                                      +,/
    Observation of semantic and phonological features on
    novel nouns A second possibility is that children have little                                                                     +,.
    trouble perceiving, encoding and representing features on                                                                         +,-
    the words in their lexicon, but that the semantic features on
    the experimental items (as they are presented as flat pictures                                                                                          +"
                                                                                                                                                                       !!"#$#%#&'            ()*&')             &$#*&%)      !!"#$#%#&'"()*&') !!"#$#%#&'"&$#*&%)
    in a book) are unreliably perceived and encoded. We call                                                                                                                                                    :;)"<75)
                                                                                                Figure 4: Classification of novel nouns as predicted by a
    this the Experimental Reject Hypothesis.
                                                                                             model that misobserves semantic features on experimental
       In this manipulation we investigate what would happen if
                                                                                             items 58% of the time
    a learner had a lexicon that faithfully represented the
    predictive features as they were distributed in the input and
                                                                                             The crucial difference between this model and the
    assumed both semantic and phonological features were
                                                                                             experimental reject model is that in the experimental reject
    relevant to classification, but didn’t reliably encode
                                                                                             model semantic features are always used, but are encoded as
    semantic features on experimental items. To do this we use
                                                                                             the wrong value (the unspecified [other] value) some
    a mixture model, where some proportion of the time (1- β)
    an item that was supposed to have the specified semantic                                 proportion of the time, whereas in the phonological
    feature value [animate] or [female] (denoted as [spe]) it                                preference model, semantic features do not factor into the
                                                                                             calculation at all some proportion of the time (β). The model
    would be classified as with that value, the rest of the time
                                                                                             can be seen in Equation 4.
    (β) it would be classified as if it had the unspecified value
                                                                                                                          p( f1 = [sem] | c ) p( f 2 | c ) p(c)
    [other]. This yields the following model:                                                   p c | f1, f 2 = (1− β )                                      (                      )
                                   p( f1 = [spe] | c ) p( f 2 | c ) p(c)                                                ∑i p( f1 = [sem] | c i ) p( f 2 | c i ) p(c i ) (4)
          (           )
         p c | f1, f 2 = (1− β )
                                 ∑i p( f1 = [spe] | c i ) p( f 2 | c i ) p(c i ) (3)                                                                                                           p( f 2 | c ) p(c)
                                                                                                                                                                                        +β
                          +β
                                 p( f1 = [other] | c ) p( f 2 | c ) p(c)                                                                                                                     ∑ p( f
                                                                                                                                                                                               i       2   | c i ) p(c i )
                               ∑ p( f 1   = [other] | c i ) p( f 2 | c i ) p(c i )              Again we evaluated the model against the children’s
                                 i
                                                                                             classification data and found a close fit (Figure 5). The best
       As with the semantic incompetence model, we found the
                                                                                             fitting value of β ranged from .49 to .83, and was .65 over
    best-fitting value of β and evaluated the model €          by
                                                                                             all, meaning that children would be choosing not to use
    comparing it to children’s behavior. This model again
€                                                                                            semantic features on 65% of classification decisions. A
    produced a close fit for all feature values (Figure 4). The
                                                                                             generalized log likelihood test showed that this model also
    model showed a consistent degree of misperception across
    all semantic features and feature combinations. The best                                 significantly outperformed the optimal naïve Bayesian
    fitting level value of β ranged from .49 to .83, where 58%                               classifier (p < 0.0001)
    was the best fit overall. This means that children would be                                                                                                              Predicted Classification with Bias to Use Phonological Information
    misperceiving semantic features on 58% of the experimental                                                                                                  1                                                                                                       "
                                                                                                                      234%)!#3!"5!36&6#'#%7"3(")&89"8'&44
                                                                                                                                                                                                                                                :'&441
    items. A generalized likelihood ratio test indicates that the                                                                                           +,0
                                                                                                                                                                                                                                                :'&44-
                                                                                                                                                                                                                                                :'&44=
    experimental reject model also significantly outperforms the                                                                                                                                                                                :'&44.
    optimal naïve Bayesian classifier (p < 0.05).                                                                                                           +,/
                                                                                                                                                            +,.
    Assumption that all features are relevant for
    classification The asymmetry between the reliability of                                                                                                 +,-
    perceiving and encoding phonological as compared to                                                                                                         +"
    semantic features could also engender a bias to prefer                                                                                                               !!"#$#%#&'           ()*&')             &$#*&%)
                                                                                                                                                                                                                 :;)"<75)
                                                                                                                                                                                                                              !!"#$#%#&'"()*&') !!"#$#%#&'"&$#*&%)
    phonological information for classification decisions, as                                  Figure 5: Classification as predicted by a model biased
    phonological information has been reliably available for a                               not to use semantic information 65% of the time
    longer period of time. Our third model, embodying the
    Phonological Preference Hypothesis, therefore looked at
    what would happen if we had a learner that was biased not                                                                                                                                      Discussion
    to use semantic features in classification some proportion of
                                                                                               Tsez noun classes are characterized by both semantic and
    the time, even if these features were represented just as
                                                                                             phonological features. Children have been shown to be able
    distributed in the input and accurately perceived during the
                                                                                             to use these features when classifying novel nouns. Here we
    experimental task. We used a second mixture model, this
                                                                                             showed that their classification patterns differ from those of
    time looking at the mixture of a Bayesian classifier that used
                                                                                             an optimal Bayesian classifier when nouns have semantic
    both semantic and phonological features, and one that only
                                                                                             and phonological features that make conflicting predictions.
    used phonological features.
                                                                                       364

We also presented three models that take into account ways             demonstrated that while children’s behavior does not align
in which the difference between semantic and phonological              with the predictions made by the optimal Bayesian
features could lead to children’s apparent preference to use           classifier, it can be predicted by modifying the terms of this
the less reliable phonological features. These models                  classifier in reasonable ways. Thus we were able to model
examined how classification would look if a learner had (a)            children’s suboptimal behavior using a Bayesian model,
misrepresented semantic features in the lexicon, (b)                   rather than adopting some other system of computation.
misencoded semantic features during the classification                    Finally, our models showed that it is plausible that these
experiment, or (c) developed a bias to use phonological                children are indeed behaving optimally with respect to some
information in noun classification due to its higher                   statistical distribution, just not one directly measureable
reliability in the early stages of lexical acquisition. All three      from the input. This point is crucial as researchers extend
models fit children’s data significantly better than the               accounts of statistical learning to a greater range of
optimal naïve Bayesian classifier did. This suggests that              problems, highlighting the fact that the critical question isn’t
although originally children did not look as though they               whether or not children are using statistics to acquire
were behaving optimally with respect to the input, they may            language, but what statistics they are using.
well be behaving optimally with respect to their intake, that
is, the input as they have represented it.                             Acknowledgments This research was supported by NSF IGERT
   It is not obvious how one would best to evaluate the                0801465 and a NSF GRF to Gagliardi. We would like to thank
alternative models with respect to one another. For example,           Masha Polinsky, the UMD Cognitive Neuroscience of Language
                                                                       Lab, the UMD Project on Children’s Language Learning and the
each model yielded a different best-fit parameter,
                                                                       UMD Computational Psycholinguistics group for helpful
corresponding to a different degree of misrepresentation or            discussion and assistance.
bias. While these best fitting parameters may differ in terms
of their ‘reasonableness’ (i.e. misrepresenting 95% of                                             References
semantic features in the lexicon at age 6 seems quite high),           Elman, J. L., Bates, E. A., Johnson, M. H., Karmiloff-Smith, A.,
it isn’t immediately clear how to measure reasonableness, or              Parisi, D., & Plunkett, K. (1996). Rethinking innateness: A
how to compare it across models. Furthermore, it is likely                connectionist perspective on development. Cambridge, MA:
that a combination of all three of these processes (and                   MIT Press.
                                                                       Gagliardi, A., & Lidz, J. (Under review) Separating input from
perhaps more that we haven’t considered here) is
                                                                          intake: Acquiring noun classes in Tsez.
influencing children’s classification decisions. This could            Gómez, R.L., & Maye, J. (2005). The Developmental Trajectory of
potentially be explored through a combined model;                         Nonadjacent Dependency Learning. Infancy, 7, 183–206.
however, as all of these models fit the data so closely, it            Hudson Kam, C.L., & Newport, E.L. (2009). Getting it right by
would be difficult to determine which and to what extent                  getting it wrong: When learners change languages. Cognitive
each type of misrepresentation or bias is involved.                       Psychology, 59, 30–66.
   This work has several important implications for research           Mintz, T.H. (2003). Frequent frames as a cue for grammatical
statistical learning and language acquisition. First, and most            categories in child directed speech. Cognition. 90, 91–117.
broadly, by combining experimental data from children                  Maye, J., Werker, J. F., & Gerken, L. (2002). Infant sensitivity to
                                                                          distributional information can affect phonetic discrimination.
acquiring an understudied language with computational
                                                                          Cognition, 82, B101–B111.
modeling techniques, we found a better understanding of                Plaster, K., Polinsky, M., & Harizanov, B. (In Press). Noun
both children’s acquisition of Tsez, and the role of statistical          Classes Grow on Trees: Noun Classification in the North-East
cues in language acquisition. Tsez was an ideal language to               Caucasus. Language and representations (tentative). John
look at, as feature types differed in their reliability as cues to        Benjamins
noun class. However, we expect that these results will be              Reeder, P.A., Newport, E.L., & Aslin, R.N. (2009). The role of
generalizable across languages, as the relative difficulty of             distributional information in linguistic category formation. In N.
acquiring semantic, as compared to phonological, features                 Taatgen and H. van Rijn (eds), Proceedings of the 31st Annual
of words will be consistent cross linguistically.                         Conference of the Cognitive Science Society. Austin, TX:
                                                                          Cognitive Science Society.
   Second, we identified an area where children’s behavior
                                                                       Reeder, P.A., Newport, E.L., & Aslin, R.N. (2010). Novel words in
does not appear to reflect the ideal inferences licensed by               novel contexts: The role of distributional information in form-
the statistical patterns in the input. Three models allowed us            class category learning. In S. Ohlsson & R. Catrambone (Eds.),
to investigate the source of this asymmetry. While each                   Proceedings of the 32nd Annual Conference of the Cognitive
model differed in where the asymmetry came from, all                      Science Society. Austin, TX: Cognitive Science Society.
employed a weakening of the statistical import of semantic             Saffran, J. R., Newport, E. L., & Aslin, R. N. (1996). Word
features. This is a distinct pattern from the finding that                segmentation: The role of distributional cues. Journal of
children learning an artificial language amplify an already               Memory and Language, 35, 606–621.
strong statistical tendency (Hudson-Kam & Newport, 2009).              Saffran, J.R. (2001). The use of predictive dependencies in
                                                                          language learning. Journal of Memory and Language, 44, 493–
Further research will determine whether or not these
                                                                          515.
patterns could be in some way related.                                 Takahashi, E. (2009). Beyond statistical learning in the acquisition
   Next, we showed that it is possible for a learner to be                of phrase structure. College Park, MD: University of Maryland
suboptimal and Bayesian at the same time. That is, we                     dissertation.
                                                                   365

