UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Generating Realistic Semantic Codes for Use in Neural Network Models
Permalink
https://escholarship.org/uc/item/9j14z1rq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Chang, Ya-Ning
Furber, Steve
Welbourne, Stephen
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

           Generating Realistic Semantic Codes for Use in Neural Network Models
                               Ya-Ning Chang (ya-ning.chang@postgrad.manchester.ac.uk)
                           Neuroscience and Aphasia Research Unit (NARU), University of Manchester,
                                             Brunswick Street, Manchester, M13 9PL,UK
                                        Steve Furber (steve.furber@manchester.ac.uk)
         Advanced Processor Technologies Group, University of Manchester, Oxford Road, Manchester, M13 9PL, UK
                                Stephen Welbourne (stephen.welbourne@manchester.ac.uk)
                           Neuroscience and Aphasia Research Unit (NARU), University of Manchester,
                                             Brunswick Street, Manchester, M13 9PL,UK
                              Abstract                              considered essential for sophisticated and efficient
                                                                    simulation using a connectionist model includes: (1) Binary
  Many psychologically interesting tasks (e.g., reading, lexical
  decision, semantic categorisation and synonym judgement)          coding: a binary coding scheme is essential for use in
  require the manipulation of semantic representations. To          connectionist models because the models consist of many
  produce a good computational model of these tasks, it is          neuron-like units whose activation values vary between 0
  important to represent semantic information in a realistic        and 1. The models are trained to match their activation
  manner. This paper aimed to find a method for generating          values to predefined targets, which need to be at the extreme
  artificial semantic codes, which would be suitable for            ends of the possible activations; (2) Sparse coding: a sparse
  modelling semantic knowledge. The desired computational
  criteria for semantic representations included: (1) binary
                                                                    coding scheme is one in which an item is represented by
  coding; (2) sparse coding; (3) fixed number of active units in    using a small number of active units in each vector. A
  a semantic vector; (4) scalable semantic vectors and (5)          sparse representation is attractive from a computational
  preservation of realistic internal semantic structure. Several    viewpoint because it allows efficient computation. By
  existing methods for generating semantic representations          controlling sparseness, the redundancy of a code can be
  were evaluated against the criteria. The correlated occurrence    minimized and learning is generally fast and relatively easy.
  analogue to the lexical semantics (COALS) system (Rohde,          Importantly, it is likely to reflect the natural structure of the
  Gonnerman & Plaut, 2006) was selected as the most suitable
  candidate because it satisfied most of the desired criteria.      representation system in the brain; (3) Fixed number of
  Semantic vectors generated from the COALS system were             active units in each vector: the idea of using a fixed number
  converted into binary representations and assessed on their       of active units in each semantic vector is not common in
  ability to reproduce human semantic category judgements           most existing coding schemes. However, it has an advantage
  using stimuli from a previous study (Garrard, Lambon Ralph,       that this coding is uniform and it makes sense to think about
  Hodges & Patterson, 2001). Intriguingly the best performing       how similar items are by measuring the Euclidian distance
  sets of semantic vectors included 5 positive features and 15
                                                                    between them – if items have different numbers of features
  negative features. Positive features are elements that encode
  the likely presence of a particular attribute whereas negative    then measuring Euclidian distance does not give a good
  features encode its absence. These results suggest that           indication of similarity (Furber, Bainbridge, Cumpstey &
  including both positive and negative attributes generates a       Temple, 2004). For connectionist models, there is a
  better category structure than the more traditional method of     particular reason to want to adopt a fixed number of active
  selecting only positive attributes.                               units, which is that only the active units can contribute to
   Keywords: semantics; semantic representations; neural            activation in later layers. Units with a zero level of
   networks; computational modelling; connectionist models.         activation do not propagate information in the network and
                                                                    therefore do not generate any weight updates in response to
                         Introduction                               the error signal; (4) Scalable semantic vectors: to keep the
Computational models are frequently used to simulate                simulations computationally tractable, it is important to
human behavioural data and help understand the underlying           keep the size of semantic vectors manageable. Vector size is
cognitive processes. Any type of computational model                an important design consideration because it determines
requires decisions to be made about what representation             how many units in the model are needed for modelling
scheme to use. Semantic representations are particularly            semantic knowledge. Given a code length n, it has a
important for models of many linguistic processes including         maximum theoretical number of items that it can code for,
spoken and written language.This paper aims to find a               which is 2n. The capacity increases dramatically as the code
method of generating semantic representations, which can            length grows. Thus, the selection of the vector length also
fulfil a set of requirements derived from the constraints           needs to consider the number of items to be represented; (5)
imposed by incorporating semantic knowledge within a                Preservation of inherent semantic structure: the most
large-scale connectionist model. A list of criteria that we         important criterion is that the semantic vectors can support
                                                                    human-like semantic classifications. They need to preserve
                                                                  198

the inherent semantic structure of the lexicon. Words which      coding scheme is good for producing coarsely structured
are semantically similar should be represented by vectors        semantic representations, it is not easily scalable and it
that are relatively close in the semantic space; by contrast,    would be very difficult to generate an artificial semantic
semantically unrelated words should tend to be far from          structure that can capture the complexity found in human
each other. Preserving these semantic relationships will         semantics.
allow for the possibility of modelling tasks like
categorization and synonym judgment, which are commonly          Co-occurrence Statistics Semantic representations can also
used to probe semantic effects.                                  be derived from very large text corpora by evaluating which
                                                                 words appear in similar types of documents or co-occur
Review of existing semantic representation schemes               within a fixed window. Several semantic representation
Several semantic representation schemes have been                schemes have been developed on the basis of this statistical
proposed either for behavioural studies or for use in            co-occurrence including Latent Semantic Analysis (LSA)
computational modelling (Dilkina, McClelland & Plaut,            (e.g., Landauer, Foltz & Laham, 1998), Hyperspace
2008; Harm & Seidenberg, 2004; Plaut, 1997; Rogers,              Analogue to Language (HAL) (e.g., Lund & Burgess, 1996)
Lambon Ralph, Garrard, Bozeat, McClelland, Hodges &              and Correlated Occurrence Analogue to Lexical Semantics
Patterson, 2004). These schemes are based on different           (COALS) (Rohde et al., 2006). These methods are all based
techniques and there does not seem to be a consensus view        on similar ideas but they are slightly different in the ways
as to how to produce a set of representations. It is therefore   they collect data and deal with the high-dimensional co-
important to review these competing coding schemes from a        occurrence matrices. LSA derives vectors based on a
modelling perspective using the criteria described above.        collection of segmented documents in which the number of
                                                                 occurrences of a word in various types of documents is
Feature Norms One traditional method is to obtain the            computed as an element in the high-dimensional co-
feature norms through experiments (e.g., McRae, Cree,            occurrence matrix. The dimensionality of the matrix is then
Seidenberg & McNorgan, 2005). In these experiments,              reduced by using Singular Value Decomposition (SVD)
subjects are given a list of words and asked to write down       while preserving the semantic relations between words as
attributes about each word. To categorise the attributes and     much as possible. Unlike LSA, the derivations in both HAL
make proper constraints on subjects’ responses, some             and COALS are based on words co-occurring within a fixed
lexical relations such as “is” and “has” are used to prompt      window in an un-segmented document. The key differences
subjects to list the features of the stimulus word. The most     between these three systems are in their ways of expressing
commonly listed features for a particular word are then          the tendency of two words to co-occur: LSA computes the
considered as the core semantic attributes for that word. The    cosines between the vectors of two words, HAL uses
collected attributes for an item can be easily converted into    distance measure and COALS uses the correlation measure.
binary codes with the presence of an attribute coded as “1”      In addition, HAL reduces the dimensionality of the matrices
and the absence as “0”. Controlling for sparseness is not so     by eliminating all but the few thousand columns with the
easy, but it may be possible to rank the features by the         largest variant values, which is different from the SVD
number of subjects that identified them, and use this as a       technique adopted by both LSA and COALS (see Rohde et
method for deciding which features to drop. Moreover, this       al., 2006 for more detailed comparisons).
method is not very flexible and practically can only be used        The semantic vectors generated by reducing a high-
for a small set of words.                                        dimensional matrix are typically real-value vectors but
                                                                 COALS also provides binary-valued vectors. For the other
Arbitrary Features Another way to generate semantic              two systems, however, it is relatively easy to convert the
representations is to use random features. Features for a        real values to binary values by thresholding. The vectors
word are assigned randomly but the assignments may still         with values greater than a certain level are replaced with the
respect broader aspects of semantic knowledge such as            value “1” and all others are replaced with the value “0”.
category knowledge. For example, the words within the            Sparse coding can be enforced by adjusting the threshold
same category can be designed to share more features than        level used when converting real-value vectors into binary.
words belonging to different categories. This method has         Similarly, the fixed number of active units in a vector can be
been applied to various computational studies designed to        designed by modellers during the binarization process by
capture abstract semantic properties including simulations       restricting the number of 1’s to the top n elements of the
of lexical decision (e.g., Plaut, 1997) and semantic             vector. By using the co-occurrence statistics, the sizes of
impairment (e.g., Rogers et al., 2004). The features for an      semantic vectors for lexical items are scalable, which is
item are assigned manually and are binary codes. The             particularly suitable for the computational modellers
control of sparseness can be achieved by adjusting the           seeking a set of representations with low computational cost.
fraction of the number of active units in a vector over the      The key advantage of this scheme is that it should be able to
code length. The fixed number of active units in a vector is     generate realistic semantic codes for any word lists of any
also controllable. In addition, the size of vector length is     length provided that the latent semantic information
scalable and determined by the modellers. Although this          contained in the structure of large corpora is sufficiently
                                                               199

detailed and can be extracted efficiently.                        vector from the SVD. This means that information
                                                                  contained in negative parts of the vector is lost. Thus the
WordNet WordNet is an online semantic database, which             questions asked here are whether negative components also
was developed by Miller in 1990. Information in WordNet           contribute to a good semantic similarity structure and, if so,
is organised by many synonymous sets. These sets are              what is the optimum number of positive and negative
linked by their lexical relations such as “is a”or “is part of”   features required to produce a best fit to human data. The
relations. A unique feature of WordNet is that it provides        following sections describe how to generate semantic
multiple word senses, which can be obtained from the              vectors based on COALS in a way that satisfies all the
database separately while other semantic systems do not           criteria discussed previously. We then go on to compare the
distinguish between word senses. Similar to the feature           performance of the vectors on a semantic categorisation task
norms, the attributes generated from WordNet have direct          using human data taken from Garrard et al.’s (2001)
semantic interpretations. The semantic vectors generated by       categorisation study.
WordNet are binary to represent the presence and the
absence of attributes, and generally rather sparse. But the       Generating Semantic Vectors based on COALS
number of semantic features for each word is not fixed and        To explore whether negative components were as important
the range could be very wide. The size of the semantic            as positive components, a binarization process of coding
vectors is less flexible because the size depends on how          both positive and negative components were used. A 100-
words relate to each other within the word list of interest. As   dimensional semantic vector was generated for all items in
a general rule the longer the list of lexical items to be coded   the Garrard et al.’s (2001) word list except one item
the longer resultant semantic vector. Since the vectors are,      “watering can”, which was discarded because the system did
directly derived from many synonymous sets in Word Net            not support the compound words. The 100-dimensional
based on the researchers’ semantic knowledge, the semantic        vector was duplicated to create two 100-dimensional vectors
structure is likely to be well preserved.                         with the first 100 dimensions coding the positive elements
Table 1 summarises the results of these evaluations. Among        and the second 100 dimensions coding the negative
these, COALS appears to be the best choice because it             elements. The two 100-dimensional vectors were combined
satisfied most of the criteria than other systems.                into a 200-dimensional semantic vector. The first half of the
                                                                  200-dimensional vector contained only the positive
Table 1: Summary of the evaluations of different semantic         components and the second half of the 200-dimensional
representation schemes                                            vector contained only the absolute values of negative
                                                                  components. Assuming the best number of positive and
                                                                  negative components is n and m respectively then the top n
                                                                  values of the first half of the combined vector and the top m
                                                                  values of the second half of the vector were selected as the
                                                                  key features. All the selected features were set to 1 and
                                                                  others are 0. This resulted in a 200-dimensional binary
                                                                  vector with the property that matching elements from the
                                                                  two halves of this vector (e.g., the 1st and 101st elements)
                                                                  code for the same feature and have a special relationship
                                                                  whereby if one is on then the other must be off. (Note: both
                                                                  paired vectors can be off indicating that neither the presence
                                                                  nor the absence of this feature is particularly important to
                                                                  the meaning of the item.)
                           Method
                                                                  Testing Procedure
The correlated occurrence analogue to the lexical semantics
(COALS) system (Rohde et al., 2006) is designed to be very        To determine the usefulness of the binary semantic codes
flexible. Although two of the criteria (i.e., sparse coding and   we examined the relationship between the category
fixed number of active units) are outside the scope of the        structures derived from the artificially generated sets of
original COALS system, they could be easily achieved by           semantic vectors and human data taken from Garrard et al.’s
manipulating the semantic vectors generated from the              (2001) study. In their study, Garrard and colleagues asked
system. However, it is crucial to examine whether the             subjects to categorise items into a living thing group and
semantic codes generated from COALS preserve enough               nonliving thing group. On a finer scale, the living thing
semantic structure that they can be used to predict the           group can be divided into animals, birds and fruit and the
human semantic data. In addition it is important to               nonliving thing group also can be subdivided into household
investigate the best method of transforming the COALS             objects, tools and vehicles. There were in total 6 subgroups.
vectors into binary codes. To generate binary vectors Rohde       Semantic vectors for 61 items in the Garrard et al.’s (2001)
and colleagues simply set negative components to 0 and            list were generated using the method described above. Each
positive components to 1 based on the original real-valued        vector had a length of 200. The n features of the first half of
                                                                200

the semantic vector represent the important positive features    active features (t). To find out what were the optimum
and the m features of the second half of the semantic vector     numbers of n and m, 24 different combinations of positive
indicate the important negative features. The numbers of         and negative features were assessed by using the three
positive features (n) and negative features (m) were varied      indices: DVI, DR and ARI. The evaluations were performed
to determine the optimum values of n and m.                      in two steps. The first step was to compare different sets of
                                                                 semantic vectors based on the predefined categories by
Evaluation of semantic vectors                                   choosing the combinations with a DVI of 6 and using the
Two parameters based on semantic distances between words         DR score to select the top candidates within groups with the
can be used to evaluate the match with the semantic              same number of features. The second step was to use the
structure in the human data: distance validity index (DVI)       ARI score as an independent additional test to confirm that
and distance ratio (DR). DVI counts the number of groups         the candidate with the highest ARI was also one of the
where the within group distance (i.e., the averaged              possible candidates from the first step.
Euclidean distance between items in the same group) is
smaller than all the between group distances (i.e., the                                     Results
averaged Euclidean distance between items in the different
groups). The larger the value of DVI the better the semantic     Searching the best semantic vectors
categories have been partitioned. This is rather coarse          Table 2 shows the results of the six candidates from 24 sets
measure of semantic structure and for this data the value of     of semantic codes with different combinations of positive
DVI ranges from 1 to 6 (i.e., the number of subgroups). The      and negative features, in which they had the maximum
expected best value is DVI=6 indicating that all the within      possible number of DVI (6). The set with 5 positive and 15
group distances are smaller than between group distances.        negative features (ID 2) had the largest value of ARI. This
DR computes the average of all the distance ratios. The          set was one of those with the maximum value of DVI and
distance ratio is the sum of between group distances to the      the value of DR for this set was also larger than that for
sum of within group distances. Ideally there will be a larger    other candidate sets with the same total number of features.
between group distance and a smaller within group distance       These results suggest that for this application a set of
so that DR should be as large as possible. It should be noted    semantic vectors with 5 positive features and 15 negative
that the value of DR is also positively correlated with the      features best captures the semantic categories generated
total number of features within a vector because it is           from human data. It is also interesting to note that among
computed on the basis of the Euclidean distance. The             the top candidates ID 1 was the only one with only positive
distance for the vectors having more features is generally       features and its ARI was much lower than that for all the
larger than that for the vectors having less features. This      other possible candidates. The differences between ARI for
indicates that DR is only a useful comparator for code sets      the top candidate and for the two other candidates (ID 3 and
with the same number of features.
                                                                 ID 5) which included both positive and negative features
   Thus far we have tacitly assumed that the subgroups will      were relatively small, suggesting that the exact number of
be exactly the same as those from human data. However,           positive and negative features may not be critical. However
even if a set of semantic codes can be shown to have a DVI
                                                                 the majority of candidate codes (4/6) did have more
of 6 and a high DR, it cannot be guaranteed that all its items
                                                                 negative than positive features.
would actually be categorized into the correct groups based
solely on their intrinsic correlations. To evaluate this we      Table 2: Results of searching the best semantic vectors
needed to test whether the clustering results based on the
intrinsic correlations among semantic vectors were similar
to semantic categories from human data. We tested this by
using the adjusted rand index (ARI) (Hubert & Arabie,
1985). ARI is commonly used to measure the similarity
between two different ways of partitioning a set of items. To
compare the partitions of human data and the artificial
semantic codes, ARI counts the number of agreements and
disagreements between them. It ranges from 0 to 1, with 0
indicating the two partitions are completely different and
with 1 indicating the two partitions are exactly the same.
   All three indices (DVI, DR and ARI) were used to
evaluate the semantic vectors. The maximum number of
active features including positive and negative in a semantic      To further test the significance of including negative
vector was set to 40 and the minimum was 10. Thus, the           codes, 20 sets of semantic codes for each of three groups
population sparseness ranged from 0.05 to 0.2. The numbers       (positive, neutral and negative-biased) were generated. Each
of positive (n) and negative (m) features varied in a            set had the same number of features, ranging from 10 to 48.
complementary manner which was dependent on the total            In the positive group, the vector included only positive
                                                               201

features whereas the vector in the neutral group had an            The y axis shows the Jaccard’s distance, a measure of
equal number of positive and negative features. In the             similarity between words. The lower the value the more
negative-biased group, the vector had more negative than           similar the clusters are. The semantic vectors can accurately
positive features with a ratio of 3:1. One-tailed paired t-tests   represent the semantic categories at a coarse scale, which
were conducted to compare both the neutral and negative-           means that the living things and nonliving things are well
biased groups to the positive group, where the three indices       separated. To compare the clustering results with human
were used as dependent variables. As predicted, the DVIs           data collected by Garrard et al. (2001), the items in Figure 1
and ARIs of the neutral and negative-biased groups were            were coloured according to its category in the human
significantly higher than that for the positive group (Table       semantic data. This clearly shows whether the clustering
3). For the DRs, the difference between the negative-biased        results were consistent with human categories. Ideally,
and the positive groups was not significant while there was        items with the same colour would be clustered together,
a significantly lower mean DR for the neutral group than for       indicating the items are clustered into the same group as in
the positive group. The comparison between the negative-           the human category. Most of the items are correctly
bias and neutral groups showed that both DVI and DR were           clustered. However there are a few interesting exceptions,
higher for the negative-biased group than for the neutral          for example, the word “chicken” was clustered into the fruit
group and there was no difference in their ARIs. Overall the       category (items coloured in purple) based on the artificial
results demonstrated the negative-biased group was superior        semantic codes, while it should have been clustered into the
to both the positive and neutral groups, confirming that the       bird category (Orange). Presumably this is because in many
inclusion of negative codes is important to capturing the          texts the word “chicken” might more frequently co-occur
way semantic knowledge is represented in humans.                   with other food (including fruit) in the kitchen context so
                                                                   this category might be more accurately described as food.
Table 3: Results of Significance Tests                             Within the nonliving things, it appears that the broader
                                                                   category of tool is well distinguished from the vehicle group
                                                                   but the boundaries between tools and household items is
                                                                   less clear. It is likely that most of the tools and household
                                                                   objects tend to occur in a similar context in the text so it
                                                                   would be difficult to differentiate them in a fine scale by
                                                                   using the co-occurrence statistic approach.
                                                                                       General Discussion
                                                                   Several schemes for generating semantic codes have been
                                                                   reviewed in this paper with a focus on the requirements of
                                                                   computational modelling. The primary aim was to
                                                                   determine an appropriate system for representing semantic
                                                                   knowledge, which could be used for a large-scale
Hierarchical Clustering Analysis                                   computational modelling of semantic related tasks.
Figure 1 shows the hierarchical clustering based on the
optimum vectors indicated in Table 2.
    Figure 1: Hierarchical cluster analysis of 61 words based on the 200-dimensional semantic vectors with 5 positive and 15
  negative features. (Items coloured base on human categories: Animal (Red), Birds (Orange), Fruit (Purple), Tool (Green),
                                             H’hold (Dark Blue),Vehicles (Purple)).
                                                                 202

The desired computational criteria were as follows: (1)            list used to train more sophisticated computational models.
binary coding; (2) sparse coding; (3) fixed number of active
units in a vector; (4) scalable vectors; (5) preservation of                           Acknowledgments
inherent semantic structure. The COALS system (Rohde et            This research was supported by grants under the Cognitive
al., 2006) provided the best fit to the criteria. The original     Foresight Initiative (jointly funded by EPSRC, MRC and
COALS system discretizes the real-valued vectors based             BBSRC - EP/F03430X/1) and the Neuroscience Research
only on the positive components. However, we evaluated             Institute at the University of Manchester.
codes with varying numbers of positive and negative
features by comparing the semantic categories generated
from the artificial semantic codes with human category data                                 References
from Garrard et al.’s (2001) study. The results showed that a      Dilkina, K., McClelland, J. L. &Plaut, D. C. (2008).A
set of semantic vectors having 5 positive and 15 negative            single-system account of semantic and lexical deficits in
features could best account for the human semantic                   five        semantic       dementia       patients.Cognitive
categories. It was perhaps surprising that the best binary           Neuropsychology, 25(2), 136-164.
vectors found here had more negative features than positive        Furber, S. B., Bainbridge, W. J., Cumpstey, J. M. & Temple,
features (i.e., 15 negative features v.s. 5 positive features).      S. (2004). A sparse distributed memory based upon n-of-
This is at variance with the prevalent assumption that only          m codes. Neural Networks, 17.
positive components should be used to construct semantic           Garrard, P., Lambon Ralph, M. A., Hodges, J. R. &
codes. Positive features reveal what attributes are likely to        Patterson, K. (2001).Prototypicality, distinctiveness, and
be present; in contrast, the negative features provide               intercorrelation: Analyses of the semantic attributes of
information about what attributes are likely to be absent. So        living and nonliving concepts. Cognitive Neuropsychology,
this result implies that knowledge of the absence of features        18(2), 125-174.
(e.g., knowing that washing machines cannot walk) is as            Harm, M. W. (2002). Building large scale distributed
important as knowledge of positive features. Given both of           semantic feature sets with wordnet. Pittsburgh, Carnegie
these types of information, it is possible to separate               Mellon University.
categories on the basis of their distance in the semantic          Harm, M. W. & Seidenberg, M. S. (2004). Computing the
space, as the optimisation results shown in Table 2. Further         meanings of words in reading: Cooperative division of
significance tests reveal that there was a clear trend for           labor        between      visual      and      phonological
semantic vectors containing both positive and negative               processes.Psychological Review, 111(3), 662-720.
features to show a more human-like semantic structure,             Hubert, L. &Arabie, P. (1985).Comparing partitions.Journal
suggesting that this may be a generally applicable principle.        of Classification, 2(1), 193-218.
Overall the best performing set of semantic vectors matched        Landauer, T. K., Foltz, P. W. &Laham, D. (1998).An
the human categorisation data quite well. Further work may           introduction to latent semantic analysis.Discourse
be conducted to investigate the performance of the present           Processes, 25(2-3), 259-284.
semantic codes on other types of semantic tasks (e.g.,             Lund, K. & Burgess, C. (1996).Producing high-dimensional
synonym judgement) for a more complete evaluation.                   semantic spaces from lexical co-occurrence.Behavior
   In addition, there are some inherent limitations of using         Research Methods Instruments & Computers, 28(2), 203-
these semantic vectors. The first is that the semantic features      208.
are not interpretable because they only encode the semantic        McRae, K., Cree, G. S., Seidenberg, M. S. &McNorgan, C.
regularities among word meanings. What exactly the feature           (2005).Semantic feature production norms for a large set
represents is difficult to interpret. But this is only a problem     of living and nonliving things.Behavior Research Methods,
in applications where a direct interpretation of features is         37(4), 547-559.
required. Another limitation is that it can generate good          Miller, G. A. (1990). Introduction to wordnet: An on-line
semantic vectors for most of the uninflected words but it            lexical database*. International Journal of Lexicography,
could be difficult to properly account for the deeper                3(4), 235.
meaning of words like morphological regularities (e.g.,            Plaut, D. C. (1997). Structure and function in the lexical
bake/baker) (Harm, 2002), which would require some                   system: Insights from distributed models of word reading
additional coding.                                                   and lexical decision. Language and Cognitive Processes,
   To summarise, a novel semantic representation scheme              12(5-6), 765-805.
was produced based on modifications to the COALS system.           Rogers, T. T., Lambon Ralph, M. A., Garrard, P., Bozeat, S.,
This was evaluated against human categorisation data, and            McClelland, J. L., Hodges, J. R. & Patterson, K. (2004).
the resultant coding scheme was able to reproduce the                Structure and deterioration of semantic memory: A
human data quite closely. The key finding was that the               neuropsychological and computational investigation.
negative features, which indicate what attributes definitely         Psychological Review, 111(1), 205-235.
do not belong to a lexical item, were at least as important as     Rohde, D. L. T., Gonnerman, L. &Plaut, D. C. (2006). An
the positive features. The semantic system developed here            improved model of semantic similarity based on lexical
can be applied to generate semantic codes for a larger word          co-occurrence. COMMUNICATIONS OF THE ACM, 8,
                                                                     627-633.
                                                                 203

