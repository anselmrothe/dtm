UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
State-Trace Analysis of Sequence Learning by Simple Recurrent Networks
Permalink
https://escholarship.org/uc/item/1tg795zn
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Yeates, Fayme
Jones, Fergal
Wills, Andy
et al.
Publication Date
2012-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                State-Trace Analysis of Sequence Learning by Recurrent Networks
                    Fayme Yeates1 (fy212@exeter.ac.uk) Andy Wills1 (A.J.Wills@exeter.ac.uk)
      Fergal Jones2 (Fergal.Jones@canterbury.ac.uk)                          Ian McLaren1 (I.P.L.McLaren@exeter.ac.uk)
                                       1
                                         School of Psychology, College of Life and Environmental
                                                  Sciences, University of Exeter, UK.
                                   2
                                     School of Psychology, Canterbury Christ Church University, UK.
                             Abstract                                     evaluate the different types of model and as a means of
                                                                          evaluating the state-trace methodology itself.
This study investigated the use of state-trace analysis (Bamber,
1979) when applied to computational models of human learning.               The computational models chosen for this analysis are the
We aimed to investigate the performance of simple recurrent               simple recurrent network (SRN) introduced by Elman
networks (SRNs) on a sequence learning task. Elman’s (1990)               (1990) and the Augmented SRN (Cleeremans and
SRN and Cleeremans & McClelland’s (1991) Augmented SRN are                McClelland, 1991). The basic SRN model is simple (see
both benchmark models of human sequence learning. The                     Figure 1), involving feed-forward input activation through a
differences between these models, comprising of an additional             hidden layer. The activations of this hidden layer are copied
learning parameter and the use of response units activated by             back on each trial into a context layer, which is then fed
output units constituted our main manipulation. The results are           back into the hidden layer as input on the next trial. This
presented as a state-trace analysis, which demonstrates that the
                                                                          ensures that the representations of the previous trial are
addition of an additional type of weight component, and response
units to a SRN produces multi-dimensional state-trace plots.              carried over, and gives the model the ability to learn
However, varying the learning rate parameter of the SRN also              sequential information.
produced two functions on a state-trace plot, suggesting that state-
trace analysis may be sensitive to variation within a single process.
Keywords: Learning; state-trace analysis; SRN; sequence
learning; Augmented SRN;
                        Introduction
State-trace analysis (Bamber, 1979) is a method that aims to
establish whether one or more underlying processes are
influencing behavior on a given task. The method has been
applied to a variety of paradigms, including remember-
know tasks (e.g. Dunn, 2008), face recognition (e.g. Loftus,
Oberg, & Dillon 2004), categorization (e.g. Newell, Dunn &
Kalish, 2010) and a variety of other areas (see Prince,
Brown & Heathcote, 2011).
   The procedure for a state-trace analysis is to plot the                Figure 1: The architecture of the SRN (Elman, 1990).
relationship between two dependent variables (dimensions)
on two or more tasks (states). If these points follow a single,             Cleeremans and McClelland (1991) further developed the
monotonic function, it can be hypothesized that the same                  SRN in order to give a better account of the sequential
latent variable underlies performance on the tasks. The                   effects demonstrated by human participants. This
influence of more than one latent variable on the tasks is                augmented simple recurrent network (AugSRN) differs
implied when the state plots do not follow the same                       from Elman’s (1990) original architecture firstly in the
function, i.e. more than one monotonic function is                        inclusion of response units post-output. As a consequence,
visualized.                                                               when making a response to the stimuli in an experiment,
   Computational models are created in the full knowledge                 this response remains primed over future trials for a short
of the processes involved in their construction. Thus, the                time (Remington, 1969), because the response units are
primary use of state-trace analysis, to attempt to quantify               activated by the output units and feed activation back into
latent psychological variables, does not seem to directly                 the output units on the next trial.
lend itself to computational modeling. But the fact that we                 The AugSRN also accounts for the priming of certain
should be able to make some predictions from the nature of                sequential pairings (Cleeremans & McClelland, 1991) by
the models about the types of processes involved in any                   assuming that back propagation is implemented on not one
simulation helps us interpret any state-trace analysis of the             set of connection weights, but two. One component is a set
data produced by the simulation. This paper seeks to apply                of slow weights, which produce small but permanent
state-trace analysis to the simulation results produced by                changes with minimal decay. These are complemented by
computational models on a sequence learning task both to                  fast weights, which have a higher learning rate but also a
                                                                      2581

greater rate of decay: simulating transient, short-term              multi-dimensional state-trace plot, as the two models are
learning.                                                            different in kind. Further to this, we aim to examine the
   Both models have successfully simulated a range of                components of the AugSRN in more detail, with the aim to
human datasets (Cleeremans, 1993), including modeling                investigate whether state-trace analysis considers these
sequence learning in serial reaction time (SRT) tasks. Jones         additions to the original SRN separate processes within the
and McLaren (2009) found that an AugSRN could produce                model.
the detailed pattern of subsequence learning demonstrated               Given this analysis, our approach was to produce a state-
by participants in their experiments. Using a two-choice             trace analysis of these models’ performance on a task based
SRT task participants received continuous strings of stimuli         closely on the two-choice serial reaction time (SRT)
that followed an exclusive-or rule two-thirds of the time. If        experiments described in Jones & McLaren (2009) and
the previous two responses were the same (XX or YY), then            Yeates, Jones, Wills, McLaren and McLaren (submitted).
the current trial would be one response (X), and if the              We aimed to compare the performance of these networks on
previous two had not been the same (XY or YX) this would             this task, varying the free parameters of the models.
lead to the other response (Y). Participants under incidental
conditions found it hard to learn about the subsequence                           Modeling Sequence Learning
XXX compared to the other types, a result which was                  The SRT paradigm involves participants responding to
successfully simulated by the AugSRN.                                stimuli on screen that follow some sequence (Nissen &
   In a later experiment Yeates, Jones, Wills, McLaren and           Bullemer, 1987; Lewicki, Czyzewska, and Hoffman, 1987).
McLaren (submitted) used the same two-choice SRT task to             Therefore, faster and more accurate responses are expected
investigate human sequence learning. Participants in this            for those trials that are predicted by the sequences learnt in
study were divided into two groups, both of which received           comparison to a control group, who would receive the same
sequences governed by a rule that they were not informed             task but with a pseudorandom ordering (e.g.
about. In one group, the current trial could be predicted two-       Anastasopoulou & Harvey, 1999, Jones & McLaren, 2009).
thirds of the time to be different to the trial before last, i.e.
‘first different to third’ (Group 1: XXY, XYY, YYX, YXX)             SRT Task Outline
and in the other the current trial would be predicted to be the      The task experienced by each network follows closely that
same as the trial before last, ‘first same as third’ (Group 2:       we have used with human participants, and lasted for two
XXX, XYX, YYY, YXY). Poorer performance was                          sessions, each with 20 blocks. Each block comprised 120
predicted in Group 2 under incidental conditions based on            continuous trials of stimuli appearing on the right or left.
Jones and McLaren’s (2009) earlier findings that                     The sequences making up each block were constructed
participants were unable to learn about subsequence XXX              differently for Group 1 and 2, and for the networks acting as
(or YYY). The manipulation did indeed produce this                   control groups. For the experimental networks, all blocks in
difference between Groups. We found that variants of the             the first session and the first fifteen sequences in the second
SRN and AugSRN could simulate these data to differing                session were constructed from 40 triplets that followed the
extents depending on the parameterisation of the model               rule for each Group (Group 1: XXY, XYY, YYX, YXX;
(Yeates et al., submitted), which suggests that the                  Group 2: XXX, XYX, YYY, YXY). Networks thus received
differences between the SRN and AugSRN may be of                     ten of each subsequence type per block.
interest in this context.                                               Two-thirds of experimental training trials followed the
   SRNs are considered to be single-process models (e.g.             rule, as the third trial in a triplet was always consistent with
Frensch & Miner, 1994; Kinder & Shanks, 2003), where                 the rule, as were half of first and second trials in a triplet by
parameters can be altered to produce different effects, but          chance when subsequences were randomly concatenated.
these involve essentially one process. The standard view is          Test and control group training blocks were made up of
that the two connection weight components in an AugSRN               pseudorandom sequences that included an equal amount of
represent the same kind of process; of learning through back         all subsequence types.
propagation, and that their differences are of amount and not
kind. Varying the learning rate affects the efficiency of            Model Construction
learning across training (Kinder & Shanks, 2001;                     The parameters varied in the model for the purpose of the
McClelland & Rumelhart, 1986). However, one might hold               state-trace analysis are the number of hidden units and the
the view that the two connection weight components are in            learning rates, as well as the presence or absence of
fact different processes within the AugSRN, accounting for           response units and presence of one or two connection
long- and short-term learning. Similarly, as response units          weight components. Two units for both input and output
were introduced to take account for short term priming of            were chosen to represent the stimuli (right or left circle fill)
the previous response (Cleeremans & McClelland, 1991),               and predictions for the next trial (right or left), respectively.
we could argue that this additional component may also               The activation of a single input unit was set to one, with the
represent an additional, different process.                          other set to zero to correspond to a left or right stimulus
   We therefore hypothesize that when comparing                      presentation. The units in each layer, from input and context
performance of the SRN and AugSRN we will see a clearly              to hidden and to output units, fed activation forward to
                                                                 2582

every unit in the layer above (see Figure 1). The activation       rate of 0.533. The AugSRN also possessed response units,
of the hidden and output units were determined by the              unlike the SRN.
logistic activation function (Rumelhart, Hinton, & Williams,         Results. Both models produced significant learning of
1986). Hidden unit activation was copied back to the               both sequences, which was analyzed by means of an
context units on each trial with a lag of one cycle of the         ANOVA with subsequences and blocks as within-subject
network. Each hidden unit also had a bias: a variable              factors, and experimental versus control as a between
connection from a unit that had a constant activation of one.      subject factor. The SRN exhibited a significant difference in
The hidden units mapped recurrently to the context units on        consistent-inconsistent MSE scores between experimental
a one-to-one basis. The feed-forward connections comprised         and control simulations, F(1,124) = 613.6, p < .001. The
of either one or two connection weights. These were                AugSRN also demonstrated learning, F(1,124) = 1113, p <
modified by the back-propagation algorithm, which we ran           .001. As this difference of differences measures the learning
without a momentum term (Rumelhart et al., 1986).                  in experimental networks compared to networks that
   To simulate the experiment with humans reported in              experienced pseudorandom sequences (controlling for
Yeates et al. (submitted), each model was run 128 times to         sequential effects), this difference is used to provide our
match the number of participants taking part in the empirical      index of learning and performance in what follows.
study. Half of these simulations acted as controls (trained on       The SRN and AugSRN constituted the two states we
pseudorandom sequences), with half receiving experimental          wished to analyze, and we plotted performance of Group 1
sequences. 32 experimental networks followed Group 1               against Group 2 on the axes as the dimensions. The plots
rules (‘first different to third’) and 32 followed Group 2         follow the trace of training over collapsed blocks of five,
rules (‘first same as third’). Initial connection weights were     with the seven points shown constituting the 35 training
set for each network to random values between -0.5 and 0.5.        blocks. Figure 2 shows the state-trace plot of this data.
Each simulation involved training for one session and
fifteen blocks of a second session, followed by five blocks
of test sequences. Therefore each simulation received 4200
training trials and 600 test trials.
   The mean square error (MSE) was calculated as the
difference between the location of the next trial, and the
prediction of the model (see Jones & McLaren, 2009). This
was taken as the measure of performance of the model on
the task. As in previous simulations of these tasks, the MSE
for trials consistent with the trained rule was taken from the
MSE for inconsistent trials (Jones & McLaren, 2009; Yeates
et al., submitted). This produces an estimate of learning
about those trained sequences, and is also computed for
control simulations. Half of the control simulations are
assigned to the dummy variable Group 1, where ‘first
different to third’ subsequences (XXY, XYY, YYX, YXX)
are taken from the matching ‘first same as third’
subsequence (XXX, XYX, YYY, YXY). The remaining 32
simulations follow the Group 2 inconsistent-consistent
calculation, with the MSE on ‘first same as third’                 Figure 2. State-trace plot showing learning of AugSRN and
subsequences taken from the MSE on ‘first different to             SRN across training blocks of simulations.
third’ subsequences. Comparing the differences between
experimental and control groups on these scores allows             Inspection of Figure 2 clearly suggests that there are two
learning to be assessed without any confound in terms of           different, monotonic functions on the plot. We analyzed the
sequential effects (see Anastasopoulou & Harvey, 1999;             data using hierarchical multiple regression, with the
Jones & McLaren, 2009). To summarize then, good learning           hypotheses that the model predicting Group 2 performance
will result in a larger difference of the form (Control            from Group 1 performance would be improved by the
network MSE for inconsistent trials - Control network MSE          addition of Model Type as a variable, indicating a multi-
for consistent trials) - (Experimental network MSE for             dimensional model of the data. We simply coded this as a
inconsistent trials - Experimental network MSE for                 dichotomous nominal variable, with the AugSRN arbitrarily
consistent trials), as a lower MSE indicates better learning.      labeled as 1 and the SRN as 2. The addition of Model Type
                                                                   into the regression model significantly improved the R2adj
                                                                   value from 70.2% to 92.1%, ΔR2 : F(1,11) = 34.1, p < .001,
State-Trace Analysis 1: SRN and AugSRN                             and overall, the model had a significant fit, F(2,11) = 76.7, p
The task was simulated on an SRN and AugSRN with 20                < .001; Group 2 = -.944xGroup 1 - .029xModel Type +
hidden units. The SRN had a learning rate of 0.4, the              .064. This corroborates the impression that the data on this
AugSRN had a slow learning rate of 0.4 and a fast learning         plot require more than one function for a good fit, which
                                                               2583

suggests that there is more than one underlying process
governing performance in these simulations. The conclusion
is that the SRN and the AugSRN differ in kind (which is
perhaps not surprising – though they are very similar types
of model), but the important finding here is that the state-
trace methodology is sensitive to this difference.
   Discussion. This confirms our predictions about how we
expected state-trace to represent learning by these different
networks on this task, producing different functions and so
confirming that they are genuinely different types of model.
This could easily be attributed to either or both of the two
differences between the SRN and AugSRN. We now
investigate whether these models are themselves best
characterized as single or multi-process models of learning.
State-Trace       Analysis      2:   Connection       weight
components
Here we ran simulations on four different models, two had
response units and two had no response units. Within these
dyads, we aimed to compare whether fast and slow weight           Figure 3. State-trace plot for Model 1 (one connection
components (the states in this state-trace analysis) were         weight component) and Model 2 (two connection weight
driving the multi-dimensional model seen in our first State-      components) across training.
Trace Analysis. Therefore Model 1 had one connection
weight component with response units, Model 2 had two
connection weight components with response units (an
AugSRN), Model 3 had one connection weight component
with no response units (a standard SRN), and Model 4 had
two connection weight components with no response units.
Both had 20 hidden units and slow and fast weights of .4
and .533 respectively, as in the previous simulation.
   Results. All four models learnt the sequences, analyzed as
in Simulation 1. Model 1 showed a significant difference
between experimental and control performance, F(1,124) =
853.6, p < .001. Models 2 and 3 showed learning, as seen in
the results of State-Trace Analysis 1. Finally, Model 4
demonstrated the same learning, F(1,124) = 1634, p < .001.
   When comparing models with one and two connection
weight components we can see from Figures 3 and 4, which
show the state-trace plots for models with and without
response units respectively, that two monotonic functions
appear.
   When conducting a hierarchical linear regression as            Figure 4. State-trace plot showing learning of Model 3 (one
described in State-Trace Analysis 1, we this time coded           connection weight component) and Model 4 (two
Model as a predictor with the values of 1 and 2 for one and       connection weight components), neither of which have
two components, respectively. Introducing Model into the          response units, across training.
regression alongside Group 1 in predicting Group 2
performance for models with response units (see Figure 3)           Discussion. Both in models with and without response
produced a significant improvement in the R2adj value from        units, multi-dimensional state-trace plots are produced when
84.7% to 93.1%, ΔR2 : F(1,11) = 15.9, p = .002, and overall,      comparing those with one or two connection weight
the model had a significant fit, F(2,11) = 89.4, p < .001;        components. The state-trace analysis suggests that the two
Group 2 = 1.231 Group 1 + .014 Model Type -                       models are driven by different underlying processes, which
.007. Similarly, when there are no response units (see Figure     in this case is due to the presence or absence of fast weights.
4) adding Model as a predictor improves the regression            Following the state-trace logic, this suggests that the two
model, with a significant improvement in the R2adj value          weight components within an AugSRN should be
from 56.0% to 89.6%, ΔR2 : F(1,11) = 40.0, p < .001, and          considered as distinct, different learning processes.
overall, the model had a significant fit, F(2,11) = 56.9, p <
.001; Group 2 = 1.008xGroup 1 + .042xModel Type - .041.
                                                              2584

State-Trace Analysis 3: Response units                            between the AugSRN and the standard SRN is the
Does the addition of response units to the basic SRN, or an       distinction between fast and slow weights.
SRN with two connection weight components, produce
separate functions on the state-trace plot? The same four
models are presented below, comparing Models 1 (no
response units) and 3 (with response units), which both have
one component, and Models 2 (no response units) and 4
(with response units), which both have two connection
weight components.
   Results. We compare Models depending on whether they
have response units or not, coded as 1 and 0, respectively.
See Figures 5 and 6 for state-trace plots of one and two
connection weight component models. We find that in
models with one component, adding Model as a variable
significantly improves the regression model, the R2adj value
improves from 92.3% to 95.6%, ΔR2 : F(1,11) = 10.3, p =
.008, and overall, the model had a significant fit, F(2,11) =
143.8, p < .001; Group 2 = .942xGroup 1 + .013xModel
Type + .006.
                                                                  Figure 6. State-trace plot showing learning of Model 2 (with
                                                                  response units) and Model 4 (no response units), which both
                                                                  have two connection weight components, across training
                                                                  blocks of simulations.
                                                                  State-Trace Analysis 4: Learning Rates
                                                                  Finally, to ensure that the differences seen in State-Trace
                                                                  Analysis 2 between one and two component models were
                                                                  not simply a result of the total amount or rate of learning,
                                                                  we varied the learning rates of the one component model,
                                                                  keeping the hidden units set at 20. We set the learning rate
                                                                  to 0.933, equal to the sum of the fast and slow learning rates
                                                                  employed in the AugSRN simulations to plot alongside the
                                                                  earlier one process model with a learning rate of 0.4. We are
                                                                  aware that a one component SRN with a learning-rate equal
                                                                  to the sum of two component’s learning-rates is not a direct
                                                                  equivalent. Nevertheless, our manipulation should allow us
Figure 5. State-trace plot showing learning of Model 1 (with      to discover if varying learning rate over this range produces
response units) and Model 3 (no response units), which both       different state-trace plots.
have only one connection weight component, across training          Results. An SRN with a Learning Rate of 0.933 learns the
blocks of simulations.                                            task, F(1,124) = 556.8, p < .001. The state-trace plot of
                                                                  these data, alongside the original Learning Rate of 0.4 can
Comparing Models 2 and 4, with response units, the                be seen in Figure 7, which clearly shows two functions.
regression does not significantly improve when adding             When adding the learning rate as a regressor into a model
Model as a variable into the regression.                          predicting Group 2 performance from Group 1 performance,
   Discussion. Whilst the functions are not as distinct as in     the R2adj value improves from 75.8% to 92.4%, ΔR2 : F(1,11)
State-Trace Analysis 2, the comparison of models with and         = 27.2, p < .001, and this model overall had a significant fit,
without response units still suggests a multi-dimensional         F(2,11) = 79.9, p < .001; Group 2 = .895xGroup 1 +
structure. That the models with two connection weight             .048xLearn Rate - .010.
components failed to reach significance is perhaps more a         Discussion. The state-trace plot and the regression analysis
criticism of the linear regression method when analyzing          clearly demonstrate two separate functions, which according
these data. The fact that the separation of the two plots is      to state-trace analysis suggests the presence of multiple
less impressive (in size and reliability) when the presence       processes. However, the two models differ only in the
(or not) of the response units is the manipulation than when      values assigned to their learning rates. State-trace analysis
the use of one vs. two sets of weights also suggests that for     proposes that a multi-dimensional state-trace plot will result
the type of model considered here, the main difference            from the presence of multiple processes in a given dataset,
                                                              2585

which implies the influence of more than one than one latent        awarded to Fayme Yeates, and an ESRC grant awarded to
variable within the SRN on performance. This suggests that          Ian McLaren and Fergal Jones.
either state-trace analysis is sensitive to differences within a
single process, or alternatively the SRN must be considered                                    REFERENCES
a multi-process model of learning.                                  Anastasopoulou, T., & Harvey, N. (1999). Assessing sequential
                                                                      knowledge through performance measures: The influence of
                                                                      short-term sequential effects. Quarterly Journal of Experimental
                                                                      Psychology, 52A, 423-448.
                                                                    Bamber, D. (1979). State-trace analysis: A method of testing
                                                                      simple theories of causation. Journal of Mathematical
                                                                      Psychology, 19, 171-181.
                                                                    Cleeremans, A. (1993). Mechanisms of implicit learning:
                                                                       Connectionist models of sequence processing. Cambridge, MA:
                                                                       The MIT Press.
                                                                    Cleeremans, A., & McClelland, J. L. (1991). Learning the structure
                                                                       of event sequences. Journal of Experimental Psychology:
                                                                       General, 120, 235-253.
                                                                    Dunn, J.C. (2008). The dimensionality of the remember-know task:
                                                                       A state-trace analysis. Psychological Review, 115(2), 426-446.
                                                                    Elman, J. L. (1990). Finding structure in time. Cognitive Science,
                                                                       14, 179-211.
                                                                    Jones, F. W., & McLaren, I. P. L. (2009). Human sequence
                                                                      learning under incidental and intentional conditions. Journal of
                                                                      Experimental Psychology: Animal Behavior Processes, 35(4),
                                                                      538-553.
                                                                    Kinder, A., & Shanks, D.R. (2001). Amnesia and the
Figure 7 State-trace plot for an SRN with different learning          declarative/non-declarative distinction: A recurrent network
rates (shown on graph).                                               model of classification, recognition, and repetition primining.
                                                                      Journal of Cognitive Neuroscience, 13, 648-669.
                  General Discussion                                Lewicki, P., Czyzewska, M., & Hoffman, H. (1987). Unconscious
                                                                      acquisition of complex procedural knowledge. Journal of
The analyses of SRNs with one or two learning components,             Experimental Psychology: Learning, Memory, and Cognition,
and those with or without response units give separate                13, 523-530.
functions on state-trace plots, suggesting the presence of          Loftus, G.R., Oberg, M.A., & Dillon, A.M. (2004). Linear theory,
more than one latent psychological variable. However,                 dimensional theory, and the face-inversion effect. Psychological
simple variation of the rate at which the SRN learnt also             Review, 111(4), 835-863.
produced a multi-dimensional state-trace plot, which raises         McClelland, J.L., & Rumelhart, D.E. (1986). Amnesia and
questions for the interpretation of multi-dimensional state-          distributed memory. In J. L. McClelland & D. E. Rumelhart
                                                                      (Eds.), Parallel distributed processing. Explorations in the
trace plots. A parameter search, varying learning rates and
                                                                      microstructure of cognition: Psychological and biological
number of hidden units, has been conducted and, within a              models (Vol. 2, pp. 503-527). Cambridge, MA: MIT Press.
reasonable range for the SRN on this SRT task, produces             Newell, B.R., & Dunn, J.C. (2008). Dimensions in data: Testing
the same functions as the data presented above. We                    psychological models using state-trace analysis. Trends in
recognise that the regression method employed in analysing            Cognitive Science, 12(8), 285-290.
the data is limited to roughly linear functions, and suggest        Newell, B.R., Dunn, J.C., & Kalish, M. (2010). The
that other methods (e.g. Newell, Dunn & Kalish, 2010;                 dimensionality of perceptual category learning: A          state-
Prince, Brown & Heathcote, 2011) are also explored. But               trace analysis. Memory & Cognition, 38(5), 563-581.
our analyses are, if anything, insensitive to the differences       Nissen, M. J., & Bullemer, P. (1987). Attentional requirements of
                                                                      learning: Evidence from performance measures. Cognitive
visualised by the plots, so this does not compromise our
                                                                      Psychology, 19, 1-32.
conclusions                                                         Prince, M., Brown, S., & Heathcote, A. (2011, October 31). The
   It seems, then, that not only multiple processes, but               Design and Analysis of State-Trace Experiments. Psychological
variations within a single process can produce multi-                  Methods. Advance online publication. doi: 10.1037/a0025809
dimensional state-trace plots. The implications for state-          Remington, R. J. (1969). Analysis of sequential effects in choice
trace analysis as a tool for the investigation of the number of       reaction times. Journal of Experimental Psychology, 82, 250-
latent variables underlying human behaviour needs to be               257.
considered, and further analysis of computational models            Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986). Learning
with this technique is recommended.                                   internal representations by error propagation. In D. E. Rumelhart
                                                                      & J. L. McClelland (Eds.). Parallel distributed processing (Vol.
                                                                      1, pp. 318-362). Cambridge, MA: Bradford Books.
                     Acknowledgments                                Yeates, F., Jones, F. W., Wills, A. J., McLaren, R., & McLaren, I.
The research reported in this paper was supported by a                P. L. (submitted). Modelling human sequence learning under
Postgraduate studentship and Exeter Graduate Fellowship               incidental conditions.
                                                                2586

