UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Unified Model of Categorical Effects in Consonant and Vowel Perception
Permalink
https://escholarship.org/uc/item/0xk3r3rf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Kronrod, Yakov
Coppess, Emily
Feldman, Naomi
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

       A Unified Model of Categorical Effects in Consonant and Vowel Perception
                                                Yakov Kronrod (yakov@umd.edu)
                                              Emily Coppess (ercoppess@umd.edu)
                                                Naomi H. Feldman (nhf@umd.edu)
                                           Department of Linguistics, 1401 Marie Mount Hall
                                           University of Maryland, College Park, MD 20742
                              Abstract                                 suggests that differences in the degree to which vowels and
                                                                       consonants are perceived categorically can be explained as
   Consonants and vowels differ in the extent to which they are        parametric variations in a single underlying model.
   perceived categorically. We use a Bayesian model of speech
   perception to explore factors that might cause this difference.        Our paper is organized as follows. First, we review evi-
   Simulations show that perception of vowels, fricatives, and         dence concerning categorical effects in consonants and vow-
   stop consonants can all be captured under a single model in
   which listeners use their knowledge of phonetic categories to       els, giving an overview of the types of explanations that have
   infer the sound that a speaker intended. This suggests that the     been proposed to account for these data. We then describe
   differences in the way we perceive vowels and consonants,           the model from Feldman et al. (2009) in detail, focusing on
   when viewed at the computational level, can be explained as
   parametric variation within a single framework.                     their results for vowel perception. In the subsequent section,
                                                                       we present simulations testing our extended model on two
   Keywords: perceptual magnet effect; categorical perception;
   Bayesian modeling; computational linguistics                        types of consonants, stop consonants and fricatives, to deter-
                                                                       mine whether a model built for vowels can also account for
   Phonetic categories influence perception of speech sounds,          patterns in consonant perception. We conclude by summariz-
with stimuli belonging to different categories being easier to         ing our findings and discussing implications for theories of
discriminate than stimuli from a single category (Liberman,            speech perception.
Harris, Hoffman, & Griffith, 1957; Kuhl, 1991). However,
different types of sounds differ in the degree to which they                Categorical Effects in Speech Perception
are perceived categorically. At one end of the spectrum,               Categorical perception in stop consonants was first described
perception of stop consonants is strongly categorical. Dis-            by Liberman et al. (1957) as consisting of a sharp change in
crimination is little better than would be expected if listen-         the identification function between different consonants, as
ers used only category assignments to distinguish sounds,              well as a peak in the discrimination function at the location of
and between-category differences are extremely pronounced              the identification boundary. The authors proposed a model in
(Liberman et al., 1957). At the other end of the spectrum,             which participants used only category assignments to deter-
vowel perception is much more continuous, with some even               mine whether sounds were the same or different. If the sounds
arguing that vowels display no categorical effects at all (Fry,        belonged to different categories, then participants would re-
Abramson, Eimas, & Liberman, 1962).                                    spond different; otherwise, they would respond same. By
   Researchers have used various mechanisms underlying                 examining participants’ identification functions, Liberman et
speech perception to explain these differences. For example,           al. could use this model to predict the extent to which par-
differences have been proposed to stem from the way each               ticipants should be able to discriminate each pair of sounds.
type of sound is stored in memory (Pisoni, 1973) and to be re-         Participants’ actual discrimination performance exceeded the
lated to innate auditory discontinuities that seem to influence        model’s predictions only by a small amount, and the authors
stop consonant perception (Pisoni, 1977; Eimas, Siqueland,             took this as evidence of a strong categorical component in
Jusczyk, & Vigorito, 1971). However, the qualitative sim-              stop consonant perception. Liberman et al.’s experiment fo-
ilarity of categorical effects in consonants and vowels sug-           cused on stop consonants that differed by place of articula-
gests that in some ways these are also instances of the same           tion, but similar findings have been obtained along the voice
phenomenon. This raises the possibility that perceptual dif-           onset time (VOT) dimension as well (Wood, 1976).
ferences among different classes of sounds are quantitative               Descriptions of categorical effects in vowels have focused
rather than qualitative.                                               primarily on the perceptual magnet effect (Kuhl, 1991). This
   In this paper we explore these similarities and differences         effect was originally proposed as a within-category phe-
at Marr’s (1982) computational level, looking at the optimal           nomenon, characterized by sounds near category centers be-
solution to the problem of inferring speakers’ intended pro-           ing more difficult to discriminate than sounds near cate-
ductions from the available acoustic information. We adapt a           gory edges, with an accompanying correlation between good-
Bayesian model from Feldman, Griffiths, and Morgan (2009),             ness ratings and discriminability. There is disagreement over
in which listeners use their knowledge of phonetic categories          whether categorical perception and the perceptual magnet ef-
to recover the sound a speaker intended. We show that an ex-           fect are separate phenomena or different variants of the same
tended version of this model can account for perceptual data           phenomenon (e.g. Lotto, Kluender, & Holt, 1998). Some
from stop consonants and fricatives as well as vowels. This            characteristics of the perceptual magnet effect are similar to
                                                                   629

pure categorical effects, such as reduced discriminability near      tity of upcoming sounds (Gow, 2001). Once a speaker selects
category centers. Data from Iverson and Kuhl (2000) sug-             a target production, T , from the category, there is assumed
gested that discrimination peaks near category boundaries are        to be additional articulatory, acoustic, and perceptual noise
separable from correlations of discrimination and goodness           σ2S that further distorts this sound. This process results in a
ratings, whereas more recent studies have found that these           speech sound S that is heard by listeners.
two effects cooccur (Tomaschek, Truckenbrodt, & Hertrich,               Listeners are trying to infer the target production through
2011). Regardless of terminology, however, categorical ef-           the noisy speech signal. To do this, listeners can use their
fects in vowel perception are much weaker than those found           knowledge that speakers tend to produce sounds near cat-
in consonant perception.                                             egory centers. Hence, they rely both on category knowl-
   In addition to stop consonants and vowels, it is natural          edge and on acoustic cues to recover the phonetic detail of
to consider categorical perception of another major class of         a speaker’s target production. If listeners encounter little
speech sounds, fricatives. In this paper, we consider categori-      noise and the category allows a large amount of meaning-
cal perception of sibilant fricatives. There has been some dis-      ful variability (e.g., coarticulation), then listeners attend more
agreement over the degree of categorical perception in frica-        to acoustic detail in perceiving sounds; however, in situa-
tives in previous research. Repp (1981) showed that fricatives       tions with high noise and low meaningful variability, they
follow patterns similar to the categorical perception found for      rely more on their knowledge of phonetic categories. This
stop consonants. However, in the same study, a subset of par-        relationship between category variance and noise plays an im-
ticipants seemed to have perception that was more continu-           portant role in determining the degree to which perception is
ous, which Repp attributed to a choice between two process-          biased toward category centers, and it thus has the potential
ing strategies, acoustic and phonetic. Others have found that        to account for differences in the degree to which vowels and
fricative perception is much less categorical than stop conso-       consonants are perceived categorically.
nants (Liberman, Cooper, Shankweiler, & Studdert-Kennedy,               Feldman et al.’s (2009) original model relied on a sim-
1967; Healy & Repp, 1982; Repp, 1984). Another more re-              plifying assumption that all categories considered by a lis-
cent study showed identification patterns consistent with cate-      tener have equal category variance. While this assumption
gorical perception together with a neural signature indicative       might be adequate for vowels, other sound categories do not
of something like perceptual warping near category centers           necessarily reflect this simplification, particularly voiced and
(Lago, Kronrod, Scharinger, & Idsardi, 2010).                        voiceless stop consonants which have been shown to have
   These data set up a continuum ranging from nearly com-            substantial differences in VOT variance (Lisker & Abram-
pletely categorical perception of stop consonants to much            son, 1964). Hence, we extend the original model proposed
more continuous perception of vowels, with fricatives falling        by Feldman et al. (2009) to allow for unequal category vari-
somewhere in between. However, this continuum is not as              ances. This section gives an overview of our extended model;
clear cut as it may seem, as neural and behavioral evidence          full derivations are omitted due to space limitations, but are
suggests that listeners attend to phonetic detail when per-          parallel to those in Feldman et al.
ceiving stop consonants (Pisoni & Lazarus, 1974; Blumstein,             The model assumes phonetic categories are Gaussian dis-
Myers, & Rissman, 2005), and the degree of categorical per-          tributions of sounds along the relevant auditory dimensions,
ception in both consonants and vowels can be influenced by           so that a speaker’s target production is normally distributed
task-related factors (Pisoni, 1975; Repp, Healy, & Crowder,          around the category mean, T |c ∼ N(µc , σ2c ). Noise in the
1979). Nevertheless, the differences between consonant and           speech signal causes the stimulus heard by listeners to be
vowel perception are robust. In what follows, we use a model         normally distributed around the target production, S|T ∼
to account for the variability in these effects within a sin-        N(T, σ2S ). We can integrate over all possible target produc-
gle framework, identifying aspects of category structure that        tions T to get an expression relating the perceived sound di-
might contribute to differences in categorical effects across        rectly to the underlying categories,
consonants and vowels.
                                                                                            S|c ∼ N(µc , σ2c + σ2S )                (1)
                     Model Overview                                     In identification tasks, listeners recover a category given
Our model is an extension of the model from Feldman et al.           the sound S, which corresponds to computing the posterior
(2009). The model assumes that listeners are trying to re-           distribution over category membership p(c|S) in the model.
cover phonetic detail about the speaker’s intended production        They can compute this by applying Bayes’ rule,
as well as category information. It differs from traditional
                                                                                                       p(S|c)p(c)
models of categorical perception in that it recognizes two                                p(c|S) =                                  (2)
different sources of within-category variability: meaningful                                         ∑c p(S|c)p(c)
variability (also referred to as category variance) and noise        If we limit ourselves to two categories but relax the assump-
variance. The category variance σ2c is assumed to arise from         tion that these have equal category variances, we need two
processes that yield information useful to listeners, such as        means (µc1 and µc2 ) and category variance parameters (σc1
coarticulatory effects that allow listeners to predict the iden-     and σc2 ). We derive the identification function by substituting
                                                                 630

    Simulation                                           Means                                                                            Variances                   Category:Noise
                                                                                                                                                                      Variance Ratio
                                     µc1                        µc2                        σ2c1                                           σ2c2          σ2S
    Vowels                           F1=224 Hz                  F1=423 Hz                  5,873                                          5,873         878           6.69
    (Equal Variance)                 F2=2413 Hz                 F2=1936 Hz                                                                (Mels)
    Stop Consonants                  60 ms                      -0.3 ms                    253.9                                          14            82.3          /p/: 3.09, /b/: 0.17
    (Unequal Variance)                                                                                                                    (ms)
    Fricatives                       19.0 Barks                 15.99 Barks                0.5992                                         0.5772        0.3098        /s/: 1.93, /S/: 1.86
    (Unequal Variance)                                                                                                                    (Barks)
             Table 1: Best fitting model parameters for vowels (Feldman et al., 2009), stop consonants, and fricatives.
Equation 1 into Equation 2 and following a parallel derivation                                  discrimination experiment, the patterns they found suggested
to that in Appendix B from Feldman et al. (2009), yielding                                      that multidimensional scaling was distorting the perceptual
                                                                                                patterns, and that the noise parameter needed to capture ex-
                                                   1                                            perimental data directly was much lower than they initially
p(c1 |S) =        r                                                                             found.1 Thus, the “Vowels (Equal Variance)” section of Ta-
                      σ21           (σ22 −σ21 )S2 +2(µc2 σ21 −µc1 σ22 )S+(µ2c σ22 −µ2c σ21 )
             1+             × exp                                            1        2         ble 1 shows the values they derived on the basis of their exper-
                      σ22                                   2σ21 σ22
                                                                                                imental data. As might be expected for relatively continuous
                                                          (3)
                                                                                                vowel perception, these parameters showed high meaningful
where σ21 = σ2c1 + σ2S and σ22 = σ2c2 + σ2S .
                                                                                                category variance relative to noise variance, indicating that
   The model assumes that listeners recover the phonetic de-
                                                                                                the bias toward category centers was small. We use these pa-
tail of a speaker’s target production in addition to category
                                                                                                rameters as a baseline for comparison in our consonant simu-
information when perceiving a speech sound, and that they
                                                                                                lations.
use this information when performing a discrimination task.
Perceiving phonetic detail corresponds to computing the pos-
terior distribution on target productions, p(T |S). Applying                                                                              Relative Distances Between Neighboring Stimuli
Bayes’ rule, where the prior p(T ) is a mixture of Gaussians                                                                      2
and the likelihood p(S|T ) is Gaussian, we obtain a posterior                                                                                                                      MDS
                                                                                                                                                                                   Model
distribution whose form is a mixture of Gaussians and whose
mean is
                                                                                                   Relative Perceptual Distance
                                                σ2c S + σ2S µc
                  E[T |S] = ∑ p(c|S)                                               (4)
                                c                 σ2c + σ2S
(see Feldman et al., 2009 for a full derivation). Each cate-
gory makes a contribution to this posterior mean with magni-
tude proportional to the posterior probability of the sound be-
longing to that category, p(c|S). The specific contribution of
each category is to bias perception toward the category mean.
The strength of the bias is controlled by the relationship be-
tween parameters σ2c and σ2S , which represent the amount of
meaningful variability and the amount of noise. Notice that
                                                                                                                                  0
the contribution of the category mean, µc , is weighted by                                                                            1   2   3     4   5 6 7 8 9            10 11 12 13
the noise variance, σ2S . This means that when there is more                                                                                             Stimulus Number
noise, listeners will rely more on their underlying knowledge
of the categories. In contrast, the acoustic information, S, is                                 Figure 1: Figure from Feldman et al. (2009) showing inter-
weighed by the meaningful variance parameter, σ2c , such that                                   stimulus distances from Iverson and Kuhl’s (1995) multidi-
when there is a lot of meaningful variability in the underly-                                   mensional scaling solution and the fitted model.
ing category, listeners will pay more attention to the acoustic
data. It is this relationship that will be critical to modeling dif-
ferences in perception between different categories of sounds.
   Feldman et al. (2009) applied their model to vowel percep-
tion (continuum from /e/ to /i/), obtaining a close fit to the                                     1 In our simulations below, we select data that use the distance
multidimensional scaling data from Iverson and Kuhl (1995)                                      measure d’ rather than multidimensional scaling data in order to
(Figure 1). However, in analyzing their own data from an AX                                     avoid this type of discrepancy.
                                                                                          631

  (a)                                                Categories and Identification (Stops)                                                       Discrimination (Stops)
                                              1.6                                                                 (b)
                                                                                                                            2.5
                                                                                      /b/ category
                                                                                      /p/ category
                                              1.4
    Proportion of Stimuli Identified as /p/
                                                                                      Perceived /b/
                                                                                                                             2
                                                                                      Perceived /p/
                                              1.2                                     Behavioral Ident.
                                                                                      Model Ident.
                                                                                                                            1.5
                                               1
                                                                                                                 D−Prime
                                              0.8                                                                            1
                                              0.6
                                                                                                                            0.5
                                              0.4
                                                                                                                             0
                                              0.2
                                                                                                                                                                Behavioral D−Prime
                                                                                                                                                                Predicted Perceptual Distance
                                                0                                                                          −0.5
                                               −50     0            50              100              150                     −40   −30   −20   −10   0     10    20     30     40     50        60
                                                                  VOT (ms)                                                                               VOT (ms)
Figure 2: Stop consonants: (a) Underlying categories, perceived distributions, and identification curve in the model, together
with behavioral identification data; (b) Interstimulus distances predicted by the model, together with behavioral d’ data.
                                                           Simulations                                           Stop Consonants
We applied this model to data from stop consonants and sibi-                                                     We first consider behavioral data from identification and dis-
lant fricatives, deriving parameters on the basis of experimen-                                                  crimination experiments on stop consonants, which have been
tal data in order to determine whether categorical effects in                                                    found to exhibit much stronger categorical effects in percep-
each type of sound can be explained as the result of optimally                                                   tion than vowels. Under our account, this difference might
inferring the phonetic detail of a speaker’s target production.                                                  stem from low category variance relative to noise variance,
Examining the resulting parameters then allows us to assess                                                      such that listeners rely more on category information. If we
the degree to which those parameters are adequate with regard                                                    are able to account for stop consonant perception with our
to existing data, as well as examine the relationship between                                                    model, then that would suggest that it may not be necessary
the two variance parameters and the degree of observed per-                                                      to posit qualitative differences in the types of computations
ceptual warping. Following Feldman et al. (2009), our gen-                                                       performed by listeners when perceiving consonants and vow-
eral strategy for fitting our parameters was as follows:                                                         els. It is not obvious that our model should be able to explain
                                                                                                                 stop consonant data, however, as other factors such as innate
1. Set µc1 on the basis of production data.                                                                      phonetic boundaries (Eimas et al., 1971) or auditory disconti-
                                                                                                                 nuities (Pisoni, 1977) might retain their influence on stop con-
                                                                                                                 sonant perception even after phonetic learning is complete.
2. Determine µc2 , σ21 , and σ22 from identification data using
                                                                                                                     For this simulation we used identification and discrimina-
   Equation 3.
                                                                                                                 tion data from Wood (1976), who examined perception of
                                                                                                                 /p/ and /b/ along a voice onset time (VOT) continuum. The
3. Determine the ratio of category variances, σ2c1 and σ2c2 , to                                                 continuum consisted of synthetic stimuli ranging from -50 to
   noise, σ2S , by fitting acoustic differences between percepts,                                                +70 ms VOT. A forced identification task as well as both a 10-
   E[T |S], in the model (Equation 4) to a distance measure                                                      ms and 20-ms difference AX discrimination task were admin-
   such as d’.                                                                                                   istered. We used 20-ms discrimination data for our simula-
                                                                                                                 tions. On the basis of data from Lisker and Abramson (1964),
We need to set one of the means in order to obtain a single                                                      we set µ/p/ at 60 ms VOT and derived the remaining parame-
identifiable set of parameters. The model is then fit to identi-                                                 ters from the identification and discrimination data. The iden-
fication data, allowing us to derive the other mean as well as                                                   tification fit produced an estimated value of -0.3 ms for the
both sums of variances (one corresponding to each category).                                                     mean µ/b/ , which was a close match to production data found
Note that the only parameter being fit directly to the discrim-                                                  in Lisker and Abramson (1964). The full set of parameters
ination data is the ratio of meaningful category variance to                                                     is found in section “Stop Consonants (Unequal Variance)” of
noise variance, which is the parameter of interest for exam-                                                     Table 1, and the resulting identification curve and category
ining the degree of bias toward category centers exhibited by                                                    distributions are shown in Figure 2(a). The fit between model
each class of sounds. In effect, the discrimination data pro-                                                    and data is very close: the model is even able to predict the re-
vide a general test of the model’s fit to behavioral data from                                                   duced within-category discriminability of voiced stops com-
each class of sounds.                                                                                            pared to voiceless stops that is observed in the empirical data.
                                                                                                           632

        (a)                                           Categories and Identification (Fricatives)               (b)                        Discrimination (Fricatives)
                                              1                                                                          1.8
                                                   /Sh/ category
                                             0.9   /s/ category                                                          1.6
   Proportion of Stimuli Identified as /s/
                                                   Perceived /Sh/
                                             0.8   Perceived /s/
                                                                                                                         1.4
                                                   Behavioral Ident.
                                             0.7   Model Ident.
                                                                                                                         1.2
                                             0.6
                                                                                                               D−Prime
                                                                                                                          1
                                             0.5
                                                                                                                         0.8
                                             0.4
                                                                                                                         0.6
                                             0.3
                                             0.2                                                                         0.4
                                             0.1                                                                         0.2                               Behavioral D−Prime
                                                                                                                                                           Predicted Perceptual Distance
                                              0                                                                           0
                                              12       14          16          18         20       22                     15   15.5    16     16.5     17    17.5    18       18.5         19
                                                        Central Frication Frequency (Barks)                                           Central Frication Frequency (Barks)
Figure 3: Sibilant fricatives: (a) Underlying categories, perceived distributions, and identification curve in the model, together
with behavioral identification data; (b) Interstimulus distances predicted by the model, together with behavioral d’ data.
This can be seen in Figure 2(b), where the perceptual distance                                                Figure 3(a) shows the identification data and the identifica-
between stimuli toward the left side of the continuum is lower                                                tion curve used in our model, together with the underlying
than that toward the right side of the continuum.                                                             and perceived category distributions that correspond to the
   As predicted, the ratio of category variance to speech sig-                                                parameters used in our simulation. Figure 3(b) compares the
nal noise was lower than that obtained for vowels for both                                                    model predictions to the observed discrimination measures.
categories of stop consonants, voiced and voiceless. These                                                    The fit is not perfect, due in part to noisy data from the orig-
findings suggest that stop consonants have less meaningful                                                    inal experiment, but both data and model show the peak in
within-category variance relative to noise variance than vow-                                                 discrimination at the same location as the inflection point in
els, leading the listener to rely on prior category knowledge                                                 the identification data.
in inferring the speakers’ target production. This causes a                                                      Given that fricatives tend to be perceived more categori-
greater pull toward category centers and hence stronger cat-                                                  cally than vowels but less so than consonants, we might ex-
egorical perception. Perceptual bias is particularly strong in                                                pect the category variance to noise ratio to be smaller for
voiced stops due to their low category variance.                                                              fricatives than for vowels, leading to a larger perceptual bias
                                                                                                              toward category centers, but larger than that for stop con-
Sibilant Fricatives                                                                                           sonants, indicating more attention focused on acoustic cues.
The previous simulation indicates that the model provides                                                     As predicted, the ratios for the sibilant fricatives are reduced
a good account for stop consonants as well as vowels. We                                                      compared to the parameters estimated for vowels (1.93 and
next apply our model to sibilant fricatives. Sibilant fricatives                                              1.86 compared to 6.69). Additionally, we see that they are
are obstruents (like stop consonants), but their characteris-                                                 close to the ratio for the voiceless stops but much higher than
tic noise components at higher frication frequencies show                                                     that of the voiced consonants, suggesting that they may be
some similarity to the higher formant structures of vowels.                                                   closer to the stop consonant end of the spectrum in terms of
As discussed above, there has been conflicting evidence on                                                    their degree of bias toward category centers.
the strength of categorical effects they exhibit. These factors
make fricatives an interesting modeling target.                                                                                               Discussion
   For this simulation we used identification and discrimina-                                                 This paper used a Bayesian model to investigate the rela-
tion data along the /s/-/S/ continuum from Lago et al. (2010).                                                tionship between categorical effects in consonant and vowel
The continuum consisted of 11 tokens with central frication                                                   perception. Our results suggest that these effects can be ex-
frequencies varying from 14.5 to 19.5 Barks. A forced identi-                                                 plained at Marr’s (1982) computational level by the same un-
fication task as well as a 2-step AX discrimination task were                                                 derlying principles: Listeners use their knowledge of pho-
administered to 12 participants. For our model, we fixed the                                                  netic categories to optimally infer a speaker’s target produc-
value of µ/s/ to 19.0 Barks based on natural productions by                                                   tion through a noisy speech signal, and this causes their per-
an adult male participant and derived values for the remaining                                                ception to be biased toward category centers. The model ac-
parameters by fitting the model to behavioral identification                                                  counts for differences in the strength of categorical effects by
and discrimination data. The resulting parameter values are                                                   assigning consonants less meaningful variability, compared
given in the “Fricatives (Unequal Variance)” entry in Table 1.                                                with noise variance, than vowels.
                                                                                                        633

   Our analysis is reminiscent of an analysis pursued by                  Eimas, P. D., Siqueland, E. R., Jusczyk, P., & Vigorito, J. (1971).
Pisoni (1973), Liberman et al. (1967), and others. Their ac-                Speech perception in infants. Science, 171(3968), 303-306.
                                                                          Feldman, N. H., Griffiths, T. L., & Morgan, J. L. (2009). The influ-
count proposes that speech perception incorporates a pho-                   ence of categories on perception: Explaining the perceptual mag-
netic mode of perception, i.e., categorical perception, and                 net effect as optimal statistical inference. Psychological Review,
an auditory mode of perception, i.e., continuous percep-                    116(4), 752-782.
                                                                          Fry, D. B., Abramson, A. S., Eimas, P. D., & Liberman, A. M.
tion. Pisoni (1973) argued that differences between conso-                  (1962). The identification and discrimination of synthetic vow-
nant discriminability and vowel discriminability could be ac-               els. Language and Speech, 5, 171-189.
counted for by assuming that listeners have less access to                Gow, D. W. (2001). Assimilation and anticipation in continuous
                                                                            spoken word recognition. Journal of Memory and Language, 45,
auditory short-term memory when hearing consonant stim-                     133-159.
uli than when hearing vowel stimuli. This distinction be-                 Healy, A. F., & Repp, B. H. (1982). Context independence and
tween phonetic and acoustic modes of perception is parallel                 phonetic meditation in categorical perception. Journal of Exper-
                                                                            imental Psychology: Human Perception and Performance, 8 (1),
to the weighted average in Equation 4, where acoustic infor-                68-80.
mation is weighted by the category variance and the category              Iverson, P., & Kuhl, P. K. (1995). Mapping the perceptual mag-
mean is weighted by the noise variance. When noise vari-                    net effect for speech using signal detection theory and multidi-
                                                                            mensional scaling. Journal of the Acoustical Society of America,
ance dominates over category variance, listeners rely more                  97(1), 553-562.
on the category mean rather than acoustic information (i.e.               Iverson, P., & Kuhl, P. K. (2000). Perceptual magnet and phoneme
phonetic mode). Otherwise, acoustic information receives                    boundary effects in speech perception: Do they arise from a com-
                                                                            mon mechanism? Perception and Psychophysics, 62(4), 874-
more weight and within-category discriminability increases                  886.
(i.e. auditory mode). Looking at ratios of category variance              Kuhl, P. K. (1991). Human adults and human infants show a “per-
to noise variance across consonants and vowels we see that                  ceptual magnet effect” for the prototypes of speech categories,
                                                                            monkeys do not. Perception and Psychophysics, 50(2), 93-107.
for vowels category variance exerts much more influence than              Lago, S., Kronrod, Y., Scharinger, M., & Idsardi, B. (2010). Cate-
noise variance and therefore listeners’ perception is drawn                 gorical perception of [s] and [sh]: An MMN study. Neurobiology
less towards the category center, causing within-category dis-              of Language Conference. San Diego, CA.
                                                                          Liberman, A. M., Cooper, F., Shankweiler, D., & Studdert-Kennedy,
criminability to increase (i.e. continuous perception). For                 M. (1967). Perception of the speech code. Psychological Review,
consonants the ratio is smaller, coinciding with a decrease in              74, 431-461.
within-category discriminability (i.e. categorical perception).           Liberman, A. M., Harris, K. S., Hoffman, H. S., & Griffith, B. C.
                                                                            (1957). The discrimination of speech sounds within and across
   Our findings suggest that perception of three types of                   phoneme boundaries. Journal of Experimental Psychology, 54(5),
sounds – vowels, stop consonants, and fricatives – adheres                  358-368.
to the same abstract computational principles. Importantly,               Lisker, L., & Abramson, A. S. (1964). A cross-language study of
                                                                            voicing in initial stops: Acoustical measurements. Word, 20, 384-
however, the idea that listeners are performing the same com-               422.
putation at an abstract level does not necessarily mean that              Lotto, A. J., Kluender, K. R., & Holt, L. L. (1998). Depolarizing
the underlying mechanisms are identical. Our analysis sim-                  the perceptual magnet effect. Journal of the Acoustical Society of
                                                                            America, 103(6), 3648-3655.
ply suggests that perception of each type of sound has been               Marr, D. (1982). Vision: A computational investigation in the human
optimized to allow listeners to recover the sound intended by               representation of visual information. San Francisco: Freeman &
a speaker. Bias toward category centers may be implemented                  Co.
                                                                          Pisoni, D. B. (1973). Auditory and phonetic memory codes in the
differently across different classes of sounds, and separate                discrimination of consonants and vowels. Perception and Psy-
mechanisms are almost certainly necessary for extracting the                chophysics, 13(2), 253-260.
various cues we have used as input to our model (formant                  Pisoni, D. B. (1975). Auditory short-term memory and vowel per-
                                                                            ception. Memory and Cognition, 3(1), 7-18.
frequencies for vowels; voice onset time for stop consonants;             Pisoni, D. B. (1977). Identification and discrimination of the rela-
and central frication frequencies for fricatives). In future                tive onset time of two component tones: Implications for voicing
work we hope to explore these issues by considering percep-                 perception. Journal of the Acoustical Society of America, 61(5),
                                                                            1352-1361.
tion of a fourth class of speech sounds, nasals, and by linking           Pisoni, D. B., & Lazarus, J. H. (1974). Categorical and noncate-
our computational approach with descriptions of sound per-                  gorical modes of speech perception along the voicing continuum.
ception at the algorithmic and implementational levels.                     Journal of the Acoustical Society of America, 55(2), 328-333.
                                                                          Repp, B. H. (1981). Two strategies in fricative discrimination. Per-
Acknowledgments We thank Sol Lago, Mathias Scharinger, and                  ception and Psychophysics, 30 (3), 217-227.
Bill Idsardi for sharing behavioral data from their experiments           Repp, B. H. (1984). Categorical perception: Issues, methods, find-
with fricatives. We also thank the Computational Psycholinguistics          ings. Speech and Language: Advances in Basic Research and
group, the PFNA Sounds group, and the Language Science Lunch                Practice, 10, 243-335.
group for valuable discussion and feedback. Finally, we thank Bill        Repp, B. H., Healy, A. F., & Crowder, R. G. (1979). Categories and
Idsardi and four anonymous reviewers for their insightful commen-           context in the perception of isolated steady-state vowels. Jour-
tary and suggestions on earlier versions of this paper. This work was       nal of Experimental Psychology: Human Perception and Perfor-
supported in part by NSF IGERT grant DGE-0801465.                           mance, 5(1), 129-145.
                                                                          Tomaschek, F., Truckenbrodt, H., & Hertrich, I. (2011). Process-
                           References                                       ing german vowel quantity: Categorical perception or perceptual
                                                                            magnet effect? Proceedings of the 17th International Conference
Blumstein, S. E., Myers, E. B., & Rissman, J. (2005). The percep-           of Phonetic Sciences, 2002-2005.
   tion of voice onset time: An fMRI investigation of phonetic cate-      Wood, C. C. (1976). Discriminability, response bias, and phoneme
   gory structure. Journal of Cognitive Neuroscience, 17(9), 1353-          categories in discrimination of voice onset time. Journal of the
   1366.                                                                    Acoustic Society of America, 60 (6), 1381-1389.
                                                                      634

