UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Solving nonogram puzzles by reinforcement learning
Permalink
https://escholarship.org/uc/item/13t021xk
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Dandurand, Frederic
Cousineau, Denis
Shultz, Thomas R.
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                            Solving nonogram puzzles by reinforcement learning
                                    Frédéric Dandurand (frederic.dandurand@gmail.com)
                              Department of Psychology, Université de Montréal, 90 ave. Vincent-d'Indy
                                                     Montréal, QC H2V 2S9 Canada
                                         Denis Cousineau (denis.cousineau@uottawa.ca)
                                      École de psychologie, Pavillon Vanier, Université d'Ottawa
                                     136 Jean Jacques Lussier, Ottawa, Ontario, K1N 6N5, Canada
                                          Thomas R. Shultz (thomas.shultz@mcgill.ca)
              Department of Psychology and School of Computer Science, McGill University, 1205 Penfield Avenue
                                                     Montreal, QC H3A 1B1 Canada
                               Abstract                               empty cell. At the beginning, the state of all cells is
                                                                      unknown (often portrayed visually by a grey color), and the
   We study solvers of nonogram puzzles, which are good
   examples of constraint-satisfaction problems. Given an             goal is to determine if each cell is empty (white) or filled
   optimal solving module for solving a given line, we compare        (black), while satisfying all of the numerical constraints.
   performance of three algorithmic solvers used to select the
   order in which to solve lines with a reinforcement-learning-
   based solver. The reinforcement-learning (RL) solver uses a                            1         2
   measure of reduction of distance to goal as a reward. We
   compare two methods for storing qualities (Q values) of state-                         1         1        0         4        3
   action pairs, a lookup table and a connectionist function          2         2
   approximator. We find that RL solvers learn near-optimal           1         2
   solutions that also outperform a heuristic solver based on the
   explicit, general rules often given to nonogram players. Only                2
   RL solvers that use a connectionist function approximator          2         1
   generalize their knowledge to generate good solutions on                     0
   about half of unseen problems; RL solvers based on lookup
   tables generalize to none of these untrained problems.
                                                                       Figure 1 - Example of a 5x5 nonogram puzzle. In the initial
   Keywords: Nonograms; problem solving; reinforcement                  state presented here, all cells are grey to indicate that the
   learning; distance-based reward; SDCC.                               problem solver does not know yet if they should be filled
                                                                                          (black) or empty (white).
                      Nonogram puzzles
Invented in Japan in the 1980s, nonograms (also called
Hanjie, Paint by Numbers, or Griddlers) are logic puzzles in          Strategies for solving nonograms
which problem solvers need to determine whether each cell             To solve nonograms, two important activities are necessary.
of a rectangular array is empty or filled, given some                 First, the problem solver needs to decide which line (row or
constraints. Nonograms are interesting problems to study              column) to solve next, and then to actually solve that line.
because they are good examples of constraint satisfaction             Problem solvers typically need to iterate through the lines,
problems (Russell & Norvig, 2003), which are ubiquitous in            progressively gathering more and more information about
real life (Shultz, 2001). Furthermore, despite their                  whether cells are empty or filled, until the actual state of
popularity among puzzle players, little work on nonograms             every cell is known. Just as in crossword puzzles where the
exists in cognitive science, either in the form of empirical          found words provide letters as clues or constraints for the
studies or modeling work. But nonograms have attracted                orthogonally intersecting words, partially solving the cells
attention in other areas. For instance, as we will see in the         on a nonogram line provides additional constraints for the
literature review section, solving nonograms has been                 intersecting lines.
studied mathematically, and a number of machine solvers                 A survey of popular web sites giving advice and tips on
exist. Finally, many rules and strategies for human players           how to solve nonogram puzzles was performed, focusing on
are described in web sites.                                           categorizing advice on selection of a line to solve, or on
   In nonograms, constraints take the form of series of               how to solve a given line. The majority of the advice relates
numbers at the head of each line (row or column) indicating           to solving lines. For instance, an exhaustive set of rules can
the size of blocks of contiguous filled cells found on that           be found on Wikipedia (January 10, 2012 version). In
line. For example, in Figure 1, the first row contains 2              contrast, there is comparatively little advice on how to
blocks of 2 filled cells, whereas row 5 contains no block of          appropriately select the next line to solve, and much of this
filled cells. Blocks have to be separated by at least one
                                                                  1452

advice is given implicitly in commented solutions of                eventually be determined. The final solution of the puzzle is
specific problems. When available, explicit and general-            given in Figure 3.
purpose advice for line selection can be summed up as                  In solving nonograms, the order in which lines are solved
follows: begin with lines for which the constraint is either 0      influences how many steps are necessary to complete the
(in which case all cells on that line are empty) or equal to        solution. A step is defined here as a single iteration on a
the length of the line (in which case, all cells are filled).       specific line to extract the maximum information possible.
Occasionally, advice is also given to the effect that, by
adding the different constraints (including an empty cell           Research on solving nonograms
between each block) one can look for those that fill up a           Nonograms have been studied mathematically, and are
complete line (e.g., the first row in Figure 1 with constraints     known to be NP-complete (Benton, Snow, & Wallach,
2 2 is completely known as XX_XX, with X as filled cells            2006), making search-based solution techniques practical
and _ as empty cells). Skilled players also realize that lines      only for small problems. More sophisticated solving
that contain blocks of large sizes are often a good place to        approaches include rule-based techniques (e.g., Yu, Lee, &
start. Finally, another general piece of advice is to pay           Chen, 2011), use of some intersection mechanism to prune
attention to lines that have changed due to updates of the          inconsistent configurations (e.g., Yen, Su, Chiu, & Chen,
cells of intersecting lines. Except for the last one, these         2010), linear programming (Mingote & Azevedo, 2009),
general advice rules often consider block constraints only,         genetic algorithms (e.g., Batenburg & Kosters, 2004) and a
and do not take into account additional constraints imposed         combination of relaxations and 2-satisfiability approaches
by cells already known to be filled or empty.                       (Batenburg & Kosters, 2009).
   To sum up, strategies for solving lines are well-described          Nonograms also have been used as a tutorial for teaching
as explicit, symbolic rules. In contrast, strategies for            university students about optimization using evolutionary or
selecting lines appear more difficult to capture in explicit,       genetic algorithms (Tsai, Chou, & Fang, 2011).
symbolic terms, except for simple cases.                               To our knowledge, there have been no attempts to use
                                                                    reinforcement learning to solve nonograms.
                   1         2
                   1         1         0        4         3         Research objectives
2         2                                                         Our objective is to compare a solver for selecting the order
1         2                                                         in which to solve lines in nonograms based on
          2                                                         reinforcement learning with three algorithmic methods:
2         1                                                         randomly, heuristically, and optimally (in the shortest
                                                                    number of steps).
          0                                                            We thus ask if an RL-based solver can learn good
                                                                    solutions, that is, solutions that are close to the optimum
    Figure 2 - Partial solution of a 5x5 nonogram puzzle.           (i.e., shortest solution); and how they generalize to unseen
                                                                    problems.
                   1         2
                   1         1         0        4         3                                    Methods
2         2
1         2                                                         Generated nonogram puzzles
          2                                                         Puzzles used for training and testing of the system have a
2         1                                                         size of 5 rows by 5 columns. The state of each cell (filled or
          0                                                         empty) is randomly selected. The puzzle presented in Figure
                                                                    1 was generated in this way, and used in the simulation.
Figure 3 - Solution of the example 5x5 nonogram, where all          Only puzzles that have a unique solution are kept that is,
  cells are known to be filled (black) or empty (white) and         puzzles for which block values correspond to one and only
  where the solution satisfies all the block sizes constraints.     one board configuration. An example of non-unique
                                                                    problem is presented in Figure 4.
   Figure 2 presents an example of a partial solution of a
nonogram puzzle after three steps. Column 3 and row 5                         1         1                           1        1
contain no block of filled cells, and thus all cells on them        1                                      1
are empty. As described above, the first row is completely          1                                      1
known. At this point, the position of the block of size 4 in        Figure 4 - Example of a non-unique puzzle. The right and
column 4 is known, and so is the position of block 3 in             the left configurations both satisfy the block size constraints.
column 5. Even though position of the blocks of size 1 in
rows 2 and 4 cannot be determined yet, by propagating
constraints from solving columns, their positions can
                                                                1453

Training and testing regimes                                        visited, after which a new round of visits is performed
RL solvers are incrementally trained on 3 different problems        starting from the best remaining candidate lines.
(starting with a single problem, and gradually increasing to           Similarly to the general advice and rules that can be found
three problems interleaved in the training set), and tested on      online, the heuristic solver does not take into account the
a novel problem. A justification for these choices is               current state of the problem.
presented in the Discussion section. Training proceeds on a
single problem for 40 episodes, then training proceeds with         Optimal nonogram solver
problems 1 and 2 for 40 more episodes and finally the solver        Here, a breadth-first search finds the minimal number steps.
is trained on all three problems for 40 final episodes. Thus        While this is feasible for the 5x5 puzzles considered here,
training involves 120 episodes in total. The reason for using       experiments with larger puzzle sizes suggest that search
this interleaved, incremental training (which rehearses             time is prohibitively long -- a well-established result for NP-
problems already learned) is to avoid catastrophic                  complete problems, as mentioned1.
interference (McCloskey & Cohen, 1989).
                                                                    Reinforcement learning (RL) solver
Optimal solver for lines                                            The reinforcement learning (RL) solver learns the expected
                                                                    value of lines using a reinforcement algorithm called
Before discussing approaches to the selection problem, it is        SARSA (Sutton & Barto, 1998). SARSA learns estimates of
important to emphasize that simulations use an optimal line         the expected value or quality (Q) of future rewards for every
solver for solving a given line once it is selected. This           state-action pair. Its learning rule is
optimal line solver can find all the new cells that can be             Q(st,at)  Q(st,at) + α [ rt+1 + γ Q(st+1,at+1) - Q(st,at)]
declared as filled or empty.                                        where Q is the predicted reward (sometimes called Quality),
   To find these cells, the optimal line solver module first        s is a state, a is an action, r is a reward, and indices t and t+1
generates every possible position of all blocks (constraints)       are used for current and next states and actions respectively;
in the line consistent with the cells already determined as         α is the learning rate, and γ is the discount factor. The name
filled or empty. Second, it computes the intersection of all        SARSA is based on the quintuple that the algorithm uses (st,
these possible positions. Finally, it identifies cells that are     at, rt+1, st+1, at+1).
always filled or always empty in these intersections as such           In the present simulations, we use a learning rate of 0.9,
cells are now known for sure to be filled or empty.                 and a discount factor of 0.9 to reward the shorter solutions
   This approach covers rules and strategies typically given        more highly. States, actions and rewards are described
to players for solving lines of nonograms, and implements           below.
rules described in other solvers (e.g., Yu et al., 2011). For
instance, with a line that is currently blank (all unknowns),          States
this method implements the rules described in Wikipedia
(January 10, 2012 version) under "Simple Boxes" and                    For a given action (a choice of a line), the state given to
"Simple spaces".                                                    the RL solver consists of the states of the cells on the
                                                                    corresponding line. In other words, the RL solver gets
Modules for selecting lines to solve                                information that is directly relevant to the line considered,
                                                                    as the action will only affect the cells in the line considered.
Given this optimal line solver for solving lines, we turn to
                                                                    In human problem solvers, this corresponds to an attention
the issue of selecting the next line (either a row or a column)
                                                                    mechanism that focuses on the relevant elements, rather
to solve.
                                                                    than seeing the complete problem configuration.
                                                                       The state of a line consists of two elements: (1) the value
Random solver                                                       of the block sizes coded using the integer corresponding to
A random solver randomly selects the next line to solve,
                                                                    the number of cells in the block; and (2) states of cell on the
with replacement, from lines that are not completely solved
                                                                    line, which can take three values: unknown (U), filled (F) or
already. Selection with replacement allows taking the same
                                                                    empty (E), For example, in Figure 2, the state of column 1 is
line twice in a row.
                                                                    [1, 1, 0, F, U, U, U, E]. Note that learning begins in the
                                                                    initial state in which all cells of the board are Unknown, and
Heuristic solver                                                    that, for lines or columns composed of 5 cells, there cannot
A heuristic solver, inspired by the advice given to humans
                                                                    be more than three blocks
described above, selects order as follows. First, it chooses
lines that have no block (easy case to solve because all cells
are blank) and lines that are filled completely (e.g., 2 2).
Then other lines are sorted and chosen in decreasing order             1
of the largest block value. Lines with the same score (i.e.,             As an approximation, if each line had to be solved once and
                                                                    only once, there would be (n+n)! combinations of sequences,
ties) are selected in the order in which they appear in the         which quickly increases with n. For n = 5, we get 3.6 million
puzzle (rows then columns). Selection of the next line to           combinations, for n = 6, we get 479 millions; and n = 7, we
solve is done without replacement until all lines have been         already have 8.7x1010 (In practice, lines often need to be visited
                                                                    multiple times; or in rare cases not need to be visited at all).
                                                                1454

  Actions                                                                 The neural network used is called sibling-descendent
   An action is a choice of the next line to solve. As there are       cascade-correlation (SDCC: Baluja & Fahlman, 1994), a
5 rows and 5 columns, the maximal number of actions                    variant of cascade correlation (CC: Fahlman & Lebiere,
available is 10. As soon as a line is completely solved, it is         1990) with reduced network depth. CC has been
not further included in the list of actions considered for             successfully used to model learning and cognitive
selection.                                                             development in numerous tasks (Shultz, 2003). Whereas
   Given a list of actions and associated Q values, the RL             default SDCC parameters are optimized for pattern
solver needs to choose an action to execute. Action selection          classification, more appropriate parameter values were
is performed with replacement (i.e., the same action can be            selected here for function approximation, namely to allow
taken again before all actions are visited). In testing mode,          for longer input and output phases (see details in:
the solver uses a greedy technique called Hardmax which                Dandurand et al., 2012). Input and output phases were
always selects the action with the largest Q value. In                 allowed to last for 200 epochs with a patience of 50 epochs.
contrast, the solver uses a Softmax approach when in                   Change threshold was set to 0.01 for input phases, and
learning mode to allow exploration of the problem space.               0.002 for output phases. Finally, the score threshold
Under Softmax, every possible action can potentially be                parameter was set to 0.025, a value that is small enough to
selected with a probability of selection increasing with the Q         approximate the targets well while limiting overfit.
value.                                                                    A cache system is used to interface SARSA and SDCC
   We limit the number of steps that are allowed for finding           because they have different processing requirements. More
a solution to 25, a value that is large enough to find a               specifically, SARSA updates its approximation function Q
solution by randomly selecting actions (see Random in                  after every action (called online learning). In contrast,
Figure 5). When failing to solve within 25 episodes in                 learning in SDCC involves multiple patterns (input-output
testing mode, the RL solver is typically stuck repeatedly              pairs) at once (called batch learning). SARSA updates the
selecting an action which does not yield any progress.                 cache buffers until there are enough patterns to make a
                                                                       batch to train CC; details can be found in Rivest and Precup
  Rewards                                                              (2003).
  To provide rewards for line selection, we use a variant of
a distance-reduction heuristic called distance-based rewards                                                                 Results
(Dandurand, Shultz, & Rey, 2012). Here, we return a reward             First, we investigate the characteristics of the puzzles used
equal to the proportion of the currently unknown cells that            for testing and training using the three algorithmic solvers.
are determined as filled or empty as a result of selecting this        A sample of 20 simulations was run, with each simulation
action (i.e., solving this line). For instance, if cell states of      learning 3 different puzzles and different random
the line are [U F U U E] before performing the action and              initializations of neural networks. The numbers of steps
[U F F U E] after, then 1 of the 3 unknowns were                       necessary to solve these puzzles, plotted in Figure 5, suggest
discovered, and this action would be rewarded with 0.33.               that puzzles are well-matched across training sessions and
                                                                       testing.
  Storing Q values                                                                                                        Random    Heuristic   Optimal
  We compare two systems for storing the rewards                                                              25
associated with state-action pairs. The first one is a classical
                                                                            Number of steps to solve puzzle
lookup table which, for each unique state-action pair                                                         20
encountered, stores the value of the reward. Values need to
be initialized to a non-zero value (here, we used 0.05) so                                                    15
that probabilities of selection are non-null under Softmax.
                                                                                                              10
  The second system consists of a connectionist function
approximator. Instead of explicitly storing all Q values, a                                                   5
neural network is used to generate an approximation of the
Q value as a single, real-valued output, taking the state-                                                    0
action pair as input. The three possible cell states are coded                                                     Test        Train0           Train1    Train2
using 2 bits: Filled (1 0), Empty (0 1) or Unknown (0 0).
Thus, a line state is a 13x1 vector (3 block values + 5 cells *            Figure 5 - Number of steps necessary to solve test and
2 bits per cell). Actions are coded as a 10x1 vector (5 rows               training nonogram puzzles using the three algorithmic
then 5 columns) of binary values with the bit at the                          solvers. Error bars represent standard errors (SE).
corresponding location set to 1, all others set to 0. Thus, for
column 1 in Figure 2, the neural network function                         Figure 6 shows performance results for the lookup table.
approximator receives as input the vector [1 1 0 1 0 0 0 0 0           As we can see, it learns near-optimal solutions for the
0 0 0 1 0 0 0 0 0 1 0 0 0 0], corresponding to the                     training material (M = 8.5 and M = 7.6 for RL and optimal
concatenation of state and action.                                     nonogram solvers respectively); outperforming the heuristic
                                                                    1455

solver. However, RL solvers based on a lookup table do not                     when good action is just below. We get back to this issue in
generalize at all; performance on the test set reaching the                    the discussion.
maximal 25 steps on all 20 replications.                                          Finally, we compare the rewards learned by the SDCC-
                                 30                                            based RL solver with the scores given by the heuristic
                                                                               solver for the first move (i.e., step 1). We find a small but
                                 25                                            significant correlation of r = 0.18, p = 0.01, This suggests
                                                              Training set 1
                                                                               that RL solvers learned good solutions without needing to
      Number of steps to solve
                                                              Training set 2
                                 20                                            fully implement the advice often given to human players for
                                                              Training set 3
                                                                               solving the initial board condition. This is unsurprising, as
                                 15                           Test set         the rules given to humans may not be optimal, and, with
                                                              Random           many steps necessary to solve these puzzles (at least about 9
                                 10                           Heuristic        steps), there are many possibilities to the optimization
                                                              Optimal          process beyond step 1.
                                 5
                                 0
                                                                                                       Discussion
                                                                               In general, performance differences between the heuristic
Figure 6 - Number of steps to solve nonogram puzzles with                      and the RL solvers can be explained by the better choices
the RL solver using a lookup table, compared with the three                    that RL solvers make beyond the first solution step when the
     algorithmic solvers, with standard error (SE) bars                        board is in its initial state. As mentioned, it is difficult to
                                                                               describe explicit and general purpose strategies for choosing
                                                                               lines to solve when the cells already solved place additional
                                 30
                                                 Connectionist function
                                                                               constraints. In fact, none of the online advice gives such
                                 25              approximator                  explicit rules, but some advice implicitly provides guidance
                                                 Connectionist trials not at   when describing solutions for specific problems. Similarly,
                                                 ceiling                       RL solvers implicitly learn good choices for the next line to
                                 20
      Steps to solve
                                                 Lookup table                  solve at any point in the solution, leading to near optimal
                                 15                                            solutions.
                                                 Random
                                 10
                                                 Heuristic
                                                                               Constraints on the choice of actions
                                                                               As mentioned, by choosing the most flexible selection
                                 5               Optimal                       strategy, the one with replacement, networks need to learn
                                                                               their way out of repeatedly selecting the same action,
                                 0
                                                                               leading to no further progress. In future research, different
 Figure 7 - Generalization performance (i.e. test set) of RL                   ways to handle this issue could be explored by placing
  solvers using connectionist function approximators and                       additional constraints on action choice. For instance, RL
lookup tables, with a ceiling at 25 steps, compared with the                   solvers could keep track of the last N action choices made,
  three algorithmic solvers, with standard error (SE) bars                     and avoid selecting from these. With N = 0, we get the
                                                                               present "with replacement" scenario, whereas N = 10 (5
  Next, we investigate generalization performance                              rows and 5 columns) would correspond to the "without
comparing the lookup table with the connectionist function                     replacement" scenario. Imposing this constraint would
approximator (see Figure 7). As we can see, the                                sidestep the need to learn their way out of repeated action
connectionist function approximator does generalize to good                    choices.
solutions of 9.1 steps (in the second column) for 7 of the 20
simulations. The other 13 are at ceiling, which, when                          Cognitive modeling
included makes the average shown in the first column.                          To our knowledge, there are no experimental studies of
Networks that reach the ceiling value typically get stuck                      humans solving nonogram puzzles. The present simulations
selecting repeatedly an action that does not yield progress,                   make testable quantitative predictions about how humans
due to the use of Hardmax and a selection strategy with                        choose which lines to solve, thus providing some guidance
replacement. Because Hardmax is only sensitive to the                          for such research
action with the largest Q value, the choice of action does not                   As a cognitive model, the present system is hybrid, with a
reflect learning all other actions2. In particular, Hardmax                    symbolic system for solving a given line, and a
may select a poor action that obtains the best Q value, even                   reinforcement learning with neural network support for
  2                                                                            choosing an appropriate ordering of lines. This modeling
    We have considered presenting results with Softmax, which
                                                                               approach is grounded in evidence for both implicit and
does reflect learning of all actions. However, we found too much
variability, and that performance was inflated due to capitalization           explicit cognitive processes (e.g., Reber, 1989). With
on chance (randomly selecting good actions within the ten                      Clarion (Sun, 2006) as a notable exception, modeling work
available).                                                                    on problem solving has mostly focused on explicit symbol
                                                                          1456

manipulation. The present work addresses the implicit              Fahlman, S. E., & Lebiere, C. (1990). The cascade-
aspects of learning how to solve problems. The fact that                    correlation learning architecture. In D. S.
advice and strategies found online have little to say about                 Touretzky (Ed.), Advances in neural information
the order in which to select lines suggests that it may well                processing systems 2 (pp. 524–532). Los Altos,
be an implicit task, difficult to verbalize as explicit rules.              CA: Morgan Kaufmann.
                                                                   McCloskey, M., & Cohen, N. J. (1989). Catastrophic
Towards a universal solver                                                  interference in connectionist networks: The
Our reinforcement learning (RL) solvers learn near-optimal                  sequential learning problem. In G. H. Bower (Ed.),
ordering of lines to solve nonogram puzzles, outperforming                  The psychology of learning and motivation (pp.
a heuristic solver based on general purpose rules for line                  109–165). San Diego: Academic Press.
selection. Our solvers show that multiple problems (here, 3)       Mingote, L., & Azevedo, F. (2009). Colored nonograms: an
can be learned by a single system, and that about half of                   integer linear programming approach. Progress in
them generalize to a novel problem when coupled with a                      Artificial Intelligence, 213–224.
function approximator to compute, rather than merely               Reber, A. S. (1989). Implicit learning and tacit knowledge.
stores, expected rewards. These results are so far limited to               Journal of experimental psychology: general,
puzzles of relatively small size.                                           118(3), 219–235.
  For future work, we could study generalization to larger         Rivest, F., & Precup, D. (2003). Combining TD-learning
puzzle sizes. Long term goals could include the design a                    with      Cascade-correlation       networks.     the
universal solver that could solve any nonogram puzzle of                    Proceedings of the twentieth International
various sizes nearly optimally. We tried training a RL solver               Conference on Machine Learning (ICML) (pp.
with different, randomly generated nonograms on every                       632–639).
learning episode, exploring some of the many simulation            Russell, S., & Norvig, P. (2003). Artificial intelligence, a
parameters (e.g., learning rates). These early attempts                     modern approach. Second edition. Upper Saddle
suggest that the task is difficult. In addition to the search               River, NJ: Prentice Hall.
space being very large, the function approximator appears to       Shultz, T. R. (2001). Constraint satisfaction models. In J.
have difficulty learning stable representations when there is               Smelser & P. B. Baltes (Eds.), International
very high variability. Future research could also explore                   Encyclopedia of the Social and Behavioral
how to learn a larger number of problems (e.g., 10 or 100)                  Sciences (pp. 2648–2651). Oxford: Pergamon.
in a reasonable time. Preliminary results suggest that it may      Shultz, T. R. (2003). Computational developmental
be beneficial to gradually increase puzzle size.                            psychology. Cambridge, MA: MIT Press.
                                                                   Sun, R. (2006). The CLARION cognitive architecture:
                   Acknowledgments                                          Extending cognitive modeling to social simulation.
                                                                            In R. Sun (Ed.), Cognition and multi-agent
We thank Arnaud Rey for helpful comments on an earlier                      interaction. New-York, NY: Cambridge University
draft. This work is supported by a post-doctoral fellowship                 Press.
to FD and a grant to TRS, both from the Natural Sciences           Sutton, R. S., & Barto, A. G. (1998). Reinforcement
and Engineering Research Council of Canada.                                 learning: an introduction. Cambridge, MA: MIT
                                                                            Press.
                         References                                Tsai, J. T., Chou, P. Y., & Fang, J. C. (2011). Learning
Baluja, S., & Fahlman, S. E. (1994). Reducing network                       Intelligent Genetic Algorithms Using Japanese
          depth in the cascade-correlation ( No. CMU-CS-                    Nonograms. Education, IEEE Transactions on,
          94-209). Pittsburgh: Carnegie Mellon University.                  (99), 1–1.
Batenburg, K., & Kosters, W. (2004). A discrete                    Yen, S. J., Su, T. C., Chiu, S. Y., & Chen, J. C. (2010).
          tomography approach to Japanese puzzles.                          Optimization of Nonogram’s Solver by Using an
          Proceedings of the 16th Belgium-Netherlands                       Efficient      Algorithm.      Technologies      and
          Conference on Artificial Intelligence (BNAIC) (pp.                Applications of Artificial Intelligence (TAAI), 2010
          243–250).                                                         International Conference on (pp. 444–449).
Batenburg, K., & Kosters, W. (2009). Solving Nonograms             Yu, C. H., Lee, H. L., & Chen, L. H. (2011). An efficient
          by combining relaxations. Pattern Recognition,                    algorithm for solving nonograms. Applied
          42(8), 1672–1683.                                                 Intelligence, 35(1), 18–31.
Benton, J., Snow, R., & Wallach, N. (2006). A
          combinatorial problem associated with nonograms.
          Linear algebra and its applications, 412(1), 30–38.
Dandurand, F., Shultz, T. R., & Rey, A. (2012). Including
          cognitive biases and distance-based rewards in a
          connectionist model of complex problem solving.
          Neural Networks, 25, 41–56.
                                                               1457

