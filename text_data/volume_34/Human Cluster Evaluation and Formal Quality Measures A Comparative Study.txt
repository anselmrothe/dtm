UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Human Cluster Evaluation and Formal Quality Measures: A Comparative Study
Permalink
https://escholarship.org/uc/item/8rr3t8dx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Lewis, Joshua
Ackerman, Margareta
de Sa, Virginia
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

 Human Cluster Evaluation and Formal Quality Measures: A Comparative Study
               Joshua M. Lewis                             Margareta Ackerman                             Virginia R. de Sa
             josh@cogsci.ucsd.edu                          mackerma@uwaterloo.ca                       desa@cogsci.ucsd.edu
            Dept. of Cognitive Science              Cheriton School of Computer Science               Dept. of Cognitive Science
       University of California, San Diego                   University of Waterloo               University of California, San Diego
                              Abstract                                     dan, & Weiss, 2002) Up until now, clustering quality mea-
   Clustering quality evaluation is an essential component of clus-
                                                                           sures and human judgment were considered complementary
   ter analysis. Given the plethora of clustering techniques and           approaches to clustering evaluation. Most papers that present
   their possible parameter settings, data analysts require sound          novel clustering algorithms include these two types of evalu-
   means of comparing alternate partitions of the same data.               ations separately.
   When proposing a novel technique, researchers commonly ap-
   ply two means of clustering quality evaluation. First, they ap-            Our study compares formal CQMs with human evalua-
   ply formal Clustering Quality Measures (CQMs) to compare                tions to determine how often they agree, and whether cer-
   the results of the novel technique with those of previous algo-
   rithms. Second, they visually present the resultant partitions          tain CQMs correlate better with human judgments than oth-
   of the novel method and invite readers to see for themselves            ers. We also evaluate the consistency of human responses–if
   that it uncovers the correct partition. These two approaches            humans are very inconsistent, then it is unlikely that they are
   are viewed as disjoint and complementary.
                                                                           good judges of cluster quality (an ideal measure is stable on
   Our study compares formal CQMs with human evaluations us-               the same partition). Further, we separate our human subjects
   ing a diverse set of measures based on a novel theoretical tax-
   onomy. We find that some highly natural CQMs are in sharp               into expert and non-expert groups to determine whether clus-
   contrast with human evaluations while others correlate well.            tering evaluation requires experience, and identify divergent
   Through a comparison of clustering experts and novices, as              strategies between the groups.
   well as a consistency analysis, we support the hypothesis that
   clustering evaluation skill is present in the general population.          To sharpen our focus on a small set of CQMs, we con-
   Keywords: clustering; validity indices; psychophysics; visual           struct a property-based taxonomy of CQMs that distinguishes
   perception; machine learning                                            them on grounds beyond their particular mathematical formu-
                                                                           lations. The CQMs selected for the study are diverse in that
                          Introduction                                     they each satisfy a distinct set of these properties.
Clustering is a fundamental data analysis tool that aims to                   Previous studies have investigated how humans choose the
group similar objects. It has been applied to a wide range                 number of groups (Lewis, 2009) and partition data (Santos
of disciplines such as astronomy, bioinformatics, psychology,              & Sá, 2005) in a clustering setting, but these approaches only
and marketing. Successful clustering often requires using a                show what humans think are the optimal partitions rather than
number of different clustering techniques and then compar-                 how they judge partition quality in general. Our study uses
ing their output. The evaluation of clusterings is an integral             a set of non-optimal partitions that humans partially order by
part of the clustering process, needed not only to compare                 quality, giving us more detailed quality judgments than in past
partitions to each other, but also to determine whether any of             work. Intuitively, in (Lewis, 2009) and (Santos & Sá, 2005)
them are sufficiently good.1                                               subjects took on the role of a k-choosing algorithm and a clus-
   As there is no universal clustering objective, there is no              tering algorithm (respectively), whereas in this study subjects
consensus on a formal definition of clustering. As a re-                   are in the role of clustering evaluators.
sult, there are a wide variety of Clustering Quality Measures                 Our main findings are as follows. Many CQMs with nat-
(CQMs), also known as internal validity indices, that aim to               ural mathematical formalizations disagree with human eval-
evaluate the quality of clusterings. To compare clusterings,               uations. On the other hand, we identify CQMs whose evalu-
researchers often select a CQM, which assigns a numerical                  ations are well correlated with those of humans. In particu-
value to a partition representing its quality.                             lar, we find that Silhouette (Rousseeuw, 1987) and Calinski-
   Researchers rarely rely on CQMs alone. There is a deep                  Harabasz (Caliński & Harabasz, 1974) are highly correlated
implicit assumption running through the clustering literature              with human evaluations. Our findings also indicate that there
that human judgment of clustering quality is quite good. Au-               is sufficient similarity between the evaluations of novices and
thors visually present the resultant partitions and invite read-           experts to suggest that clustering evaluation is a task that does
ers to see for themselves that the new method performs well.               not require specific training (though it may benefit from train-
To take one example, in their influential paper on spectral                ing). This opens the door for using human computation re-
clustering Ng, Jordan and Weiss write, “The results are sur-               sources such as Amazon’s Mechanical Turk to quickly solicit
prisingly good... the algorithm reliably finds clusterings con-            a large number of clustering quality judgments from novices
sistent with what a human would have chosen.” (Ng, Jor-                    as part of the data analysis process. Nevertheless, experts
    1 If no good clusterings have been found the underlying dataset        show much less sensitivity to the number of clusters and re-
may have no good clustering (the data is not “clusterable”,                late more closely to a greater range of clustering quality mea-
see (Ackerman & Ben-David, 2009) for more on clusterability).              sures than novices, indicating a nuanced approach to the eval-
                                                                      1870

uation problem. Regarding consistency, we find that even                                    where n is the number of points in the dataset.
novices are more consistent in their evaluations than our set
of CQMs.                                                                                                              Methods
                                                                                            We ran two groups of human subjects and a group of clus-
                    Clustering quality measures                                             tering quality measures on a partition evaluation task. Our
                                                                                            human subjects were divided into a novice group with little
In this section we introduce the formal machinery describing
                                                                                            or no knowledge of clustering methods and an expert group
the CQMs selected for our study.
                                                                                            with detailed knowledge of clustering methods.
    Let X be a finite domain set. A distance function is a sym-
metric function d : X × X → R+ , such that d(x, x) = 0 for all
x ∈ X. A k-clustering C = {C1 ,C2 , . . . ,Ck } of dataset X is a
partition of X into k disjoint subsets (so, ∪iCi = X). A clus-
tering of X is a k-clustering of X for some 1 ≤ k ≤ |X|. Let
|C| denote the number of clusters in clustering C. For x, y ∈ X
and clustering C of X, we write x ∼C y if x and y belong to the
same cluster in C and x 6∼C y, otherwise. Finally, a CQM is a
function that maps clusterings to real numbers.
    Gamma: This measure was proposed as a CQM by (Baker
& Hubert, 1975) and it is the best performing measure in
(Milligan, 1981). Let d + denote the number of times that a
pair of points that was clustered together has distance smaller
than two points that belong to different cluster, whereas d −
denotes the opposite result.
    Formally, let d + (C) = |{{x, y, x0 , y0 } | x ∼C y, x0 6∼C
y , d(x, y) ≤ d(x0 , y0 )}|, and d − (C) = |{{x, y, x0 , y0 } | x ∼C
  0
y, x0 6∼C y0 , d(x, y) ≥ d(x0 , y0 )}|. The Gamma measure of C is
 d + (C)−d − (C)
 d + (C)+d − (C)
                   .
    Silhouette:             The Silhouette measure was defined
by (Rousseeuw, 1987). Silhouette is the default clus-
tering quality measure in MATLAB.
    Let dist(x,Ci ) = avgy∈Ci d(x, y).                  The silhouette of
a point x with respect to clustering C is S(x,C) =
    min j6=i dist(x,C j )−dist(x,Ci )
 max(min j6=i dist(x,C j ),dist(x,Ci )) where x ∈ Ci . The silhouette of a
clustering C is sumx∈X S(x,C).
    Dunn’s Index: Dunn’s Index (Dunn, 1974) compares the
maximum within-cluster distance to the minimum between-
                                                     minx6∼ y d(x,y)
cluster distances. Dunn’s Index of C is maxx∼C y d(x,y) .
                                                            C
    Average Between and Average Within: The Average Be-
tween and Average Within measures evaluate the between-
cluster separation and within-cluster homogeneity, respec-
tively. The average between of C is avgx6∼C y d(x, y). The av-
erage within of C is avgx∼C y d(x, y).
    Calinski-Harabasz: The Calinski-Harabasz measure
(Caliński & Harabasz, 1974) makes use of cluster centers.                                  Figure 1: All stimuli. Datasets are in rows; partitions are in
Let ci = |C1i | ∑x∈Ci x denote the center-of-mass of cluster Ci ,                           columns.
and x̄ the center-of-mass of X. Let B(C) = ∑Ci |Ci ||ci − x̄|2
and W (C) = ∑Ci ∑x∈Ci |x − ci |2 . The Calinski-Harabasz of C                               Human subjects and stimuli
is   n−k
     k−1   · WB(C)
               (C) .                                                                        Twelve human subjects were recruited for this project as the
    Weighted inter-intra: The weighted inter-intra measure                                  novice group, 9 female and 3 male, with an average age of
is proposed by (Strehl, 2002). It compares the homogeneity                                  20.3 years. The novice subjects have no previous exposure to
of the data to its separation. Let intra(Ci ) = avgx,y∈Ci d(x, y)                           clustering. The expert group consists of 5 people and includes
and inter(Ci ,C j ) = avgx∈Ci ,y∈C j d(x, y). The Weighted inter-                           the authors of this paper. All experts have studied clustering
                                                          1
                                                     ∑i n−|C
                                                            i|
                                                               ∑ j6=i inter(Ci ,C j )       in an academic setting, and 4 have done research on the sub-
intra of a clustering C is (1 − 2k        n ) · (1 −                                  ),
                                                         ∑i |C 2|−1 intra(Ci )              ject.
                                                               i
                                                                                        1871

    We used 19 different two dimensional datasets to generate        classifications. To make the responses as comparable as pos-
our clustering stimuli, drawn from (Lewis, 2009), and chosen         sible we normalize response vectors to a length of one within
to represent a range of dataset types including mixtures of          each dataset. Human subjects have to classify two positive
Gaussians and datasets with hierarchical structure. In order         and one negative partition per dataset, even if every partition
to maintain responsiveness of the stimulus presentation inter-       is quite bad, so by normalizing within dataset we make the
face, we subsampled 500 points randomly from each dataset.           CQM responses similar in structure—partitions are judged
We use synthetic datasets in order to better generate a wide         only relative to other partitions within a dataset.
range of stimuli, and our datasets are 2D to facilitate visual-         Because cluster centroids are chosen randomly, increasing
ization.                                                             k is likely to increase the chance of getting an undesirable
    Each dataset is randomly clustered nine times in the fol-        partition (e.g. a partition with very few data points). Ad-
lowing manner. For each of the nine clusterings, we first draw       ditionally, partitions with higher k require more effort to in-
the number of partitions, k, from a uniform distribution over        terpret, and therefore we might expect novice subjects to be
the integers 2 to 6. Second we choose cluster centroids us-          biased towards a lower k. For these reasons our correlations
ing two strategies: for four of the clusterings we randomly          control for k by partialing out a vector of k values for each
select k centroids from the original dataset, and for five of the    partition. Geometrically this is equivalent to projecting each
clusterings we select k centroids from a Laplacian Eigenmap          response vector onto the hyperplane orthogonal to the vector
embedding of the data. Finally we color points based on the          of k values.
identity of their nearest centroid in the appropriate space. The
goal of this approach is to create stimuli with varied cluster-                                  Results
ing quality.                                                         Correlation
    Each trial consisted of all nine different partitions of the
same dataset randomly arranged per trial in a 3 by 3 grid            Table 1 shows correlation coefficients between all measures
(see Figure 1 for a visualization of all the stimuli). The           for both expert and novice responses, with k factored out. The
datasets were shown as scatter plots with colored points on          correlation between expert and novice human positive clas-
a black background to reduce brightness-related eye strain.          sifications is higher than the correlation between any CQM
For novice subjects, trials were organized into three blocks         and either human positive classification. The negative human
of 19, where each dataset appeared once per block and the            classifications have a similarly high correlation. The absolute
order of the datasets within each block was randomized. Ex-          values of the correlation coefficients between CQMs and ex-
pert subjects were tested on one block of non-randomized             pert classifications are strictly greater than or equal to those
datasets. We instructed subjects to choose the two best par-         between CQMs and novice classifications, indicating a closer
titioned displays and the one worst partitioned display from         relationship between expert strategies and the dataset char-
the nine available on every trial (leaving six displays implic-      acteristics summarized by the CQMs when k is factored out.
itly chosen as neutral).                                             k itself correlates very strongly with the novices and less so
                                                                     with the experts. Silhouette provides the best overall correla-
Analysis                                                             tion with expert classifications, and Avg Within provides the
We analyzed our novice subjects for internal consistency of          best overall correlation with novice classifications (save k).
their positive and negative classifications across blocks and
found that even our least consistent subject performed well          Consistency
above chance. We did not exclude any subjects due to in-             The most undesirable form of inconsistency across subjects
consistency and we did not analyze internal consistency for          or CQMs is both positive and negative responses to the same
experts as they were only tested on one block.                       stimulus. For experts, stimuli with a number of positive clas-
    To analyze consistency across subjects we use Fleiss’            sifications 3 or higher never receive a negative classification,
κ (Fleiss, 1971) and include neutral responses. Fleiss’ κ            and only once does this occur for stimuli with 2 positive re-
measures the deviation between observed agreement and the            sponses. In contrast the CQMs exhibit much more disagree-
agreement attributable to chance given the relative frequency        ment and novices seem to fall somewhere in between. The
of classifications and normalized for the number of raters.          quantitative measure κ bears this out: CQMs score 0.128,
Neutral classifications are twice as frequent as non-neutral,        novices score 0.183 and experts score 0.213. κ ranges from
and positive classifications are twice as frequent as negative       −1 to 1, with −1 representing complete disagreement, 1 rep-
classifications, so the compensation for relative frequency in       resenting complete agreement and 0 representing the amount
Fleiss’ κ makes it well-suited to our data. In addition, we          of agreement expected by chance. While there is no standard
perform a consistency analysis on the clustering quality mea-        significance test for differences in κ, the rating scale sug-
sures by discretizing their classifications in a manner similar      gested by Landis and Koch (Landis & Koch, 1977) would
to the human data.                                                   characterize the CQM and novice rater groups each as in
    We analyze the relationship between novice classifications,      slight agreement, and the expert raters as in fair agreement.
expert classifications and clustering quality measures by cal-       To test whether any one measure was significantly harming
culating the Pearson’s correlation coefficient, ρ, between           CQM consistency we left each out in turn from the analysis
                                                                 1872

Table 1: Correlation coefficients between human responses and CQMs with k factored out (except for the k column). Text in
bold (excluding k column) if p < .0025 after Bonferroni correction for n = 20 comparisons per subject group and α = .05.
                                Expert Positive    Expert Negative    Novice Positive   Novice Negative                                    Avg Within                     W-Inter/Intra
                                                                                                           Gamma      Silhouette    Dunn                Avg Btw
                           ρ                                                                                                                                       CH                       k
                  Expert Pos       1              -.35               .56                -.19              -.15        .46           .40    -.39         .34       .44     .19             -.43
                 Expert Neg                         1                -.13                .44               .09       -.27          -.12     .44         -.18      -.36   -.30              .32
                 Novice Pos                                            1                -.04              -.13        .39           .40    -.20          .23       .30    .04             -.73
                 Novice Neg                                                               1                .08       -.27           .01     .30         -.07      -.25   -.27              .71
                                                                                                                      and novice evaluations, on both the positive and negative clas-
Table 2: A summary of the number of partitions for which a
                                                                                                                      sifications.
high degree of agreement was achieved by the raters. If a par-
tition is classified as negative or positive by 80% - 100% of                                                         Comparing experts with novices
raters, it would be added to the top row, and similarly for the                                                       Evaluations of experts and novices have a correlation score
60% - 79% bucket. The total possible number of agreed upon                                                            of 0.56, higher than the correlation of any CQM with any of
partitions is 57 (19 datasets * 3 possible negative/positive re-                                                      the two subject groups. This suggests that a cluster evaluation
sponses to partitions per dataset).                                                                                   skill is present in the general population.
         % Majority       Experts                 Novices                          CQMs                                  On the other hand, we observe some interesting differences
        80% - 100%          19                       3                               1                                between the two groups of subjects. One of the most no-
         60% - 79%          20                      11                               7                                table differences between experts and novices is that, while
       Sum >= 60%           39                      14                               8                                both groups prefer clusterings with fewer clusters, novices
                                                                                                                      rely much more heavily on this heuristic.
                                                                                                                         Experts seem to use more, and more complex strategies
and found values ranging from 0.098 to 0.172, which is in                                                             than novices. Positive expert classifications correlate well
line with the CQM consistency with no measure left out, and                                                           with two more measures than positive novice classifications.
in every case less consistent than the novice subjects. Finally,                                                      No measure considered correlates better with novice classifi-
we left out both Avg Within and Between, since they measure                                                           cations than with expert classifications, and in the great ma-
quality on intentionally simple and distinct dimensions, and                                                          jority of cases the correlation is higher with expert classifica-
found a κ of 0.110.                                                                                                   tions.
   In Table 2 we summarize the consistency of experts,                                                                   With a cover of at most six domain elements on any input
novices and cluster quality measures. It shows how often cer-                                                         dataset (see Definition 5 below), Dunn’s measure is (accord-
tain percentages of raters are able to agree on negative or pos-                                                      ing to this measure of complexity) the simplest measure that
itive classifications for particular stimuli. Experts agree over                                                      we explore. While positive expert evaluations correlate well
60% of the time on more samples (39), than do novices (14)                                                            with five distinct measures, Dunn’s measure is one of three
or CQMs (8).                                                                                                          measures that correlate well with novice evaluations. This
                                                                                                                      further illustrates that novices rely on fewer simpler strate-
                        Discussion                                                                                    gies, which indicates that expert evaluations may be more so-
                                                                                                                      phisticated and reliable.
Comparing human evaluations with CQMs
Some natural quality measures have low correlation with hu-                                                           Consistency
man evaluations. Most notably, Gamma has low correlation                                                              Given the difficulty of knowing whether humans or CQMs do
with both positive and negative human classifications for both                                                        a reasonable job of evaluating clustering quality, one might
novices and experts. W-Inter/Intra has low correlation with                                                           hope that at least they are consistent across individuals (or
the positive classifications of both subject groups. This shows                                                       measures). Consistency indicates that some repeatable pro-
that a natural mathematical formalization does not suffice to                                                         cess is at work and that its repeatability is minimally affected
guarantee that the evaluations of clusterings produced using                                                          by changes in input. Of course CQMs are perfectly consistent
the CQM will seem natural to humans.                                                                                  on a within measure basis—given the same partition they will
   There are also CQMs that correlate well with human eval-                                                           always report the same quality–and one is tempted to suggest
uations. Of these the most notable are CH and Silhouette.                                                             that between measure consistency is an unfair point of com-
These two popular measures correlate well with both expert                                                            parison; aren’t all the measures using quite different evalu-
                                                                                                                   1873

ative procedures, and didn’t we select them to be distinct?
                                                                        Table 3: A taxonomy of the seven quality measures used in
We did, but CQMs purport to evaluate clustering quality in
                                                                        the study.
general. Insofar as they evaluate this more nebulous property
                                                                                                                                                         W-Inter/Intra
they should be consistent, even if their methods differ. As it
                                                                                                         Silhouette          Avg Within
turns out, they are somewhat consistent with each other, just
                                                                                                 Gamma                                    Avg Btw
not as consistent as humans. Further, the consistency story
did not vary when we tested all the leave-one-out subsets of                                                          Dunn                          CH
CQMs, indicating that CQM consistency is not being skewed
by just one divergent measure.                                                Order-consist.     X       X            X      X            X         X    X
   Human experts are the most consistent group in this study.                  Sep-invariant     X       X            X      X            X         X    X
This lends empirical support to the common practice of seek-                  Hom-invariant      X       X            X      X            X         X    X
ing human visual evaluations of partition quality. Novices are                     Bounded       X       X            X      X            X         X    X
less consistent, and as discussed above there is evidence that               Constant Cover      X       X            X      X            X         X    X
the evaluations they provide are less sophisticated. Despite                    Norm-based       X       X            X      X            X         X    X
the unfavorable comparison to experts, it is notable that sub-
jects with no formal knowledge of cluster analysis are able
to respond more consistently than a set of CQMs. This lends             over normed vector spaces. Normed CQMs take a quadruple
credence to the notion that our ability to evaluate partitions is       of the form (V, X,C, k · k), where V is a vector space, X a fi-
acquired in the natural course of visual development.                   nite subset of V , and k · k is a norm over V . Normed CQMs
                                                                        can rely on centers-of-mass of clusters that are not necessar-
      A Property-Based Taxonomy of CQMs                                 ily in X, but are part of the vector-space V . Observe that the
In the absence of formal guidelines for CQM selection2 , in             centers-of-mass are not defined for un-normed CQMs. We
particular for selecting a versatile set of CQMs, we develop            define the properties for CQMs in general, but one can apply
a property-based framework for distinguishing CQMs based                any property to a normed CQM by using the norm to define
on such a framework for clustering algorithms discussed in              the distance function. That is, set d(x, y) = kx − yk for all
(Ackerman, Ben-David, & Loker, 2010b) (also see (Bosagh-                x, y ∈ X.
Zadeh & Ben-David, 2009) and (Ackerman, Ben-David, &                    Invariance and consistency properties
Loker, 2010a)). The framework consists of identifying natu-
                                                                        Invariance properties describe changes to the underlying data
ral properties of CQMs and classifying measures based on the
                                                                        that do not affect the quality of a clustering. Consistency
properties that they satisfy. For the purposes of our study we
                                                                        properties describe similarity conditions under which clus-
use this framework to select meaningfully versatile CQMs.
                                                                        terings have similar quality. We propose two new invariance
This taxonomy may have independent interest for choosing
                                                                        properties.
CQMs in other settings. Note that these properties are de-
scriptive only, and not necessarily desirable.                          Definition 1 (Separation Invariance). A CQM m is
   Our taxonomy of CQMs follows a line of work on theo-                 separation-invariant if for all X and distance functions d and
retical foundations of clustering beginning with the famous             d 0 over X where d(x, y) = d 0 (x, y) for all x ∼C y, m(C, X, d) =
impossibility result by (Kleinberg, 2003), which showed that            m(C, X, d 0 ).
no clustering function can simultaneously satisfy three spe-                A separation invariant CQM is not affected by changes to
cific properties. (Ackerman & Ben-David, 2008) reformu-                 between-cluster distances. Conversely, homogeneity invari-
late these properties in the setting of CQMs, and show that             ant CQMs depend only on between-cluster distances, and are
these properties are consistent and satisfied by many CQMs.             invariant to changes to within-cluster distances.
We follow up on (Ackerman & Ben-David, 2008) by study-                  Definition 2 (Homogeneity Invariance). A CQM m is
ing natural properties that can be used to distinguish between          homogeneity-invariant if for all X and distance functions
CQMs.                                                                   d and d 0 over X where d(x, y) = d 0 (x, y) for all x 6∼C y,
   In Table 3, we present a taxonomy of our seven cluster-              m(C, X, d) = m(C, X, d 0 ).
ing quality measures. Each property, defined below, aims to
                                                                            Observe that separation-invariance and homogeneity-
capture some fundamental feature that is satisfied by some
                                                                        invariance can also be viewed as consistency properties. An
measures.
                                                                        additional consistency property, order consistency, is an adap-
Normed clustering quality measures                                      tation of an analogous property of clustering functions pre-
A clustering quality measure m takes a domain set X, a dis-             sented in (Jardine & Sibson, 1971). Order consistency de-
tance function d over X, and a clustering C of X, and outputs a         scribes CQMs that depend only on the order of pairwise dis-
non-negative real number. Some quality measures are defined             tances.
                                                                        Definition 3. A CQM m is order consistent if for all d and d 0
   2 Although there are no formal guidelines for CQM selection,
                                                                        over X such that for all p, q, r, s ∈ X, d(p, q) < d(r, s) if and
some interesting heuristics haven been proposed, see, for example,
(Vendramin, Campello, & Hruschka, 2009).                                only if d 0 (p, q) < d 0 (r, s), m(C, X, d) = m(C, X, d 0 ).
                                                                     1874

Domain and range properties                                            Ackerman, M., & Ben-David, S. (2009). Clusterability:
A bounded range can aid in interpreting the results of a CQM,            A theoretical study. Proceedings of AISTATS-09, JMLR:
in particular if the bounds are attainable by some clusterings.          W&CP, 5, 1–8.
Definition 4 (Bounded). A CQM m is bounded if there ex-                Ackerman, M., Ben-David, S., & Loker, D. (2010a). Charac-
ist datasets X1 over d1 and X2 over d2 , and clusterings C1              terization of Linkage-based Clustering. In Proceedings of
of X1 and C2 of X2 , so that m(C1 , X1 , d1 ) ≤ m(C, X, d) ≤             colt.
m(C2 , X2 , d2 ) for all C, X, and d.                                  Ackerman, M., Ben-David, S., & Loker, D. (2010b). Differ-
                                                                         entiating clustering paradigms: a property-based approach.
   Our next property describes the quantity of domain ele-               In Advances in neural information processing systems.
ments that effect the CQM. First, we introduce the notion of           Baker, F., & Hubert, L. (1975). Measuring the power of
an m-cover of a clustering, a subset of the domain which has             hierarchical cluster analysis. Journal of the American Sta-
the same quality as the entire set. For clustering C of X, and           tistical Association, 70(349), 31–38.
X 0 ⊆ X, let C|X 0 denote the clustering C0 of X 0 where for all       Bosagh-Zadeh, B., & Ben-David, S. (2009). A uniqueness
x, y ∈ X 0 , x ∼C0 y if and only if x ∼C y.                              theorem for clustering. In Proceedings of the 25th confer-
   An m-cover of clustering C of X is any set R ⊆ X, so that             ence on uncertainty in artificial intelligence, auai press.
m(X, k) = m(R,C|R). We define clustering quality measures              Caliński, T., & Harabasz, J. (1974). A dendrite method for
that have a constant size cover for all clusterings.                     cluster analysis. Communications in Statistics-Simulation
Definition 5 (Bounded Cover). A CQM m has bounded cover                  and Computation, 3(1), 1–27.
if there exists a constant r so that for every data set X and          Dunn, J. (1974). Well-separated clusters and optimal fuzzy
clustering C of X, there exists an m-cover of C of cardinality           partitions. Cybernetics and Systems, 4(1), 95–104.
at most r.                                                             Fleiss, J. L. (1971). Measuring nominal scale agreement
   CQMs that have a bounded cover search the domain space                among many raters. Psychological Bulletin, 76(5), 378-
for some local features, ignoring most of the information in             382.
the dataset.                                                           Jardine, N., & Sibson, R. (1971). Mathematical taxonomy.
                                                                         John Wiley and Sons, Inc., New York.
                           Conclusions                                 Kleinberg, J. (2003). An impossibility theorem for clustering.
We perform an empirical study comparing human evalua-                    In Advances in neural information processing systems 15:
tions of clustering with formal clustering quality measures.             Proceedings of the 2002 conference (p. 463).
To select a versatile set of CQMs, we develop a theoretical            Landis, J. R., & Koch, G. G. (1977, March). The measure-
property-based taxonomy of CQMs. Our study shows that                    ment of observer agreement for categorical data. Biomet-
some CQMs with seemingly natural mathematical formula-                   rics, 33(1), 159–174.
tions yield evaluations that disagree with human perception.           Lewis, J. M. (2009). Finding a better k: A psychophysical
On the other hand, we identify CQMs (CH and Silhouette)                  investigation of clustering. In N. A. Taatgen & H. van Rijn
that have significant correlation with human evaluations.                (Eds.), Proceedings of the 31st annual conference of the
   Our consistency analysis reveals that even novices are at             cognitive science society (p. 315-320).
least as consistent as a broad set of CQMs, and perhaps more           Milligan, G. (1981). A Monte-Carlo study of 30 internal
consistent. We also find significant correlations between the            criterion measures for cluster-analysis. Psychometrika, 46,
evaluations of expert and novice subjects. This lends support            187–195.
to the common practice of seeking human visual evaluations             Ng, A. Y., Jordan, M., & Weiss, Y. (2002). On spectral
of partition quality. If one needs to evaluate a very large num-         clustering: analysis and an algorithm. In T. G. Dietterich,
ber of partitions it may be reasonable to use human compu-               S. Becker, & Z. Ghahramani (Eds.), Advances in neural
tation via a service such as Mechanical Turk to rank parti-              information processing systems 14 (pp. 849–856). Cam-
tions efficiently (or at least throw out the really bad ones). Fi-       bridge, MA: MIT Press.
nally, experts appear to use more sophisticated strategies than        Rousseeuw, P. (1987). Silhouettes: a graphical aid to the
novices, indicating that training can improve human cluster-             interpretation and validation of cluster analysis. Journal of
ing evaluation performance.                                              computational and applied mathematics, 20, 53–65.
                                                                       Santos, J., & Sá, J. M. de. (2005). Human clustering on
                       Acknowledgments                                   bi-dimensional data: An assessment (Tech. Rep. No. 1).
This work is funded by NSF Grant #SES-0963071, Divvy:                    INEB Instituto de Engenharia Biomédica, Porto, Portu-
Robust and Interactive Cluster Analysis (PI Virginia de Sa).             gal. Available from http://www.di.ubi.pt/˜lfbaa/
Thanks to Cindy Zhang for valuable code contributions.                   entnetsPubs/JMS TechReport2005 1.pdf
                                                                       Strehl, A. (2002). Relationship-based clustering and cluster
                            References                                   ensembles for high-dimensional data mining.
Ackerman, M., & Ben-David, S. (2008). Measures of clus-                Vendramin, L., Campello, R., & Hruschka, E. (2009). On the
   tering quality: A working set of axioms for clustering. In            comparison of relative clustering validity criteria. Sparks.
   Advances in neural information processing systems.
                                                                   1875

