UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Bayesian Model of Rule Induction in Raven's Progressive Matrices
Permalink
https://escholarship.org/uc/item/0227t8z1
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Little, Daniel R.
Lewandowsky, Stephan
Griffiths, Thomas L.
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

            A Bayesian Model of Rule Induction in Raven’s Progressive Matrices
                                         Daniel R. Little (daniel.little@unimelb.edu.au)
                                    School of Psychological Sciences, The University of Melbourne
                                                        Parkville VIC 3010 Australia
                                Stephan Lewandowsky (stephan.lewandowsky@uwa.edu.au)
                                      School of Psychology, The University of Western Australia
                                                              Crawley WA 6009
                                       Thomas L. Griffiths (tom griffiths@berkeley.edu)
                                     Department of Psychology, University of California, Berkeley
                                                       Berkeley CA 94720-1650 USA
                              Abstract                                  been successfully applied to similar tasks, such as numerical
                                                                        sequence prediction (i.e., which number follows in the se-
   Raven’s Progressive Matrices (Raven, Raven, & Court, 1998)
   is one of the most prevalent assays of fluid intelligence; how-      quence: 1, 2, 3, 5, 7, 11?; Austerweil & Griffiths, 2011) and
   ever, most theoretical accounts of Raven’s focus on producing        rule-based categorization (Goodman, Tenenbaum, Feldman,
   models which can generate the correct answer but do not fit hu-      & Griffiths, 2008). Examining Raven’s within the context
   man performance data. We provide a computational-level the-
   ory which interprets rule induction in Raven’s as Bayesian in-       of a Bayesian model allows exploration of questions about
   ference. The model computes the posterior probability of each        what people’s priors (or in non-Bayesian terms, inductive bi-
   rule in the set of possible rule hypotheses based on whether         ases) might be like for rules of the variety used in the Raven’s
   those rules could have generated the features of the objects in
   the matrix and the prior probability of each rule. Based on fits     test. Finally, the Bayesian formalism provides an extensible
   to both correct and incorrect response options across both the       framework for using standard extensions to Bayesian models
   Standard and Advanced Progressive Matrices, we propose sev-          to capture other, more process-based interpretations of fac-
   eral novel mechanisms that may drive responding to Raven’s
   items.                                                               tors known to be relevant to performance on Raven’s, such as
   Keywords: Rule induction, Bayesian inference, Raven’s Pro-           memory and learning.
   gressive Matrices                                                       Here we present a Bayesian model of Raven’s which in-
                                                                        terprets rule induction as Bayesian inference in which a set
                          Introduction                                  of rules with some prior probability are evaluated based on
Raven’s Progressive Matrices (Raven et al., 1998; Raven’s               their ability to have plausibly generated the features of the
from here on) is one of the most widely used assays of fluid            items shown in the matrix. Rules are then sampled based on
intelligence, and much attention has focused on the under-              their posterior probability and Bayesian model averaging is
lying elemental cognitive processes. Raven’s has arguably               used to predict which answers are most likely given the pos-
gathered more attention in the cognitive literature than any            terior distribution. Unlike extant models, which examine how
other psychometric measure of fluid intelligence, largely be-           successful the model is at predicting correct responses (e.g.,
cause it is an induction task par excellence that can be mod-           Carpenter et al., 1990; Lovett et al., 2010; McGreggor et al.,
eled computationally (see e.g., Carpenter, Just, & Shell, 1990;         2010), our model also makes predictions about the proportion
Verguts, De Boeck, & Maris, 2000). For example, Carpenter               of responses involving the various incorrect options.
et al. (1990) presented a production-system model of Raven’s                             Bayesian Model of Raven’s
to support a two-factor theory of Raven’s with working mem-
                                                                        Solving a Raven’s problem can be conceptualized as a three-
ory capacity (WMC) as the first factor and a second factor
                                                                        stage process involving feature extraction, rule-inference and
related to the ability to abstract relations. This latter ability
                                                                        prediction.1 As illustrated in Figure 1, Raven’s items have the
has been associated with several attributes including rule gen-
                                                                        following composition:
eration speed (Verguts & De Boeck, 2002), inference speed
(Rasmussen & Eliasmith, 2011), and analogical comparison
(Lovett, Forbus, & Usher, 2010; McGreggor, Kunda, & Goel,                                      O11     O12    O13
2010).                                                                                         O21     O22    O23                          (1)
   These extant models of Raven’s have focused on cognitive                                    O31     O32     ?
processes and mechanisms that underlie the inference of rules           where Oi j is the object in the ith row and jth column. As-
from the objects in the matrix. Further insight can be gained           suming the features of each object are extracted successfully,
by exploring a computational-level analysis (Marr, 1982). As
                                                                            1 In the present model, we follow Carpenter et al. (1990) by hand-
performance in Raven’s relies primarily on rule induction, the
                                                                        coding the features of the items. Several methods for extracting the
task is conducive to instantiation within a Bayesian frame-             features of Raven’s items have been proposed (Lovett et al., 2010;
work. For instance, Bayesian models of rule induction have              McGreggor et al., 2010; Rasmussen & Eliasmith, 2011).
                                                                    1918

  A)                                 B)                                equal to ε whenever the rule could not have generated the fea-
                                                                       tures of O13 and (1 − ε) whenever a rule could have generated
                                                                       the features of O13 , where ε is a small number (ε = .01 in our
                                                                       simulations below). We make the further assumption that a
                                                                       rule may work within neither, only one, or both of the rows
                                                                       (or columns), in which case the probability of generating a
                                                                       feature given a rule across both rows is the product of the
     1       2       3        4            1       2   3     4
                                                                       probabilities from each row separately:
     5       6       7        8            5       6   7     8
                                                                                     (1 − ε)2 if the rule works for both rows
                                                                                    
                                                                       p ( f |g) =      ε (1 − ε) if the rule works for only one row          (3)
                                                                                            ε2     if the rule works for neither row
                                                                                    
Figure 1: Two examples of matrices like those in the Raven’s
test. A: Example of an item containing a pairwise incremental          Priors over rules
rule, a constant rule and a permutation rule. B: Example of            Carpenter et al.’s (1990) analysis of Raven’s identified a tax-
an item containing a constant rule and an XOR rule.                    onomy of rules used to create the Raven’s problems. These
                                                                       rule types can be classified as involving transformations (e.g.,
                                                                       a quantitative pairwise increment or decrement of a feature
such that
        n each object o can be decomposed into N features,
           ij      ij                                                  from one object in the matrix to the next, or a permuta-
Oi j = f1 , ..., fN , then the goal is to infer the rule that          tion of objects within a row or column), rules requiring logi-
generated the features of the last object in each row and col-         cal operations (e.g., AND conjunctions, OR disjunctions and
umn from the features of the first two objects in each row             exclusive-or, XOR, relations between features; Matzen et al.,
and column. By design, the rules that are applied to gener-            2010) and a constant rule in which features are maintained
ate O13 from O11 and O12 are the same as the rules used to             unchanged across items.2 To provide a concrete example,
generate O23 from O21 and O22 . We refer to the set of rules           Figure 1 presents two sample Raven’s-like problems. The
that apply to each row G, where G = {g1 , ..., gM } is a collec-       matrix in panel A contains a pairwise incremental rule (i.e.,
tion of M rules for each feature. The third object in a row            the dots increase across items from left to right) and a permu-
is assumed to have been generated by applying these rules              tation rule (i.e,. objects with 1, 2 and 3 triangles are permuted
to the features of the first two objects: O13 = G (O11 , O12 )         across rows and columns). The matrix in panel B contains a
and O23 = G (O21 , O22 ). We assume a separate set of rules,           constant rule (i.e., the center dot appears in all items) and an
H, may apply to each of the features of the objects within             XOR rule (i.e., features which appear in the first two objects
a column, O31 = H (O11 , O21 ) and O32 = H (O12 , O22 ). The           do not appear in the third object and features which appear
column rules and the row rules may be different; however,              only in one of the first two objects also appear in the third
because O33 can be predicted using either the rows or the              object). Participants must infer these rules from the objects
columns, we restrict the following analysis to the row rules,          in the matrix and select the missing lower right object in each
G, but it also applies comparably to the column rules, H.              matrix from the set of possible response options below each
    We assume that infering a rule which generated a feature of        matrix.
the third object in a row or column can be conceptualized as              In total we used eight different rules derived from the tax-
finding the posterior probability of each possible rule applied        onomy presented in Carpenter et al. (1990; see also, Matzen
to that feature:                                                       et al., 2010) and further analyses: 1) constant, 2) increment
                                   p ( f |g) p (g)                     or 3) decrement, 4) permutation, 5) logical AND (i.e., main-
                    p (g| f ) =  M
                                                                (2)    tain common features and delete unique features between ob-
                                 ∑ p ( f |gi ) p (gi )                 jects), 6) logical OR (i.e., maintain unique features between
                                i=1
                                                                       objects), 7) logical XOR (see Figure 1, panel B), and 8) a
where p ( f |g) is the likelihood of generating feature, f , given     Distribution of 2 rule. 3
the rule, g.                                                               2 Carpenter et al. (1990) refers to permutation rules as Distribu-
                                                                       tion of 3 rules because the feature values appear once in the three
Likelihood                                                             objects within a row or column and to XOR rules as Distribution
In Raven’s, all of the rules apply to individual features (which       of 2 rules because the feature only appears in two of the three ob-
                                                                       jects. Carpenter also refers to logical OR rules as addition and logi-
may be discrete or continuous valued; e.g., the three dots in          cal AND rules as subtraction.
Figure 1, panel A); hence, within a row, the likelihood will               3 Six items that were generated using idiosyncratic rules were re-
have a value of 0 or 1 depending on whether or not the rule            moved from the 72 Raven’s Standard Progressive Matrices (RSPM)
successfully produces the features of the third object from the        and Advanced Progressive Matrices (RAPM) items that we tested.
                                                                       We included the Distribution of 2 rule in our set because there are
features of the first two objects in that row. To allow for mis-       two items in the RAPM set which use a Distribution of 2 rule that is
calculations in the evaluation of a rule, we set the likelihood        inconsistent with an XOR rule.
                                                                   1919

   We tested three prior distributions on rules. First, we as-           Once the posterior probability of each rule is computed for
sumed that each rule had an equal prior probability (i.e., a          each feature using Equation 2, we compute the missing ob-
uniform prior probability). Vodegel Matzen, van der Molen,            jects as follows: For a given row rule, Gm , we predict the
and Dudink (1994) conducted a study using single rule ma-             features of O33 by applying the rule to to the features O31 and
trices and found there was a clear order of rule difficulty, in       O32 . That is, Ô33 = Gm (O31 , O32 ). Response proportions are
which the easiest was constant in a row, followed by quanti-          determined by computing the relative similarity of each re-
tative pairwise progression, permutation and logical rule op-         sponse option, Rk to each object  in the predictive posterior by
erations, which were the most difficult. To capture this order        s{Ô33 ,Rk } = exp −c × d{Ô33 ,Rk } , where d{Ô33 ,Rk } is the Eu-
of difficulty we developed a second prior which assumed that
the probability of a rule was proportional to the ease with           clidean distance between Rk and Ô33 (i.e., the square root of
which that rule could be generated (e.g., the complexity of           the summed squared differences between the features of Ô33
the rules, which is related to the mental effort necessary to         and Rk ) and c is a parameter which determines the steepness
infer and use a rule). For this prior (hereafter referred to as       of the similarity gradient.
the Carpenter prior), we used the frequency with which each              Similarities are weighted by their probability in the pre-
rule occured in Carpenter et al.’s (1990) analysis as a proxy         dictive posterior and normalized across response options to
for the ease with which each rule could be generated and set          determine the probability that the model chooses each re-
the prior probabilities to be proportional to the presentation        sponse option (see e.g., Rasmussen & Eliasmith, 2011). In
frequency. Finally, we assumed that the prior may be related          our baseline model, we set c equal to 10 which results in
to the accuracy with which items containing those rules could         strong responding to the response options which are repre-
be solved. Again, the probabilities are also related to the ease      sented most strongly in the posterior because the similarity
or complexity with which a rule can be generated, but for this        gradient is quite steep when c >> 1; however, preliminary
prior, we use the relationship between each rule and accuracy         examination of the model fits to some items suggested that
on items generated using that rule as a proxy for complexity.         human responses were influenced by the similarity of some
To compute this accuracy-based prior, we fit a logistic regres-       distractors to the correct response. For these items, we set c
sion model using the rule profile of each item as the predictor       equal to 1, which implies a shallow similarity gradient and
variables (i.e., for each item, i, and for each rule, j, we set an    greater confusability between similar response options; we
indicator equal to 1 if item i was generated using rule j and to      exhaustively tested each item to determine whether lowering
0, otherwise) and the proportion correct for each item as the         c improved the fit for that item. We refer to this version of the
dependent variable. We then transformed the resulting expo-           model as the Baseline + similarity model. In this model, 20
nentiated regression weights such that they ranged between            of the 66 items had a lower c value than the other items (i.e.,
0 and 1 and summed to 1 across all of the rules. The actual           for these items, c = 1).
probabilities for each of these priors are listed in Table 1.            Initial inspection of the model predictions additionally re-
                                                                      vealed a propensity for subjects not to choose response op-
            Table 1: Prior probabilities for each rule.               tions which also appear as items in the matrix. To handle
                                                                      this, we introduced a heuristic into the model such that all ob-
    Rule               Uniform     Carpenter   Accuracy-based
    Constant              .125        .194           .150             jects that appeared in the matrix were removed from the pos-
    Increment             .125        .223           .185             terior predictive distribution and from the response set before
    Decrement             .125        .223           .141             computing the response proportions. Through exhaustively
    Permuation            .125        .058           .116
    Logical AND           .125        .039           .167             testing each item, we determined that this heuristic improved
    Logical OR            .125        .058           .119             the model’s predictions for 52 of the 66 items. We refer to
    Logical XOR           .125        .165           .119             this version of the model as the Baseline + heuristic model.
    Distribution of 2     .125        .039           .081
                                                                      We additionally tested a full model which incorporated both
                                                                      similarity-based responding and the response heuristic.
Predicting the response from the posterior
The perceptual complexity of the objects affects how easily           Table 2: Chi-square values for the fit to choice probabili-
rule inferences can be generated (Primi, 2001). For example,          ties for Raven’s Standard Progressive Matrices and Advanced
Meo, Roberts, and Marucci (2007) showed that performance              Progressive Matrices. The model that provides the best fit to
was significantly worse when features within items were dif-          the data for each prior is shown in bold.
ficult to identify. We incorporate this finding into the current
model by assuming that the response is based on the similar-            Model                   Uniform     Carpenter    Accuracy-based
ity between response options and objects predicted from the             Baseline Model           43603        37363            40699
                                                                        Baseline + Heuristic     32642        32816            32871
rules in the posterior, and for items which have features which         Baseline + Similarity    23385        13884            19948
are difficult to extract, the similarity does not need to be very       Full Model               11761        12258             9010
high in order for the response options to match the object in
the posterior.
                                                                  1920

                                        Baseline Model                                                             0
                           RSPM                                           RAPM
                1                                               1
                                                                                                                  −1
               0.8                                             0.8
p(Correct)                                        p(Correct)
               0.6                                             0.6                                                −2
               0.4                                             0.4
                                                                                                                  −3
                                                                                                   log Observed
               0.2                                             0.2
                                                                                                                  −4
                0                                               0
                 0    10     20    30                            0   10          20   30
                           Item                                           Item                                    −5
                                          Full Model                                                              −6
                           RSPM                                           RAPM
                 1                                              1
                                                                                                                  −7
               0.8                                             0.8
  p(Correct)                                      p(Correct)
               0.6                                             0.6
                                                                                                                  −8
               0.4                                             0.4                                                −9
               0.2                                             0.2                                                  −9   −8   −7   −6   −5   −4   −3   −2   −1   0
                                                                                                                                    log Predicted
                 0                                              0
                  0   10      20   30                            0   10          20   30
                           Item                                           Item
                                                                                           Figure 3: Predicted (full model) and observed response pro-
Figure 2: Baseline model and full model fits to proportion                                 portions for all response options from RSPM and RAPM. The
correct data from RSPM and RAPM (Little, Lewandowsky &                                     dotted line surrounds options for which the model predicts
Craig, 2012). Model predictions are shown by the black dots,                               near zero response proportions.
the observed data are shown by the solid black line.
                                                                                           full model (with an accuracy-based prior) across all of the
        Comparing the model to human performance                                           response options against the observed response proportions.4
                                                                                           For a large proportion of response options, the model predicts
Descriptive statistics and accuracy data for the Raven’s data                              the observed proportions correctly; however, the model also
were previously reported in Little, Lewandowsky, and Craig                                 erroneously predicts a large number of response options near
(2012); here, we are primarily concerned with how often each                               0 (predicted log proportion less than -6). These items are
response option was chosen for each item. (Note that we have                               also the items for which the model overpredicts the correct
removed omissions from this data and look at the distribution                              response in Figure 2. Examination of the response profiles
of responses across the response options only for participants                             for individual items reveals that for many items this overpre-
who actually gave a response). We fit the four versions of the                             diction is not detrimental to the qualitative pattern of results
model using the three different prior distributions to the re-                             (see Figure 4). One possible explanation for the discrepant re-
sponse proportions from the RSPM and RAPM. Chi-square                                      sults is that people are guessing the answer with some small
fit statistics are shown in Table 2. Based on these fit val-                               probablity which would reduce the accuracy for some of the
ues, it is evident that adding similarity-based responding im-                             items and increase the proportion of false alarms to some of
proves the fit over adding the response heuristic to the base-                             the distractors.
line model. Adding both modifications results in a substan-
tial improvement when a uniform or an accuracy-based prior                                                                         Discussion
is used and a marginal improvement when the prior based
on Carpenter’s analysis is used. The overall best fit is found                             This paper has defined a Bayesian model of Raven’s Progres-
when the full model is used with an accuracy-based prior;                                  sive Matrices that provides an account of Raven’s based on
consequently, we now focus on this model’s predictions.                                    the idea that people infer rules by computing the posterior
                                                                                           probability of those rules and using the rules to generate plau-
   Figure 2 shows the accuracy predictions for the baseline
                                                                                           sible responses. We considered three priors and two ways
and full models for the RSPM items (reordered according to
                                                                                           in which the model could be modified to accomodate human
accuracy rate observed in Little et al., 2012) and the RAPM
                                                                                           performance. Ultimately, a model incorporating an accuracy-
items. The baseline model clearly predicts the correct an-
                                                                                           based prior and both modifications provided the best fit to the
swer for most of the Raven’s items; however, this model over-
                                                                                           data.
predicts the propensity with which people choose the correct
                                                                                              The success of the accuracy-based prior suggests that
item for both RSPM and RAPM. By contrast, for the RSPM
                                                                                           rules vary in how they contribute to accurate performance in
items, the full model accurately predicts the decrease in ac-
                                                                                           Raven’s. This relationship may reflect sensitivity to differing
curacy across the items. For the RAPM items, the full model
                                                                                           levels of complexity between the rules, and one way to handle
predicts the decrease in accuracy for the hardest Raven’s
                                                                                           this prior in a more principled way is to instantiate the rules
items, but still overpredicts the proportion correct for items
in the middle difficulty range.                                                               4 Proportions equal to 0 or 1 were corrected by setting these pro-
   Figure 3 shows the (log transformed) predictions of the                                 portions equal to 1/ (4N) or [N − (1/ (4N))] /N, respectively.
                                                                                       1921

                   1                       1
                       SPM C7                   SPM C12
                                                                    1
                                                                         APM II13
                                                                                                ture different aspects of human cognition. One important as-
                                                                                     Data
                                                                                     Model
                                                                                                pect of performance on Raven’s is that differences in WMC
                 0.5                      0.5                      0.5                          correlate highly with accuracy. Furthermore, the correlation
                                                                                                with WMC increases as the items become more difficult (if
                   0                       0                        0                           the overall correlation between Raven’s and WMC is large
                  1                        1                        1                           enough; Little et al., 2012). The model does not currently in-
                       SPM D11
                                                SPM D12                  APM II17
                                                                                                corporate WMC; however, one possibility for extending the
                 0.5                      0.5                      0.5                          model is to represent the hypothesis space as a sampling dis-
                                                                                                tribution from the prior using importance sampling.
   p(Response)
                  0                        0                        0                              In an importance sampling scheme (Shi, Griffiths, Feld-
                  1                        1                         1                          man, & Sanborn, 2010) samples from the prior are weighted
                       SPM E7                   SPM E8                   APM II25
                                                                                                by their likelihood to approximate the posterior distribution;
                 0.5                      0.5                      0.5                          more samples lead to a better approximation of the poste-
                                                                                                rior. Differences in WMC could be modelled by varying the
                  0                        0                         0                          number of samples with high WMC participants having more
                  1                        1                         1                          samples from the prior. With more samples, the model is
                       SPM E12                    APM II8                APM II36
                                                                                                more likely to generate the correct answer. This idea is rem-
                 0.5                      0.5                      0.5                          iniscent of the difference between Carpenter et al.’s (1990)
                                                                                                FAIRRAVEN and BETTERRAVEN models. The BETTER-
                  0                        0                         0                          RAVEN model was given access to more rules than the FAIR-
                        1 2 3 4 5 6 7 8          1 2 3 4 5 6 7 8           1 2 3 4 5 6 7 8
                                                Response Option                                 RAVEN model and consquently was able to mimic the per-
                                                                                                formance of participants with highly accurate Raven’s per-
                                                                                                formance and, by implication, higher WMC.
Figure 4: Model prediction profiles for a selection of items.
Items SPM C12 (i.e. Standard Progressive Matrices item 12                                          An alternative account of the relationship between WMC
from Set C) and APM II13 illustrate cases in which the model                                    and Raven’s is that higher WMC permits faster learning of
predicts response probabilities of 0 for responses that people                                  what rules are likely to be necessary (Verguts & De Boeck,
occassionally select. Items SPM E12 and APM II25 illus-                                         2002). In support of this account, Carlstedt, Gustafsson,
trate two examples in which the model makes incorrect pre-                                      and Ullstadius (2000) found that WMC was correlated more
dictions. For item APM II36, the item with the highest er-                                      strongly with homogenous intelligence test items (which all
ror rate from either test, the model predicts that the correct                                  required the same rule to solve) than with heterogenous in-
response (option 2) should be selected most frequently, but                                     telligence test items. Presumably, learning the relevant rule
humans prefer options 1 and 8.                                                                  is easier for homogenous test items than for heterogenous
                                                                                                items. In other related tasks, such as rule-based catego-
                                                                                                rization, WMC is known to be correlated with learning rate
using a common language with a formal definition of com-                                        (Lewandowsky, 2011; Sewell & Lewandowsky, in press). By
plexity, such as first-order logic (cf. Goodman et al., 2008).                                  this account, the rules at the end of the test are more diagnos-
   The better fit produced by the similarity-based prediction                                   tic because they have had more time to be learned, thereby
modification suggests that people vary in how responses are                                     leading to greater divergence between low and high ability in-
generated to different Raven’s items. For instance, for some                                    dividuals. Learning in the Bayesian model of Raven’s could
items, responses are generated by comparing each response                                       be instantiated by using a special case of importance sampling
option to the possibilities in the predictive posterior; for these                              known as particle filter sampling (Doucet, Freitas, & Gordon,
items, small differences in features of the response options                                    2001; Sanborn, Griffiths, & Navarro, 2010). In a particle fil-
do not result in a large difference in response prediction. For                                 ter model, a set of particles representing possible rules are
other items, the response must match the objects in the predic-                                 drawn in proportion to their prior probabilities. As one pro-
tive posterior exactly. An aim for future research would be to                                  gresses through the Raven’s items, probabilities are updated
identify what makes a feature hard to identify. This would al-                                  in proportion to the success of each rule. Particles represent-
low appropriate a priori specification of the similarity-based                                  ing rules are maintained if they work, but are replaced with
prediction rather than the post-hoc approach adopted here. Fi-                                  new samples from the prior if they do not. Again, higher
nally, the heuristic mechanism suggests that people limit their                                 WMC could be modelled using a larger number of samples.
responding to only the plausible response alternatives reject-                                  A particle filter model of Raven’s would consider both Car-
ing alternatives which are implausible because they duplicate                                   penter et al.’s (1990) and Verguts and De Boeck’s (2002) ac-
items which appear as objects in the matrix.                                                    counts to be correct. That is, higher WMC allows for access
   One of the advantages of formulating a Bayesian model                                        to more rules by virtue of allowing more samples from the
of this task is that we can make use of recent work that                                        prior; higher WMC also allows for faster learning by allowing
has explored how Bayesian models can be extended to cap-                                        a larger number of particles to be updated from trial to trial.
                                                                                             1922

Consequently, a particle filter model of WMC and Raven’s                 memory capacity and fluid abilities: The more difficult the
provides a synthesis of these two approaches.                            item, the more more is better. Manuscript submitted for
   A limitation of Carpenter et al. (1990) and of our own work           publication.
is that the inputs to the model are hand-coded (Lovett et al.,         Lovett, A., Forbus, K., & Usher, J. (2010). A structure-
2010). Hand-coding ignores potentially important spatial rep-            mapping mode of Raven’s Progressive Matrices.             In
resentations between objects. Furthermore, Carpenter et al.              S. Ohlsson & R. Catrambone (Eds.), Proceedings of the
(1990) did not model the process of rule discovery, but in-              32nd Annual Conference of the Cognitive Science Society.
stead fixed the set of rules that were available to the model.           Austin, TX: Cognitive Science Society.
The second criticism is less problematic because the rule set          Marr, D. (1982). Vision. New York: W. H. Freeman and
is comprehensive, covering the set of rules necessary to han-            Company.
dle most of the items; rule discovery is couched in terms of           Matzen, L. E., Benz, Z. O., Dixon, K. R., Posey, J., Kroger,
updating the prior probability of each of the rules based on             J. K., & Speed, A. E. (2010). Recreating Raven’s: Soft-
how well those rules work to explain the observed features.5             ware for systematically generating large numbers of Raven-
Our Bayesian model is susceptible to the first criticism; how-           like matrix problems with normed properties. Behavior Re-
ever, in the present case, we argue that the model provides              search Methods, 42, 525-541.
a good first step toward understanding how the features are            McGreggor, K., Kunda, M., & Goel, A. (2010). A fractal
used once they are extracted. It is possible that the feature ex-        analogy approach to the Raven’s test of intelligence. In
traction process might be modeled by introducing a prior over            AAAI workshops at the 24th AAAI conference on Artificial
features (such as the Indian Buffet process prior, Austerweil            Intelligence (p. 69-75). Atlanta, GA: Association for the
& Griffiths, 2010; Griffiths & Ghahramani, 2011). We leave               Advancement of Artificial Intelligence.
this as a prospect for future development.                             Meo, M., Roberts, M. J., & Marucci, F. S. (2007). Element
                                                                         salience as a predictor of item difficulty for Raven’s Pro-
Acknowledgments This work was supported by an ARC
                                                                         gressive Matrices. Intelligence, 35, 359-368.
Discovery Project Grant DP120103888 and an Australian
                                                                       Primi, R. (2001). Complexity of geometric inductive reason-
Professorial Fellowship to the second author.
                                                                         ing tasks: Contribution to the understanding of fluid intel-
                            References                                   ligence. Intelligence, 30, 41-70.
                                                                       Rasmussen, D., & Eliasmith, C. (2011). A neural model of
Austerweil, J. L., & Griffiths, T. L. (2010). Learning invari-           rule generation in inductive reasoning. Topics in Cognitive
   ant features using the transformed Indian Buffet process.             Science, 3, 140-153.
   Advances in Neural Information Processing Systems, 23.              Raven, J., Raven, J. C., & Court, J. H. (1998). Manual for
Austerweil, J. L., & Griffiths, T. L. (2011). Seeking confir-            Raven’s Progressive Matrices and vocabulary scales. Sec-
   mation is rational for deterministic hypotheses. Cognitive            tion 4: The Advanced Progressive Matrices. Oxford, UK:
   Science, 35, 499-526.                                                 Oxford University Press.
Carlstedt, B., Gustafsson, J.-E., & Ullstadius, E. (2000). Item        Sanborn, A. N., Griffiths, T. L., & Navarro, D. J. (2010).
   squencing effects on the measurement of fluid intelligence.           Rational approximations to rational model: Alternative al-
   Intelligence, 28, 145-160.                                            gorithms for category learning. Psychological Review, 117,
Carpenter, P. A., Just, M. A., & Shell, P. (1990). What one              1144-1167.
   intelligence test measures: A theoretical account of the pro-       Sewell, D. K., & Lewandowsky, S. (in press). Attention and
   cessing in the Raven Progressive Matrices test. Psycholog-            working memory capacity: Insights from blocking, high-
   ical Review, 97, 404-431.                                             lighting and knowledge restructuring. Journal of Experi-
Doucet, A., Freitas, N. de, & Gordon, N. (2001). Sequential              mental Psychology: General.
   monte carlo methods in practice. New York, NY: Springer.            Shi, L., Griffiths, T. L., Feldman, N. H., & Sanborn, A. N.
Goodman, N. D., Tenenbaum, J. B., Feldman, J., & Griffiths,              (2010). Exemplar models as a mechanism for performing
   T. L. (2008). A rational analysis of rule-based concept               bayesian inference. Psychonomic Bulletin & Review, 17,
   learning. Cognitive Science, 32, 108-154.                             443-464.
Griffiths, T. L., & Ghahramani, Z. (2011). The Indian Buffet           Verguts, T., & De Boeck, P. (2002). The induction of solution
   process: An introduction and review. Journal of Machine               rules in Raven’s Progressive Matrices. Journal of Cognitive
   Learning Research, 12, 1185-1224.                                     Psychology, 14, 521-547.
Lewandowsky, S. (2011). Working memory capacity and cat-               Verguts, T., De Boeck, P., & Maris, E. (2000). Generation
   egorization: Individual differences and modeling. Journal             speed in Raven’s Progressive Matrices test. Intelligence,
   of Experimental Psychology: Learning, Memory and Cog-                 27, 329-345.
   nition, 37, 720-738.                                                Vodegel Matzen, L. B. L., van der Molen, M. W., & Dudink,
Little, D. R., Lewandowsky, S., & Craig, S. (2012). Working              A. C. M. (1994). Error analysis of Raven test performance.
    5 We also tested a model with an expanded set of logical rules       Personality and Individual Differences, 16, 433-445.
(e.g., NAND, NOR, etc) but this made no difference to the qualita-
tive pattern of model fits.
                                                                   1923

