UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Identifying Kinematic Cues for Action Style Recognition

Permalink
https://escholarship.org/uc/item/7vw5g0t4

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)

Author
Hidaka, Shohei

Publication Date
2012-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Identifying Kinematic Cues for Action Style Recognition
Shohei Hidaka (shhidaka@jaist.ac.jp)
Japan Advanced Institute of Science and Technology,
1-1 Asahidai, Nomi, Ishikawa 923-1292, Japan

Abstract
Recognition of emotional states from other’s actions is one of
key capability for smooth social interaction. The present study
provides a computational-theory-level analysis on which
feature may take a crucial role in recognition of emotional
attributes in human actions represented as point-light display.
Lead by the previous theoretical works and empirical findings,
the velocity and acceleration profile was investigated as a
major feature of emotional attributes classification. The
results showed that emotional attributes in actions as well as
action types could be identified by covariance of velocity
profiles among multiple body parts. Since, despite different
velocity profiles in different actions, these features for
emotional attributes were found commonly in multiple
different actions, it suggests that the action styles may be
mediated by an information channel parallel to action types
per se.
Keywords: Action style recognition; biological motion;
emotion; social cognition.

Introduction
Our bodily motion is coherent, smooth and effortless.
From bodily motion, we perceive other’s state such as mood,
emotional expression, and intention (Blake & Shiffrar,
2007). Perception of other’s state takes a crucial role in
social context. Although most of us can easily “read” what
others intend to do through their actions, there is a
significant gap from the physical motion – a set of
trajectories of multiple body parts with a large degree of
freedom (Bernstein, 1967). Recognition of motion is vitally
important to any animal kinds. Detection of another animal,
possibly a pray, a predator, or a conspecific, and the
following detailed identification what it is and how it may
behave is essential to take an emergent actions to it
(Johnson, Bolhuis, & Horn, 1985). Humans are social
animals. Not surprisingly, our visual system is highly
specialized to recognize others’ state. The present study
aims to provide a computational-level description on how
people recognize emotional status in others’ actions.
Perception of biological motion
How do we recognize implicit patterns in different styles of
actions? The past experimental literature has explored
capacity of motion perception using point-light displays
(Johansson, 1973) in which the point-lights attached in
major joints are only visible in the dark background (Figure
1a). Thus the available information is point-wise kinematic
motion in multiple body parts. Despite of the limited
information, people can recognize identity (Troje, Westhoff,
& Lavrov, 2005), gender (Kozlowski & Cutting, 1977;
Troje, 2002), emotions (Pollick et al., 2001; Atkinson; 2009;
Hobson & Lee, 1999), dynamics such as the weight of a

lifted object (Bingham, 1987) of actions from point-light
displays.
Not only demonstrating human capacity, the studies using
point-light display have suggested features extracted in
action perception. Accumulating empirical studies on action
perception have suggested that velocity and its higher order
derivatives in a single or multiple body parts as one of
major correlates to emotional attributes in actions: duration
of action (Pollick et al., 2001), velocity (DeMeijer, 1989),
acceleration (force or the second order time derivatives)
(Chang & Troje, 2008; 2009) and jerk or the third order
time derivatives (Cook, Saygin, Swain, & Blakemore, 2009),
pairwise counter-phase oscillation (Chang & Troje, 2008;
2009). In particular, we highlight the contribution of the
higher order derivatives of velocity and importance of its
covariational structure. Of relevance, Chang & Troje (2009)
found that, not one of either but a pair of feed motion was a
major cue for discrimination of walking direction.
Past computational models on action recognition
Consistent to these empirical findings, most of the
theoretical approach works on some kind of statistical
regularities among motion profiles. According to a recent
review (Troje, 2008), perception of biological motion has
the multi-level processing on local and global motion
properties. The feature processing consists of four layers
from early (low-level) to late (high-level) processing: life
detection, structure-from-motion, action recognition, and
style recognition. The system detect autonomous agent, and
construct body structure from its detailed analysis, then is
followed by more detailed action analysis.
A couple of computational models are available for
structure-from-motion and action recognition (Giese &
Poggio, 2003; Lange & Lappe; 2006), and a few for postaction-recognition-level style perception (Troje, 2002;
Pollick, Lestou, Ryu, Cho, 2002; Davis & Gao, 2004) in
vision science. In the model of structure-from-motion and
action recognition, the model identifies body structure and
subsequently actions from the pixel-based visualization of
point-light displays. In Giese & Poggio (2003), the model
was built based on neuro-physiological findings on visual
cortex, and was applied to recognition of action types and
action direction in normal, masked, or scrambled point-light
displays.
While, the post-action-recognition-level models for style
perception typically assume the either/both 2D or 3D pointlight on the major joints and also which action is to be
executed is readily available prior to the recognition of
action style (Troje, 2002; Davis & Gao, 2004; Pollick et al.,
2001). For example, Troje (2002) have proposed a
computational model of gender identification in gait

1679

presented as point-light display. The model was built upon
the three stages: First a set of postures is encoded based on
Fourier decomposition, the low-dimensional projection of
extracted features is obtained by principal component
analysis (PCA), and then it is fed to classifier (as a similar
model, see also Davis & Gao (2004)).
Simple, transparent, yet general model
Although the previous theoretical works have offered
successful pattern recognizer for biological motion, there
are three shortcomings. First, most of the studies have been
closed in one special type (or its slight variant) of action
which often has a unique constraint such as periodicity (e.g.,
walking, running; e.g., Troje, 2002). Second, related to the
first point, a limited number of body parts specialized for
each action tends to be (e.g., arm movement for tennis
swing (Pollick et al., 2002) ). Although not all the model is
specialized, in turn, such a generalized model typically loses
transparency of mechanism as a cost of generality (For
example, multi-layer physiologically-plausible model, Giese
& Poggio, 2003). One of drawback of complex models
(using nonlinear filters or feature decomposition technique
such as Fourier decomposition and PCA) is that the
estimated parameters do not necessarily offer interpretation
on which natural features are informative such as body parts
and time course (Pollick & Paterson, 2008). Moreover, such
model often outperformed human recognition (Troje, 2002;
Davis. & Geo, 2004; Pollick & Paterson, 2008) rather than
explaining use of features in human recognition.
The theoretical assumptions in the model
In the present study, we employed the simplest possible
framework – a variant of linear regression – in order to
characterize the motion cues in whole body interaction for
multiple types of actions and emotional attributes. The
model has the three major assumptions as follows. (1) The
major joint (point-light) is specified and readily available
prior to action and style recognition as well as the previous
post-structure-from-motion models. Specifically, the pointlight coordinate was directly fed to the model. (2) The
velocity profile (and its higher-order derivatives) is
supposed a primary source of information for style
recognition. (3) The model integrates local (single-joint
motion) and global (multi-joint motion) in form of linear
combination. This is simply implemented as linear
regression in which the best linear combination of them was
estimated by optimization of recognition/classification
performance.
On the other hand, we do not assume that action is
specified prior to recognition of action style, instead we
rather expect to find generalizable features of action style
common in multiple types of actions. Since people can
recognize different styles in unconventional actions (Moore,
Hobson, & Lee, 1997; Hobson & Lee, 1999), a model for
human biological motion is required to be general for
multiple actions.
Specifically, in the present study, we analyzed the human
bodily actions while the actors were given different
emotional contexts (Ma et al., 2006). These actions are

experimentally manipulated which emotional context was
intended to be under each action performance. Given such a
set of human actions, our first goal is to recover the latent
emotional attributes which the actor intended to hold (or so
experimentally manipulated) from the physical motions. By
doing so, we describe how the emotional attributes are
expressed in the different bodily actions. More specifically,
we focus on the following questions: Is it possible to find a
general features regardless of different types of actions? If
so, which types of features take crucial roles?

Biological motion library
Ma et al. (2006) have created an open-access biological
motion library, consisting of data recorded from a motion
tracking system (point-light actors: Figure 1a). The dataset
contains 30 naïve actors each performing 5 actions (walk,
knock, lifting, throw, and combinations of the four actions)
in 4 emotional contexts (angry, happy, sad, and neutral) (see
Ma et al., 2006 for more detail). Each action was performed
after the subject was given a background story manipulating
the emotional context how the subject is supposed to
perform the action. In the present study, we used a subset of
actions mainly using right hand, i.e., knock, lift, and throw.
As an example, the joint angle of right arm and its angle
velocity while 5 repeating the same actions is drawn in 1b.

Figure 1: (a) Point-light actor (no link in the model and
behavioral test), (b) A temporal profile of right-handelbow-shoulder joint angle (solid line) and its velocity
profile (black) in 5 repeating knock, lift, and throw actions.

Model of Emotion Recognition from Actions
We analyzed a subset of the data from biological motion
library (Ma et al., 2006). This subset included 15 male and
15 female actors performing 3 different actions (knock, lift,
throw) in each of 4 different emotional contexts (neutral,
angry, happy, sad). Although the model was trained with
actions with neutral emotion as well as actions with
emotional attributes, only the three non-neutral emotions
were used in the test to facilitate comparison to behavioral
data. The additional neutral actions in the training data give
the model a chance to learn actions in emotionally neutral
context which human subjects have experienced out of
laboratory experiment. Each action in each emotional
context was repeated 5 times on each of 2 trials, producing
3600 actions in total.

1680

Features: covariance of velocity, acceleration, and jerk.
In order to implement previous theoretical findings
regarding velocity profiles as a cue for biological motion
perception, we used the velocity, acceleration, and jerk
covariance profiles to identify actions and emotions. Each
of these was used to define features for the regression model.
Features came in two kinds: local and global. For instance,
variance of acceleration is a local motion property (singlepoint motion) that captures smoothness of motion over an
interval. The covariance of acceleration between multiple
points is a global motion property that captures the degree
of temporal coordination between two body parts.
Variance/covariance was evaluated for each action defined
by joint-angle of right arm (see also Ma et al., 2006 for
details). We used a nested model structure to identify the
contribution of each kind of information to action and
emotion identification. The simplest model was a velcocityonly model that included only the single-point variance and
two-point covariances for each joint. This model was
subsumed by an acceleration model that also included
acceleration variance and covaraiance, and both were nested
in a model that included jerk variance/covariance
information. Since, at each moment, velocity and
acceleration of 15 body parts were obtained, 15 variances
and 105 covariances were obtained for each action. Thus, a
total 120, 240, or 360 dimensions across pairwise body parts
were used for classification in the Velocity, Acceleration
and Jerk models, respectively. Because it produced the most
parsimonious fits to behavioral data, the Acceleration model
was analyzed most extensively, and is the model discussed
if a different model is not mentioned specifically.
Classification with automatic dimension reduction
These normalized variance and covariance features were
used to classify emotions and actions using a multi-class
sparse logistic regression model (Yamashita et al., 2008).
Model parameters were estimated in a hierarchical Bayesian
framework, which penalizes parameters that do not
contribute significantly to improving prediction. This is
done with a sparsity that reduces the likelihood of the model
in proportion to the number of non-zero parameters (w)
multiplied by a scaling parameter  (Figure 1c for its
graphical model). Specifically, the probably of each action i
belonging to class k P(yik) follows the multinomial
distribution with probability represented as logistic function
of linear combination of the given features xij for data i of
dimension j with the weights wjk as follows.
1
(1)



p y ik   1  exp   x ij w jk  


 j



The loglikelihood of class of data given by Equation 1
combined with the prior probability on the weights w is
maximized. The sparseness prior is given as follows.



pw jk    jk exp  2 1  jk w 2jk
p jk   



1
jk

where weights follows the gamma distribution with the
hyper parameter jk which follows a fixed-parameter gamma
distribution (Jeffrey’s prior). This prior prefers zero-value

weights, and thus penalize non-zero weights without
sufficient information to classification of the given data.
Thus, without any free parameter to adjust, most of
weights on non-relevant dimensions were supposed to be
excluded from the model on course of optimization.
The each action in the dataset is randomly assigned either
training or test samples. The 3300 training samples were
used to estimate the parameters in the classification model,
and its performance with the 300 test samples was evaluated.
The reported results were averaged across 10 randomly
generated sets of test/training samples.

Figure 2: The sparse logistic regression linking the
velocity features to given emotion/action class.
Classification performance in the model
The average correct classification of the Acceleration model
with velocity and acceleration profile as features was 97.5%
for action types and 69.73% for emotion classes at 33% as
chance level of both action and emotion classification. The
response patterns of the model in emotion classification
were shown at the bottom panel in Figure 2. In order to
evaluate the model’s prediction on the action/emotion
classification, we conducted the behavioral study on
action/emotion classification. The detail of the model’s
prediction would be discussed with the behavioral data.

Behavioral study: action/emotion classification
In order to test the prediction of the proposed model, we run
behavioral experiment in which adult participants were
asked to classify the type of actions and emotions given
human actions presented as a point-light display. A subset
data from the biological motion library was used as stimuli.
Participants
10 graduate students at Indiana University were recruited.
Material
Action-emotion stimuli were sampled from the biological
motion library (Ma et al., 2006). Nine pairwise
combinations of 3 actions (knock, lift, and throw) and 3
emotions (angry, happy, sad) were sampled from each of 3
selected actors. This yielded 27 video clips in total. The
viewing angle was fixed to look down the actor from actors’
left side, so that the view capture the both front and side
aspects of actions. From this fixed angle, point-lights from
different joints rarely overlapped.
Procedure
The experimental procedure consisted of two separated
phases. In familiarization phase, 9 video clips (3 actions by
3 emotions) which were not used in the following phase
were presented on a computer monitor simultaneously. Each
was accompanied by a label identifying both the action and
emotion in the clip. Participants were told that they would

1681

be asked to categorize similar clips by action and emotion,
and that they should watch the clips until they instructed to
watch the clips until they were satisfied that they felt they
could do so.
A test phase followed immediately after the familiarization
phase. Each participant watched a series of 15 second in
which a point-light actor performed one of the 3 actions in
the style of one of the three emotions. Participants were
asked to determine the action and emotion in each video.
Presentation of the stimulus on each trial was ended either
when the clip ended or when the participant pressed a button
to advance to the next trial. The test phase consists of 27
trials, with presentation order randomized by participant.
Together, the familiarization and test phases lasted
approximately 10 minutes for each participant.

Results and Discussion
The proposed model provides quantitative prediction on
classification of emotional attributes based on statistical
structure in velocity profiles. Here we compared
classification performance of action and emotion in the
human behavior and the models. The correct ratio in action
classification was nearly perfect in both human (98.61%)
and the Acceleration model (97.5%) to chance level 33%.
The correct ratio in emotion recognition was comparably
medium level in both human (68.98%) and model (69.73%)
to chance level 33%. The result showed the model achieved
comparable performance in biological motions in both
action and emotion classification.
Data fitting and comparison of models
Since the action classification was nearly at ceiling, we
analyzed the classification error patterns emotional
classification in detail (Figure 3). Figure 3 shows the
proportion of responses for each type of emotion. In order to
analyze which kind of feature human subjects utilized, we
compared the three variant of the models with nest-structure
feature sets: Velocity model with only velocity profile,
Acceleration model with velocity and acceleration profile,
Jerk model with velocity, acceleration, and jerk (up to the
third order derivative). The goodness-of-fit for each model
was evaluated to what extent human responses in the
behavioral data followed a multinomial distribution with the
average proportion of responses in each model as
parameters. Note that, although the feature set in the models
were different, none of the three models were optimized for
fitting of the behavioral data (thus no free parameter).
Instead they were optimized to classify the emotional
attributes in actions. The log-likelihood of data for Velocity,
Acceleration, Jerk model were -93.931 (R2 = 0.810), 90.051
(R2 = 0.890), and -89.116 (R2 = 0.900), respectively. The
likelihood ratio test revealed significant difference in
likelihood of Velocity model from the other two models
(2(1)>3.8807, p<0.05), but did not find significant
difference between Acceleration and Jerk model (2(1)=
0.7479, p=0.33). This result of model comparison suggested
that velocity profile alone was not sufficient to capture
behavioral patterns, but velocity and acceleration profile

might be sufficient since the additional jerk profile made
little additional contribution for data fitting. Therefore,
hereafter we analyzed the Acceleration model as the
representative model.
Action-specificity of emotion attributes
Next, we tested the hypothesis that recognition of emotion
attributes is specific to each action types. If so, the model
trained to classify emotional attributes for each action
(Action-specific model) would capture behavioral patterns
better than the model trained to classify them for all the
three actions together. The log-likelihood of Action-specific
acceleration model was -90.6381 (R2 = 0.890) which is
slightly worse but not significantly different from that of
non-action-specific acceleration model (2(1)= 0.5871,
p=0.444). Therefore, the action-specific model did not
necessarily offer a better account for human recognition.
Classification with only average velocity
One of largest qualitative difference between human and
model was found in the proportion of response “happy” to
angry action: human recognizers confused angry with happy
more than with sad, whereas the model recognizers
confused it with sad more than happy. According to the
post-experimental interview to the participants, many of
them reported that they relied on average velocity of actions.
Typically angry actions tended to be fast, sad actions were
slow in the current stimuli, and happy ones were in the
middle of them. This may be a potential reason why for
human perceiver angry actions tended to be confused with
happy ones rather than with sad ones, and also the model
did not included as its features for classification.
Therefore, in order to evaluate the contribution of average
velocity, we performed additional analysis. A past study has
reported that the average velocity, or duration of from
beginning to end of action, (and its correlated factors such
as duration) was one of major correlates to subjects’ rating
of emotion attribute (Polllick et al., 2001). Indeed, we found
the angry actions were fast and sad actions tended to be
slow on average in the data used in the present study. The
four-way ANOVA on the factor (emotion types, action
types, repetition, and trial), revealed the significant main
effect of all the factors but trials (p<0.01).
However, the average velocity of the actions alone was
not enough for classification of action style. The correct
ratio using the average velocity, duration, peak velocity of
each action was 35.9% (chance level 25%) which was not
comparable to human performance (68.98%). Even after we
classify the subset of data separate for each action, the
classification performance did not improve significantly
(Average of three actions: 36.4, Knock: 34.1, Lift: 38.0%,
Throw: 37.1% for the chance level 25%). This result
suggested the average velocity of actions alone could not
fully explain the action style recognition.
In sum, the current model-based analyses suggested that
the covariance profile of acceleration in multiple body parts
carried significant amount of information on emotion
attributes in actions. Despite of very different velocity
profiles in three actions, knock, lift, and throw, the

1682

classification of emotion regardless of the actions was as
successful as action-specific classification. This result
suggested that, to some extent, emotional attributes in
actions were more general rather than specific for each
action.

body parts or their relationship, they would offer a
specifically testable prediction on which body parts may be
potentially informative.

Figure 4: The variance/covariance in velocity profile
significantly relevant to each emotion attribute mapped on a
body scheme. The white and gray cell indicates effective
variance/covariance of velocity and acceleration,
respecitively. No lower triangle cells were presented due to
its symmetricity. The bottom right panel showed the number
of effective dimensions for each emotion attribute.

Figure 3: The response patterns for each emotion type in
human subjects (upper panel) and the model (bottom panel).
Distributed cues in emotion recognition
Next, we analyzed the effective feature dimensions for
each emotion attribute in the Acceleration model. In the
sparse logistic regression, the fewest possible feature
dimensions were automatically selected among all the given
dimensions. The selected dimensions, variance and
covariance of acceleration profiles, were supposed to be a
spatiotemporal “template” informative to action/emotion
attributes. Thus we analyzed which features are specified for
each of emotion attributes. Figure 4 depicted the interconnection between body parts which were identified as
significant features for emotion discrimination (see also
criterion of the dimension selection in the model). In each
panel of Figure 4, the thin and thick lines indicate
covariance of velocity and acceleration for a pair of body
parts respectively which is also coded by intensity in the
adjacent matrix. The number of effective dimensions for
velocity/acceleration
and
local
(variance)/global
(covariance) was shown at the bottom right panel.
We found the numbers of effective velocity dimensions
(either local or global) were consistent with the average
velocity: the largest number of effective dimensions in
angry actions which tended to be performed fast, meanwhile
the smallest number of them in sad actions which tended to
be performed slow. Also the total number of effective
dimensions for each action was consistent to the
classification performance (Figure 3): the model found
fewest effective dimensions for happy actions and had
lowest accuracy in identifying them. Overall, we found
more global features (pairwise covariance) than the local
features (single-point variance). This result suggested that
the emotional attributes were distributed rather than specific
to a small number of body parts. Since these patterns found
in the present model were directly interpreted as those on

General Discussion
In the present study, we provided a computational model
which specifies characteristic kinematic features for
recognition of emotional attributes in actions. Following the
lead of the past studies, we analyzed the velocity profile
with special attention. Classification with covariance of
velocity and acceleration profile among multiple joints
showed comparable performance as good as human
classification. Moreover, by comparing multiple models
trained with different feature types, it suggested that (1)
velocity alones was not sufficient but combined with
acceleration or higher order derivatives might characterize
the human emotion recognition, and (2) there may be
common emotional attributes invariant to action-specific
motion profiles.
Action style as parallel process rather than hierarchy
The present analysis showed that, based on covariance of
velocity profile across whole body, emotion attributes may
be characterized beyond specificity of each action. However,
recent review on action recognition offers a contradictory
view to the present study: recognition of action style needs
action recognition in prior to it. According to a recent
review (Troje, 2008), perception of biological motion is
multi-level processing on local and global motion properties.
The feature processing consists of four layers from early
(low-level) to late (high-level) processing: life detection,
structure-from-motion, action recognition, and style
recognition. Once both agent and action are identified,
pattern recognition at a “subordinate” level (Rosch, 1988)
helps to retrieve further information about the details (i.e.,
action style) of both.
In the present study, we propose an alternative view on
action style perception: The emotion attributes can be
identified with or without pre-specification of action types.

1683

In the present analysis, we found that the model without
action-specific features can account for behavioral
classification performance as well as that with actionspecific features. Therefore, we speculate that action style is
“parallel” process rather than hierarchical one to action
recognition which may be coded independently from the
action types.
Future works
One of the future directions is to extend the behavioral
study so that we can evaluate subjects’ attention to body
parts and its time course using an additional measure (e.g.,
eye movements). The extended behavioral study would
allow us to directly test the model’s detailed prediction
about which body parts and their relationship may be
informative to action/emotion classification (Figure 4).

Acknowledgments
The author is grateful to Daniel Yurovsky for his discussion
on the current manuscript. This study was supported by
Artificial Intelligence Research Promotion Foundation and
Grant-in-Aid for Scientific Research B No. 23300099.

References
Atkinson, A. P. (2009). Impaired recognition of emotions
from body movements is associated with elevated motion
coherence thresholds in autism spectrum disorders.
Neuropsychologia, 47, 3023–3029.
Bernstein, N. A. (1967). The coordination and regulation of
movements. Oxford: Pergamon.
Bingham, G. P. (1987). Kinematic form and scaling: Further
investigations on the visual perception of lifted weight.
Journal of Experimental Psychology: Human Perception
and Performance, 13, 2, 155-177,
Blake, R. & Shiffrar, M., (2007). Perception of Human
Motion. Annual Review of Psychology, 58, 47–73.
Chang, D. H. F., & Troje, N. F. (2008). Perception of
animacy and direction from local biological motion
signals. Journal of Vision, 8, (5):3, 1–10.
Chang, D. H. F., & Troje, N. F. (2009). Acceleration carries
the local inversion effect in biological motion perception.
Journal of Vision, 9, (1):19, 1–17
Chang, D. H. F., & Troje, N. F. (2009). Acceleration carries
the local inversion effect in biological motion perception.
Journal of Vision, 9(1):19, 1–17.
Cook, J., Saygin, A. P., Swain, R., & Blakemore, S-H.,
(2009). Reduced sensitivity to minimum-jerk biological
motion in autism spectrum conditions. Neuropsychologia,
47, 14, 3275-3278.
DeMeijer, M, (1989). The contribution of general features
of body movement to the attribution of emotions. Journal
of Nonverbal Behavior, 13, 4, 247-268.
Giese, M. A. & Poggio, T. (2003). Neural Mechanisms for
the recognition of biological movements, Nature Reviews
Neuroscience, 4, 179-192.
Hobson, R. P. & Lee, A. (1999). Imitation and Identification
in Autism, Journal of Child Psychological Psychiatry, 40,
4, 649-659.

Hubert, B., Wicker, B., Moore, D. G., Monfardini, E.,
Duverger, H., Fonse´ca, D. Da, Deruelle, C. (2006).
Recognition of Emotional and Non-emotional Biological
Motion in Individuals with Autistic Spectrum Disorders.
Journal of Autism Developmental Disorders, 37, 7, 13861392.
Johhanson, G. (1973). Visual perception of biological
motion and a model for its analysis. Perception &
Psychophysics, 14, 2, 201-211.
Johnson, M. H., Bolhuis, J. J., & Horn, G. (1985).
Interaction between acquired preferences and developing
predispositions during imprinting. Animal Behaviour, 33,
1000–1006.
Lange, J., & Lappe, M. (2006). A model of biological
motion perception from conﬁgural form cues. Journal of
Neuroscience, 26, 11, 2894–2906.
Ma, Y., Paterson, H. M., Pollick, F. E. (2006). A motion
capture library for the study of identity, gender, and
emotion perception from biological motion, Behavior
Research Methods, 38, 1, 134-141.
Moore, Hobson, & Lee (1997). Components of person
perception: An investigation with autistic, non-autistic
retarded and typically developing children and
adolescents., British Journal of Developmental
Psychology, 15, 401-423.
Pollick, F. E., Lestou, V., Ryu J. Cho, S-B. (2002)
Estimating the efficiency of recognizing gender and affect
from biological motion., Vision Research, 42, 2345-2355.
Pollick, F. E., Paterson, H., Bruderlin, A. & Sanford, A. J.
(2001) Perceiving affect from arm movement. Cognition,
82, B51-B61.
Pollick F. E., Paterson, E. (2008). Movement style,
Movement features, and the recognition of affect from
human motion, In Shipley, T. F. & Zacks, J. M.,
Understanding Events from Perception to Action, New
York: Oxford University Press, 286-307.
Rosch, E. (1988). Principles of categorization. In A. Collins
& E. E. Smith (Eds.), Readings in cognitive science (pp.
312-322). Sam Mateo: Morgenkaufmann.
Troje, N. F. (2002). Decomposing biological motion: A
framework for analysis and synthesis of human gait
patterns. Journal of Vision, 2, 371-387.
Troje, N. F. (2008). Biological motion perception. In
Basbaum, A. et al. (Eds.), The senses: A comprehensive
reference (pp. 231–238). Oxford: Elsevier.
Troje, N. F., Westhoff, C., & Lavrov, M. (2005). Person
identification from biological motion: effects of structural
and kinematic cues. Perception & Psychophysics, 67 (4),
667-675.
Pollick, F. E., Paterson, H. M., Bruderlin, A., Sanford, A. J.,
(2001). Perceiving affect from arm movement. Cognition,
82, B51–B61.
Yamashita, O., Sato, MA., Yoshioka, T., Tong F., Kamitani
Y. (2008). Sparse estimation automatically selects voxels
relevant for the decoding of fMRI activity patterns.
Neuroimage. 42, 4, 1414-29.

1684

