UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Identifying Kinematic Cues for Action Style Recognition
Permalink
https://escholarship.org/uc/item/7vw5g0t4
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Author
Hidaka, Shohei
Publication Date
2012-01-01
Peer reviewed
  eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                        Identifying Kinematic Cues for Action Style Recognition
                                              Shohei Hidaka (shhidaka@jaist.ac.jp)
                                         Japan Advanced Institute of Science and Technology,
                                             1-1 Asahidai, Nomi, Ishikawa 923-1292, Japan
                             Abstract                                  lifted object (Bingham, 1987) of actions from point-light
                                                                       displays.
   Recognition of emotional states from other’s actions is one of
   key capability for smooth social interaction. The present study        Not only demonstrating human capacity, the studies using
   provides a computational-theory-level analysis on which             point-light display have suggested features extracted in
   feature may take a crucial role in recognition of emotional         action perception. Accumulating empirical studies on action
   attributes in human actions represented as point-light display.     perception have suggested that velocity and its higher order
   Lead by the previous theoretical works and empirical findings,      derivatives in a single or multiple body parts as one of
   the velocity and acceleration profile was investigated as a         major correlates to emotional attributes in actions: duration
   major feature of emotional attributes classification. The
   results showed that emotional attributes in actions as well as
                                                                       of action (Pollick et al., 2001), velocity (DeMeijer, 1989),
   action types could be identified by covariance of velocity          acceleration (force or the second order time derivatives)
   profiles among multiple body parts. Since, despite different        (Chang & Troje, 2008; 2009) and jerk or the third order
   velocity profiles in different actions, these features for          time derivatives (Cook, Saygin, Swain, & Blakemore, 2009),
   emotional attributes were found commonly in multiple                pairwise counter-phase oscillation (Chang & Troje, 2008;
   different actions, it suggests that the action styles may be        2009). In particular, we highlight the contribution of the
   mediated by an information channel parallel to action types         higher order derivatives of velocity and importance of its
   per se.
                                                                       covariational structure. Of relevance, Chang & Troje (2009)
   Keywords: Action style recognition; biological motion;              found that, not one of either but a pair of feed motion was a
   emotion; social cognition.                                          major cue for discrimination of walking direction.
                                                                       Past computational models on action recognition
                          Introduction                                    Consistent to these empirical findings, most of the
   Our bodily motion is coherent, smooth and effortless.               theoretical approach works on some kind of statistical
From bodily motion, we perceive other’s state such as mood,            regularities among motion profiles. According to a recent
emotional expression, and intention (Blake & Shiffrar,                 review (Troje, 2008), perception of biological motion has
2007). Perception of other’s state takes a crucial role in             the multi-level processing on local and global motion
social context. Although most of us can easily “read” what             properties. The feature processing consists of four layers
others intend to do through their actions, there is a                  from early (low-level) to late (high-level) processing: life
significant gap from the physical motion – a set of                    detection, structure-from-motion, action recognition, and
trajectories of multiple body parts with a large degree of             style recognition. The system detect autonomous agent, and
freedom (Bernstein, 1967). Recognition of motion is vitally            construct body structure from its detailed analysis, then is
important to any animal kinds. Detection of another animal,            followed by more detailed action analysis.
possibly a pray, a predator, or a conspecific, and the                   A couple of computational models are available for
following detailed identification what it is and how it may            structure-from-motion and action recognition (Giese &
behave is essential to take an emergent actions to it                  Poggio, 2003; Lange & Lappe; 2006), and a few for post-
(Johnson, Bolhuis, & Horn, 1985). Humans are social                    action-recognition-level style perception (Troje, 2002;
animals. Not surprisingly, our visual system is highly                 Pollick, Lestou, Ryu, Cho, 2002; Davis & Gao, 2004) in
specialized to recognize others’ state. The present study              vision science. In the model of structure-from-motion and
aims to provide a computational-level description on how               action recognition, the model identifies body structure and
people recognize emotional status in others’ actions.                  subsequently actions from the pixel-based visualization of
Perception of biological motion                                        point-light displays. In Giese & Poggio (2003), the model
How do we recognize implicit patterns in different styles of           was built based on neuro-physiological findings on visual
actions? The past experimental literature has explored                 cortex, and was applied to recognition of action types and
capacity of motion perception using point-light displays               action direction in normal, masked, or scrambled point-light
(Johansson, 1973) in which the point-lights attached in                displays.
major joints are only visible in the dark background (Figure              While, the post-action-recognition-level models for style
1a). Thus the available information is point-wise kinematic            perception typically assume the either/both 2D or 3D point-
motion in multiple body parts. Despite of the limited                  light on the major joints and also which action is to be
information, people can recognize identity (Troje, Westhoff,           executed is readily available prior to the recognition of
& Lavrov, 2005), gender (Kozlowski & Cutting, 1977;                    action style (Troje, 2002; Davis & Gao, 2004; Pollick et al.,
Troje, 2002), emotions (Pollick et al., 2001; Atkinson; 2009;          2001). For example, Troje (2002) have proposed a
Hobson & Lee, 1999), dynamics such as the weight of a                  computational model of gender identification in gait
                                                                   1679

presented as point-light display. The model was built upon         experimentally manipulated which emotional context was
the three stages: First a set of postures is encoded based on      intended to be under each action performance. Given such a
Fourier decomposition, the low-dimensional projection of           set of human actions, our first goal is to recover the latent
extracted features is obtained by principal component              emotional attributes which the actor intended to hold (or so
analysis (PCA), and then it is fed to classifier (as a similar     experimentally manipulated) from the physical motions. By
model, see also Davis & Gao (2004)).                               doing so, we describe how the emotional attributes are
Simple, transparent, yet general model                             expressed in the different bodily actions. More specifically,
Although the previous theoretical works have offered               we focus on the following questions: Is it possible to find a
successful pattern recognizer for biological motion, there         general features regardless of different types of actions? If
are three shortcomings. First, most of the studies have been       so, which types of features take crucial roles?
closed in one special type (or its slight variant) of action
which often has a unique constraint such as periodicity (e.g.,                    Biological motion library
walking, running; e.g., Troje, 2002). Second, related to the          Ma et al. (2006) have created an open-access biological
first point, a limited number of body parts specialized for        motion library, consisting of data recorded from a motion
each action tends to be (e.g., arm movement for tennis             tracking system (point-light actors: Figure 1a). The dataset
swing (Pollick et al., 2002) ). Although not all the model is      contains 30 naïve actors each performing 5 actions (walk,
specialized, in turn, such a generalized model typically loses     knock, lifting, throw, and combinations of the four actions)
transparency of mechanism as a cost of generality (For             in 4 emotional contexts (angry, happy, sad, and neutral) (see
example, multi-layer physiologically-plausible model, Giese        Ma et al., 2006 for more detail). Each action was performed
& Poggio, 2003). One of drawback of complex models                 after the subject was given a background story manipulating
(using nonlinear filters or feature decomposition technique        the emotional context how the subject is supposed to
such as Fourier decomposition and PCA) is that the                 perform the action. In the present study, we used a subset of
estimated parameters do not necessarily offer interpretation       actions mainly using right hand, i.e., knock, lift, and throw.
on which natural features are informative such as body parts       As an example, the joint angle of right arm and its angle
and time course (Pollick & Paterson, 2008). Moreover, such         velocity while 5 repeating the same actions is drawn in 1b.
model often outperformed human recognition (Troje, 2002;
Davis. & Geo, 2004; Pollick & Paterson, 2008) rather than
explaining use of features in human recognition.
   The theoretical assumptions in the model
   In the present study, we employed the simplest possible
framework – a variant of linear regression – in order to
characterize the motion cues in whole body interaction for
multiple types of actions and emotional attributes. The
model has the three major assumptions as follows. (1) The
major joint (point-light) is specified and readily available
prior to action and style recognition as well as the previous       Figure 1: (a) Point-light actor (no link in the model and
post-structure-from-motion models. Specifically, the point-        behavioral test), (b) A temporal profile of right-hand-
light coordinate was directly fed to the model. (2) The            elbow-shoulder joint angle (solid line) and its velocity
velocity profile (and its higher-order derivatives) is             profile (black) in 5 repeating knock, lift, and throw actions.
supposed a primary source of information for style
recognition. (3) The model integrates local (single-joint            Model of Emotion Recognition from Actions
motion) and global (multi-joint motion) in form of linear          We analyzed a subset of the data from biological motion
combination. This is simply implemented as linear                  library (Ma et al., 2006). This subset included 15 male and
regression in which the best linear combination of them was        15 female actors performing 3 different actions (knock, lift,
estimated by optimization of recognition/classification            throw) in each of 4 different emotional contexts (neutral,
performance.                                                       angry, happy, sad). Although the model was trained with
   On the other hand, we do not assume that action is              actions with neutral emotion as well as actions with
specified prior to recognition of action style, instead we         emotional attributes, only the three non-neutral emotions
rather expect to find generalizable features of action style       were used in the test to facilitate comparison to behavioral
common in multiple types of actions. Since people can              data. The additional neutral actions in the training data give
recognize different styles in unconventional actions (Moore,       the model a chance to learn actions in emotionally neutral
Hobson, & Lee, 1997; Hobson & Lee, 1999), a model for              context which human subjects have experienced out of
human biological motion is required to be general for              laboratory experiment. Each action in each emotional
multiple actions.                                                  context was repeated 5 times on each of 2 trials, producing
   Specifically, in the present study, we analyzed the human       3600 actions in total.
bodily actions while the actors were given different
emotional contexts (Ma et al., 2006). These actions are
                                                               1680

Features: covariance of velocity, acceleration, and jerk.          weights, and thus penalize non-zero weights without
In order to implement previous theoretical findings                sufficient information to classification of the given data.
regarding velocity profiles as a cue for biological motion            Thus, without any free parameter to adjust, most of
perception, we used the velocity, acceleration, and jerk           weights on non-relevant dimensions were supposed to be
covariance profiles to identify actions and emotions. Each         excluded from the model on course of optimization.
of these was used to define features for the regression model.        The each action in the dataset is randomly assigned either
Features came in two kinds: local and global. For instance,        training or test samples. The 3300 training samples were
variance of acceleration is a local motion property (single-       used to estimate the parameters in the classification model,
point motion) that captures smoothness of motion over an           and its performance with the 300 test samples was evaluated.
interval. The covariance of acceleration between multiple          The reported results were averaged across 10 randomly
points is a global motion property that captures the degree        generated sets of test/training samples.
of temporal coordination between two body parts.
Variance/covariance was evaluated for each action defined
by joint-angle of right arm (see also Ma et al., 2006 for
details). We used a nested model structure to identify the
contribution of each kind of information to action and
emotion identification. The simplest model was a velcocity-
only model that included only the single-point variance and
two-point covariances for each joint. This model was                  Figure 2: The sparse logistic regression linking the
subsumed by an acceleration model that also included               velocity features to given emotion/action class.
acceleration variance and covaraiance, and both were nested        Classification performance in the model
in a model that included jerk variance/covariance                  The average correct classification of the Acceleration model
information. Since, at each moment, velocity and                   with velocity and acceleration profile as features was 97.5%
acceleration of 15 body parts were obtained, 15 variances          for action types and 69.73% for emotion classes at 33% as
and 105 covariances were obtained for each action. Thus, a         chance level of both action and emotion classification. The
total 120, 240, or 360 dimensions across pairwise body parts       response patterns of the model in emotion classification
were used for classification in the Velocity, Acceleration         were shown at the bottom panel in Figure 2. In order to
and Jerk models, respectively. Because it produced the most        evaluate the model’s prediction on the action/emotion
parsimonious fits to behavioral data, the Acceleration model       classification, we conducted the behavioral study on
was analyzed most extensively, and is the model discussed          action/emotion classification. The detail of the model’s
if a different model is not mentioned specifically.                prediction would be discussed with the behavioral data.
Classification with automatic dimension reduction
These normalized variance and covariance features were              Behavioral study: action/emotion classification
used to classify emotions and actions using a multi-class          In order to test the prediction of the proposed model, we run
sparse logistic regression model (Yamashita et al., 2008).         behavioral experiment in which adult participants were
Model parameters were estimated in a hierarchical Bayesian         asked to classify the type of actions and emotions given
framework, which penalizes parameters that do not                  human actions presented as a point-light display. A subset
contribute significantly to improving prediction. This is          data from the biological motion library was used as stimuli.
done with a sparsity that reduces the likelihood of the model      Participants
in proportion to the number of non-zero parameters (w)             10 graduate students at Indiana University were recruited.
multiplied by a scaling parameter  (Figure 1c for its             Material
graphical model). Specifically, the probably of each action i      Action-emotion stimuli were sampled from the biological
belonging to class k P(yik) follows the multinomial                motion library (Ma et al., 2006). Nine pairwise
distribution with probability represented as logistic function     combinations of 3 actions (knock, lift, and throw) and 3
of linear combination of the given features xij for data i of      emotions (angry, happy, sad) were sampled from each of 3
dimension j with the weights wjk as follows.                       selected actors. This yielded 27 video clips in total. The
                                      
                                             1
                                                (1)                viewing angle was fixed to look down the actor from actors’
    p y ik   1  exp   x ij w jk                         left side, so that the view capture the both front and side
                                          
                          j            
   The loglikelihood of class of data given by Equation 1          aspects of actions. From this fixed angle, point-lights from
combined with the prior probability on the weights w is            different joints rarely overlapped.
maximized. The sparseness prior is given as follows.               Procedure
                              
    pw jk    jk exp  2 1  jk w 2jk                           The experimental procedure consisted of two separated
                                                                   phases. In familiarization phase, 9 video clips (3 actions by
    p jk       1
                    jk                                             3 emotions) which were not used in the following phase
   where weights follows the gamma distribution with the           were presented on a computer monitor simultaneously. Each
hyper parameter jk which follows a fixed-parameter gamma          was accompanied by a label identifying both the action and
distribution (Jeffrey’s prior). This prior prefers zero-value      emotion in the clip. Participants were told that they would
                                                               1681

be asked to categorize similar clips by action and emotion,         might be sufficient since the additional jerk profile made
and that they should watch the clips until they instructed to       little additional contribution for data fitting. Therefore,
watch the clips until they were satisfied that they felt they       hereafter we analyzed the Acceleration model as the
could do so.                                                        representative model.
  A test phase followed immediately after the familiarization       Action-specificity of emotion attributes
phase. Each participant watched a series of 15 second in            Next, we tested the hypothesis that recognition of emotion
which a point-light actor performed one of the 3 actions in         attributes is specific to each action types. If so, the model
the style of one of the three emotions. Participants were           trained to classify emotional attributes for each action
asked to determine the action and emotion in each video.            (Action-specific model) would capture behavioral patterns
Presentation of the stimulus on each trial was ended either         better than the model trained to classify them for all the
when the clip ended or when the participant pressed a button        three actions together. The log-likelihood of Action-specific
to advance to the next trial. The test phase consists of 27         acceleration model was -90.6381 (R2 = 0.890) which is
trials, with presentation order randomized by participant.          slightly worse but not significantly different from that of
Together, the familiarization and test phases lasted                non-action-specific acceleration model (2(1)= 0.5871,
approximately 10 minutes for each participant.                      p=0.444). Therefore, the action-specific model did not
                                                                    necessarily offer a better account for human recognition.
                 Results and Discussion                             Classification with only average velocity
   The proposed model provides quantitative prediction on              One of largest qualitative difference between human and
classification of emotional attributes based on statistical         model was found in the proportion of response “happy” to
structure in velocity profiles. Here we compared                    angry action: human recognizers confused angry with happy
classification performance of action and emotion in the             more than with sad, whereas the model recognizers
human behavior and the models. The correct ratio in action          confused it with sad more than happy. According to the
classification was nearly perfect in both human (98.61%)            post-experimental interview to the participants, many of
and the Acceleration model (97.5%) to chance level 33%.             them reported that they relied on average velocity of actions.
The correct ratio in emotion recognition was comparably             Typically angry actions tended to be fast, sad actions were
medium level in both human (68.98%) and model (69.73%)              slow in the current stimuli, and happy ones were in the
to chance level 33%. The result showed the model achieved           middle of them. This may be a potential reason why for
comparable performance in biological motions in both                human perceiver angry actions tended to be confused with
action and emotion classification.                                  happy ones rather than with sad ones, and also the model
Data fitting and comparison of models                               did not included as its features for classification.
   Since the action classification was nearly at ceiling, we           Therefore, in order to evaluate the contribution of average
analyzed the classification error patterns emotional                velocity, we performed additional analysis. A past study has
classification in detail (Figure 3). Figure 3 shows the             reported that the average velocity, or duration of from
proportion of responses for each type of emotion. In order to       beginning to end of action, (and its correlated factors such
analyze which kind of feature human subjects utilized, we           as duration) was one of major correlates to subjects’ rating
compared the three variant of the models with nest-structure        of emotion attribute (Polllick et al., 2001). Indeed, we found
feature sets: Velocity model with only velocity profile,            the angry actions were fast and sad actions tended to be
Acceleration model with velocity and acceleration profile,          slow on average in the data used in the present study. The
Jerk model with velocity, acceleration, and jerk (up to the         four-way ANOVA on the factor (emotion types, action
third order derivative). The goodness-of-fit for each model         types, repetition, and trial), revealed the significant main
was evaluated to what extent human responses in the                 effect of all the factors but trials (p<0.01).
behavioral data followed a multinomial distribution with the           However, the average velocity of the actions alone was
average proportion of responses in each model as                    not enough for classification of action style. The correct
parameters. Note that, although the feature set in the models       ratio using the average velocity, duration, peak velocity of
were different, none of the three models were optimized for         each action was 35.9% (chance level 25%) which was not
fitting of the behavioral data (thus no free parameter).            comparable to human performance (68.98%). Even after we
Instead they were optimized to classify the emotional               classify the subset of data separate for each action, the
attributes in actions. The log-likelihood of data for Velocity,     classification performance did not improve significantly
Acceleration, Jerk model were -93.931 (R2 = 0.810), 90.051          (Average of three actions: 36.4, Knock: 34.1, Lift: 38.0%,
(R2 = 0.890), and -89.116 (R2 = 0.900), respectively. The           Throw: 37.1% for the chance level 25%). This result
likelihood ratio test revealed significant difference in            suggested the average velocity of actions alone could not
likelihood of Velocity model from the other two models              fully explain the action style recognition.
(2(1)>3.8807, p<0.05), but did not find significant                   In sum, the current model-based analyses suggested that
                                                                    the covariance profile of acceleration in multiple body parts
difference between Acceleration and Jerk model (2(1)=
                                                                    carried significant amount of information on emotion
0.7479, p=0.33). This result of model comparison suggested
                                                                    attributes in actions. Despite of very different velocity
that velocity profile alone was not sufficient to capture
                                                                    profiles in three actions, knock, lift, and throw, the
behavioral patterns, but velocity and acceleration profile
                                                                1682

classification of emotion regardless of the actions was as         body parts or their relationship, they would offer a
successful as action-specific classification. This result          specifically testable prediction on which body parts may be
suggested that, to some extent, emotional attributes in            potentially informative.
actions were more general rather than specific for each
action.
                                                                      Figure 4: The variance/covariance in velocity profile
                                                                   significantly relevant to each emotion attribute mapped on a
                                                                   body scheme. The white and gray cell indicates effective
                                                                   variance/covariance of velocity and acceleration,
                                                                   respecitively. No lower triangle cells were presented due to
                                                                   its symmetricity. The bottom right panel showed the number
   Figure 3: The response patterns for each emotion type in        of effective dimensions for each emotion attribute.
human subjects (upper panel) and the model (bottom panel).
Distributed cues in emotion recognition
                                                                                      General Discussion
   Next, we analyzed the effective feature dimensions for             In the present study, we provided a computational model
each emotion attribute in the Acceleration model. In the           which specifies characteristic kinematic features for
sparse logistic regression, the fewest possible feature            recognition of emotional attributes in actions. Following the
dimensions were automatically selected among all the given         lead of the past studies, we analyzed the velocity profile
dimensions. The selected dimensions, variance and                  with special attention. Classification with covariance of
covariance of acceleration profiles, were supposed to be a         velocity and acceleration profile among multiple joints
spatiotemporal “template” informative to action/emotion            showed comparable performance as good as human
attributes. Thus we analyzed which features are specified for      classification. Moreover, by comparing multiple models
each of emotion attributes. Figure 4 depicted the inter-           trained with different feature types, it suggested that (1)
connection between body parts which were identified as             velocity alones was not sufficient but combined with
significant features for emotion discrimination (see also          acceleration or higher order derivatives might characterize
criterion of the dimension selection in the model). In each        the human emotion recognition, and (2) there may be
panel of Figure 4, the thin and thick lines indicate               common emotional attributes invariant to action-specific
covariance of velocity and acceleration for a pair of body         motion profiles.
parts respectively which is also coded by intensity in the         Action style as parallel process rather than hierarchy
adjacent matrix. The number of effective dimensions for               The present analysis showed that, based on covariance of
velocity/acceleration     and     local     (variance)/global      velocity profile across whole body, emotion attributes may
(covariance) was shown at the bottom right panel.                  be characterized beyond specificity of each action. However,
   We found the numbers of effective velocity dimensions           recent review on action recognition offers a contradictory
(either local or global) were consistent with the average          view to the present study: recognition of action style needs
velocity: the largest number of effective dimensions in            action recognition in prior to it. According to a recent
angry actions which tended to be performed fast, meanwhile         review (Troje, 2008), perception of biological motion is
the smallest number of them in sad actions which tended to         multi-level processing on local and global motion properties.
be performed slow. Also the total number of effective              The feature processing consists of four layers from early
dimensions for each action was consistent to the                   (low-level) to late (high-level) processing: life detection,
classification performance (Figure 3): the model found             structure-from-motion, action recognition, and style
fewest effective dimensions for happy actions and had              recognition. Once both agent and action are identified,
lowest accuracy in identifying them. Overall, we found             pattern recognition at a “subordinate” level (Rosch, 1988)
more global features (pairwise covariance) than the local          helps to retrieve further information about the details (i.e.,
features (single-point variance). This result suggested that       action style) of both.
the emotional attributes were distributed rather than specific       In the present study, we propose an alternative view on
to a small number of body parts. Since these patterns found        action style perception: The emotion attributes can be
in the present model were directly interpreted as those on         identified with or without pre-specification of action types.
                                                               1683

In the present analysis, we found that the model without            Hubert, B., Wicker, B., Moore, D. G., Monfardini, E.,
action-specific features can account for behavioral                   Duverger, H., Fonse´ca, D. Da, Deruelle, C. (2006).
classification performance as well as that with action-               Recognition of Emotional and Non-emotional Biological
specific features. Therefore, we speculate that action style is       Motion in Individuals with Autistic Spectrum Disorders.
“parallel” process rather than hierarchical one to action             Journal of Autism Developmental Disorders, 37, 7, 1386-
recognition which may be coded independently from the                 1392.
action types.                                                       Johhanson, G. (1973). Visual perception of biological
Future works                                                          motion and a model for its analysis. Perception &
   One of the future directions is to extend the behavioral           Psychophysics, 14, 2, 201-211.
study so that we can evaluate subjects’ attention to body           Johnson, M. H., Bolhuis, J. J., & Horn, G. (1985).
parts and its time course using an additional measure (e.g.,          Interaction between acquired preferences and developing
eye movements). The extended behavioral study would                   predispositions during imprinting. Animal Behaviour, 33,
allow us to directly test the model’s detailed prediction             1000–1006.
about which body parts and their relationship may be                Lange, J., & Lappe, M. (2006). A model of biological
informative to action/emotion classification (Figure 4).              motion perception from conﬁgural form cues. Journal of
                                                                      Neuroscience, 26, 11, 2894–2906.
                    Acknowledgments                                 Ma, Y., Paterson, H. M., Pollick, F. E. (2006). A motion
The author is grateful to Daniel Yurovsky for his discussion          capture library for the study of identity, gender, and
on the current manuscript. This study was supported by                emotion perception from biological motion, Behavior
Artificial Intelligence Research Promotion Foundation and             Research Methods, 38, 1, 134-141.
Grant-in-Aid for Scientific Research B No. 23300099.                Moore, Hobson, & Lee (1997). Components of person
                                                                      perception: An investigation with autistic, non-autistic
                                                                      retarded and typically developing children and
                         References                                   adolescents., British Journal of Developmental
Atkinson, A. P. (2009). Impaired recognition of emotions              Psychology, 15, 401-423.
  from body movements is associated with elevated motion            Pollick, F. E., Lestou, V., Ryu J. Cho, S-B. (2002)
  coherence thresholds in autism spectrum disorders.                  Estimating the efficiency of recognizing gender and affect
  Neuropsychologia, 47, 3023–3029.                                    from biological motion., Vision Research, 42, 2345-2355.
Bernstein, N. A. (1967). The coordination and regulation of         Pollick, F. E., Paterson, H., Bruderlin, A. & Sanford, A. J.
  movements. Oxford: Pergamon.                                        (2001) Perceiving affect from arm movement. Cognition,
Bingham, G. P. (1987). Kinematic form and scaling: Further            82, B51-B61.
  investigations on the visual perception of lifted weight.         Pollick F. E., Paterson, E. (2008). Movement style,
  Journal of Experimental Psychology: Human Perception                Movement features, and the recognition of affect from
  and Performance, 13, 2, 155-177,                                    human motion, In Shipley, T. F. & Zacks, J. M.,
Blake, R. & Shiffrar, M., (2007). Perception of Human                 Understanding Events from Perception to Action, New
  Motion. Annual Review of Psychology, 58, 47–73.                     York: Oxford University Press, 286-307.
Chang, D. H. F., & Troje, N. F. (2008). Perception of               Rosch, E. (1988). Principles of categorization. In A. Collins
  animacy and direction from local biological motion                  & E. E. Smith (Eds.), Readings in cognitive science (pp.
  signals. Journal of Vision, 8, (5):3, 1–10.                         312-322). Sam Mateo: Morgenkaufmann.
Chang, D. H. F., & Troje, N. F. (2009). Acceleration carries        Troje, N. F. (2002). Decomposing biological motion: A
  the local inversion effect in biological motion perception.          framework for analysis and synthesis of human gait
  Journal of Vision, 9, (1):19, 1–17                                   patterns. Journal of Vision, 2, 371-387.
Chang, D. H. F., & Troje, N. F. (2009). Acceleration carries        Troje, N. F. (2008). Biological motion perception. In
  the local inversion effect in biological motion perception.          Basbaum, A. et al. (Eds.), The senses: A comprehensive
  Journal of Vision, 9(1):19, 1–17.                                    reference (pp. 231–238). Oxford: Elsevier.
Cook, J., Saygin, A. P., Swain, R., & Blakemore, S-H.,              Troje, N. F., Westhoff, C., & Lavrov, M. (2005). Person
  (2009). Reduced sensitivity to minimum-jerk biological               identification from biological motion: effects of structural
  motion in autism spectrum conditions. Neuropsychologia,              and kinematic cues. Perception & Psychophysics, 67 (4),
  47, 14, 3275-3278.                                                   667-675.
DeMeijer, M, (1989). The contribution of general features           Pollick, F. E., Paterson, H. M., Bruderlin, A., Sanford, A. J.,
  of body movement to the attribution of emotions. Journal             (2001). Perceiving affect from arm movement. Cognition,
  of Nonverbal Behavior, 13, 4, 247-268.                               82, B51–B61.
Giese, M. A. & Poggio, T. (2003). Neural Mechanisms for             Yamashita, O., Sato, MA., Yoshioka, T., Tong F., Kamitani
  the recognition of biological movements, Nature Reviews              Y. (2008). Sparse estimation automatically selects voxels
  Neuroscience, 4, 179-192.                                            relevant for the decoding of fMRI activity patterns.
Hobson, R. P. & Lee, A. (1999). Imitation and Identification           Neuroimage. 42, 4, 1414-29.
  in Autism, Journal of Child Psychological Psychiatry, 40,
  4, 649-659.
                                                                1684

