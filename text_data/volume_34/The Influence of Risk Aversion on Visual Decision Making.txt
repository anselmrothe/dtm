UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Influence of Risk Aversion on Visual Decision Making
Permalink
https://escholarship.org/uc/item/8hv2z9cv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Yang, Ruixin
Cottrell, Garrison
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                     The Influence of Risk Aversion on Visual Decision Making
                                               Ruixin Yang (r4yang@cs.ucsd.edu)
                                         Department of Computer Science and Engineering
                                                           9500 Gilman Drive
                                                        La Jolla, CA 92093 USA
                                            Garrison W. Cottrell (gary@ucsd.edu)
                                         Department of Computer Science and Engineering
                                                           9500 Gilman Drive
                                                        La Jolla, CA 92093 USA
                            Abstract                                   fixation (Milstein & Dorris, 2007; Milstein & Dorris, 2011;
                                                                       Platt & Glimcher, 1999) or from the perspective of noisy
  The ability to decide between multiple fixation targets in
  complex visual environments is essential for our survival.           sensors, modifying the probability of a target's location
  Evolution has refined this process to be both rapid and cheap,       given the noise (Navalpakkam et al., 2010). Alternative
  allowing us to perform over 100,000 saccades a day.                  approaches that have achieved comparable accuracy to
  Previous models for visual decision making have focused              expected value strategies include a study on rhesus monkeys
  on maximizing reward magnitude or expected value                     by Milstein and Dorris (2011), where they show reward
  (EV = probability of reward × magnitude of reward).                  magnitude alone may explain saccadic preparation and
  However, such methods fail to incorporate utility, or
  happiness derived from reward, optimizing strictly on
                                                                       reaction time data.
  nominal reward values. We propose an alternative model for              Despite the success of expected value models, there are
  visual decision making, maximizing utility as opposed to             many real life examples where behavior does not maximize
  value under the assumption of a decreasing marginal utility          expected value. For example, for many types of large
  curve. To test our model, we asked 10 UCSD graduate                  investment (e.g. automobile, home, healthcare), there is a set
  students to participate in an eyetracking experiment where           of insurance policies to reduce risk. Insurance companies
  they choose between different fixation targets presented on a        sustain themselves by making a small profit while reducing
  brief display. The reward for each target was generated from
  fixed, predetermined distributions with different variance that      risk for consuming individuals. If it were the case that
  was initially unknown to the subjects. The subjects were             every individual viewed reward maximization as
  asked to maximize their reward for each test session within          maximizing their expected value, these institutions would
  the experiment. Comparing our results with expected value            no longer be profitable and would cease to exist. However,
  and reward optimizing hedge algorithms, we show that utility-        there are many instances where individuals are willing to
  based models more accurately reflect human behavior in               disproportionally sacrifice some of their assets to reduce the
  visual decision making tasks.
                                                                       probability of an extremely undesirable outcome. As a
  Keywords: Visual decision making; risk aversion; utility             result, insurance is often viewed as mutually beneficial and
  theory; reward.                                                      is encouraged in many situations. Bernoulli (1738/1954)
                                                                       provided the first examples of deviation from expected
                        Introduction                                   value behavior, stating that decisions should be based on an
Target selection is a complex optimization task that the               individual’s current wealth, with less wealthy individuals
human visual system must complete thousands of times per               being more averse to risky decisions. Brocas and Carrillo
day. Assuming a probabilistic, stationary distribution for             (2009) presented a simple illustration where two perfectly
reward, the problem is directly reducible to the multi-armed           rational individuals may come to opposite conclusions in
bandit problem (Lai & Robbins, 1985; Freund & Schapire,                situations of uncertainty due to differences in utility
1997; Auer et al., 2003; Chaudhuri, Freund & Hsu, 2009).               preferences. The goal of our experiment is explore the
Despite the complexity of the problem, an efficient, low-              concept of utility maximization and risk aversion in the
cost algorithm is necessary to permit rapid saccades to be             context of visual decision making, where decisions are made
made in the noisy, low-resolution perceptual environment.              in quick succession, without much time for the conscious
Previous attempts to model visual decision making involve              evaluation of value.
a probabilistic, reward magnitude framework where the                     We provide an alternative model for visual decision
probability of fixation is weighted based on the expected              making that attempts to maximize utility, or happiness
value (EV), defined as the probability of reward multiplied            derived from reward, rather than expected value. In the
by the magnitude of reward (Milstein & Dorris, 2007;                   following sections, we will formally define our model, as
Navalpakkam et al., 2010; Platt & Glimcher, 1999).                     well as provide a mathematical justification for why
Depending on the approach, the definition of the probability           expected value fails to account for behavior with respect to
of reward can be taken either from the traditional economic            most types of reward. We also compare the predictions of
context as the probability of obtaining a reward upon target           our model with those made by the expected value model and
                                                                   2564

two well-known machine learning algorithms. Our results
show that a risk averse, utility maximization model
performs significantly better than the other models at
describing human eye movement behavior under all
experimental conditions.
                     Model Description
Assume that there are       targets: , where           .
   - Let       be the probability of encountering target     at
   location .
   - Let be the value for fixating on target .
   The expected value for fixating at location l can be
calculated as follows:
                                                                      Figure 2: A property of the decreasing marginal utility
                                                                      curve is that individuals should demonstrate risk averse
   However, value alone is often a poor measurement for               behavior in their decision making. Despite having
reward (Bernoulli, 1954; von Neumann & Morgenstern,                   identical expected values, a fair chance between       and
1953). This is due to the fact that people tend to have a                 yields , which is less than the utility provided by
decreasing marginal utility for most types of reward.                                , which yields     . Note that the choice
                                                                      between      and     has greater variance than the single
                                                                      point,               .
                                                                    reward to one which provides a fair gamble between
                                                                    two outcomes that preserves expected value. This result
                                                                    generalizes: individuals with a decreasing marginal utility
                                                                    for reward should always prefer the choice with lower
                                                                    variance, given options of equal expected value.
                                                                      Replacing value         with utility       , we obtain the
                                                                    following equation for expected utility:
                                                                      The goal of the subject is to find the location in the scene
                                                                    that contains the target which maximizes expected utility.
                                                                    Therefore, the objective function for maximizing utility is:
   Figure 1: Individuals tend to have a decreasing
   marginal utility for most types of reward due to
   priorities in reward allocation (see text). Note that for
   decreasing marginal utility, the utility derived from              To model           , we note that it is monotonically
   doubling the value of reward is less than twice the              increasing, but has a decreasing slope leading to diminishing
   utility of the original value:                                   returns with increased reward. For our model, we represent
                                                                    the curve using a natural logarithm because of its simplicity
   Intuitively, decreasing marginal utility arises from how         and because it shares the same properties as the utility
consumption of reward is allocated. The first units of a            curve. It is important to note that no two individuals hold
reward such as money tend to be spent towards essential             the same utility function for reward, and the natural
necessities, while later units tend to be used on luxury            logarithm reflects a hypothetical utility function of the
goods. Similar arguments could be made for other rewards            average individual.
such as food, shelter, and material goods. Figure 1 depicts a         The new objective function thus becomes:
standard decreasing marginal utility curve.
   One implication of holding a decreasing marginal utility
curve is that the risk averse decision will frequently
maximize utility. For example, Figure 2 shows that under            where is a constant value reflecting the magnitude of risk
the decreasing marginal utility assumption, an individual           aversion. In our experiment,         is a value that must be
will always prefer an action that generates a guaranteed            learned by the subject for each experimental phase.
                                                                2565

Based on feedback, in each trial we approximate                        Targets in the experiment appeared at 7 degrees
as a weighted average across past observed rewards,                 eccentricity from the central fixation point, and were
                                            . Similarly,     is     horizontally aligned. The target stimuli were 1.8 degree in
approximated by the weighted average of the observed                height and was each encompassed by a 3.6 × 3.6 degrees
rewards sequence (                ). Since the targets in our       square border. The location where each target appeared was
experiment are visually dissimilar,         becomes close to 0      randomly generated from trial to trial. Subjects viewed the
or 1 depending on the target. This allows us to approximate         display on a 19-inch cathode ray tube (CRT) monitor at a
               and simplifies our calculation.                      distance of 30 inches from the screen.
                                                                       The experiment consists of an initial training phase and
                  Experimental Methods                              three test phases of 50 trials each. In the training phase, the
                                                                    rewards for the two targets were drawn from discrete
Subjects                                                            uniform distributions, U[0, 50] and U[50, 100]. Subjects
                                                                    were required to learn which target yielded the higher
Ten naive subjects participated in the experiment after             reward and fixate on that target for at least 75 percent of the
providing informed consent. All subjects were right-handed          training trials before being allowed to proceed to the test
graduate students from the University of California, San            phase. This was done to ensure the subject was accustomed
Diego Computer Science and Engineering department.                  to making quick and accurate saccades while wearing the
                                                                    eyetracking device. Subsequently, each test phase consisted
Experimental Procedure                                              of two targets of equal mean, but different variance.
   Figure 3: The basic experimental setup. At start of                 Figure 4: The distribution of target reward for each test.
   every trial, subjects were asked to hold a central                  Of the two targets, the lower variance target (4A) was
   fixation for 250 ms before being presented the target               kept constant for all three tests, while the distribution
   display. Once the targets are presented, the subject                of the higher variance targets (4B), (4C), and (4D)
   must make a saccade to one of the targets within                    changed for Tests 1, 2, and 3 respectively.
   500 ms, when feedback is displayed. If no decision is
   made within the 500 ms, the subject receives no reward              The reward for each target was generated from the
   for the trial and is notified that they must make their          distributions described in Figure 4. For each test, the higher
   decision faster. Subjects are permitted to spend as long         variance target’s distribution was adjusted, while the lower
   as they wish on the feedback screen prior to starting the        variance distribution was kept constant across all of the
   next trial to make adjustments to their strategy.                tests. Subjects did not know the identity or probability
                                                                    distribution of the target at the beginning of each trial or test
We model our experiment using similar parameters to those           phase in the experiment. They were instructed to learn the
used in Navalpakkam et al. (2010). At the beginning of              distributions and maximize their reward for each test phase.
every trial, subjects were asked to indicate they were ready           We used an Eyelink 1000 eyetracker from SR Research to
by fixating at a center fixation point and pressing the “enter”     record the subjects’ eye movements. At the beginning of
key on the keyboard. Each trial begins with a central               each test, the eyetracker was recalibrated using a nine-point
fixation ‘+’ presented for 250 milliseconds. Subjects               calibration across the edge and center of the display.
indicate their choice by saccading to one of the targets.
Then, subjects were presented with an image containing two          Modeling Procedure
targets labeled ‘A’ and ‘B’ for 500 milliseconds. Figure 3          Performance for each model was measured by the
provides an illustrated description of each trial.                  percentage of trials in which the model matches the human
                                                                2566

fixation decision. For each trial, the utility-based risk averse         important to take note of one subtle difference in the
model picked the location that maximized the objective                   objective function of the multi-armed bandit problem with
function described in Equation [4], while the expected                   our problem. The objective function of the multi-armed
value model maximized                                  . We chose the    bandit minimizes regret (defined as the difference in
constant from Equation [4] by binary search across the                   reward between the ideal and the chosen action) as
[0, 10] interval, finding the value of that best predicted the           opposed to maximizing accumulated reward. To address
human subject data. However, the results were not sensitive              this, we linearly transformed the reward obtained to range
to the exact choice of .                                                 from [0, 1] and compute the loss of each action as the
   Every model prediction was compared with the decision                 difference, 1 - reward. In our Hedge implementation, we
made by the subject. Updates to both models (i.e. running                tested the entire range of temperature values [0, 1] in
averages to              and          ), were done based on the          increments of 0.05. We find the best temperature setting
experimental feedback provided to the subjects for each                  to be            , and use it for our analysis. For Normal-
individual trial, as if the model had made the same choice as            Hedge, the algorithm is self-adapting around a variable
the subject. In addition to the greedy algorithms, where the             constraint (note this variable is unrelated to the variable
model decision is always the one that maximized the                        from Equation [4]). We solve for using line search as
objective function, we compared the performance of three                 recommended by the authors. A detailed description of
policies that include exploration of less-valued alternatives:           the algorithm as well as the proof on performance bounds
ε-greedy, decreasing ε, and softmax (Sutton & Barto, 1998).              may be found for Hedge in Freund & Schapire (1997) and
We tested epsilon and softmax temperature values of                      Normal-Hedge in Chaudhuri, Freund & Hsu (2009).
0.05, 0.10, and 0.20. The exploration algorithms may be                     In all our models, we excluded two subjects from our
summarized subsequently as follows:                                      experiment as their data lay beyond two standard
                                                                         deviations from the mean number of saccades to the lower
   ε-greedy:                                                             variance target. Note that we did not purposely remove
   Initialize ε
                                                                         risk seekers as this is equivalent to removing data with
   For every trial                                                       respect to the higher variance target due to the fact that
      Generate random number r [0, 1]                                    there are two targets. Of these, one of the subjects was
      If r > ε                                                           removed because he systematically fixated only at the
          Take action maximizing the objective function                  target that appeared on the left side of the display,
      Else
                                                                         regardless of the identity of the target.
          Randomly generate action from uniform distribution
                                                                            To address potential location bias concerns in making
                                                                         saccades, we recruited only right-handed subjects for our
   Decreasing ε:                                                         study. In addition, we tested our models with and without
   Initialize ε, dec_rate = ε / (num_trials – 1)                         two location-based prior probabilities obtained from
   For every trial                                                       subject responses. The priors were the probability of
      Generate random number r [0, 1]                                    fixating at each potential target location, and the
      If r > ε
          Take action maximizing the objective function
                                                                         probability of returning, given a previous saccade to the
      Else                                                               same location on the previous trial. In all of our models,
          Randomly generate action from uniform distribution             there was no significant change in performance when we
      Update ε = ε – dec_rate                                            incorporated the priors under a Bayesian setting. For this
                                                                         and all model comparisons used in this paper, we used a
   Softmax:                                                              paired t-test to compare the performance between models.
   Initialize τ
   For every trial                                                                                  Results
      Generate action i based on probability density:
                                                                         Behavioral Data
                                                                            The results of the human experiment are presented in
                                                                         Figure 5. For each test phase, we maintained a record
      where         is the running average of reward for choosing i.     of the number of saccade decisions to each target.
                                                                         The reward distribution for higher variance target in
   Aside from comparing prediction performance against                   Tests 1-3 shared the same mean, but differed in
expected value, we also compared our results against two                 variance as shown in Figure 4 (B-D) respectively, so
well-known machine learning algorithms, Hedge (Freund                    the utility for these choices will increase. The lower
& Schapire, 1997) and Normal-Hedge, (Chaudhuri,                          variance target maintained the same distribution for all
Freund & Hsu, 2009) from the multi-arm bandit problem                    three tests. In Test 3, the subjects showed significantly
literature. Both algorithms are designed to maximize                     risk averse behavior (p = 0.0321), choosing the lower
reward, given a single parameter value ( for Hedge                       variance target 54.8 percent of the time. As the difference
and         for Normal-Hedge). Before we continue, it is                 in variance between the targets decreased, subjects
                                                                     2567

                                                                     Figure 6: A comparison of the model fits between the
                                                                     greedy expected value and greedy utility-based risk
                                                                     averse (RA) strategies in predicting human data. In all
   Figure 5: The results from the human experiment. The              three test conditions, the utility-based risk averse
   gray line represents indifference between the two                 strategy significantly outperformed the expected value
   targets. The reward distribution for the higher variance          (EV) strategy for T = 100 simulations (p < 0.001). The
   target in Tests 1-3 shared the same mean, but differed            gray dotted line represents chance performance, while
   in variance as shown in Figure 4 (B-D) respectively.              the red dotted line represents fit obtained by always
   Of the three tests, subjects showed significant risk              choosing the lower variance target (an omniscient
   averse behavior in Test 3 (the test with greatest                 model). Given a limited history of reward, the subject
   difference in variance between the two targets), where            may choose the higher variance target as a result of a
   they chose the lower variance target 54.8% of the time.           greedy action. Performance above the red dotted line
                                                                     suggests that the algorithm was fairly accurate its
became increasingly indifferent between the two targets.             prediction of when the subject chose to take such
Subjects in Test 2 chose the lower variance target 54.2              greedy action as opposed to exploring the other option.
percent of the time (p = 0.0861), while subjects in Test 1
chose the lower variance target 50.8 percent of the time          exploration, the greedy version of the algorithm will
(p = 0.621).                                                      significantly outperform their exploration counterpart.
                                                                     Figure 6 compares the utility-based risk averse strategy
Choosing a Value for                                              with the expected value strategy in predicting human
Recall from Equation [4] in the model description where a         behavior. The results show that although both models
constant, , was included to allow for fine-tuning of the          perform significantly above chance, maximizing across
magnitude of risk-averse preferences. One interesting,            utility significantly outperforms value maximization
and perhaps surprising result is that most reasonable             (p < 0.001) for all three test conditions. The red dotted
settings of     outperform the expected value model in            line provides a benchmark for how much performance
predicting human behavior. For this reason, we simply             may be obtained from a strategy defined by choosing only
chose a local maximum using a binary search across                the lower variance target.         Note that this is an
positive values of . Exceptions to this include                   overprediction, since at the beginning of each test phase,
(when the strategy reduces to random) and extremely               the subject does not know which of the two targets holds
large values of (when most rewards share approximately            lower variance (or even the value of their reward).
the same value).
                                                                  Comparison with Hedge
Comparison with Expected Value                                    We compare the fit of the utility-based risk averse
We simulated the expected value and utility-based risk            strategy with two well-known algorithms for solving the
averse strategies for 100 simulations ( = 2.48) using             multi-armed bandit problem from machine learning. We
greedy, ε-greedy, decreasing ε, and softmax exploration           choose to implement hedge algorithms over an alternate
functions (Sutton & Barto, 1998). Our results show that           strategy, Exp4 (Auer et al., 2003) due to its superior
all non-greedy algorithms perform significantly worse             performance under conditions where the reward
than their greedy counterparts (p < 0.001). Simple                distribution is fixed (the reward probability distributions
exploration strategies yield poor performance because             are fixed for our experiment as shown in Figure 4). We
although they are capable of accurately capturing the             test twenty values for the temperature value ( ) in Hedge
probability of exploration, they fail at correctly                and present the result for the optimal setting,
predicting the trials on which they occur. As a result,           For Normal-Hedge, we find the constraint, , via line
since the probability of performing a reward maximizing           search as recommended by the authors. Figure 7 shows a
action for any given trial is greater than the probability of     summary of our results. In all three test conditions, the
                                                              2568

utility-based risk averse strategy significantly outperforms     function. However, despite these limitations, we believe
hedge algorithms under their optimal settings (p < 0.001).       that the current work presents a starting point for
                                                                 analyzing visual decision making under uncertainty.
                                                                                   Acknowledgements
                                                                 We would like to thank the members of the GURU
                                                                 research lab, and the Perceptual Expertise Network for
                                                                 comments and feedback on this work. The work was
                                                                 supported in part by NSF Grant #SBE0542013.
                                                                                          References
                                                                 Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R.
                                                                   (2003). The non-stochastic multi-armed bandit
                                                                   problem. SIAM Journal on Computing, 32, 48-77.
                                                                 Bernoulli, D. (1954). Exposition of a new theory on the
                                                                   measurement of risk. Econometrica, 22, 23-36.
                                                                 Brocas, I., & Carrillo, J. (2009). Information acquisition and
  Figure 7: The results comparing the performance                  choice under uncertainty. Journal of Economics and
  between the utility-based risk averse strategy with              Management Strategy, 18, 423-455.
  Hedge (H) and Normal-Hedge (NH) algorithms on                  Chaudhuri, K., Freund, Y., & Hsu, D. (2009). A
  predicting human data. In all three test conditions, the         parameter-free hedging algorithm. Advances in Neural
  utility-based risk averse strategy significantly                 Information Processing Systems, 22, 297-305.
  outperformed the Hedge and Normal-Hedge strategies
                                                                 Freund, Y., & Schapire, R. (1997). A decision-theoretic
  for T = 100 simulations (p < 0.001). The gray dotted
                                                                   generalization of on-line learning and an application to
  line represents chance performance, while the red
                                                                   boosting. Journal of Computer and System Sciences,
  dotted line represents performance obtained from only
                                                                   55, 119-139.
  choosing the lower variance target.
                                                                 Kahneman, D. & Tversky A. (1979). Prospect theory: an
                                                                   analysis of decision under risk. Econometrica, 47,
             Discussion and Future Work
                                                                   263-291.
Our work shows that by constructing models from a                Kusev, P., van Schaik, P., Ayton, P., Dent, J., & Chater, N.
utility maximization standpoint, we are able to make               (2009). Exaggerated risk: prospect theory and probability
predictions regarding human behavior that would                    weighting in risky choice. JEP:LMC, 35: 1487-1505.
otherwise be impossible in situations involving risk.
                                                                 Lai, T. L., & Robbins, H. (1985). Asymptotically efficient
Previous models in saccadic prediction involved a
                                                                   adaptive allocation rules. Advances in Applied
direct integration of the probability and magnitude
                                                                   Mathematics, 6, 4-22.
of reward, ignoring risk derived from variance in
                                                                 Milstein, D., & Dorris, M. (2007). The influence of
reward distributions. In this paper, we present evidence
suggesting the importance of such parameters when                  expected value on saccadic preparation. Journal of
modeling visual decision making. Our findings show that            Neuroscience, 27, 4810-4818.
under conditions of uncertainty, the human visual                Milstein, D., & Dorris, M. (2011). The relationship
system takes a risk averse approach, taking account of             between saccadic choice and reaction times
the variance of the reward distribution in addition to             with manipulations of target value. Frontiers in
the mean.                                                          Neuroscience, 5.
  However, the current utility-based risk averse model           Navalpakkam, V., Koch, C., Rangel, A., & Perona, P.
does not address all questions that arise with the                 (2010). Optimal reward harvesting in complex
incorporation of risk. In particular, the work does not            perceptual environments. PNAS, 107: 5232-5237.
address issues raised from prospect theory (Kahneman &           Platt, M., & Glimcher, P. (1999). Neural correlations of
Tversky, 1979; Tversky & Kahneman, 1992; Kusev et al.,             decision variables in parietal cortex. Nature, 400:
2009). For example, in the context of the experiment,              233-238.
there is no loss associated with viewing any target, and         Sutton, R., & Barto, A. (1998). Reinforcement learning: An
thus the asymmetry between loss and gain perception                introduction. MIT Press, Cambridge, MA.
could not be modeled. Likewise, there are many situations        Tversky, A., & Kahneman, D. (1992). Advances in prospect
where risk seeking behavior is exhibited and is the utility        theory: cumulative representation of uncertainty. Journal
optimizing choice. While both conditions may arise in              of risk and uncertainty, 5, 297-323.
vision, prospect theory could not be modeled under the           von Neumann, J., & Morgenstern, O. (1953). Theory of
current experimental framework, and risk seeking                   games and economic behavior. Princeton University
behavior would require a change in the shape of the utility        Press, Princeton, NJ.
                                                             2569

