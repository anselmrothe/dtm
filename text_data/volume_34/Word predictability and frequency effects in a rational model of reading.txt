UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Word predictability and frequency effects in a rational model of reading
Permalink
https://escholarship.org/uc/item/2216v9b8
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Bicknell, Klinton
Levy, Roger
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                     University of California

          Word predictability and frequency effects in a rational model of reading
                        Klinton Bicknell1 (kbicknell@ucsd.edu) & Roger Levy2 (rlevy@ucsd.edu)
                                    1 Department    of Psychology, UC San Diego, La Jolla, CA, USA
                                    2 Department    of Linguistics, UC San Diego, La Jolla, CA, USA
                              Abstract                                   Pollatsek, 2003) and SWIFT (Engbert, Nuthmann, Richter,
   This paper presents results from the first rational model of eye      & Kliegl, 2005) in two ways. First, while those models se-
   movement control in reading to make predictions for the full          lect parameters to maximize the fit to human data, the current
   range of the eye movement record. The model identifies the            work selects parameters to optimize the efficiency of read-
   text through Bayesian inference and makes eye movement de-
   cisions to maximize the efficiency of text identification, go-        ing, here characterized as rapid and accurate identification of
   ing beyond leading approaches which select model parame-              the contents of the text. To the extent that the model behavior
   ters to maximize the fit to human data. Two simulations with          reproduces effects seen in human data, then, it enables un-
   the model demonstrate that it can produce effects of word pre-
   dictability and frequency on eye movements in reading similar         derstanding those effects as resulting from the properties of
   to those produced by humans, providing evidence that many             efficient solutions to the task. Second, the current work in-
   properties of human reading behavior may be understood as             cludes a model of the process of identification from visual
   following from the nature of efficient text identification.
                                                                         input, and in so doing derives effects of linguistic variables
   Keywords: eye movements; reading; rational analysis; com-
   putational modeling                                                   (such as word frequency and predictability) as resulting from
                                                                         efficient identification, while models such as E-Z Reader and
                          Introduction                                   SWIFT directly specify the effects of linguistic variables on
During reading, comprehenders must decide when and where                 eye movement behavior through functions whose form is
to move their eyes 3–4 times every second. Over the past                 stipulated exogenously to the model. Modeling identification
decades, it has been demonstrated that comprehenders make                from visual input should allow for the model to be used to
these rapid, fine-grained decisions by combining information             understand a range of effects that are known to influence
from a range of sources including visual input, the motor sys-           eye movements but which leading approaches cannot capture,
tem, and linguistic knowledge (for reviews see Rayner, 1998,             such as information density within words (Hyönä, Niemi, &
2009), making reading one of the most complex learned tasks              Underwood, 1989), word misidentification (Slattery, 2009;
that humans face every day. Gaining a better understanding               Levy, Bicknell, Slattery, & Rayner, 2009), and visual neigh-
of this process promises to yield insights about how readers             borhoods (Pollatsek, Perea, & Binder, 1999). The model also
deploy linguistic knowledge for real-time comprehension as               goes beyond the only previous rational model of eye move-
well as about how humans learn to perform complex tasks                  ment control in reading, Mr. Chips (Legge, Hooven, Klitz,
more generally. In this paper, we present the first rational             Mansfield, & Tjan, 2002), in making predictions about not
model of eye movement control in reading that makes pre-                 only the location of fixations but also their duration, which
dictions for the full range of the eye movement record. We               is important to gaining a full understanding of a range of ef-
model readers as performing Bayesian inference on the iden-              fects on eye movements in reading, especially the effects of
tity of the text, combining their probabilistic language knowl-          linguistic variables.
edge (the prior) with noisy perceptual input about the text (the            In the following section, we describe our rational frame-
likelihood) to form and repeatedly update a posterior distribu-          work for reading and the details of our model of eye move-
tion over the possible text identities. The model uses a param-          ment control in reading. We then focus the remainder of the
eterized behavior policy for determining when and where to               paper on using the model to understand the effects on eye
move the eyes, which is sensitive to the posterior distribu-             movements in reading of word frequency and predictability,
tion over the text, and with parameters selected to optimize             two of the most reliable linguistic effects in the eye movement
identification efficiency. We evaluate the model by examin-              record. We first use the model qualitatively to provide expla-
ing the effects it produces for two linguistic variables: word           nations for why the effects of these variables seen empirically
frequency and predictability. We present the results of two              should result from efficient reading behavior, and then present
simulations showing that the model produces effects of these             the quantitative results of two simulations demonstrating that
variables similar to those of humans, across four different eye          these effects are evident in the model’s behavior.
movement measures reflecting both the locations and dura-
tions of fixations. The success of the model in deriving these                       Reading as Bayesian inference
effects from principles of probabilistic inference and rational          In the proposed framework, we model the goal of reading as
action suggests that many aspects of human reading behav-                efficient text identification. While it is clear that this is not all
ior may be profitably understood as properties of the set of             that readers do – inferring the underlying structural relation-
efficient solutions to the problem of reading.                           ships among words in a sentence and discourse relationships
   This model goes beyond leading models of eye movement                 between sentences that determine text meaning is a funda-
control in reading such as E-Z Reader (Reichle, Rayner, &                mental part of most reading – all reader goals necessarily in-
                                                                     126

volve identification of at least part of the text, so we take text      acuity, then, means a lower sample variance, yielding higher
identification to be a reasonable first approximation. There            quality visual input. We use the visual acuity function from
are two sources of information relevant to this goal: visual in-        Engbert et al. (2005), in which λ decreases exponentially
put and language knowledge, which the model combines via                with retinal eccentricity and decreases asymmetrically, falling
Bayesian inference. Specifically, it begins with a prior dis-           off more slowly to the right than the left.3 In order to scale the
tribution over possible identities of the text given by its lan-        quality of visual information, we multiply each acuity λ by
guage model, and combines this with noisy visual input about            the overall visual input quality Λ (values given in the simula-
the text at the eyes’ position (giving the likelihood term) to          tions below.) Visual input about non-alphabetic characters is
form a posterior distribution over the identity of the text tak-        veridical knowledge of their identity. Visual input is limited to
ing into account both the language model and the visual input           the 19 character positions with the highest acuity (eccentric-
obtained thus far. On the basis of the posterior distribution,          ities between -7 and 12), roughly corresponding to estimates
the model then decides whether or not to move its eyes (and             that readers of English obtain useful information from about
if so where to move them to) and the cycle repeats.                     19 characters, and more from the right of fixation than the left
   An implemented model in this framework must formalize                (Rayner, 1998). Note that in the model each letter is equally
a number of pieces of the reading problem, including the pos-           confusable with all others, following Norris (2006, 2009), but
sible actions available to the reader and their consequences,           ignoring work on letter confusability (which could be added
the nature of visual input, the nature of language knowledge,           to future model revisions; Engel, Dougherty, & Brian Jones,
a means of combining visual input with prior expectations               1973; Geyer, 1977).
about the form and structure of the text, and a behavior pol-
icy determining how the model will choose actions on the                Language knowledge
basis of its posterior distribution over the identity of the text.      In general, any generative model of linguistic knowledge that
In the remainder of this section, we present the details of our         assigns probabilities to text can be used as the prior distri-
formalizations of these pieces.1                                        bution on the identity of the text. For the simulations in this
                                                                        paper, we use very simple probabilistic models of language
Formal problem of reading: Actions                                      knowledge: word n-gram models (Jurafsky & Martin, 2009).
We assume that on each of a series of discrete timesteps, the           These models encode the probability of each word condi-
model obtains visual input around the current location of the           tional on the n − 1 previous words. While this is obviously
eyes, and then chooses between three actions: (a) continuing            a crude representation of the rich knowledge of language that
to fixate the currently fixated position, (b) initiating a sac-         human readers have, it serves here to illustrate the qualitative
cade to a new position, or (c) stopping reading. If the model           effects of using linguistic context in reading.
chooses option (a), time simply advances, and if it chooses             Inference about text identity
option (c), then reading immediately ends. If a saccade is ini-
                                                                        Given both visual input and language knowledge, the model
tiated (b), there is a lag of two timesteps, representing time
                                                                        makes inferences about the identity of the text w via standard
required to plan a saccade, during which the model again ob-
                                                                        Bayesian inference, where the prior is given by the probabil-
tains visual input around the current position, and then the
                                                                        ity of generating text identity w from the language model and
eyes move toward the intended target. Because of motor error,
                                                                        the likelihood is the probability of generating the visual input
the actual landing position of the eyes is normally distributed
                                                                        I from text with identity w under the visual input model:
around the intended target with standard deviation given by
a linear function of the intended distance, with parameters                                    p(w|I) ∝ p(w)p(I|w).
taken from Engbert et al. (2005).2
                                                                        Behavior policy
Noisy visual input
                                                                        The model uses a simple policy with two parameters, α and
The visual input obtained by a reader on a given timestep               β , to decide between actions based on the marginal probabil-
is generated from the following process, independently for              ity m of the most likely character c in each position j,
each character position. Each letter is represented as a 26-
dimensional vector, where a single element is 1 and the others                                 m( j) = max p(w j = c)
                                                                                                           c
are zeros, and visual input about a letter is a sample from a 26-
dimensional Gaussian with a mean equal to the letter’s true             where w j indicates the character in the jth position. A high
identity and a diagonal covariance matrix Σ = λ −1 I, where             value of m indicates relative confidence about the character’s
λ is the reader’s visual acuity at that position. Higher visual         identity, and a low value relative uncertainty.
                                                                           Given the values of this statistic m, the model decides be-
    1 See Bicknell and Levy (2010b) for further computational de-
                                                                        tween four possible actions, as illustrated in Figure 1. If the
tails.
    2 In the terminology of the literature, the model has only ran-         3 While  we call refer to it here as visual acuity, it is clear from
dom motor error (variance), not systematic error (bias). Follow-        the asymmetric nature of this function that it also has an attentional
ing Engbert and Krügel (2010), systematic error may arise from          component. For now, however, we make the simplifying assumption
Bayesian estimation of the best saccade distance.                       that it is unchanging over time.
                                                                    127

      (a)   m = [.6, .7, .6, .4, .3, .6]:   Keep fixating (3)                  ter within that word. Thus, the initial probability of the true
      (b)   m = [.6, .4, .9, .4, .3, .6]:   Move back (to 2)                   identity of the fixated character will start at or above the initial
      (c)   m = [.6, .7, .9, .4, .3, .6]:   Move forward (to 6)                probability π of the true word, and – when the word is iden-
      (d)   m = [.6, .7, .9, .8, .7, .7]:   Stop reading                       tified correctly – the model’s confidence about the identity of
                                                                               the fixated character is likely to reach the threshold α near
Figure 1: Values of m for a 6 character text under which a                     the same time that confidence about the identity of the fixated
model fixating position 3 would take each of its four actions,                 word reaches the threshold. As a consequence, the amount of
if α = .7 and β = .5.                                                          visual input that is needed to reach the threshold which initi-
                                                                               ates a saccade is largely a function of the distance between π
value of this statistic for the current position of the eyes is                and α. For more predictable words, π is closer to α, so less
less than the parameter α, the model chooses to continue fix-                  visual input will be needed on average to reach α, translating
ating the current position (1a). Otherwise, if the value of m( j)              into shorter and fewer fixations on the word.
is less than the parameter β for some leftward position, the                   Frequency
model initiates a saccade to the closest such position (1b). If
                                                                               The most obvious intuition for the effect of frequency in the
no such positions exist to the left, then the model initiates a
                                                                               model is parasitic on the effect of predictability: words that
saccade to n characters past the closest position to the right
                                                                               are lower frequency are less predictable on average. Thus, as
for which m( j) < α (1c).4 Finally, if no such positions exist
                                                                               with words of higher predictability, there should be on aver-
to the right, the model stops reading (1d). Intuitively, then, the
                                                                               age shorter and fewer fixations on words of high frequency.
model reads by making a rightward sweep to bring its confi-
dence in each character up to α, but pauses to move left to                                     Simulation 1: full model
reread any character whose confidence falls below β .
                                                                               We now assess the effects of word predictability and fre-
Predictability and frequency in rational reading                               quency that the model does in fact produce. We use the model
                                                                               to simulate reading of a modified version of the Schilling cor-
The general findings about the effects of word predictability                  pus (Schilling, Rayner, & Chumbley, 1998) of typical sen-
and frequency on eye movements in reading can be summa-                        tences used in reading experiments. The arguments just de-
rized relatively simply: words that are less predictable and                   scribed predict qualitatively that the model will make more
lower frequency tend to receive more and longer fixations                      and longer fixations on words of lower predictability and fre-
(Rayner, 1998, 2009). Here we describe intuitions for why                      quency. In addition, we quantitatively compare the model’s
our model should qualitatively reproduce these effects.                        frequency effects to those of human readers of the Schilling
Predictability                                                                 corpus, which have been reported by Pollatsek et al. (2006).
The basic intuition for why the model should produce effects                   Methods
of word predictability is very closely related to the reason for               Model implementation We implemented our model with
frequency effects in isolated word recognition reaction times                  weighted finite-state automata (wFSAs) using the OpenFST
given by Norris (2006, 2009). In short, the lower the prior                    library (Allauzen, Riley, Schalkwyk, Skut, & Mohri, 2007).
probability of a word, the more visual input about it is needed                While inference in the wFSA is exact, for efficiency we
to become confident in its identity. A bit more formally, this                 used Monte Carlo sampling from the wFSA to estimate the
intuition is clearest if we make the simplifying assumption                    model’s confidence m in each character position.
that prior to obtaining any visual information about a word,
the model has near-veridical knowledge of the preceding con-                   Model parameters and language model We set the overall
text. In that case, the probability of the true identity of the                visual input quality Λ to 4. The model’s language knowledge
word is given by the word’s predictability in context π. Visual                was an unsmoothed bigram model created using a vocabulary
input about the word will then (on average) increase the prob-                 set consisting of the 500 most frequent words in the British
ability of the word’s true identity under the model’s beliefs.                 National Corpus (BNC) as well as all the words in our test
Recall that under our behavior policy, the eyes will remain in                 corpus. From this vocabulary, we counted every bigram in
this position until the model’s confidence in the identity of the              the BNC for which both words were in vocabulary. Due to
character at that position exceeds the threshold α. Because in-                the intense computation required for exact inference, we then
formation is being obtained about the entire word simultane-                   trimmed this set by removing rare bigrams that occur less than
ously, the probability of the identity of the fixated character is             200 times (except that we do not trim any bigrams that occur
closely tied to the identity of the entire word. Specifically, the             in our test corpus). This left a set of about 19,000 bigrams,
model’s confidence in the identity of the word gives a lower                   from which we constructed the bigram model.
bound on the model’s confidence in the identity of a charac-                   Optimization of policy parameters We define reading ef-
    4 The role of n is to ensure that the model does not center its visual
                                                                               ficiency E to be an interpolation of speed and accuracy
field on the first uncertain character. For the present simulations, we
did not attempt to optimize this parameter, but fixed n at 3.                                         E = (1 − γ)L − γT
                                                                           128

where L is the log probability of the true identity of the text
                                                                                  First fixation duration (timesteps)
                                                                                                                        8                                                                             8
                                                                                                                                                        full model                                                                full model
under the model’s beliefs at the end of reading, T is the num-
                                                                                                                                                                          Gaze duration (timesteps)
                                                                                                                                                        without context                                                           without context
ber of timesteps before the model stopped reading, and γ                                                                7                                                                             7
gives the relative value of speed. For the present simulations,
                                                                                                                        6                                                                             6
we use γ = .05, which produces reasonably accurate read-
ing. To find optimal values of the policy parameters α and β                                                            5                                                                             5
for this definition of efficiency, we use the P EGASUS method
                                                                                                                        4                                                                             4
(Ng & Jordan, 2000) to transform this stochastic optimization
problem into a deterministic one on which we can use stan-                                                                    −6    −5    −4   −3        −2    −1    0                                      −6    −5    −4   −3    −2    −1    0
dard optimization algorithms. We then use coordinate ascent                                                                         (Log) predictability                                                          (Log) predictability
(in logit space) to find the optimal values of α and β . This
                                                                                                                        1.0                                                                           1.0
procedure resulted in optimal values α = .88 and β = .98.5                                                                            full model                                                                                  full model
                                                                                                                                                                                                                                  without context
                                                                                  Probability of skipping                                                                 Probability of refixation
                                                                                                                        0.8           without context                                                 0.8
Test corpus To ensure that results did not depend on
smoothing, we tested the model only on sentences from the                                                               0.6                                                                           0.6
Schilling corpus in which every bigram occurred in the BNC.                                                             0.4                                                                           0.4
Unfortunately, only 8 of the corpus sentences initially met
                                                                                                                        0.2                                                                           0.2
this criterion, so we made single-word changes to 25 more
(mostly proper names and rare nouns), producing a total of                                                              0.0                                                                           0.0
33 sentences for which every bigram occurred in the BNC.                                                                       −6    −5   −4       −3     −2    −1   0                                       −6    −5   −4   −3     −2    −1   0
                                                                                                                                    (Log) predictability                                                          (Log) predictability
Analysis We used the model to perform 50 stochastic simu-
lations of the reading of our modified version of the Schilling                  Figure 2: Effects of word predictability in both models on first
corpus. For each run, we calculated four standard eye move-                      fixation durations, gaze durations, the rate of skipping, and
ment measures for each word in the corpus: first fixation du-                    the rate of making a refixation, as estimated using Gaussian
ration, gaze duration (defined to be the sum of all first pass                   kernel regression with standard deviation equal to 1/8th of
fixations), skipping probability (whether or not word was di-                    the range of log-predictability values. The 95% confidence
rectly fixated), and refixation probability (the probability of                  intervals are bootstrapped from 1000 dataset replicates.
more than one first pass fixation). We then averaged each of
these four measures for each word token in the corpus, yield-
ing a single mean value for each measure for each word.                          fewer refixations for more predictable words.
   In order to compare the fixation duration measures to hu-
                                                                                 Frequency Figure 3 (red lines) shows the effects of fre-
mans, we converted the model’s timesteps into milliseconds.
                                                                                 quency (binned by rounding down, to facilitate comparison
We performed this scaling by multiplying the duration of each
                                                                                 to Pollatsek et al., 2006) on the four aggregate measures. The
fixation by a conversion factor set to be equal to the mean hu-
                                                                                 results across all four measures show a reasonable quantita-
man gaze duration divided by the mean model gaze duration
                                                                                 tive fit to the human data (blue lines). Further, comparing the
for the highest frequency bin. That is, we scaled the model
                                                                                 overall size of the effect (i.e., the difference of the highest and
predictions to exactly match the human mean for gaze dura-
                                                                                 lowest frequency bins) of the model to the human data shows
tions in the highest frequency bin.
                                                                                 a striking fit in effect direction and magnitude for all four
Results                                                                          measures. One unpredicted result here, however, is that the
                                                                                 effect of frequency on the duration measures does not appear
For each word in our modified version of the Schilling cor-                      completely monotonic.
pus, we defined its predictability to be its probability under
the bigram language model, and we defined its frequency to                       Discussion
be its overall probability in the data from which the bigram
language model was constructed.                                                  In summary, these results demonstrate that effects of pre-
                                                                                 dictability and frequency in the model’s behavior resemble
Predictability Figure 2 (red lines) shows the effect of pre-                     that of human readers in many respects. Predictability effects
dictability on the four aggregate measures. As predicted by                      on all four aggregate measures are monotonic and in the same
both the intuition given above, and in agreement empirical                       direction as predicted. Frequency effects on all four measures
human data, there are shorter fixations, more skipping, and                      are in the same direction as predicted, and the total magni-
    5 It may at first seem puzzling that α < β . However, this is a gen-         tude of the effect is quite similar to that displayed by human
eral property of optimal behavior for the model. While saccades to               readers, despite the fact that we have not made any attempt to
leave a character are initiated as soon as confidence m > α, because             fit the human data, excepting only the scaling parameter that
of the saccade execution delay, m is usually substantially higher than           converts model timesteps to milliseconds. Overall quantita-
α when the eyes leave the character. Hence, it is a reasonable strat-
egy for the threshold for regressions β to be accordingly higher. See            tive fits on all four measures showed reasonable agreement
also Bicknell and Levy (2010b) for further discussion.                           to human data, but the fixation duration measures displayed
                                                                           129

                                350
                                                                                                                                                              predictions change when it can no longer make use of lin-
                                                     ●        full model                                                      ●       full model
                                                                                                            350
                                                                                                                                                              guistic context to help recognize words.
 First fixation duration (ms)
                                                              without context                                                         without context
                                                                                Gaze duration (ms)
                                                              humans                                                                  humans
                                300
                                                                                                            300     ●                                         Methods
                                                                                                                                  ●        ●
                                250                                                                         250
                                                                                                                         ●                                    Except the following, the methods were identical to those of
                                        ●
                                                                   ●
                                                                                                                                                              Simulation 1. We replaced the bigram language model with
                                                         ●                                                                                         ●
                                200
                                               ●
                                                                                                            200                                               a unigram language model. Training was performed in the
                                                                           ●
                                                                                                                                                              same manner, except that instead of including only the most
                                       −6     −5         −4       −3       −2                                      −6    −5    −4         −3       −2         common 500 words in the BNC, we included all words that
                                      (Log) frequency (binned)                                                    (Log) frequency (binned)
                                                                                                                                                              occur at least 200 times (corresponding to a frequency of 2
                                1.0                                                                         1.0
                                                                                                                                                              per million; about 19,000 words). Finally, we increased the
                                        ●   full model                                                                        ●       full model
                                            without context                                                                           without context
                                                                                                                                                              overall visual input quality Λ from 4 to 10. Because the new
 Probability of skipping                                                        Probability of refixation
                                0.8
                                            humans
                                                                                                            0.8
                                                                                                                                      humans                  language model gives poorer information about the text, more
                                                                           ●
                                0.6                                ●
                                                                                                            0.6                                               visual input is needed to reach similar levels of confidence
                                                                                                                                                              in word identities. Increasing the overall input quality to 10
                                0.4                      ●                                                  0.4
                                               ●
                                                                                                                   ●                                          results in the new model taking a similar number of timesteps
                                                                                                                         ●
                                0.2    ●                                                                    0.2
                                                                                                                                  ●
                                                                                                                                           ●
                                                                                                                                                              to read a sentence as the previous model.
                                                                                                                                                   ●
                                0.0                                                                         0.0
                                                                                                                                                              Results and discussion
                                      −6      −5     −4           −3       −2                                     −6    −5    −4          −3       −2
                                      (Log) frequency (binned)                                                    (Log) frequency (binned)
                                                                                                                                                              Predictability Figure 2 (green lines) shows the effect of
                                                                                                                                                              predictability on the four aggregate measures for the model
Figure 3: Effects of word frequency in both models on first                                                                                                   without context. Because the model does not make use of
fixation durations, gaze durations, the rate of skipping, and                                                                                                 linguistic context in identifying words, any apparent effects
the rate of making a refixation. The 95% confidence inter-                                                                                                    of predictability must reflect effects of other variables corre-
vals are bootstrapped from 10000 dataset replicates. Mean                                                                                                     lated with predictability (e.g., frequency and length). We can
values from human readers of the Schilling corpus reported                                                                                                    then use these results as a baseline to determine the amount of
by Pollatsek et al. (2006) are shown for comparison.                                                                                                          the full model’s apparent predictability effect that was in fact
                                                                                                                                                              driven by predictability. The results across all four measures
                                                                                                                                                              show that predictability effects are smaller for this model
some non-monotonicity.                                                                                                                                        without context, indicating that the full model’s use of con-
   It is perhaps unsurprising that predictability seems to have                                                                                               text was important in producing its predictability effects.
the most consistent effect, given the large role that pre-
dictability plays in the model, and the relatively straightfor-                                                                                               Frequency Figure 3 (green lines) shows the effect of fre-
ward predictions made previously. More surprising are the                                                                                                     quency on the four aggregate measures. Across all four mea-
apparent non-monotonicities in the predictions for how the                                                                                                    sures, the size of the frequency effect in this model also shows
fixation duration measures should vary with respect to word                                                                                                   a reasonable quantitative fit to human data, although the refix-
frequency. One possibility is that these arise from our arti-                                                                                                 ation rates and first fixation durations are about twice as far
ficial removal of many low-frequency words from the lan-                                                                                                      from human data as the full model. As with the full model,
guage model, which may have meant that some of the low-                                                                                                       however, the direction and magnitude of all frequency effects
frequency words in the Schilling corpus had artificially few                                                                                                  is a close match to human data. The higher refixation rate
visual neighbors, yielding an anti-frequency effect. The next                                                                                                 and lower word skipping rate of this model relative to the full
simulation investigates this hypothesis.                                                                                                                      model likely reflect the model’s poorer language knowledge
                                                                                                                                                              (cf. Bicknell & Levy, 2010a). Finally, and most importantly,
                                      Simulation 2: Model without context                                                                                     we see that the problem of non-monotonicity is substantially
The main goal of Simulation 2 is to explore the possibility                                                                                                   reduced for first fixation durations and completely eliminated
that removing low frequency words from the model’s vocabu-                                                                                                    for gaze durations, supporting our argument that trimming the
lary (which was necessary for computational efficiency) con-                                                                                                  vocabulary may have been responsible for some of the non-
tributed to the non-monotonicities we observed in the effects                                                                                                 monotonicity in the previous simulation results.
of word frequency on fixation durations. Our strategy is to
simplify the language model, which makes the computations                                                                                                                        General discussion
faster to carry out, allowing for the use of a larger vocabu-                                                                                                 In this paper, we presented the first rational model of eye
lary. Specifically, we replace the previous bigram language                                                                                                   movement control in reading to make predictions for the en-
model, which made use of linguistic context, with a unigram                                                                                                   tirety of the reading record. We gave intuitions for why it
language model that includes only word frequency informa-                                                                                                     should produce effects of word predictability and frequency
tion and cannot make use of linguistic context. This simplified                                                                                               qualitatively similar to those produced by human readers, and
language knowledge also allows us to test how the model’s                                                                                                     presented two simulations empirically testing the effects of
                                                                                                                                                        130

these variables on model behavior. Simulation 1, using a full              Bicknell, K., & Levy, R. (2010a). Rational eye movements in read-
version of the model with parameters selected to maximize                     ing combining uncertainty about previous words with contextual
                                                                              probability. In S. Ohlsson & R. Catrambone (Eds.), Proceedings
the agent’s reading efficiency, demonstrated that the model                   of the 32nd Annual Conference of the Cognitive Science Society
yields effects of frequency and predictability that are qual-                 (pp. 1142–1147). Austin, TX: Cognitive Science Society.
itatively – and in frequency’s case, quantitatively – similar              Bicknell, K., & Levy, R. (2010b). A rational model of eye move-
to those of human readers, though the predictions for fixa-                   ment control in reading. In Proceedings of the 48th Annual Meet-
                                                                              ing of the Association for Computational Linguistics ACL (pp.
tion durations on words of intermediate frequency did not ap-                 1168–1178). Uppsala, Sweden: Association for Computational
pear completely monotonic. We hypothesized that these non-                    Linguistics.
monotonicities may have been a result of the full model’s                  Engbert, R., & Krügel, A. (2010). Readers use Bayesian estimation
small vocabulary, which had to be artificially limited for tech-              for eye movement control. Psychological Science, 21, 366–371.
                                                                           Engbert, R., Nuthmann, A., Richter, E. M., & Kliegl, R. (2005).
nical reasons. Simulation 2 tested this hypothesis using a                    SWIFT: A dynamical model of saccade generation during read-
model with simpler language knowledge but a larger vocab-                     ing. Psychological Review, 112, 777–813.
ulary, and provided some evidence that alleviating this limi-              Engel, G. R., Dougherty, W. G., & Brian Jones, G. (1973). Correla-
                                                                              tion and letter recognition. Canadian Journal of Psychology, 27,
tation helps to make the frequency effects more monotonic.                    317–326.
In addition, by demonstrating that a model that cannot make                Geyer, L. H. (1977). Recognition and confusion of the lowercase
use of predictability information shows smaller apparent pre-                 alphabet. Perception & Psychophysics, 22, 487–490.
dictability effects, Simulation 2 demonstrated that the pre-               Hyönä, J., Niemi, P., & Underwood, G. (1989). Reading long words
                                                                              embedded in sentences: Informativeness of word halves affects
dictability effects obtained for the full model were not likely               eye movements. Journal of Experimental Psychology: Human
to have been merely an artifact of the correlation between                    Perception and Performance, 15, 142–152.
word predictability and other variables such as word length.               Jurafsky, D., & Martin, J. H. (2009). Speech and language pro-
                                                                              cessing: An introduction to natural language processing, com-
    Taken together, these results demonstrate that the rational               putational linguistics, and speech recognition (2nd ed.). Upper
reading framework can produce reasonable effects of word                      Saddle River, NJ: Prentice Hall.
predictability and frequency on four aggregate measures of                 Legge, G. E., Hooven, T. A., Klitz, T. S., Mansfield, J. S., & Tjan,
eye movement behavior: first fixation durations, gaze dura-                   B. S. (2002). Mr. Chips 2002: new insights from an ideal-
                                                                              observer model of reading. Vision Research, 42, 2219–2234.
tions, skip rates, and refixation rates. While the quantitative            Levy, R., Bicknell, K., Slattery, T., & Rayner, K. (2009). Eye move-
fit to human data is not perfect, the fact that it is such a good             ment evidence that readers maintain and act on uncertainty about
match is striking given that we fit no free parameters to hu-                 past linguistic input. Proceedings of the National Academy of Sci-
                                                                              ences of the United States of America, 106, 21086–21090. (Cor-
man data, except the conversion of timesteps to milliseconds                  rection in: Proceedings of the National Academy of Sciences of
– a parameter that all timestep-based models must include.                    the United States of America, 107, 5260)
(In future work, determining the model’s best possible fit to              Ng, A. Y., & Jordan, M. (2000). PEGASUS: A policy search method
human data will require tuning the only two other truly free                  for large MDPs and POMDPs. In Uncertainty in Artificial Intel-
                                                                              ligence, Proceedings of the Sixteenth Conference (pp. 406–415).
parameters of our model – the agent’s value of speed relative              Norris, D. (2006). The Bayesian reader: Explaining word recog-
to accuracy γ and the overall visual input quality Λ.) Instead                nition as an optimal Bayesian decision process. Psychological
of being selected to maximize the model’s fit to human data,                  Review, 113, 327–357.
the policy parameters α and β of our model were set to val-                Norris, D. (2009). Putting it all together: A unified account of word
                                                                              recognition and reaction-time distributions. Psychological Re-
ues that optimized the efficiency with which the model identi-                view, 116, 207–219.
fied the text, given the agent’s particular goal function. Future          Pollatsek, A., Perea, M., & Binder, K. S. (1999). The effects of
work must be done to explore the predictions of our model                     “neighborhood size” in reading and lexical decision. Journal of
                                                                              Experimental Psychology: Human Perception and Performance,
for a wider range of eye movement phenomena observed in                       25, 1142–1158.
reading, extending our analyses of the model’s behavior both               Pollatsek, A., Reichle, E. D., & Rayner, K. (2006). Tests of the
with more dependent measures, such as character landing po-                   E-Z Reader model: Exploring the interface between cognition and
sitions within words and regressive saccades, and with more                   eye-movement control. Cognitive Psychology, 52, 1–56.
                                                                           Rayner, K. (1998). Eye movements in reading and information
independent variables, such as word length.                                   processing: 20 years of research. Psychological Bulletin, 124,
                                                                              372–422.
                       Acknowledgments                                     Rayner, K. (2009). The 35th Sir Frederick Bartlett lecture: Eye
                                                                              movements and attention in reading, scene perception, and visual
This research was supported by NIH Training Grant T32-                        search. The Quarterly Journal of Experimental Psychology, 62,
DC000041 from the Center for Research in Language at UC                       1457–1506.
                                                                           Reichle, E. D., Rayner, K., & Pollatsek, A. (2003). The E-Z Reader
San Diego to K. B. and by NSF grant 0953870 and NIH grant                     model of eye-movement control in reading: Comparisons to other
R01-HD065829, both to R. L.                                                   models. Behavioral and Brain Sciences, 26, 445–526.
                                                                           Schilling, H. E. H., Rayner, K., & Chumbley, J. I. (1998). Com-
                             References                                       paring naming, lexical decision, and eye fixation times: Word fre-
                                                                              quency effects and individual differences. Memory & Cognition,
Allauzen, C., Riley, M., Schalkwyk, J., Skut, W., & Mohri, M.                 26, 1270–1281.
    (2007). OpenFst: A general and efficient weighted finite-state         Slattery, T. J. (2009). Word misperception, the neighbor frequency
    transducer library. In Proceedings of the Ninth International Con-        effect, and the role of sentence context: Evidence from eye move-
    ference on Implementation and Application of Automata, (CIAA              ments. Journal of Experimental Psychology: Human Perception
    2007) (Vol. 4783, p. 11-23). Springer.                                    and Performance, 35, 1969–1975.
                                                                       131

