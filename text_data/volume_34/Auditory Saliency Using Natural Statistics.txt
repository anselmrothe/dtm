UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Auditory Saliency Using Natural Statistics
Permalink
https://escholarship.org/uc/item/30k404zz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Tsuchida, Tomoki
Cottrell, Garrison
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                    Auditory Saliency Using Natural Statistics
                                             Tomoki Tsuchida (ttsuchida@ucsd.edu)
                                              Garrison W. Cottrell (gary@ucsd.edu)
                                           Department of Computer Science and Engineering
                                                   9500 Gilman Drive, Mail Code 0404
                                                      La Jolla CA 92093-0404 USA
                               Abstract                                by using spectrographic images as inputs to the model. Al-
                                                                       though this is a reasonable approach, these models fail to cap-
   In contrast to the wealth of saliency models in the vision lit-     ture several important aspects of the auditory modality. First,
   erature, there is a relative paucity of models exploring audi-
   tory saliency. In this work, we integrate the approaches of         this approach treats time as simply another dimension within
   (Kayser, Petkov, Lippert, & Logothetis, 2005) and (Zhang,           the spectrographic representation of the sound. Even though
   Tong, Marks, Shan, & Cottrell, 2008) and propose a model of         these models utilize asymmetric temporal filters, the resulting
   auditory saliency. The model combines the statistics of natural
   soundscapes and the recent past of the input signal to predict      saliency map at each time point is contaminated by informa-
   the saliency of an auditory stimulus in the frequency domain.       tion from the future. Second, spectrographic features are not
   To evaluate the model output, a simple behavioral experiment        the most realistic representations of human auditory sensa-
   was performed. Results show the auditory saliency maps cal-
   culated by the model to be in excellent accord with human           tions, since the cochlea exhibits complex nonlinear responses
   judgments of saliency.                                              to sound signals (Lyon, Katsiamis, & Drakakis, 2010). Fi-
   Keywords: attention; saliency map; audition; auditory percep-       nally, Itti et al.’s model determines the saliency values from
   tion; environmental statistics                                      the current input signal, with no contribution from the life-
                                                                       time experience of the organism. This makes it impossible
                          Introduction                                 for the model to account for potential perceptual differences
In general, attention plays a very important role in the survival      induced by differences in individual experience.
of an organism, by separating behaviorally relevant signals
from irrelevant ones. One approach to understanding how                              The Auditory Saliency Model
attention functions in the brain is to consider the “saliency          In this work, we propose the Auditory Salience Using Natural
map” over the sensory input space, which may determine sub-            statistics model (ASUN) as an extension of the SUN model.
sequent motor control targets or selectively modulate percep-          The extension involves (1) using realistic auditory features
tual contrast thresholds. The brain’s putative saliency maps           instead of visual ones, and (2) combining long-term statis-
can be thought of as interest operators that organisms use to          tics (as in SUN) with short-term, temporally local statistics.
enhance or filter sensory signals.                                     Although the SUN model has both a top-down, task-based
   Many visual saliency models have been investigated, but             component and a bottom-up, environmentally driven compo-
relatively little attention has been paid to modeling auditory         nent, here we restrict ourselves to just the bottom-up portion
saliency. However, since the fundamental necessity for per-            of SUN. SUN defines the bottom-up saliency of point x in the
ceptual modulation remains the same regardless of modal-               image at time t as:
ity, the principles of visual saliency models should apply
equally well to auditory saliency with appropriate sensory                                   sx (t) ∝ − log P(Fx = fx )            (1)
input features. Two representative visual saliency models
are the center-surround contrast model (Itti, Koch, & Niebur,             Here, f is a vector of feature values, whose probability is
2002) and the SUN (Salience Using Natural Statistics) model            computed based on prior experience. This is also known as
(Zhang et al., 2008). Itti et al.’s model is neurally-inspired,        the “self-information” of the features, and conveys that rare
with the response of many feature maps (e.g., orientation,             feature values will attract attention. In the SUN model, this
motion, color) combined to create a salience map. The                  probability is based on the lifetime experience of the organ-
SUN model uses a single feature map learned using Indepen-             ism, meaning that the organism already knows when feature
dent Components Analysis (ICA) of natural images, and the              values are common and when they are rare. Assuming the
salience at any point is based on the rarity of the feature re-        primary purpose of attention is to separate remarkable events
sponses at that point - novelty attracts attention. Here, rarity       from the humdrum, it is logical to equate the rarity of the
is based on statistics taken from natural images, so the model         event with the saliency of it. For example, a loud bang may
assumes experience is necessary to represent novelty.                  be salient not only because of its physical energy content, but
   Previous works that apply the visual saliency paradigm to           also because of its relative rarity in the soundscape. An or-
the auditory domain include the models of (Kayser et al.,              ganism living under constant noise may not find an explosion
2005) and (Kalinli & Narayanan, 2007). Both adapt the visual           to be as salient as another organism acclimated to a quieter
saliency model of (Itti et al., 2002) to the auditory domain           environment.
                                                                   1048

   For features, SUN uses ICA features learned from natu-               Feature Transformations
ral images, following Barlow’s efficient coding hypothesis              A model of auditory attention necessarily relies upon a model
(Barlow, 1961). This provides a normative and principled                of peripheral auditory processing. The simplest approach to
rationale for the model design. While ICA features are not              modeling the cochlear transduction is to use the spectrogram
completely independent, they justify the assumption that the            of the sound, as was done in (Kayser et al., 2005). More phys-
features are independent of one another, making the compu-              iologically plausible simulations of the cochlear processing
tation of the joint probability of the features at a point compu-       require the use of more sophisticated transformations, such
tationally simple. This is the goal of efficient coding: By ex-         as Meddis’ inner hair cell model (Meddis, 1986). However,
tracting independent features, the statistics of the visual world       the realism of the model comes at a computational cost, and
can be efficiently represented. Although the saliency filters           the complexity of the feature model must be balanced against
used in Kayser et al.’s model have biophysical underpinnings,           the benefit. Given these considerations, the following fea-
exact shape parameters of the filters cannot be determined              ture transformations were applied to the audio signals in the
in a principled manner. More importantly, their model does              ASUN model:
not explain why the attention filters should be the way they
are. In contrast, by using filters based on the efficient cod-         1. At the first stage, input audio signals (sampled at 16 kHz)
ing hypothesis, the SUN and ASUN models make no such                       are converted to cochleagrams by applying a 64-channel
assumptions; the basic feature transformation used (Gamma-                 Gammatone filterbank (from 200 to 8000 Hz.) Response
tone filters) reasonably approximate the filters learned by the            power of the filters are clipped to 50dB, smoothed by con-
efficient encoding of natural sounds (Lewicki, 2002), and the              volving with a Hanning window of 1 msec and downsam-
distributions of filter responses are learned from the environ-            pled to 1 kHz. This yields a 64-dimensional frequency de-
ment as well. Assuming that the attentional mechanism is                   composition of the input signal.
modulated by a lifetime of auditory experience is neurologi-
cally plausible, as evidenced by the experience-induced plas-          2. At the second stage, this representation is further di-
ticity in the auditory cortex (Jääskeläinen, Ahveninen, Bel-            vided into 20 frequency bands comprised of 7 dimensions
liveau, Raij, & Sams, 2007).                                               each (with 4 overlapping dimensions,) and time-frequency
   Here, we extend this model to quickly adapt to recent                   patches are produced using a sliding window of 8 sam-
events by utilizing the statistics of the recent past of the signal        ples (effective temporal extent of 8 msec). This yields 20
(the “local statistics”) as well as the lifetime statistics. Denot-        bands of 7 × 8 = 56-dimensional representation of 8 msec
ing the feature responses of the signal at time t as Ft , saliency         patches.
at t can be defined as the rarity in relation to the recent past
(from the input signal) as well as to the long-term past beyond        3. Finally, for each of the four sound collections (described
suitably chosen delay k:                                                   below), a separate Principal Components Analysis (PCA)
                                                                           is calculated for each of the 20 bands separately. Retaining
                                                                           85% of the variance reduces the 56 dimensions to 2 or 3
        s(t) ∝ − log P(Ft = ft | Ft−1 , ..., Ft−k , Ft−k−1 , ...)
                                    |      {z      } | {z }                for each band.
                                      recent past     long past
                                                                           This set of transformations yield a relatively low-
   In this paper, we simply define t − k as the onset of the test
                                                                        dimensional representation without sacrificing biological
stimulus. Under the simplifying assumption of independence
                                                                        plausibility. The result of these transformations at each time
between the lifetime and local statistics, this becomes
                                                                        point, ft , provides input for subsequent processing. Figure 1
                                                                        illustrates this feature transformation pipeline.
              s(t) ∝ − log P(Ft = ft |Ft−1 , ..., Ft−k )
                                                                        Density Estimation Method
                     − log P(Ft = ft |Ft−k−1 , ...)
                   = slocal (t) + sli f etime (t),                      In order to calculate the self-information described in equa-
                                                                        tion 1, the probability of feature occurrences P(F = ft ) must
where slocal (t) and sli f etime (t) are the saliency values calcu-     be estimated. Depending on the auditory experience of the
lated from the local and lifetime statistics, respectively. By          organism, this probability distribution may vary. To assess
using the local statistics at different timescales, the model           the effect of different types of lifetime auditory experiences,
can simulate various adaptation and memory effects as well.             1200 seconds worth of sound samples were randomly drawn
In particular, adaptation effects emerge as the direct conse-           from each of the following audio collections to obtain empir-
quence of dynamic information accrual, which effectively                ical distributions:
suppresses the saliency of repeated stimuli as time proceeds.
With such local adaptation effects, the model behaves simi-            1. “Environmental”: collection of environmental sounds,
larly to the Bayesian Surprise model (Baldi & Itti, 2006), but             such as glass shattering, breaking twigs and rain sounds
with asymptotic prior distributions provided by lifetime ex-               obtained from a variety of sources. This ensemble is ex-
perience.                                                                  pected to contain many short, impact-related sounds.
                                                                    1049

                                                                     0.014                                                                 0.04                                                   0.045
               Audio waveform                                        0.012
                                                                                                                                          0.035
                                                                                                                                           0.03
                                                                                                                                                                                                   0.04
                                                                                                                                                                                                  0.035
                                                                      0.01
                                                                                                                                                                                                   0.03
                                                                                                                                          0.025
                                                                     0.008
                                                                                                                                                                                                  0.025
                                                                                                                                           0.02
                                                                                                                                                                                                   0.02
                                                                     0.006
            Gammatone filterbank                                     0.004
                                                                                                                                          0.015
                                                                                                                                           0.01
                                                                                                                                                                                                  0.015
                                                                                                                                                                                                   0.01
                                                                     0.002
                                                                                                                                          0.005                                                   0.005
                                                                          0                                                                  0                                                       0
                                                                         −100   −50   0   50   100   150   200   250    300   350   400     −200   −150   −100   −50   0   50   100   150   200     −150   −100   −50   0   50   100   150
                Cochleagram                                                      (a) Dimension 1                                                   (b) Dimension 2                                         (c) Dimension 3
                                                                     Figure 2: Gaussian mixture model fits (red) against the empir-
          Split into frequency bands                                 ical distribution of feature values (blue). The mixture model
                              ...                                    is used to estimate P(Ft = ft |Ft−k−1 , ...).
   20
  8 ms
          PCA        PCA        PCA
                                                                                                                                    (a) Short and long tones
                   Features
Figure 1: Schematics for the feature transformation pipeline.
Input signals are first converted to smoothed cochleagram.
This is separated into 20 bands of 8 msec patches. The di-                                                                      (b) Gap in broadband noise
mensions of each band are reduced using PCA.
2. “Animal”: collection of animal vocalizations in tropical
   forests from (Emmons, Whitney, & Ross, 1997). Most of
   the vocalizations are relatively long and repetitious.
                                                                                                                       (c) Stationary and modulated tones
3. “Speech”: collection of spoken English sentences from the
   TIMIT corpus (Garofolo et al., 1993). This is similar to the
   animal vocalizations, but possibly with less tonal variety.
4. “Urban”: this is a collection of sounds recorded from a city
   (van den Berg, 2010), containing long segments of urban
   noises (such as vehicles and birds), with a limited amount                                                                   (d) Single and paired tones
   of vocal sounds.
                                                                     Figure 3: Spectrograms and saliency maps for simple stim-
   In the case of natural images, ICA filter responses follow        uli. Left columns are the spectrograms of the stimuli, and
the generalized Gaussian distribution (Zhang et al., 2008).          right columns are the saliency maps (top) and saliency values
However, the auditory feature responses from the sound col-          summed over frequency axis (bottom). Due to the nonlin-
lections did not resemble any parameterized distributions.           ear cochleogram transform, the y-axes of the two plots are
Consequently, a Gaussian mixture model with 10 components            not aligned. (a) Between short and long tones, the long tone
was used to fit the empirical distributions for each band from       is more salient. (b) Silence in a broadband noise is salient
each of the collections. Figure 2 shows examples of density          compared to the surrounding noise. (c) Amplitude-modulated
model fits against empirical distributions. The distributions        tones are slightly more salient than stationary tones. (d) In
from each collection represent the lifetime statistics portion       a sequence of closely spaced tones, the second tone is less
of ASUN model, and each corresponds to a model of saliency           salient.
for an organism living under the influence of that particular
auditory environment.                                                distribution every 250 msec. This will be improved in future
   The local statistics of the input signal were estimated us-       work, where we plan to apply continually varying mixture
ing the same method: at each time step t of the input signal,        models to eliminate such transitions.
the probability distribution of the input signal from 0 to t − 1
was estimated. For computational reasons, the re-estimation                                                            Qualitative Assessments
of the local statistics were computed every 250 msec. Unfor-         In (Kayser et al., 2005), the auditory saliency model repro-
tunately, this leads to a discontinuity in the local probability     duces basic properties of auditory scene perception described
                                                                  1050

  in (Cusack & Carlyon, 2003). Figure 3 shows the saliency            the left or right key to indicate which stimuli sounded “more
  maps of the ASUN model using the “Environmental” life-              interesting” (2AFC.) Each experiment block consisted of 160
  time statistics. These examples demonstrate that the model is       such trials: 75 pairings balanced with left-right reversal, plus
  capable of reproducing basic auditory salience phenomena.           10 catch trials in which a single stimulus was presented to
                                                                      one side. Each subject participated in a single block of the
                 Human Ratings of Saliency                            experiment within a single experimental session.
  In order to test the validity of the model in a more quantita-
  tive manner, a human rating experiment similar to (Kayser et        Model Predictions
  al., 2005) was performed. In this experiment, seven subjects        To obtain the model predictions, the same trial stimuli (in-
  were asked to pick the more “interesting” of two stimuli. The       cluding the preceding noise mask) were input to the models
  goal of the experiment was to obtain an empirical rating of the     to produce saliency map outputs. To reduce border effects,
  “interestingness” of various audio stimuli, which we conjec-        10% buffers were added to the beginning and end of the stim-
  ture is monotonically related to the saliency. By presenting        uli and removed after saliency map calculation. The portion
  the same set of stimuli to the saliency models, we can also         of the saliency map that corresponded to the noise mask were
  calculate which of the sounds are predicted to be salient. We       also removed from peak calculations.
  assume that the correct model of saliency will have a high             In (Kayser et al., 2005), saliency maps for each stimuli pair
  degree of correlation with the human ratings of saliency ob-        were converted to scores by comparing the peak saliency val-
  tained this way.                                                    ues. It is unclear what the best procedure is to extract a single
  Materials                                                           salience score from a two-dimensional map of salience scores
                                                                      over time. Following (Kayser et al., 2005), we also chose the
  Audio snippets were created from a royalty-free sound collec-
                                                                      peak salience over the snippet. To make predictions, the score
  tion (SoundEffectPack.com, 2011), which contains a variety
                                                                      for the left stimulus was subtracted from that of the right stim-
  of audio samples from artificial and natural scenes. In order
                                                                      ulus in each trial pair. This yielded values between −1 and 1,
  to normalize the volume across samples, each sample was di-
                                                                      which were then correlated against the actual choices subjects
  vided by the square root of the arithmetic mean of the squares
                                                                      made (−1 for the left and 1 for the right.)
  of the waveform (RMS). To create snippets used in the exper-
                                                                         Seven different candidate models were evaluated in this
  iment, each sample was divided into 1.2-second snippets, and
                                                                      experiment. (1) The chance model outputs −1 or 1 ran-
  the edges were smoothed by a Tukey window with 500 ms of
                                                                      domly. This model serves as the baseline against which to
  tapering both sides. Snippets containing less than 10% of the
                                                                      measure the chance performance of other models. (2) The
  power of a reference sinusoidal signal were removed in order
                                                                      intensity model outputs the Gammatone filter response in-
  to filter out silent snippets.
                                                                      tensity. This model simply reflects the distribution of inten-
     From this collection, 50 high-saliency, 50 low-saliency and
                                                                      sity within the sound sample. (3) The Kayser model uses
  50 large-difference snippets were chosen for the experiments.
                                                                      the saliency map described in (Kayser et al., 2005). Finally,
  The first two groups contained snippets for which the Kayser
                                                                      ASUN models with different lifetime statistics were evalu-
  and ASUN models agreed on high (or low) saliency. Snippets
                                                                      ated separately: (4) “Environmental” sounds, (5) “Animal”
  in the last group were chosen by virtue of producing high-
                                                                      sounds, (6) “Speech” sounds, and (7) “Urban” sounds.
  est disagreements in the predicted saliency values between
  Kayser and ASUN models.                                             Results
     With these snippets, 75 trial pairs were constructed as fol-
  lows:                                                               To quantify the correspondence between the model predic-
                                                                      tion and the human judgments of saliency, Pearson product-
(1) High saliency difference trials (50): Each pair consists of       moment correlation coefficients (PMCC) were calculated be-
     one snippet from the high-saliency and another from the          tween the model predictions and human rating judgment re-
     low-saliency groups.                                             sults (N=7) across all 75 trials. All subjects responded cor-
                                                                      rectly to the catch trials, demonstrating that they were paying
(2) High model discrimination trials (25): Both snippets were
                                                                      attention to the task. Figure 4 shows the correlation coeffi-
     drawn from the large-difference group uniformly.
                                                                      cient values for the ASUN models for each type of dataset
     We expected both models to perform well on high saliency         from which lifetime statistics were learned. The correlation
  difference trials but to produce a performance disparity on the     between the ASUN model predictions and the human subjects
  high model discrimination trials.                                   (M = 0.3262, SD = 0.0635) was higher than the correlation
                                                                      of the Kayser model predictions (M = 0.0362, SD = 0.0683).
  Procedure                                                           The result shows that the ASUN model family predicted
  In each trial, each subject was presented with one second           the human ratings of saliency better than the Kayser model
  of white noise (loudness-adjusted using the same method as          (t(6) = 7.963, p < 0.01.)
  above) followed immediately by binaural presentation of a              To evaluate the model performance in context, across-
  pair of target stimuli. The subject would then respond with         subject correlation was also calculated. Since the models
                                                                  1051

                   1                                                                                                             1
                                                                                                                                                                                  **
                                                                       **                                                                                                                                *
                 0.8                                                                                                           0.8
                 0.6                                                                                                           0.6
 Correlations                                                                                                  Correlations
                 0.4                                                                                                           0.4
                 0.2                                                                                                           0.2
                   0                                                                                                             0
                −0.2                                                                                                          −0.2
                −0.4                                                                                                          −0.4
                              Inten
                                           er
                                                                                                                                                         Inten
                                                   nv)                                                                                                                er
                                                           nima
                                                                           h)
                                                                                    (Urb
                                                                                                  ll)
                                                                                                                                     Subje
                                                                                                                                                                              nv)
                                                                                                                                                                                          Anim
                        om                                                                                                                                                                          ASU                 (All)
                                                                                                                                                Rand                                                   N (S
                                   sity
                                          Kays    N (E         l)   ASU                 an)   ASU                                                             sity
                                                                                                                                                                     Kays    N (E             al)           peec
                       Rand                                                                                                               cts       om
                                                                                                 N (A                                                                                                                 ASU
                                                                                                                                                                                                                 h)
                                                 ASU
                                                          N (A         N (S                                                                                                 ASU                                          N
                                                                                                                                                                                       ASU
                                                                           peec   ASU                                                                                                                ASU
                                                         ASU                         N                                                                                                     N(            N (U
                                                                                                                                                                                                             rban
                                                                                                                                                                                                                  )
Figure 4: Correlation coefficient between various models and                                               Figure 5: Correlation coefficient between various models and
human ratings of saliency (N=7.) ASUN models correlated                                                    human ratings of saliency. A subset of data for which the
with the human ratings of saliency significantly better than                                               same trial pairs were presented was analyzed (N=3). Across-
the Kayser model.                                                                                          subject performance was estimated using the correlation co-
                                                                                                           efficients for all possible pairs from the three subjects.
are not fit to individual subjects, this value provides the ceil-
ing for any model predictions. Because three of the seven                                                  equally well (t(6) = 0.3763, p = 0.7091.) In contrast, in high
subjects went through the same trial pairs in the same order,                                              model discrimination trials, ASUN models performed signif-
these trials were used to calculate the across-subject correla-                                            icantly better than the Kayser model (t(6) = 17.31, p < 0.01.)
tion value, and the model responses. Figure 5 shows the cor-                                               Note that the high model discrimination group was not picked
relation values including the across-subject correlation. The                                              based on the absolute value (or “confidence”) of the model
result shows that the difference between the across-subject                                                predictions, but rather solely on the large difference between
correlations (M = 0.6556, SD = 0.0544) and the ASUN                                                        the two model predictions. This implies the procedure itself
model predictions (M = 0.4831, SD = 0.0432) was significant                                                does not favor one model or the other, nor does it guarantee
(t(2) = 16.9242, p = 0.0035), indicating that the models do                                                performance disparity on average. Nevertheless, the result
not yet predict saliency at the subject-consensus level. Never-                                            shows that the ASUN models perform better than the Kayser
theless, the ASUN model correlations were still significantly                                              model in those trials, suggesting the performance disparity
higher than the Kayser model (M = 0.1951, SD = 0.0815) at                                                  may be explained in large part from those trials.
(t(2) = −9.855, p = 0.0101).
   The performance for the Kayser model in this experiment                                                                                                      Discussion
was notably worse than what was reported in (Kayser et al.,                                                In this work, we demonstrated that a model of auditory
2005). There are several possible explanations for this. First,                                            saliency based on the lifetime statistics of natural sounds is
the audio samples presented in this experiment were roughly                                                feasible. For simple tone signals, auditory saliency maps cal-
normalized for the perceived loudness. This implies that a                                                 culated by the ASUN model qualitatively reproduce phenom-
saliency model that derives saliency values from the loudness                                              ena reported in the psychophysical literature. For more com-
measure in large part may not perform well in this experi-                                                 plicated audio signals, assessing the validity of the saliency
ment. Indeed, the intensity model does not predict the result                                              map is difficult. However, we have shown that the relative
above chance (t(6) = 0.66, p = 0.528). Although the Kayser                                                 magnitudes of the saliency map peaks correlate with human
model does combine information other than the intensity im-                                                ratings of saliency. The result was robust across different
age alone, it is possible that the predictive power of the model                                           training sound collections, which suggest a certain common-
is produced largely by loudness information.                                                               ality in the statistical structure of naturally produced sounds.
   Second, as described previously, some of the trial pairs                                                   There are aspects of the saliency model that may be im-
were chosen intentionally to produce maximal difference be-                                                proved to better model human physiology. For example,
tween the Kayser and ASUN models, and this produced the                                                    there is ample evidence of temporal integration at multiple
large performance disparity. Figure 6 support this hypothesis:                                             timescales in human auditory processing (Poeppel, 2003).
in the high saliency difference trials, both models performed                                              This indicates that the feature responses of the input signal
                                                                                                        1052

                   1                                                                                               Acknowledgments
                 0.8                                                                           We thank Dr. Christoph Kayser for kindly providing us with
                                                                                               the MATLAB implementation of his model. We also thank
                 0.6                                                                           Cottrell lab members, especially Christopher Kanan, for in-
 Correlations
                 0.4                                                                           sightful feedback. This work was supported in part by NSF
                                                                                               grant #SBE-0542013 to the Temporal Dynamics of Learning
                 0.2                                                                           Center.
                   0
                                                                                                                         References
                −0.2
                                                                                               Baldi, P., & Itti, L. (2006). Bayesian Surprise Attracts Human
                −0.4                                                                              Attention. In Nips 2005 (pp. 547–554).
                        er                                                                     Barlow, H. B. (1961). Possible Principles Underlying the
                                    nv)          l)
                                                            ech)
                                                                         )
                       Kays                  nima                     rban     ASU                Transformations of Sensory Messages. Sensory Communi-
                                   N (E
                                                        (Spe                      N (A            cation, 217–234.
                                                                    N (U
                                  ASU
                                            N (A                                      ll)
                                                                                               Cusack, R., & Carlyon, R. (2003). Perceptual asymmetries in
                                           ASU        ASU          ASU
                                                         N                                        audition. J Exp Psychol Human Percept Perf , 29(3), 713–
                                                                                                  725.
                               (a) High saliency difference trials                             Emmons, L. H., Whitney, B. M., & Ross, D. L. (1997).
                                                                                                  Sounds of the neotropical rainforest mammals. Audio CD.
                   1                                                                           Garofolo, J. S., Lamel, L. F., Fisher, W. M., Fiscus, J. G., Pal-
                                                      **                                          lett, D. S., Dahlgren, N. L., et al. (1993). Timit acoustic-
                 0.8
                                                                                                  phonetic continuous speech corpus. Linguistic Data Con-
                 0.6                                                                              sortium, Philadelphia.
 Correlations
                 0.4                                                                           Itti, L., Koch, C., & Niebur, E. (2002). A model of saliency-
                                                                                                  based visual attention for rapid scene analysis. TPAMI,
                 0.2                                                                              20(11), 1254–1259.
                   0                                                                           Jääskeläinen, I. P., Ahveninen, J., Belliveau, J. W., Raij, T., &
                                                                                                  Sams, M. (2007). Short-term plasticity in auditory cogni-
                −0.2                                                                              tion. Trends Neurosci., 30(12), 653–661.
                −0.4                                                                           Kalinli, O., & Narayanan, S. (2007). A saliency-based au-
                                                                                                  ditory attention model with applications to unsupervised
                        er          nv)          l)
                                                        peec
                                                                         )          ll)
                       Kays                  nima
                                                            h)      N (U       ASU
                                                                                                  prominent syllable detection in speech. In Interspeech 2007
                                   N (E
                                                                        rban      N (A
                                                                                                  (pp. 1941–1944). Antwerp, Belgium.
                                            N (A       N (S
                                  ASU                                                          Kayser, C., Petkov, C., Lippert, M., & Logothetis, N. (2005).
                                           ASU                     ASU
                                                      ASU                                         Current biology; mechanisms for allocating auditory atten-
                                                                                                  tion: An auditory saliency map. , 15(21), 1943–1947.
                              (b) High model discrimination trials                             Lewicki, M. S. (2002). Efficient coding of natural sounds.
                                                                                                  nature neurosci, 5(4), 356–363.
Figure 6: Correlation coefficients for the subsets of trials. (a)                              Lyon, R. F., Katsiamis, A. G., & Drakakis, E. M. (2010).
For High saliency difference trials, both Kayser and ASUN                                         History and future of auditory filter models. In Iscas (pp.
models show high correlation to human rating of saliency,                                         3809–3812). IEEE.
and there are no significant differences between them. (b) For                                 Meddis, R. (1986). Simulation of mechanical to neural trans-
High model discrimination trials, ASUN models show sig-                                           duction in the auditory receptor. JASA, 79(3), 702–711.
nificantly higher correlation with human ratings of saliency                                   Poeppel, D. (2003). The analysis of speech in different
compared to the Kayser model.                                                                     temporal integration windows: cerebral lateralization as
                                                                                                  ’asymmetric sampling in time’. Speech Communication,
                                                                                                  41(1), 245–255.
may be better modeled by multiple parallel streams of in-
                                                                                               SoundEffectPack.com. (2011). 3000 sound effect pack. Re-
puts, each convolved with exponentially decaying kernels of
                                                                                                  trieved 2011-03-31, from tinyurl.com/7f4z2wo
varying timescales. This may be especially important for cal-
                                                                                               van den Berg, H. (2010). Urban and nature sounds. Retrieved
culating saliency of longer signals, such as music and spo-
                                                                                                  2011-02-27, from http://tinyurl.com/89mr6dh
ken phrases. In order to accommodate higher-level statistical
                                                                                               Zhang, L., Tong, M. H., Marks, T. K., Shan, H., & Cottrell,
structure, the model can be stacked in a hierarchal manner as
                                                                                                  G. W. (2008). SUN: A bayesian framework for saliency
well, with appropriate feature functions at each level. These
                                                                                                  using natural statistics. Journal of vision, 8(7), 1-20.
expansions will provide insights into the nature of attentional
modulations in human auditory processing.
                                                                                            1053

