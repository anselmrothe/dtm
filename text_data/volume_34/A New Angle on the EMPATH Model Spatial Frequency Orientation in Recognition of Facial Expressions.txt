UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A New Angle on the EMPATH Model: Spatial Frequency Orientation in Recognition of Facial
Expressions
Permalink
https://escholarship.org/uc/item/6kb90113
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Li, Rentao
Cottrell, Garrison
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                        A New Angle on the EMPATH Model:
               Spatial Frequency Orientation in Recognition of Facial Expressions
                                                     Rentao Li (reli@ucsd.edu)
                                         Department of Computer Science, 9500 Gilman Drive
                                                         La Jolla, CA 92093 USA
                                             Garrison Cottrell (gary@eng.ucsd.edu)
                                         Department of Computer Science, 9500 Gilman Drive
                                                         La Jolla, CA 92093 USA
                             Abstract                                   obtained from the NimStim set of facial expressions
                                                                        (Tottenham, Tanaka, Leon, McCarry, Nurse, Hare, Marcus,
  Many have investigated the sensitivity of face processing to
  both spatial frequencies and face orientation, but few have           Westerlund, Casey, & Nelson 2009), and were distorted
  researched the sensitivity of face processing to the orientation      with an orientation filter of bandwidth 23o in the Fourier
  of spatial frequencies. One recent exception has been Yu,             domain, where the center of the filter ranged from -60o to
  Chai, & Chung (2011), which investigated facial expression            90o in increments of 30o. Unfiltered images were used for
  recognition in regards to the orientation of spatial filters and      comparison.
  showed that most information is contained in the horizontal              Their experiment consisted of having 15 normally-sighted
  orientation. Here, we model the Yu, Chai, & Chung (2011)              human subjects try to recognize the expression displayed by
  study using the EMPATH model, a feed-forward neural                   each image under a four-way forced choice. The results
  network that has been used to model facial expression
                                                                        indicate that the human observers had the most success with
  recognition (Dailey, Cottrell, Padgett, & Adolphs 2002). We
  used the NimStim set of facial expressions, which were the            images filtered at orientations near the horizontal (-30o, 0o,
  basis for the Yu, Chai, & Chung (2011) experiment, and                and 30o), suggesting that horizontal spatial information is
  followed their method of filtering images through different           most important for recognizing facial expressions. One
  spatial orientations. Our results show that this simple,              modest exception to this trend is the fearful face; the human
  biologically plausible model produces very similar results to         subjects tended to be significantly biased towards labeling a
  that of human subjects in their study.                                face as fearful as the orientation filter approached 90 o,
  Keywords: emotions; facial expressions; spatial frequency;            which seems to indicate that much of the information for
  neural network; face recognition.                                     fear is represented vertically.
                                                                           The purpose of this current experiment is to determine if a
                        Introduction                                    neural network model can produce similar results as the
Many studies have been conducted regarding the role of                  human subjects, especially in regards to the increased
spatial frequencies in human face recognition (Näsänen,                 recognition performance for horizontal orientations and the
1999; Costen, Parker, & Craw, 1996; Gold, Bennett, &                    preference towards fear for vertical orientations. Such
Sekuler, 1999), although the uniqueness of sensitivity of               evidence would provide greater support for Yu, Chai, &
faces to spatial frequency has been debated (Williams,                  Chung’s (2011) findings and further validate EMPATH’s
Willenbockel, & Gauthier (2009). However, few have                      flexibility and accuracy in modeling human face
explored how different orientations of spatial frequencies              recognition.
impact recognition of facial images. One such experiment
was done by Yu, Chai, & Chung (2011), who passed facial                                           Methods
images through orientation filters from -60 to 90 degrees in            The Model
increments of 30 degrees. They found that the spatial                   The neural network used for this experiment closely
information near horizontal (between -30 and 30 degrees)                followed the EMPATH model developed by Dailey et al.
were the most important for normally-sighted human                      (2002), consisting of a biologically plausible, three-layer,
respondents to recognize facial expressions.                            feed-forward perceptron. EMPATH has been shown to have
                                                                        remarkable face recognition performance on aligned,
Background: Yu, Chai, & Chung’s Experiment                              grayscale images from Ekman and Friesen’s POFA (1976).
                              (2011)                                    Without being tuned specifically to those images, the
                                                                        network classified the emotions Anger, Disgust, Fear,
The aim of the Yu, Chai, & Chung (2011) experiment was
                                                                        Happiness, Sadness, and Surprise with 90% accuracy on
to determine which spatial orientations on the face
                                                                        average, compared to 91.6% for human subjects (Dailey et
contained the most information for identifying emotions.
                                                                        al., 2002). For this experiment, we kept much of the settings
The four emotions they tested were the closed-mouth forms
                                                                        (outlined below) identical to those of the original EMPATH
of anger, fear, happiness, and sadness. Images were
                                                                    1894

model, so the network was not tailored for the spatially           forms of anger, fear, happiness, and sadness; the open-
filtered images or the NimStim dataset.                            mouth form of surprise; and the closed-mouth form of
   The first layer consisted of a set of model neurons based       disgust.
on the magnitude of Gabor filters, which have become a
standard way to model complex cells in the early visual
cortex (Daugman, 1985). In all, 40 different Gabor filters
were used, in combinations of 5 scales and 8 orientations;
filtering was done by passing the face images through a 29
by 35 “grid” of filters, resulting in 40,600 responses per
image. Note that the orientations of the Gabor filters were
the same as in the original EMPATH model, and were not                  Figure 1: Two of the cropped images, corresponding to
changed to fit with the spatial filtering used in this study.                          Happiness and Sadness
   In order to reduce the dimensionality of the data set, we
performed principal component analysis (PCA) on the                   The additional expressions were chosen for the training
Gabor filter outputs, producing 50 principal components            set so that the network would have a more comprehensive
(again based on the original EMPATH model). In this                exposure to the range of different emotions, making it more
second layer, the principal components capture the                 similar to the experience of the human subjects in the
distinguishing features of each facial expression but abstract     experiment. Although the testing set also contained six
away from details unique to each face; hence they allow the        expressions, our method for producing the network’s output
network to generalize to novel faces that are not part of the      was able to emulate a four-way forced choice among the
training set. As Dailey et al. stated, these components are        four expressions used by Yu, Chai, & Chung (2011), which
similar to face cells in the inferior temporal cortex (2002).      effectively limited the output to only those four choices
   Lastly, the principal components were fed into the third        (detailed in “Training, Validating, and Testing”).
layer, consisting of a simple linear perceptron with six
softmax outputs representing anger, fear, happiness,               Processing the training set: In order to better replicate the
surprise, disgust, and sadness. This perceptron was trained        images used by Yu, Chai, & Chung (2011), the 30 sets of
using stochastic gradient descent with the cross-entropy           images were closely cropped about the face using an oval
error criterion. We used an “all-or-none” teaching signal          mask so that only an oval-shaped portion of the face was
that had “1” for each correct expression and “0” for the           visible. The parts that were cropped out were filled in with a
incorrect expressions. In order to replicate the four-way          uniform gray color of RGB value 127, and the entire image
forced-choice employed by Yu, Chai, and Chang (2011), we           was adjusted to have a root-mean-square contrast value of
only took the results of the four relevant emotions via a          0.096, as per specifications given in Yu & Chung (2011).
process described in “Training, Validating, and Testing.”          Examples of images used in the training set are shown in
   As stated in Dailey et al., we acknowledge that this            Figure 2.
perceptron is very simplistic (2002). However, since it was
powerful enough to map the principal components to
emotion categories, we did not feel that a non-linear
classifier was needed.
The Images
The images used in the testing set by Yu, Chai, & Chung                      Figure 2: Two images from the training set
(2011) consisted of morphs of images taken from the
NimStim set (2009), which reduced variations among faces           Processing the testing set: The process for creating the
(such as race, gender, etc.). Without access to these exact        testing set was very similar to that of the training set. The 30
morphs, however, we simply used some of the original               original sets of images were first cropped, aligned, and then
NimStim images to create our training and testing sets.            processed using Yu, Chai, & Chung’s (2011) filters at six
Given this difference, our results still closely matched those     orientations from -60o to 90o in increments of 30o, which
of Yu, Chai, & Chung (2011).                                       selectively pass information at the specified orientations.
   Our testing and training sets consisted of grayscale            Afterwards, the oval “mask” was applied to the images in
images of 30 different people (17 male, 13 female). Two of         the same way as that of the training set, and all of the
the images are shown in Figure 1. These images were                images were again normalized to have the same root-mean-
judged to be the most frontally aligned, making them the           square contrast of 0.096. Examples of the test images are
most suitable for EMPATH. Each image was 240 x 292                 shown in Figure 3. These were then processed by the same
pixels in size and was cropped closely about the face.             Gabor filters as used in the training set, and the resulting
   Both the testing and training sets contained images of six      filter responses were projected onto the 50 eigenvectors
different emotions for each of the 30 people. These                using the PCA that was computed on the training set.
expressions were comprised of both open and closed-mouth
                                                               1895

                                                                                                                     Results
                                                                  Our model was able to fit the human data very well in
                                                                  several measures. Much of the data presented by Yu, Chai,
                                                                  & Chung (2011) is displayed in the form of confusion
                                                                  matrices, which pit the human responses against the actual
                                                                  targets. We used the same technique to display our data.
                                                                  Since Yu, Chai, & Chung (2011) presented their results on a
                                                                  poster, many of their figures lack numerical data. As such,
                                                                  much of our analysis will be dependent on comparing the
                                                                  visual presentations of data. The color spectrum of our
    Figure 3: All 6 filtering conditions shown horizontally       confusion matrices were closely matched to that used by
         from -60o (top left) to 90o (bottom right).              Yu, Chai, & Chung (2011) so that comparisons and
                                                                  conclusions can be made.
Training, validating, and testing
The last layer of the model was a 50-input, 6-output single       Performance on Unfiltered Images
layer perceptron with softmax outputs trained using cross         Figures 4a and 4b show the confusion matrix for the
entropy. This procedure leads to outputs that compute the         unfiltered images given by Yu, Chai, & Chung (2011) and
conditional probability of the category given the inputs          by our model, respectively. In addition to the hits, the
(Bishop, 1995). Hence the output of the network is a              columns show false alarms and rows depict misses.
probability distribution over the facial expression categories.
   Cross-validation and early stopping were used to prevent
overfitting the network to the training set. Since there are
thirty individuals in the training set, we performed thirty
instances of cross validation, each time holding out a
different individual in the training set to use for early
stopping. This comprehensive cross-validation made the
testing less prone to unevenness among the images. Overall,
there were 30 independent test cycles; each time 1 set was
chosen for testing, 1 chosen for validation, and the
remaining 28 were used as the training set. The aggregate
performance from these 30 sets of tests constituted the
results for each filtering condition. The validation set was
taken from the processed test set as a guide to know when to
stop training (of course the validation and test images were
never the same). Training was completed for each cycle
once the cross-entropy error for the validation set was
minimized using gradient descent, and the weights of the          Figure 4a: Presented target vs. human responses (unfiltered)
network were then used for the testing set.                               as presented in Yu, Chai, & Chung (2011).
   The testing procedure involved computing the weighted
sum of the 50-element test set using the weights from                                                            Unfiltered
training, then again applying the softmax function. A simple                                                 Observer Response
                                                                                                   Angry       Fearful   Happy     Sad
max function was used to judge if the testing outputs                                                                                     1
matched the teaching signals, and to create the confusion                                 Angry                                           0.9
                                                                                                                                          0.8
matrix. However, since the weights were trained with six
                                                                      Target Presented
                                                                                                                                          0.7
facial expressions, the max function was applied only                                    Fearful
                                                                                                                                          0.6
among the four target emotions to create a four-way forced
                                                                                                                                          0.5
choice similar to what a human subject would have to
                                                                                                                                          0.4
perform. This is valid because the softmax function created                              Happy
                                                                                                                                          0.3
outputs that were probabilities of each emotion being
                                                                                                                                          0.2
correct; thus, although the teaching signal was “all or                                    Sad
                                                                                                                                          0.1
nothing,” the outputs were not. We note that when humans
                                                                                                                                          0
undergo the task of selecting among four target emotions, it                                       0.23        0.22       0.26     0.29
                                                                                                           Proportion of Responses
is entirely possible that the emotion they perceive is not
among the four options, and thus they may have to answer
                                                                               Figure 4b: Presented target vs. model responses, for
with their second or third choice. This is essentially what we
                                                                                              unfiltered images.
have emulated with our network.
                                                              1896

   Both the model and the human subjects demonstrated
very good performance overall in recognizing the unfiltered
images. The model also exhibited similar behavior as the
human subjects in terms of having false alarms for sad faces
when presented with Angry and Fearful faces. This is likely
a characteristic of the closed-mouth sad faces in the
NimStim set in general, since similar false alarms were
present in Tottenham et al. (2009). The total proportion of
responses was also similar between EMPATH and the
human subjects, which suggests that the model was
sensitive to many of the same facial features that the human
subjects used for classification.
Performance for individual filters
The results for the individual filters demonstrate that                  Figure 5b: EMPATH performances for each filtering
recognition performance decreased as the filter orientations                                   condition.
approached 90o. Figures 5a and 5b illustrate the
performances of humans and of our model, respectively.
   Both sets of confusion matrices distinctly show greater
occurrences of misses and false alarms at orientations near
the vertical; i.e. 60o, -60o, and 90o. Some other general
trends can be drawn from the data. For both humans and the
model, sad faces tended to draw more false alarms and
misses, regardless of the filter condition. Angry expressions
tended to lose their uniqueness as the filters neared vertical,
resulting in many misses, and few hits and false alarms.
   One informative visualization of recognition performance
is plotting the d prime calculations for each filter, which
indicates how strong a signal is in relation to surrounding
noise (Abdi, 2010). Hence, the d prime calculation for each
filtering condition is proportional to how recognizable the
expression is with that filter. Figures 6a and 6b depict
graphs of d primes for each filtering condition normalized to
                                                                        Figure 6a: Human data from Yu, Chai, & Chung (2011),
the d prime of the unfiltered images (higher d primes still
correlate to higher recognition performance).                        showing d primes of each filtering condition, normalized to
                                                                     the unfiltered condition. Note that data for -90o was copied
                                                                                          from data for 90o.
                                                                        Figure 6b: Data from the EMPATH model showing d
 Figure 5a: Human performance for each filtering condition.            primes of each filter normalized to the unfiltered images.
                From Yu, Chai, & Chung (2011).
                                                                       Both d prime charts demonstrate lower recognition
                                                                    performance as the filters approached 90 o. We note that
                                                                1897

results from the EMPATH model are not as symmetrical as             sadness, and that both were biased against anger. In both
those from the human data (e.g. the discrepancy between the         cases, anger was the most difficult expression to recognize
values for -30 and 30 degrees for happiness, and the M-             and also the most difficult with which to be confused, based
shaped graph for fear). This is likely due to the fact that the     on the overall percentage of responses. The human subjects
original images are not vertically balanced; i.e. the positive      and EMPATH also had difficulty recognizing sad faces, but
and negative filters each obstruct slightly different features      there was a high false-alarm rate as well. The Spearman
of the expressions. The resulting images were likely                rank correlation between the two matrices was very good, at
different enough to confuse the network. It would be                r = 0.976 (p < 0.001) for the complete matrices. Since we
worthwhile in the future to explore this phenomenon of              were also interested in the misses and false alarms, we also
asymmetry, especially since it was not apparent in the              calculated the rank just for the off-diagonals, which was
human data.                                                         very similar at r = 0.942 (p < 0.001).
   As noted earlier, fearful faces were less affected by the
filter orientations as the other three emotions. Both d prime           Table 1a: Aggregate performance of human subjects for
charts show that fear was the most easily recognized at the                             all 6 filter orientations.
90o orientation. The earlier confusion matrices (Figures 5a
and 5b) likewise depict a relatively steady percentage of hits
for fearful faces. Much of this is attributed to the fact that
both human observers and EMPATH exhibited a significant
bias towards fear at the vertical orientations, which
increased the occurrences of both hits and false alarms.
Figure 7 illustrates EMPATH’s high proportion of
responses for fear at the vertical orientations. At the
horizontal orientations, each emotion constituted close to
25% of the responses, but at the vertical orientation,
responses in favor of fear approached 40%.
                                                                         Table 1b: Aggregate performace of EMPATH for all 6
                                                                                            filter orientations
     Figure 7: Total proportion of responses exhibited by
               EMPATH for each filter orientation.
Aggregate Performance of 6 filters
EMPATH’s cumulative performance across all 6 filters is
also similar to that of the human observers. Tables 1a and
1b show the overall performance of the human subjects and                                      Discussion
of EMPATH, respectively.                                                  The aim of this present experiment was to model the
   Firstly, the two tables show that with the exception of            experiment conducted by Yu, Chai, & Chung (2011) and
anger, EMPATH does significantly better in recognizing                determine if their results can be replicated using a neural
expressions than do the human subjects. Of course, the              network that was not specifically tuned to their images or
model can always be tailored to perform either better or            their data. Our results demonstrate a strong similarity to the
worse, but we did not want to make adjustments just to suit         pattern of human responses, particularly in showing that
these images. Secondly, the two tables depict many similar          information for facial expressions lies primarily on the
trends in the responses between the humans and the model.           horizontal orientation, with the modest exception of fearful
The overall proportion of responses shows that both the             faces, which solicited heavy bias from both human
humans and the model were biased towards fear and                   observers and EMPATH as the orientation approached
                                                                1898

vertical. In particular, there was a very high proportion of                           Acknowledgements
hits and false alarms, suggesting that the vertical filter         We thank Dion Yu and Susana Chung for sharing their
accentuated features in other expressions normally                 filtering code, and for providing us with ample clarification
attributed to fear. Based on this data, it seems that much of      of their experiment. We thank Nim Tottenham for sharing
the information for fear lies on the vertical, making it           the NimStim dataset with us. We also thank Gary’s
distinct from other expressions. It would be interesting to        Unbelievable Research Unit (GURU) for providing
conduct further experiments with other image sets to               feedback on this project, and Matthew Tong in particular for
determine if this phenomenon is a trait of the NimStim             overseeing part of the reconstruction of EMPATH. This
images or if it is more universal.                                 work was supported in part by NSF grant #SBE-0542013.
   Some discussion regarding our use of d prime is
warranted. The procedure used by Yu, Chai, & Chung
(2011) to calculate d prime follows the standard guideline of
                                                                                             References
d’ = ZH - ZFA, where ZH and ZFA denote the inverse                 Abdi, H. (2010). Signal detection theory (SDT). In Peterson,
Gaussian distribution of hits and false alarms, respectively          P.L. Baker, E., & B. McGaw (Eds.). International
(Abdi, 2010). However, since this formula is typically used           Encyclopedia of Education. New York: Elsevier.
for two-way “Yes – No” tasks, the validity of using it for a       Alexander, J.R.M. (2006) An approximation to d' for n-
four-way forced choice is debatable, since each emotion has           alternative forced choice. University of Tasmania
one “Yes” response and three distinct “No” responses                  technical report, available at eprints.utas.edu.au.
attached to it. Very little literature exist detailing d prime     Bishop, C. M. (1995). Neural networks for pattern
calculations for multiple-way forced choice scenarios, but            recognition. Oxford: Oxford University Press.
Alexander       (2006)     described    an    easily-computed      Costen N.P., Parker D.M., & Craw I (1996). Effects of high-
approximation to the original version in Green & Swets                pass and low-pass spatial filtering on face identification.
(1966). Based on that, we have recalculated our graph of d            Percept Psychophys, 58, 602–612.
prime, which we depict in Figure 8. It should be noted that        Dailey M.N., Cottrell G.W., Padgett C., & Adolphs, R.
this approximation does not take false alarms into account.           (2002). EMPATH: a neural network that categorizes
This resulted in a significantly higher d prime for fearful           facial expressions. J. Cog. Neuro., 14, 1158-1173.
expressions, which were actually greater for filtering             Daugman, J. G. (1985). Uncertainty relation for resolution
conditions near vertical than for unfiltered images. Given            in space, spatial frequency, and orientation optimized by
that this is an approximation, the validity may of course also        two-dimensional visual cortical filters. Journal of the
be debated, but we nonetheless present both calculations.             Optical Society of America A, 2, 1160–1169.
This serves as a prediction of how the Yu, Chai, & Chung           Ekman, P., & Friesen, W. (1976). Pictures of facial affect.
(2011) data will look if it were analyzed in the same way.            Palo Alto, CA: Consulting Psychologist Press.
   Given EMPATH’s demonstrated consistency in modeling             Gold J., Bennett P.J., & Sekuler A.B. (1999). Identification
human face recognition, another possible future experiment            of band-pass filtered letters and faces by human and ideal
could be to determine which filtering orientations are ideal          observers. Vision Res, 39, 3537–3560.
for recognition of each particular expression. It seems,           Green, D. M. & Swets, J. A. (1966). Signal detection theory
based on this study, that the majority of expressions with the        and psychophysics. New York: Wiley.
exception of fear would have an ideal filtering condition          Nasanen R. (1999). Spatial frequency bandwidth used in the
near the horizontal, but determining exact orientations               recognition of facial images. Vision Res, 39, 3824–3833.
would form testable hypotheses generated by the model.             Williams, N.R., Willenbockel, V. & Gauthier, I. (2009).
                                                                      Sensitivity to spatial frequency and orientation content is
                                                                      not specific to face perception. Vision Res, 49, 2353–
                                                                      2362.
                                                                   Tottenham N., Tanaka J.W., Leon A.C., McCarry T., Nurse
                                                                      M., Hare T.A., Marcus D.J., Westerlund A., Casey B.J.,
                                                                      Nelson C. (2009). The NimStim set of facial expressions:
                                                                      judgments from untrained research participants.
                                                                      Psychiatry Research, 168, 242-249.
                                                                   Yu D., Chai A., & Chung S.T.L. (2011). Orientation
                                                                      information in encoding facial expressions. Poster
                                                                      presented at the Vision Sciences Society 2011 Annual
                                                                      Meeting, Naples, Florida.
                                                                   Yu D. & Chung S.T.L. (2011). Critical orientation for face
    Figure 8: N-choice d prime approximations, following              identification in central vision loss. Optometry and Vision
 procedure outlined by Alexander (2006) and normalized to             Science, 88, 724-732.
                     the unfiltered images.
                                                               1899

