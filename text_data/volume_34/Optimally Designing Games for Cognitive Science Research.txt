UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Optimally Designing Games for Cognitive Science Research
Permalink
https://escholarship.org/uc/item/42r005ng
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Raffert, Anna
Zaharia, Matei
Griffiths, Thomas
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                       Optimally Designing Games for Cognitive Science Research
                                               Anna N. Rafferty (rafferty@cs.berkeley.edu)
                                                  Matei Zaharia (matei@cs.berkeley.edu)
                              Computer Science Division, University of California, Berkeley, CA 94720 USA
                                            Thomas L. Griffiths (tom griffiths@berkeley.edu)
                               Department of Psychology, University of California, Berkeley, CA 94720 USA
                                 Abstract                                     the success of previous actions, leading to complex strate-
                                                                              gies. We propose using Markov decision processes (MDPs)
   Collecting cognitive science data using games has the potential
   to be a powerful tool for recruiting participants and increasing           to predict people’s actions in games. MDPs incorporate the
   their motivation. However, designing games that provide use-               current and future benefit of an action, and thus allow us to
   ful data is a difficult task that often requires significant trial and     take into account the incentive structures and rules of a partic-
   error. In this work, we consider how to apply ideas from op-
   timal experiment design to designing games for cognitive sci-              ular game. By combining MDPs with ideas from optimal ex-
   ence experiments. We use Markov decision processes to model                periment design, we create a framework for finding the game
   players’ actions within a game, and then make inferences about             that will provide the highest expected information gain.
   the parameters of a cognitive model from these actions. We
   present a general framework for finding games with high ex-                   The plan of the paper is as follows. The next section pro-
   pected information gain based on this approach. We apply this              vides background on optimal experiment design and MDPs.
   framework to Boolean concept learning, inferring the difficulty            We then show how to combine these ideas in a framework
   of Boolean concepts from participants’ behavior. We show that
   using games with higher expected information gain allows us                for optimal game design. The remainder of the paper ap-
   to make this inference more efficiently.                                   plies this general framework to the specific case of learning
   Keywords: optimal experiment design; Markov decision pro-                  Boolean concepts, illustrating the benefits of optimal game
   cess; computer games; concept learning                                     design. We introduce a novel concept learning game, and
                                                                              use our approach to optimize the game parameters. Two be-
                             Introduction                                     havioral experiments show that our optimized game results in
Computer games have become increasingly popular tools for                     more efficient estimation of the difficulty of learning different
gathering psychological data and for educational purposes                     kinds of Boolean concepts, and that the actual amount of in-
(e.g., Michael & Chen, 2005; Von Ahn, 2006; Siorpaes &                        formation obtained from players is positively correlated with
Hepp, 2008; Klopfer, 2008), providing a way to recruit large                  the expected information gain of the game.
numbers of motivated participants. However, creating games
                                                                                                      Background
that actually result in useful data requires significant engi-
neering, normally based on trial and error. In this paper we                  The optimal game design framework we propose relies on
propose a method for automating the process of designing                      ideas from Bayesian experiment design and Markov decision
games, using ideas from optimal experiment design.                            processes, which we will introduce in turn.
   The key problem in designing games for cognitive science                   Bayesian Experiment Design
research is finding a game that provides as much informa-                     Bayesian experiment design, a subfield of optimal experiment
tion as possible about the research question being addressed.                 design, seeks to choose the experiment that will maximize the
For traditional experiments, the field of optimal experiment                  expected information gain about a parameter θ (Atkinson et
design seeks to choose the design that will give the most in-                 al., 2007; Chaloner & Verdinelli, 1995). In cognitive science,
formation about the dependent variable (Atkinson, Donev, &                    this procedure and its variations have been used to design
Tobias, 2007). We adapt this method to identify the game                      more informative experiments that allow for clearer discrimi-
that will give the most information about the parameters of a                 nation between alternative hypotheses (Myung & Pitt, 2009).
cognitive model. By automating the process of game design,                    Throughout this paper, let ξ be an experiment (or game) de-
we limit the trial and error necessary to find a game that will               sign and y be the data collected in the experiment. Then the
provide useful data, while still reaping the benefits of using                Bayesian experimental design procedure is as follows:
games rather than traditional experiments.                                                            Z
   Adapting optimal experiment design methods to game                             maximize U(ξ) =        p(y|ξ)U(y, ξ)dy
design requires predicting people’s behavior within games,                                            Z
which may differ from behavior in traditional experiments.                           where p(y|ξ) =      p(y|ξ, θ)p(θ)dθ
For instance, in a categorization experiment a participant’s re-                                      Z
sponse to a stimulus may roughly correspond to whether she                            and U(y, ξ) =     (H(p(θ|y, ξ)) − H(p(θ))) dθ,        (1)
believes the stimulus is in the category. However, in a game
this relationship is complicated by competing incentives. For                 where H(p) is the Shannon entropy
                                                                                                              R
                                                                                                                      of a probability distri-
example, available actions in a game may be contingent on                     bution p, defined as H(p) = p(x) log(p(x))dx. Thus, the
                                                                          893

procedure maximizes the expected utility of an experiment ξ,                where a is the set of action vectors for all players. The ex-
defined as the information gain over all outcomes y weighted                pectation is approximated by sampling θ from the prior p(θ),
by their probabilities p(y|ξ) under the current prior.                      and then simulating players’ actions given θ by calculating
                                                                            the Q-values for the MDP and sampling from Equation 3.
Markov Decision Processes
                                                                               The remaining quantity in Equation 4 is p(θ|a, ξ). Intu-
The Bayesian experiment design procedure uses p(θ|y, ξ) to
                                                                            itively, this quantity connects actions taken in the game with
calculate the information gain from an experiment. This
                                                                            the parameter of the cognitive model that we seek to infer,
quantity represents the impact that the data y collected from
                                                                            θ. For a game to yield useful information, it must be the case
experiment ξ have on the parameter θ. In a game, the data y
                                                                            that people will take different actions for different values of θ.
are a series of actions, and to calculate p(θ|y, ξ), we must in-
                                                                            Concretely, we expect that players’ beliefs about the reward
terpret how θ affects those actions. Via Bayes’ rule, we know
                                                                            model and the transition model may differ based on θ. For
p(θ|y, ξ) ∝ p(y|θ, ξ)p(θ). We thus want to calculate p(y|θ, ξ),
                                                                            instance, in a categorization task with two objects A and B, θ
the probability of taking actions y given a particular value
                                                                            might determine the probability that A is a positive instance
for θ and a game ξ. To do so, we turn to Markov decision
                                                                            and B is a negative instance of the category. If taking a par-
processes (MDPs), which provide a natural way to model se-
                                                                            ticular action leads to positive rewards only when a positive
quential actions. MDPs and reinforcement learning have been
                                                                            instance is observed, then we would expect that the value of
used previously in game design for predicting player actions
                                                                            θ is large if many players take that action when observing A.
and adapting game difficulty (Erev & Roth, 1998; Andrade,
Ramalho, Santana, & Corruble, 2005; Tan & Cheng, 2009).                        The process of inferring θ from actions assumes that each
   MDPs describe the relation between an agent’s actions and                θ corresponds to a particular MDP. If this is the case, we can
the state of the world and provide a framework for defining                 calculate a distribution over values of θ based on the observed
the value of taking one action versus another (see Sutton &                 sequences of actions a of all players in the game ξ:
Barto, 1998). Formally, an MDP is a tuple hS, A, T, R, γi,
where S is the set of possible states and A is the set of ac-                              p(θ|a, ξ) ∝ p(θ)p(a|θ, ξ)                      (5)
tions that the agent may take. The transition model T gives                                          = p(θ)p(a|MDPθ , ξ)                  (6)
the probability p(s0 |s, a) that the state will change to s0 given
that the current state is s and the agent takes action a. The                                        = p(θ) ∏ p(ai |MDPθ , ξ),            (7)
                                                                                                             i
reward model R(s, a, s0 ) describes the probability of receiving
a reward r ∈ R given that action a is taken in state s and the
resulting state is s0 . Finally, the discount factor γ represents           where ai is the vector of actions taken by player i and MDPθ
the relative value of immediate versus future rewards. The                  is the MDP derived for the game based on the parameter θ.
value of taking action a in state s is defined as the expected              Calculating this distribution can be done exactly if there is a
sum of discounted rewards and is known as the Q-value:                      fixed set of possible θ or by using Markov chain Monte Carlo
                                                                    !       (MCMC) methods if the set of θ is large or infinite (see Gilks,
                                                                            Richardson, & Spiegelhalter, 1996).
 Q(s, a) = ∑ p(s0 |s, a) R(s, a, s0 ) + γ  ∑   p(a0 |s0 )Q(s0 , a0 ) ,
             s0                            0
                                          a ∈A                                 Now that we have defined p(θ|a, ξ), we can use this to
                                                                    (2)     find the utility of a game. Equation 4 shows that this cal-
where p(a0 |s0 ) is the probability that an agent will take action          culation follows simply if we can calculate the entropy of the
a0 in state s0 and is defined by the agent’s policy π. We assume            inferred distribution. In the case of a fixed set of possible
that people’s actions can be modeled as a Boltzmann policy,                 θ, H(p(θ|a, ξ)) can be calculated directly. If MCMC is used,
as in Baker, Saxe, and Tenenbaum (2009):                                    one must first infer a known distribution from the samples and
                                                                            then take the entropy of that distribution. For example, if θ is
                      p(a|s) ∝ exp(βQ(s, a)),                       (3)
                                                                            a multinomial and p(θ) is a Dirichlet distribution, one might
where higher values of β mean the agent is more likely to                   infer the most likely Dirichlet distribution from the samples
choose the best action, while β = 0 results in random actions.              and find the entropy of that distribution.
                   Optimal Game Design                                         We have now shown how to (approximately) calculate
                                                                            U(ξ). To complete the procedure for optimal game design,
We can now define a procedure for optimal game design,
                                                                            any optimization algorithm that can search through the space
identifying the game with maximum expected information
                                                                            of games is sufficient. Maximizing over possible games is
gain for θ. We assume there is an existing game design with
                                                                            unlikely to have a closed form solution, but stochastic search
parameters to adjust, corresponding to point values, locations
                                                                            methods can be used to find an approximate solution to the
of items, or any other factor that can be varied. To apply
                                                                            maximum utility game. For example, one might use simu-
Bayesian experiment design to choosing a game, we define
                                                                            lated annealing (Kirkpatrick, Gelatt, & Vecchi, 1983). This
the utility of a game ξ as the expectation of information gain
                                                                            method allows optimization of both discrete and continuous
over the true value of θ and the actions chosen by the players:
                                                                            parameters, where neighboring states of current game are
            U(ξ) = E p(θ,a) [H(p(θ)) − H(p(θ|a, ξ))],               (4)     formed by perturbations of the parameters to be optimized.
                                                                        894

                                                                     The likelihood p(d|h) is then a simple indicator function:
                                                                                                 
                                                                                                    1 if h ` d
 dim1
 Dim. 1                                                                                 p(d|h) ∝
                                                                                                    0 otherwise
                                                                                                                     ,             (10)
          dim2
           Dim. 2
                            II           II
                                         II            III
                                                       III
                                                                     where h ` d if the stimulus classification represented by d
                                                                     matches the classification of that stimulus in hypothesis h.
                                                                     We seek to infer the prior p(h), which represents the diffi-
             dim33
            Dim.                                                     culty of learning different concepts and thus gives an implicit
                                                                     ordering on structure difficulty. In our earlier terminology, θ
                                                                     is a prior distribution on concepts p(h). For simplicity, we as-
                           IV
                           IV            VV            VI
                                                       VI            sume all concepts with the same structure have the same prior
Figure 1: Boolean concept structures. In each structure, eight       probability, so θ is a 6-dimensional multinomial.
objects differing in three binary dimensions are grouped into
two categories of four elements. Each object is represented as       Corridor Challenge
a corner of a cube based on its combination of features, and         To teach people Boolean concepts we created the game Cor-
the objects chosen for one category in each problem type are         ridor Challenge, which requires learning Boolean concepts to
represented by dots.                                                 achieve a high score. Corridor Challenge places the player in
                                                                     a corridor of islands, some of which contain a treasure chest,
                                                                     joined by bridges (Figure 2).1 The islands form a linear chain
 Optimal Games for Boolean Concept Learning                          and the bridges can be crossed only once, so players cannot
We have described a general framework for automatically              return to previous chests. Some chests contain treasure, while
finding games that are potentially highly informative about          others contain traps; opening a chest with treasure increases
model parameters. To test this framework, we applied it to a         the player’s score and energy, while opening a chest with a
particular question: What is the relative difficulty of various      trap decreases these values. Each chest has a symbol indi-
Boolean concept structures? This question has been studied           cating whether it is a trap; symbols differ along three binary
in past work (e.g, Shepard, Hovland, & Jenkins, 1961; Grif-          dimensions and are categorized as a trap based on one of the
fiths, Christian, & Kalish, 2008), so we can compare our re-         Boolean concepts. Players are shown a record of the symbols
sults to those produced using more traditional methods. We           from opened chests and their meanings (see the right hand
first describe Boolean concept learning, and then turn to the        side of Figure 2). Players are told to earn the highest score
game we created and the application of optimal game design.          possible without running out of energy, which is depleted by
                                                                     moving to a new island or opening a trapped chest. When a
Boolean Concepts                                                     player runs out of energy, the level is lost and she cannot ex-
In Boolean concept learning, one must learn how to catego-           plore the rest of the level; surviving a level earns the player
rize objects that differ along several binary dimensions. We         250 points. Corridor Challenge games may consist of several
focus on the Boolean concepts explored in Shepard et al.             levels. Each level is a new corridor with different chests, but
(1961). In these concepts, there are three feature dimensions,       the same symbols are used and they retain the same meaning
resulting in 23 possible objects, and each concept contains          as on the previous level. At the start of each level, the player’s
four objects. This results in a total of 70 concepts with six        energy is restored, but points are retained from level to level.
distinct structures, as shown in Figure 1. Shepard et al. (1961)     Optimizing Corridor Challenge
found that the six concept structures differed in learning dif-
ficulty, with a partial ordering from easiest to most difficult      Applying optimal game design to Corridor Challenge re-
of I > II > {III, IV, V} > VI. Similar results were observed         quires specifying the parameters to optimize in the search
in later work (Nosofsky, Gluck, Palmeri, McKinley, & Glau-           for the optimal game, formulating the game as an MDP, and
thier, 1994; Feldman, 2000) although the position of Type VI         specifying the model for how the player’s prior on concepts
in the ordering can vary (Griffiths et al., 2008).                   (θ) relates to the MDP parameters. The structure of Corridor
   To model learning of Boolean concepts, we assume learn-           Challenge allows for many variants that may differ in the ex-
ers’ beliefs about the correct concept h can be captured by          pected information gain. To maximize expected information
Bayes’ rule (Griffiths et al., 2008):                                gain while keeping playing time relatively constant we lim-
                                                                     ited the game to two levels, with five islands per level. We
                   p(h|d) ∝ p(h)p(d|h)                       (8)     then used optimal game design to select the number of points
                                                                     gained for opening a treasure chest, points lost for opening
                            = p(h) ∏ p(d|h),                 (9)     a trap chest, the energy lost when moving, the symbols that
                                   d∈d
                                                                         1 Corridor Challenge uses freely available graphics from
where each d ∈ d is an observed stimulus and its classifi-           http://www.lostgarden.com/2007/05/dancs-miraculously
cation, and observations are independent given the category.         -flexible-game.html
                                                                 895

                                                                       would like to estimate, which is this prior distribution.
                                                                       Reward model: When the player moves from one island to
                                                                       another, the reward model specifies R(s, a, s0 ) = 0, and when
                                                                       the player opens a chest, R(s, a, s0 ) is a fixed positive number
                                                                       of points with probability p(x in concept) and a fixed negative
                                                                       number of points with probability (1 − p(x in concept)).
                                                                          By using the MDP framework and assuming that the player
                                                                       updates her beliefs after seeing information, we ignore the
                                                                       value of information in understanding people’s decisions; that
                                                                       is, we assume people make decisions based on their current
                                                                       information and do not consider the effect that information
                                                                       gained now will have on their future decisions.
                                                                                 Experiment 1: Inferring Difficulty
                                                                       To test our framework, we first used the optimal game de-
Figure 2: Corridor Challenge game UI (Level one of the ran-            sign procedure to find a version of Corridor Challenge with
dom game in Experiment 1). In this screenshot, the player              high expected information gain, and then ran an experiment
has opened the first chest and moved to the second island.             in which players played the optimized game or a randomly
                                                                       chosen game with lower expected information gain.
appeared on the chests, and the Boolean concept used to cate-          Search Methods
gorize the chests. For simplicity, we assumed that the number
                                                                       We used simulated annealing to search for the best design for
of points gained (or lost) for a particular action is equal to the
                                                                       Corridor Challenge (Kirkpatrick et al., 1983). The expected
amount of energy gained (or lost) for that particular action.
                                                                       information gain of a game was approximated by sampling 35
   Given particular specifications for these variants of the
                                                                       possible θ vectors uniformly at random (reflecting a uniform
game, we can define an MDP. Note that we define the MDP
                                                                       prior on θ), simulating the actions of n = 25 players in the
based on a player’s beliefs, since these govern the player’s ac-
                                                                       game, and using the simulated data to infer p(θ|ξ, a). We ap-
tions, and these beliefs do not include knowledge of the true
                                                                       proximated p(θ|ξ, a) using the Metropolis-Hastings MCMC
concept that governs the classification of the symbols:
                                                                       algorithm (Gilks et al., 1996), with a Dirichlet proposal dis-
States: The state is represented by the player’s energy, her
                                                                       tribution centered at the current state. The parameter β for the
current level and position in the level, and the symbols on all
                                                                       Boltzmann policy was set to 1.
chests in the current level.
                                                                          To execute the search, we parallelized simulated anneal-
Actions: The player can open the current chest (if there is
                                                                       ing by using several independent search threads. Every five
one) or move to the next island.
                                                                       iterations, the search threads pooled their current states, and
Transition model: The player transitions to a new state based
                                                                       each thread selected one of these states to continue searching
on opening a chest or moving to a new island. In both cases,
                                                                       from, with new starting states chosen probabilistically such
the symbols on the chests stay the same, with the current sym-
                                                                       that states with high information gain were more likely to be
bol removed if the player opens the chest. If a player chooses
                                                                       chosen. Each search state is a game, and the next state was
to move, she knows what state will result: her position will
                                                                       found by selecting a parameter of the current game to per-
move forward one space and her energy will be depleted by
                                                                       turb. If the selected parameter was real-valued, a new value
a known constant. If the result is negative energy, then the
                                                                       was chosen by sampling from a Gaussian with small variance
game transitions to a loss state. However, if a player opens
                                                                       and mean equal to the current value; if the selected parameter
a chest, her beliefs about what state will occur next is depen-
                                                                       was discrete, a new value was selected uniformly at random.
dent on p(h|d), her beliefs about the true concept given the
data d she has observed so far. The player will gain energy if         Search Results
the symbol x on the current chest is in the concept. Taking an         The stochastic search found games with significantly higher
expectation over possible concepts h, this probability is              information gain than the initial games, regardless of start-
                                                                       ing point. This demonstrates that the evaluation and search
              p(x in concept) = ∑ I(h ` x)p(h|d),             (11)
                                  h
                                                                       procedure may be able to eliminate some trial and error in
                                                                       designing games for experiments. Qualitatively, games with
where I(h ` x) = 1 if x is in the concept h and 0 otherwise.           high information gain tended to have a low risk of running
The probability of decreased energy is (1 − p(x in concept)).          out of energy, at least within the first few moves, and a di-
Based on the Bayesian model presented above, the player’s              verse set of stimuli on the chests. These games also generally
current beliefs p(h|d) are dependent on the prior probabil-            had positive rewards with larger magnitudes than the negative
ity distribution over concepts. Thus, the transition model as-         rewards. The game with the highest information gain used a
sumed by the player is dependent on the parameter θ that we            true concept of Type II, although several games with similarly
                                                                   896

       (a)               Type I                          Type II                        Type III                        Type IV                             Type V                        Type VI
               −1                              −1                              −1                              −1                                 −1                             −1
             10                              10                              10                              10                              10                                10
               −2                              −2                              −2                              −2                                 −2                             −2
             10                              10                              10                              10                              10                                10
        θ1     −3                       θ2     −3                       θ3     −3                       θ4     −3                       θ5        −3
                                                                                                                                                                           6
                                                                                                                                                                           θ     −3
             10                              10                              10                              10                              10                                10
               −4                              −4                              −4                              −4                                 −4                             −4
             10                              10                              10                              10                              10                                10
                    0            0.05               0            0.05               0            0.05               0            0.05                  0            0.05              0            0.05
                        Probability                     Probability                     Probability                     Probability                        Probability                    Probability
       (b)               Type I                          Type II                        Type III                        Type IV                             Type V                        Type VI
               −1                              −1                              −1                              −1                                 −1                             −1
             10                              10                              10                              10                              10                                10
               −2                              −2                              −2                              −2                                 −2                             −2
             10                              10                              10                              10                              10                                10
        θ1     −3                       θ2     −3                       θ3     −3                       θ4     −3                       θ5        −3
                                                                                                                                                                           6
                                                                                                                                                                           θ     −3
             10                              10                              10                              10                              10                                10
               −4                              −4                              −4                              −4                                 −4                             −4
             10                              10                              10                              10                              10                                10
                    0            0.05               0            0.05               0            0.05               0            0.05                  0            0.05              0            0.05
                        Probability                     Probability                     Probability                     Probability                        Probability                    Probability
Figure 3: Results of Experiment 1, in the form of posterior distributions on concept difficulty from participants’ responses
in (a) the optimized game and (b) the random game; red lines indicate the mean of each distribution. Each panel shows the
distribution over the inferred difficulty of a concept with the given structure (Types I-VI), as reflected by its prior probability in
the Bayesian model. Concepts with higher prior probability are easier to learn. Note the logarithmic scale.
high information gain had true concepts with different struc-                                            for the optimized game and the random game; if a concept
tures. While the information gain found for any given game is                                            has higher prior probability, it will be easier to learn. These
approximate, since we estimated the expectation over possi-                                              distributions were obtained via MCMC using a Metropolis-
ble priors with only 35 samples, this was sufficient to separate                                         Hastings algorithm on both the prior and the noise parame-
poor games from relatively good games; we explore this rela-                                             ter β. Results show samples generated from five chains with
tionship further in Experiment 2.                                                                        100,000 samples each; the first 10% of samples from each
                                                                                                         chain were removed for burn-in.
Experiment Methods                                                                                          Qualitatively, the distributions inferred from the optimized
Participants. A total of 50 participants were recruited online                                           game appear more tightly concentrated than those from the
and received a small amount of money for their time.                                                     random game; this is confirmed by the actual information
Stimuli. Participants played Corridor Challenge with param-                                              gain, which was 3.30 bits for the optimized game and 1.62
eters set based either on an optimized game (expected infor-                                             bits for the random game. This implies that we could halve
mation gain of 3.4 bits) or on a random game (expected in-                                               the number of participants by running the optimized game.
formation gain of 0.6 bits). The symbols differed along the                                                 For both games, the ordering of the mean prior probabili-
dimensions of shape, color, and pattern.                                                                 ties of each type, shown by red lines in Figure 3, is the same
Procedure. Half of the participants were randomly assigned                                               as that found in previous work, except for Type VI. Our in-
to each game design, and played the game in a web browser.                                               ferred distributions for Type VI placed significant probability
The participants were shown text describing the structure of                                             on many values, suggesting that we simply did not gain much
the game, and then played several practice games to familiar-                                            information about its actual difficulty. We do infer that Type
ize them with interface. The first practice game simply had                                              VI is easier than Types III, IV, or V, which has a precedent in
chests labeled “Good” and “Bad”; the next three games used                                               the results of Griffiths et al. (2008).
Boolean concepts of increasing difficulty based on previous
work. All practice games used different symbols from one an-                                                  Experiment 2: Estimating Information Gain
other and from the final game. Practice games used the point                                             To verify the relationship between actual and expected in-
and energy values from the game chosen for their condition                                               formation gain, we conducted a second experiment in which
(i.e., the random game or the game found by the search) in or-                                           players played games with a range of information gains. In
der to make players aware of these values, but the symbols in                                            order to isolate the impact of the symbols on the chests and
the practice games were identical across conditions. The final                                           the true concept we fixed the point structures to those found
game differed by condition. After completing the final game,                                             for the optimized game in Experiment 1 and conducted new
participants were asked to rate how fun and how difficult the                                            searches over the remaining variables. We then selected
game was, both on 7-point Likert scales. Additionally, they                                              games from the search paths that had varying expected infor-
were shown the stimuli and categorization information that                                               mation gains, demonstrating that even without changing the
they observed during the final game, and asked to classify the                                           incentive structure a range of information gains was possible.
remaining stimuli from the game that were not observed.
                                                                                                         Methods
Results                                                                                                  Participants. A total of 175 participants were recruited on-
Figure 3 shows the inferred distribution over the prior prob-                                            line and received the same payment as in Experiment 1.
ability of each concept (θi ) based on participants’ actions                                             Stimuli. Participants played one of seven new games.
                                                                                                   897

                                                                               ing the fit of a partially observable Markov decision process
                                    5                                          model to the fit of an MDP model would help to determine
                                                                               whether this is an issue. In the optimal game design proce-
                                                                               dure, one might also want to explore other metrics than en-
                                    4
                                                                               tropy for measuring a game’s expected utility. For instance,
          Actual Information Gain
                                                                               one might use a loss metric that considers the distance of
                                    3                                          samples from the true value of θ. Finally, it would be in-
                                                                               teresting to explore whether there are advantages beyond mo-
                                    2
                                                                               tivation for using games rather than traditional experiments.
                                                                               While there remain many areas for future exploration, this
                                                                               work gives a starting point for designing highly informative
                                    1                                          games and gives experimental support that these games can
                                                                               provide meaningful data for cognitive science.
                                    0
                                     0   1       2      3       4    5         Acknowledgements. This work was supported by a DoD NDSEG
                                         Expected Information Gain             Fellowship to ANR, a Google Ph.D. Fellowship to MZ, and NSF
                                                                               grant number IIS-0845410 to TLG.
Figure 4: Results of Experiment 2, showing expected versus
actual information gains (r = 0.72). Each circle represents a                                             References
game, and the least-squares regression line is shown.                          Andrade, G., Ramalho, G., Santana, H., & Corruble, V. (2005).
                                                                                 Extending reinforcement learning to provide dynamic game bal-
                                                                                 ancing. In Proceedings of the Workshop on Reasoning, Represen-
Procedure. Procedure matched Experiment 1.                                       tation, and Learning in Computer Games, 19th IJCAI (pp. 7–12).
                                                                               Atkinson, A., Donev, A., & Tobias, R. (2007). Optimum experimen-
Results                                                                          tal designs, with SAS (Vol. 34). New York: Oxford University
                                                                                 Press.
We compared the actual and expected information gains for                      Baker, C. L., Saxe, R. R., & Tenenbaum, J. B. (2009). Action
                                                                                 understanding as inverse planning. Cognition, 113(3), 329–349.
the seven new games and the optimized game from Experi-                        Chaloner, K., & Verdinelli, I. (1995). Bayesian experimental design:
ment 1, all of which used the same point structure. As shown                     A review. Statistical Science, 10(3), 273–304.
in Figure 4, we found that expected and actual information                     Erev, I., & Roth, A. (1998). Predicting how people play games: Re-
                                                                                 inforcement learning in experimental games with unique, mixed
gain were positively correlated (r(6) = 0.72, p < 0.05). This                    strategy equilibria. American Economic Review, 88(4), 848–881.
demonstrates that the design of the game does influence how                    Feldman, J. (2000). Minimization of Boolean complexity in human
much information we can infer from human players’ actions,                       concept learning. Nature, 407, 630-633.
                                                                               Gilks, W., Richardson, S., & Spiegelhalter, D. J. (Eds.). (1996).
and that this gain is predicted by our estimates.                                Markov chain Monte Carlo in practice. Suffolk, UK: Chapman
                                                                                 and Hall.
                                             Conclusion                        Griffiths, T. L., Christian, B. R., & Kalish, M. L. (2008). Using cate-
                                                                                 gory structures to test iterated learning as a method for identifying
We have presented a general framework for the optimal de-                        inductive biases. Cognitive Science, 32, 68-107.
sign of games for cognitive science experiments that adapts                    Kirkpatrick, S., Gelatt, C., & Vecchi, M. (1983). Optimization by
ideas from Bayesian experiment design. Our methodology                           simulated annealing. Science, 220(4598), 671–680.
                                                                               Klopfer, E. (2008). Augmented learning: Research and design of
hinges on using Markov decision processes to infer cogni-                        mobile educational games. Cambridge, MA: The MIT Press.
tively relevant information from players’ actions. By com-                     Michael, D., & Chen, S. (2005). Serious games: Games that edu-
bining this model with a stochastic search method, we were                       cate, train, and inform. Boston, MA: Thomson Course Technol-
                                                                                 ogy.
able to find appreciably more informative games for Boolean                    Myung, J., & Pitt, M. (2009). Optimal experimental design for
concept learning. Our experimental results demonstrate that                      model discrimination. Psychological Review, 116(3), 499.
using this framework to infer concept difficulty gives simi-                   Nosofsky, R. M., Gluck, M., Palmeri, T. J., McKinley, S. C., &
                                                                                 Glauthier, P. (1994). Comparing models of rule-based classifica-
lar results to prior work, and that the expected information                     tion learning: A replication and extension of Shepard, Hovland,
gain of a game predicts the actual information gain when the                     and Jenkins (1961). Memory & Cognition, 22, 352-369.
game is played by real players. One of the chief advantages of                 Shepard, R. N., Hovland, C. I., & Jenkins, H. M. (1961). Learning
                                                                                 and memorization of classifications. Psychological Monographs,
this framework is its applicability to a wide variety of scenar-                 75. (13, Whole No. 517)
ios, from creating other types of games that examine different                 Siorpaes, K., & Hepp, M. (2008). Games with a purpose for the
psychological questions to designing optimally informative                       semantic web. Intelligent Systems, 23(3), 50–60.
                                                                               Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning. Cam-
assessments within educational software.                                         bridge, MA: MIT Press.
   There are a number of ways in which this initial test of                    Tan, C., & Cheng, H. (2009). IMPLANT: An integrated MDP and
a framework for designing games could be expanded. In                            POMDP learning ageNT for adaptive games. In Proceedings of
                                                                                 The Artificial Intelligence and Interactive Digital Entertainment
this work, we ignored the value of information for compu-                        Conference (pp. 94–99).
tational tractability. However, people may consider how their                  Von Ahn, L. (2006). Games with a purpose. Computer, 39(6),
future knowledge will be affected by their current actions,                      92–94.
leading to deviations from our model’s predictions; compar-
                                                                         898

