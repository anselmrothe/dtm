UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Explanations of Counterfactual Inferences
Permalink
https://escholarship.org/uc/item/6m90461j
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Edwards, Brian
Rips, Lance
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                     Explanations of Counterfactual Inferences
                                   Brian J. Edwards (Brian.Edwards@u.northwestern.edu)
                Department of Psychology, Northwestern University, 2029 Sheridan Rd., Evanston, IL 60208 USA
                                              Lance J. Rips (Rips@northwestern.edu)
                Department of Psychology, Northwestern University, 2029 Sheridan Rd., Evanston, IL 60208 USA
                              Abstract                                   the actual state to the counterfactual state and then tracing
   When engaging in counterfactual thought, people must
                                                                         the consequences of that intervention (Pearl, 2000; see also
   imagine changes to the actual state of the world. In this study,      Woodward, 2003). The intervention severs the causal link
   we investigated how people reason about counterfactual                between the antecedent and its immediate causes, and as a
   scenarios by asking participants to make counterfactual               result of this “graph surgery,” the counterfactual states of
   inferences about a series of causal devices (i.e., answer             upstream events would be the same as in the actual world.
   questions such as If component X had not operated [had                However, downstream events that are a consequence of the
   failed], would components Y, Z, and W have operated?) and             antecedent would change states according to the causal laws
   to explain their reasoning. Participants avoided breaking
   deterministic causal links (i.e., W always causes X), but were        governing the system. To illustrate this approach, consider a
   willing to break probabilistic causal links (i.e., W sometimes        causal chain A → B → C and a counterfactual antecedent If
   causes X) to keep prior causal events in the same states as in        B had not occurred… (in the actual world, A, B, and C all
   the actual world. Participants’ explanations supported this           occurred). A person using pruning theory would intervene
   pattern of inferences. When the causal links were                     on B to change the state of B from present to absent. Since
   deterministic, participants reasoned diagnostically to infer that     upstream events (A) are unaffected by this intervention, A
   the states of prior causal events would have been different in
                                                                         would still have been present in the counterfactual world.
   the counterfactual world. In contrast, when the links were
   probabilistic, participants cited the links’ unreliability as an      But since C is an effect of B, B’s absence would in turn
   explanation for why the states of prior causal events would           cause C to be absent.
   have been the same as in the actual world. Additionally,                 Pruning theory might appeal to reasoners in two ways.
   participants who were told that a component “had failed” (vs.         First, by keeping all the events that are causally prior to the
   “had not operated”) were more likely to attribute the state of        antecedent in the same states as in the actual world, pruning
   that component to it being “internally broken” and infer that         theory creates a counterfactual world that is maximally
   causally upstream components would have operated. Our
   results suggest that people use their explanation of the
                                                                         similar to the actual world with respect to these prior events.
   antecedent event (the “if” clause) to guide their counterfactual      Second, the pruning approach makes counterfactual thinking
   inferences. We discuss the implications of these findings for         computationally easy. The strategy of always keeping prior
   two rival Bayes-net theories of counterfactual reasoning:             events in their original states allows reasoners to avoid the
   Pearl’s (2000) and Hiddleston’s (2005).                               cognitively challenging process of reasoning backwards to
   Keywords: Counterfactuals, causation, explanation                     determine the counterfactual states of upstream causes.
                                                                            However, other researchers have questioned whether the
                          Introduction                                   type of change pruning theory proposes is necessarily the
                                                                         most reasonable way to modify the causal system in the
People often engage in counterfactual reasoning (e.g., If I              counterfactual situation (e.g., Hiddleston, 2005). One
hadn’t partied the night before the exam, then I would have              criticism of pruning theory is that it is very disruptive to the
passed the exam) to second-guess decisions, attribute credit             structure of a causal system and can require reasoners to
or blame, and diagnose causal relations (see Byrne, 2005,                violate causal laws. Consider a deterministic causal system
for a review). Reasoning about counterfactual scenarios                  in which A, without exception, always causes B. In this
such as the preceding example requires imagining changes                 setting, one might be reluctant to imagine a counterfactual
to the actual state of the world—for instance, imagining a               world in which A occurred, but B did not occur (e.g., in
counterfactual world in which I hadn’t partied the night                 answering the question If B had not occurred, would A have
before the exam. One of the central issues in the study of               occurred?). Thus, when reasoning about this counterfactual
counterfactual reasoning is how people re-imagine the world              scenario, one might be more likely to infer that the reason B
to satisfy the antecedent of a counterfactual scenario. (The             did not occur was that A did not occur, and the absence of A
antecedent is the “if” clause, and we will refer to the “then”           caused B to be absent too (Hiddleston, 2005). We will call
clause as the consequent.) In particular, what types of events           this alternative minimal-network theory. When the causal
do people keep the same in the actual and counterfactual                 links are probabilistic (i.e., A sometimes causes B), however,
worlds and what types of events do people change?                        minimal-network theory proposes that A might or might not
   One way people might reason about counterfactual                      have occurred, since either possibility is “legal” in
scenarios, which we will call pruning theory, is by using an             accordance with the system’s causal laws.
intervention to change the state of the antecedent event from
                                                                     318

   Table 1 compares the predictions of pruning theory and         inferences about the operating states of the other
minimal-network theory for a device in which component            components. Specifically, we predict that participants in the
A’s operating usually causes component B to operate and           not operated condition will reason diagnostically about the
component B’s operating always causes component C to              states of the other components based on the device’s causal
operate (at present, all three components are operating). The     structure, consistent with minimal-network theory. In
device’s structure is illustrated as follows:                     contrast, we predict that participants in the failed condition
                                                                  will reason that since the antecedent component is broken,
                                                                  its operating state is not diagnostic of the states of the other
                                                                  components. Thus, participants will break the causal links
                                                                  between the antecedent and its causes and infer that causally
     Table 1: Comparison of Pruning Theory and Minimal-
                                                                  prior components would have operated in the counterfactual
                         network Theory
                                                                  situation, consistent with pruning theory. In addition to
                                                                  examining participants’ inferences about which components
              If component B had         If component C had
                                                                  would and would not have operated in the counterfactual
              not operated, would        not operated, would
                                                                  situation, we analyzed participants’ explanations of their
              component A have           component B have
                                                                  reasoning. In Experiment 1, we analyzed people’s
              operated?                  operated?
                                                                  explanations of why they thought the non-antecedent
Pruning                  Yes                       Yes
                                                                  components would or would not have operated. In
Theory
                                                                  Experiment 2, we analyzed people’s explanations of why
Minimal-               Maybe                       No             the antecedent event would have occurred.
network
Theory
                                                                                          Experiment 1
   Previous empirical work has explored whether people’s          Participants in this experiment received a series of problems
counterfactual inferences are consistent with either of these     about a set of eight hypothetical devices, each with four
two theories of counterfactual reasoning. In one experiment,      components. For each device, they answered counterfactual
Sloman and Lagnado (2005) presented people with causal            questions of the form If component X had not operated [had
information about a simple rocket-ship device with the            failed], would components Y, Z, and W have operated? and
causal structure A → B and asked them a variety of                provided explanations justifying their reasoning.
counterfactual questions. Sloman and Lagnado found
evidence that people engaged in pruning when they were            Method
told that a component was prevented from operating, but not
when told that the component was observed not to have             Materials. The questionnaire booklets contained three
operated. However, subtle differences in wording across           pages of instructions followed by 24 pages of questions. The
their experiments led to significantly different patterns of      instructions explained the experimental task and told
counterfactual inferences, making it difficult to generalize      participants how to interpret the diagrams of the causal
from the data. In another study, Rips (2010) asked people         devices on the following pages. Each question page
counterfactual questions about three- and four-component          contained a written description of how a device operated
mechanical devices. Although participants’ counterfactual         (e.g., Component A’s operating always causes component B
inferences did not provide strong support for either pruning      to operate, etc.), which was accompanied by the
theory or minimal-network theory, their inferences were           corresponding diagram in Figure 1.
more closely aligned with minimal-network theory (see also           As shown in Figure 1, there were eight different causal
Dehghani, Iliev, & Kaufmann, 2012).                               devices, all of which had “diamond” structures. The devices
   In the two experiments in this study, we presented             varied in whether the causal links between components were
participants with counterfactual questions for which pruning      deterministic (solid lines in Figure 1) or probabilistic
theory and minimal-network theory make different                  (dashed lines), and whether components B and C had to
predictions. The wording of these questions was                   operate together to cause D to operate (arc connecting links
manipulated across two between-subjects conditions. One           in Figure 1) or could independently cause component D to
group of participants was told that a component of a              operate (no arc). The order of the devices was
mechanical device “had not operated,” and another group           counterbalanced across participants. We used devices with
was told that the component “had failed” (e.g., If                diamond structures for two reasons. First, previous causal
Component B had not operated/had failed…). The neutral            reasoning studies have used diamond structures and have
“had not operated” wording does not suggest a particular          found that people make accurate causal inferences about
explanation for the state of the component; however, the          these systems (Meder, Hagmayer, & Waldmann, 2008,
“had failed” wording suggests an explanation that is local to     2009). Second, pruning theory and minimal-network theory
the component (e.g., the component is internally broken).         make different predictions for many of the counterfactual
Thus, we predict that participants in the not operated and        questions about these devices.
failed conditions will make different counterfactual
                                                              319

   After learning how each device works, participants were           For each counterfactual question, participants indicated
told the device’s current operating state, which was always       which of the three non-antecedent components would have
that “at present, components A, B, C, and D are all               operated in the counterfactual state. For each component,
operating.” Next, participants were asked a counterfactual        participants could say that the component (1) would have
question about the device, such as If component B had not         operated, (2) would not have operated, or (3) might or might
operated, would components A, C, and D have operated?             not have operated. To gain insight into how participants
For each of the eight devices, participants answered three        were reasoning about the counterfactual questions,
counterfactual questions, one question each with A, B, and        participants also indicated the order in which they reasoned
D as the antecedent component. Since the devices were             about the non-antecedent components. After making these
symmetric with respect to components B and C, we did not          inferences, participants justified their answers by responding
ask a separate question in which C was the antecedent. The        to the prompt “Please explain why you answered in the way
order of the antecedent components for these questions            you did.”
(ABD vs. DBA) was balanced across participants.
                                                                  Procedure. Participants received the questionnaire booklet
    Figure 1: Causal Devices Used in Experiments 1 and 2          from the experimenter and answered the questions at their
                                                                  own pace. The experiment took approximately 30 minutes
                                                                  to complete.
                                                                  Participants. Participants were 32 undergraduate students
                                                                  at Northwestern University. Participants received course
                                                                  credit for their participation.
                                                                  Results and Discussion
                                                                  We analyzed participants’ answers to the counterfactual
                                                                  questions (e.g., If component B had not operated, would
                                                                  component A have operated?) to see if their inferences were
                                                                  consistent with minimal-network theory or pruning theory.
                                                                  Responses of “would have operated” were scored as +1,
                                                                  responses of “would not have operated” were scored as -1,
                                                                  and responses of “might or might not have operated” were
                                                                  scored as 0. The mean score for participants was higher in
                                                                  the failed condition (M = -0.14) than in the not operated
                                                                  condition (M = -0.43), F(1, 32) = 7.07, MSe = 7.29, p = .01.
                                                                     In two cases, pruning theory and minimal-network theory
                                                                  make the same predictions: (1) when component A was the
                                                                  antecedent, and (2) for the devices in which components B
                                                                  and C must both operate in order for component D to
                                                                  operate (jointly caused devices), when component B was the
                                                                  antecedent and component D was the consequent. In case
   In this figure, solid arrows indicate deterministic links      (1), both theories say that components B, C, and D would all
   (e.g., A always causes B) and dashed arrows indicate           not have operated, and in case (2), both theories say that
   probabilistic links (e.g., A usually causes B). All causal     component D would not have operated. For all the other
   relationships are in the direction shown by the arrows.        counterfactual questions, pruning theory predicts that the
   The arcs indicate that component B and component C             consequent component definitely would have operated
   operating together cause component D to operate, but           (producing positive scores), whereas minimal-network
   component B or component C operating alone never               theory predicts that the consequent component either (a)
   causes component D to operate (jointly caused devices).        definitely would not have operated or (b) might or might not
   The absence of an arc indicates that component B or            have operated (producing negative or 0 scores respectively).
   component C operating alone causes component D to                 When we restricted our analysis to the cases in which
   operate (separately caused devices).                           pruning theory and minimal-network theory make different
                                                                  predictions, the mean score for participants in the failed
   Participants were randomly assigned to one of two              condition was 0.17 and the mean score for participants in
experimental conditions. In the not operated condition,           the not operated condition was -0.27. As was the case with
participants learned that the antecedent component                the entire data set, the difference between conditions was
(component B in the preceding example) “had not                   significant, F(1, 32) = 11.96, MSe = 6.27, p =.002. The
operated.” In the failed condition, participants learned that     mean score for the not operated condition was significantly
the antecedent component “had failed.”                            less than 0, t(17) = -4.50, p < .001; however, the mean score
                                                              320

for the failed condition was not significantly different from      have operated than participants in the failed condition, and
0, t(17) = 1.68, n.s.                                              they made inferences that were better predicted by minimal-
   Next, we examined the serial order (1, 2, or 3) in which        network theory. The analysis of participants’ explanations
participants reasoned about the three non-antecedent               also showed that most participants in the not operated
components. The most interesting case is the one in which          condition used causal backtracking to diagnose the
component B was the antecedent since participants could            counterfactual operating states of upstream components. In
work their way downstream (i.e., reason about component D          contrast, participants in the failed condition were more
first) or upstream (i.e., reason about component A first).         likely than participants in the not operated condition to say
Most participants (69%) started upstream, reasoning about          that the operating states of upstream components were
component A before component D (Binomial test, p < .001).          independent of, and could not be diagnosed from the state of
The mean serial position for component A was 1.44,                 the antecedent.
whereas the mean position for component D was 2.32. The
order in which participants reasoned about the components                                  Experiment 2
did not differ across the failed and not operated conditions,.     The pattern of inferences and reasoning strategies in
   We also examined participants’ explanations of their            Experiment 1 suggests that participants in the not operated
counterfactual reasoning to see if the explanations were           and failed conditions may have generated different
consistent with pruning theory or minimal-network theory.          explanations for why the antecedent component had not
We classified explanations in two ways.                            operated. We therefore performed a second experiment to
   (1) Explanations were coded as causal backtracking if           investigate the possible relationship between participants’
participants used the state of the antecedent component to         explanations of why the antecedent component had not
reason diagnostically about the states of upstream                 operated and their counterfactual inferences.
components. A sample causal-backtracking explanation was
“If B wasn’t operating that would mean A wasn’t working
                                                                   Method
since A always causes B.” Causal-backtracking explanations
are consistent with minimal-network theory.                        The experiment contained two parts, an inference task and
   (2) Explanations were coded as causes are independent of        an explanation task. The same eight causal devices from
effects if they suggested that the states of upstream “cause”      Experiment 1 were used in Experiment 2 (see Figure 1). As
components are not affected by the states of downstream            in Experiment 1, participants were randomly assigned to
“effect” components. A sample explanation was “Neither A,          either the not operated condition or the failed condition.
B, nor C are dependent on D so they all will have operated.”
Such an explanation is consistent with pruning theory.             Materials.
   Notice that these three types of explanations are only             Inference task: The inference task was identical to
applicable when there are components that are causally             Experiment 1 except that participants did not provide
upstream of the antecedent component. Thus, we restricted          explanations of their counterfactual inferences during this
the following analyses to the counterfactual questions in          part of the experiment.
which B or D was the antecedent. The data were coded by a             Explanation task: In the explanation task, participants
person who was unfamiliar with the experimental                    described why the antecedent component had not operated.
hypotheses, and 25% of the data were coded independently           Note that this is a different type of explanation than the ones
by a second coder. Inter-coder reliability was 90%.                participants provided in Experiment 1; in Experiment 1,
   Participants in the not operated condition were                 participants explained why the non-antecedent components
significantly more likely to provide “causal-backtracking”         would or would not have operated. The explanation-task
explanations than participants in the failed condition (65%        booklet included three pages of instructions followed by 24
vs. 32% respectively, F(1,24) = 12.9, MSe = 16.4, p = .001).       pages of questions. As in the inference task and Experiment
In contrast, participants in the failed condition were             1, participants received information about how the causal
significantly more likely to provide “causes are independent       devices work and told that “at present, components A, B, C,
of effects” explanations than participants in the not operated     and D are all operating.” Participants in the not operated
condition (25% vs. 9% respectively, F(1, 21) = 5.57, MSe =         condition were asked questions of the form If component X
3.90, p = .03). Participants in the not operated condition         had not operated, which of the following would best explain
were significantly more likely to provide “causal-                 why? Participants in the failed condition were asked a
backtracking” explanations than “causes are independent of         question that was identical except that “not operated” was
effects” explanations (t(14) = 6.33, p < .001); however,           replaced by “failed.” For each device, participants answered
participants in the failed condition did not significantly         this question for each of components A, B, and D as the
prefer either type of explanation.                                 antecedent. For each participant, the order of the devices,
   In sum, participants in the not operated and failed             and within each device, the order of the antecedent
conditions differed in their counterfactual inferences.            components, was the same in the inference and explanation
Participants in the not operated condition had a stronger          tasks.
tendency to say that non-antecedent components would not              When component B was the antecedent, participants
                                                                   selected an explanation from the following list:
                                                               321

   (1) Component B was internally broken.                           classified as pruning explanations. When the links between
   (2) Factors external to the device prevented component B         the antecedent and its causes were deterministic,
   from operating.                                                  explanation 4 was classified as a minimal-network
   (3) Component B operates unreliably, and component B             explanation. When the links between the antecedent and its
   just didn’t operate this time.                                   causes were probabilistic, explanation 5 was classified as a
   (4) Component A did not operate, which in turn caused            minimal-network explanation. All other responses were
   component B not to operate.                                      classified as “other.” Since neither explanation 4 nor
   (5) Component A operated, but component B just didn’t            explanation 5 (the two possible minimal-network
   operate this time because the connection between                 explanations) is applicable when component A was the
   component A and component B is unreliable.                       antecedent, the following analyses were conducted only for
   (6) Component A operated, but the connection between             the counterfactual questions in which component B or D
   component A and component B was broken.                          was the antecedent.
                                                                       Participants were significantly more likely to choose
   The list of explanations was similar when component D            minimal-network explanations than pruning explanations
was the antecedent, except that “component D” was                   (61% vs. 21% respectively, t(31) = 5.31, p < .001). This
substituted for “component B” and “component B and/or1              pattern was observed in both the not operated (60% vs. 14%
component C” was substituted for “component A.” When                respectively, t(15) = 4.92, p < .001) and failed conditions
component A was the antecedent, only the first three answer         (61% vs. 27% respectively, t(15) = 2.85, p = .01). Notice
choices were included since component A’s operation is not          that participants in the failed condition were significantly
caused by other components. The order of the answer                 more likely to choose pruning explanations than participants
choices (above order vs. reverse order) was balanced across         in the not operated condition (F(1,29) = 4.56, MSe = 3.04, p
participants.                                                       = .04). The results are shown in Figure 2.
   After choosing an explanation, participants rated their
confidence on a 0-9 scale with one-point increments, where               Figure 2: Percent of Minimal-Network and Pruning
0 = “not at all confident” and 9 = “extremely confident.”                             Explanations by Condition
Procedure. Half of the participants completed the inference
task followed by the explanation task and the remaining
participants completed the explanation task followed by the
inference task. Each task took approximately 20 minutes
with the entire experiment taking approximately 40 minutes.
Participants. Participants were 32 undergraduate students
at Northwestern University who had not participated in
Experiment 1. Participants received course credit for their
cooperation.
Results and Discussion
Inference Task. The inference task replicated the findings
of Experiment 1. The mean score for participants in the
failed condition was significantly higher than for
participants in the not operated condition. This was true for
all counterfactual questions (M = -0.16 vs. M = -0.48                  Interestingly, when the causal links between the
respectively, F(1,30) = 14.47, MSe = 4.14, p < .001) and for        antecedent and its causes were probabilistic, participants in
the subset of counterfactual questions for which pruning            both conditions were significantly more likely to choose a
theory and minimal-network theory make different                    minimal-network explanation (e.g., Component A operated,
predictions (M = 0.27 vs. M = -0.25 respectively,                   but component B just didn’t operate this time because the
F(1, 30) = 19.27, MSe = 4.94, p < .001).                            connection between component A and component B is
                                                                    unreliable) than a pruning explanation (e.g., Component B
Explanation Task. Participants’ explanations were coded             was internally broken; Factors external to the device
as consistent with pruning theory, consistent with minimal-         prevented component B from operating; Component A
network theory, or consistent with neither theory.                  operated, but the connection between component A and
Explanations 1, 2, and 6 (see Method section) were                  component B was broken), (Not operated condition: t(15) =
                                                                    5.81, p < .001, Failed condition: t(15) = 5.00, p < .001). All
   1
     If either component B or component C operating alone could     these explanations (both the pruning and minimal-network
cause component D to operate, the “and” wording was used.           explanations) imply, and in some cases state explicitly, that
Otherwise, the “or” wording was used.                               causally upstream components would have operated. Even
                                                                322

though this counterfactual state is consistent with both           upstream components would have operated. Other studies
pruning theory and minimal-network theory, participants in         that have varied the wording of counterfactual questions
both conditions preferred minimal-network explanations.            have found similar effects (Sloman & Lagnado, 2005).
   As in Experiment 1, minimal-network theory better                  Each type of wording supports a particular (and different)
explained the inferences of participants in the not operated       explanation for the counterfactual antecedent. The
condition compared to pruning theory. While participants in        differences in participants’ explanations across conditions
the failed condition were more likely than participants in the     suggest that these explanations may in turn shape
not operated condition to say that non-antecedent                  participants’ counterfactual inferences. Hempel (1965)
components would have operated, participants in both               famously proposed that causal explanations support
conditions preferred minimal-network explanations over             predictive inferences, and our data suggest such a
pruning explanations. Thus, Experiment 2 suggests that             connection between explanation and inference in
minimal-network theory might provide a starting point for a        counterfactual reasoning (Goodman, 1955). Specifically, we
good psychological theory of counterfactual reasoning.             propose that when engaging in counterfactual reasoning,
                                                                   people integrate their explanation of the counterfactual
                   General Discussion                              antecedent with their knowledge of the system’s causal
In the two experiments in this paper, we examined (1)              structure to infer the system’s counterfactual state.
participants’ counterfactual inferences about the states of
variables in a causal system and (2) participants’                                     Acknowledgements
explanations of their reasoning. Alternative theories of           We thank Ben Rottman, Steven Sloman, and members of
counterfactual reasoning such as pruning theory and                the Northwestern University Higher Level Cognition
minimal-network theory make different predictions about            Laboratory for their valuable feedback on these
how people should modify (or preserve) the system’s causal         experiments. We thank Samantha Thompson and Joanna
structure when reasoning about a counterfactual scenario.          Westerfield for their research assistance. The research was
   A defining characteristic of pruning theory is the proposal     supported by an NSF graduate research fellowship (BJE)
that people treat counterfactuals as interventions. Under this     and IES Grant R305A080341 (LJR).
account, people should simulate the counterfactual state by
intervening on the causal system, and we would expect them                                  References
to break both probabilistic and deterministic causal links         Byrne, R. M. J. (2005). The rational imagination: How
and say that upstream components would have operated.                 people create alternatives to reality. Cambridge, MA:
Furthermore, they should endorse an interventionist                   MIT Press.
explanation for the counterfactual state of the antecedent         Dehghani, M., Iliev, R., & Kaufmann, S. (2012). Causal
component, such as “factors external to the device prevented          explanation and fact mutability in counterfactual
the antecedent component from operating.”                             reasoning. Mind and Language.
   Our data provide evidence against this hypothesis.              Goodman (1955). Fact, fiction, and forecast. Cambridge,
Participants in the neutrally worded not operated condition           MA: Harvard University Press.
made counterfactual inferences that preserved deterministic        Hempel, C. G. (1965). Aspects of scientific explanation. In
causal relationships between components’ operating states.            C. G. Hempel, Aspects of scientific explanation and other
When the causal links between the antecedent component                essays in the philosophy of science (pp. 331-496). New
and its causes were deterministic, participants inferred that         York: Free Press.
the antecedent component’s causes would not have                   Hiddleston, E. (2005). A causal theory of counterfactuals.
operated, which in turn caused the antecedent component               Nous, 39, 632-657.
not to operate. However, when the causal links were                Meder, B., Hagmayer, Y., & Waldmann, M. R. (2008).
probabilistic, participants inferred that the antecedent              Inferring interventional predictions from observational
component’s causes would have operated, but the                       learning data. Psychonomic Bulletin & Review, 15, 75-80.
antecedent component would not have operated because the           Meder, B., Hagmayer, Y., & Waldmann, M. R. (2009). The
links were unreliable. These inferences and explanations are          role of learning data in causal reasoning about
consistent with minimal-network theory, which proposes                observations and interventions. Memory & Cognition, 37,
that people should prefer “legal” counterfactual states that          249-264.
preserve the system’s (deterministic) causal laws, but they        Pearl, J. (2000). Causality. Cambridge, UK: Cambridge
are inconsistent with pruning theory.                                 University Press.
   We also found that participants in the not operated and         Rips, L. J. (2010). Two causal theories of counterfactual
failed conditions reasoned differently about the                      conditionals. Cognitive Science, 34, 175-221.
counterfactual scenarios. The failed wording suggested to          Sloman, S. A., & Lagnado, D. A. (2005). Do we "do"?
participants that the antecedent component was internally             Cognitive Science, 29, 5-39.
broken. Accordingly, these participants modified the               Woodward, J. (2003). Making things happen: A theory of
devices’ causal structure by breaking the causal links                causal explanation. New York: Oxford University Press.
between the antecedent and its causes, and they inferred that
                                                               323

