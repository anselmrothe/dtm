UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Temporal Dynamics of Action Perception: The Role of Biological Appearance and Motion
Kinematics
Permalink
https://escholarship.org/uc/item/2bm7s8tv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Urgen, Burcu Aysen
Plank, Markus
Ishiguro, Hiroshi
et al.
Publication Date
2012-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                                    Temporal Dynamics of Action Perception:
                      The Role of Biological Appearance and Motion Kinematics
                                        Burcu Aysen Urgen (burgen@cogsci.ucsd.edu)
                              Department of Cognitive Science, University of California, San Diego
                                            9500 Gilman Drive, La Jolla, CA 92093 USA
                                                Markus Plank (mplank@ucsd.edu)
                              Institute for Neural Computation, University of California, San Diego
                                            9500 Gilman Drive, La Jolla, CA 92093 USA
                                      Hiroshi Ishiguro (ishiguro@ams.eng.osaka-u.ac.jp)
                                Department of Adaptive Machine Systems, Osaka University
                                                          Suita, Osaka, Japan
                                             Howard Poizner (hpoizner@ucsd.edu)
                              Institute for Neural Computation, University of California, San Diego
                                            9500 Gilman Drive, La Jolla, CA 92093 USA
                                         Ayse Pinar Saygin (saygin@cogsci.ucsd.edu)
                              Department of Cognitive Science, University of California, San Diego
                                            9500 Gilman Drive, La Jolla, CA 92093 USA
                             Abstract                                    neural systems that support action and body movement
     We studied action perception and the role of visual form            perception is currently an active research area in
     and visual motion kinematics of the observed agent using a          cognitive science and neuroscience.
     stimulus set of human and humanoid robot actions and                     Artificial agents such as robots can perform
     electroencephalogram (EEG). Participants viewed 2s.                 recognizable body movements, but can have varying
     videos of three agents (Human, Android, Robot) performing           degrees of biological appearance (form) and motion. As
     recognizable actions: Human had biological form and                 such, they provide us with an opportunity to study the
     motion, Android had biological form and non-biological
     motion, and Robot had non-biological form and non-                  specificity of neural responses to the seen agent’s form
     biological motion. Early in processing (P200), Robot was            and motion (as well as mismatches between the two). A
     distinguished from the other agents, likely due to low-level        prominent idea in action perception is simulation theory,
     visual properties of the stimuli. We found a right temporal         whereby others’ actions are understood via an internal
     N170, which was most pronounced for Human, indicating               sensorimotor simulation of the seen action in our own
     possible modulation of this face- and body-sensitive ERP            body representations (Barsalou, 2009). Supporting this,
     component by biological motion. There was a centro-
                                                                         neural activity for action perception shows modulation
     parietal negativity (N300) that was most pronounced for
     Robot, and a later one (N400) for Human and Android. In             by the degree of similarity between the observed action
     the same time period (N300), Android was distinguished in           or actor, and the observers’ own body (Buccino et al.,
     the frontal channels from the other agents. A late positivity       2004; Calvo-Merino, Grezes, Glaser, Passingham, &
     (P600) distinguished Human, again in frontal channels.              Haggard, 2006; Rizzolatti & Craighero, 2004). In terms
     These results highlight differential spatiotemporal cortical        of artificial agents such as robots, one might thus
     patterns during action perception depending on the viewed           predict that increasing human-likeness engages
     agent’s form and motion kinematics.
                                                                         simulation mechanisms more effectively.
     Keywords: action perception; body perception; biological                 On the other hand, human resemblance is not
     motion; social robotics; artificial agents; neuroimaging;           necessarily always a positive feature in artificial agent
     EEG, ERP; uncanny valley                                            design. According to the uncanny valley theory, as an
                                                                         agent is made more human-like, the reaction to it
                          Introduction                                   becomes more and more positive and empathetic, until
Successfully perceiving and understanding others’ body                   a point is reached at which the agent becomes oddly
movements is of biological significance, from hunting                    repulsive (Mori, 1970), an effect well-known in
prey and avoiding predators, to communication and                        robotics and animation. Despite anecdotal evidence,
social interaction. The functional properties of the                     there is little scientific data to characterize the uncanny
                                                                         valley (MacDorman & Ishiguro, 2006; Saygin,
	                                                                 1	  
                                                                         2469

Chaminade, Ishiguro, Driver, & Frith, 2011;                     hand, biological motion is important for engaging
Steckenfinger & Ghazanfar, 2009).                               simulation, Android responses are instead expected to
     Previous studies on the perception of actions of           show the same pattern as the Robot. As for the uncanny
humanoid robots have not found consistent results for           valley theory, we would expect neural responses for the
or against simulation theory (Chaminade & Cheng,                Android to be distinct from the other conditions. Of
2009). In a recent fMRI study, a more complex                   course, the simulation theory and the uncanny valley
relationship between neural responses and the human-            theory are not mutually exclusive, and there may be
likeness of the observed agent was observed (including          evidence for both, possibly at different brain regions
potential neural signals related to the uncanny valley),        and in different time periods.
suggesting that focusing on simulation theory may be
too narrow (Saygin, Chaminade, & Ishiguro, 2010).                                       Methods
Furthermore, the specific role of biological appearance         Participants
or biological motion in action processing have not been         Twelve adults participated in the study. Participants
sufficiently explored in previous work, but is an area of       were recruited from the student community at the
interest in both social robotics and cognitive                  University of California, San Diego (3 females, mean
neuroscience (Chaminade, Hodgins, & Kawato, 2007;               age: 24.4). All participants were right-handed, had
Kanda, Miyashita, Osada, Haikawa, & Ishiguro, 2008;             normal or corrected-to-normal vision and no history of
Saygin, Chaminade, Urgen, & Ishiguro, 2011).                    neurological disorders. Participants were either paid $8
     Although fMRI studies have identified the brain            per hour or received course credit for their participation.
areas that are involved in action observation, much less        They were informed about the nature of the study and
is known about temporal aspects of body movement                signed consent forms in accordance with the UCSD
processing (Hirai, Fukushima, & Hiraki, 2003; Jokisch,          Human Research Protections Program.
Daum, Suchan, & Troje, 2005; Krakowski et al., 2011;
Press, Cook, Blakemore, & Kilner, 2011). Since action           Stimuli and Procedure
processing is a naturally temporally unfolding event, it        The experimental stimuli consisted of 2-second videos
is important to further study its neural dynamics.              of three agents performing recognizable actions: A
     In the present study, we manipulated the form and          Human, an Android, and a Robot (Figure 1).
the motion of the observed agent and recorded neural
activity in the human brain using high-density
electroencephalography (EEG), which allows us to
investigate neurophysiological processes on a
millisecond time scale. We used a unique stimulus set
of well-matched human and humanoid robot actions
(Saygin, Chaminade, Urgen et al., 2011). The stimuli
consisted of videos of three agents: Human, Android,
and Robot (Figure 1). Human had biological form and
motion, Android had biological form and non-
biological motion, and Robot had non-biological form
and non-biological motion. The latter two were actually
the same robot videotaped in two different appearances,
but with identical kinematics. Another dimension of the          Figure 1. Still frames from a drinking action for Robot,
stimuli was the congruence in the form and movement                 Android and Human agents and the experimental
kinematics of the agents. Whereas Human and Robot                        features of interest (form and motion).
had congruence in their form and movement kinematics
(both being biological or non-biological, respectively),        The Android was Repliee Q2 (Ishiguro, 2006), and the
Android had incongruence in its form and movement               Robot condition was the same robot in a modified
kinematics as it had a biological appearance but non-           appearance (Saygin, Chaminade, Ishiguro et al., 2011).
biological movement kinematics.                                 We recorded EEG as participants watched video clips
     Our goal is to study the temporal dynamics of              of the 3 agents carrying out five different upper body
action perception and its modulation by the seen agent’s        actions (drinking, picking an object, hand waving,
form and motion in relation to current theories in the          talking, nudging). The experiment consisted of 15
field. Neural signals that may index simulation process         blocks of 60 trials with equal number of videos of each
would be expected to show some specificity to the               agent.
Human condition. If the simulation process is driven                 The stimuli were displayed on a 22’ Samsung
primarily by appearance, responses to the Android are           monitor at 60 Hz. In order to prevent an augmented
expected to be similar to the Human. If on the other            visual evoked potential at the beginning of the movie
	                                                        2	  
                                                                2470

onset that might occlude subtle effects between                  irrelevant comparisons. Since our design was not a full
conditions, we displayed two consecutive gray screens            2x2 factorial design with form and motion (lacking the
(700-1000 ms and 500-700 ms, respectively) before                non-biological form and biological motion condition)
each video clip. In order to minimize eye movement               the main effect/interaction structure of a conventional
artifacts, subjects were instructed to fixate a fixation         ANOVA does not correspond to the experimental
cross at the center of the screen. In order to control for       comparisons of interest (the effect of form, of motion,
subjects' attention throughout the experiment, every             and of congruence of form and motion). Four of the
random 6-10 trials, a comprehension question was                 analysed time windows showed the following 5 ERP
displayed (e.g., Drinking? Yes/No) and subjects                  components that significantly differed between
responded with a bimanual key press.                             experimental conditions: An occipital P200 (155-205
                                                                 ms), a central temporal N170 (155-205 ms), a centro-
EEG Recordings and Analysis                                      parietal and frontal N300 (270-370 ms), a frontal N400
EEG was recorded at 512 Hz from 64 ActiveTwo                     (430-540 ms), as well as a central and frontal P600
Ag/AgCl electrodes (Biosemi, Inc.) following the                 (630-800 ms). Where we presented data from selected
International 10-10 system. The electrode-offset level           channels, these were chosen as representative channels
was kept below 25 uV. Four additional electrodes were            among those in the same region (as evident in the scalp
placed above and below the right eye, and lateral to the         distributions in Figure 2) for distinguishing one of the
eyes to monitor occulomotor activity. The data were              agents (Human, Android or Robot), thus showing the
analyzed with MATLAB and the freely available                    modulation of the respective component by form,
EEGLAB toolbox (Delorme & Makeig, 2004). Data                    motion, or congruence of form and motion. The
was high-pass filtered at 1 Hz, low-pass filtered at 50          reported p-values have been corrected for multiple
Hz, and re-referenced to average mastoids. Atypical              comparisons unless stated otherwise (at alpha level
epochs of electromyographic activity were removed                0.05).
from further analysis by semiautomated epoch rejection
procedures as implemented in EEGLAB. In order to                                         Results
discard eye-related artifacts, the data were decomposed          EEG scalp topographies of the three conditions differed
by extended infomax ICA using binica as implemented              both spatially and temporally. Early on, the processing
in EEGLAB. The data were epoched time-locked to the
onset of the video clips ranging from 200 ms preceding
onset to 2000 ms after onset. Data was explored both
qualitatively and quantitatively. Grand Average Event-
related potentials (ERP) were computed using the
BrainVision Analyzer 2 software package (BrainVision,
Inc.). For display purposes, ERPs were low-pass
filtered at 25 Hz.
      Scalp topographies for the different conditions
were generated. We identified specific channels and
time periods for statistical analysis. For an unbiased
analysis of differences between conditions, temporal
regions of interest were determined from the mean
grand average ERP activity across all conditions by
visual inspection of all channels. The specific time
window for each component was chosen to be the
narrowest time window that was common to all
channels that featured the respective component. This
led to the selection of six time windows: 75-150 ms,
155-205 ms, 210-260 ms, 270-370 ms, 430-540 ms and
630-800 ms from stimulus onset. Not all channels had
visible components in the ERP plots, but the temporal
regions were chosen to be inclusive of all possible
components of interest. Within each time window, we
                                                                      Figure 2. Scalp topographies corresponding to A)
applied paired t-tests to compare individual mean
                                                                    155-205 ms (P200/N170), B) 270-370 ms (N300), C)
amplitudes between conditions (Robot, Android,
                                                                         430-540 ms (N400), D) 630-800 ms (P600).	  
Human). The rationale of applying paired t-tests instead
of ANOVA was because the former provide a test of                of Robot was distinguished from Human and Android,
our experimental hypotheses without considering                  with an increased positivity across occipital regions for
	                                                         3	  
                                                                 2471

the latter two agent conditions (Fig. 2A). Then, Robot            showed less pronounced negativity for Android
was distinguished from Human and Android with a                   compared to Human and Robot in the most anterior-
stronger negativity across frontal, central, and centro-          frontal channels bilaterally, possibly indicating a
parietal areas (Fig. 2B). Later, Robot was again                  modulation by the (in)congruence of form and motion
distinguished from the other two agents with an                   (Fig. 3, Fp1: p<0.001; Fp2: p<0.05). The responses for
increased positivity in centro-parietal regions, and              Human and Robot did not differ.
Human was distinguished in the frontal regions (Fig.                   Between 430 ms and 540 ms, we observed a
2C). In a later stage, Human was distinguished with a             comparable negative amplitude in centro-parietal
stronger positivity in frontal regions (Fig. 2D).                 channels for Human and Android, which was absent for
        The ERPs were then quantitatively compared                the Robot condition (Fig. 3 CP1, CP2), resulting in
across conditions to explore the role of biological form          significant differences (Fig. 3 CP1 and CP2: p<0.05). In
and biological action processing. Figure 3 shows ERP              the same time window, in frontal channels, Human
plots from representative channels in which the                   elicited an increased negativity compared to Android
component of interest showed statistically significant            and Robot (Fig 3. Fp1: p<0.05; Fp2: p<0.01).
amplitude modulations across conditions.                               Finally, between 630 ms and 800 ms, we observed
        In the time window between 155-205 ms, we                 a late positivity peaking in frontal channels, which was
observed an occipital positivity (P200) that was                  increased for Human vs. Android (Fig. 3 AF8: p<0.01).
stronger for Human and Android as compared to Robot               The responses for Android and Robot did not differ in
(p<0.05). Although Human elicited an increased P200               this time interval.
than Android, Human and Android did not differ
significantly, indicating a form-based modulation of                                     Discussion
this component (Fig. 3, Iz). The same time window also            We investigated the temporal characteristics of neural
showed an N170 in right centro-temporal channel T8,               activity during the perception of actions using a unique
which showed a motion-sensitive amplitude modulation              stimulus set of well-matched human and humanoid
(Fig. 3, T8): Here, Human (featuring biological motion)           robot actions to manipulate the visual form and visual
elicited increased negative amplitude compared to                 motion kinematics of the observed agent as we recorded
Android and Robot (p<0.05 and p<0.01, respectively);              electrical brain potentials (EEG). We found that neural
the latter conditions did not differ significantly.               activity during action perception is modulated
                                                                  differentially by the appearance and motion of the agent
                                                                  being observed, allowing us to observe the unfolding of
                                                                  perceptual and cognitive processes during action
                                                                  perception.
                                                                  P200
                                                                  We found that an early stage of visual processing of the
                                                                  actions between 155-205 ms showed a form-sensitive
                                                                  modulation, where Robot (non-biological appearance)
                                                                  was distinguished from the other two agents (biological
                                                                  appearance, Figure 3 Iz). This is consistent with
                                                                  previous research on the P200 component, which is
                                                                  generally associated with early visual processing and is
                                                                  known to be sensitive to physical properties of visual
                                                                  stimuli (Luck & Hillyard, 1994). Since Robot had a
                                                                  distinct appearance from Human and Android,
        Figure 3. ERP Plots for selected channels depicting       including low-level differences such as higher contrast
    the condition effects for each component. (A: Android,        and spatial frequencies, we interpret this effect as
                     H: Human, R: Robot)                          indicative of early perceptual differences, sensitive to
                                                                  the visual appearance of the agent being observed.
        Between 270 ms and 370 ms, there was a centro-
parietal and frontal N300 (Fig. 3, CP1, CP2, F1, F2,              N170
Fp1, Fp2). Robot elicited a more pronounced negativity            The early negative component N170, especially in the
compared to Human and Android in frontal and centro-              right hemisphere, has been associated with face and
parietal channels bilaterally (Fig. 3 CP1: p<0.01; CP2:           body processing in previous ERP research (de Gelder et
p<0.01, F1: p<0.01; F2: p<0.01), indicating form-                 al., 2010). In our study, the agents had different levels
sensitive modulation. The N300 amplitudes of Human                of anthropomorphism in their faces and bodies (i.e.
and Android did not differ. The same time window                  biological vs. non-biological both in form and motion).
	                                                          4	  
                                                                  2472

The Robot had a mechanical looking face with no                  functional significance of the ERP components
movement, the Android and Human had similar facial               observed in action perception.
appearance, but the Human face also featured biological
motion (even though the actions used here did not                Implications for Action Processing
feature prominent facial expressions and were upper              Although action processing has been an active area of
body movements). We found that the amplitude of the              study in cognitive neuroscience, most work to date has
N170 was modulated by the anthropomorphism of the                used fMRI rather than electrophysiology. More
agent, as manifested by a larger N170 for Human                  specifically, a number of studies have focused on the
compared to the other agents. Since previous work on             perception of human and robot agents with fMRI, with
the N170 used static faces and bodies, our result may            inconsistent support for the simulation theory (Saygin,
indicate that dynamic (biological) facial/bodily motion          Chaminade, Ishiguro et al., 2011). Here, we add new
also elicits the N170. Another possibility is that the           ERP results to this literature, providing information
amplitude of the N170 might be differentially                    about the role of humanoid form and humanoid motion
modulated depending on the presence of a context, as in          during the course of action perception.
our case the face was perceived together with the body                The stimuli used here were previously utilized in
during the performance of an action, whereas in                  an fMRI repetition-suppression study in which brain
previous work, still faces and bodies were shown as              activity did not show evidence for form-based or
stimuli. As such, our results offer possible new studies         motion-based simulation per se, but instead was most
to understand the functional significance of the N170            significantly affected by form-motion incongruence
component.                                                       (Saygin, Chaminade, Ishiguro et al., 2011). Here, with a
                                                                 more time-resolved method, we found distinct stages of
N300/N400 complex                                                processing during which neural responses differed
The N300/N400 complex with an anterior distribution              based on both the form and the motion of the seen agent.
has been associated with the mapping of visual input             These effects were likely lost due to the temporal
onto representations in semantic memory (Sitnikova,              insensitivity of fMRI, highlighting the importance of
Holcomb, Kiyonaga, & Kuperberg, 2008). The                       using multiple, complementary techniques.
increased centro-parietal negativity that we found for                A well-known face-sensitive component, the N170,
the robot condition in the 270-370 ms time interval              was elicited by our stimuli. Previous work on this ERP
(Figure 3, CP1 and CP2) over anterior regions may                signature of face processing has used static face stimuli,
reflect a difficulty in mapping visual input onto existing       as opposed to movies including the body as we did here.
semantic representations, since robots are currently not         Our data suggest new possible ways in which the N170
very familiar, certainly not in the context of actions           can be modulated. Specifically we hypothesize that
such as those in our stimuli (e.g., drinking from a cup).        either biological motion of the face and/or the context
If this interpretation is correct, we can also deduce this       provided by the body are modulators of the N170.
process being driven primarily by the form of the agent,              Our data did not reveal patterns of activity that can
for if motion was a factor, the Android were equally, if         be linked straightforwardly to simulation theory. There
not more difficult to match to semantic memory. There            was some selectivity for the Human (for whom
was also a significant effect in the same time range in          simulation theory would predict differential effects,
frontal channels, where Android differed from the other          whether driven by form or motion) for the frontal N400
two agents (Figure 3, Fp1, Fp2). Given the Android               and P600, but there is little prior literature on actions
represents a mismatch between form and motion being              for these components, and no link to sensorimotor
potentially linked to the uncanny valley phenomenon              simulation that we are aware of. The uncanny valley
(Ishiguro, 2006; Saygin, Chaminade, Ishiguro et al.,             theory also cannot account for all of the patterns in our
2011), this could be a potential component to explore in         data, although the frontal N300 response could be
future studies on the uncanny valley, or on congruence           interpreted as biomarker for the uncanny valley. These
of form and motion more generally.                               components should be viewed as possible indices
                                                                 related to each theory, to be tested in new studies.
P600 (late positivity)                                                Overall, in this first ERP study of action perception
In previous work, a late positivity or P600 has mostly           with human and humanoid agents, we highlight the
been studied in the domain of language and is most               complexity of action processing that can be revealed
commonly associated with syntactic processing                    using more time-resolved methods. We found distinct
(Friederici, 2004). Few studies have interpreted the             neural signatures of the viewed agent’s form and
P600 in other domains (Sitnikova et al., 2008). In our           motion in different time periods, both early (perceptual)
data (Figure 3, AF8), we found that this component was           and late (cognitive) in processing. These results do not
elicited most strongly by the Human condition. This              globally fit into either simulation or uncanny valley
can lead us new avenues of research to understand the            frameworks, although a focus on specific components
	                                                         5	  
                                                                 2473

such as the N170 and N300/400 in upcoming studies                      Ishiguro, H. (2006). Android science: conscious and
might help better understand the mechanisms of action                      subconscious recognition. Connection Science, 18(4),
perception and its neural basis. Work on neural                            319-332.
dynamics of action processing can not only shed light                  Jokisch, D., Daum, I., Suchan, B., & Troje, N. F. (2005).
on the cognitive neuroscience of action perception, but                    Structural encoding and recognition of biological
also to inform the burgeoning field of social robotics                     motion: evidence from event-related potentials and
(Saygin, Chaminade, Urgen et al., 2011).                                   source analysis. Behavioral Brain Research, 157(2),
                                                                           195-204.
                   Acknowledgements                                    Kanda, T., Miyashita, T., Osada, T., Haikawa, Y., &
This research was supported by the Kavli Institute for Brain               Ishiguro, H. (2008). Analysis of humanoid appearances
and Mind (APS), California Institute of Telecommunications                 in human-robot interaction. IEEE Transactions on
and Information Technology (Calit2) (BAU, APS), NSF                        Robotics, 24(3), 725-735.
(SBE-0542013, Temporal Dynamics of Learning Center),                   Krakowski, A. I., Ross, L. A., Snyder, A. C., Sehatpour, P.,
and ONR (MURI Award # N00014-10-1-0072, HP). The                           Kelly, S. P., & Foxe, J. J. (2011). The neurophysiology
authors would like to thank Marta Kutas, Arthur Vigil,                     of human biological motion processing: A high-density
members of the Intelligent Robotics Laboratory (Osaka                      electrical mapping study. Neuroimage, 56(1), 373-383.
University) for the creation of the stimuli, and Joe Snider            Luck, S. J., & Hillyard, S. A. (1994). Electrophysiological
(Poizner Lab) for his help in the experimental setup.                      correlates of feature analysis during visual search.
                                                                           Psychophysiology, 31(3), 291-308.
                          References                                   MacDorman, K. F., & Ishiguro, H. (2006). The uncanny
Barsalou, L. W. (2009). Simulation, situated                               advantage of using androids in cognitive and social
     conceptualization, and prediction. Philosophical                      science research. Interaction Studies, 7(3), 297-337.
     Transactions of the Royal Society of London B,                    Mori, M. (1970). The uncanny valley. Energy, 7(4), 33-35.
     364(1521), 1281-1289.                                             Press, C., Cook, J., Blakemore, S. J., & Kilner, J. M. (2011).
Buccino, G., Lui, F., Canessa, N., Patteri, I., Lagravinese, G.,           Dynamic modulation of human motor activity when
     Benuzzi, F., et al. (2004). Neural circuits involved in the           observing actions. Journal of Neuroscience, 31(8),
     recognition of actions performed by nonconspecifics: an               2792-2800.
     FMRI study. Journal of Cognitive Neuroscience, 16(1),             Rizzolatti, G., & Craighero, L. (2004). The mirror-neuron
     114-126.                                                              system. Annual Review of Neuroscience, 27, 169-192.
Calvo-Merino, B., Grezes, J., Glaser, D. E., Passingham, R.            Saygin, A. P., Chaminade, T., & Ishiguro, H. (2010). The
     E., & Haggard, P. (2006). Seeing or doing? Influence of               perception of humans and robots: Uncanny hills in
     visual and motor familiarity in action observation.                   parietal cortex. In S. Ohlsson & R. Catrambone (Eds.),
     Current Biology, 16(19), 1905-1910.                                   Proceedings of the 32nd Annual Conference of the
Chaminade, T., & Cheng, G. (2009). Social cognitive                        Cognitive Science Society (pp. 2716-2720 ). Portland,
     neuroscience and humanoid robotics. Journal of                        OR: Cognitive Science Society.
     Physiology Paris, 103(3-5), 286-295.                              Saygin, A. P., Chaminade, T., Ishiguro, H., Driver, J., &
Chaminade, T., Hodgins, J., & Kawato, M. (2007).                           Frith, C. F. (2011). The thing that should not be:
     Anthropomorphism influences perception of computer-                   Predictive coding and the uncanny valley in perceiving
     animated characters' actions. Social Cognitive and                    human and humanoid robot actions. Social Cognitive
     Affective Neuroscience, 2(3), 206-216.                                and Affective Neuroscience.
Delorme, A., & Makeig, S. (2004). EEGLAB: an open                      Saygin, A. P., Chaminade, T., Urgen, B. A., & Ishiguro, H.
     source toolbox for analysis of single-trial EEG                       (2011). Cognitive neuroscience and robotics: A
     dynamics including independent component analysis.                    mutually beneficial joining of forces In L. Takayama
     Journal of Neuroscience Methods, 134(1), 9-21.                        (Ed.), Robotics: Systems and Science. Los Angeles, CA.
de Gelder, B., Van den Stock, J., Meeren, H.K.M, Sinke,                Sitnikova, T., Holcomb, P. J., Kiyonaga, K. A., &
     J.B.A., Kret, M.E., Tamietto, M. (2010). Standing up for              Kuperberg, G. R. (2008). Two neurocognitive
     the body: Recent progress in uncovering the networks                  mechanisms of semantic integration during the
     involved in processing bodies and bodily expressions.                 comprehension of visual real-world events. Journal of
     Neurosci. Biobehav. Rev., 34, 513-527.                                Cognitive Neuroscience, 20(11), 2037-2057.
Friederici, A. D. (2004). Event-related brain potential                Steckenfinger, S. A., & Ghazanfar, A. A. (2009). Monkey
     studies in language. Current Neurology Neuroscience                   visual behavior falls into the uncanny valley.
     Report, 4(6), 466-470.                                                Proceedings of the National Academy of Sciences of the
Hirai, M., Fukushima, H., & Hiraki, K. (2003). An event-                   United States of America, 106(43), 18362-18366.
     related potentials study of biological motion perception
     in humans. Neurosci Lett, 344(1), 41-44.
	                                                               6	  
                                                                       2474

