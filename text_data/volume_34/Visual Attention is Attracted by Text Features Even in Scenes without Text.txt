UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Visual Attention is Attracted by Text Features Even in Scenes without Text
Permalink
https://escholarship.org/uc/item/17q754gx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Wang, Hsueh-Cheng
Lu, Shijian
Lim, Joo-Hwee
et al.
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

           Visual Attention is Attracted by Text Features Even in Scenes without Text
                                         Hsueh-Cheng Wang (hchengwang@gmail.com)
                                                   Shijian Lu (slu@i2r.a-star.edu.sg)
                                            Joo-Hwee Lim (joohwee@i2r.a-star.edu.sg)
                                                 Marc Pomplun (marc@cs.umb.edu)
                               Department of Computer Science, University of Massachusetts at Boston,
                                           100 Morrissey Boulevard, Boston, MA 02125 USA
                                           Institute for Infocomm Research, A*STAR, Singapore
                                                    1 Fusionopolis Way, Singapore 138632
                              Abstract                                   detection in natural scenes is critically important for people
                                                                         to survive in everyday modern life, for example, by drawing
   Previous studies have found that viewers’ attention is
   disproportionately attracted by texts, and one possible reason        attention to traffic signs or displays showing directions to a
   is that viewers have developed a “text detector” in their visual      hospital or grocery store. Our previous studies (Wang &
   system to bias their attention toward text features. To verify        Pomplun, 2011; under revision) suggested that attention
   this hypothesis, we add a text detector module to a visual            seems disproportionately attracted by texts but that the
   attention model and test if the inclusion increases the model’s       specific visual features of texts, e.g., edge density, rather
   ability to predict eye fixation positions, particularly in scenes     than typically salient features such as color, orientation,
   without any text. A model including text detector, saliency,
   and center bias is found to predict viewers’ eye fixations
                                                                         intensity, or contrast, are the main attractors of attention.
   better than the same model without text detector, even in text-       This finding was in line with the results in Baddeley and
   absent images. Furthermore, adding the text detector – which          Tatler (2006) that high spatial frequency edges, not
   was designed for English texts – improves the prediction of           contrasts, predict where we fixate.
   both English- and Chinese-speaking viewers’ attention but                Automatic text detection has been a hot topic in the fields
   with a stronger effect for English-speaking viewers. These            of computer vision and pattern recognition for its practical
   results support the conclusion that, due to the viewers’
                                                                         applications. The special features of texts, e.g., the small
   everyday reading training, their attention in natural scenes is
   biased toward text features.                                          variation of the stroke width (see Epshtein, Ofek, & Wexler,
                                                                         2010; Jung, Liu, & Kim, 2009) or edge density (Lu,
   Keywords: real-world scenes; text detector; eye movements;            submitted) have been used to develop text detectors.
   visual attention.                                                     Although many text detection techniques, i.e., texture-
                                                                         based, region-based, and stroke-based methods, have been
                          Introduction                                   reported, many non-text objects, such as windows, fences,
When inspecting real-world scenes, human observers                       or brick walls, easily cause false alarms (see Lu, submitted;
continually shift their gaze to retrieve information. Viewers’           Ye, Jiao, Huang, & Yu, 2007, for a review). Furthermore,
attention has been found to be biased toward visually salient            many established text detectors are restricted under
locations, e.g., high-contrast areas, during scene viewing or            commercial patents. Therefore, only few text detectors are
search (Itti & Koch, 2001) or toward the center of the screen            freely available or have been tested in visual attention
when viewing scenes on computer monitors (Tatler, 2007).                 studies.
Since it is also known that viewers pay a disproportionate                  Lu, Wang, Lim, and Pomplun (submitted) developed
amount of attention to faces (Cerf, Frady, & Koch, 2009),                specialized text features, e.g., histograms of edge width and
Judd, Ehinger, Durand, and Torralba (2009) equipped their                edge density, trained with Support Vector Machine (SVM)
model of visual saliency with a face detector (Viola &                   classifiers. The study reported better performance compared
Jones, 2004) and a person detector (Felzenszwalb,                        with earlier studies (e.g., Epshtein, et al., 2010; Jung, et al.,
McAllester, & Ramanan, 2008). In those images that                       2009) on public text-detecting competition datasets
contained depictions of people, their model with all features            (ICDAR2003 and ICDAR2005). In the present study, we
combined outperformed models trained on typical saliency                 used the automatic text detector developed by Lu et al.
features such as color, orientation, intensity, and contrast.            (submitted) to test whether it can improve the prediction of
Cerf et al. (2009) refined the “standard” saliency model by              viewers’ fixations. This detector employs contrast of strokes
adding a channel of manually-defined regions of faces,                   over background, width of strokes, joints of horizontal and
texts, and cellphones, and demonstrated that the                         vertical strokes, and stroke structure as key variables
enhancement of the model significantly improved its ability                 Although manually-defined regions of texts were shown
to predict eye fixations in natural images.                              to improve the prediction of eye fixations in text-present
  Besides depictions of people, texts in natural scenes are              images (Cerf et al., 2009), it is unclear if viewers’ attention
usually important pieces of information, which could be                  is biased toward any non-text objects which share some
shown on depictions of signs, banners, advertisement                     features of texts, particularly in text-absent images. In the
billboards, license plates, and other objects. Human text                present study, two eye-movement datasets obtained in our
                                                                         previous investigations (Wang & Pomplun, under revision)
                                                                     2505

are re-analyzed. The goals of the present study are (1) to        were placed in front of homogeneous background and the
investigate the contribution of the automatic text detector to    other half were placed on inhomogeneous background.
the prediction of eye fixations in real-world scenes, and (2)     Figure 1 shows an example of all four conditions with
to verify the hypothesis that viewers’ text detection skills      words and drawings on homogeneous background. The
are “trained” through exposure to language and affect             eccentricity of the text or the drawing was randomly
attentional control even in text-absent scenes                    assigned and varied between 200 and 320 pixels (average:
                                                                  253 pixels). The minimum polar angle, measured from the
        Experiment 1: Unconstrained Texts                         screen center, between the text and the drawing in each
   We superimposed unconstrained texts onto real-world            image was set to 60 degrees to avoid crowding of the
scenes, i.e., placed them in unexpected locations, in front of    artificial items. All texts and drawings were resized to cover
either homogeneous background, i.e., in regions with the          approximately 2500 pixels.
lowest luminance contrast in the image before placing the
text parts, or inhomogeneous background, i.e., those areas           Table 1: Examples of texts (words and scrambled words)
with the highest luminance contrast, and found that texts         and object drawings used in Experiment 1.
attracted more attention than non-text objects. This dataset
is chosen for re-analysis in the present study since the                               Item A                 Item B
stimuli contain both text-present and text-absent images.           Texts              sled (dsle)            yoyo (yyoo)
Two models, both including saliency and center-bias maps
(channels), but one with and one without text-detector map          Object
are compared in order to determine whether the inclusion of         Drawing
the text detector improves the prediction of fixations,
particularly in text-absent images.
Method
  Participants. Twelve students from the University of
Massachusetts at Boston participated. All had normal or
corrected-to-normal vision and were between 19 and 40
years old. Each participant received 10 dollars for a half-
hour session.
   Apparatus. Eye movements were recorded using an SR
Research EyeLink Remote system with a sampling                                     (a)                           (b)
frequency of 1000 Hz. Subjects sat 65 cm from an LCD
monitor approximately 34 x 25 degrees of visual angles. A
chin rest was provided to minimize head movements. After
calibration, the average error of visual angle in this system
is 0.5˚. Stimuli were presented on a 19-inch Dell P992
monitor with a refresh rate of 85 Hz and a screen resolution
of 1024×768 pixels. Although viewing was binocular, eye
movements were recorded from the right eye only.
   Stimuli. Two hundred natural-scene images were selected                         (c)                           (d)
from the LabelMe dataset (Russell, Torralba, Murphy &
Freeman, 2008). Eighty out of these images were randomly          Figure 1. An example of 4 conditions of stimuli for low-
selected to be superimposed with one text and one line            frequency words drawn on homogeneous background. (a)
drawing. The other 120 images were presented without any          Word of Item A (sled) vs. drawing of Item B, (b) word of
modification. For the placement of texts and line drawings,       Item B (yoyo) vs. drawing of Item A, (c) scrambled word of
two different items (items A and B in Table 1) were chosen        Item A (dsle) vs. drawing of Item B, and (d) scrambled
for each scene, and their addition to the scene was               word of Item B (yyoo) vs. drawing of Item A.
performed under four different conditions: either (1) a word
describing item A (e.g., “sled” as shown in Table 1) and a           Procedure. Equal numbers of subjects freely viewed
drawing of item B, (2) a word describing item B (e.g.,            stimuli from conditions 1, 2, 3, and 4 in a counter-balanced
“yoyo”) and a drawing of item A, (3) a scrambled version of       design (described below), and each stimulus was presented
a word describing item A (e.g., “dsle”) and a drawing of          for 5 seconds. The free viewing task has been widely used
item B, and (4) a scrambled version of a word describing          in previous studies (e.g., Judd et al, 2009; Cerf et al., 2009).
item B (e.g., “yyoo”) and a drawing of item A. All four           The software “Eyetrack” developed by Jeffrey D. Kinsey,
conditions of text-drawing pairs were presented in a              David J. Stracuzzi, and Chuck Clifton, University of
between-subject design, i.e., each participant only viewed        Massachusetts Amherst, was used for recording eye
one of these conditions. Half of the words (object labels)        movements.
                                                              2506

   Analysis. Two eye movement measures were taken:                 The averages of correlations and ROC values for each
correlation (R) and Receiver Operating Characteristic              viewer were calculated for all, text-present, text-absent, text
(ROC). The Pearson correlation coefficient R between two           in front of homogeneous (H-BG), and text in front of
maps is computed according to sampling points taken every          inhomogeneous backgrounds (INH-BG) images, and an
10 pixels along the x and y axes, and then the correlation         ANOVA and paired t-tests were performed to analyze the
coefficient between saliency/center-bias/text-detector and         differences between these values
attentional maps (described below) are obtained. An
example of a stimulus image and its attention, saliency,           Results and Discussion
center-bias, and text-detector maps is shown in Figure 2.            Models with and without Text-Detector Maps. The
The computation of the ROC measure is described in                 average R and ROC values of all 12 viewers are shown in
Hwang, Higgins & Pomplun (2009). If a map had higher               Table 2. Text-detector maps overlap attentional maps the
correlation or ROC values with regard to the subjects’             best when the images contain text in front of homogeneous
fixations, the map was considered a better predictor of            background, and the worst in text-absent images. These
visual attention. The chance level is 0.5 for ROC and 0 for        results are consistent with the finding by Judd et al. (2009)
R.                                                                 that object detectors by themselves do not predict attention
                                                                   well when the objects are absent and therefore should be
                                                                   used in conjunction with other features.
                                                                     Table 2: The average R and ROC of saliency (Sali),
                                                                   center-bias (Center), text-detector (TextDet), saliency
                                                                   combined with center-bias (SC), and all combined (SCT)
                                                                   maps as predictors of the attentional maps for 3-second
                                                                   viewing. H-BG represents images in front of homogeneous
             (a)                               (b)                 background, and INH-BG represents images on
                                                                   inhomogeneous background.
                                                                                    Sali    Cen        TextDet     SC        SCT
                                                                   R -All           0.14    0.16       0.15        0.18      0.20
                                                                   Text-Present     0.11    0.12       0.20        0.14      0.16
         (c)                 (d)                   (e)                H-BG          0.09    0.10       0.24        0.10      0.12
                                                                      INH-BG        0.14    0.15       0.15        0.17      0.19
Figure 2. An example of (a) stimulus image, (b) attention          Text-Absent      0.15    0.19       0.12        0.21      0.22
(3-second viewing) (c) saliency, (d) center-bias, and (e)          ROC - All        0.65    0.63       0.63        0.69      0.72
text-detector maps.                                                Text-Present     0.61    0.61       0.66        0.64      0.70
                                                                      H-BG          0.55    0.60       0.67        0.58      0.67
   Saliency was calculated by the freely available computer
                                                                      INH-BG        0.67    0.62       0.64        0.70      0.72
software “Saliency Map Algorithm” using the standard Itti,
                                                                   Text-Absent      0.67    0.64       0.62        0.72      0.73
Koch, and Niebur (1998) saliency map based on color,
intensity, orientation, and contrast. A center-bias map was
obtained using a two-dimensional Gaussian distribution at             One-way ANOVAs with the factor “predictor” showed
the center of the screen with 3 degrees of visual angle (90        that the performances of Sali, Cen, TextDet, SC, and SCT
pixels in our experiment setting). The text-detector maps          maps differed significantly in all, text-present, H-BG, INH-
were computed using the automatic text detector which              BG, and text-absent images for R, all Fs(4; 55) > 3.64, ps <
analyzes features such as variation of edge width and edge         .05, and ROC, all Fs(4; 55) > 11.17, ps < .01. SC (without
density.                                                           text-detector) obtained significantly lower measures than
   For the attentional map, we excluded the initial center         SCT (with text-detector maps) for all, text-present, H-BG,
fixation and included all other fixations within a given           INH-BG, and text-absent images for R, all ts(11) > 3.93, ps
viewing duration. The attentional map was built according          < .01, and ROC, all ts(11) > 7.68, ps < .001. The results
to each fixation in an image by a two-dimensional Gaussian         indicate that the text detector improved the prediction of
distribution centered at the fixation point, where the             viewers’ visual attention. It is interesting to see that the SCT
standard deviation was one degree of visual angle to               obtained higher R and ROC than the SC even in text-absent
approximate the size of the human fovea. Then we simply            images. One plausible explanation is that some non-objects
summed up these Gaussian distributions for fixations               containing text-like features catch a disproportionate
weighted by their durations (see Pomplun, Ritter, &                amount of attention.
Velichkovsky, 1996).                                                Text-Present vs. Text-Absent and H-BG vs. INH-BG
  We computed the attentional maps for each image                  Images. The five predictors were analyzed in one-way
inspected by each viewer for the initial 1.5, 2, …, 5 seconds.     ANOVAs with the factor “image type,” and the results
                                                               2507

demonstrate that both R and ROC values significantly                 hypothesis is that viewers have developed a “text detector”
differed in all, text-present, text-absent, H-BG, and INH-BG         because they are exposed to texts everyday and become
images, all Fs(4; 55) > 4.91, ps < .01, and all Fs(4; 55) >          sensitive to text-patterns. Wang and Pomplun (under
4.72, ps < .01, respectively, except ROC for Cen, F(4; 55) =         revision) found that native speakers of English and Chinese-
0.92, p > .4. The text detector (TextDet) performed better           speakers were both attracted by English and Chinese texts in
for text-present images than text-absent ones with regard to         real-world scenes but were attracted more strongly by the
R, t(11) = 10.67, p < .001 as well as ROC, t(11) = 5.66, p           texts of their native languages. The reason might be that
< .001. Homogeneous background images obtained higher                English and Chinese texts share some common features,
values than inhomogeneous background images for both R,              such as the histogram of edge width, but also contain their
t(11) = 7.31, p < .001, and ROC, t(11) = 3.94, p < .01.              unique features, e.g., Chinese texts usually contain vertical,
  Visual Attention over Time. SCT outperformed SC                    horizontal, and diagonal strokes but fewer “curves” (such as
(without text detector) for all viewing durations for R and          in “O” or “G” in English). In Experiment 2, the dataset in
ROC in both text-present images, both ts(11) > 9.68, ps              Wang and Pomplun (submitted) was reanalyzed and our
< .001, and text-absent ones, both ts(11) > 3.93, ps < .01.          expectation was that the text detector (Lu, submitted)
The difference between SCT and SC was larger in text-                designed for English texts will perform better prediction of
present images than in text-absent ones. In text-present             gaze fixations for English-speaking viewers than for
images, the R of TextDet initially dominated but decreased           Chinese-speaking ones.
over time, while the R of Sali increased (see Figure 3a)..
These data suggest that texts are typically detected early           Method
during the inspection process and receive sustained attention           Participants. In the group of non-Chinese English
while the viewers are reading them, thereby elevating the            speakers, 14 students from the University of Massachusetts
occurrence of text features near fixation. Later in the              at Boston participated. All of them were native speakers of
process, viewers tended to be guided more strongly by                English, and none of them had learnt any Chinese or had
saliency as defined by the Itti and Koch algorithm. In text-         participated in Experiment 1. For the group of Chinese
absent images, the R of Sali, Cen, and TextDet increased             speakers, 16 native speakers of Chinese were recruited at
over time, indicating that the corresponding mechanisms              China Medical University, Taiwan. Each participant
became more important during the later – likely more                 received 10 US dollars or 100 Taiwan dollars, respectively,
focused and fine-grained (Unema, Pannasch, Joos, &                   for participation in a half-hour session. All had normal or
Velichkovsky, 2005) – stages of inspection. Clearly, Sali            corrected-to-normal vision.
and Cen played more important roles when texts are absent.              Apparatus. At both sites, the experiment setup was
                                                                     identical to Experiment 1.
    0.25                            0.25
              Sali                           Sali                       Stimuli. As shown in Figure 4, the original texts were
              Cen                            Cen
    0.23      TextDet               0.23     TextDet                 either rotated by 180 degrees or replaced by Chinese texts.
              SC                             SC
              SCT                            SCT                     The rationale for using upside-down English texts was to
    0.21                            0.21
                                                                     keep the low-level features such as regular spacing and
    0.19                            0.19
                                                                     similarity of letters but reduce possible influences of higher-
                                                                     level processing such as meaning. Figure 4a illustrates C1,
 R 0.17                          R 0.17                              in which half of the original texts were rotated and the other
                                                                     half was replaced with Chinese texts. In C2, as
    0.15                            0.15
                                                                     demonstrated in Figure 4b, the upside-down texts in C1
    0.13                            0.13                             were replaced with Chinese texts, and the Chinese texts in
                                                                     C1 were replaced with the original, but upside-down,
    0.11                            0.11
                                                                     English texts.
    0.09                            0.09
       1000        3000     5000       1000       3000      5000
          Viewing Duration (ms)           Viewing Duration (ms)
                      (a)                            (b)
Figure 3. Correlations for 1.5-, 2-, …, and 5-second viewing
of (a) text-present and (b) text-absent images.
  Experiment 2: English vs. Chinese Texts and
                        Native Speakers                              Figure 4. Example of Chinese and upside-down English
                                                                     texts used in Experiment 2. (a) Condition C1 (b) Condition
                                                                     C2.
In Experiment 1, we showed that the addition of a text-
detector map to saliency and center-bias maps makes the
model a better predictor of viewers’ visual attention. Our
                                                                 2508

   Procedure. The procedure was identical to Experiments              English vs. Chinese-Speaking Viewers. As shown in
1 except that half of the subjects viewed condition 1 (C1)         Figure 6, TextDet predicted English-speaking viewers’
stimuli and the others viewed condition 2 (C2) stimuli in a        attention better than Chinese-speaking viewers’ attention for
between-subject counter-balanced design.                           all viewing durations in both text-present images, t(7) =
   Analysis. The analyses were identical to Experiment 1.          23.12, p < .001, and text-absent images, t(7) = 5.38, p < .01.
                                                                   These results indicate that the text detector that was
                                                                   designed for English texts performed better at predicting the
                                                                   allocation of attention for English-speaking viewers than for
                                                                   Chinese-speaking ones.
                                                                      Table 3: The average R and ROC of saliency (Sali),
                                                                   center-bias (Cen), text-detector (TextDet), saliency
                                                                   combined with center-bias (SC), and all combined (SCT)
                (a)                             (b)                maps as predictors of attentional maps for 5-second
                                                                   viewing. En represents English-speaking viewers, and Ch
                                                                   means Chinese-speaking viewers.
                                                                                     Sali   Cen         TextDet       SC       SCT
                                                                   R (En)            0.17   0.17        0.14          0.20     0.21
                                                                    Text-Present     0.15   0.16        0.16          0.19     0.21
                (c)                             (d)                 Text-Absent      0.18   0.17        0.12          0.21     0.22
                                                                   R (Ch)            0.17   0.16        0.12          0.19     0.20
Figure 5. An example of (a) stimulus image, (b) text-               Text-Present     0.15   0.15        0.14          0.18     0.19
detector map, (c), attentional map of an English-speaking           Text-Absent      0.18   0.17        0.11          0.20     0.21
viewer (5-second viewing), and (d) attentional map of a            ROC (En)          0.69   0.61        0.60          0.72     0.73
Chinese-speaking viewer (5-second viewing).                         Text-Present     0.68   0.62        0.63          0.71     0.73
                                                                    Text-Absent      0.69   0.61        0.59          0.72     0.73
Results and Discussion
                                                                   ROC (Ch)          0.68   0.60        0.60          0.70     0.71
     Models with and without Text-Detector Maps. The                Text-Present     0.67   0.61        0.62          0.69     0.71
average R and ROC of all 14 English-speaking and 16                 Text-Absent      0.68   0.60        0.58          0.70     0.70
Chinese-speaking viewers are shown in Table 3. For
English-speaking viewers, one-way ANOVAs showed that
the Sali, Cen, TextDet, SC, and SCT maps performed                         0.16
differently in all, text-present, and text-absent images for R,
all Fs(4; 65) > 8.47, ps < .01, and for ROC, all Fs(4; 65) >
53.78, ps < .001. SCT predicted attentional maps better than               0.14
SC in all, text-present, and text-absent images for R, all               R
ts(13) > 3.49, ps < .01, and ROC, all ts(13) > 6.61, ps <                  0.12
.001. For Chinese-speaking viewers, similar results were
obtained - the performances of Sali, Cen, TextDet, SC, and                                                   Text-Present (En)
                                                                            0.1                              Text-Present (Ch)
SCT maps significantly differed for both R, all Fs(4; 75) >
                                                                                                             Text-Absent (En)
33.91, ps < .001, and ROC, all Fs(4; 75) > 22.86, ps < .001.
                                                                                                             Text-Absent (Ch)
SCT yielded better prediction of attentional maps than SC                  0.08
for both R, all ts(15) > 4.85, ps < .001, and ROC, all ts(15)                 1000      2000      3000        4000       5000
> 5.29, ps < .001. The results of SCT are consistent with                                     Viewing Duration (ms)
Experiment 1 in that the text detector improved the
prediction of viewers’ visual attention, even in text-absent       Figure 6. The R values of TextDet for 1.5-, 2-, …, and 5-
images.                                                            second viewing of text-present and text-absent images by
   Text-Present vs. Text-Absent Images. For English-               English-speaking (En) and Chinese-speaking (Ch) viewers.
speaking viewers, TextDet performed better in text-present
images than in text-absent ones for both R, t(13) = 6.41, p <                         General Discussion
.001, and ROC, t(13) = 5.58, p < .001. For Chinese-                   In Experiment 1, we found that adding a text detector to
speaking viewers, similar results were found: text-present         an attention model improved its prediction of viewers’
images obtained higher R and ROC than text-absent ones,            visual attention, even in text-absent images. Our results
t(15) = 4.97, p < .001, and t(15) = 7.35, p < .001,                suggest that non-text objects whose features resemble those
respectively.                                                      of texts (such as high spatial frequency edges) catch a
                                                                   disproportionate share of attention. Based on the current
                                                               2509

data, it seems that the viewers’ “biological text detectors”           Vision and Pattern Recognition (CVPR), San Francisco,
are somewhat similar to the artificial system and influence            USA, 2963–2970.
the viewers’ distribution of attention when viewing real-           Felzenszwalb, P., McAllester, D., & Ramanan, D. (2008). A
world images. From a time-course analysis, it appears that             Discriminatively Trained, Multiscale, Deformable Part
the biological text detector influences the allocation of              Model. Computer Vision and Pattern Recognition
attention particularly strongly during later stages of image           (CVPR), Anchorage, Alaska, USA, 1-8.
inspection when viewers are increasingly likely to attend to        Hwang, A. D., Higgins, E. C., & Pomplun, M. (2009). A
detailed local structures (see Unema et al., 2005) for                 model of top-down attentional control during visual
semantic interpretation of perceived text.                             search in complex scenes. Journal of Vision, 9(5), 1–18
   Whereas the results of Experiment 1 could have been                 (25).
caused by the text detection algorithm being sensitive to           Itti, L, Koch, C., & Niebur, E. (1998). A Model of Saliency-
visual features that generally attract attention, such as edge         Based Visual Attention for Rapid Scene Analysis. IEEE
density, this interpretation becomes implausible given the             Trans Pattern Analysis and Machine Intelligence 20 (11):
results of Experiment 2. We found that the text detector               1254-1259.
designed for English texts predicted English-speaking               Itti, L., & Koch, C. (2001). Computational Modeling of
viewers’ attention better than Chinese-speaking viewers’,              Visual Attention. Nature Reviews Neuroscience.
supporting the hypothesis that viewers have developed a                2(3):194-203.
“text detector” that is sensitive to text patterns they are         Jung, C., Liu, Q., and Kim, J. (2009). A stroke filter and its
familiar with. It is interesting to see that the way we learn to       application for text localization. Pattern Recognition
read influences our allocation of visual attention in everyday         Letters, 30(2):114–122.
life, even when there are no texts presented and we are not         Judd, T., Ehinger, K., Durand, F., & Torralba, A. (2009).
specifically looking for any texts.                                    Learning to predict where humans look, IEEE
   While the present study has demonstrated the influence of           International Conference on Computer Vision (ICCV),
language on visual attention in real-world scenes, further             Kyoto, Japan, 2106 - 2113.
research needs to identify the visual features that underlie        Lu, S., Wang, H.-C., J.-H. Lim, & Pomplun, M. (submitted).
this effect. This could be achieved by using text detection            Learning Text Saliency for Automatic Text Detection in
algorithms for different writing systems and test their                Natural Scenes.
individual components as predictors of native and non-              Pomplun, M., Ritter, H., & Velichkovsky B., (1996).
native speakers’ attention in natural scenes. Besides a more           Disambiguating Complex Visual Information: Toward
comprehensive understanding of attentional control in                  Communication of Personal Views of a Scene, Perception,
humans, such studies may also result in technological                  25, 8, 931-948.
advances. Human viewers can easily locate texts in natural          Russell, B. C., Torralba, A., Murphy, K. P., & Freeman, W.
scenes, performing clearly better than current text-detection          T. (2008). LabelMe: a database and web-based tool for
techniques even when the texts are degraded by noise,                  image annotation, International Journal of Computer
rotated, distorted, or shown from unusual perspectives.                Vision, 77, 1-3, 157-173.
Consequently, the results of this line of research, such as         Tatler, B. W. (2007). The central fixation bias in scene
analyzing what features or local structures are actually               viewing: Selecting an optimal viewing position
learned by the biological text detector, might contribute to           independently of motor biases and image feature
the development of more effective automatic text detectors,            distributions. Journal of Vision, 7(14):4, 1-17.
which could, for example, make a great difference to                Torralba, A., Oliva, A., Catelhano, M., & Henderson, J.M.
visually challenged people’s lives.                                    (2006). Contextual guidance of eye movements and
                                                                       attention in real-world scenes: The role of global features
                     Acknowledgments                                   in object search. Psychological Review, 113, 766-786.
Preparation of the article was supported by Grant                   Unema, P. J. A., Pannasch, S., Joos, M., & Velichkovsky,
R01EY021802 from the National Eye Institute to Marc                    B.M. (2005). Time course of information processing
Pomplun.                                                               during scene perception. Visual Cognition, 12(3), 473-494.
                                                                    Viola, P. & Jones, M. (2004) Robust real-time face
                                                                       detection. International Journal of Computer Vision,
                          References                                   57(2), 137-154.
Baddeley, R. J. & Tatler, B. W. (2006). High frequency              Wang, H. C. & Pomplun M. (2011). The attraction of visual
   edges (but not contrast) predict where we fixate: A                 attention to texts in real-world scenes. The Annual
   Bayesian system identification analysis. Vision Research,           Meeting of the Cognitive Science Society (Cogsci2011),
   46(18), 2824-2833.                                                  2733–2738.
Cerf, M., Frady, E. P., & Koch, C. (2009). Faces and text           Ye, Q., Jiao, J., Huang, J., & Yu, H. (2007). Text detection
   attract gaze independent of the task: Experimental data             and restoration in natural scene images. Journal of Visual
   and computer model. Journal of Vision, 9(12):10, 1–15.              Communication and Image Representation. 18, 504-513.
Epshtein, B., Ofek, E., & Wexler Y. (2010). Detecting text
   in natural scenes with stroke width transform. Computer
                                                                2510

