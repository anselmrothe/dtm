UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Containment Metaphors
Permalink
https://escholarship.org/uc/item/3jg2z2qm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Nayak, Sushobhan
Mukerjee, Amitabha
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                          Learning Containment Metaphors
                                               Sushobhan Nayak (snayak@iitk.ac.in)
                                               Amitabha Mukerjee (amit@iitk.ac.in)
                                                  Indian Institute of Technology Kanpur
                                                          Kanpur, UP 208016 India
                              Abstract                                  on identifying and analyzing metaphors. The earliest rule-
                                                                        based attempts - e.g. (Fass, 1991) were based on hand-coded
   We present a computational approach that traces the de-              knowledge and metaphors were identified as a violation of se-
   velopmental process, from containment image schemas to
   metaphors, in four phases: a) perceptual discovery of image          lectional restrictions in a given context (e.g. “my car drinks
   schemas, b) associating perceptual arguments and the relation        gasoline”).
   with linguistic units, c) discovering a linguistic structure en-        Other approaches use syntactic and co-occurrence statistics
   coding the schema, and finally d) enriching the semantics of
   the schema via extended language usage (via a corpus). In            across large corpora to identify metaphors. We may call these
   the first three phases, we use no prior knowledge about either       attempts corpus-driven; work here may include Shutova, Sun,
   the perceptual or language domains; in the corpus analysis, we       and Korhonen (2010) who demonstrate metaphor paraphras-
   use the WordNet ontology. Our input is an animation based
   on the Heider-Simmel video, together with a small corpus of          ing using noun-verb clustering, or Kintsch (2000) who effec-
   transcribed commentaries. From the image sequence, we clus-          tively uses Latent Semantic Analysis to interpret metaphors
   ter the visual angle subtended by a landmark, and find that          like “My lawyer is a shark”. Cormet (Mason, 2004) is able
   one cluster reflects containment. This is then correlated with
   the sentences from the adult commentaries uttered contempo-          to find mappings given separated datasets for two domains,
   raneously with containment situations, yielding strong object-       e.g. it finds LIQUID → MONEY once provided with LAB and
   nouns and relation-preposition associations. For discovering         FINANCE specific corpora to train from. Corpus-based ap-
   linguistic constructs, we use no knowledge of grammatical cat-
   egory or syntax but find recurring patterns using the approach       proaches keep the metaphor mapping implicit, i.e. while the
   of (Solan, Ruppin, Horn, & Edelman, 2002). Knowing the               system can identify many metaphorical usages, the source
   units involved, we can identify several phrasal patterns (e.g.       domain has no grounding. Even distinguishing source from
   “X moved into”, “in the Y”). We then search a corpus with
   the “in the Y” schema to identify container words. We find           target domain is difficult, e.g. TIME co-occurs more often
   that the most common class involving containment is location         with SPEND than MONEY. Also, due to a primary reliance
   (66%), followed by group membership (20%), time, and cog-            on verbs, it becomes difficult to treat ontological metaphors
   nition (17% each). These may be thought of as language-based
   non-spatial enrichments for the image schema.                        like CONTAINER that are more preposition dependent. Most
                                                                        importantly, purely linguistic approaches are hard to extend
   Keywords: containment metaphor; grounded concepts; selec-            - e.g. container metaphors may invoke other attributes of the
   tional preference
                                                                        schema (e.g. ‘stir excitement’, or “the idea jumped out”).
                                                                           A third category of work, which we may call embodied
                           Introduction                                 modeling (Narayanan, 1997), is more cognitively motivated.
Containment metaphors arise in infancy and may help orga-               A model is learned from a tagged training set simulating pre-
nize the adult conceptual system (Mandler, 2010). The ear-              motor cortical representation of movement (Bailey, 1997);
liest structures(image schema) may arise initially in percep-           this is then mapped to other domains to interpret metaphoric
tion, but are then enriched by language in several ways, and            usage such as “India releases the stranglehold on business”.
extended to various non-spatial categories. Thus, a sentence            The embodied approach is appealing and elegant, but is hard
such as “I put a lot of energy into washing the windows.”               to scale up because new training data for learning the schema
reflects the schema ACTIVITIES ARE CONTAINERS in the in-                have to be manually created, and the syntactic structures re-
fluential Conceptual Theory of Metaphor (CTM) (Lakoff &                 quire knowledge of the language.
Johnson, 1980).                                                            In this work, we present a grounded model where initial
   While such extensions of the initial spatial schema become           image schemas are discovered from untagged video, and are
conventionalized in a linguistic group, they retain the ground-         then associated with textual commentary without using any
ing. So while starting with the final text is not very useful,          prior language models. We focus on n-gram models, and on
the grounded interpretation gives it much more flexibility. A           discovering merged paths through the sentence graphs (Solan
computational study of the process would a) suggest mech-               et al., 2002). Once we have a basic construct, we can enrich
anisms for understanding this process, and b) may itself be             the schema by exposing the learner to lots of language situ-
useful computationally - e.g. by providing an interpretation            ations, which is simulated here by considering the 1-million
via simulation using the original grounding.                            word Brown corpus.
   While much computational work has been done on
metaphors, there appears no work that attempts such a ver-              Motivation
tical sweep from the initial perceptual schema to a language            This work combines ideas of metaphorical extension from the
corpus. The emphasis within the NLP paradigm has been                   seminal work of Lakoff and Johnson (1980) together with the
                                                                    2079

developmental ideas from Mandler (2010). Both suggest a
strong role for spatiality in adult conceptual structures. Con-
tainment is discriminated by infants by the age of 2.5 months,
and becomes “accessible” by 5.5 months, when it is used
for multiple activities including visual and manual explo-
ration (Spelke & Hespos, 2002). This may imply the presence           Figure 1: Multimodal input: 2D video “Chase”: Three
of a mental structure incorporating arguments like a container        shapes, big-square([BS]), small-square([SS]) and circle([C])
and at least one trajector, and a function that given a config-       interacting with each other and the static box([box]).
uration, accepts it as an instance of containment. This struc-
ture, which may be called an initial image schema, is even-
tually mapped to language, when containment is acquired               be considered as instances of the image schema. We wish
before support (IN before ON before UNDER). This acqui-               to learn such an image schema given a simple video as input
sition reflects an awareness not only of the preposition, but         (Figure 1), where three objects - a big square ([BS]), a small
also for the linguistic argument structure that maps the im-          square ([SS]) and a circle ([C]) are moving around, interact-
age schema. But after this point, linguistic usage adapts the         ing with each other and going in and out of a static [box] via
concept in ways that are specific to the linguistic-cultural con-     a door. Though the objects deform a bit while rotating, and
text (Spelke & Hespos, 2002). Extensions emerge involving             also occasionally overlap, it is relatively straightforward to
new structures that transfer the relationship to new domains,         segment them.
not only in language, but also in thought. Over increasing ex-           The linguistic database consists of a co-occurring narrative
posure, many of these extensions become conventionalized,             with 36 descriptions of the video. These narratives exhibit a
many of which are listed in the CTM corpus.                           wide range of linguistic variation in terms of linguistic focus,
                                                                      lexical choice and construction. In an earlier work in learning
   To get a baseline check, we compiled 85 containment
                                                                      prepositions and nouns from the same multi-modal data, we
metaphors from Lakoff and Johnson (1980); of these, 65 in-
                                                                      used dynamic bottom-up attention to correlate objects seen in
volve prepositions in/into/out (IN a lot of trouble, INTO the
                                                                      the video with their linguistic counterparts(nouns) (Mukerjee
century); the remaining 20 involve verbs explode, erupt, fill
                                                                      & Sarkar, 2007). In this work, we further consider the subset
or adjectives full, empty - which profile other aspects of the
                                                                      of utterances which have a temporal overlap with the frames
containment schema (fullness, enclosed-ness etc).
                                                                      during which a containment situation prevails.
   Given this picture of the metaphor acquisition, extension,
and conventionalization process, our goal is to try to model          Acquiring Containment Prepositions
this embodied developmental process computationally, right
upto the point where language affects and changes the im-             In spatial reasoning, there have been several attempts at defin-
age schema. In this process, we would like to minimize the            ing spatial relations involving continuum measures defined
domain knowledge available to the system; we assume only              over different geometric features on object pairs. Regier
a large set of statistical learning tools, and a preference for       (1996), a seminal work in preposition grounding, uses an-
smaller explanatory structures. Of course, we cannot model            gle measures and a connectionist network to correlate videos
many important factors like social, interactive aspects.              and prepositions. The work, however, is limited in the sense
                                                                      that Regier uses videos annotated with single words like IN ,
   The next section focuses on how an uninformed agent, with
                                                                      OUT, THROUGH etc. while we hope to learn these schemas by
a capability for statistical learning, may acquire the contain-
                                                                      clustering the untagged video. Also, because his videos are
ment schema as a cluster in its sensory space. The follow-
                                                                      tagged with prepositions, he never has to work to discover
ing sections discuss the discovery of linguistic units (and n-
                                                                      the preposition; we have to discover these units from the un-
grams), the discovery of linguistic constructions associated
                                                                      constrained unparsed narrative. Mukerjee and Sarkar (2007)
with containment, and finally, the mapping to a large corpus.
                                                                      use the same dataset as ours, but use a measure based on vi-
                                                                      sual proximity - the Stolen Voronoi Area - to cluster space
      Learning Containment from Perception                            using Kohonen Self Organising Maps (SOMs). We initially
Linguistic concepts are cognitively characterized in terms            tried these two approaches and find that in an unsupervised
of image schemas, which are schematized recurring pat-                clustering task (k-means with 6 classes), these earlier models
terns from the embodied domains of force, motion, and                 do not work well for distinguishing the inside and outside of
space(Langacker, 1987; Lakoff & Johnson, 1980). The pre-              irregular (L- or U-shaped) containers (1st row, Fig 2). In a
cise structure of an image schema remains quite unclear, with         supervised scenario they show good results training with so-
different authors using differing characterizations. In this          phisticated neural-nets over multiple epochs, but our goal is
work, we take an image schema to consist of two related               to try not to use supervision data.
structures. First is the list of arguments which participate in          Another feature implicated in place learning in animals is
the associated relation or activity. The other is a characteriza-     visual angle (Rolls et al., 1999) - the angle subtended by a
tion of the situation in terms of a function defined over some        landmark on the retinal image. We attempted to improve on
feature space, so that situations satisfying this function may        the previous features by using a single feature - - the total
                                                                  2080

angle subtended by a landmark at the object position. With           Linguistic Elements Describing Containment
this measure, we find that when the resulting feature space is       We now detail the discovery of linguistic elements pertaining
clustered, one of the clusters works quite well for identifying      to containment through correlation of the commentary with
the IN-schema. Computing this feature involves computing             the acquired schemata. We restrict our analysis to utterances
the angle that the landmark, [box], would subtend at each            occurring while [BS] is in the IN-cluster w.r.t. [box]. Since
point in the space; the result is measured and clustered using       the commentaries are unconstrained and there is no syntactic
k-Means (k = 6). We can see in Fig 2 (bottom row left) that          information, every uttered word is a possible label. Given an
one cluster completely covers what may be thought as the in-         uttered unit wi , the probability that it refers to schema s j , is
side of [box], whereas the the outside is graded between a           given by:
number of clusters. If we accept this as a characterization
for an image schema for containment, then the distribution                                       P(wi |s j )P(s j ) P(wi |s j )
                                                                                 P(s j |wi ) =                      ∝              = Arij
of visual angle in this cluster will serve to represent this re-                                     P(wi )             P(wi )
lation. To test whether this model, learned from the single
[box] shape, really represents the category of containment re-       This metric, the relative association(Arij ), is prone to give er-
lations, we generalize and evaluate it over a number of other        roneous results for infrequent units, while working well for
shapes. The results of applying the same learned distribution        high frequency words. For example, it gives an association
to three novel shapes is shown in Fig 2(bottom row). We find         value of 1 for a word that has been uttered only once in the
that regions with varied levels of ‘IN-ness’ have been sepa-         whole commentary. To counter this trend, we also subscribe
rately grouped, validating our choice of features. While for         to mutual information between states s j and words wi , which
closed convex shapes the measure has a clear demarcation             eliminates the possibility of uninformative rare words being
of ‘inside’(360◦ angle), it gives a more graded assessment for       assigned a high score. The word-object association is then
open figures as well, such as the open-top square in Fig 2(2nd       estimated using the product of mutual information of word wi
row, 4th fig).                                                       and state s j with their joint probability,
                                                                                                                     Pr(wi , s j )
                                                                                        Am i j = Pr(wi , s j ) log
                                                                                                                   Pr(wi )Pr(s j )
                                                                     where Am    i j is the mutual association. We use this measure
                                                                     because if W (= ∪i wi ) and S(= ∪i si ) are two random variables
                                                                     then their Mutual Information I(W, S) would be
                                                                                                                    Pr(wi , s j )
                                                                          I(W, S) = ∑ ∑ Pr(wi , s j ) log                          = ∑ Am
                                                                                         i     j                  Pr(wi )Pr(s j ) ∑   i j
                                                                                                                                          ij
                                                                     Ami j is, thus, the contribution of each word object pair. The
Figure 2: k-Means Clustering of Space. a) Voronoi and An-            results are shown in Fig 3. Notice that in, inside and into
gle features (top row) and b) Visual Angle feature (bottom           emerge as the three dominant monograms (their frequencies
row). The inside of all the containers has been clearly identi-      in the containment subset are 28, 26 and 15 of 1100 words).
fied as a separate cluster only in the latter case.                      Before moving onto syntax discovery, we observe that
                                                                     the nouns corresponding to the three objects in the video,
                                                                     had been acquired earlier using a attentional correlation
   At this stage, the system computes the visual angle sub-          model (Mukerjee & Sarkar, 2007). These will be used in the
tended at an object position, for a landmark (the box or some        next section: “big square” for [BS], “little square” for [SS],
other shape). The two arguments participating in this compu-         and “circle” for [C].
tation are the container and the trajector, though the system
does not use these terms; these relationships are implicit in        Deriving Syntactical Structure
the feature computation. If the visual angle falls within the        Discovering syntactic structures of a containment preposition
distribution associated with containment (the IN-cluster), it        like IN will enable us to discover other labels for objects par-
is accepted as an instance of this relation. Thus the system         ticipating in containment. For example, if we know that only
has both the arguments, and the acceptance function charac-          object A with label lA is in container B with label lB , the per-
terizing the image schema, This acquisition is pre-linguistic,       ceptual arguments of IN are {trajector:A, container:B}. Now,
from perceptual data alone. When a pre-discovered object,            suppose at the same time we hear the utterance: ‘lA goes into
say [BS], lies in IN-cluster, we have the argument structure         lC ’. If we know the linguistic argument structure associated
{[BS] IN [box]} or IN([BS],[box]). After learning the unit           with IN, then we can have a high confidence from this single
IN , we can attempt to map this perceptual argument structure        instance that lC is most likely another label for the container
to linguistic syntactical structure, thereby discovering aspects     B. Once internalized, this process would help the agent recre-
of syntax.                                                           ate context in a novel discourse, by simulating the action and
                                                                 2081

                (a) Monogram association                (b) Bigram association                   (c) Trigram association
Figure 3: Association of words with the containment cluster. Both the association measures, as used previously for noun-label
learning, are shown. The red bars indicate the Relative Frequency measure while the green bars are for the Mutual Information
Measure.
identifying probable trajectors and landmarks via the syntac-        found are:
tic argument structure. In the example above, if we don’t                            
                                                                                           ball
                                                                                                   
know what ‘goes’ refers to, some idea of this may be formed
                                                                                the  door  
                                                                                                                 
by realizing that it is something that A and B may be partici-                        box                 move
pating in. This goes along with Siskind (1996)’s constraining                                         →  came  → into
                                                                                        square       
hypotheses with partial knowledge: “ When learning word                        
                                                                               
                                                                                                      
                                                                                                              got
meanings, children use partial knowledge of word meanings                              [circle]
to constrain hypotheses about the meanings of utterances that
contain those words.”                                                                               
                                                                                              in
   We start our discovery of syntactic structure by analyz-                               inside  → the → box
ing bi- and tri-gram correlations, which are associated with                                 into
the same metric as in the monogram case. We observe that
                                                                        Clearly, these structures are discovering some relevant pat-
the prominent bi-grams are inside the, into the and in
                                                                     terns for containment, including the group in | into |
the. The tri-grams that emerge are inside the box, in
                                                                     inside which was also observed in the trigram model. The
the box and into the box. These associations could help
                                                                     noun cluster - with box and door is also an impressive dis-
learn the label of not only the containment schema, but also
                                                                     covery. But most effective is the fact that box is identified
the container itself. Note that the attention based model for
                                                                     as a label appearing after the IN while the trajector appears
learning nouns cannot learn the container/box, which is never
                                                                     before.
dynamically salient. Thus, it’s label is not known. However it
                                                                        One of the uses of this structure would be the discovery
is prominent in these containment sentences, and discounting
                                                                     of synonyms. For example, consider the sentence large
the frequent word the in trigrams such as “{ inside | in
                                                                     square moves into the box, which is being uttered as the
| into} the box”, we may associate box with the [BOX],
                                                                     agent perceptually knows that [BS] is moving into IN-cluster,
treating it as a label for the container. We also note the pres-
                                                                     activating schema IN([BS],[box]). By aligning this with the
ence of other fragments (big square goes and is trying
                                                                     linguistic input, large square is learned as a referent for
to) as prominent trigrams; but these are present only in this
                                                                     [BS], i.e. a synonym for “big square”. Also, an unit appear-
context, and will be diluted as the agent looks at other con-
                                                                     ing frequently in the trajector position in the discourse is “it”,
tainment situations. But despite some glimmers, the n-gram
                                                                     which has no constant referent; it is possible now to discover
approach is not very illuminating regarding the construction
                                                                     that this is a unit which may refer to multiple objects. Further
encoding for containment.
                                                                     analysis may reveal that “it” is strongly correlated to the most
   A richer model of syntactic structure has been developed          recently observed trajector - and thus our system may begin
over many years by Edelman and his group, implemented as             its journey of understanding anaphora, another computation-
the tool ADIOS (Solan et al., 2002). In this approach, a graph       ally promising domain in language.
called Representational Data Structure (RDS) is constructed
from the morphologically segmented input sentences. It then           Metaphorical Mappings from Language Usage
repeatedly scans and modifies the RDS in an attempt to merge         We have alluded to learning metaphorical maps through asso-
edges and come up with a more compact description of the             ciation before. We are in a position where we have grounded
input. In the process, a number of syntactic clusters and            concepts of agents taking part in an event, and the concept
constructions are discovered without any prior knowledge of          of containment, through a mapping for linguistic element IN.
grammatical categories. Examples of some of the patterns             Consider the following occurrences:
                                                                 2082

1. What state is the project IN?            IN-STATE (project,
                                                                       Table 1: Selectional association strength of different classes
    state)
                                                                                     Class      SA       Class      SA
2. He did it IN three minutes. IN-TIME (he, 3min)
 Here, the abstract concepts of STATE , TIME etc. are under-                        location 0.658        act      0.058
 stood in terms of a container, thanks to their syntactic associ-                    group     0.201 artifact 0.077
 ation with linguistic instances of “IN a BOX (container)”, for                       time     0.175    object     0.055
 which the learner has a physical basis. It’s therefore prudent                    cognition 0.164       food     -0.030
 to assume that metaphorical concepts would occur in similar                          state    0.145 animal -0.042
 lexico-syntactic environments in language usage. We have
 demonstrated the ability of the agent in discovering the ‘con-
 tainer’ through ADIOS; the question we would like to ad-                WordNet (Feinerer & Hornik, 2011) is our knowledge-base
 dress now is, would it also be able to discover the object of        for class c. WordNet was developed as a system that would be
 containment in a novel context with sentences of myriads of          consistent with the knowledge acquired over the years about
 different structure? We investigated it by running ADIOS on          how human beings process language. To represent the early
 IN-containing sentences of Brown corpus. We find that it can,        learner’s limited concept-repository, only top level classes of
 indeed, distinguish ‘containers’ from other elements of a sen-       WordNet are considered. We use the Brown Corpus as the
 tence, as evidenced by the following pattern:                        sample space to determine the selectional preferences. All the
                                                                      sentences involving the containment concepts, i.e. all 21,480
                                                              
                       building war           f ight       car        sentence-parts with words in/into/inside were extracted.
      in → the →                                                      The sentences with IN were converted to the functional form
                        group death woods cellar
                                                                      of IN(pobj) in a rather simple way: the first occurrence of
    While discovery of ‘container’ elements would be the first        a noun after IN in the tagged corpus was assigned to the
 evidence of mapping of abstract concepts to the ‘container’,         concept. For example, the sentence fragment into a hot
 the mappings would be prominent only if further evidence             cauldron is converted to IN(cauldron).
 abounds in language, so that learning due to false evidence             The most occurring ‘container’ words were world, way,
 is minimized. The propensity of a concept to be described            order, years, case, states etc. The resultant associa-
 as a ‘container’ can be gauged through the regularity of its         tions are shown in Table 1. Notice that Location class has
 occurrence in the object position of IN. In literature, selec-       the highest association for container schema, activating a LO -
 tional preference (SP)(Resnik, 1993) is used abundantly to           CATIONS ARE CONTAINERS mapping. Group class also has
 measure regularities of a verb w.r.t. the semantic class (sub-       a strong association to containers, representative of the no-
 ject, object etc.) of its argument. It has been used previously      tion that groups or teams are visualized as containers (in a
 for word-sense disambiguation(Resnik, 1993) and metaphor             group, in a team). Time, Cognition and State also show
 interpretation(Mason, 2004). While it has only been used for         high associativity, while Food and Person class demonstrate
 finding verb-preferences, we will adapt it to include prepo-         a significantly negative mapping. The negative numbers only
 sitional preferences, so that we are able to learn contain-          represent the weakness of mapping, and should not be treated
 ment metaphors. While verbs have different syntactic re-             as repudiating existence of the same. The association mea-
 lations like verb-object or subject-verb, the prepositions we        sures only demonstrate that some mappings are used more
 are considering, have only one relation to the trailing noun,        in language, and consequently, are stronger in our cognition
 that of Object of Preposition (pobj)(according to the Stanford       than others. In fact, in the original metaphor list(Lakoff &
 Parser(Marneffe & Manning, 2006)). Therefore, the formula-           Johnson, 1980), the most prominent mappings to container
 tion from Resnik (1993) is slightly modified. The selectional        are those of Cognition(15%), State(14%), Location(7.3%),
 association of class c for predicate p(IN) is defined as:            Group(8.6%), Time(5.4%) and Act(4.8%), somewhat rep-
                                                                      resentative of their strength acquired from the whole cor-
                                1             P(c|p)                  pus. Similarly, the least occurring classes in the list too are
                A(p, c) =          P(c|p) log
                              S(p)               P(c)                 Plant(0.3%), Animal(0.3%) and Food(1%). Some examples
                                                                      of sentences from both the Brown corpus and metaphor list
 where,
                                                                      are presented below:
                                                        P(c|p)        • STATE/COGNITION AS A CONTAINER
        S(p) = D(P(c|p)||P(c)) = ∑ P(c|p) log
                                        c                P(c)            – Meredith began falling in love. (Brown)
                                                                         – We’re IN a mess. (Lakoff & Johnson, 1980)
 and,
                                                count(p,w)            • TIME AS A CONTAINER
                          f req(p, c) ∑wεc classes(w)
              P(c|p) =                =                                  – We’re well into the century. (Lakoff & Johnson, 1980)
                            f req(p)        f req(p)                     – There comes a time in the lives of most of us when we
 where count(p, w) is the number of time word w occurred,                   want to be alone. (Brown)
 and classes(w) is the number of classes it belongs to.               • LOCATION AS A CONTAINER
                                                                  2083

   – If you’ve travelled in Europe a time or two , . . . (Brown)      Chalnick, A., & Billman, D. (1988). Unsupervised learn-
   – . . . and begin to feel at home in the capitals of Eu-             ing of correlational structure. In Proc. 10th Annual Cogsci
      rope. . . (Brown)                                                 Conf. (pp. 510–516).
• ACT AS A CONTAINER                                                  Fass, D. (1991). Met*: A method for discriminating
   – How did you get into window-washing as a profession?               metonymy and metaphor by computer. Computational Lin-
      (Lakoff & Johnson, 1980)                                          guistics, 17(1), 49-90.
                                                                      Feinerer, I., & Hornik, K. (2011). wordnet: Wordnet inter-
                          Conclusion                                    face [Computer software manual]. Available from http://
                                                                        CRAN.R-project.org/package=wordnet (R package
In this work, we proposed a plausible method for a primi-               version 0.1-8.)
tive artificial agent with multi-modal input handling and fea-        Hill, J. A. C. (1983). A computational model of language ac-
ture extraction capability, to internalize linguistic concepts of       quisition in the two-year old. Cognition and Brain Theory,
containment. Containment metaphors are a primary way in                 6, 287–317.
which humans understand abstract concepts of state/emotion            Kintsch, W. (2000, July). Metaphor comprehension: A com-
etc. and it’s therefore necessary for a cognitive system to             putational theory. Psychonomic Bulletin and Review, 257-
be able to do so if it’s to acquire human level intelligence.           266.
Through a grounded system and selectional preference of IN,           Lakoff, G., & Johnson, M. (Eds.). (1980). Metaphors we live
we also showed how the model can internalize conventional               by. University of Chicago Press.
metaphors in vogue in English.                                        Langacker, R. (Ed.). (1987). Foundations of cognitive
   In the work, we have used some novel ideas, like that                grammar I: Theoretical prerequisites. Stanford University
of modifying selectional preference to include prepositional            Press.
bindings, which is, to our knowledge, unexplored in litera-           Mandler, J. M. (2010). The spatial foundations of the con-
ture. Also, through simple methods of spatial feature extrac-           ceptual system. Language and Cognition, 2(1), 21-44.
tion, unsupervised clustering and word-cluster association,           Marneffe, B. M. Marie-Catherine de, & Manning, C. D.
we have been able to extract the idea of containment.                   (2006). Generating typed dependency parses from phrase
   Nonetheless, this work can benefit from much improvisa-              structure parses. In Proceedings of LREC 2006.
tion. The image schema for containment that we provide, is a          Mason, Z. J. (2004, March). Cormet: a computational,
very crude one. While it’s possible to extend it to more gen-           corpus-based conventional metaphor extraction system.
eral scenarios, as has been demonstrated, we haven’t inves-             Comput. Linguist., 30, 23–44.
tigated its limitations on non-convex shapes. That, however,          Mukerjee, A., & Sarkar, M. (2007). Grounded acquisition of
hardly undermines our contribution here since in non-convex             containment prepositions. In Proc. ICON.
shapes, even adults find it difficult to separate regions into        Narayanan, S. (1997). Knowledge-based action representa-
inside/outside, and it largely becomes an outcome of some               tions for metaphor and aspect (karma). Unpublished doc-
context. Furthermore, our aim was to show that even with                toral dissertation, CS, UC Berkeley.
so meagre a sense-world (that of one video and associated             Regier, T. (1996). The human semantic potential: Spatial lan-
commentaries), an artificial agent can get some semblance of            guage and constrained connectionism. Bradford Books.
human-like notions of containment and employ them to in-              Resnik, P. S. (1993). Selection and information: A class-
culcate metaphorical mappings in its system. Instead of 81              based approach to lexical relationships (Tech. Rep.).
seconds of learning, however, the human learner has days and          Rolls, E., et al. (1999). Spatial view cells and the represen-
months and years of exposure, and clearly this can lead to the          tation of place in the primate hippocampus. Hippocampus,
construction of extremely rich and diverse schemata.                    9(4), 467–480.
   While we have handled concepts that easily fall into a             Shutova, E., Sun, L., & Korhonen, A. (2010). Metaphor
container mould due to usage of IN, we have not been able               identification using verb and noun clustering. In Proc. of
to model other container metaphors that have different at-              COLING (pp. 1002–1010).
tributes, like the 20 out of our list of 85 that depend on verbs      Siskind, J. (1996). A computational study of cross-situational
and adjectives related to substance. Simulating all that is in-         techniques for learning word-to-meaning mappings. Cog-
deed a Herculean task in this limited set-up. We intend to              nition, 61, 39-91.
look into these aspects and also, to matters of finding more          Solan, Z., Ruppin, E., Horn, D., & Edelman, S. (2002). Au-
mappings using WordNet’s synsets (at present we are using               tomatic acquisition and efficient representation of syntactic
only the lexical files – synset usage would lead to discovery           structures. In Proc. of NIPS.
of more maps, but it would also make the process noisy) in            Spelke, E., & Hespos, S. (2002). Conceptual development in
our future work.                                                        infancy: The case of containment. Representation, mem-
                                                                        ory, and development: Essays in honor of Jean Mandler,
                          References                                    223–246.
Bailey, D. (1997). A computational model of embodiment in
   the acquisition of action verbs.
                                                                  2084

