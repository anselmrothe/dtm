UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
When gestures catch the eye: The influence of gaze direction on co-speech gesture
comprehension in triadic communication
Permalink
https://escholarship.org/uc/item/0vt435ns
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Holler, Judith
Kelly, Spencer
Hagoort, Peter
et al.
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

   When gestures catch the eye: The influence of gaze direction on co-speech gesture
                                      comprehension in triadic communication
                                              Judith Holler (judith.holler@mpi.nl)1,2
                                                Spencer Kelly (skelly@colgate.edu)3
                                             Peter Hagoort (peter.hagoort@mpi.nl)1,4
                                               Asli Ozyurek (asli.ozyurek@mpi.nl)1,5
                  1
                    Max Planck Institute for Psycholinguistics, Wundtlaan 1, 6525XD Nijmegen, The Netherlands
         2
           University of Manchester, School of Psychological Sciences, Coupland Building 1, M13 9PL Manchester, UK
   3
     Colgate University, Psychology Department, Center for Language and Brain, Oak Drive 13, Hamilton, NY 13346, USA
         4
           Donders Institute for Brain, Cognition and Behaviour, Montessorilaan 3, 6525 HR Nijmegen, The Netherlands
              5
                Centre for Language Studies, Radboud University, Erasmusplein 1, 6525HT Nijmegen, The Netherlands
                              Abstract                                 with that contained in the accompanying speech (e.g., Holle
   Co-speech gestures are an integral part of human face-to-face
                                                                       & Gunter, 2007; Holler, Shovelton & Beattie, 2009; Kelly,
   communication, but little is known about how pragmatic              Barr, Church & Lynch, 1999; Kelly, Kravitz & Hopkins,
   factors influence our comprehension of those gestures. The          2004; Willems, Özyürek & Hagoort, 2007). However, one
   present study investigates how different types of recipients        limitation of studies on the comprehension of gestures is
   process iconic gestures in a triadic communicative situation.       that many of them have presented stimuli in isolation, that
   Participants (N = 32) took on the role of one of two recipients     is, video clips showing iconic gestures (and sometimes a
   in a triad and were presented with 160 video clips of an actor      torso) but, crucially, no head or facial information, and
   speaking, or speaking and gesturing. Crucially, the actor’s eye
   gaze was manipulated in that she alternated her gaze between        those studies that have included the face have tended to
   the two recipients. Participants thus perceived some messages       focus on the lips. In face-to-face communication, however,
   in the role of addressed recipient and some in the role of          gestures are not only accompanied by speech and mouth
   unaddressed recipient. In these roles, participants were asked      movements, but also by a multitude of additional nonverbal
   to make judgements concerning the speaker’s messages. Their         social cues. Instead of focusing our attention solely on
   reaction times showed that unaddressed recipients did               speech and gesture when listening to someone speaking we
   comprehend speaker’s gestures differently to addressees. The
                                                                       are required to divide our cognitive resources in such a way
   findings are discussed with respect to automatic and
   controlled processes involved in gesture comprehension.             that allows us to take in and combine all of those cues. How
                                                                       we process and comprehend iconic gestures in more situated
   Keywords: co-speech iconic gesture; eye gaze; recipient             contexts that are much closer to real life situations therefore
   status; communicative intent; multi-party communication.
                                                                       remains a wide-open issue.
                                                                          Of particular interest in this respect is the influence of eye
                          Introduction                                 gaze, one of the most powerful nonverbal social cues
When we speak, we frequently move our bodies to                        (Pelphrey & Perlman, 2009; Senju & Johnson, 2009). Eye
supplement what we say with co-speech gestures. A large                gaze is not only an omnipresent contextual cue when
proportion of these gestures are iconic in nature.                     observing co-speech gestures, it is also inherently linked to
Importantly, iconic gestures bear a close link with the                the perception of communicative intent (Kampe, Frith &
speech that they accompany on semantic and temporal                    Frith, 2003; Schilbach et al., 2006) and the regulation of
levels and have therefore been argued to constitute an                 social interaction (Argyle & Cook, 1976; Goodwin 1981;
integral part of human language (McNeill, 1992; Kendon,                Kendon, 1967). This begs the question of how the co-
2004) and thus of speaker’s utterances (i.e., ‘composite               occurrence of gaze and gesture influences recipients’
utterances’, Kendon, 2004). While iconic gestures have                 comprehension. The present study addresses this very
been shown to fulfill a variety of cognitive functions which           question.
appear to benefit the speaker him or herself (e.g., Chawla &              To do so, it builds on a couple of recent studies that have
Krauss, 1994; Hostetter, Alibali & Kita, 2007), there is a             begun to focus on the issue of perceived communicative
growing body of evidence that their production is also                 intent in conjunction with gesture comprehension. Kelly et
linked to the speaker’s communicative intent (Gerwing &                al. (2007, 2010) showed that participants integrated co-
Bavelas, 2004; Holler & Stevens, 2007; Kelly, Byrne &                  occurring information from speech and gesture less strongly
Holler, 2011; Özyürek, 2002).                                          when the two modalities were perceived as not intentionally
   The comprehension of iconic gestures, especially with               coupled (e.g., male hands gesturing accompanied by a
respect to the attribution of communicative intentions, has            female voice speaking) than when they were perceived as
been considerably less well researched. What we do know is             intended to form a composite utterance (e.g., male hands
that iconic gestures successfully communicate semantic                 gesturing accompanied by a male voice speaking). This is
information and that recipients integrate this information             the first empirical evidence that the perceived intentional
                                                                   467

stance of a communicator influences recipients’ processing           communicative situation involving one speaker and two
of iconic gesture and speech.                                        recipients, combined with a manipulation of the speaker’s
   However, as many previous studies in this field, these two        eye gaze direction which will indicate to participants when
studies did not present gestures in their natural context but,       they are an addressed and when they are an unaddressed
instead, in isolation of any facial cues (including eye gaze)        recipient. Thus, it is the first experimental study looking
with the aim to control for the influence of lip movements.          specifically on the effect of social eye gaze on speech and
In addition, and in line with the predominant gesture                iconic co-speech gestures.
comprehension paradigm at the time, both of the studies
used mismatching speech-gesture stimuli, that is, stimuli in                                   Method
which the information provided by speech conflicted with
that depicted by the accompanying iconic gestures. Whilst            Participants
this was an ideal test bed for first enquiries into the semantic     Thirty-two female, right-handed German native speakers
integration of speech and gesture, it compromises the                participated in the experiment (mean age = 21.6yrs) and
generalisability of such findings to more natural speech-            were compensated with 8€ payment.
gesture utterances, something the present study aims to
overcome.                                                            Design
   Another recent study that has addressed the topic of
                                                                     The study employed an experimental paradigm simulating
communicative intent and gesture comprehension was
                                                                     multi-party communication involving one speaker-gesturer
conducted by Straube et al. (2010). In their study,
                                                                     and two recipients, only one of which was a ‘real’
participants watched video clips of a speaker who looked
                                                                     participant (the other one was fictive). Participants were
directly at them or who was oriented away from the camera.
                                                                     made to believe that the other participant taking on the role
In contrast to previous studies on co-speech gesture
                                                                     of the second recipient was located in a different room.
comprehension, Straube et al.’s paradigm did include the
                                                                        A confederate acted as the speaker-gesturer and produced
speaker’s head and eye gaze, and, furthermore, they avoided
                                                                     scripted utterances to a video camera (we used pre-recorded
the use of mismatching gestures. However, the authors
                                                                     instead of live stimuli to ensure that all participants were
manipulated multiple nonverbal social cues simultaneously
                                                                     presented with identical stimuli). These pre-recorded video
(body/torso orientation, gesture orientation, as well as gaze
                                                                     clips were presented to participants while making them
direction), preventing us to draw conclusions about the
                                                                     believe that they were engaging in a live communication
effect of gaze direction specifically on participants’
                                                                     with the speaker, who they thought was located in yet a
comprehension. In addition, the information depicted by the
                                                                     different room to themselves but connected to them via a
gestures used as stimuli was redundant with that in speech
                                                                     live camera link. (The second recipient was, allegedly, also
(e.g., the speaker referred to a ‘round bowl’ in speech
                                                                     connected to the person acting as speaker via a live-camera
accompanied by a gesture depicting a round, bowl-like
                                                                     link, in the same way as they were.)
shape). It is therefore not possible to identify whether the
                                                                        To prevent participants from realising that the speaker
differences in participants’ comprehension (measured in the
                                                                     was pre-recorded, they were told that the camera link was a
form of their neural response and memory performance for
                                                                     one-way connection in that the two recipients were able to
the stimuli) between the two conditions (frontal/averted)
                                                                     hear and see the speaker but that the speaker was not able to
was due to differences in their perception of speech, gesture,
                                                                     hear or see them (and the two recipients were, of course, not
or a combination of the two.
                                                                     able to see or hear each other). They were also told that the
   The studies by Kelly et al. (2007, 2010) and Straube et al.
                                                                     speaker had been asked to stand in front of two different
(2011) are laudable first attempts tapping the issue of
                                                                     cameras, that she knew of the two recipients’ presence, and
communicative intent and gesture comprehension and useful
                                                                     that she had been told that each camera was hooked up to
stepping stones for further investigations on this topic. The
                                                                     one of the recipients’ computer monitors, allowing them to
present study aims to build on this work by investigating the
                                                                     hear and see what she was communicating. In order to
effect of perceived communicative intent, as signaled
                                                                     create a more plausible situation and convince subjects that
through the speaker’s eye gaze direction, on the
                                                                     the speaker did not memorise the entire set of scripted
comprehension of iconic gestures. Importantly, this study
                                                                     sentences by heart, the video clips showed the speaker
will be presenting gestures in a more natural context
                                                                     looking down before each sentence was spoken; participants
(including the head), manipulating social eye gaze as the
                                                                     were told that a laptop had been positioned on a table in
only social cue of interest, and it will be based on gestures
                                                                     front of the speaker displaying a black and white drawing
that match the speech but which are complementary in
                                                                     accompanied by a couple of words before each trial, and
nature. This, in conjunction with the particular experimental
                                                                     that the speaker had been instructed to communicate the
paradigm employed in this study, will allow us to tap into
                                                                     contents of the information displayed on the screen
the processing of gesture directly, and to zoom into
                                                                     spontaneously and in a way that felt natural to them (no
recipients’ processing of the verbal and the gestural
                                                                     explicit mention of gesture was made). They (the actual
components of the speaker’s messages separately. We will
                                                                     participants) were then informed that the speaker would
do so by creating a set-up simulating a triadic
                                                                     sometimes address them by looking into the camera linked
                                                                 468

to their own on monitor, and sometimes the other, second            speaker’s message) or the content of the gesture performed
recipient, by looking into the respective other camera. This        by the speaker in the video (gesture-related targets [20
created two different views for the actual participant, one in      items per condition]; designed to tap into the processing of
which they were directly gazed at, and one in which they            the gestural component of the speaker’s message).
observed the speaker’s gaze being averted (see Fig 1). Gaze
direction, as implemented through this manipulation,                Task
constituted our main IV (within-participants). In addition, as      Participants were asked to judge “whether the word
a second IV (modality), we manipulated the occurrence of            displayed on the screen had been mentioned by the speaker
gesture in association with the sentences spoken by the             in the preceding video”, thus requiring ‘yes’ answers for all
speaker in the video clips (within-participants).                   speech-related targets, and ‘no’ answers for all gesture-
                                                                    related targets. Reactions times (RTs) to participants’ yes/no
Stimuli                                                             answers (delivered via a button box; yes = dominant hand)
The experiment set-up described in the preceding section            as well as errors1 were recorded.
required the creation of four types of video stimuli: a) Direct
gaze (speech only), b) Averted gaze (speech only), c) Direct                                     Results
gaze (speech + gesture), d) Averted gaze (speech + gesture)         RTs for gesture and speech-related targets were entered into
(Fig. 1). In each video vignette, the actor spoke a short           two separate 2 (gaze: direct vs. averted) x 2 (modality:
sentence (canonical SVO structure), e.g. ‘she goes through          speech only vs. speech+gesture) repeated measures
the list’ (‘sie geht durch die Liste’). Crucially, the verb         ANOVA, excluding errors (constituting 2% of the total
included in the sentence was always manner unspecific, i.e.,        number of trials) and outliers (2 SD).
‘to go through’ (durchgehen). The iconic gestures
accompanying these verbs always specified the manner of             Speech-related targets
action, e.g., to tick items on the list (abhaken). This
manipulation allowed us to measure participants’                    Our first comparison concerned participants’ responses to
comprehension of the gestures independently of speech,              the speech-related targets (e.g., ‘to go through’
without using mismatching gestures (see Introduction).              (durchgehen)) designed to tap primarily into the processing
Participants watched the videos on a computer screen in a           of the verbal component of the speaker’s composite
soundproof experimental test booth; the audio signal was            utterances. This analysis revealed a significant main effect
presented via closed-back headphones. Materials were                of modality (p = .0001), with slower response times in the
presented with Presentation® software (www.neurobs.com).            speech + gesture conditions than in the speech only
                                                                    conditions. The main effect of gaze was not significant (p =
              A                 C                                   .090), and neither was the interaction between gaze and
                                                                    modality (p = .870).
              B                 D
      Figure 1: Examples of the four types of video stimuli
 used, A = Direct gaze (S only), B = Averted gaze (S only),           Figure 2: Addressed and unaddressed recipients’ (AR/UR)
     C = Direct gaze (S + G), D = Averted gaze (S + G).                    RTs (ms) in the speech-only and speech + gesture
                                                                         conditions for speech-related targets (error bars = SE).
Procedure
   First, participants completed six practice trials (showing a     Gesture-related targets
different actor). Before the start of the experiment proper,        Our main comparison focused on participants’ responses to
the experimenter made a fake phone call to check whether            the gesture-related targets (e.g., ‘to tick’ (abhaken))
‘the other participants’ were ready to start.                       intended to tap primarily into the processing of the gestural
   Participants then watched 160 videos (40 stimuli per
condition). Each video clip was followed by a written word
                                                                       1
(presented in capital letters, centre of screen) that matched             Due to restrictions on space, we only report our RT results
either the verb contained in the preceding spoken sentence          here. Note, however, that the error rate analysis revealed very few
(speech-related targets [20 items per condition]; designed to       significant differences, and those that did emerge did not relate to
tap into the processing of the verbal component of the              the relevant differences in RTs in a meaningful way.
                                                                469

component of the speaker’s composite utterances. This               decision on the modality in which this information was
analysis revealed no main effect of modality (p = .216) and         presented (since the gestural component of their mental
no main effect of gaze (p = .087). However, the interaction         representation is ‘incomplete’ or ‘fuzzy’). Further, we
between gaze and modality was significant (p = .045).               would argue that the underlying mechanism leading to this
Independently from the omnibus interaction effect, we               fuzzy representation would not simply be one of reduced
carried out two a priori contrasts (UR S+G vs. AR S+G; UR           attention. The reason for this is that, first, addressed and
S-only vs. AR S-only). These showed that the interaction is         unaddressed recipients do not differ in the number of errors
driven by unaddressed recipients taking significantly longer        they made, and, second, the modality effect we found for the
to respond in the S+G condition than addressed recipients in        speech-related targets is as strong for unaddressed as for
the S+G condition (p = .026). The comparison of                     addressed recipients. Thus, our data suggest that
unaddressed and addressed recipients’ RTs in the speech-            unaddressed recipients do process the gestural information
only conditions was not significant.                                but less strongly so, and that the reason for this is a
                                                                    modulation of perceived communicative intent rather than a
                                                                    pure decrease in attention.
                                                                        An alternative possibility is what we have called the
                                                                    ‘Competing Modalities Hypothesis’. The rationale
                                                                    underlying this account is that, while addressed recipients
                                                                    (in both dyadic and multi-party interactions) are expected to
                                                                    engage in mutual gaze with a speaker (Argyle & Dean,
                                                                    1976; Kendon, 1967), unaddressed recipients are free to
                                                                    disengage from the process of gazing at the speaker. This
                                                                    means that the default situation requires recipients who are
                                                                    directly addressed through a speaker’s gaze to split their
 Figure 3: Addressed and unaddressed recipients’ (AR/UR)            attention between information coming from multiple
       RTs (ms) in the speech-only and speech + gesture             modalities including speech, gesture, and gaze (and
    conditions for gesture-related targets (error bars = SE).       additional facial cues). Unaddressed recipients, on the other
                                                                    hand, have fewer visual social cues to process since the
                         Discussion                                 speaker’s gaze is averted from them. They may therefore
The present study has investigated co-speech gesture                zoom in on gesture, instead of processing gesture and gaze
comprehension in the presence of eye gaze, a powerful               simultaneously, and may thus have more cognitive
social cue integral to human face-to-face communication.            resources available to focus on the processing of gesture. As
Our findings reveal that recipients’ gesture comprehension          a consequence, in the current paradigm, unaddressed
is indeed influenced by the speaker’s eye gaze direction.           recipients are taking longer than addressed recipients to
More specifically, we have shown that, when a speaker’s             respond to the gesture-related targets (requiring a ‘no’
eye gaze is used to signal communicative intent in the sense        answer) because the gestural component of the speaker’s
of address, recipients who are currently unaddressed (but           utterance constitutes a more prominent component of their
ratified participants in a communication, Goffmann, 1981)           mental representation of the event described (since they
do process speech-accompanying iconic gestures differently          focused on gesture more). To declare something as not
to addressed recipients. This finding advances our                  having been mentioned by the speaker despite the stronger
understanding of human communication by pointing to an              memory trace of the gesture being at the forefront of their
important way in which pragmatic processes shape and                mind appears to be a difficult task.
influence the comprehension of co-speech gestures, which,               The present study was a fruitful undertaking as it has
to date, has been addressed by only a very small number of          shown that recipient status can influence gesture processing,
studies (Kelly et al., 1999, 2007, 2010; Straube et al., 2011).     but also because it allowed us to formulate two possible
    There are at least two competing interpretations of our         accounts of a potential process model explaining those
effects of eye gaze direction on gesture comprehension. One         effects. Currently, we are unable to unequivocally declare
is what we have termed the ‘Fuzzy Representation                    one of them as the more appropriate one, but further studies
Hypothesis’. According to this hypothesis, due to                   are currently underway tackling this issue (this on-going
unaddressed recipients perceiving the speaker’s gestures as         research will also add further insights into participants’
not intended for them (but for the other, gazed at recipient        visual fixations in the two recipient roles, and it tests
instead), they interpret the gesture to a lesser degree. As a       recipients’ gesture comprehension avoiding the suppression
consequence, they take longer to respond to the gesture-            of gestural information/no responses). That said, we believe
related targets because they have constructed merely a              that the Competing Modalities Hypothesis provides the
partial, or fuzzy, mental representation of the gestural            more intuitive account, and some additional data we have
meaning. They are aware that something relating to the              collected speak to this preliminary conclusion, too: as a
meaning of the word displayed on the screen may have been           follow-up analysis, we obtained ratings for all of our stimuli
presented to them, but they have a hard time making a quick         from an independent set of participants which gave us an
                                                                470

insight into the degree of ambiguity/clarity of the individual        easily and quickly accomplished that higher-order processes
gesture stimuli in the absence of speech. If the Fuzzy                involved in the processing of pragmatic information, and
Representation Hypothesis should hold, gestures that are              judgements of speaker-intentions in particular, may not have
more ambiguous in the absence of speech (i.e., less                   come into play.
pantomimic) should cause unaddressed recipients particular                 Responses to the gesture-related targets tell a different
problems of interpretation (since they require more                   story, however. Here, participants were slower in general,
interpretation effort and more integration work).                     indicating that this task might have been perceived as more
Unaddressed recipients should therefore have taken                    difficult (i.e., saying ‘no’ to indicate that a certain meaning
especially long to respond to those iconic gestures.                  had not been mentioned by the speaker, despite the meaning
However, when we correlated unaddressed recipients’ RTs               of the word displayed on screen being related to the
with the ambiguity ratings of the individual gestures, no             meaning in the speaker’s gesture). This seems plausible
relationship of this sort was found. One could argue, of              since participants here had to consult their mental
course, that more pantomimic gestures should slow                     representation more carefully by actively teasing apart what
unaddressed recipients down also if we assume that the                they heard and what they saw to arrive at a decision. In
Competing Modalities Hypothesis holds, since they might               order to answer accurately, they were essentially required to
‘stick’ particularly well in the participant’s mind; that is, the     suppress the gestural information they had received. This
gestural components of utterances accompanied by more                 contrasts with the speech-target situation where intrusion of
pantomimic gestures might become particularly prominent               the gestural information may have slowed participants down
parts of unaddressed recipients’ mental representations.              since there was more information to process, but the
However, this argument only holds if we assume that                   gestural information did not interfere as such; after all,
unaddressed recipients also integrate the verbal and the              ‘ticking’ items on a list is part of the event of ‘going
gestural information less than addressed recipients.                  through’ a list. To answer ‘yes’, which participants were
According to the Fuzzy Representation Hypothesis, this                required to do for speech-related targets, is still correct, even
would be the case, since processing the gestural information          if the gestural information is taken into account. Answering
less well will also affect the integration of this information        ‘no’ to the gesture-related targets involved a very different
with speech. According to the Competing Modalities                    process, as it required the temporary suppression of the
Hypothesis, however, unaddressed recipients may integrate             gestural information. Consequently, slower and more
speech and gesture to the same extent as addressed                    difficult information processing may have led to the
recipients, with the addition that they process the gestural          involvement of more controlled, higher-order cognitive
information more strongly than them. Therefore, our                   operations, which do take into consideration the intentional
favoured interpretation is the Competing Modalities                   stance of a speaker.
Hypothesis, but further research is needed before we can
draw firm conclusions.                                                Effects of eye gaze direction on speech processing
                                                                      Our results reveal that speech comprehension was not
Automatic and controlled processes in gesture                         different for addressed and unaddressed recipients2. One
comprehension                                                         possibility therefore is that gesture comprehension processes
With regard to the difference in response patterns for the            are more sensitive to perceived communicative intent as
speech-related and gesture-related targets, our results also          signalled through a speaker’s gaze than the processing of
relate to the distinction between automatic and controlled            speech is. However, we have to remain cautious with
processes in co-speech gesture comprehension (Kelly et al.,           drawing such a conclusion from the present data. This is
2010). Bear in mind that these two sets of targets were               because the current paradigm required participants to judge
designed to tap the processing of the speaker’s information           speech (i.e., whether a certain word had been mentioned in
in different ways. What is striking is that, in the case of           the preceding clip). As a consequence, participants will have
speech-related targets - a comparatively easy task - we               devoted much attention to the processing of speech,
observed a clear modality effect, as has been demonstrated            irrespective of their recipient status, since they were always
by previous comprehension studies. The semantic                       required to respond. In other words, a paradigm that does
integration of gesture and speech has been argued to be               not ask participants to explicitly devote attention to the
automatic in the sense of a low-level, fast and obligatory            verbal modality might reveal a modulation of eye gaze
process (Kelly et al., 2010; see also Kelly, Özyürek &                direction for the processing of speech only utterances also.
Maris, 2010). This explanation would account for the                  In fact, based upon the Competing Modalities Hypothesis,
intrusion of the gestural information (longer RTs in the
speech + gesture conditions) despite participants here                   2
                                                                           The lack of an effect of recipient status on the processing of
having been asked to judge the content of speech only. At             speech stands, at first sight, in slight contrast to a study by Schober
the same time, the results show a lack of an effect of our            & Clark (1989) who found overhearers to be slower and less
gaze manipulation, indicating that this task (saying ‘yes’ in         accurate in their understanding of verbal references than
response to a visually presented word that was presented              addressees. However, in their study, overhearers were not official
auditorily immediately prior to this) might have been so              recipients of the communication, which distinguishes them from
                                                                      unaddressed recipients (Goffman, 1981).
                                                                  471

one might expect to see such a modulation, since                    self activate brain regions associated with mentalising
unaddressed recipients have more cognitive resources                regardless of modality. Journal of Neuroscience, 23,
available that they are able to devote to the processing of         5258-5263.
speech. The present study was designed to mainly measure          Kelly, S. D., Barr, D., Church, R. B., & Lynch, K. (1999).
the processing of gesture, and future research is needed to         Offering a hand to pragmatic understanding: The role of
provide more conclusive answers to the question of how              speech and gesture in comprehension and memory.
speakers’ eye gaze direction influences the comprehension           Journal of Memory and Language, 40, 577-592.
of speech.                                                        Kelly, S. D., Byrne, K., & Holler, J. (2011). Raising the ante
                                                                    of communication: Evidence for enhanced gesture use in
Conclusion                                                          high stakes situations. Information, 2, 579-593.
In sum, our study suggests that recipients keep an eye on         Kelly, S. D., Kravitz, C., & Hopkins, M. (2004). Neural
where speakers are looking, and this subtle piece of                correlates of bimodal speech and gesture comprehension.
information has a significant impact on the extent to which         Brain and Language, 89, 253-260.
co-speech gestures are processed.                                 Kelly, S. D., Özyürek , A., & Maris, E. (2010). Two sides of
                                                                    the same coin: Speech and gesture mutually interact to
                   Acknowledgments                                  enhance comprehension. Psychological Science, 21, 260-
We thank four anonymous reviewers for their helpful                 267.
feedback on an earlier version of this paper. We also thank       Kelly, S. D., Ward, S., Creigh, P., & Bartolotti, J.
the European Commission for funding this research (Marie            (2007). An intentional stance modulates the integration of
Curie Fellowship, EU Project number: 255569), Manuela               gesture and speech during comprehension. Brain and
Schuetze and Nick Wood for extensive help with creating             Language, 101, 222-233.
the experimental materials, Ronald Fischer and Idil Kokal         Kendon, A. (1967). Some functions of gaze direction in
for their help with programming, the participants who took          social interaction. Acta Psychologica, 26, 22-63.
part in our study, as well as the NBL and GSL lab groups,         Kendon, A. (2004). Gesture: Visible action as utterance.
Ivan Toni, Natalie Sebanz and Guenther Knoblich, for                Cambridge: Cambridge University Press.
valuable feedback in discussions of the design of our study.      McNeill, D. (1992). Hand and mind: What gestures reveal
                                                                    about thought. Chicago: University of Chicago Press.
                                                                  Özyürek, A. (2002). Do speakers design their cospeech
                       References                                   gestures for their addressees? The effects of addressee
Argyle, M. and Cook, M. (1976). Gaze and mutual gaze.               location on representational gestures. Journal of Memory
  Cambridge University Press.                                       and Language, 46, 688–704.
Chawla, P., & Krauss, R. M. (1994). Gesture and speech in         Pelphrey, K. A., & Perlman, S. B. (2009). Charting brain
  spontaneous and rehearsed narratives. Journal of                  mechanisms for the development of social cognition. In J.
  Experimental Social Psychology, 30, 580-601.                      M. Rumsey & M. Ernst (Eds.), Neuroimaging in
Gerwing, J. & Bavelas, J.B. (2004). Linguistic influences on        Developmental Clinical Neuroscience. Cambridge
  gesture’s form. Gesture, 4, 157–195.                              University Press.
Goodwin, C. (1981). Conversational organization:                  Schilbach, L, Wohlschläger, AM, Newen, A, Krämer, N,
  Interaction between speakers and hearers. New York:               Shah, NJ, Fink, GR, Vogeley, K (2006). Being With
  Academic Press.                                                   Others: Neural Correlates of Social Interaction.
Holle, H., & Gunter, T. C. (2007). The role of iconic               Neuropsychologia, 44, 718-30.
  gestures in speech disambiguation: ERP evidence.                Schober, M. F., & Clark, H. H. (1989). Understanding by
  Journal of Cognitive Neuroscience, 19, 1175-1192.                 addressees and overhearers. Cognitive Psychology, 21,
Holler, J., Shovelton, H., & Beattie, G. (2009). Do iconic          211-232.
  gestures really contribute to the semantic information          Skipper, J. I., Goldin-Meadow, S., Nusbaum, H. C., &
  communicated in face-to-face interaction? Journal of              Small, S. L. (2009). Gestures orchestrate brain networks
  Nonverbal Behavior, 33, 73-88.                                    for language understanding. Current Biology, 19, 661-
Holler, J., & Stevens, R. (2007). An experimental                   667.
  investigation into the effect of common ground on how           Senju, A., & Johnson, M. H. (2009). The eye contact effect:
  speakers use gesture and speech to represent size                 Mechanisms and development. Trends in Cognitive
  information in referential communication. Journal of              Sciences, 13, 127-134.
  Language and Social Psychology, 26, 4-27.                       Straube, B., Green, A., Jansen, A., Chatterjee, A., &
Hostetter, A. B., Alibali, W. M., & Kita, S. (2007). I see it       Kircher, T. (2010). Social cues, mentalizing and the
  in my hand's eye: Representational gestures are sensitive         neural processing of speech accompanied by gestures.
  to conceptual demands. Language and Cognitive                     Neuropsychologia, 48, 382–393.
  Processes, 22, 313-336.                                         Willems, R. M., Ozyurek, A., & Hagoort, P. (2007). When
Kampe, K., Frith, C.D., & Frith, U. (2003). "Hey John":             language meets action: The neural integration of gesture
  Signals conveying communicative intention towards the             and speech. Cerebral Cortex, 17, 2322-2333.
                                                              472

