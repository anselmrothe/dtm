UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling online word segmentation performance in structured artificial languages
Permalink
https://escholarship.org/uc/item/2636g7jg
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Meylan, Stephan
Kurumuda, Chigusa
Frank, Mike
et al.
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

Modeling online word segmentation performance in structured artificial languages
                 Stephan Meylan                               Chigusa Kurumada                         Benjamin Börschinger
            smeylan@stanford.edu                           kurumada@stanford.edu               benjamin.borschinger@mq.edu.au
            Department of Psychology                       Department of Linguistics                  Department of Computing
                Stanford University                            Stanford University                       Macquarie University
                                  Mark Johnson                                             Michael C. Frank
                           mark.johnson@mq.edu.au                                      mcfrank@stanford.edu
                            Department of Computing                                    Department of Psychology
                              Macquarie University                                         Stanford University
                               Abstract                                   Griffiths, & Johnson, 2009). Thus, dependency structure has
                                                                          the potential to reduce segmentation performance.
   Lexical dependencies abound in natural language: words tend
   to follow particular words or word categories. However, arti-             In principle, however, there may also be the potential for
   ficial language learning experiments exploring word segmen-            increased performance in segmenting structured languages,
   tation have so far lacked such structure. In the present study,        especially if the learner is able to learn the dependency struc-
   we explore whether simple inter-word dependencies influence
   the word segmentation performance of adult learners. We                ture of the language along with the structure of individual
   use a continuous testing paradigm instead of an experiment-            words. For this reason, Goldwater et al. (2009)’s bigram
   final test battery to reveal the trajectory of learning and to al-     model outperformed other segmentation models. Modeling
   low detailed comparison with three computational models of
   word segmentation. Adult performance on languages with de-             dependency structure might also provide synergistic gains in
   pendencies is equal or lower to those without. Of the mod-             learning: Johnson and Tyler (2010) found that an ideal learner
   els tested, all perform worse on languages with dependencies,          that acquired words and word-object correspondences simul-
   though a novel particle filter-based lexical segmentation model
   produces learning curves most similar to human subjects.               taneously was far more successful at both than each indepen-
   Keywords: Word segmentation; statistical learning; bigrams;            dently, but only when the learner assumed a rich collocation
   computational modeling; dependency structures.                         structure in the language.
                                                                             Our current experiments test the relationship between de-
                           Introduction                                   pendency structure and segmentation performance for both
Human learners can use distributional information to segment              human learners and computational models. We created lan-
an unbroken speech stream into individual words after a short,            guages with varied levels of category size asymmetry and test
ambiguous exposure (Saffran, Aslin, & Newport, 1996; Saf-                 adult subjects’ performance in word segmentation based on
fran, Newport, & Aslin, 1996). Past artificial language learn-            these languages. Two-alternative forced-choice tests admin-
ing experiments have typically generated words according to               istered after a discrete training phase, as used by past studies,
a uniform word frequency distribution and randomly concate-               do not produce an interpretable time course of learning. Thus,
nated word types to create the artificial languages. The pro-             in Experiment 1 we use a new experimental paradigm that
cess of learning to segment such synthesized languages may                provides us with a time course of learning by testing subjects
deviate significantly from learning to segment natural lan-               throughout the duration of exposure to stimuli (Kurumada,
guage, which contains asymmetric word frequency distribu-                 Meylan, & Frank, under review). In Experiment 2, we cor-
tions, systematic dependency structure, and correlated varia-             roborate the results of the first experiment using a classic two-
tion in the lengths and frequencies of words.                             alternative forced choice paradigm. Both experiments, and a
   In natural language, word-by-word transitions are gov-                 set of simulations with three computational models, show that
erned by dependency relationships between word categories.                asymmetric word-category sizes support adult segmentation
For example, English prepositional phrases have the internal              learning considerably better than symmetric category sizes,
structure PP → P + NP, and an NP typically consists of a de-              but that performance on languages with dependencies is gen-
terminer and a noun. Since prepositions and determiners are               erally worse than performance on languages with randomly
lexical classes containing a relatively small number of short             ordered words.
words, many instances of PPs often result in collocations used
frequently in discourse (e.g., in the house or on a map).                                         Experiment 1
   Does such collocation structure make segmentation easier               To test the effects of dependency structure on word segmenta-
or more difficult? Both possibilities have some support in the            tion, we created two classes of artificial lexicons, one consist-
literature. Frequent phrases are known to be problematic for              ing of 12 word types and another of 8, and concatenated them
segmentation mechanisms, especially for algorithms that rely              to make languages with different dependency structures. Fig-
on transitional probabilities (TPs). Due to high internal TPs,            ure 1 is a diagram of the grammars that we used to produce
these phrases are often segmented as one unit, rather than sep-           the 12 word languages. Each sentence had four words. Three
arated into the multiple words that they contain (Goldwater,              of the language types (which we refer to as “1515,” “2424,”
                                                                      2002

            1-5-1-5                          3-3-3-3                by concatenating 2 – 4 syllables from a randomly selected
           language                         language                inventory of consonant-vowel syllables. Stimuli were synthe-
                                                                    sized with the MBROLA synthesizer (Dutoit, Pagel, Pierret,
                                                                    Bataille, & Van Der Vrecken, 1996) at a constant pitch of
                                                                    100Hz with 225ms vowels and 25ms consonants. Each sylla-
                                                                    ble appeared in only one word type in each lexicon.
                                                                       For the 12-type condition, 4 distinct languages were cre-
            2-4-2-4                        unstructured
           language                         language                ated by manipulating how many of 12 types (distinct word
                                                                    forms) appeared in each of four sentences positions (Figure
                                                                    1): 1515, in which the words in the first and third sentence
                                                                    position were drawn from categories of a single type and the
                                                                    second and the fourth from categories with five types each,
                                                                    2424, in which the words in the first and third sentence posi-
                                                                    tion were drawn from categories consisting of two types and
Figure 1: Schematic representations of the 4 types of lan-          the second and fourth from categories with four types, 3333 in
guages used in Experiment 1. Circles represent individual           which the word in each sentence position was selected from
words, arrows represent stochastic transitions.                     a category with three types, and unstructured, in which four
                                                                    words were selected uniformly at random from a single cat-
                                                                    egory of 12 types (without replacement, to avoid in-sentence
and “3333”) were generated via a simple finite state grammar,       repetition).
while the fourth (unstructured) language type was generated            To reflect the relationship between frequency and word
by uniformly concatenating words with no notion of sentence         length seen in natural language (Zipf, 1965), the category as-
position.                                                           signment of lexical items ensured word length and frequency
   Each of the structured languages had a category structure        were inversely correlated (the shortest words appeared in the
such that sentences followed the form ABCD, where each cat-         categories with the fewest types). Within each condition, 16
egory had a unique and non-overlapping set of words in it.          language variants were made using different phonemic inven-
The 1515 language, for example, had one word in the first           tories to control for unwanted phonological effects. The total
category, five in the second, and so forth. We refer to such a      number of word tokens per subject was 240 and the number of
language throughout as having “asymmetric” category sizes.          sentences was 60 in all languages. The total token frequencies
In contrast, a 3333 language has “symmetric” category sizes.        of words were 60-12-60-12 in the 1515 condition, 30-15-30-
The 8 word languages were generated similarly, but only in-         15 in the 2424 condition, 20-20-20-20 in the 3333 condition.
cluded three conditions (“1313,” “2222,” and unstructured).         In the randomized condition each word appeared in each sen-
   We used a sentence-by-sentence segmentation paradigm             tence position 5 times. Note that there was no discrete testing
(Kurumada et al., under review) to test adult learners’ seg-        phase separate from the training phrase; rather, subjects were
mentation performance and gather detailed information about         tested in the continuous paradigm on their knowledge of the
learning trajectories in these languages. Learners listened to      language as they learned it.
a set of sentences presented in a Flash web applet; after each         Stimuli for the 8-type condition were similar to those in the
sentence they were asked to click between syllables in an or-       12-type condition except that we created only 3 languages:
thograpghic transcription to indicate where they thought word       1313, with the first and third word position drawn from cat-
boundaries fell.                                                    egories with a single type and the second and fourth posi-
                                                                    tion drawn from categories with three types in each, 2222,
Methods
                                                                    in which each word position is selected from a category with
Participants For the 12 word languages, we posted 128               two types, and randomized, where each sentence was com-
separate assignments on Amazon’s Mechanical Turk (AMT)              posed from four words randomly selected from all 8 word
crowdsourcing platform and received 124 assignments from            types. 32 phonetic variants of each language were generated
distinct individuals. For the 8 word languages, we posted 96        to control for phonological effects. The per-subject exposure
assignments and received 95. Subjects were paid $1.00 for           was 240 tokens over 60 sentences, and the total token token
completing the task and were told that they would be paid           frequencies were 60-20-60-20 in the 1313 condition, and 30-
an additional $1.00 if they performed in the top quartile of        30-30-30 in the 2222 condition; in the unstructured condition
participants. The mean task duration was 14 minutes and 17          each word appeared 7 or 8 times in each position.
seconds. Subjects needed to transcribe a common English
                                                                    Procedure Before the experimental trials began, partici-
word at the beginning of the task to access the task to confirm
                                                                    pants were instructed to listen to and transcribe a short, com-
that they were English speakers.
                                                                    mon English word to confirm that their computer’s audio sys-
Stimuli We created two classes of lexicons differing in the         tem was working. Participants were then instructed that they
total number of word types (8 and 12). Words were made              would be presented with 60 consecutive sentences in a novel
                                                                2003

                                         12 Word Languages                                                   8 Word Languages
                          0.8             1515                                                0.8             1313
                                          2424                                                                2222
                          0.7             3333                                                0.7             Unstructured
                                          Unstructured
          Token F-score                                                       Token F-score
                          0.6                                                                 0.6
                          0.5                                                                 0.5
                          0.4                                                                 0.4
                          0.3                                                                 0.3
                          0.2                                                                 0.2
                          0.1                                                                 0.1
                                0   10     20    30      40   50   60                               0   10    20     30      40   50   60
                                                Trial                                                               Trial
Figure 2: Token F-scores (a measure of segmentation performance for individual words) plotted for Experiment 1. Symbols
represent mean performance for each condition (see legend) on a single trial, across participants. Black lines show a smoothed
estimate of performance (Lowess curves with a smoothing span of .5). Gray lines at bottom show permutation baselines based
on mean token F-score from reshuffling participant responses 1000 times.
artificial language, and that from the beginning they would                             the more asymmetrical the categories in the language were,
need to click the boundaries between syllables presented on                             the better participants learned. In both the 8-type and the
screen to indicate what they thought the boundaries between                             12-type, the slope of the unstructured condition is steepest,
words were in each trial. While they would not know the                                 demonstrating the greatest amount of learning of the latent
words at first, they were told that they would be able to dis-                          structure.
cern patterns and recognize at least some of the words as the
experiment progressed. Each trial contained both an audio                                  We analyzed token-by-token segmentation performance
presentation and an orthographic transcription with selectable                          separately for the 8 word and the 12 word languages using
boundaries between each syllable. After clicking segments in                            mixed-effects logistic models (Gelman & Hill, 2006; Jaeger,
a sentence subjects clicked a button marked “next” to proceed                           2008). The dependent variable in these models was whether a
to the next trial.                                                                      particular token had been segmented correctly. In the 12 word
                                                                                        languages, we found a strong positive main effect of the log
Results and Discussion                                                                  input frequency of that token (β = .19, p < .001) and a nega-
To assess subjects’ segmentation performance, we calculated                             tive effect of word length (β = −.56, < .001), as well as sig-
the precision, recall, and F-score (the harmonic mean of pre-                           nificant effects of the 3333 test condition (β = −.69, p < .05)
cision and recall, as described in Goldwater et al., 2009). This                        and the 2424 test condition (β = −.62, p < .05) with respect
metric was computed by subject and trial, both for boundary                             to the unstructured condition. The 1515 condition also had
decisions and tokens.1 Figure 2 shows token F-score aggre-                              a negative effect effect on token segmentation (β = −.29),
gated across subjects for each language and condition.                                  though this was not reliably different than the unstructured
   All conditions showed some learning. Unstructured lan-                               condition (p > .3). As in the 12 word languages, in the 8
guages were learned best, and learning was faster in the 8                              word languages there was a positive main effect of the log
word than the 12 word languages. Trajectories of the struc-                             input frequency of that token (β = .26, < .001) and negative
tured conditions maintained a relatively constant ordering:                             effect of word length (β = −.68, < .001). There were also
    1 In our example sentence (“indiangorrillaseatbananas”), we
                                                                                        significant effects of the 1313 condition (β = −.71, p < .05)
compute these measures for a participant who gave the segmenta-
                                                                                        and the 2222 condition (β = −.95, p < .01).
tion “indian|gorillas|eatbana|nas.” Computing word boundaries, the
participant would have 2 hits, 1 miss, and 1 false alarm, leading to                       To summarize: in both the 12 word and the 8 word lan-
precision of .66 (hits / (hits + false alarms)), and recall of .66 (hits /              guages we observed the best performance in the unstruc-
(hits + misses)), for an F-score of .66. On the other hand, for word                    tured condition and the worst performance in the condi-
tokens, the participant would have 2 hits (“indian” and “gorillas”),
2 misses (“eat” and “bananas”) and 2 false alarms (“eatbana” and                        tion where types are symmetrically distributed across cate-
“nas”), for precision of .5, recall of .5, and F-score of .5.                           gories/sentence positions.
                                                                             2004

                                                                          guage.” For the test trials, the two options were presented
                                  2AFC (12 type condition)                aurally and subjects could replay the audio if desired.
                     1.0                                                  Results and Discussion
                                    -                       -
                                                                          In the 8 word languages, performance was at ceiling, with all
                     0.8
                           -                    -                         condition means above 90% in the forced choice test. As such
Proportion Correct
                                                                          there was no meaningful variance across languages and we
                     0.6                                                  do not discuss these conditions further. In the 12 word lan-
                                                                          guages, the pattern of mean performance across conditions
                                                                          corroborated our findings in Experiment 1 (Figure 3). Al-
                     0.4
                                                                          though there was considerable variation across participants,
                                                                          performance was higher in the unstructured and asymmetric
                     0.2                                                  conditions; performance was lowest in the symmetric con-
                                                                          dition. We analyzed trial-by-trial 2AFC performance for
                     0.0                                                  the 12 word languages using a mixed-effects logistic model.
                                                                          There was a weak negative effect of trial number (β = −.02,
                           1515    2424        3333     unstructured      p < .001; recall all learning happened in this experiment after
                                   Types of languages                     the trial phase) and word length, though unlike in the first ex-
                                                                          periment longer word length facilitated segmentation in the
                                                                          2AFC (β = .40, p < .001). The 3333 condition had a sig-
Figure 3: Proportion of correct 2AFC trials for the languages             nificant negative effect on performance (β = −.38, p < .05).
in the 12-type conditions in Experiment 2. Each point repre-              Thus, 2AFC results support the negative effect of symmetri-
sents a participant. The black bar shows the condition mean               cal category sizes seen in Experiment 1.
and the dotted line at-chance (50%) performance.
                                                                                      Online Segmentation Models
                                     Experiment 2                         Our goal in modeling human performance in Experiment 1
                                                                          was to understand whether proposed ideal observer models
Experiment 2 was conducted to ensure that the effects of
                                                                          are affected by structural complexity in the input in the same
dependency structure could be captured in a classic two-
                                                                          way as human subjects. We thus selected a range of models
alternative forced choice task, where subjects were asked to
                                                                          that have been suggested by previous literature to fit human
distinguish between a word from the language and a distrac-
                                                                          performance: an incremental version of a transitional prob-
tor.
                                                                          ability (TP) learner (Frank, Goldwater, Griffiths, & Tenen-
Methods                                                                   baum, 2010); PARSER, a heuristic, memory-based model
                                                                          (Perruchet & Vinter, 1998); and a new online implementa-
Participants For the 12 word languages, 144 assignments                   tion of a probabilistic segmentation model (Börschinger &
were posted on AMT, of which we received 133 completed by                 Johnson, 2011) using a particle filter. All models took unseg-
distinct individuals. For the 8-type condition, 96 assignments            mented strings of characters representing syllables as input
were posted, of which we received 89. The same payment                    and produced segmented output sentence-by-sentence.
method as in Experiment 1 was used.
                                                                          Models and Parameters
Stimuli The process of generating stimuli was nearly iden-
tical to the procedures presented in Experiment 1. For the                TP-based model The transitional probability model is a
training phase, 150 input sentences (totaling 600 tokens) were            boundary-finding approach that segments on the basis of sta-
generated. A discrete test phase consisted of 30 pairs of a tar-          tistically less likely transitions from syllable to syllable un-
get word from the language and a length-matched distractor                der the premise that within-word syllable TPs are higher than
word, composed of syllables randomly selected from that lan-              those at word boundaries (Saffran, Aslin, & Newport, 1996).
guage’s syllabic inventory. Such test trials were intended to             In the present study, we calculate syllable bigram counts at
test if subjects had reliably learned which words belonged to             the end of each sentence and calculate TP as
the language.
                                                                                                           c(a, b)
Procedure Participants were instructed that they would lis-                                   p(a|b) =                                (1)
                                                                                                         ∑y∈V c(a, y)
ten to 150 sentences in a novel artificial language, then take a
short test on what they had learned about the language. Train-            where a and b are syllables, c(a, b) is the count of the bigram
ing sentences were presented aurally; when the audio file fin-            ab, and V is all bigrams observed up to that point in the cor-
ished playing subjects needed to press “next” to advance to               pus. Sentence boundaries, which contain potentially useful
the next trial. In the test phase they were asked to choose               information, were limited by a special symbol and treated as
which of the two options “sounded like it came from the lan-              syllables. We systematically varied the threshold (0 – 1 in
                                                                       2005

                           Unsmoothed TP                          PARSER                     Bigram Particle Filter           Human Subjects
                 1.0
                 0.8
                                                                                                                                                          8 Word Languages
 Token F-score
                 0.6                                                                                                                                          "1313"
                                                                                                                                                             "2222"
                 0.4                                                                                                                                         unstructured
                 0.2
                 0.0
                 1.0
                 0.8
                                                                                                                                                          12 Word Languages
 Token F-score
                 0.6                                                                                                                                          "1515"
                                                                                                                                                              "2424"
                 0.4                                                                                                                                          "3333"
                                                                                                                                                              unstructured
                 0.2
                 0.0
                       0   10   20    30 40   50   60   0   10   20    30 40   50   60   0    10   20   30 40   50   60   0   10   20   30 40   50   60
                                     Trial                            Trial                             Trial                           Trial
Figure 4: Lowess curves of mean token F-scores (smoothing span = .5) for the best fitting (using RMSE) parameter setting for
each of the three models, plotted alongside human performance from Experiment 1.
.1 increments) at which a TP was low enough to constitute a                                             terior distribution over segmentations (Börschinger & John-
word boundary, and placed boundaries between all syllables                                              son, 2011). As in the original Bayesian bigram model, the
where TP was below the threshold.                                                                       lexicon for the particle filter is generated using a Dirich-
                                                                                                        let process, enforcing a probability distribution which gives
PARSER PARSER (Perruchet & Vinter, 1998) is a lexicon-                                                  higher probability to smaller lexicons with shorter words, and
finding model that makes use of a persistent collection of                                              to a small number of high-probability collocations. Unlike
segments whose weights increase when encountered in the                                                 the original version of the lexical model, however, the parti-
input and decrease with exposure to new data lacking such                                               cle filter inference algorithm allows the model to be run incre-
segments. Found items similar to ones that are already in                                               mentally through a corpus in a single pass, producing sequen-
the collection also prompt a decrease in the weights of sim-                                            tially segmented sentences for our evaluation. In a particle
ilar segments. The PARSER model has a high number of                                                    filter, each particle constitutes a different set of segmentation
adjustable parameters, including initial weight, max segment                                            hypotheses (see Börschinger & Johnson, 2011 for details). In
length for consideration, the decay rate of material in the                                             our current study, we manipulated the number of different hy-
stored lexicon, the interference weight, a threshold to de-                                             potheses that the model could track, with fixed concentration
termine whether a parse should be considered a new word,                                                parameters of α0 = 12 and α1 = 24 in the 12 word languages
and the reactivation gain. We adapted the model to output                                               and α0 = 8 and α1 = 16 in the 8 type languages.
sentence-by-sentence segmentations, taking the initial parse
of a new sentence (guided by the weights of the collection                                              Model Results
in memory) as the segmentation decisions for each sentence.
For our simulations here, decay rate was most important, so                                             For each model, we fit a single free parameter to human data,
we systematically manipulated this parameter. Among the                                                 choosing one value that maximized Pearson’s r and the one
other parameters, we used a max segment length for consid-                                              that minimized RMSE (threshold to treat as a boundary in the
eration of 3, an initial weight of 1, an interference rate of .005,                                     TP model, the decay rate of observed chunks in PARSER, and
a reactivation gain of .005, and a threshold of consideration                                           the number of particles for the particle filter bigram model).
as a new segment of 1.                                                                                  Figure 4 shows performance at the best-fitting parameter set-
                                                                                                        ting (according to RMSE) for each of the three models along-
Bigram Model with Particle Filter Inference The bigram                                                  side the human data; Table 1 shows the RMSE and Pearson’s
model with particle filter inference is a new version of the                                            r with the associated parameter setting in parentheses.
Bayesian bigram model of Goldwater et al. (2009) that uses a                                               The bigram particle filter was the best-fitting model on both
particle filter rather than a Gibbs sampler to estimate the pos-                                        Pearson’s r and RMSE. The particle filter also captured the
                                                                                                2006

           Model              Pearson’s r    RMSE                      providing frequent tokens that served as anchors for segmen-
           Unsmoothed TP      .59 (.3)       .29 (.3)                  tation (Kurumada et al., under review). Thus, as suggested
           PARSER             .70 (.05)      .23 (.008)                by our previous work, it may be the case that high frequency
           Particle Filter    .71 (4)        .07 (8)                   material facilitates language learning by promoting segmen-
                                                                       tation of adjacent material.
     Table 1: Two metrics of modelfit for the three models
                                                                                           Acknowledgments
                                                                       Thanks to the members of the Language and Cognition Lab
ordering of conditions observed in the human data, though
                                                                       for valuable discussion.
the 8 type structured conditions were minimally distinguished
from one another. PARSER performed well on r, but the for-                                      References
get rate (the fit parameter) with the highest correlation on r
                                                                       Börschinger, B., & Johnson, M. (2011). A particle filter algo-
resulted in relatively low absolute performance (in the range
                                                                          rithm for bayesian wordsegmentation. Proceedings of the
of 0 - .2). Fitting PARSER to RMSE resulted in the selec-
                                                                          Australasian Language Technology Association Workshop,
tion of a much lower forget rate. The unsmoothed TP model
                                                                          10–18.
performed poorly on both metrics. Though condition order-
                                                                       Dutoit, T., Pagel, V., Pierret, N., Bataille, F., & Van
ing for the TP model matched human results in the 12 type,
                                                                          Der Vrecken, O. (1996). The MBROLA project: To-
both structured categories in the 8 type displayed no learning
                                                                          wards a set of high quality speech synthesizers free of use
while the unstructured condition significantly outperformed
                                                                          for non-commercial purposes. In Proceedings of the fourth
human learners. Fit to RMSE, all three of the tested mod-
                                                                          international conference on spoken language (Vol. 3, pp.
els demonstrated best performance on the unstructured input;
                                                                          1393–1396). Philadelphia, PA.
fit to r there is a clear advantage for the unstructured condi-
                                                                       Frank, M., Goldwater, S., Griffiths, T. L., & Tenenbaum, J. B.
tion, the only exception to this pattern being higher scores on
                                                                          (2010). Modeling human performance in statistical word
PARSER for the 1515 condition.
                                                                          segmentation. Cognition, 117(2), 107–25.
                     General Discussion                                Gelman, A., & Hill, J. (2006). Data analysis using regres-
                                                                          sion and multilevel/hierarchical models. Cambridge, UK:
We began by noting a difference between natural languages                 Cambridge University Press.
and the artificial languages that have previously been used            Goldwater, S., Griffiths, T., & Johnson, M. (2009). A
to study the phenomenon of ‘statistical word segmentation.’               Bayesian framework for word segmentation: Exploring the
Natural languages have complex inter-word dependencies,                   effects of context. Cognition, 112, 21-54.
while previous experiments have purposefully created lan-              Jaeger, T. F. (2008). Categorical data analysis: Away from
guages that lacked such dependencies. While this initial sim-             anovas (transformation or not) and towards logit mixed
plification was useful, it is uncertain whether dependency                models. Journal of Memory and Language, 59(4), 434–
structure would be positive or negative for segmentation per-             446.
formance. Our experiments suggest that adults learn better             Johnson, E., & Tyler, M. (2010). Testing the limits of statis-
from a language without dependencies than one with them.                  tical learning for word segmentation. Developmental Sci-
These results suggest that structural variability can be more             ence, 13(2), 339–345.
helpful for the purpose of learning word sefmentation than             Kurumada, C., Meylan, S. C., & Frank, M. C. (under re-
structural regularity, though whether this holds with larger              view). Zipfian frequency distributions facilitate word seg-
languages remains to be examined.                                         mentation in context.
    Our method of using learning curves provided for a higher-         Perruchet, P., & Vinter, A. (1998). PARSER: A model for
resolution examination of the dynamics of both human word                 word segmentation. Journal of Memory and Language,
segmentation and of models instantiating hypotheses about                 39(246-263).
the mechanisms of segmentation. The measurement of learn-              Saffran, J. R., Aslin, R., & Newport, E. (1996). Statisti-
ing across time allows for a richer investigation of the per-             cal learning by 8-month-old infants. Science, 274(5294),
formance for each model, revealing the baseline from which                1926.
models initialize, their rate of learning, and their final attain-     Saffran, J. R., Newport, E. L., & Aslin, R. N. (1996). Word
ment after having been exposed to the full set of stimuli.                segmentation: The role of distributional cues. Journal of
    In our estimation of whether the mechanisms of ‘statisti-             Memory and Language, 35(4), 606-621.
cal learning’ can scale to the task of learning the lexicon of         Zipf, G. (1965). Human behavior and the principle of least
a natural language, we must take into account the difficulties            effort: An introduction to human ecology. New York,
posed by dependency structure. Nevertheless, our studies—                 Hafner.
both experimental and computational—showed that there was
an advantage for languages with an asymmetrical dependency
structures. Languages with variability in the number of types
assigned to categories facilitated segmentation, perhaps by
                                                                   2007

