UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Inferring aspectuality on French sentences: a minimalist approach
Permalink
https://escholarship.org/uc/item/8mp798m9
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Munch, Damien
Dessalles, Jean-Louis
Publication Date
2012-01-01
Peer reviewed
  eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               Inferring aspectuality on French sentences: a minimalist approach
                                   Damien Munch (Munch@Telecom-Paristech.fr) and
                           Jean-Louis Dessalles (Jean-Louis.Dessalles@Telecom-Paristech.fr)
                                     INFRES, Institut Telecom ParisTech, 46 rue Barrault
                                                      75013 Paris FRANCE
                            Abstract                               languages than English (Dowty, 1979; Comrie, 1985). Since
   Current models of temporality in language are either
                                                                   then, Reichenbach’s model has been steadily improved. The
   inaccurate or too complex to be cognitively plausible. We       three coordinates have been changed for intervals and/or
   present a cognitive model of the computation of aspect in       have been increased in number (Comrie, 1985; Gosselin,
   French. Our approach emphasizes the importance of               1996; Elson & McKeown, 2010).
   minimalism for cognitive plausibility: structures and              There have been attempts to process aspect in a
   computation are kept simple and combinatorial explosion is      minimalist way as well. Recent TimeML versions (Saurí &
   avoided. Though the model and its current implementation
                                                                   al., 2009) consider four attributes: Progressive, Perfective,
   remain partial for now, our approach opens the way to a
   generic and cognitively plausible method for the                Perfective_progressive and None. Smith (1991) proposes a
   determination of aspect.                                        model based on only three viewpoints: Imperfective,
                                                                   Perfective, and Neutral. Ghadakpour (2004) uses only two
   Keywords: Cognitive minimalism; Natural Language
                                                                   viewpoints, called Figures and Grounds.
   Processing; Temporal aspect; Temporal reasoning
                                                                      Lexical models, in which temporal knowledge is stored in
                                                                   static lexical attributes, face the problem of attribute
                        Introduction
                                                                   defeasibility. For instance, the verb “to hit” is supposed to
Humans are experts in the communication of temporal                have the Punctual attribute; therefore, “she hit the wall for
information. The coherence of discourse relies on the              one minute before leaving” receives a repetitive
correct expression of time and aspect, both in narratives          interpretation (several knocks); however, “The small galaxy
(e.g. to mark causality) and in argumentative discussions          hit (collided with) the Milky Way for ten million years
(think of an alibi). Though significant progress has been          before collapsing” can receive a non-repetitive
achieved in modeling temporal processing, current models           interpretation, in contradiction with the supposed Punctual
are either inaccurate or too complex to be cognitively             attribute of the verb.
plausible. In the present paper, we stick to the idea that a          Procedural models, in which lexical entries are given
plausible model should rely on a minimum number of                 computation power, are able to deal with context. For
principles. The paper presents new elements in that                instance, in Gosselin’s (1996) and Schilder’s (2004) models,
direction.                                                         the function assigned to en (in French) or in (in English)
   Linguists have established various categorizations of           checks whether the complement of the preposition involves
aspect, tense and modality (Vendler, 1967; Comrie, 1976;           duration. Schilder's model even checks whether the
Vetters, 1996, among others). They explain variations of           phenomenon happened in the past or not. Procedural
meaning by postulating the existence of rich semantic              models, however, are not parsimonious as long as they do
structure stored in lexical entries. For example, Comrie           not set limits to the computational power of words. For
(1976), based on (Vendler, 1967), associates binary                instance Person’s (2004) implementation of Gosselin’s
attributes such as achievement, accomplishment,                    model of French temporality associates a specific computing
semelfactive or activity to verbs. The challenge is to infer       rule to each tense and each temporal marker (preposition,
aspect, such as repetition and perfectivity, and to predict        temporal adverbs, …). Similar procedural approaches, in
semantic incorrectness from the combination of attributes          which temporal lexemes are given significant computation
when processing a sentence. One problem is to limit the            power, are proposed by others authors like Saussure (2003)
number of attributes that lexical entries may instantiate in       and Schilder (2004). Though these models try to remain
their structure. Another problem is to show that the chosen        parsimonious in fact, they are not parsimonious in principle.
lexical attributes are sufficient.                                 Models in which words may have unlimited power (i.e. they
   In addition to fixed attributes attached to the lexical         may perform any computation like Turing machines) do not
entries, some logicians and computer scientists introduced a       qualify as cognitive models, not only because they lack
procedural component in their models of temporal                   parsimony but also because they cannot explain how
interpretation. To process tense, Reichenbach (1947)               children learn the mechanisms of temporality of the
introduced a minimalist model based on three dynamical             surrounding language.
coordinates: Event, Reference and Speech. Despite its                 Models of temporality face another problem. The
impressive description power, this model does not account          temporal meaning of a sentence cannot be deduced only
for tense in nested clauses (Hwang & Schubert, 1992) and it        from temporal information stored in lexemes. Moens and
fails to explain the behavior of some tenses in other              Steedman (1988) have shown that the mental
                                                              2055

representations corresponding to events are not reducible to
tense and aspect. They are closer to concepts such as causal
sequences, preparatory processes, goals and consequent                                 Table 1: Tested sentences
states. According to these authors, temporal attributes stored
in the lexicon cannot capture the richness of interpretation         (1) Elle mangera du gâteau en février.
that is accessible to humans. Temporal interpretation would               She will eat (be eating) cake in February.
involve causal relationships that lie beyond strict linguistic            (30%) unique/perfective and slice of February
processing. Models such as Event Calculus (Shanahan,                      (80%) multiple/imperfective
1999), modeled by Mueller (2004), do take background
knowledge into account. The problem, for such models, is to          (2) * Elle mangera du gâteau en 30 minutes.
circumscribe the effect of context, not only to avoid                     * She will eat (be eating) cake (or from the cake)
unrealistic processing time, but also to keep the systematic              within 30 minutes.
character of some temporal phenomena.                                     (30%) unique/perfective
   Our aim is to design a cognitively plausible model of             (3) Elle atteindra le sommet en février.
temporality that avoids the previously mentioned difficulties             She will reach the top in February.
(attribute defeasibility, unlimited computational power and               (76%) unique/perfective and slice of February
unlearnability, prohibitive processing time, loss of
systematicity). We favor a minimalist approach, in which             (4) Elle mangera (à la cantine) pendant deux mois.
both structures and procedures are kept minimal. In what                  She will be eating (at the canteen) for two months.
follows, we will first list a limited set of examples in French           (100%) multiple/imperfective
that we use as benchmark. Then we will see how Gosselin’s
and Schilder’s models behave on such examples. We chose                Table 2 shows classical sentences (examples 5-8) that
these two models because they use concepts similar to ours,         were not included in the test.
such as viewpoint, anchoring and granularity. We will then
describe our model and its single procedure: tMerge. We                              Table 2: Sentences variations
conclude with a discussion in which we assess the
plausibility and the generality of our approach.                     (5) Elle atteindra le sommet en 30 minutes.
                                                                          She will reach the top within (the next) 30 minutes.
                    Temporal correctness                                  unique/perfective
Table 1 shows a few examples that have been tested for               (6) * Elle atteindra le sommet pendant 30 minutes.
acceptability. We asked thirty-five French native speakers to             * She will be reaching the top for 30 minutes.
answer multiple choices questions about forty-one sentences
in French. These sentences were designed to test all                 (7) Elle atteindra le sommet pendant le prochain mois.
combinations of tense, event type and time adverbials. They               She will reach the top during the next month.
were proposed in random order and participants were                       unique/perfective and slice of the next month
allowed to stop whenever they want. We got an average of             (8) Elle mangera du gâteau pendant les 30 prochaines
twenty answers by sentence. All sentences have the same                   minutes.
form: verb phrase + prepositional phrase. Participants were               She will eat (be eating) cake during the next 30
asked the following questions:                                            minutes.
   a. Is the sentence correct / incorrect (“one wouldn't say              unique/perfective
        that”) ?
   b. Does the event occur several times (repetition) / only
                                                                       The challenge is to account not only for the acceptability
        once (possibly with breaks) ?
                                                                    or incorrectness of sentences, but also for the judgements
   c. Is the event finished / not finished / don't now ?
                                                                    about repetition, perfectiveness and wholeness. The next
   d. Is the event taking place during the whole period
                                                                    section examines how Gosselin’s and Schilder’s models
        indicated by the prepositional phrase / during only
                                                                    perform on this kind of examples.
        part of it / don't now ?
Participants could provide two sets of answers for a given
sentence if they thought there were two meanings.
                                                                                  The computation of aspect
Table 1 shows some of the results. The binary values                Gosselin’s (1996) model represents perfectivity by
unique/multiple, perfective/imperfective and whole/slice            considering intervals with two different types of boundaries:
refer to coded answers to questions b-d. Percentages for a          intrinsic and extrinsic. Theses boundaries are retrieved from
given sentence correspond to participants who found the             the aspectual type of events (telicity, punctuality,
sentence correct. They may add up to more than 100% when            dynamicity). For instance “manger du gâteau” (“to eat
several interpretations were given.                                 cake”) will take extrinsic boundaries because its aspectual
                                                                    type is supposed to be semelfactive (this information is
                                                                    provided by some external cognitive processing).
                                                                2056

   Repetition appears during conflict resolution, when the           the document timestamp (Figure 2). Note that this
granularities of two intervals are different. For instance in        computation does not seem to be always valid in English
example (1), “manger du gâteau” and “février” (February)             (for instance, in “She will defeat her opponent in 30
do not have the same granularity; the conflict is solved by          minutes”, meaning “She will play during 30 minutes and
iterating the interval of “manger du gâteau”.                        win”, the duration of the event should be DUR and not
   Conflict resolution also involves instructions which can          DTS).
move one or both boundaries of an interval. This will lead to
shrinking, expanding or moving one of the conflicting                    Anchored: TSDUR
intervals. Slices in our examples would result from                      Unanchored: If Tense = Past
shrinking the interval of the adverbial phrases (“février”, “le           then DUR
prochain mois”).                                                          else TSP1G
   In example (2) (“manger du gâteau en 30 minutes”),                           where TS = DTS+DUR
“manger du gâteau” is represented by an interval [B1,B2]                        and G = gran(DTS)
with extrinsic boundaries, whereas “30 minutes” is
represented by an interval [ct1,ct2] with intrinsic
                                                                         Figure 2: function used by “in”, adapted from Schilder
boundaries. Step b (Figure 1) succeeds, but step c fails
                                                                                                 (2004)
because the two intervals have incompatible boundaries
types.
                                                                        Contrary to Gosselin, Schilder chooses to assign functions
   Though Gosselin’s model seems to work fine, it is at the
                                                                     to all lexemes, not only the 'temporal' ones. On the other
expense of simplicity. Specific instructions are assigned to
                                                                     hand, processing is somewhat simpler, as it treats
'operators', that is, to every lexeme with a temporal
                                                                     prepositions as unary instead of binary operators, as
meaning, such as tenses and temporal prepositions. Figure 1
                                                                     proposed by Pratt & Francez (2001). However, Schilder
shows the instructions associated to the preposition “en” +
                                                                     model has the same drawback as Gosselin’s: each lexeme is
duration. The problem is not only the actual complexity of
                                                                     given a dedicated function. As long as there is no indication
such instructions, but also the fact that this complexity is not
                                                                     about how to limit the computational complexity of these
bound in principle.
                                                                     functions, models cannot be considered minimalist.
    a) associate an interval [ct1,ct2] to the temporal                                   A minimalist model
 adverbial
                                                                     In this paper, we present a model which is minimal in terms
    b) ct1 < ct2 (non-ponctual adverbial, boundaries are
                                                                     of structures, procedure and memory use. We rely on one
 dissociated)
                                                                     single fixed-sized semantic structure, called temporal
    c) [ct1, ct2] CO [B1,B2] (adverbial coincides with the
                                                                     Semantic Structure (tSS), and one single non-recursive
 event)
                                                                     operation, called temporal merge (tMerge). Note that the
    d) [I,II] ACCESS [B1,B2] (boundaries of the event must
                                                                     use of the term merge (related to Chomsky, 1995) instead of
 be 'accessible' by the reference interval ; I ≤ B1 and II ≥
                                                                     unification is debatable (Jackendoff, 2005). To achieve this
 B2)
                                                                     reduction, we decided to exclude several operations from
    The interval of the event [B1,B2] must be intrinsic
                                                                     the temporal processing proper, in line with (Moens &
 (when “pendant” + duration need extrinsic boundaries).
                                                                     Steedman, 1988), considering that they required access to
                                                                     other cognitive modules (general knowledge and perception
     Figure 1: instructions for “en” + duration, adapted from        abilities, syntax, determination). These operations include:
                         Gosselin (1996)                                - time location (whether a situation is located in time
                                                                             or not)
   Schilder (2004) uses neither intervals nor boundaries in             - temporal granularity (or order of magnitude)
his model. Events are given one of the four aspectual values                 consistency checking
defined in TimeML (Saurí & al., 2009): Perfective,                      - causality and anteriority checking
Progressive, Perfective_progressive and None.                           - self-similarity checking
   Schilder’s model can detect granularity incompatibilities,           - phrase syntactic hierarchy
though it is not clear whether they are solved by operations         For our purposes, these operations need not be represented
like slice or repetition.                                            in a cognitively realistic way, as they are considered
   To deal with the examples of Tables 1-2, he proposes two          external to the model. We implemented them in a basic form
different functions for each temporal preposition, depending         in our perception module. We now describe the two central
on whether the complement is anchored or not. Figure 2               components of the model, tSS and tMerge.
shows instructions for “in” (note that this function applies to
the English or German “in”). In example (5), the event               The Temporal Semantic Structures (tSS)
“reach the top” occurs at 'timestamp' TS, which is the
                                                                     The tSS is the only structure processed in the model. A tSS
'Document timestamp' (DTS) plus the given duration (DUR)
                                                                     is a non-recursive structure. It contains three attributes: an
“30 minutes”. The granularity (G) of the event is given by
                                                                 2057

image reference (ImageID) and two switches (Viewpoint,                    Anchoring The anchoring switch indicates whether
Anchoring).1 These attributes may be uninstantiated at a                             Perception is able to provide some (absolute or
given step of the processing.                                                        relative) location for the perceptive image. The
                                                                                     Anchoring switch may take two values: anchored
ImageID The image identifier is a reference to a perceptive                          (a) and unanchored (u). For example “(any) 30
            representation. The term 'image' includes any                            minutes” will be unanchored, but “these 30
            perceptive representation, either stored or                              minutes” will be anchored. Some lexemes are
            constructed. The mere use of ImageID in the tSS                          ambiguous in this respect: “in February” may
            allows us to grossly simplify several processes                          mean that we deal with a specific (anchored) or a
            which are supposed to be performed in an                                 recurring (unanchored) period. Anchoring bears a
            external     module.       That     module        (called                close relation to determination. “The concert” is
            'Perception') is loosely defined as including                            likely to designate an anchored time period,
            anything which does not pertain to syntactic                             whereas “a concert” refers to an unanchored time
            processing or to temporal processing proper (as                          period.
            defined in our model). This includes all forms of
            memory and all forms of knowledge, such as the                The temporal merge operator (tMerge)
            granularity or the date of events. It also includes           The tMerge procedure (Figure 3) is launched whenever a
            the ability to decide about anteriority and about             phrase is recognized by the syntactic analysis. In other
            self-similarity.                                              words, the syntactic and semantic analyses are fully
                                                                          synchronous. tMerge receives two tSS as input, plus the
Viewpoint The Viewpoint switch may take two values,                       indication that one (the head) dominates the other (the
            figure (f) and ground (g) (Ghadakpour, 2003). It              complement). It returns a unique tSS as result. These
            may be defined in the lexical entry (for example,             elements appear as H, C and R respectively in Figure 3 (tSS
            the French 'imparfait' (imperfect tense) requires a           are indicated in square brackets). Even if we deal here with
            ground). However, it is most often determined                 temporal merge exclusively, we assume that the semantic
            during temporal processing. Viewpoints are a key              merge operation performed in several other specialized
            element of our model. They provide information                modules (spatial relationships, determination, perception).
            about how the speaker regards the temporal                      (1) The essential part of tMerge consists in a basic merge
            phenomenon at a given stage of processing: either             operation (bottom of figure 3): corresponding switches in
            'from the outside' and considering the overall                the two input tSS are merely matched for compatibility to
            event (figure), or 'from within' (ground) (exactly            produce R.
            as for space). These two values correspond to                   (2) When basic unification succeeds, unification proceeds
            standard aspect values: perfective and                        to the Perception module (see figure 3) where it generates a
            imperfective. However, we consider the                        new image (this process, omitted from our model, merely
            viewpoint as a switch that may change value                   concatenates images identifiers). The perceptive merge may
            during processing.                                            apply viewpoint constraints to the resulting tSS depending
                                                                          of the nature of the phenomenon: indivisible events are
                                                                          bound to be figures, whereas self-similar events must be
                                [vp=g, an=?]                 →          [vp=f, an=a]
  (5) Predication
                                [vp=f, an=a]                 →          [vp=g, an=?]
  (4) Zoom-in                                                                                                           Perception
                                                                                                                           Images,
                              C[vp=g, an=a]                  →          C[vp=f, an=a]      slice of C                    Magnitudes,
  (3) Granularity                       or                                                                                    ...
       Conflict               H[vp=f, an=u]                  →          H[vp=g, an=?] repetition of H
                              H[im1] + C[im2]                →          R[im1+im2]
  (2) Image merge
                              H[vp, an] + C[vp, an]          →          R[vp, an]
  (1) Basic merge
                                                            Fig 3: tMerge steps
  1
    For the sake of simplicity, we omit switches related to tense.
                                                                      2058

grounds. The perceptive merge also checks for granularity               Head:            “manger du gâteau” (“to eat cake”)
compatibility. All the other operations of figure 3 aim at                               [im/i_eat_cake, vp/g, an/?]
rescuing basic merge and perceptive merge in case of                    Complement: “en février” (“in February”)
failure.                                                                                 [i_february, vp/f, an/a]
   (3) The Granularity conflict rescue operation is triggered
in examples like: “She will eat the cake in February”, where            The basic merge (figure 3, (1)) detects a viewpoint
the orders of magnitude are hour vs. month. Depending on             conflict. The conflict could be solved either by zooming-in
viewpoint and anchoring constraints, the complement                  on “en février” (figure 3, (4)) or by predicating “manger du
element will be sliced (she will eat cake once at some point         gâteau” (figure 3, (5)), but then the perceptive merge
in February) or the head will be repeated (she will be eating        (figure 3, (2)) will detect a granularity difference. We must
cake repeatedly throughout February).                                predicate “manger du gâteau” and zoom-in on “en février”
   (4) The Zoom-in rescue operation may switch a viewpoint           in both case. This leaves us with two solutions.
that is blocking unification from figure to ground. It can              In the first solution, the figure of the head is repeated, and
only apply if the input tSS is anchored, if it has an                a ground-ground merge becomes possible. In the second
instantiated imageID (for example we can zoom-in on “this            solution, the complement is sliced and a figure-figure merge
month” but not on “30 minutes”), and if Perception is able           becomes possible. Slicing is allowed by the fact that
to create a zoomed image (as in “she reached the top in ten          “February” is anchored (figure 3, (3)). We thus get the two
hours”, where one must imagine some definite ultimate                following interpretations:
climbing phase lasting ten hours).
   (5) The last rescue operation is Predication. Its effect is to    Result 1: “manger du gâteau (plusieurs fois) en février” (“to
switch one tSS to an anchored figure. It requires that               eat cake several times throughout February”)
imageID be instantiated and it can be used only once in a               [i_eat_cake_february, vp/g, an/u]
sentence.                                                            Result 2: “manger du gâteau” (une fois) en (un moment de)
   The model, characterized by the tSS and the tMerge                février (” (“to eat cake once at some point in February”)
operation, claims to be minimalist. tSS are not recursive            [i_eat_cake_february, vp/f, an/u]
(i.e. a tSS does not contain or refer to another structure of
same nature, contrary to feature structures like those used in       (2) * Elle mangera du gâteau en 30 minutes
HPSG for instance). A tSS has a fixed length and cannot                 (* She will eat (be eating) cake (or from the cake) within
grow. Moreover, the tMerge operator is 'amnesic', which              30 minutes.)
means that the input tSS are lost after the resulting tSS has
been computed. This prevents the model from using                       As previously, the tSS of “manger du gâteau” receives a
unrealistic memory resources in uncontrollably growing               ground viewpoint. By contrast with example (1), there is no
structures. Many models are monotonic, which means that              granularity conflict, but the complement “en 30 minutes” is
the structures they process can only grow in size and                not anchored. Let’s consider the step where the two phrases
complexity during processing, becoming unrealistic for               “manger du gâteau” (“to eat cake”) and “en 30 minutes”
large inputs (Ghadakpour, 2003). Our model is non-                   (“within 30 minutes”) are to be unified.
monotonic and therefore avoids this problem.
   Our model has been implemented in Prolog. The program                Head:            “manger du gâteau” (“to eat cake”)
provides all admissible solutions for an input sentence and                              [i_eat_cake, vp/g, an/?]
signals incorrect sentences.                                            Complement: “en 30 minutes” (“within 30 minutes”)
                                                                                         [i_30_minutes, vp/f, an/u]
                         Examples
The model and its implementation account for all sentences              There is no way to solve the viewpoint conflict: the
of our test, including the examples listed in Tables 1 and 2.        complement cannot be zoomed-in because it is unanchored.
It detects “incorrect” sentences; it correctly predicts              Predication cannot be used to solve the viewpoint conflict,
repetition, slice and perfective and imperfectives aspects.          since it creates an anchoring conflict. The model returns an
Examples (1) and (2) are detailed below.                             error, as expected.
(1) Elle mangera du gâteau en février                                                           Conclusion
(She will eat/be eating cake in February)                            We have shown how some of the mechanisms of French
                                                                     aspectuality could be predicted using a minimalist model.
   The determiner “du” introduces a ground viewpoint. On             We share various notions and mechanisms with Gosselin’s
the other hand, “en” is associated with a figure. Let’s              and Schilder’s models, including anchoring, granularity
consider the step where the two phrases “manger du gâteau”           checking and dynamic conflict resolution. Our model
(“to eat cake”) and “en février” (“in February”) are to be           departs from theirs by the fact that lexical structures are
unified.                                                             fixed instead of including algorithms. There is only one
                                                                     procedure in our model: tMerge, which is not attached to the
                                                                 2059

lexicon and can be synchronized with syntactic analysis.         Gosselin, L. (1996). Sémantique de la temporalité en
Our model is able to detect and solve aspectual effects such       français. Bruxelles: Duculot.
as repetition and slice, to identify the perfectivity and        Hwang, C.H., & Schubert, L. K. (1992). Tense Trees as the
progressivity of events, and to detect incorrect sentences.        fine structure of discourse. Proceedings of ACL’1992,
The output of the model is congruent with the majority             232-240.
judgment among the participants we tested.                       Jackendoff, R. (2005). Alternative minimalist visions of
   The notion of semantic incorrectness is relative, as a          language. Chicago Linguistics Society (CLS), 41, 189-
substantial number of participants considered these                226.
sentences as acceptable (e.g. 30% for example (2)).              Moens, M., & Steedman, M. (1988). Temporal ontology and
Acceptability seems to depend on several factors that are to       temporal reference. Computational linguistics. 14 (2), 15-
be investigated: differences in the kind of computations           28.
performed in Perception, or differences in judging as            Mueller, E. T. (2003). Story understanding through multi-
correct sentences that wouldn’t be uttered by a native             representation model construction. In Hirst, G. and
speaker but that could still make sense. Another possible          Nirenburg, S. (eds), Text Meaning: Proceedings of the
source of variation among participants may be judgments of         HLT-NAACL 2003 Workshop, (pp. 46-53). East
relevance. For instance, “Elle mangera du gâteau en 30             Stroudsburg, PA: Association for Computational
minutes” (understood as: “She will eat cake within a period        Linguistics.
of 30 minutes”) may be acceptable in a context in which any      Person, C. (2004). Traitement automatique de la
consumption of cake is supposed to require more than thirty        temporalité du récit: implémentation du modèle
minutes. We are currently investigating these phenomena.           linguistique SdT. PhD Thesis, Université de Caen Basse-
   Though we are confident in the fundamental principles           Normandie.
and in the overall architecture of the model, we need to         Pratt, J., & Francez, N. (2001). Temporal Generalized
check its validity against a much larger variety of                Quantifiers, Linguistics and Philosophy 24, 187–222.
phenomena, not only in French but also in other languages.       Reichenbach, H. (1947). Elements of Symbolic Logic. Free
For instance, we are currently investigating how the English       Press.
progressive “V-ing” (Deo, 2009) can be explained as a sub-       Saurí, R., Goldberg, L., Verhagen, M., & Pustejovsky, J.
categorization of the ground viewpoint depending of                (2009). Annotating Events in English. TimeML
perceptive information. These investigations may bring us          Annotation Guidelines. Brandeis University. Version
to adapt and augment the model, while hopefully keeping its        TempEval-2010.
minimalist character.                                            Saussure, L. de (2003). Temps et pertinence. Eléments de
                                                                   pragmatique cognitive du temps. Bruxelles: Duculot.
                        References                               Schilder, F. (2004). Extracting meaning from temporal
Chomsky, N. (1995) The Minimalist Program. Cambridge               nouns and temporal prepositions. ACM Transactions on
   MA: MIT Press.                                                  Asian Language Information Processing (TALIP), 3 (1),
Comrie, B. (1976). Aspect. Cambridge university press.             33-50.
Comrie, B. (1985. Tense. Cambridge university press.             Shanahan, M. (1999). The event calculus explained. In
Deo, A. (2009). Unifying the imperfective and the                  Wooldridge, M.J., Veloso, M. (eds.), Artificial
   progressive: partitions as quantificational domains.            Intelligence Today. LNCS, 1600, (pp. 409–430). Berlin:
   Linguistics & Philosophy 32 (5), 475–521.                       Springer.
Dowty, D. (1979). Word Meaning and Montague Grammar.             Smith, C. S. (1991). The Parameter of Aspect. Dordrecht,
   Dordrecht: D. Reidel.                                           NL: Kluwer.
Elson, D., & McKeown, K. (2010). Tense and Aspect                Vendler, Z. (1967). Linguistics in Philosophy. Cornell
   Assignment in Narrative Discourse. In Proceedings of the        University Press.
   Sixth International Conference in Natural Language            Vetters, C. (1996). Temps, aspect et narration. Rodopi.
   Generation.
Ghadakpour, L. (2003). Le système conceptuel, à l'interface
   entre le langage, le raisonnement et l'espace qualitatif:
   vers un modèle de représentations éphémères. PhD
   Thesis, Ecole Polytechnique.
                                                             2060

