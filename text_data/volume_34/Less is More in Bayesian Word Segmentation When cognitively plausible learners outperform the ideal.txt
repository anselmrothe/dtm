UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Less is More in Bayesian Word Segmentation: When cognitively plausible learners
outperform the ideal
Permalink
https://escholarship.org/uc/item/931571rp
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Phillips, Lawrence
Pearl, Lisa
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                          “Less is More” in Bayesian word segmentation: When
                             cognitively plausible learners outperform the ideal
                                                   Lawrence Phillips (lawphill@uci.edu)
                                                 Department of Cognitive Sciences, 2235
                                                       SBSG Irvine, CA 92697 USA
                                                       Lisa Pearl (lpearl@uci.edu)
                                                 Department of Cognitive Sciences, 2314
                                                       SBSG Irvine, CA 92697 USA
                              Abstract                                  addition, GGJ conducted an ideal learner analysis, which
                                                                        assumes unlimited processing and memory resources for the
    Purely statistical models have accounted for infants’ early
    ability to segment words out of fluent speech, with                 learner. PGS investigated the impact of this assumption,
    Bayesian models performing best (Goldwater et al. 2009).            finding a limited ―Less is More‖ effect (Newport 1990)
    Yet these models often incorporate unlikely assumptions,            where cognitive resource limitations help, rather than
    such as infants having unlimited processing and memory              hinder, some Bayesian learners. We examine the effect of
    resources and knowing the full inventory of phonemes in             the phoneme assumption in addition to these cognitive
    their native language. Following Pearl, et al. (2011), we           resource assumptions. We find not only that syllable-based
    explore the impact of these assumptions on Bayesian learners        Bayesian learners can do well at word segmentation but also
    by utilizing syllables as the basic unit of representation. We      a much more robust ―Less is More‖ effect in our
    find a significant ―Less is More‖ effect (Pearl et al 2011;
                                                                        constrained Bayesian learners. This suggests that the unit of
    Newport 1990) where memory and processing constraints
    appear to help, rather than hinder, performance. Further,           representation for models of language acquisition plays a
    this effect is more robust than earlier results and we suggest      crucial role. Here, using more cognitively plausible
    this is due a relaxing of the assumption of phonemic                assumptions showcases a surprising learning effect, the
    knowledge, demonstrating the importance of basic                    ―Less is More‖ effect that has been hypothesized to explain
    assumptions such as unit of representation. We argue that           language acquisition success in children.
    more cognitively plausible assumptions improve our
    understanding of language acquisition.                                   The syllables as the representational unit
       Keywords: language acquisition; Bayesian modeling;
    cognitively plausible learning; less is more; statistical           The first evidence that infants possess categorical
    learning; word segmentation                                         representations of syllabic units appears at 3 months: Eimas
                                                                        (1999) finds that infants have categorical representations of
                         Introduction                                   syllables whereas infants at this age have no categorical
  Knowledge of words plays a crucial role in language                   representation of phonemes. Since word segmentation first
acquisition but requires a child to identify words out of               occurs around 7.5 months (Jusczyk & Aslin 1995), it is
fluent speech. Children seem to accomplish this word                    likely that infants have robust access to syllables at this age.
segmentation very early (~7.5 months (Jusczyk & Aslin                   In contrast, knowledge of phonemes does not occur until
1995; Echols et al. 1997; Jusczyk et al., 1993a)), and                  approximately 10 months (Werker & Tees 1984) making it
therefore many strategies have been proposed for this early             unlikely the learner has adult knowledge of their native
success. One popular explanation for initial language                   language phonemes during the initial stages of word
learning relies purely on distributional information, rather            segmentation. Although it is possible that word
than language-specific biases. This idea is bolstered by                segmentation and phoneme learning bootstrap from one
findings that infants keep track of the statistical regularities        another, we consider a more conservative approach which
in speech (Saffran et al. 1996), and because languages vary             assumes infants only have access to syllabic information.
greatly in their cues to word boundaries which would                      While the success of previous statistical word
weaken the use of language specific knowledge. One very                 segmentation models is heartening, how dependent is their
successful, purely distributional, learning approach uses               success on the assumption of the phoneme as a
Bayesian inference (Goldwater, Griffith & Johnson 2009                  representational unit? With this question in mind, we
(GGJ), Pearl, Goldwater & Steyvers 2011 (PGS)). However,                modify existing phoneme-based statistical models of word
these Bayesian models incorporate modeling assumptions                  segmentation that use Bayesian inference (GGJ, PGS) to
that are unlikely to be true. Both have assumed that the                operate over syllables. All of our modified Bayesian
basic unit of representation available to the infant is the             learners treat syllables as atomic units in the same way
phoneme. We will argue from experimental evidence that                  phonemes are thought of as atomic units. This mimics the
syllables (or syllable-like representations) are a more natural         performance of infants who are able to discriminate between
representation for infants at this stage of acquisition. In             syllables such as /ba/, /bu/, and /lu/, but who are unable to
                                                                    863

recognize the phonemic similarity between /ba/ and /bu/             where we have human judgments of syllabification we used
which does not exist between /ba/ and /lu/ (Jusczyk &               them; second, when not, we automatically syllabify our
Derrah 1987).                                                       corpus in a language-independent way. We take human
  Utilizing syllables alleviates the learning problem               judgments of syllabification from the MRC Psycholinguistic
somewhat because it reduces the number of potential                 Database (Wilson 1988), but not all words in the Pearl-
boundary positions (e.g., a baby has three syllables but five       Brent corpus have syllabifications in the MRC dictionary.
phonemes). However, a potential sparse data problem then            To solve this problem we used the Maximum-Onset
surfaces: A model operating over English phonemes must              Principle to syllabify all remaining words. This principle
track statistics over approximately 40 units; a model               states that the onset of any syllable should be as large as
operating over English syllables must track statistics over         possible while still remaining a valid word-initial cluster.
approximately 4000 units, while using less data than a              We use this principle out of convenience for the kind of
phoneme-based model since there are fewer syllable tokens           syllabification that infants might possess. Given a lack of
than phoneme tokens. This increases the statistical difficulty      experimental evidence as to the exact nature of infant
of the task tremendously. Additionally, because syllables are       syllabification, this representation is likely only an
treated as atomic, almost all phonotactic information about         approximation. Approximately 25% of lexical items were
English is lost in the model. Although previous work (e.g.          syllabified automatically. Only 3.6% of human judgments
Gambell & Yang 2006) shows that heuristic syllable-based            on our items differ from automatic syllabification. Each
models can perform quite well, it is unclear a priori whether       unique syllable is then treated as a single, indivisible unit
a distributional learner with phonemes or syllables will            losing all sub-syllabic phonetic (and phonotactic)
produce better results for Bayesian word segmentation, due          information.
to the tradeoffs just mentioned.
  In changing our unit of representation, we attempt to             Models
create a more psychologically faithful model of word                Bayesian models are well suited to questions of language
segmentation. To foreshadow our results, we show that               acquisition because they explicitly distinguish between the
successful Bayesian word segmentation does not depend on            learner’s pre-existing beliefs (prior) and how the learner
the phoneme assumption. Moreover, by utilizing a more               evaluates incoming data (likelihood), using Bayes’ theorem:
cognitively plausible unit of representation, we find a much
more robust ―Less is More‖ effect. The success of our                                    ( | )       ( | ) ( )
models demonstrates the effectiveness of this purely
statistical approach. Replicating and extending results from          The Bayesian learners we use are those of GGJ as well as
PGS concerning the surprising utility of processing                 the constrained learners of PGS. All learners are based on
constraints for Bayesian word segmentation. This suggests           the same underlying hierarchical Bayesian models
that the task of word segmentation may be structured to be          developed by GGJ. The first of these models assumes
more easily learned with strong memory limitations, such as         independence between words (a unigram assumption) while
those that infants have. Moreover, Bayesian models may be           the second assumes words depend only on the word before
on the right track with respect to the kind of strategies           them (a bigram assumption). To encode these assumptions
infants are using during early word segmentation, since the         into the model, GGJ use a Dirichlet Process (Ferguson,
learners demonstrate this ―Less is More‖ behavior, and              1973), which supposes that the observed sequence of words
infants are thought to as well. In addition, the fact that this     w1 … wn is generated sequentially using a probabilistic
pattern of results was only hinted at by the phoneme-based          generative process. In the unigram case, the identity of the
models of PGS means that the unit of representation for             ith word is chosen according to:
models of language acquisition has a strong, non-trivial
effect on the results found.                                                                          ( )    ( )
                                                                         (        |          )                               (1)
                          Methods
                                                                    where ni-1(w) is the number of times w appears in the
Corpus                                                              previous i – 1 words, α is a free parameter of the model, and
                                                                    P0 is a base distribution specifying the probability that a
We test our syllable-based models using English child-
                                                                    novel word will consist of the phonemes x1 … xm:
directed speech from the Pearl-Brent corpus (CHILDES:
MacWhinney, 2000). This modification of the Brent corpus
                                                                       (             )    ∏       ( )                        (2)
contains 100 hours of child-directed speech from 16 mother-
child pairs. We restrict ourselves, however, to child-directed
utterances before 9 months of age, leaving 28,391 utterances        In the bigram case, a hierarchical Dirichlet Process (Teh et
(3.4 words per utterance, 10.4 phonemes per utterance, 4.2          al. 2006) is used. This model additionally tracks the
syllables per utterance, on average).                               frequencies of two-word sequences and is defined as in:
  While there are many ways to syllabify a corpus
                                                                                                                (    )    ( )
automatically, we opted for a two-stage approach. First,               (        |                       )                     (3)
                                                                                                                   (   )
                                                                864

                                                                    Additionally, larger values of d indicate a stricter memory
                    ( )      ( )                                    constraint. All our results here use a set, non-optimized
   (         )                                           (4)
                                                                    value for d of 1.5, which was chosen to implement a heavy
                                                                    memory constraint. Having sampled a set of boundaries, the
where ni-1(w’,w) is the number of times the bigram (w’,w)           DMCMC learner can then update its beliefs about those
has occurred in the first i – 1 words, bi-1(w) is the number of     boundaries and subsequently update its lexicon.1 Because of
times w has occurred as the second word of a bigram, bi-1 is        the decay function, the DMCMC’s sampling is biased
the total number of bigrams, and β and γ are free model             towards boundaries in recently seen utterances and thus the
parameters.                                                         DMCMC learner implements a recency effect.
  In both the unigram and bigram case, this generative                In addition to comparing our syllable-based learners
model implicitly incorporates preferences for smaller               against the original phoneme-based learners, we also
lexicons by preferring words that appear frequently (due to         compare our learners against other syllable-based learners.
(1) and (3)) as well as shorter words in the lexicon (due to        The first baseline is the Transitional Probability (TP) model
(2) and (4)). The ideal learner based on this model is fit          based on Gambell & Yang (2006), which calculates TPs
using Gibbs sampling (Geman & Geman 1984), run over the             over syllables and places boundaries at all local minima.
entire corpus, sampling every potential word boundary               Our second baseline is a ―Syllable=Word” learner which
20,000 times. GGJ found that their bigram ideal learner             simply assumes that all syllables are words (a strategy that
performed better than their unigram ideal learner, so we            can be very useful in languages containing many
begin by examining this distinction in our syllable-based           monosyllabic words, like English).
Bayesian learners. In addition, we will consider the
constrained learners that PGS investigated—incorporating                                       Results
processing and memory constraints.
  The Dynamic Programming Maximization (DPM) learner                We measure our results in terms of precision, recall and F-
incorporates a basic processing limitation: linguistic              score, where precision is defined as (5) and recall is defined
processing occurs online rather than in batch after a period        as (6):
of data collection. Thus, the DPM learner processes one
utterance at a time, rather than processing the entire corpus                                                                  (5)
at once. This learner uses the Viterbi algorithm to converge
on the optimal word segmentation for the current utterance,
                                                                                                                               (6)
conditioned on the utterances seen so far. In all other
aspects, the DPM learner is essentially identical to the Ideal
model: it has perfect memory for previous utterances and            F-score is the harmonic mean of the two:
unlimited processing resources.
  The Dynamic Programming Sampling (DPS) learner is                                                                            (7)
similar to the DPM learner in processing utterances
incrementally, but is additionally motivated by the idea that         Precision and recall are considered jointly, through the
infants, and human beings in general, are not ideally               harmonic mean, because it is possible for learners to
rational. This could mean that infants do not always select         succeed on one measure while failing on the other. For
the best segmentation. Instead, infants select segmentations        instance, a learner that posits only a single boundary scores
probabilistically. So, they will often choose the best              100% on precision if that boundary is correct. In
segmentation but occasionally choose less likely                    comparison, the same learner will have just over 0% recall.
alternatives, based on the likelihood of the various                Similarly, a learner could posit boundaries at every position,
segmentation alternatives. To implement this, the DPS               producing a 100% recall with very low precision because
learner uses the Forward algorithm to compute the                   many of the boundaries were false. As the F-score balances
likelihood of all possible segmentations and then chooses a         these two measures, a high F-score indicates the learner is
segmentation based on the calculated distribution.                  succeeding at both precision and recall. We can make these
  The Decayed Markov Chain Monte Carlo (DMCMC)                      measurements over individual word tokens, word
learner also processes data incrementally, but uses a               boundaries, and lexical items.
DMCMC algorithm (Marthi et al. 2002) to implement a                  In order to prevent overfitting, we train each learner on
memory constraint. This learner is similar to the original          90% of the corpus and then test the learner on the remaining
GGJ ideal learner in that it uses Gibbs sampling. However,          10%. This train-test validation was done five times for each
the DMCMC learner does not sample all boundaries;                   learner. Given the probabilistic nature of our learners, all
instead, it samples some number s of previous boundaries
using the decayed function ba-d to select the boundary to
sample, where ba is the number of potential boundary                   1
                                                                          All DMCMC learners sample s=20,000 boundaries per
locations between b and the end of the current utterance a
                                                                    utterance. According to PGS, this works out to approximately 89%
and d is the decay rate. Thus, the further b is from the end of     less processing than the original ideal learner in GGJ, which
the current utterance, the less likely it is to be sampled.         samples every boundary 20,000 times.
                                                                865

results presented here are averaged over the five iterations to     inherent in word bigrams, in addition to TPs. These word
ensure the validity of each learner’s performance.                  bigrams may help supplement the sparseness of the TP data.
  Table 1 shows the F-score for word tokens over all of the           A desired behavior for all learners is undersegmentation
syllable-based learners. First, we observe that, in all cases,      since children are known to undersegment the input they
the Bayesian bigram learners outperform their unigram               receive (Peters 1983). All of our Bayesian learners exhibit
equivalents. In the unigram case, all constrained learners          this behavior. This can be seen by comparing the values of
(DPM, DPS, DMCMC) significantly outperform the ideal                boundary precision and recall. High boundary precision
learner; in contrast, in the bigram case this is true for the       (indicating the boundaries are often correct) but low recall
DMCMC learners only. This indicates that constrained                (indicating not enough boundaries are put in) indicates
learning helps generally if statistics cannot be tracked across     general undersegmentation, whereas high boundary recall
words. However, if bigram statistics can be tracked, a              (indicating a lot of boundaries are put in) but low boundary
memory constraint is only beneficial for the DMCMC                  precision (indicating the boundaries are not often correct)
strategy. Additionally, all learners outperform the TP              indicates oversegmentation. Although this trend of
baseline learner and all bigram learners outperform the Syl         undersegmentation exists for both unigram and bigram
= Word baseline.                                                    learners, we present data only on our bigram learners since
                                                                    the results are qualitatively similar. Table 3 shows the
                        Unigram         Bigram                      boundary precision and recall for all Bayesian bigram and
      Ideal             53.12           77.06                       comparison learners. The Syl=Word baseline learner tends
      DPM               58.76           75.08                       to oversegment, so although it performs much better than
                                                                    the TP learner and the Bayesian unigram learners, its error
      DPS               63.68           77.77
                                                                    pattern does not match what we expect from infants. In
      DMCMC             55.12           86.26                       contrast, all of our Bayesian learners are producing more
      TP                43.98                                       undersegmentations than oversegmentations. Table 4
      Syl=Word          72.41                                       presents sample segmentation errors from the Ideal and
                                                                    DMCMC bigram learners.
   Table 1. Word token F-scores across all syllable-based
   models. Constrained Bayesian learners that significantly                           Boundary Precision        Boundary Recall
    outperform their ideal counterpart (p<.05) are in bold.       Ideal               96.50                     80.45
                                                                  DPM                 96.49                     76.21
               Syl-U       Phon-U        Syl-B       Phon-B       DPS                 95.78                     79.72
 Ideal         53.1        54.8          77.1        71.5         DMCMC               94.11                     91.57
 DPM           58.8        65.9          75.1        69.4         TP                  90.00                     53.14
 DPS           63.7        58.5          77.8        39.8         Syl = Word          76.26                     100
 DMCMC         55.1        67.8          86.3        73.0
                                                                        Table 3. Boundary precision and recall for all bigram
                                                                                  Bayesian and comparison learners.
     Table 2. Token F-scores for syllable-based (Syl) vs.
phoneme-based (Phon) models, comparing Unigram (U) and
                                                                      Bigram Ideal                      Bigram DMCMC
    bigram (B) learners. Learners in bold outperform their
                    baseline counterparts.                            putit away                        put it away
                                                                      Iloveyou                          I love you
  Clearly our syllable-based learners perform well, but are           Let’ssee what that feltlike       Let’s see what that feltlike
syllables a better unit of representation than phonemes for           If youdon’t like it               Ifyou don’t like it
this task? Table 2 compares our syllable-based learners with
the original phoneme-based models of PGS. We see that in             Table 4. Example output from Bigram Ideal and DMCMC
the unigram case, phoneme-based learners outperform their                  learners. Undersegmentation is marked in italics.
syllable-based counterparts, except in the case of the DPS
learner. In the bigram case, however, all syllable-based              To explain the difference between our ideal and
models outperform their phoneme-based equivalents. This             constrained learner results, we can examine the token and
suggests that the bigram assumption is crucial to a syllable-       lexicon item recall scores, as shown in Table 5. We observe
based learner. We speculate that this is due to an additional       that both the DMCMC learners identify fewer word types
source of information that the bigram learner has access to.        than their ideal learner counterparts, as shown by their
In particular, because the unigram learner assumes that             comparatively low lexicon recall scores. The token recall
words are independent of one another, the TPs between               score for the DMCMC learners, however, is higher than
syllables are the only source of boundary information.              their ideal learner counterparts. Since this requires the
Because there are roughly 4000 syllables, there will often be       DMCMC learners to identify more word tokens from a
cases where a problem of sparse data arises. In contrast, the       smaller stock of lexical items, it can be inferred that these
bigram learner has access to the boundary information
                                                                866

DMCMC learners are identifying more frequently occurring            phenomenon suggests that, irrespective of the
words than the ideal learners.                                      representational unit, memory-constrained learners are
                                                                    biased towards identifying more commonly occurring units,
                           Token Recall         Lexicon Recall      a potentially useful bias in language acquisition.
     Uni-Ideal             44.96                73.44                 In effect, this strategy in word segmentation may help in
     Uni-DMCMC             48.09                68.9                learning the important things. Although this has been
                                                                    hypothesized by the literature on ―Less is More” in artificial
     Bi-Ideal              72.47                79.69
                                                                    language learning (Kersten & Earles 2001; Cochran et al.
     Bi-DMCMC              85.43                76.84               1999), we are unaware of experimental support for why
                                                                    constrained processing helps in real language acquisition.
     Table 5. Token and lexicon recall for the Ideal and            The fact that we can help to explain, from a computational
  DMCMC learners. Lower lexicon recall with higher token            perspective, why ―Less is More” is beneficial highlights a
   recall implies that the DMCMC learners identify more             very major contribution computational modeling can make
                 frequently occurring words.                        to developmental research more generally.
                                                                      For our claim regarding the impact of the unit of
                                                                    representation, we can compare the syllable-based learner
                         Discussion                                 results with those of phoneme-based learners. Table 2
Our results support two broad findings. First, we find that         highlights a number of crucial distinctions. First, and most
memory-constrained learners outperform their ―ideal”                basically, syllable-based learners perform well, and in the
equivalents, which we take as support for the ―Less is More”        bigram case better than phoneme-based learners. This
hypothesis (Newport 1990). In particular, limited cognitive         suggests that the tradeoff between number of potential
resources, rather than hurting learner, seem to help word           boundaries and number of potential transitional probabilities
segmentation. Second, because this effect was obscured in           works out in favor of the syllable-based learner. This
the phoneme-based learners of PGS, we argue that the unit           underscores the utility of a Bayesian inference strategy for
of representation posited by a model of language acquisition        the initial stages of word segmentation – without access to
has a crucial impact on the results found. In particular,           phonotactics, stress, acoustic cues, or innate linguistic
making more cognitively plausible assumptions may yield             knowledge, a learner can be very successful at segmenting
answers to the puzzling behaviors we observe—namely, that           words from fluent speech.
children, who are more cognitively limited than adults,              Still, there is a major difference in the performance of the
nonetheless are far more successful at language acquisition.        sub-optimal (DPS) learner – the syllable-based DPS learner
 What exactly is causing the ―Less is More” effect here?            has comparable performance to the Ideal learner while its
Perhaps it is due to the properties of online vs. batch             phoneme-based equivalent suffers greatly. We speculate that
unsupervised probabilistic learning algorithms. Liang &             this is due to the number of potential segmentations the
Klein (2009) show that for unsupervised models using                phoneme-based learner considers, compared to the syllable-
Expectation-Maximization, online models not only                    based learner since the DPS learner chooses a segmentation
converge more quickly than batch models, but, also in cases         probabilistically, the phoneme-based learner may be more
as varied as word segmentation, part-of-speech induction            easily led astray in the initial stages of segmentation, and
and document classification, can actually outperform their          never recover. In addition, we also notice a strengthening of
batch equivalents. However, this explanation fails to               the ―Less is More” effect in the syllable-based learner,
account for our results in two ways: (a) the most direct            compared to its phoneme-based counterpart (Ideal vs.
online equivalent of our batch model (the DPM learner)              DMCMC). By making more realistic assumptions about the
actually performs worse than the Ideal model, and (b) this          learner’s unit of representation, we also create a learner that
does not explain the performance boost caused by sub-               exhibits the kind of behavior that infants show. This
optimal segmentation (the DPS learner).                             highlights one benefit of pursuing more cognitively
  Perhaps the answer lies in the kinds of words these models        plausible computational models, as opposed to models that
identify. We find, as in table 5, that our ideal bigram learner     are more idealized.
segments 72.5% of the words in the input, building a                  In that vein, there are a number of areas where we could
lexicon that contains 80% of the actual word-types it               improve the existing syllable-based Bayesian learners. First,
encounters. Yet we find that a learner with memory                  some segmental cue information is likely available to
constraints (the DMCMC learner) can successfully segment            infants such as phonotactics or articulatory cues. Similarly,
85% of the words in the input, although this makes up only          suprasegmental cues such as primary stress are known to
76.8% of the word-types encountered. This suggests that             affect infant word segmentation (Jusczyk et al. 1999) and
while an ideal learner identifies more lexical items, the           there is evidence that stressed and unstressed syllables are
memory-constrained learner identifies more frequent lexical         represented separately in infants (Pelucchi, Hay, & Saffran
items. Not only is this true in both the unigram and bigram         2009). Finally, the exact form which infants use to represent
syllable-based learners, but it is also true of the equivalent      syllables is unclear. While it is our view that syllabification
phoneme-based learners of PGS. The robustness of this               must be learned by infants, we make no attempt here to
                                                                867

explain by what means this occurs. When one looks cross-          Jusczyk, P.W. & Derrah, C. 1987. Representation of speech
linguistically, languages treat syllabification in very             sounds by young infants. Developmental Psychology,
different ways. In addition, languages vary significantly on        23(5), 648-654.
the number of syllable types they have – languages such as        Jusczyk, P.W., Cutler, A. & Redanz, N.J. 1993a. Infants’
English number their unique syllables in the thousands,             preference for the predominant stress patterns of
while some languages, like Japanese, have very few unique           English words. Child Development, 64(3), 675-687.
syllables. To ensure that our pattern of results is truly         Jusczyk, P.W., Friederici, A.D., Wessels, J.M.I., Svenkerud,
representative of word segmentation generally and not just          V.Y. & Jusczyk, A.M. 1993b. Infants’ sensitivity to the
in English, syllable-based word segmentation models must            sound patterns of native language words. Journal of
be tested across multiple languages.                                Memory and Language, 32, 402-420.
  In conclusion, this study highlights the benefits of using      Jusczyk, P.W., Luce, P.A. & Charles-Luce, J. 1994.
empirical research from psychology to inform decisions on           Infants’ sensitivity to phonotactic patterns in the
how to model language acquisition: not only can we identify         native language. Journal of Memory and Language, 33,
                                                                    630-645.
the strategies that are likely to be used by children, but we
                                                                  Jusczyk, P.W., Houston, D.M. & Newsome, M. 1999. The
may also discover potential explanations for existing,
                                                                    beginnings of word segmentation in English learning
sometimes puzzling, observations about child language
                                                                    infants. Cognitive Psychology, 39, 159-207.
acquisition, as with the ―Less is More” hypothesis.
                                                                  Kersten, A.W. & Earles, J.L. 2001. Less really is more for
                                                                    adults learning a miniature artificial language. Journal of
                    Acknowledgments                                 Memory and Language, 44, 250-273.
We would like to thank Caroline Wagenaar and James                Liang, P. & Klein, D. 2009. Online EM for unsupervised
White for their help on syllabifying the Pearl-Brent corpus.        models. Human Language Technologies: The 2009
In addition, we are very grateful to Robert Daland,                 Annual Conference of the North American Chapter of
Constantine Lignos, and the audiences at the Psycho-                the ACL, 611-619.
Computational Models of Human Language Acquisition                MacWhinney, B. 2000. The CHILDES project: Tools for
2012 and the Linguistic Society of America meeting in 2012          analyzing talk. Mahwah, NJ: Lawrence Erlbaum
for their helpful comments.                                         Associates.
                                                                  Marthi, B., Pasula, H., Russell, S. & Peres, Y., et al. 2002.
                         References                                 Decayed MCMC filtering. In Proceedings of 18th UAI
Brent, M.R. & Siskind, J.M. 2001. The role of exposure to           319-326.
  isolated words in early vocabulary development.                 Newport, E. 1990. Maturational constraints on language
  Cognition, 81, 31-44.                                             learning. Cognitive Science, 14, 11-28.
Cochran, B., McDonald, J. & Parault, S. 1999. Too smart           Pearl, L., Goldwater, S., & Steyvers, M. 2011. Online
  for their own good: The disadvantage of superior                  Learning Mechanisms for Bayesian Models of Word
  processing capacity for adult language learners. Journal          Segmentation, Research on Language and Computation,
  of Memory and Language, 41, 30-58.                                special issue on computational models of language
Echols, C.H., Crowhurst, M.J. & Childers, J.B. 1997. The            acquisition. DOI 10.1007/s11168-011-9074-5.
  perception of rhythmic units in speech by infants and           Pelucchi, B., Hay, J., & Saffran, J. 2009. Learning in
  adults. Journal of Memory and Language, 36, 202-225.              reverse: Eight-month-old infants track backward
Eimas, P.D. 1999. Segmental and syllabic representations in         transitional probabilities. Cognition, 113, 244-247.
  the perception of speech by young infants. Journal of the       Peters, A. 1983. The Units of Language Acquisition,
  Acoustical Society of America, 105(3), 1901-1911.                 Monographs in Applied Psycholinguistics, New York:
Ferguson, T. 1973. A Bayesian analysis of some                      Cambridge University Press.
  nonparametric problems. Annals of Statistics, I, 209-230.      Saffran, J.R., Aslin, R.N. & Newport, E.L. 1996. Statistical
Frank, M. C., Goodman, N. D., & Tenenbaum, J. 2009.                    learning by 8-Month-Old Infants. Science, 274, 1926
  Using speakers’ referential intentions to model early cross          1928.
  situational word learning. Psychological Science, 20, 579       Teh, Y., Jordan, M., Beal, M., & Blei, D. 2006.
  585.                                                              Heirarchical Dirichlet processes. Journal of the American
Gambell, T. & Yang, C. 2006. Word Segmentation: Quick               Statistical Association, 101(476), 1566-1581.
  but not dirty. Manuscript. New Haven: Yale University           Wilson, M.D. 1988. The MRC Psycholinguistic Database:
Geman S. & Geman D. 1984. Stochastic Relaxation, Gibbs              Machine Readable Dictionary, Version 2, Behavioral
  Distributions, and the Bayesian Restoration of Images.            Research Methods, Instruments and Computers, 20 6-11.
  IEEE Transactions on Pattern Analysis and Machine               Werker, J.F. & Tees, R.C. 1984. Cross-language speech
  Intelligence, 6, 721-741.                                         perception: Evidence for perceptual reorganization
Goldwater, S., Griffiths, T. & Johnson, M. 2009. A Bayesian         during the first year of life. Infant Behavior &
  framework for word segmentation: Exploring the effects of         Development, 7, 49-63.
  context. Cognition 112(1), 21-54.                              Xu, F. & Tenenbaum, J.B. 2007. Word learning as Bayesian
                                                                       inference. Psychological Review, 114(2), 245-272.
                                                              868

