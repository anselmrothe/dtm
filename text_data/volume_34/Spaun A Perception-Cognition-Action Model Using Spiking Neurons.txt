UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Spaun: A Perception-Cognition-Action Model Using Spiking Neurons
Permalink
https://escholarship.org/uc/item/168466tf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Stewart, Terrence
Choo, Feng-Xuan
Eliasmith, Chris
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

              Spaun: A Perception-Cognition-Action Model Using Spiking Neurons
                                         Terrence C. Stewart (tcstewar@uwaterloo.ca)
                                             Feng-Xuan Choo (fchoo@uwaterloo.ca)
                                           Chris Eliasmith (celiasmith@uwaterloo.ca)
                                    Centre for Theoretical Neuroscience, University of Waterloo,
                                                         Waterloo, ON, N2L 3G1
                             Abstract                                      Input to the model consists of idealized and hand-written
   We present a large-scale cognitive neural model called Spaun
                                                                        digits and symbols (Figure 1a). These images are given as a
   (Semantic Pointer Architecture: Unified Network), and show           28x28 grid of pixels. This input domain has the advantage
   simulation results on 6 tasks (digit recognition, tracing from       of including significantly variable, real-world input, while
   memory, serial working memory, question answering,                   also providing a reasonably limited semantics that the model
   addition by counting, and symbolic pattern completion). The          must reason about. Output from the model consists of the
   model consists of 2.3 million spiking neurons whose neural           motion of a 2-degree-of-freedom arm. The neural model
   properties, organization, and connectivity match that of the
                                                                        generates a sequence of target locations which directly drive
   mammalian brain. Input consists of images of handwritten
   and typed numbers and symbols, and output is the motion of a         the controller for the arm. This provides the model with its
   2 degree-of-freedom arm that writes the model’s responses.           own handwriting output (Figure 1b).
   Tasks can be presented in any order, with no “rewiring” of the
   brain for each task. Instead, the model is capable of internal
   cognitive control (via the basal ganglia), selectively routing
   information throughout the brain and recruiting different
   cortical components as needed for each task.
   Keywords: Neural engineering; cognitive architecture;
   spiking neurons; cognitive control; whole-brain systems
                         Introduction
In a forthcoming book, Eliasmith (2012) details a neural
architecture for biological cognition called the semantic
pointer architecture (SPA). This architecture, based on the
Neural Engineering Framework (Eliasmith & Anderson,
2003), uses groups of spiking neurons to form distributed
representations of high-dimensional vectors, which can in
turn encode symbol-like tree structures.                   Synaptic
connections between groups of neurons compute particular
functions on those vectors, allowing high-level cognitive
algorithms to be implemented in detailed spiking neuron
models.
   In this paper, we present an overview of the Semantic                  Figure 1: Example visual input and arm-movement output
Pointer Architecture: Unified Network (Spaun) model and                 from Spaun. Input digits are from the MNIST database, and
discuss its behaviour on six different tasks. We demonstrate             input symbols are used to inform the model of details of the
that this biologically plausible spiking neuron model has the           current task. The model produces output by controlling a 2-
following features:                                                         joint arm, and variation in the internal representations
   Task Flexibility: No changes are made to the model                          produces the variation in its output hand-writing.
between tasks. Visual input indicates which task to do next.
   Motor Plans: Model output provides a motor plan for a                We can think of Spaun as having a single, fixed eye and a
simple 2-joint arm, giving hand-written digits as responses.            single 2-joint arm. The eye does not move, but instead the
   Visual Memory: Even after an input has been recognized               experimenter changes the image falling on it by showing
and classified as a particular symbol, details of the original          different inputs over time, with each input shown for
image can still be recovered and used.                                  150ms, followed by 150ms of blank background. To begin a
   Compositionality: Multiple items can be represented and              specific task, Spaun is shown the letter “A” followed by a
reliably bound together, allowing for the creation and                  number between zero and seven. The subsequent input is
manipulation of symbol-tree-like structures.                            then interpreted by the model in the context of the specified
   Symbolic Induction: Language-like patterns in visual                 task and processed accordingly, resulting in arm movements
input can be discovered after only a few presentations, and             that provide Spaun’s response. All internal processing is
used to guide subsequent responses.                                     performed using spiking neurons, with neural properties and
                                                                        connectivity consistent with the mammalian brain.
                                                                    1018

              Neural Engineering Framework                                                     Spaun
While complete details on the construction of Spaun are             The Semantic Pointer Architecture: Unified Network model
available elsewhere (Eliasmith, 2012; <http://nengo.ca>), it        consists of multiple modules, depicted in Figure 2. These
is based on our continuing work on the Neural Engineering           modules are considered to be cortical and subcortical areas
Framework (NEF; Eliasmith & Anderson, 2003). The NEF                that implement different operations. All components consist
is a generic method for converting high-level algorithms            of LIF neurons connected via synaptic weights (Eq. 3), but
into realistic spiking neuron models.                               each area computes a different set of functions.
   Two basic principles of the NEF are that a) groups of               To perform a particular task, information must be
neurons form distributed representations of vectors, and b)         selectively routed between cortical areas, as each task uses a
connections between groups of neurons specify a                     different subset of the components. This is achieved
computation to be performed on those vectors. Importantly,          through an action selection system modelled after the
the NEF provides a method for analytically solving for the          mammalian basal ganglia and thalamus.               We have
synaptic connection weights that will efficiently compute           previously shown that this model matches the anatomy and
any given function.                                                 timing behaviour of the basal ganglia (Stewart, Choo, &
   While the NEF supports any type of neural model, for             Eliasmith, 2010) and provides enough flexibility to perform
Spaun we use Leaky Integrate-and-Fire (LIF) neurons. The            planning and problem solving in our model of the Tower of
various properties of the LIF model (refractory period,             Hanoi task (Stewart & Eliasmith, 2011).
capacitance, resistance, post-synaptic time constant, etc.) are        The model presented here is the first use of this neural
set to be consistent with known neurophsyiological results          action selection system with multiple tasks and detailed
for the various brain regions modelled.                             perceptual-motor systems. The action selection system is a
   To represent a vector using a group of neurons, the NEF          neural production system, allowing us to write rules of the
generalizes the idea of preferred direction vectors. Each           form “if cortical area X1 matches the vector a and cortical
neuron in a group has a randomly chosen vector e for which          area X2 matches the vector b, then send vector c to area X3
it will fire most strongly. In particular, the amount of            and route the vector from area X4 to area X5”. These rules
current J flowing into the neuron is the dot product of the         are implemented by converting the rules into functions,
preferred vector e with the represented value x, times the          applying Eq. 3, and using the resulting synaptic connection
neuron's gain α, plus the background current Jbias (Eq. 1).         weights between the cortex, basal ganglia, and thalamus.
   While Eq. 1 lets us convert x into neural activity, we can          Importantly, the set of rules is fixed across all the tasks,
also do the opposite by computing d via Eq. 2. This                 giving a single, unified model. All input to Spaun is
produces a set of linear decoding weights that can be               through its perceptual system, and all behavioral output
multiplied by the activity of each neuron in the group. The         from its motor system. The representational repertoire,
result is the optimal least-squares linear estimate of x. Thus,     background knowledge, cognitive mechanisms, neural
given a spiking pattern we can estimate what value is               mechanisms, etc. remain untouched while the system
currently represented by those neurons.                             performs any of the tasks in any order.
   Most crucially, we can use d to calculate the synaptic
connection weights that will compute particular operations.
To compute a linear operation where one group of neurons
represents x and a second group should represent Mx, where
M is an arbitrary matrix, we set the connection weights
between neuron i in the first group and neuron j in the
second group to ωij as per Eq. 3. For non-linear operations,
we need to compute a new set of d values via Eq. 4.
     J = e⋅x J bias                                    (1)
     d =    ij =∫ a i a j dx  j =∫ a j x dx
              −1
                                                         (2)
      ij = j e j M d i                                 (3)                      Figure 2: The Spaun architecture
     d =   ij =∫ ai a j dx  j =∫ a j f  x  dx (4)
       f x     −1
This approach allows us to convert a high-level algorithm              The Spaun model (Figure 2) consists of three hierarchies,
written in terms of vectors and computations on those               an action selection mechanism, and five subsystems. The
vectors into a detailed spiking neuron model. Importantly,          first hierarchy is the visual system, which compresses an
this approach works for recurrent connections as well. For          input image into a high-level abstract representation of that
example, we can implement memory by connecting a group              input. We adapt Hinton's (2010) Deep Belief Network to
of neurons back to itself with connections weights                  use LIF spiking neurons (via the NEF) and use it to
determined by Eq. 3 where M is the identity matrix. If that         compress a 28x28 image into a 50-dimensional vector we
group of neurons is currently representing x, then given no         refer to as a semantic pointer: it is semantic because the
external input it will drive itself to keep representing x, thus    high-level representation maintains similarity relationships
storing information over time.                                      from the image space; and it is a pointer because, as we will
                                                                1019

see, the system can recover the original information from
the compressed form. Similarly, Spaun includes a motor
hierarchy which dereferences an output semantic pointer
representing a number into a motor plan to drive a two
degree-of-freedom arm (DeWolf, 2010). A third internal
hierarchy (discussed in more detail below in the serial
working memory section) forms a working memory capable
of binding and unbinding arbitrary semantic pointers,               Figure 3: Input-output pairs for 20 different inputs. Each
providing the compositionality that is crucial for complex            input (on the left) is correctly recognized by the model,
cognition. The working memory component also provides               which produces the output (on the right) via motor control.
stable representations of intermediate task states, task
subgoals, and context. Anatomically, these functions cover        Tracing from memory
large portions of prefrontal and parietal cortex.                 While the previous task showed that the model can treat
   The five subsystems, from left to right in Figure 2, are       very different stimuli as tokens of the same type, we also
used to: 1) map the visual hierarchy output to a conceptual       want the model to be able to be sensitive to the variations
representation as needed (information encoding); 2) extract       within a type. To demonstrate this ability, we ask the model
relations between input elements (transformation                  to do digit recognition, but to draw its response in the same
calculation); 3) evaluate the reward associated with the          style as the original input.
input (reward evaluation); 4) map output items to a motor             This is implemented by defining, for each digit, five
semantic pointer (information decoding); and 5) control           different motor control sequences that draw five visually
motor timing (motor processing). Several of the                   distinct versions of that digit. If the input pattern is X and
subsystems and hierarchies consist of multiple components         the motor sequence is Y, then we can define the tracing
needed to perform the identified functions. For instance, the     function f(X)=Y. We build a neural connection between the
working memory subsystem includes eight distinct memory           vision system and the motor system via Eq. 3, and allow it
systems, each of which can store semantic pointers. Overall,      to be selectively controlled via the action selection in the
the model uses 2,341,212 spiking leaky integrate-and-fire         basal ganglia. If we now present new input X new that was
(LIF) neurons. Additional details necessary to fully re-          not among the 5 original inputs, the resulting Ynew value will
implement the model, and a downloadable version of the            be the model's linear extrapolation of what motor sequence
model can be found at <http://nengo.ca>.                          would be appropriate for that novel input pattern.
                                                                      To use this system, we use the sequence A0[X? and add
Digit recognition                                                 a single rule to the basal ganglia action selection system:
The simplest task for Spaun is digit recognition. We present            • If the visual input is ? and state is 0, route the
the input sequence A1[X?, where X is a randomly chosen                       pattern in working memory to the motor system via
hand-written digit from the MNIST database. Each symbol                      the tracing function.
is shown for 150ms, with 150ms between symbols. To                This rule, combined with the previous ones for digit
perform this task, Spaun includes synaptic connections in         recognition, result in the behaviour shown in Figure 4. Note
the action selection system to implement the following            that it is capable of drawing 2's both with and without loops,
algorithm:                                                        6's where the loops join in different locations, and generally
     • If visual input matches A, store ? in the state area       following the slanting of the digits. It is not a perfect
         of the working memory. (This tells the system it is      reconstruction of the original input, but it does demonstrate
         about to start a new task.)                              that while the internal neural representation of all 2's are
                                                                  very similar (as shown by Spaun's success in the previous
     • If ? matches state, route the output of the visual
                                                                  task), there are still variations in the representation due to
         encoding system to the state area of working
                                                                  different visual features, and those variations can be used to
         memory. (This identifies and remembers which
                                                                  successfully drive behaviour.
         task is to performed.)
     • If the visual input is [, activate the routing between
         the information encoding system and the general-
         purpose working memory. (This tells Spaum to
         store whatever digits appear next.)
     • If the visual input is ? and state is 1, route the
         pattern in working memory to the motor system.
Responses from this model for different inputs X are shown          Figure 4: Input-output pairs for tracing from memory. The
in Figure 3. Recognition accuracy is 94%, which compares             inputs (on the left) are recognized by the model, and then
well to humans on this task (~98%; Chaaban and                     recreated from memory based on representational similarity
Scheessele, 2007). The model is thus capable of correctly              to five previously known example pairs for each digit.
categorizing over a wide range of input variability.
                                                              1020

Serial Working Memory                                                      To implement this task, the sequence is presented in the
For this task, Spaun is given a list of numbers and must                same manner as the serial working memory task. We then
repeat them back, in order. The algorithm used here is                  present the symbol P for a query based on position, or the
based on our previous work (Choo & Eliasmith, 2010) on a                symbol K for a query based on the kind of digit to look for
special-purpose serial memory model.                                    in the list. The action selection rules route the appropriate
   The NEF gives us a method for representing vectors via               transformation vector to the working memory area,
spiking neurons.         The Semantic Pointer Architecture              providing the required information to the motor system.
approach maps symbols onto particular vectors, so one
vector might represent ONE while another vector represents
TWO. Each vector, when represented, would produce a
distinct firing pattern in that population of neurons.
   To represent an ordered sequence, we cannot simply add
the vectors together, since we could not distinguish
ONE+TWO from TWO+ONE. Instead, we can perform
binding by creating a new vector from two different vectors.
We start by introducing new vectors for different positions:
P1, P2, P3, etc. We then store the sequence ONE, TWO by
representing the vector ONE⊗P1+TWO⊗P2.                      Many
mathematical operations can be used for ⊗; we choose
circular convolution since it is easy to accurately implement
in spiking neurons using the NEF. This approach to
representation using vectors is generally known as a Vector                 Figure 5: Serial recall of the sequence 4,3,2,6. Infero-
Symbolic Architecture (Gayler, 2003), and has been shown                 temporal cortex (IT) holds the compressed representation of
to scale well to adult-level vocabulary and grammar.                       the visual input. Striatum (Str) activity determines how
   The action selection rules for this task are based on the                 good a match each rule is to the current state. Globus
previous one, with the addition of the ] marker to indicate              pallidus internus (GPi) performs action selection, inhibiting
the end of a sequence. Figure 5 shows not only the input                all but the current best-matching rule. Frontal cortex (FCtx)
and output of the model, but also the ongoing spiking                     holds task information, and working memory (WM) stores
behaviour in various areas during the execution of the task.                                the list as a single vector:
   Figure 5 also shows a method for interpreting what is                        FOUR⊗P1+THREE⊗P2+TWO⊗P3+SIX⊗P4.
currently being represented in a particular cortical area. The
second working memory line shows how similar the current
pattern in the working memory is to various different ideal
patterns1. In particular, after the presentation of the final
digit (t=2 seconds), the value being represented is similar to
FOUR⊗P1, THREE⊗P2, TWO⊗P3, and SIX⊗P4. In
other words, this one group of neurons is capable of storing
any arbitrary sequence of digits. As the sequence gets
longer, accuracy decreases, and both primacy and recency
effects are seen (Choo & Eliasmith, 2010).
Question Answering
In addition to simply repeating a sequence, Spaun is capable
of answering questions about the sequence. In particular, it
can identify which digit is at a given location, and it can
identify the location of a given digit.
   This is accomplished by adjusting the transformation
which takes the contents of working memory and routes it to
the motor area. Given the vector S=FIVE⊗P1+SIX⊗P2,
we can find the digit in position 1 by computing S⊘P1,
where ⊘ is circular correlation, since S⊘P1≈FIVE. The
accuracy of this approximation is dependent on the length of
                                                                          Figure 6: Answering questions about a list. The first case
the sequence, the number of neurons used, and the
                                                                          presents the list 9,4,7,3,0 and asks what is position 5. The
dimensionality of the vectors.
                                                                           model correctly answers 0. The second case presents the
   1
                                                                           list 8,6,9,4,7 asks where the 6 can be found. The model
     Formally, this is the dot product between the ideal vector for                 correctly answers that it is in location 2.
that symbol and the decoded value found using Eq. 2.
                                                                    1021

Addition by Counting                                              After a three such examples, they must generalize to a new
For the fifth task, we show that Spaun is capable of              case:
performing sequences of internal actions, where there are a
multiple steps to go through before producing a final output.     Training Set
This is demonstrated here by performing mental addition via            • Input: Biffle biffle rose zarple. Output: rose zarple.
counting. That is, to compute 4+3, the model must go                   • Input: Biffle biffle frog zarple. Output: frog zarple.
through the steps of counting 4, 5, 6, and 7, entirely                 • Input: Biffle biffle dog zarple. Output: dog zarple.
internally, and then finally producing the output 7.              Test Case
   Spaun achieves this by having multiple, general-purpose             • Input: Biffle biffle quoggie zarple. Output: ?
working memories. We use the first of these (WM1; the
same neurons that stored the list and the recognized              Hadley suggests that this task requires rapid variable
numbers in the previous task) to store the current value. A       creation because the second last item in the list can take on
second group of neurons (WM2) stores the number of                any form, but human cognizers can nevertheless identify the
counting steps that are needed, and a third group (WM3)           overall syntactic structure and identify “quoggie zarple” as
stores how many steps have been made.                             the appropriate response. So it seems that a variable has
   Figure 7 shows Spaun performing this task over time for        been created, which can receive any particular content, and
the specific case of 4+3. Importantly, the model produces         that will not disrupt generalization performance.
accurate results for any single-digit addition. Furthermore,         Figure 8 shows Spaun’s behavior on a stimulus sequence
Spaun exhibits the expected linear relationship between           with the same structure as that proposed by Hadley.
subvocal counting and response times, as seen in human            Importantly, this is done extremely quickly (~2 seconds,
subjects (Cordes et al., 2001). That is, each counting step       consistent with human performance), and without changing
requires 419±10ms, which is within the empirical range of         neural connection weights. In other words, there is no
344±135ms for subvocal counting (Landauer, 1962).                 learning rule; Spaun is able to learn to complete this pattern
   Successful counting demonstrates that flexible action          without any neural rewiring.
selection is effectively incorporated into Spaun. It also            To achieve this result, we use a simplification of our
shows that the model has an understanding of order                earlier work with a neural model capable of performing
relations over numbers, and can exploit that knowledge to         Raven's Matrices (Rasmussen & Eliasmith, 2011). We store
produce appropriate responses.                                    the representation of the first list in one area of working
                                                                  memory (WM1) and a representation of the second list in
                                                                  another area (WM2). Since we are using the semantic
                                                                  pointer method of representing lists, these lists are encoded
                                                                  as two high-dimensional vectors (V1 and V2). We can then
                                                                  compute the transformation T which takes the first vector
                                                                  (V2=V1⊗T, so T=V2⊘V1). We implement this is Spaun
                                                                  by adding a cortical area which computes the transformation
                                                                  between two working memory components, and adding
                                                                  rules to the basal ganglia to route this information
                                                                  appropriately when performing this task. As more examples
                                                                  are given, the value T is built up as the average over all the
                                                                  examples, improving accuracy.
    Figure 7: Adding 4+3 by mentally counting 4, 5, 6, 7
  (WM1). WM2 keeps track of the fact that it should stop
   counting when it reaches the third step, and WM3 keeps
             track of what step it is at (0, 1, 2, 3).
Pattern Completion
Finally, we show that Spaun is capable of quickly
identifying and responding to patterns in its input via
inductive learning. This specifically targets a type of task
that has long been held to be problematic for connectionist
approaches: the ability to rapidly create and bind symbolic
variables (e.g. Marcus, 2001; Jackendoff, 2002). The
following example (from Hadley, 2009) shows a pattern              Figure 8: Pattern completion solving Hadley' rapid variable
completion task that humans are readily able to solve given        creation problem. The input consists of pairs of lists. After
only a few items. They are told that if they hear “biffle             seeing 0094→94, 0014→14, and 0024→24, it correctly
biffle rose zarple”, the correct response is “rose zarple”.         concludes that given 0074, it can complete the pattern by
                                                                                           outputting 74.
                                                              1022

                          Discussion                                 Spaun presents a detailed spiking neural model capable of
                                                                   visual recognition, cognitive control, working memory,
The basic components of the model presented here are not
                                                                   symbolic manipulation, and producing hand-written motor
new; we have previously published spiking neuron models
                                                                   outputs. This sort of model is required for connecting high-
capable of exhibiting list memory (Choo & Eliasmith,
                                                                   level cognitive theory and behavioural data to the biological
2010), pattern completion (Rasmussen & Eliasmith, 2011),
                                                                   constraints available from neuroscience.
action selection (Stewart, Choo, & Eliasmith, 2010), and
sequential reasoning (Stewart & Eliasmith, 2011).
However, the work presented here is the first demonstration                                 References
of these capabilities in a single, unified model. There are no
adjustments made to the model between tasks, and indeed            Chaaban, I., & Scheessele, M. R. (2007). Human
the model can seamlessly go from one task to the next.               performance on the USPS database. Technical Report,
  Furthermore, we feel that an important feature of this             Indiana University South Bend.
cognitive model is that it includes the entire system: visual      Choo, F., Eliasmith, C. (2010). A Spiking Neuron Model of
perception, cognition, and motor action. The neural                  Serial-Order Recall. In Richard Cattrambone & Stellan
representations used throughout the model are the same, as           Ohlsson (Eds.), 32nd Annual Conference of the Cognitive
are the underlying computational principles, and methods of          Science Society. Portland, OR: Cognitive Science Society.
mapping to neural spikes. As a result, this single model has       Cordes, S., Gelman, R., Gallistel, C. R., & Whalen, J.
neuron responses in visual areas that match known visual             (2001). Variability signatures distinguish verbal from
responses, as well as neuron responses and circuitry in basal        nonverbal counting for both large and small numbers.
ganglia that match known responses and anatomical                    Psychonomic Bulletin & Review, 8(4), 698–707.
properties of basal ganglia, as well as behaviorally accurate      DeWolf, T. (2010). NOCH: A framework for biologically
working memory limitations, as well as the ability to                plausible models of neural motor control. Masters Thesis.
perform human like induction, and so on. Spaun is thus both          University of Waterloo, Waterloo.
physically and conceptually unified.                               Eliasmith, C. (2012). How to build a brain: A neural
  It should be noted that the current set of tasks Spaun can         architecture for biological cognition. Oxford University
perform are in a constrained semantic space – that of lists of       Press, New York, NY.
numbers. However, the basic principle of using high-               Eliasmith, C. & Anderson, C. (2003). Neural Engineering:
dimensional vectors that can be bound together (i.e.                 Computation, representation, and dynamics in
semantic pointers) generalizes to more complex domains.              neurobiological systems. Cambridge: MIT Press.
  Furthermore, the model’s architecture is not tightly tied to     Gayler, R. (2003). Vector Symbolic Architectures Answer
the set of tasks being implemented. That is, rather than             Jackendoff’s Challenges for Cognitive Neuroscience, in
having particular components to perform each task, the               Slezak, P. (ed). Int. Conference on Cognitive Science,
components presented here provide generic cognitive                  Sydney: University of New South Wales, 133–138.
capacities, and any given task can recruit these components        Hadley, R. F. (2009). The problem of rapid variable
as needed. For example, the Pattern Completion task                  creation. Neural computation, 21(2), 510–32.
requires use of a component that can find the transformation       Hinton, G.E. (2010). Learning to represent visual input.
that relates the information in two different areas of working       Phil. Trans. Roy. Soc. B, 365, 177-184.
memory. This cortical component would also be useful for           Jackendoff, R. (2002). Foundations of language: Brain,
performing other tasks, such as a Raven's Matrix task                meaning, grammar, evolution. Oxford University Press.
(Rasmussen & Eliasmith, 2011).                                     Landauer, T. (1962). Rate of implicit speech. Perceptual
  For the model presented here, all synaptic connection              and Motor Skills, 1, 646.
weights between neurons are analytically derived, rather           Marcus, G. F. (2001). The algebraic mind. MIT Press,
than having them be learned, as in traditional neural                Cambridge, MA.
network models. While this demonstrates that our model is          Rasmussen, D., Eliasmith, C. (2011). A neural model of
capable of learning without connection weight changes (as            rule generation in inductive reasoning. Topics in
in the pattern completion task), it leaves open the question         Cognitive Science, 3(1), 140-153.
of how these connections are learned in the real brain.            Stewart, T.C., Bekolay, T., Eliasmith, C. (2012). Learning
While we do not have a complete developmental story for              to select actions with spiking neurons in the basal ganglia.
the various cortical components, we have developed a                 Frontiers in Decision Neuroscience. 6.
dopamine-based reinforcement learning system (Stewart,             Stewart, T.C., Choo, X., and Eliasmith, C. (2010). Dynamic
Bekolay, & Eliasmith, 2012) that has been integrated with            Behaviour of a Spiking Model of Action Selection in the
Spaun in an n-arm bandit task, but the results are not               Basal Ganglia. 10th Int. Conf. on Cognitive Modeling.
presented here due to space limitations. This system allows        Stewart, T.C., Eliasmith, C. (2011). Neural Cognitive
Spaun to learn the connections between the cortical                  Modelling: A Biologically Constrained Spiking Neuron
components and the basal ganglia, allowing the model to              Model of the Tower of Hanoi Task. In L. Carlson, C.
learn to recruit different components for different tasks.           Haelscher, & T. Shipley (Eds.), 33rd Annual Conference
                                                                     of the Cognitive Science Society.
                                                               1023

