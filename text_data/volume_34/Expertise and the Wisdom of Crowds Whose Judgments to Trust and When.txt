UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Expertise and the Wisdom of Crowds: Whose Judgments to Trust and When
Permalink
https://escholarship.org/uc/item/3938d1jk
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Author
Welsh, Matthew
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

       Expertise and the Wisdom of Crowds: Whose Judgments to Trust and When
                                   Matthew B. Welsh (matthew.welsh@adelaide.edu.au)
                                                  University of Adelaide, North Tce
                                                     Adelaide, SA 5005, Australia
                             Abstract                                by Surowiecki (2004). The observation is simply that, when
                                                                     making decisions under uncertainty, the median or mean
  The Wisdom of Crowds describes the fact that aggregating a
  group’s estimate regarding unknown values is often a better        estimate of a crowd is often a better predictor than the
  strategy than selecting even an expert’s opinion. The efficacy     estimate of a randomly chosen individual – even an expert.
  of this strategy, however, depends on biases being non-              This initially surprising observation results simply from
  systematic and everyone being able to make a meaningful            the underlying mathematics of the problem. If any biases or
  assessment. In situations where these conditions do not hold,      errors in people’s estimates are independent, then they will
  expertise seems more likely to produce the best outcome.           tend to be in random directions and thus, when averaged,
  Amateurs and professional judgments are examined in a
  subjective domain – reviews of shows from an Arts festival –
                                                                     will be removed. This has allowed researchers to
  asking which group provides better information to the              demonstrate that even having the same individual make an
  potential theatre-goer. In conclusion, while following the         estimate twice and averaging those values can produce
  crowd produces good results, where a smaller number of             better estimates – so long as some degree of independence
  reviews are available, taking expertise into account improves      can be established between the two estimates (Herzog &
  their usefulness and discrimination between shows.                 Hertwig, 2009; Vul & Pashler, 2008).
   Keywords: Expertise, Wisdom of Crowds, subjective                   For the wisdom of crowds to work, therefore, one needs
   judgment.                                                         to be considering a domain in which biases in people’s
                                                                     judgments are not systematically related to those of other
                         Introduction                                people. If this condition is met, then one expects that
When making decisions between diverse options, we often              averaging the judgments of a group regarding the quality of
do not have sufficient time or resources to conduct the sorts        a particular show would provide a better estimate of how
of thorough analyses recommended by decision analysts                much you will enjoy it than relying on the advice of any
(see, e.g., Newendorp & Schuyler, 2000). Instead, we rely            single reviewer.
on simple rules to greatly reduce the complexity of our
decision making while maintaining as much quality as                 Expertise
possible (Gigerenzer & Todd, 1999). Perhaps the simplest             By comparison with the wisdom of crowds, expertise is a
such rule is: if someone recommends option A, then I will            harder creature to pin down. While we all have an implicit
select option A.                                                     understanding of what expertise is, actually defining it
  This approach, of course, requires that you have some              proves surprisingly difficult (see, e.g., Shanteau, 2002;
idea of whether or not you should trust the opinion of the           Weiss, 2003) and people commonly confuse it with simple
person offering it, which is easy when it is a person you            length of experience (Malhotra, Lee, & Khurana, 2005).
know but more difficult when you are forced to rely on the             Despite this, given that we know there is such a thing as
opinions of strangers – as is often the case.                        expertise and that people are employed on the basis of this
  As an example, consider a person’s decisions regarding             to provide expert advice, it would seem reasonable for us to
what to spend his/her entertainment budget on. While they            expect that this advice will be valuable – more valuable, at
could wait and hope that their friends will go to see all of         least, than a non-expert’s judgment.
the various shows that they were interested in, more often,
they will have to rely on reviews from either professional           Decision Criteria
reviewers or sites such as “Rotten Tomatoes” that aggregate          An important question, which should be asked before
amateur review data. In either case, the criteria on which the       continuing, relates to the decision criteria being used. This is
reviewers have provided their rating is generally unknown            important as, when we ask a question, we can only receive
to the people using the information.                                 meaningful responses if the person understands and answers
  The question, then, is how to make the best use of the             the question we have asked. In the case of reviews of
available information – from both professional and amateur           entertainment, then, what is the question that is being asked?
reviewers – in order to make informed decisions about the              The difficulty here is that expert and non-expert reviewers
quality of entertainment on offer.                                   may be answering different questions. Experts might be
                                                                     answering the question – how much artistic merit does the
The Wisdom of Crowds                                                 show have? Non-experts, by comparison, may be answering
The wisdom of crowds describes a well-known effect first             the simpler question – how much did you enjoy the show. In
discussed by Galton (1907) and more recently repopularized           both cases, the judgment is subjective and dependent on the
                                                                 1131

reviewers personal tastes but, in the first, it is also being       Looking at Figure 1, one sees that both subplots seem to
judged against taught norms of quality.                           display similarly shaped distributions – a decay function of
  A secondary concern is the fact that most reviews are           some type. The figure is, however, somewhat misleading as
undertaken on an absolute scale, whereas people are far           the y-axis of the Amateur subplot is displayed as if the
more comfortable and more accurate making relative                highest count was 100 when, in fact, it was 529 (as
judgments (see, e.g., Stewart, Brown, & Chater, 2005;             indicated by the high value on the y-axis).
Stroop, 1932). Given this, we need to be cautious in
interpreting what a reviewer may mean by any given                Figure 1. Histogram of number of reviews per reviewer by
review.                                                           reviewer group. Note: Amateur y-axis is non-linear at top.
                                                                                                            Amateurs                                Professionals
This Study                                                                                  529                                          12
In this study, reviews of entertainment will be analyzed in                                  80                                          10
                                                                          Frequency Count
order to determine how a person could best use the available                                                                              8
                                                                                             60
information to select a show to attend. It thus overlaps                                                                                  6
significant with problems such as the Netflix Prize (Bennett                                 40
                                                                                                                                          4
& Lanning, 2007) but is approached from a psychological                                      20
                                                                                                                                          2
rather than machine learning stance – that is, incorporating
                                                                                                                                          0
concepts such as expertise and considerations of why we                                       0
                                                                                               0       10      20      30        40        0       10      20   30       40
have the data we do and how this should affect its use (for                                                                Reviews per Reviewer
further discussions of this, second, point, see, Welsh &            That is, while only a modest proportion (12/54) of the
Navarro, 2011; Welsh, Navarro, & Begg, 2011).                     professionals reviewed only a single show, the majority of
                                                                  amateurs (529/735) did so.
                          Method
The data sets selected for analysis consisted of reviews of                                                                  Results
acts performing at the 2011 Adelaide Fringe Festival – a
large, “unjuried” Arts Festival held annually in Adelaide,        Indirect Comparisons
Australia. Being an unjuried festival, any act is free to         As an initial approach to the question of whose reviews
register to perform without being selected by the festival’s      should be trusted, the distributions of star-ratings within
governing body. As such, the quality of performances is           each database were compared. Figure 2 shows the
(presumably) more variable than would be observed in a            histograms of this data.
juried festival where acts must convince the festival’s jury
of their quality before registering.                              Figure 2. Histogram of Star ratings by reviewer group.
   Given this, selecting a quality show to attend from the
                                                                                                            Amateurs                                Professionals
hundreds (750 in 2011) on offer becomes a difficult task in                             1000 Mean = 4.33
                                                                                                                                        120 Mean = 3.55
                                                                                             Median = 5
the absence of reliable indicators of quality. To this end, two                          800
                                                                                             Mode = 5
                                                                                                                                            Median = 3.5
                                                                                                                                        100 Mode = 4
databases of reviews were acquired: first, the Adelaide
                                                                     Frequency Count
                                                                                                                                         80
Fringe’s summary of published, professional reviews from                                    600
                                                                                                                                         60
newspapers and news websites – labeled simply “Fringe”                                      400
hereafter; and, second, the database from BankSA’s                                                                                       40
                                                                                            200
“Talkfringe” website which allows anyone to register and                                                                                 20
post reviews of any Fringe shows that they have seen.                                         0
                                                                                                   1    2      3       4     5
                                                                                                                                          0
                                                                                                                                               1    2      3    4    5
   All of the Talkfringe reviews use the same 1 to 5 ‘Star’                                                                      Star Rating
rating system (with half stars). The professional reviews,
however, were in a variety of formats. To maintain                  Looking at Figure 2, one sees that the two distributions
comparability, therefore, only professional reviews that used     differ significantly from one another, as confirmed by an
a 5-star rating system were included in the analyses.             independent samples t-test, t(1799) = 13.9, p < .001,
                                                                  Cohen’s d = 0.81. The Amateurs display something close to
                Data Characterization                             an exponential distribution of star-ratings, with a median
                                                                  and mode at 5 and a mean of 4.33, while the professionals
The Fringe database records 365 reviews in the required 5-
                                                                  display something closer to a Gaussian, with a mean and
star format, made by 54 reviewers – an average of 6.8
                                                                  median around 3.5 and a mode at 4. This raises questions
reviews per reviewer. By contrast, the Talkfringe database
                                                                  about the discriminability of Amateur reviews – that is,
contains 1436 reviews made by 731 reviewers. Figure 1
                                                                  whether seeing a 5 star review from an amateur allows you
displays this information as a histogram of reviews per
                                                                  to conclude anything meaningful about that show.
reviewer for the Amateurs (Talkfringe) and Professionals
                                                                    There are, however, alternate possible explanations for
(Fringe) separately. Between the two databases, reviews
                                                                  this pattern of responses. The first is that amateurs tend to
were obtained for a total of 420 shows, with each being
                                                                  be less discriminating in their tastes than the professional
reviewed an average of 4.3 times.
                                                              1132

and, thus, enjoy shows more. The second, however, is a                         reviews and a comparison of the distribution of star ratings
selection effect – while professionals are told which shows                    within this group with that for the complete datasets shown
to attend and write reviews of all of the shows that they                      in Figure 2 revealed no noticeable differences. Figure 3
attend, amateurs choose shows that they think they will like                   plots the mean reviews provided by each group for each
and are less likely to write a review unless motivated by                      show against that calculated from the other group.
particularly enjoying or disliking the show. Given that more                     Looking at Figure 3, one can see that the relationship
popular shows attract greater audiences, and assuming a                        between the amateur and professional reviews is positive,
positive relationship between quality and popularity, this                     but not particularly strong – confirmed by a correlation
will tend to result in large numbers of high-star reviews for                  r(190) = 0.32, p < .001, indicating significant disagreement
popular shows and relatively few reviews of any sort for                       between the two groups on the quality of shows.
less popular shows.                                                              A closer examination of the figure reveals that a partial
   Based on this reasoning, one could assume that any show                     explanation for the poor correlation may be restricted range
that has multiple, high-star reviews from amateur reviewers                    – with relatively few datapoints in the lower left quadrant.
is likely to have been a popular show.                                         Again, this is likely to reflect selection biases, with all type
                                                                               of reviewers more likely to attend and review popular shows
Direct Comparisons                                                             – which, in turn, are likely to be of higher quality.
   The above discussion considers only the distributions of
star ratings, rather than those instances where we have                        Quality by Popularity
reviews of the same show made by both amateur and                              Given the data above, what can we say about how a person
professional reviewers. An examination of the two                              should go about selecting a show to see? As noted above,
databases revealed that, of the 420 shows, 191 of these were                   there is an assumption that higher quality shows are more
‘shared’; that is, had been reviewed by at least one member                    likely to become more popular and that the number of
of each reviewer group.                                                        reviews can be used as a proxy for popularity. This means
   Looking only at these ‘shared’ shows, the difference                        that we can compare the star-ratings for shows of differing
between the professional and amateur groups (3.59 versus                       popularity to see how these variables interact. Figure 4,
4.33) is almost exactly the same as for the full dataset (3.55                 below, plots show star-ratings against number of reviews for
versus 4.33) and remains significant by a paired samples t-                    all 420 shows contained in both databases.
test, t(1231) = 11.2, p < .001, Cohen’s d = 0.79.
                                                                               Figure 4. Scatterplots of number of reviews (show
Figure 3. Scatterplot of mean amateur versus mean                              popularity) versus mean rating (show quality) for Amateur
professional review for all 191 ‘shared’ shows. NB – some                      and Professional reviewers. NB – some jitter has been added
jitter has been added to the points to reduce overlap and                      on the y-axis to facilitate display.
facilitate display.
                                                                                                 5
                                                                                                 4
                                                                                                 3
                                                                                                 2
                                                                                                 1                                                   Amateurs
                                     5                                                           0
                                                                                                  0   10       20          30           40           50         60
     Mean Professional Star Rating
                                                                                                 5
                                                                                   Mean Rating
                                                                                                 4
                                     4                                                           3
                                                                                                 2
                                                                                                 1                              Amateurs (1−5 reviews/show)
                                                                                                 0
                                     3                                                            0        1         2              3            4              5
                                                                                                 5
                                                                                                 4
                                     2                                                           3
                                                                                                 2
                                                                                                 1                                             Professionals
                                                                                                 0
                                     1                                                            0        1         2              3            4              5
                                                                                                                    Number of Reviews
                                                                                 Looking at Figure 4, one sees that the mean ratings of
                                         1      2       3        4      5      shows that received low numbers of reviews vary quite
                                             Mean Amateur Star Rating          significantly – indeed for shows with only one or two
                                                                               reviews, the mean ratings are fairly uniformly distributed
  Despite the removal of over 200 shows that lacked a
                                                                               across the 1-to-5 range.
rating from each group, a consideration of only the
                                                                                 For shows with higher numbers of reviews, however, one
overlapping shows still contains the majority of the review
                                                                               sees a striking pattern emerge – as the number of reviews
data as these 191 shows attracted 1233 of the total 1801
                                                                               increases, so does the minimum mean rating that that show
                                                                            1133

received. Comparing the bottom two subplots, one sees that        rather than few reviews. Figure 5 thus plots number of
this pattern emerges early in both the amateur and                reviews per amateur reviewer against star ratings.
professional reviews; no show with 3 or more reviews
averages less than a 2-star rating.                               Figure 5. Scatterplot comparing number of reviews to mean
  Looking across the top subplot of Figure 4, one can see         star rating (amateurs only). ‘Jitter’ has been added to the
this predictive power continues for higher numbers of             data along the y-axis to prevent datapoints overlapping The
reviews: no show with 6 or more reviews was rated lower           red line shows the overall mean for each group of reviewers.
than 3 star (on average); no show with 14 or more reviews
                                                                                                     5
was rated lower than 4 star (on average); and the 7 shows
that were reviewed by 25 or more people all averaged at
least 4.5 star reviews.                                                                              4
  This would seem to confirm the prediction that popularity
                                                                      Mean Star Rating with 95% CI
and quality are, in fact, linked and suggest that an
appropriate strategy for selecting a quality show would be to                                        3
select one that many people have reviewed – even without
reading those reviews.                                                                               2
Expert vs Non-Expert Reviews
                                                                                                     1
A final question to be addressed is that of expertise. While
we have, above, divided reviewers according to whether
they are Professional or Amateurs – and assume that this                                             0
                                                                                                      0   5   10   15     20      25   30   35   40
reflects some difference in expertise (in reviewing shows) –                                                       Number of Reviews
the data afford us some scope to test this assumption.
   Looking once more at Figure 4, for example, one can see          Looking at Figure 5, one sees a trend as the number of
a suggestive pattern in the comparison between the Amateur        reviews that a person has posted increases; specifically, as
and Professional results – where the speed at which the           the number of reviews increases, the average review tends
predictive multiple reviews increases seems greater for the       to decrease, r(729) = -0.20, p < .001.
Professional. That is, having had multiple Professional             This could be explained by a drop-off in the quality of
reviewers attend a show may be a better indicator of quality      shows – if everyone were seeing the same shows and there
than having had the same number of Amateurs review it.            were only a small number of genuinely 5-star shows, for
  A more important question, however, is whether we can           example. Given the number of shows involved, however,
establish that expert reviews are better than non-expert          and how many of these received 5 star ratings from
reviews. The difficulty, of course, is in determining how we      someone, this seems an unlikely explanation. Instead, it
measure the quality of a review – after the fact and in the       seems more likely that we have support for the idea that
absence of any objective standard. A simple wisdom of             increased experience in reviewing (and, therefore, seeing
crowds approach would suggest that we use the median or           more shows) changes the ratings that one is likely to give.
mean review from all reviewers as the standard but this runs        Suggestively, the most prolific reviewers in Figure 5 give
into the problem of non-discriminability in the amateur data      average ratings that are more typical of Professional
where too many shows will all be rated 5-star.                    reviewers than the other Amateurs. That is, their mean
  There are, however, at least two methods of using the           ratings tend to be between 3 and 4 rather than 4 and 5.
current data to shed light on the relative usefulness of            The question remains, however, as to whether this reflects
professional and amateur reviews in selecting a good show.        better reviews; and the problem is, of course, that as
                                                                  enjoyment of a show is highly subjective, it is possible that
Measuring the Expertise of Amateur Reviewers                      what is the better (i.e., more predictive) review differs
The first of these involves a comparison of the differences       between individuals.
within the two groups. For example, it seems a reasonable            On the basis of these results, for example, one might
assumption that those Amateurs who review more shows              conclude that the more shows one is inclined to see, then the
become more expert in doing so. The same relationship, of         more similar one’s own ratings will be to those of
course, is less likely to hold in the Professional reviewers as   Professional reviewers. If so, then one should weight
the assumption is that these people have significant previous     professional reviews more highly than amateur ones – or,
experience that is not available to us through the data set;      where these are unavailable, downgrade ‘overly-
and which is likely to outweigh any effect of the relatively      enthusiastic’ amateur reviews.
few reviews they made during this event. Given the above,
it seems necessary to restrict this discussion to differences     Consistency of Different Reviewers
within the Amateur group.                                         A second consideration in what makes one review better
   What then are the differences between the more and less        than another is their reliability. That is, when two people
‘expert’ amateurs – that is, between those who posted many        have seen the same show, are they inclined to give the same
                                                                  rating? A comparison between the Amateurs and
                                                              1134

Professionals on such a measure might allow one to have            reviews, which results in people only writing a review if
greater or lesser confidence in one group’s ratings.               they are motivated to do so - which, we suggest is most
   Within the Professional reviewers group, there were 70          likely when they particularly like or dislike a show. This
shows that had been reviewed by at least 2 reviewers –             effect will, therefore, tend to push results even further
which yielded a total of 97 pair-wise comparisons (due to          towards the extremes and, given the effect described above,
some shows being rated by three or four reviewers). Thirty         this will tend to push more people into the very high part of
of these had exactly the same rating, with another 40              the rating range.
differing by only half a star. Overall, the average difference        Thus we have a large number of reviews that are
between ratings of the same show by professional reviewers         relatively uninformative – reflecting the fact that a person
was approximately half a star (M = 0.56, SD = 0.52).               predisposed to like a particular show really liked it. A result
   The Amateur group, by comparison, had 228 shows with            of this is the lack of discrimination in the amateur data
multiple reviewers, which resulted in 10,401 pair-wise             where, because so many reviews give 5-star ratings, it
comparisons. This number, however, is dominated by the             simply doesn’t help us to make a decision regarding which
relatively small number of very popular shows – those on           of these shows we should attend and short-circuits attempts
which we see a ceiling effect resulting from the selection         to use the wisdom of crowds based on median values – as
bias. The most popular show, for example, has 60 reviews,          we would end up comparing 5-stars with 5-stars.
58 of which are 5-star – with one 1-star and one 3-star               A second (but related) concern is that the majority of
review making up the numbers. This show contributes 1770           amateur reviewers (529 of 731) wrote only a single review.
unique pair-wise comparisons – over a sixth of the total –         Given what we know about people’s inability to directly
and would thus, if included, overwhelm any effects of the          assess values, the use of relative preferences (e.g.,
inter-rater reliability more generally. To ensure                  converting the ratings to rankings) is a sound method for
comparability with the Professional results, therefore, only       improving our understanding of what people’s expressed
shows that had been reviewed by between 2 and 4 reviewers          preferences actually mean. With only one review per
(the numbers observed in the professional sample) were             reviewer, however, we cannot meaningfully assess relative
included in the analyses. This resulted in the removal of 79       preferences.
shows, leaving 149 and a total of 404 unique pair-wise                By comparison, a professional reviewer, while exercising
comparisons.                                                       some choice over which shows to see will also have some
   Of these, 120 had exactly the same rating, 114 differed by      dictated by their employers and will be asked to write a
half a star and 170 differed by 1 full star or more. The           review of all of the shows that they see. They are, from our
average difference between the amateur reviewers’ ratings          data, far more likely to see multiple shows, and have a less-
for these shows was 0.82 stars (SD = 0.87), significantly          skewed distribution of ratings. They were also, in the subset
higher than that observed in the Professional reviewers’           of shows with a relatively few reviewers, more often in
ratings, t(499) = 2.83 p = .002.                                   agreement with one another than were the amateurs.
                                                                      This means that, in relying on professional reviews, one is
                          Discussion                               better able to discriminate between their preferences for
The results paint a complex picture of the relationships           those shows that they have seen and also can be more
between reviewer expertise and the use of aggregation              assured that their review is reliable – that is, that another
strategies such as the wisdom of crowds for reviews from           professional reviewer would have a similar opinion.
multiple sources.                                                     An addendum to this is that the data support the idea that
   Perhaps the single best predictor of show quality (i.e.,        the difference between amateurs and professional is related
how much people enjoyed the show) was the total number             to experience/expertise. Amateurs who reviewed larger
of reviews that the show had received – reinforcing the            numbers of shows gave ratings that were more like the
assumption that popularity and quality are linked. Note,           professionals. This could suggests that people are, in fact,
however, that this is a distinct effect from the wisdom of         rating shows on a relative scale but that the single-review
crowds as the results suggest that we don’t need to look at        amateurs have fewer shows to compare with and thus the
the ratings provided by reviewers at all. Instead, all we need     chance of the show being amongst the best they have seen is
to do is “follow the crowd” and they will lead us to good          relatively greater. The professionals and high-rate amateurs,
shows.                                                             by comparison, have a great many shows to compare the
   In cases without such overwhelming endorsement,                 current show to and thus the likelihood of it being judged
however, we are forced to rely on the numerical ratings            exceptional (5-star) is relatively less.
provided by the expert and amateur reviewers and can run
into difficulties in determining what to do.
                                                                   Caveats
   The first problem we observed in the data was the strong        In so subjective a domain, there are, of course, a number of
selection bias in the amateur data; because people tend only       caveats to consider in conjunction with the arguments made
to pay to see shows that they expect to like, the distribution     above. A primary one, of course, is that we have not made
of star ratings gets shifted to the right – with more 5-star       any attempt to look at the types of shows that different
reviews. Added to this is the voluntary nature of amateur          people have attended and rated. If we expect that different
                                                               1135

people have different tastes in entertainment, then we could         In these cases, therefore, following the advice of more
conduct a far more fine-toothed analysis of preferences.           expert reviewers (i.e., professionals and experienced
   This importance of this for the current findings, however,      amateurs) seems more likely to provide discrimination as
is that one might expect a difference in preferences between       they display less selection bias in their shows seen, meaning
professional and amateur reviewers. For example, while             that they tend to write reviews of a variety of shows and
purely speculative, it would seem entirely feasible that           have clearly discriminable preferences between these.
professional reviewers prefer more serious art whereas the
amateurs prefer lighter, comedic events.                                              Acknowledgments
   If this is the case, then one would have to take into           Thanks to ExxonMobil and Santos for their support of the
account such between group differences when determining            CIBP group in the Australian School of Petroleum; to
whose reviews should be taken into account when making a           Michelle Read from the Adelaide Fringe Festival and Simon
decision. That is, knowing that professionals reliably tend to     Evans at BankSA for their assistance in accessing the
rate a show highly may be of no help at all if it is a type of     review databases; and to Dan Navarro, Anna Ma-Wyatt,
show that you do not enjoy.                                        Steve Begg and three reviewers for their comments.
   A second caveat is that there has not, as yet, been any
attempt to weight or rank the data, which would, as
described earlier, be expected to improve the predictive
                                                                                            References
power of ratings – from those reviewers who reviewed               Bennett, J., & Lanning, S. (2007). The Netflix Prize.
multiple shows at least. An appropriate application of such          Proceedings of KDD Cup and Workshop, San Jose, CA.
tools, however, requires a fundamental grasp on the nature         Galton, F. (1907). Vox populi. Nature, 75, 450-451.
of the data; a grasp that has been greatly strengthened by the     Gigerenzer, G., & Todd, P. M. (Eds.). (1999). Simple
exploratory approach taken here.                                     heuristics that make us smart. Oxford, UK: Oxford
                                                                     University Press.
Future Research                                                    Herzog, S. M., & Hertwig, R. (2009). The wisdom of many
Given the findings and the caveats noted above, a number of          in one mind: improving individual judgments with
directions for continuing the research suggest themselves.           dialectical bootstrapping. Psychological Science, 20(2),
The first is to examine the data in finer detail, dividing           231-237.
shows according to type - to see whether specific reviewers        Malhotra, V., Lee, M. D., & Khurana, A. K. (2005).
can be identified as having preferences between these.               Domain experts influence decision quality: towards a
   Data beyond the ratings could also be accessed – for              robust method for their identification. Journal of
example, using ticket sales to directly measure the                  Petroleum Science and Engineering, Special Issue.
popularity of a show rather than simply assuming that              Newendorp, P. D., & Schuyler, J. (2000). Decision Analysis
number of reviews is a reflection of popularity.                     for Petroleum Exploration. Aurora, CO: Planning Press.
   This additional information, used in conjunction with           Shanteau, J. (2002). Performance-based assessment of
ranking and weighting algorithms, could then be used to              expertise: how to decide if someone is an expert or not.
generate predictive models for individuals based on the              European J. of Operational Research, 136(2), 253-263.
shows that they have seen and how much they enjoyed them           Stewart, N., Brown, G. D. A., & Chater, N. (2005).
and using one half of the data to predict the other – in a           Absolute      identification    by      relative   judgment.
similar fashion to the Netflix recommendation algorithms             Psychological Review, 112(4), 881-911.
developed as part of the Netflix Prize competition (Bennett        Stroop, J. R. (1932). Is the judgment of the group better than
& Lanning, 2007).                                                    that of the average member of the group? Journal of
   Finally, experimental work designed to directly measure           Experimental Psychology, 15(5), 550-562.
selection biases in reviews could be conducted, building on        Surowiecki, J. (2004). The Wisdom of Crowds. New York,
the work herein. Similarly, such work could potentially              NY: Random House.
distinguish between alternative judgment strategies – for          Vul, E., & Pashler, H. (2008). Measuring the crowd within:
example, if experts are attempting to provide ‘absolute’             probabilistic     representations      within    individuals.
quality judgments whereas amateurs are just indicated                Psychological Science, 19(7), 645-647.
whether they like a show or not.                                   Weiss, D. J. (2003). Empirical assessment of expertise.
                                                                     Human Factors, 45(1), 104-116.
Conclusions                                                        Welsh, M. B., & Navarro, D. J. (in press). Seeing is
                                                                     believing: priors, trust and base rate neglect.
Within a domain such as entertainment reviews, good                  Organizational Behavior and Human Decision Processes.
decisions can be made by following the crowd – if not                Accepted April 6th 2012.
always using the wisdom of crowds, per se. Where choices           Welsh, M. B., Navarro, D. J., & Begg, S. H. (2011).
need to be made between shows, however, amateur                      Number preference, precision and implicit confidence. In
reviewers ratings tend to cluster too closely around the             L. Carlson, C. Hölscher & T. Shipley (Eds.), Proceedings
maximum rating – as a result of selection bias in both show          of the 33rd Annual Conference of the Cognitive Science
choice and the decision to write a review.                           Society (pp. 1521-1526) Austin, TX: CSS.
                                                               1136

