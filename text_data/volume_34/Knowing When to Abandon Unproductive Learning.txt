UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Knowing When to Abandon Unproductive Learning
Permalink
https://escholarship.org/uc/item/7wj7w42n
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Shultz, Thomas
Doty, Eric
Dandurand, Frederic
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                             Knowing When to Abandon Unproductive Learning
                                            Thomas R. Shultz (thomas.shultz@mcgill.ca)
                            Department of Psychology and School of Computer Science, McGill University
                                          1205 Penfield Avenue, Montreal QC, Canada H3A 1B1
                                                 Eric Doty (eric.doty@mail.mcgill.ca)
                                                Department of Psychology, McGill University
                                           1205 Penfield Avenue, Montreal QC, Canada H3A 1B1
                                      Frédéric Dandurand (frederic.dandurand@gmail.com)
                                Department of Psychology, Université de Montréal, 90 ave. Vincent-d'Indy
                                                        Montréal, QC H2V 2S9 Canada
                              Abstract                                     Cessation of learning without mastery is considerably
                                                                        more problematic, despite being an important component of
   Autonomous learning is the ability to learn effectively
   without much external assistance, which is a desirable               autonomous learning in biological agents. It may be useful
   characteristic in both engineering and computational-                to analyze such early quitting in terms of costs and benefits.
   modeling. We extend a constructive neural-learning                   The total cost of learning can be conceptualized as energy
   algorithm, sibling-descendant cascade-correlation, to monitor        expenditure (of the learning effort) plus opportunity cost
   lack of progress in learning in order to autonomously abandon        (the value of the best alternative not chosen, whether other
   unproductive learning. The extended algorithm simulates              learning or exploitation of resources): 𝐶𝑜𝑠𝑡𝑇𝑜𝑡𝑎𝑙 =
   results of recent experiments with infants who abandon
   learning on difficult tasks. It also avoids network overtraining     𝐸𝑛𝑒𝑟𝑔𝑦𝐿𝑒𝑎𝑟𝑛 + 𝐶𝑜𝑠𝑡𝑂𝑝𝑝𝑜𝑟𝑡𝑢𝑛𝑖𝑡𝑦 . Then the net payoff of
   effects in a more realistic manner than conventional use of          learning is the benefit of successful learning minus the total
   validation test sets. Some contributions and limitations of          cost of learning: 𝑃𝑎𝑦𝑜𝑓𝑓𝑁𝑒𝑡 = 𝐵𝑒𝑛𝑒𝑓𝑖𝑡𝐿𝑒𝑎𝑟𝑛 − 𝐶𝑜𝑠𝑡𝑇𝑜𝑡𝑎𝑙 . In
   constructive neural networks for achieving autonomy in               continuing to work on an unlearnable problem, there would
   learning are briefly assessed.                                       be a large negative payoff, cost without benefit. Having
   Keywords: autonomous learning; abandoning learning;                  started to learn such a difficult problem, it could be sensible
   constructive neural networks; SDCC.                                  to abandon it when lack of progress becomes evident.
                           Introduction                                 Previous Work on Abandoning Learning
Autonomous learning is the ability to learn effectively                 Recent computational modeling does suggest that a key
without much external assistance. As such, autonomy is a                factor in deciding to abandon learning early is whether
desired quality in fields such as machine learning and                  learning progress is being made (Schmidhuber, 2005, 2010).
artificial intelligence where the effectiveness of learning             In that work, learning progress is monitored by tracking the
systems is seriously compromised whenever human                         first derivative of error reduction to identify intrinsic
intervention is required. It is likewise a desired feature in           rewards, while a reinforcement-learning module selects
cognitive science where a goal is to understand the adaptive            actions to maximize future intrinsic rewards. These models
functioning of human and other biological agents in their               curiously conflate novelty with learning success, but it
natural environments. An important characteristic of                    seems more correct to base novelty on initial error, and
autonomous learners is that they can shape their own                    compute learning success as recent progress in error
learning and development, in large part by choosing what                reduction. These models also include a reinforcement-
problems to work on. Such choices include selecting a                   learning controller that selects actions, and an external
problem to learn and deciding whether to continue learning              network to track learning progress. It seems simpler to
on the selected task or abandon it in favor of something else.          continue learning by default until lack of progress is
                                                                        detected, perhaps in terms of stagnation in error reduction.
Knowing When to Quit                                                       In an idealized learning model, infant looking was
Knowing when to stop learning has two obvious                           modeled by information-theoretic properties of stimuli
components – quitting when the problem has been mastered                (Kidd, Piantadosi, & Aslin, 2010). The negative log
and when it is unlikely to be mastered. In the constructive             probability of an event (corresponding to the number of bits
neural networks that we favor, victory is declared, and                 of information conveyed by a stimulus) was conditioned on
learning terminated, when the network is correct on all                 observing previous events. The larger the negative log
training examples, in the sense of producing outputs that are           probability, the more surprising the current event. As
within some score-threshold of their target values (Fahlman             predicted, 7- to 8-month-old infants were more likely to
& Lebiere, 1990; Shultz, 2003).                                         look away from either highly informative or uninformative
                                                                        events. The authors dubbed this the Goldilocks effect as
                                                                    2327

infants prefer to work on tasks that are neither too easy nor       example, on problems with a random structure and
too difficult, but just about right in terms of complexity.         insufficient regularities. Of course, some potentially
Although interesting and consistent with an idealized               learnable problems are so difficult that their patterns may
statistical model, these findings are not tied to any neural        only seem random. In either case, learning may be
computational mechanisms. Also, this model is presumably            frustratingly slow and thus signal to stop and turn to
restricted to repeated sequences of events.                         something else more feasible. Here, we apply our extended
   Other recent experiments reported that 17-month-olds             algorithm to learning problems of varying randomness,
attend longer to learnable versus unlearnable artificial-           discuss its potential to cover the infant experiments just
language grammars, taking more trials and more time on              reviewed, and briefly assess the overall ability and
grammars in which a valid generalization over input                 limitations of CANNs to learn autonomously.
utterances could be made (Gerken, Balcomb, & Minton,
2011). Thus, there is now independent evidence that infants                                     Method
may have an implicit metric of their learning progress and
can direct their attention to more learnable material.              Algorithm Extension for Abandoning Learning
                                                                    As noted, each of the two phases in CC and SDCC assesses
Constructive Artificial Neural Networks                             progress within a phase. We define a learning cycle as an
Constructive artificial neural networks (CANNs) grow a              input phase, which recruits a hidden unit, followed by an
network topology while learning, inspired by principles of          output phase, which employs the new recruit to help reduce
brain function and statistical mechanics. Among the                 network error. (The first learning cycle has only an output
attractive features of CANNs are graded knowledge                   phase, and no input phase.) To assess learning progress
representations, capacity for change and self-organization,         across learning cycles, we implemented a new, outside loop
and neurological plausibility. CANNs such as cascade-               to assess progress at the end of each output phase, according
correlation (CC) grow by recruiting new hidden units whose          to the following algorithm, in which a counter is initialized
activity correlates with network error (Fahlman & Lebiere,          to 0:
1990). An extension, sibling-descendant cascade-correlation         If first learning cycle, then record current error and continue
(SDCC), dynamically decides whether to install a newly              to input phase
recruited unit on the current highest layer (as a sibling) or on    Otherwise, compare current error to previous error as
its own higher layer (as a descendant), thus optimizing the         absolute difference
network topology for the problem being learned (Baluja &               If absolute difference > threshold x previous error, then
Fahlman, 1994). Unit recruitment corresponds roughly to                reset counter to 0 and continue to input phase
processes of neurogenesis and synaptogenesis in the service            Otherwise,
of learning (Shultz, Mysore, & Quartz, 2007). Such CANNs                  If counter = patience, then abandon learning
have been used to simulate many cognitive, linguistic, and                Otherwise, increment counter by 1 and continue to
social phenomena while addressing important and                           input phase
longstanding issues about development and learning                     This algorithm is analogous to the progress-assessing
(Shultz, 2003; Shultz & Fahlman, 2010). They have also              loops already used in the output and input phases of CC and
yielded testable predictions, many of which have been               SDCC, which compute an absolute difference between a
confirmed in psychological research. Moreover, CANNs                current and previous measure (network error for output-
have also made considerable progress on several aspects of          phase and learning-cycle loops, covariance for input-phase
autonomous learning, including network construction in              loops), and test if this difference is greater than a threshold
which new abilities are built on top of earlier achievements.       proportion of the previous value. If the absolute difference
   In the present work, we extend SDCC to abandon learning          exceeds this product, learning continues. If it does not
that is failing to make progress. This is a natural extension       exceed this product, then there is a check to determine if a
for SDCC, which already is able to change phases when it            patience parameter value has been reached. If patience has
detects lack of progress. Both CC and SDCC operate in two           been exceeded, then the current loop is terminated;
phases: output phase, in which connection weights entering          otherwise the patience counter is incremented by 1 and
output units are adjusted to reduce network error, and input        learning continues. Resetting the counter to 0 whenever the
phase in which weights entering hidden units are adjusted in        threshold proportion is exceeded insures that the number of
order to increase the covariance between candidate-unit             cycles without exceeding the threshold proportion must be
activation and network error, which ends up recruiting the          consecutive rather than sporadic. Although we rarely alter
candidate that best tracks network error. Output phase ends         the threshold and patience parameters for output and input
when error reduction stagnates, whereas input phase ends            phases, here we do explore some parametric variation for
when the covariances between candidate activation and               assessing progress across learning cycles.
network error stop changing.
   We hypothesized that, if error stagnation continues even         Continuous XOR
after recruitment, this could additionally signal that the          We tested our extended algorithm on a continuous version
problem might be unlearnable. This would be the case, for           of the exclusive-or (XOR) problem. This is a well
                                                                2328

understood problem in which the simplicity of binary XOR           algorithm is effective at detecting lack of progress in
is replaced by a more complex continuous version (Shultz &         learning and show what underlies grouped results to follow.
Elman, 1994; Shultz, Oshima-Takane, & Takane, 1995).
Starting from 0.1, input values are incremented in steps of
0.1 up to 1, producing 100 x, y input pairs that are
partitioned into four quadrants of the input space, as
illustrated in Figure 1. There is a single output unit with a
sigmoid activation function. Values of x up to 0.5 combined
with values of y above 0.5 produce a positive output target
(0.5), as do values of x above 0.5 combined with values of y
up to 0.5. Input pairs in the other two quadrants yield a
negative output target (-0.5). These constitute the training
patterns for conditions that are completely learnable.
                                                                        Figure 2. Training error in two networks. With 50%
                                                                        learnability, learning stops at 99 epochs. With 100%
                                                                              learnability, learning stops at 420 epochs.
     Figure 1. Schematic drawing of the continuous-XOR                To see a more general picture, mean per-pattern training
  problem. Gray sectors yield a positive output while white        error for 20 networks under each learnability condition is
                     sectors a negative output.                    plotted in Figure 3, again for a learning threshold of .15.
                                                                   Per-pattern error for a network is computed by dividing total
   To implement problems of different levels of difficulty,        network error by the number of patterns. Each curve is cut
we vary learnability, defined as the percentage of target          off at the mean number of output-phase epochs to abandon
outputs that are not randomly selected: 0, 25, 50, 75, 80, 85,     learning for that level of learnability, even though some
90, 95, or 100. If a fresh random number in the range [1,          networks surpass this number. Figure 3 provides a more
100] ≥ the particular learnability percentage, then the output     complete demonstration that error reduction is greater with
target (-0.5 or 0.5) is selected by a .5 chance.                   higher learnability and that the extended algorithm is
   Generalization test patterns are generated by incrementing      effective at detecting lack of learning progress. Generally,
x and y values by 0.1 to .94 starting from 0.14. There are 81      the lower the learnability, the earlier learning is abandoned,
such test patterns, all with correct outputs.                      at least up to 90% learnability.
   In preliminary simulations, it became apparent that
learning results were also sensitive to variation in the
threshold parameter, so we varied threshold systematically
(.05, .1, .15, .2, and .3), while holding patience at 2.
                             Results
We do not present all of our results here, but only those
needed to make important points about basic principles.
Learning Threshold of .15
Typical training-error results are plotted in Figure 2 for two
networks, one exposed to patterns with 50% learnability and
the other exposed to patterns with 100% learnability.
Learning threshold is here set to .15. The diamonds just
above the error curves indicate the particular output-phase          Figure 3. Mean per-pattern training error for 20 networks
epochs at which a hidden unit is recruited. As is typical for          under each learnability condition over learning cycles.
all threshold values, error is reduced much further with full
learnability than with 50% learnability. Moreover, as is              Mean learning times are shown in Figure 4, which plots
typical for thresholds of .1 and higher, learning is               the mean output epochs and SE bars for the same 20
abandoned much earlier with 50% learnability than with             networks. This shows more abstractly that low levels of
100% learnability. These results suggest that the extended         learnability lead to early abandonment of learning.
                                                               2329

Moreover, the inverted U-shaped curve reveals a substantial         back-propagation, which have no natural stopping point and
Goldilocks effect wherein networks show more sustained              where there is no a priori idea of how many hidden units
learning for problems of moderate difficulty, peaking at            with which to equip a network. Such validation test sets are
90% learnability.                                                   ordinarily unnecessary for CANNs, which start small and
                                                                    keep growing until the problem is learned. With substantial
                                                                    numbers of random patterns to be memorized, as here, it can
                                                                    be beneficial to also use test error as a training aid, even for
                                                                    CANNs. With a learning threshold of .15, the extended
                                                                    SDCC algorithm was unable to detect, from training error
                                                                    alone, that learning was not progressing, in the sense of
                                                                    generalization ability. Although validation test sets are
                                                                    useful for programmers, they are unrealistic for autonomous
                                                                    learners. Whenever target values and the resulting error
                                                                    signals are available, it is likely that learners would use
                                                                    them to adjust connection weights, thus effectively
                                                                    eliminating such examples from the validation test set.
     Figure 4. Mean output epochs (with SE bars) for 20             Learning Threshold of .3
           networks with learning threshold of .15.                 This raises the question of whether other, less sensitive
                                                                    learning-threshold values could be used to curtail learning
  However, in these simulations, the prolonged learning             investment in unproductive tasks like our 50-90%
characteristic of the Goldilocks peak does not often yield          learnability conditions. The answer, as revealed in Figure 6,
superior performance. This is illustrated for these same            is yes for a learning threshold of .3. In this case, there are no
networks in Figure 5, which plots mean per-pattern test             general increases in test error, except at 0% learnability.
error for each learnability condition. Notice the rise in error
on test patterns for learnability conditions in the 50-90%
range. Such increases in test error over training suggest that
networks are over-fitting the training patterns and starting to
memorize the random training patterns instead of
abstracting a function to account for the examples. Their
earlier success in bringing error down is presumably due to
abstracting the continuous-XOR function. But from then on,
their only recourse is to start memorizing the random
patterns. At 0% learnability, it is impossible to abstract even
a basic idea of the exclusive-or problem.
                                                                    Figure 6. Mean test error averaged across 20 networks under
                                                                                      each learnability condition.
                                                                       However, the Goldilocks effect for these same networks
                                                                    disappears, as revealed in Figure 7. The learning-time peak
                                                                    is now at 100% learnability as all other conditions have
                                                                    abandoned learning earlier. More generally, we find a trade-
                                                                    off between the Goldilocks effect and avoidance of rising
                                                                    test error. As learning threshold increases, the likelihood of
                                                                    finding a Goldilocks effect drops.
  Figure 5. Mean per-pattern test error averaged across 20                                   Discussion
         networks under each learnability condition.
                                                                    Interpretation of Results
  A rise in test error in what is typically called the              Our results show that monitoring progress across learning
validation test set is conventionally used by programmers to        cycles can be used to abandon learning that is unlikely to be
determine when to stop network learning. This can be                successful. This is both realistic and adaptive because, with
particularly important when using static networks with              many problems and domains to learn, it is wasteful to
                                                                2330

devote time and energy to learn tasks that are too difficult or      before being able to predict whether they might succeed. It
impossible. In an abstract sense, on an admittedly different         would be interesting to see if this is also true of biological
task, our simulations show the ability to capture results like       learners. If learners exhibit shortcuts to avoid attempted
those in two new experiments on learning in human infants.           learning, this would imply generalizing across learning
Infants spend more time learning artificial grammars that            content due to previous experience, as when learning shuts
are possible to learn than they do on grammars that are              down in the presence of mathematical equations.
impossible to learn (Gerken, et al., 2011). Similarly, our              We also found that overtraining effects can be eliminated
neural networks abandon learning impossible tasks, but not           with high learning thresholds. This is more realistic for
tasks that are possible to learn. Further, the network results       autonomous agents than is monitoring error increases on a
show that the more difficult the task, the earlier that learning     validation set of test patterns. Moreover, we find that the
is abandoned, a finding that could serve as a prediction for         Goldilocks and overtraining effects tend to occur in the
new human experiments.                                               same circumstances, at relatively low rather than high
                                                                     learning thresholds. Goldilocks peaks are due to the
                                                                     increased learning times caused by low learning thresholds.
                                                                        There is, of course, more to autonomous learning than
                                                                     abandoning unsuccessful learning. There is also, for
                                                                     example, the choice of which problems to try to learn. We
                                                                     hypothesize that novelty detection, characterized by high
                                                                     initial error, plays a role in choosing learning problems.
                                                                     Abandoning fruitless learning is an essential component of
                                                                     autonomous learning because, as noted, it frees the learner
                                                                     to search for and work on more appropriate problems.
                                                                     Achieving Autonomy in Learning
                                                                     Our results show that a small extension to SDCC can
Figure 7. Mean output epochs (with SE bars) for 20                   provide a useful mechanism for detecting lack of progress in
networks, with learning threshold of .30.                            learning, which is an essential component of autonomous
                                                                     learning. In this context, it is worth considering how
   Another infant experiment showed a Goldilocks effect in           CANNs such as SDCC fare in terms of other aspects of
the sense of spending more learning effort on problems of            autonomous learning (Douglas & Sejnowski, 2007).
moderate difficulty than on problems that are too easy or too        Although there are no completely autonomous artificial
difficult (Kidd, et al., 2010). Our networks show this effect        learning creatures yet, it is also true that CANNs have made
as well, but add a qualification that the Goldilocks effect          considerable progress in increasing autonomy in learning.
diminishes at higher levels of a learning-threshold                     In terms of network construction, SDCC, unlike
parameter. This offers another prediction to test in human           algorithms for human-designed networks, autonomously
experiments. The psychological equivalent of learning                designs and builds a network topology that is well suited to
threshold could be sensitivity to changes in error.                  the problem being learned. The emerging topology can be
   In our model, easy tasks are discarded because they have          flat or deep or anything in between, and learning stops when
been learned, whereas overly difficult tasks are abandoned           the problem has been mastered.
because learning has stalled. This identifies two different             Unlike the ordered hierarchies of some static network
explanations for turning away from a learning task, one              topologies, SDCC implements a potentially deep,
based on success and the other on failure. In contrast,              heterarchical topology in which increasingly higher-level,
learning may continue as long as some detectable progress            more abstract concepts are composed of simpler ones. Each
is being made.                                                       new hidden unit in SDCC receives signals from input units
   Our model offers a plausible neural mechanism for such            and any existing lower level hidden units, thus continually
phenomena that allows for further theoretical exploration            building on existing knowledge. Knowledge-representation
and extensions. We plan to apply our algorithm to alternate          analysis shows that the first hidden units learn to represent
tasks and problems, including those used in psychology               the most obvious and superficial aspects of a problem
experiments and those that vary on dimensions of difficulty          domain, whereas later hidden units refine and abstract that
other than the proportion of random training patterns.               knowledge (Shultz, 2003). This componential structure is
   Our model predicts that learners need to have learning            further enhanced in knowledge-based CC (KBCC), where
experience with a problem in order to determine whether to           whole, previously learned sub-networks compete to be
continue with it or not. At least with inexperienced learners,       recruited (Shultz & Rivest, 2001; Shultz, Rivest, Egri,
there is no shortcut to avoid actually trying to learn.              Thivierge, & Dandurand, 2007).
Supporting this idea, we found that amount of first-trial               With regard to data selection, like many other artificial
error does not predict learnability on the problems we               neural networks, SDCC focuses on inputs that predict its
studied here. Learners may need to give it a serious try             output, quickly ignoring inputs that are not predictive.
                                                                 2331

Although such non-predictive inputs are rarely included in          Fahlman, S. E., & Lebiere, C. (1990). The cascade-
practice, it is important to note that, when they are included,       correlation learning architecture. In D. S. Touretzky (Ed.),
they are rapidly and functionally eliminated by learning of           Advances in neural information processing systems 2 (pp.
near-zero connection weights. It would be feasible to                 524-532). Los Altos, CA: Morgan Kaufmann.
eliminate such detected irrelevant inputs from training             Gerken, L. A., Balcomb, F. K., & Minton, J. L. (2011).
patterns altogether, effectively allowing learning to focus           Infants avoid ‘labouring in vain’ by attending more to
attention on what is important while creating a more                  learnable     than    unlearnable     linguistic   patterns.
efficient network.                                                    Developmental Science, 14, 972-979.
   Among the issues that remain challenges for CAANs, as            Kidd, C., Piantadosi, S. T., & Aslin, R. N. (2010). The
well as for other network learning algorithms, are single-            Goldilocks Effect: Infants’ preference for stimuli that are
trial learning, temporal spacing effects, the wake-sleep              neither too predictable nor too surprising. In S. Ohlsson &
cycle, synaptic meta-plasticity, relations between brain              R. Catrambone (Eds.), Proceedings of the 32nd Annual
structure and function, real-time learning in a changing              Conference of the Cognitive Science Society (pp. 2476-
world, and social learning (Douglas & Sejnowski, 2007).               2481). Austin, TX: Cognitive Science Society.
   The role of supervision of learning is a complex topic           Schmidhuber, J. (2005). Self-motivated development
deserving more extended discussion than we can provide                through rewards for predictor errors / improvements. In
here. Suffice it to say that CANNs can learn without a                Developmental Robotics 2005 AAAI Spring Symposium.
teacher.                                                              Stanford University, CA.
   For more genuine and more complete autonomy in                   Schmidhuber, J. (2010). Formal theory of creativity, fun,
learning, we believe that it will be important to examine the         and intrinsic motivation. IEEE Transactions on
evolution of learning methods and to implement                        Autonomous Mental Development (1990-2010), 2, 230-
computational models in robots, with pressures for real-time          247.
behavior in fluid environments. Evolution through natural           Shultz, T. R. (2003). Computational developmental
selection is the most plausible natural source of learning            psychology. Cambridge, MA: MIT Press, (Chapter
mechanisms in both biological and artificial agents (Dunlap           Chapter).
& Stephens, 2009). Based on the cost-benefit analysis we            Shultz, T. R., & Elman, J. L. (1994). Analyzing cross
presented in the Introduction, it might be possible to show           connected networks. In J. D. Cowan, G. Tesauro & J.
that abandonment of learning itself is favored by natural             Alspector (Eds.), Advances in Neural Information
selection in evolution simulations. And, of course, robotic           Processing Systems 6 (pp. 1117-1124). San Francisco,
applications pose a particularly challenging test of learning         CA: Morgan Kaufmannn.
autonomy.                                                           Shultz, T. R., & Fahlman, S. E. (2010). Cascade-
                                                                      Correlation. In C. Sammut & G. I. Webb (Eds.),
                   Acknowledgements                                   Encyclopedia of Machine Learning, Part 4/C (pp. 139-
                                                                      147). Heidelberg, Germany: Springer-Verlag.
This research was supported by the Natural Sciences and             Shultz, T. R., Mysore, S. P., & Quartz, S. R. (2007). Why
Engineering Research Council of Canada, with an operating             let networks grow? In D. Mareschal, S. Sirois, G.
grant to TRS and a fellowship to FD. Planning this work               Westermann         &     M.      H.      Johnson     (Eds.),
benefitted from discussions with LouAnn Gerken, Nick                  Neuroconstructivism: Perspectives and prospects (Vol. 2,
Chater, and Scott Fahlman. We are also grateful to Vincent            pp. 65-98). Oxford, UK: Oxford University Press.
Berthiaume for relevant pilot work, Simon Reader for                Shultz, T. R., Oshima-Takane, Y., & Takane, Y. (1995).
pointers to papers on evolution and learning, and Caitlin             Analysis of unstandardized contributions in cross
Mouri for helpful comments on an earlier draft.                       connected networks. In D. Touretzky, G. Tesauro & T. K.
                                                                      Leen (Eds.), Advances in Neural Information Processing
                         References                                   Systems 7 (pp. 601-608). Cambridge, MA: MIT Press.
Baluja, S., & Fahlman, S. E. (1994). Reducing network               Shultz, T. R., & Rivest, F. (2001). Knowledge-based
   depth in the cascade-correlation learning architecture. In.        cascade-correlation: Using knowledge to speed learning.
   Pittsburgh, PA: School of Computer Science, Carnegie               Connection Science, 13, 1-30.
   Mellon University.                                               Shultz, T. R., Rivest, F., Egri, L., Thivierge, J.-P., &
Douglas, R., & Sejnowski, T. (2007). Final workshop                   Dandurand, F. (2007). Could knowledge-based neural
   report: future challenges for the science and engineering          learning be useful in developmental robotics? The case of
   of learning, National Science Foundation (USA).                    KBCC. International Journal of Humanoid Robotics, 4,
   Washington, DC.                                                    245-279.
Dunlap, A. S., & Stephens, D. W. (2009). Components of
   change in the evolution of learning and unlearned
   preference. Proceedings of the Royal Society B-Biological
   Sciences, 276, 3201-3208.
                                                                2332

