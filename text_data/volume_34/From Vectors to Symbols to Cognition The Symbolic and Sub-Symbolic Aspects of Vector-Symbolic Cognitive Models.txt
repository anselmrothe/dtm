UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
From Vectors to Symbols to Cognition: The Symbolic and Sub-Symbolic Aspects of Vector-
Symbolic Cognitive Models
Permalink
https://escholarship.org/uc/item/0df6q3v5
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Kelly, Matthew
West, Robert
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                       From Vectors to Symbols to Cognition:
      The Symbolic and Sub-Symbolic Aspects of Vector-Symbolic Cognitive Models
                                      Matthew A. Kelly (mkelly11@connect.carleton.ca)
                                            Robert L. West (robert_west@carleton.ca)
                                           Institute of Cognitive Science, Carleton University
                                      1125 Colonel By Drive, Ottawa, Ontario, K1S 5B6 Canada
                              Abstract                                 attempt to lay out a simple system for understanding VSAs
                                                                       in terms of basic operations and symbolic/sub-symbolic
   To achieve a full, theoretical understanding of a cognitive
   process, explanations of the process need to be provided at         decisions, and thereby provide a comprehensive and
   both symbolic (i.e., representational) and sub-symbolic levels      comprehensible introduction to VSAs for newcomers, and a
   of description. We argue that cognitive models implemented          common frame of reference for those already using VSAs.
   in vector-symbolic architectures (VSAs) intrinsically operate       By providing a high-level overview that integrates the
   at both of levels and thus provide a needed bridge. We              techniques of existent VSA-based cognitive models into a
   characterize the sub-symbolic level of VSAs in terms of a           coherent picture we hope to highlight as yet unexplored
   small set of linear algebra operations. We characterize the         avenues of research and sketch what a VSA-based account
   symbolic level of VSAs in terms of cognitive processes, in          of cognition as a whole would look like.
   particular how information is represented, stored, and                 In this analysis, the vectors represent symbolic
   retrieved, and classify vector-symbolic cognitive models in
                                                                       information. These vectors, or symbols, can be combined
   the literature according to their implementation of these
   processes. On the basis of our analysis, we speculate on            and manipulated using a small number of operations, which
   avenues for future research, and suggest means for theoretical      can be understood as sub-symbolic processes. However, the
   unification of existent models.                                     information processing models built from these operations
                                                                       are themselves, best characterized at a symbolic level of
                                                                       description. Importantly, the modelling decisions made at
   Keywords: Vector symbolic architectures; Holographic
                                                                       the sub-symbolic level are to some degree independent of
   reduced representations; cognitive modelling; symbolic
   modelling; sub-symbolic modelling.                                  the modelling decisions made at the symbolic level. This
                                                                       paper is divided into two parts to reflect these two levels of
                                                                       description, symbolic and sub-symbolic.
                          Introduction
To achieve a full, theoretical understanding of a cognitive                           The Sub-Symbolic Level
process and how it relates to the physical world,
explanations of the process need to be provided at both                VSAs are closely related to the better-known tensor product
symbolic (i.e., representational) and sub-symbolic levels of           representations (Smolensky, 1990), but unlike tensor
description. The classic symbolic approaches to modelling              product representations, VSAs can compactly represent
do not account for how the symbol manipulations described              symbolic expressions of arbitrary complexity. A number of
in the model could arise from neural tissue, or account for            VSA techniques exist in the literature, including
how the symbols themselves come into existence. Classic                Holographic Reduced Representations (HRRs; Plate, 1995),
connectionist approaches are more concerned with neural                frequency-domain HRRs (Plate, 1994), some earlier forms
plausibility, but are notoriously opaque, doing little to aid          of holographic associative memory (Eich, 1982; Murdock,
our understanding of the cognitive processes modelled. By              1982), as well as binary spatter codes (Kanerva, 1992),
contrast, the vector-symbolic approach to modelling                    Multiply-Add-Permute coding (Gayler, 2003), and square
explicitly provides an account at both levels of description.          matrix representations (Kelly, 2010).
   Vector Symbolic Architectures (VSAs), a term coined by                 Each VSA technique uses the same set of basic
Gayler (2003; but see also Plate, 1995), are a set of                  operations, but implements the operations differently. Thus
techniques for instantiating and manipulating symbolic                 the choice of a particular VSA dictates how symbols are
structures in distributed representations. VSAs have been              instantiated and manipulated and defines the model at the
used to successfully model a number of different cognitive             sub-symbolic level. To ground the discussion, we mainly
processes (e.g., analogical mapping in Eliasmith & Thagard,            discuss Holographic Reduced Representations (HRRs)
2001; letter position coding in Hannagan, Dupoux, &                    (Plate, 1995), as HRRs are the most widely used VSAs in
Christophe, 2011; semantic memory in Jones & Mewhort,                  the cognitive modelling literature. Also, HRRs are used as
2007). It has been argued that VSAs provide a bridge                   the basis for the Neural Engineering Framework (NEF;
between conventional symbolic modelling and both                       Eliasmith, 2007) and thus demonstrably have a clearly
connectionist modelling (Rutledge-Taylor & West, 2008)                 defined and plausible neural implementation. However, the
and more realistic models of neural processing (Eliasmith,             other VSA techniques are similar and most anything that can
2007). However, if we are to take the bridging metaphor                be done with an HRR can be done with any VSA technique.
seriously, it is important to clarify which parts of a VSA are
symbolic in nature and which are sub-symbolic. We will
                                                                   1768

n-Space and Similarity                                               Superposition (+) versus Binding (*)
In a VSA, a symbol, or representation, is an n-dimensional           The key difference between superposition and binding is
vector: a list of n numbers that defines the coordinates of a        their effect on similarity. Superposition is similarity-
point in an n-dimensional space. VSAs work best for values           preserving: the sum of two vectors is a vector that falls in
of n in the hundreds or thousands (Plate, 1995).                     the angle between them. Conversely, binding is similarity
  A vector can be understood as a line drawn from the                destroying: the circular convolution of two vectors is
origin (the zero coordinates) to the coordinates specified by        roughly orthogonal to the two original vectors. The purpose
the vector. The length of the line is the vector's magnitude.        of superposition is to combine representations to create a
The direction of the vector encodes the meaning of the               new representation that is similar to all of the combined
representation. Similarity in meaning can thus be measured           representations. The purpose of binding, on the other hand,
by the size of the angles between vectors. This is typically         is to create "chunks": unique identifiers for combinations of
quantified as the cosine of the angle between vectors. The           representations.
cosine of vectors a and b can be calculated as:                         Most VSA use a form of vector addition for superposition.
                                                                     Vector addition is computed by adding together the
  cosine(a, b) = (a • b) / ( (a • a)0.5 (b • b)0.5 )                 corresponding elements of the two vectors. So, for example,
                                                                     {1,4,7} + {5,4,2} = {6,8,9}.
where • is the dot product. A cosine of 1 means the vectors             To bind, HRRs use circular convolution, * , which can be
are identical, -1 means they are opposites, and 0 means they         computed rapidly using element-wise multiplication, ◦ , and
are completely dissimilar. If each vector has a magnitude of         the fast Fourier transform, fft, and its inverse, fft -1:
one, the cosine is just the dot-product of the vectors. Thus,
some systems rescale all vectors to a magnitude of one after            a * b = fft -1( fft(a) ◦ fft(b) )
vector operations. In memory systems where new memories
are superimposed on old memories, such re-scaling causes a           Essentially circular convolution is a lossy way of
recency effect and rapid forgetting because new memories             scrambling the information of the two vectors together to
will make-up a fixed fraction of memory, regardless of the           produce a new vector of the same dimensionally.
quantity of previous experience.                                        Consider the problem of learning the meaning of the
  While the cosine measures the angle between two vectors,           phrase "kick the bucket", a colloquial euphemism for death.
the cosine is often described as a measure of distance. As it        Suppose the cognitive model has a vector representation of
is more intuitive to describe similarity as a measure of             the concept kick and a vector representation of the concept
distance than as a measure of the angle, for convenience, we         bucket. The sum (superposition) of those two vectors will
can imagine the vectors as describing points on a                    produce a vector that is close to both kick and bucket,
hypersphere, such that the size of the angles are the                indicating that the phrase “kick the bucket" has a meaning
distances between them.                                              similar to kick and to bucket. But in order for the cognitive
                                                                     model to be able to learn that the phrase "kick the bucket"
Atomic versus Complex representations                                has a distinct meaning that is not a function of its parts, the
Representations in a VSA are either atomic or complex. An            model needs to be able to assign to "kick the bucket" a
atomic representation is a unique representation, a symbol           distinct identifier. Binding is the operation that performs this
that cannot be broken down into sub-symbols. In an HRR,              function in VSA-based models. The vector kick * bucket is
values for an atomic representation are typically generated          dissimilar to the vectors kick, bucket, and kick + bucket.
by random sampling from a standard normal distribution.                 Binding and superposition can also be used jointly to
By assigning random values to the vectors, atomic                    address the binding problem (Gayler, 2003), that is, the
representations will be uniformly distributed across the             question of how to couple sets of attributes together such
surface of the hypersphere, such that the atomic                     that the attributes of one object are not confused with the
representations will have little to no similarity to each other.     attributes of another. For example, given a small red square
  Complex representations can be created by either                   and a large blue circle, the complex representation (small *
combining atomic representations or recursively combining            red * square) + (large * blue * circle) creates a single
complex representations. Critically, in a VSA, a complex             vector that distinctively represents the knowledge that the
representation has the same dimensionality as an atomic              square is small and red and the circle is large and blue.
representation, allowing representations both atomic and
complex to be compared, or combined together to create               Unbinding
representations of arbitrary complexity. VSAs have two               Unbinding is an inverse of binding that allows vectors that
operators for combining representations: superposition and           have been bound together in a complex representation to be
binding. In HRRs, superposition is vector addition and               unpacked and recovered. Circular correlation, #, is the
binding is circular convolution. We denote vector addition           unbinding operator for HRRs. Given a pair of vectors bound
by +, and circular convolution by *. Binding and                     together, and one of the pair, referred to as the probe,
superposition, along with random permutation, are the basic          unbinding produces an approximation of the other vector,
operations used to create complex representations in VSAs.           referred to as the target, i.e.,
                                                                        p # (p * t) = a ≈ t
                                                                 1769

                                                                                     The Symbolic Level
where p is the probe, t is the target, and a is an                 When making a vector-symbolic model, decisions need to
approximation of the target.                                       be made at both the symbolic and sub-symbolic levels. At
   Unbinding can be understood as binding with the inverse         the sub-symbolic level, the modeller needs to decide how to
of the probe. The inverse of any vector x is a re-ordering of      instantiate symbols as vectors and symbol-manipulation as
the elements of x, i.e. a permutation of x, such that,             vector algebra. Conversely, at the symbolic level, the
                                                                   modeller needs to make decisions about how to structure,
   x # x = x * inverse(x) ≈ δ                                      manage, store, and retrieve those symbols. Choosing to use
                                                                   HRRs rather than another kind of VSA can define the sub-
where δ is the identity vector for binding, i.e. for any vector    symbolic level, but this choice is largely independent of the
x, x * δ = x. Thus binding with the inverse of a vector            decisions to be made at the symbolic level.
unbinds what that vector has been associated with:                     In fact, we have already seen two examples of
                                                                   manipulations at the symbolic level. The first was
   a # (a * b) = inverse(a) * (a * b) ≈ δ * b = b                  combining binding and addition to create a vector that
                                                                   encodes information about bound entities (e.g., small red
In HRRs, the inverse of any vector x = {x1 ... xn} is:             square and large blue circle). The second was combining
                                                                   permutation and binding to create a bound entity that
   inverse(x) = {x1, xn, xn-1, ... x3, x2}                         maintained information about order. Essentially, all VSA
                                                                   systems work in the same way. Vectors encode the desired
   Circular convolution, *, is commutative, i.e., the order of     information according to some sort of scheme (i.e., by
binding does not matter when using circular convolution.           combining the operations discussed above), and then, when
Given vectors a and b, their association a * b = b * a, and        needed, the information is retrieved from the vectors.
likewise, when unbinding, b # (a * b) = b # (b * a) ≈ a.
                                                                   Encoding and Storage
Permutation
                                                                   BEAGLE (Jones & Mewhort, 2007) and DSHM (Rutledge-
Gayler (2003) describes random permutation as an operation         Taylor & West, 2008) use the terms environmental vectors
used "to quote or protect the vectors from the other               and memory vectors. We extend the use of this terminology
operations". Permutation of the numbers 1 ... n defines a          to other vector-symbolic models. An environmental vector is
unary function that can transform a vector. A randomly             a vector that stands for atomic perceptions from the
chosen permutation of a vector is unlikely to be similar to        environment (e.g., a red circle needs two environmental
the original vector, but the permutation is also reversible.       vectors, one for circle and one for red). Environmental
Given p, there is a permutation p-1 such that, p-1(p(a)) = a.      vectors are fixed and do not change. A memory vector is a
When permuted, the information within a vector is                  complex representation stored by the model and used to
essentially hidden and protected from being affected by            produce behaviour. In some systems, memory vectors
other vector operations.                                           change with experience. Additionally, we use the term
   For example, as noted above, circular convolution is            experience vector to refer to a representation that stands for
commutative, that is, a*b = b*a. This property of circular         the model's current experience of its environment created by
convolution can be useful, but it can be a hindrance in            combining environmental vectors (e.g., an experience vector
situations where the order of items matter, e.g. "dog feed"        could represent the perception of a red circle by convolving
and "feed dog" are phrases which carry different meanings          the environmental vectors of circle and red).
by virtue of differences in word order.                              By examining the relationship between environmental,
   A non-commutative variant of circular convolution can be        experience, and memory vectors across vector-symbolic
defined using a random permutation p and its inverse p-1. By       models, we distinguish between three main approaches to
always randomly permuting one of the arguments before              storage. In a many-to-one vector model, all experience
convolution, one defines a binding operation that is non-          vectors are summed into a single memory vector for storage.
commutative, i.e. while a*b = b*a, p(a)*b ≠ p(b)*a.                In a one-to-one vector model, each experience vector is
Unbinding then uses the inverse permutation p-1, e.g.              stored as a separate memory vector among an ever-growing
                                                                   number of memory vectors. In a many-to-many vector
   cosine( p-1(a # (a * p(b))), b) ≈ 0.71                          model, there are a fixed number of memory vectors, and
   cosine( p-1(b # (a * p(b))), a) ≈ 0                             incoming environmental vectors are used to update them.
                                                                   Each of these approaches has strengths and weaknesses.
Non-commutative binding is used by the BEAGLE model
(Jones & Mewhort, 2007) to bind vectors that stand for             Many-to-one In a many-to-one vector model, such as
words in sentences in order to construct representations of        TODAM (Murdock, 1983) or CHARM (Eich, 1982),
the semantics of each of those words. For a variety of other       memory is modelled as a single, high-dimensional vector.
uses of random permutation in VSAs, see Gayler (2003),             All experience vectors are added to the memory vector.
Sahlgren, Holst, and Kanerva (2008), and Kelly (2010).             There is a limit to how much can be stored in the vector
                                                                   before mistakes start to be made. Mistakes are, of course, of
                                                                   interest to psychologists, and the pattern of mistakes made
                                                               1770

by a many-to-one vector model allow it to mimic human           environmental vector for keyboard and move the memory
forgetting in list-recall tasks. If the goal is to model how    vector for keyboard closer to the environmental vector for
people store a small amount of recently learned or closely      computer. Over time, the result of this is to organize the
related information, a single memory vector suffices.           space so that memory vectors are clustered around
  Many-to-one vector models also have the advantage of a        environmental vectors that they co-occur with so that the
clear neural implementation. In the Neural Engineering          distance between the vectors equals strength of association.
Framework (NEF; Eliasmith, 2007) binding, unbinding, and           Another, more complicated example involves binding and
and superposition can all be implemented through neural         the use of the placeholder vector. The placeholder vector is
connectivity. In the NEF interpretation, a many-to-one          an atomic (i.e, random) vector, but it is used to encode all
memory is a neural group with self-recurrent connections        associations, and thus can be used as a universal retrieval
that acts as a working memory or buffer, and many such          cue. Consider the phrase or stimulus blue triangle. Without
buffers could exist in the brain.                               using the placeholder, we could update memory as follows:
One-to-one In a one-to-one vector model, such as                   memoryblue += blue * triangle
MINERVA (Hintzman, 1986), the Iterative Resonance                  memorytriangle += blue * triangle
Model (Mewhort & Johns, 2005), and the Holographic
Exemplar Model (Jamieson & Mewhort, 2011), each                 By binding together the environmental vectors for blue and
experience vector is represented as a separate memory.          triangle and adding the result to the memory vectors for
While this approach to modelling memory is both simple          blue and triangle (an operation denoted by +=), we move
and successful, the ever growing number of vectors that         the two memory vectors towards the point in space
need to be stored and accessed by the memory system is          described by the vector blue * triangle, and thereby move
both neurally implausible and computationally impractical       memoryblue and memorytriangle closer together. But people
for modelling tasks in which very large amounts of              almost never get the concepts blue and triangle confused
knowledge are relevant, e.g., semantic priming tasks (Jones     with each other. This is because blue is a colour (or an
& Mewhort, 2007). However, these models are able to             adjective), and triangle is a shape (or a noun), i.e. they are
reproduce a wide variety of memory effects, providing a         different sorts of thing.
unitary account of episodic, semantic, and implicit memory,        Conversely, consider updating using the placeholder:
indicating that, although their warehouse-style management
of vectors is implausible, their processes of storage and          memoryblue += placeholder * triangle
retrieval provide a good analogue for biological memory.           memorytriangle += blue * placeholder
Many-to-many Many-to-many vector models, such as                This moves memoryblue towards placeholder * triangle,
BEAGLE (Jones & Mewhort, 2007) and DSHM (Rutledge-              i.e. towards all properties of triangles, and moves
Taylor, 2008), can be understood as a hybrid of the earlier     memorytriangle towards blue * placeholder, i.e. towards all
many-to-one and one-to-one approaches. In many-to-many          things that are blue. Thus, by using a placeholder, the
memory, for each item of interest, there is a randomly          memory vectors for nouns will cluster together in one region
generated environmental vector and a specially constructed      of space, and the vectors for adjectives will cluster together
memory vector. In BEAGLE the items of interest are words:       in another region of space, and things that are colours will
the environmental vector stands for the word's orthography      cluster separately from things that are coloured. This is a
or phonology and the memory vector stands for the word's        subtle distinction but Jones and Mewhort (2007) have
meaning. In DSHM, the items are objects relevant to the         shown it to be very important and very powerful.
experimental task: the environmental vector stands for the
percept of the object and the memory vector stands for the      Retrieval
concept of the object.                                          There are two categories of information retrieval processes
  Like the one-to-one models, the management of the             used in vector-symbolic models: unbinding, which retrieves
vectors in many-to-many systems is computationally              information from a particular vector, and resonance, which
expensive and, at this point, neurally implausible. However,    allows information to be retrieved from the entire library of
the ability to generate memory vectors that stand for           vectors in memory. Many-to-one models, such as TODAM
particular concepts in very powerful (e.g., Rutledge-Taylor,    (Murdock, 1982) only use unbinding. One-to-one models
Vellino, & West, 2008) and allows these systems to capture      that do not use binding to encode associations, such as
numerous different phenomena (e.g., Rutledge-Taylor &           MINERVA (Hintzman, 1986), only use resonance. In many-
West, 2008) and represent vast quantities of data (Jones &      to-many systems, these two retrieval processes are
Mewhort, 2007).                                                 complementary. For example, resonance can be used to
  For example, to create an association between keyboards       retrieve a vector, which can then be unbound.
and computers, each time a computer and keyboard co-
occur a copy of the environmental vector for keyboard can       Unbinding Consider a simple example where the, agent is
be added to the memory vector for computer and a copy of        given a set of coloured shapes to remember: blue triangle,
the environmental vector for computer can be added to the       green square, red circle. In a many-to-one vector model this
memory vector for keyboard. The effect to this would be to      could be encoded by binding (*) the vectors for the shapes
move the memory vector for computer closer to the               to the colours, then summing to create a memory vector:
                                                            1771

                                                                   sum of knowledge than what is typically necessary to model
   memory = blue*triangle + green*square + red*circle              a psychology experiment (e.g., in modelling word
                                                                   pronunciation, such as in Kwantes & Mewhort, 1999).
The colour of any one of these shapes could then be recalled          In the Iterative Resonance Model (IRM; Mewhort &
by unbinding (#) using the shape to probe memory:                  Johns, 2005), resonance is iterated, and with each iteration b
                                                                   is increased until a decision to stop iterating is made,
   triangle # memory ≈ blue                                        resulting in either successful retrieval or a failure to retrieve.
                                                                   This approach has two benefits: (1) the number of iterations
In a many-to-many vector model, unbinding may use the              can be used to predict response time in memory tasks, and
placeholder as the probe. The placeholder is a special,            (2) it eliminates b as a tweaking parameter by introducing a
randomly generated atomic vector that acts as a key to all of      theory-driven approach to setting its value.
memory. The placeholder is initially used in binding:
                                                                   Resonance (many-to-many) Although the term resonance is
   memoryblue = placeholder * triangle                             used to describe retrieval in many-to-many vector models,
   memorytriangle = placeholder * blue                             the implementation is different and simpler: Essentially, the
                                                                   memory vector most similar to the probe is retrieved. This
The placeholder can then be used in unbinding:                     can be understood as a kind of spreading activation
                                                                   (Rutledge-Taylor & West, 2008). The probe and memory
   placeholder # memorytriangle ≈ blue                             vectors can be understood as points on a hypersphere, such
                                                                   that the cosine measures the distance between them. One
Resonance (one-to-one) The term resonance comes from               can imagine a ripple of activation spreading out from the
MINERVA (Hintzman, 1986), but it is implemented                    probe across the surface of the hypersphere. The memory
differently across different models. In MINERVA, the               vectors closest to the probe become active in working
process of resonance begins by measuring the similarity            memory, with the closer vectors becoming active sooner.
(cosine) of each vector in memory to the probe. Then               This model of resonance allows BEAGLE (Jones &
resonance computes a weighted sum of all vectors in                Mewhort, 2007) to make semantic priming reaction time
memory. This sum, termed the echo, is what the model               predictions (e.g., that doctor is recognized faster when
retrieves from memory. Each vector in the sum is weighted          preceded by nurse than when preceded by an unrelated
by its similarity to the probe raised to an exponent.              prime such as stapler) and to model the fan-effect in DSHM
   For example, the three shapes might be represented as:          (Rutledge-Taylor & West, 2008).
   memory1 = triangle + blue                                                                Conclusions
   memory2 = square + green
                                                                   We hold that, in order to bridge the gap between human
   memory3 = circle + red
                                                                   experience and neural connectivity, explanations at both the
                                                                   symbolic and sub-symbolic levels of description are
If the probe is triangle, then the echo would approximate:
                                                                   necessary parts of theory in cognitive science. As we
                                                                   illustrate in this paper, cognitive models that use vector-
   echo ≈ 0.5bmemory1 + 0.0bmemory2 + 0.0bmemory3
                                                                   symbolic architectures intrinsically operate at both of these
   echo ≈ 0.5b(triangle + blue)
                                                                   levels of description and thereby provide a needed bridge
                                                                   between the two kinds of explanation.
such that the memory system would remember that the
                                                                      At the sub-symbolic level is the vector-symbolic
triangle is blue. The exponent b is a small, positive integer
                                                                   architecture itself, and the linear algebra operations on
that is odd-numbered so as to preserve the sign of the
                                                                   vectors that comprise the architecture: similarity,
similarity. Note that the similarity values of 0.5 and 0.0 are
                                                                   superposition, binding, unbinding, permuting, un-
approximate. Random vectors in a high dimensional space
                                                                   permuting. All of these operations are easily amenable to
have an expected cosine of 0, but the actual cosine between
                                                                   neural implementation, as in the NEF (Eliasmith, 2007).
any two random vectors will be a little more or a little less.
                                                                      At the symbolic level, we have the cognitive model itself,
   The exponent b critically allows one-to-one vector models
                                                                   and the cognitive processes that define it. On the basis of
to function even when there is a very large amount of data
                                                                   their storage and retrieval mechanisms, we classify existing
in memory. If the exponent b is 1, the result of resonance
                                                                   vector-symbolic cognitive models into many-to-one, one-to-
roughly imitates decoding in a simple associative memory,
                                                                   one, and many-to-many vector models. This classification
such as a Hopfield network. With an exponent greater than
                                                                   scheme highlights stark differences between these models.
one, resonance increases the signal to noise ratio in the echo
                                                                      Many-to-many vector models differ from the other two
by increasing the relative weighting of the memory vectors
                                                                   classes of model in two important ways. First, many-to-
most similar to the probe. If b is too low, a large number of
                                                                   many models use a placeholder vector to stand for "this item
partial matches in memory could easily overwhelm an exact
                                                                   I am thinking about". The placeholder acts as a symbol with
match to the probe, resulting in a poor echo. With a high b,
                                                                   an important functional role but no perceptual or conceptual
the echo will essentially just be the most similar vector in
                                                                   meaning. It may be useful to incorporate other kinds of
memory to the probe. In MINERVA, a b of 3 is standardly
                                                                   function vectors in future models, e.g., a wildcard vector to
used, but a b of 3 may be too low when modelling a larger
                                                               1772

stand for "that item that I'm not thinking about", vectors to     Eliasmith, C. (2007). How to build a brain: From function to
stand for emotional states, for truth values, et cetera.            implementation. Synthese, 159, 373-388.
   Secondly, in many-to-many models, memory vectors are           Gayler, R. (2003). Vector symbolic architectures answer
labelled and stand for particular concepts, whereas in one-         Jackendoff’s challenges for cognitive neuroscience. ICCS/
to-one models concepts are an emergent phenomenon                   ASCS International Conference on Cognitive Science.
produced by the echoes retrieved using resonance                  Hannagan, T., Dupoux, E., & Christophe, A. (2011).
(Hintzman, 1986). While the conceptual representations in           Holographic string encoding. Cognitive Science, 35,
many-to-many models are powerful, having a predefined               79-118.
number of concepts is implausible and limiting.                   Hintzman, D. L. (1986). "Schema abstraction" in multiple-
   Many-to-one vector models can be constructed in NEF as           trace memory models. Psychological Review, 93,
self-recurrent neural groups and are understood as working          441-428.
memory buffers. By contrast, one-to-one and many-to-many          Jamieson, R. K., & Mewhort, D. J. K. (2011).
models are best understood as models of long-term memory,           Grammaticality is inferred from global similarity: A reply
but as yet lack a neural explanation.                               to kinder (2010). The Quarterly Journal of Experimental
   Finding a means of translating one-to-one and many-to-           Psychology, 64, 209-216.
many vector models into neural models may provide a route         Jones, M. N., & Mewhort, D. J. K. (2007). Representing
to a unified, vector-symbolic account of memory storage             word meaning and order information in a composite
and retrieval. As we noted earlier, a one-to-one model              holographic lexicon. Psychological Review, 114, 1-37.
behaves somewhat like a Hopfield network when the                 Kelly, M. A. (2010). Advancing the theory and utility of
resonance exponent b is set to 1. To implement a one-to-one         holographic reduced representations. (Master’s thesis,
model as a network, one needs to find a mechanism                   School of Computing, Queen’s University).
analogous to b that can act to increase the signal to noise       Kanerva, P. (1996). Binary spatter-coding of ordered k-
ratio in the echo. We speculate that the vector-symbolic            tuples. Proceedings of the 1996 International Conference
intersection circuit proposed by Levy and Gayler (2009)             on Artificial Neural Networks, 869-873.
might provide a start for developing such a mechanism.            Kwantes, P. J., & Mewhort, D. J. K. (1999). Modeling
   We suspect that the memory vectors that stand for                lexical decision and word naming as a retrieval process.
concepts in many-to-many vector models are, in fact, the            Canadian Journal of Experimental Psychology, 53,
echoes in one-to-one vector models. That is to say, we agree        306-315.
with Hintzman (1986) that concepts are an emergent                Levy, S.D., & Gayler, R.W. (2009). A distributed basis for
property of retrieval. Using a one-to-one model to do the           analogical mapping. New frontiers in analogy research;
kind of large scale modelling in many-to-many models is             Proceedings of the Second International Analogy
impossible because one-to-one models store all experiences          Conference - Analogy 09, 165-174.
without any form of compression. However, a neural                Mewhort, D. J. K., & Johns, E. E. (2005). Sharpening the
implementation of a one-to-one model would naturally be             echo: An iterative-resonance model for short-term
lossy in its storage, and so could provide a plausible account      recognition memory. Memory, 13, 300-307.
of concept formation over a lifetime of experiences.              Murdock, B. B. (1982). A theory for the storage and
   Unification in other areas, such as representation, is           retrieval of item and associative information.
important too. Incorporating a vector-symbolic model of             Psychological Review, 89, 609–626.
string encoding (Hannagan et al., 2011) into the BEAGLE           Plate, T. A. (1994). Distributed representations and nested
model of semantics (Jones & Mewhort, 2007) could, for               compositional structure. (Doctoral dissertation,
instance, allow BEAGLE to model how shared orthography              Department of Computer Science, University of Toronto).
can help and hinder in understanding the meaning of words.        Plate, T. A. (1995). Holographic reduced representations.
   Eventually, we hope to see developed a vector-symbolic           IEEE Transactions on Neural Networks, 6, 623– 641.
cognitive architecture, which not only presents a unified and     Rutledge-Taylor, M. F., Vellino, A., & West, R. L. (2008). A
neurally plausible approach to representation, storage, and         holographic associative memory recommender system,
retrieval, but also extends the vector-symbolic account             Proceedings of the Third International Conference on
beyond its roots in memory theory, and integrating it into          Digital Information Management, 87-92.
accounts of emotions, attention, perception, and                  Ruteldge-Taylor, M. F. & West R. L. (2008). Modeling the
consciousness. As cognitive scientists, it is important to          fan-effect using dynamically structured holographic
keep in mind our ultimate, lofty, and collective goal of a          memory. Proceedings of the 30th Annual Conference of
theory that unifies not only all aspects of the cognition, but      the Cognitive Science Society, 385-390.
all relevant levels of description.                               Sahlgren, M., Holst, A., & Kanerva, P. (2008). Permutations
                                                                    as a means to encode order in word space. Proceedings of
                         References                                 the 30th Annual Conference of the Cognitive Science
                                                                    Society, 1300-1305.
Eich, J. M. (1982). A composite holographic associative           Smolensky, P. (1990). Tensor product variable binding and
   recall model. Psychological Review, 89, 627–661.                 the representation of symbolic structures in connectionist
Eliasmith, C., & Thagard, P. (2001). Integrating structure          systems. Artificial Intelligence, 46, 159-216.
   and meaning: a distributed model of analogical mapping.
   Cognitive Science, 25, 245-286.
                                                              1773

