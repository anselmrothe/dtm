UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Learning of Relational Abstractions via Structural Alignment
Permalink
https://escholarship.org/uc/item/6w60k2ns
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Kandaswamy, Subu
Forbus, Kenneth
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

           Modeling Learning of Relational Abstractions via Structural Alignment
                                       Subu Kandaswamy (subu@u.northwestern.edu)
                                       Kenneth D. Forbus (forbus@northwestern.edu)
                            Qualitative Reasoning Group, Northwestern University, 2133 Sheridan Road
                                                       Evanston, IL 60201 USA
                             Abstract                                whether or not simple exposure to more examples was
                                                                     sufficient to promote learning. They found significant
   Learning abstract relationships is an essential capability in
   human intelligence. Christie & Gentner (2010) argued that         differences between Sequential and Comparison, and
   comparison plays a crucial role in such learning. Structural      between Solo and Comparison, but the difference between
   alignment highlights the shared relational structure between      Sequential and Solo were not significant. This provides
   compared examples, thereby making it more salient and             additional evidence that it is comparison specifically that is
   accessible for subsequent use. They showed that 3-4 year old      promoting learning.
   children who compared examples in a word-extension task
   showed higher sensitivity to relational structure. This paper
   shows how a slight extension to an existing analogical model
   of word learning (Lockwood et al 2008) can be used to
   simulate their experiments. This provides another source of
   evidence for comparison as a mechanism for learning
   relational abstractions.
                          Introduction
Our ability to abstract and reason with relations between
objects is an essential part of our intelligence. As children,
we acquire a variety of relations, including spatial relations
such as above, and on, and functional relations like edible
and dangerous. How children acquire and use such
relational abstractions is an important question in cognitive
development. Gentner (2003) has argued that comparison
promotes learning new relational abstractions. The idea is
that structural alignment highlights common structure,
which becomes more salient and available for subsequent
use.
   One line of evidence for this theory comes from an
experiment by Christie & Gentner (2010). To show that
children (ages 3-4) were learning new relations, they used
novel spatial relational categories in a word extension task,
as illustrated in Figure 1. Here the relationship might be               Figure 1: Example of stimulus set from Christie & Gentner
characterized as “An animal above another identical                                               (2010)
animal”. In the Solo condition, children were shown a
single standard (here, Standard 1) and told it was a novel
                                                                        This paper shows that this phenomena falls out of
noun (e.g. “Look, this is a jiggy! Can you say jiggy?”). In
                                                                     computational models of analogical generalization already
the Comparison condition, children were invited to compare
                                                                     proposed for word learning. We start by summarizing the
two examples (e.g. “Can you see why these are both
                                                                     necessary background, including the models of analogical
jiggies?” when presenting Standard 1 and Standard 2
                                                                     matching and generalization used and how we use sketch
simultaneously). In both conditions, children were then
                                                                     understanding to reduce tailorability by producing portions
presented with a forced-choice task, where they had to
                                                                     of the input representations automatically. Next we describe
choose which one of the alternatives is a jiggy (e.g. “Which
                                                                     an extension to a similarity-based word learning model
one of these is a jiggy?” when presented with the relational
                                                                     (Lockwood et al 2008) that enables it to model this task.
match and object match cards). Children in the Solo
                                                                     Then we describe two simulation experiments that
condition preferred the object match, while those in the
                                                                     demonstrate that this model is capable of exhibiting
Comparison condition chose relational matches twice as
                                                                     behavior consistent with that described in Christie &
often as object matches. This provides evidence that
                                                                     Gentner (2010), including sensitivity analyses to shed light
comparison can lead to learning new relational abstractions.
                                                                     on why it does so. After discussing related work, we close
In a second experiment, a third condition, Sequential, was
                                                                     with a discussion of future work.
added, where children saw two standards serially, to test
                                                                 545

                        Background                                  compute qualitative relationships from digital ink. For
                                                                    example,        it    automatically      computes        topological
Our model is based on Gentner’s (1983) structure-mapping
                                                                    relationships (e.g. inside, touching) and relative positions
theory of analogy and similarity. In structure-mapping,
                                                                    (e.g. above, right of) for the entities in a sketch. It also
comparison involves a base and target, both structured,
                                                                    includes a model of mental rotation, which uses SME to
relational representations. Comparison results in one or
                                                                    first do a qualitative shape match which then guides a
more mappings, which contain a set of correspondences that
                                                                    quantitative match (Lovett et al 2009). This enables it to
describe how the elements in the structured representations
                                                                    compute          relationships     such        as      sameShape,
align, a score that indicates the overall structural quality of
the match, and possibly candidate inferences representing           reflectedOnXAxis, and so on. Conceptual information
knowledge that could be projected from base to target (as           can be introduced by adding attribute information to entities
well as from target to base). Our computational model of            in the sketch. For example, the top entities in Standard 1 of
comparison is the Structure-Mapping Engine, SME                     Figure 1 might be described as identical elephants, one
(Falkenhainer et al 1989; Forbus et al 1994). Here SME is           positioned above the other. The attributes are derived from
used both as a component in our model of analogical                 a large, independently-developed knowledge base 1. The
generalization (described below) and in making the decision         relationships automatically computed by CogSketch, along
in the forced-choice task. The score is normalized to be            with the conceptual attributes provided for an entity, provide
between zero and one, by dividing it by the score obtained          the inputs for our simulation. Moreover, CogSketch
for the maximum self-mapping of base and target.                    provides a mechanism for dividing a sketch into
   Analogical generalization is modeled via SAGE                    subsketches, which is what we use to combine all of the
(Sequential Analogical Generalization Engine), an                   elements of a stimulus set onto the same sketch, for
extension of SEQL (Kuehne et al 2000) which incorporates            convenience. Figure 2 provides an example of a sketched
probabilities and analogical retrieval. Information about           stimulus set.
concepts is stored in generalization contexts (Friedman &
Forbus, 2008). Each generalization context maintains a set
of examples of that concept, plus generalizations concerning
it. Examples are provided incrementally. For each new
example, the most similar prior examples and
generalizations are retrieved via a model of analogical
reminding (MAC/FAC, Forbus et al 1995). The retrieved
items are compared, via SME, with the new example. For
each comparison, if the score of the best mapping is over a
threshold (the assimilation threshold), the compared items
are assimilated into a generalization – a new one in the case
of two examples, or an updated version of the existing
generalization in the case of an example and a
generalization. The assimilation process keeps the common
structure of the mapping, replacing non-identical entities
with abstract place-holders. Associated with each fact in
generalizations is a probability, based on the number of
times a statement aligning with it appears in an example
(Halstead & Forbus, 2005).             For example, in a
generalization about swans, the fact that swans are birds
might have a probability of 1.0, while the probability that
their color is white might be 0.999 while the probability that
their color is black might be 0.001. Non-overlapping facts
                                                                         Figure 2: The sketched version of the stimulus set of Figure 1
are kept, albeit given a low probability (i.e., 1/N, where N is
the number of examples assimilated into that
generalization). Facts whose probability drops below the
probability cutoff are removed from the generalization.
SAGE is the central component in our word-learning model,
as explained below.
   Tailorability is an important problem in cognitive
simulation. To reduce tailorability, we use automatically
constructed visual and spatial representations.          These
representations are computed by CogSketch (Forbus et al
2011), an open-domain sketch understanding system.
CogSketch uses models of visual and spatial processing to              1
                                                                         OpenCyc, see www.opencyc.org
                                                                546

Word Learning via Analogical Generalization                        The original experiment used 8 stimulus sets. We encoded
                                                                   8 sketches of animals, using CogSketch. Each element of
We model the learning of words as follows. For each word,
                                                                   the stimulus set (e.g. Standard 1, Standard 2, etc.) was
there is a generalization context. Every time the word is
                                                                   drawn as a separate subsketch. CogSketch’s default
used, an appropriate subset of the world is encoded to
                                                                   encoding methods were used, plus an additional query to
capture information about what that word denotes, and is
                                                                   ascertain if any of the entities in a subsketch had the same
provided to the generalization context for that word as an
                                                                   shape as any other, and if so, what transformation held
example. The generalizations constructed by SAGE can be
                                                                   between them (where no transformation implies the
considered as the meanings for the words. Note that such
meanings can be probabilistic, since SAGE computes                 relationship sameShape). Moreover, filters were used to
frequency information for every statement in the                   automatically remove three types of information: Redundant
generalization. The ability to track multiple generalizations      information (e.g. given (rightOf B A), (leftOf A B)
provides a mechanism for handling multiple senses of a             is redundant), irrelevant information (e.g., global estimates
word. The ability to store unassimilated examples provides         of glyph size like MediumSizeGlyph), and bookkeeping
a means of handling edge cases, and helps provide noise            information (e.g. relationships describing timestamps). The
immunity in the face of changes in the underlying                  table below shows the final encoding for the sketches
distribution of examples of a concept.                             stimulus set (Figure 2) and the resultant generalization.
   This account has been used to successfully model spatial
propositions of contact in English and in Dutch (Lockwood                       Table 1: Encoding for the sample sketch.
et al 2008). It makes no commitment to the particulars of
encoding, because this is a complex issue, especially since                  Standard-1                Standard-2
evidence from studies of novice/expert differences suggest         (sameShapes Object-99               (sameShapes Object-104
that encoding strategies evolve with learning (Chi et al                          Object-420)                       Object-425)
                                                                   (above Object-99                    (above Object-104
1981). When using CogSketch as a source of stimuli, we use                  Object-420)                        Object-425)
an entire subsketch as the relevant material to encode.            (isa Object-420 Elephant)           (isa Object-425 Dog)
                                                                   (isa Object-99 Elephant)            (isa Object-104 Dog)
                Simulation Experiments                                                Generalization for “jiggy”
Now let us see how this model can be used to simulate the
                                                                   (above (GenEntFn 1 0 jiggy) (GenEntFn 0 0 jiggy))
experiments of (Christie & Gentner, 2010). We begin with           (sameShapes (GenEntFn 1 0 jiggy) (GenEntFn 0 0 jiggy)
their Experiment 1.
Simulation Experiment 1                                               An interesting open parameter concerns the number of
Recall that Experiment 1 used two conditions to show               conceptual attributes that children might be encoding.
children the new concept, followed by a forced-choice task.        While we suspect that a large number of attributes would be
We model these as follows:                                         encoded2, we do not know of data that provides specific
                                                                   estimates. Consequently, we perform a sensitivity analysis
      Forced-choice task: Each of the choices is used with        by running the simulation while varying the number of
       the generalization context for the word to retrieve the     conceptual attributes to ascertain their impact on the results.
       most similar generalization or example for that             Specifically, we varied the number of attributes from zero to
       choice. The choice whose similarity score is highest        nine. We assumed that encoding is reasonably uniform, i.e.
       constitutes the decision. For simplicity, We start with     that the same attributes would always be computed for
       an empty generalization context for every novel word        identical objects. For simplicity, we further assumed that
       used.                                                       the set of attributes computed for one entity had no overlap
      Solo Condition: The single example is added to the          with the set of attributes computed for another entity whose
       generalization context for the word.                        shape is different. Given these assumptions, we used
      Comparison Condition: The two examples are added            synthetic       attributes       (e.g.     Uniquestandard-
       to the generalization context, but since the                1MtAttribute8) for convenience.
       experimenter has asserted that they are both examples          Figure 3 shows the results. From the data, we can see that
       of the concept, we assume that the child is more            the model chose the relational match 100% of the time for
       likely to assimilate them into a generalization, which      the Comparison condition. This is qualitatively consistent
       is modeled by lowering the assimilation threshold           with the behavior of participants in the Comparison
       from its default of 0.8 to 0.1. We also assume that the     condition, where participants chose the relational match
       probability cutoff is 0.6, so that facts which do not       around 60% of the time. We believe that the lack of object
       appear in the shared structure will be eliminated from      matches in this simulation condition are due to the use of
       the generalization.                                         completely independent attributes for each entity type in the
                                                                      2
                                                                        See the Specificity Conjecture (Forbus & Gentner 1989).
                                                               547

stimuli sets. Since they are independent, no attributes are                                                     model, fillers would be added to some other generalization
left in the generalization after assimilation. The more                                                         context, thus 2a and 2b look identical from the perspective
overlapping attributes there are, the more likely an object                                                     of our model. We model the new condition as follows:
match is to become possible.
   Returning to Figure 3, in the Solo condition, as the                                                              Sequential Condition: The two examples are added to
number of attributes rises, the proportion of object matches                                                          the generalization context, but with the default
rises (i.e., the proportion of relational matches falls). Again,                                                      assimilation threshold 0.8.
this provides a good qualitative fit for the results of (Christie
& Gentner 2010) Experiment 1. Since attributes are more                                                         Again we varied the number of conceptual attributes, in the
salient to children, due to lack of relevant domain                                                             same way as in Simulation Experiment 1.
knowledge (Ratterman & Gentner, 1998), it is reasonable to
                                                                                                                                                         1
assume that they would encode more attributes than
                                                                                                                      Prop. Relational Match Selected
                                                                                                                                                        0.9
relations, which is compatible with the simulation results.                                                                                             0.8
   Recall that we assume that the probability cutoff is set                                                                                             0.7
high enough that non-overlapping information is                                                                                                         0.6
immediately filtered out. (Since these are novel concepts,                                                                                              0.5
                                                                                                                                                                                                          Sequential
                                                                                                                                                        0.4
there can be at most two examples in any generalization,                                                                                                0.3                                               Solo
and hence the probability of any fact not in the overlap                                                                                                0.2
would be 0.5, which is less than the 0.6 threshold.) Would                                                                                              0.1
adding in probabilistic information improve the fit of the                                                                                               0
                                                                                                                                                              0   1   2    3   4    5   6     7   8   9
model to human data? To determine this, we tried changing
                                                                                                                                                                          Object Attributes
the probability cutoff to its usual default of 0.2. This leads
to all attributes remaining in the generalization, which
results in the score for the object match being boosted so                                                                                    Figure 4: Results of Simulation Experiment 2
high that it always wins over the relational match, regardless
of the experimental condition used. This suggests that when                                                       Figure 4 illustrates the results. As anticipated, the results
children are invited to compare, they do indeed restrict                                                        for the Sequential condition are identical to the results the
themselves to keeping exactly the overlapping structure.                                                        model generates for the Solo condition. This is because of
                                                                                                                the model does not generalize the two standards, and hence
                                                                                                                the choices will be compared to the exemplars in the
                                            1
                                                                                                                generalization context. This makes the results of the
         Prop. Relational Match Selected
                                           0.9
                                           0.8
                                           0.7
                                           0.6
                                           0.5
                                                                                             Comparison
                                           0.4
                                           0.3                                               Solo
                                           0.2
                                           0.1
                                            0
                                                 0   1   2    3   4    5   6     7   8   9
                                                             Object Attributes
                        Figure 3: Results for Simulation Experiment 1
Simulation Experiment 2
Experiment 2 in (Christie & Gentner 2010) actually consists                                                                                               Figure 5: Sensitivity analysis
of two experiments. Both involved a new condition, the
Sequential condition, designed to rule out non-comparison                                                       Comparison condition be the same as the Solo condition.
explanations. In Experiment 2a, fillers, in the form of                                                           We know of no psychological evidence that would
pictures of familiar objects, were interposed between the                                                       provide constraints on the value of the assimilation
serial presentation of the standards. No invitation to                                                          threshold.    Consequently, we performed a sensitivity
compare was issued. In Experiment 2b, no fillers were                                                           analysis by varying the assimilation threshold between 0.1
used, and the Solo and Comparison conditions from                                                               and 0.9, while varying the number of attributes from zero to
Experiment 1 were added, by way of replication. In our                                                          nine. Figure 5 illustrates the results. The region marked as
                                                                                                          548

black indicates a high proportion of relational match choices     relational learning, as measured by responses in the forced-
and then the contour fades down gradually.                        choice task.
   The slope of the contour indicates that the model readily         There are several lines of future work that suggest
generalizes the standards when both the assimilation              themselves. First, we intend to explore if the model can
threshold and the number of object attributes are low. This       handle closely related phenomena (e.g. Gentner & Namy
can be interpreted as follows. A low assimilation threshold       1999, who used a similar experimental paradigm but with
corresponds to a higher willingness to accept the standards       pre-existing concepts instead of novel concepts). Second,
as belonging to the same category, which fits the                 we plan on exploiting more of CogSketch’s automatic
assumptions of our model. A low number of object                  encoding capabilities, by using it to automatically
attributes indicates a leaner encoding i.e. not enough            decompose object-level spatial descriptions into edge-level
attention was paid to the object, or it may be unfamiliar.        representations. The sketches in the stimuli will be
This is a second possible explanation for why some children       represented by a set of constituent edges, their attributes and
chose the relational match for the Sequential condition.          relations that hold between them (e.g. (isa edge2
                                                                  StraightEdge)               (edgesParallel              edge2
                       Related Work                               edge4)). For example, a square can be segmented into
There have been several prior computational models of             four constituent edges. These more detailed spatial
word learning. For example, Siskind (1996) developed an           representations will contain more shared attributes and
algorithm for cross-situational learning of word/meaning          relations and hence would naturally introduce more overlap
mappings. He used synthetic conceptual representations            between entities. This would provide a test of our
and lexicons to examine its scaling properties and noise          hypothesis that such overlap is responsible for participants
immunity. Our use of arbitrary predicates is similar to his       in the Comparison condition sometimes choosing the object
use of synthetic conceptual representations, but our visual       match. Finally, we plan to extend this model to explore how
representations are grounded in prior cognitive science           object labels promote uniform relational encoding and re-
research. It is an open question whether Siskind’s algorithm      representation (Gentner, 2010).
would work on realistic conceptual representations, and
similarly, it is an open question as to whether our word                              Acknowledgments
learning algorithm can scale to the size of vocabularies that     This research was supported by the Spatial Intelligence and
his does. Another model, described in (Roy and Pentland           Learning Center (SILC), an NSF Science of Learning
2002) uses speech and vision signals as input, to tackle the      Center, SBE-1041707, and by a grant from the Socio-
problem of how children learn to segment these perceptual         Cognitive Architectures Program of the Office of Naval
streams while at the same time learning word meanings. A          Research.
particularly novel aspect of their approach was modeling a
corpus of infant-directed speech they gathered, to ensure                                  References
their inputs were naturalistic.        Our use of sketch
understanding is motivated by the hypothesis that it forces       Chi, M. T. H., Feltovich, P., & Glaser, R. (1981).
us to incorporate high-level vision, while factoring out most     Categorization and representation of physics problems by
of the complexities of signal processing. The relatively          experts and novices. Cognitive Science, 5: 121-152.
crude visual processing techniques used in Roy &                  Christie, S. & Gentner, D. (2010). Where hypotheses come
Pentland’s system, compared to mammalian vision systems,          from: Learning new relations by structural alignment.
suggests that theirs, too, is an approximation, albeit a more     Journal of Cognition and Development, 11 (3). 356-373.
signal-rich version than ours. Neither of these models, nor         Falkenhainer, B., Forbus, K. and Gentner, D. (1989). The
any other word learning model that we are aware of, has           Structure Mapping Engine: Algorithm and examples.
tackled the role of comparison in learning relational             Artificial Intelligence, 41, 1-63.
abstractions.                                                       Forbus, K., Ferguson, R. and Gentner, D. (1994).
                                                                  Incremental structure-mapping. Proceedings of the Sixteenth
                         Discussion                               Annual Conference of the Cognitive Science Society,
                                                                  August.
We have shown that a model of word learning based on                Forbus, K., Gentner, D., and Law, K. (1995). MAC/FAC:
analogical generalization, using automatically encoded            A model of similarity-based retrieval. Cognitive Science, 19,
sketches augmented by conceptual information, can                 141-205.
simulate the behavior found in (Christie & Gentner 2010).         Forbus, K. and Gentner, D. (1989). Structural evaluation of
The invitation to compare, we argue, leads the child to           analogies: What counts? Proceedings of the Cognitive
aggressively attempt to form a generalization between the         Science Society.
two new exemplars, as modeled by lowering the                       Forbus, K., Usher, J., Lovett, A., and Wetzel, J. (2011).
assimilation threshold and only keeping overlapping               CogSketch: Sketch understanding for Cognitive Science
structure. This finding is robust across a wide range of          Research and for Education. Topics in Cognitive Science.
choices for the number of object attributes. Moreover, serial     pp 1-19.
presentation to the model, as with humans, does not lead to
                                                              549

   Friedman, S. & Forbus, K. (2008). Learning Causal
Models via Progressive Alignment & Qualitative Modeling:
A Simulation. Proceedings of the 30th Annual Conference
of the Cognitive Science Society (CogSci).
   Gentner, D. (1983). Structure-mapping: A theoretical
framework for analogy. Cognitive Science, 7, 155-170.
Gentner, D. (2003). Why we’re so smart. In D. Gentner and
S. Goldin-Meadow (Eds.), Language in mind: Advances in
the study of language and thought (pp.195-235).
Cambridge, MA: MIT Press.
   Gentner, D. (2010). Bootstrapping the mind: Analogical
processes and symbol systems. Cognitive Science 34,752-
775.
   Halstead, D. and Forbus, K. (2005). Transforming
between Propositions and Features: Bridging the Gap.
Proceedings of AAAI-2005. Pittsburgh, PA.
   Kuehne, S., Forbus, K., Gentner, D. and Quinn, B. (2000).
SEQL: Category learning as progressive abstraction using
structure mapping. Proceedings of CogSci 2000, August.
   Lockwood, K., Lovett, A., and Forbus, K. (2008).
Automatic Classification of Containment and Support
Spatial Relations in English and Dutch. Proceedings of
Spatial Cognition.
   Lovett, A., Tomai, E., Forbus, K. & Usher, J. (2009).
Solving geometric analogy problems through two-stage
analogical mapping. Cognitive Science, 33(7), 1192-1231.
   Ratterman, M., & Gentner, D. (1998) More evidence for a
relational shift in the development of analogy: Children’s
performance on a causal-mapping task. Cognitive
Development, 13, 453-478.
   Roy, D. and Pentland, A. (2002). Learning words from
sights and sounds: A computational model. Cognitive
Science, 26,113-146.
   Siskind, J. (1996). A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition 61,39-91.
                                                             550

