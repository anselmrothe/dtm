UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Generalization of Learning in Games of Strategic Interaction
Permalink
https://escholarship.org/uc/item/01d9w6x3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Juvina, Ion
Lebiere, Christian
Gonzalez, Cleotide
et al.
Publication Date
2012-01-01
Peer reviewed
  eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                    Generalization of Learning in Games of Strategic Interaction
                             Ion Juvina (ijuvina@cmu.edu), Christian Lebiere (cl@cmu.edu)
                             Department of Psychology, Carnegie Mellon University, 5000 Forbes Ave.
                                                       Pittsburgh, PA 15213 USA
                     Cleotilde Gonzalez (coty@cmu.edu), & Muniba Saleem (msaleem@cmu.edu)
                    Department of Social and Decision Sciences, Carnegie Mellon University, 5000 Forbes Ave.
                                                       Pittsburgh, PA 15213 USA
                             Abstract                                  other words, individuals develop strategies for easier games
   We present a laboratory study investigating the generalization
                                                                       and apply them to more complex games. (2) Another
   of learning across two games of strategic interaction. The          explanation says that expecting others to do what they did in
   participants’ performance was higher when a game was                the past (and expecting that they will think you will do what
   played after, as compared to before, a different game. We           you did in the past, etc.) can coordinate expectations about
   found that the generalization of learning from one game to          which of many equilibria will happen (Devetag, 2005). (3)
   another was driven by both surface and deep similarities            Finally, Knez and Camerer (2000) found that generalization
   between the two games. We developed a computational                 of learning across games strongly depended on the presence
   cognitive model to investigate mechanisms of generalization.
   Model development highlighted some of the challenges of             of superficial, surface similarity (what they call ‘descriptive’
   cognitive modeling in general and modeling strategic                similarity) between the two games. When the games
   interaction in particular. We found that development of             differed in (what we call) surface characteristics (e.g.,
   reciprocal trust was a key factor that explained the observed       actions were numbered differently in the two games)
   generalization effect.                                              transfer of learning from one game to another did not occur.
   Keywords: Cognitive modeling; Game theory; Strategic                This result is at odds with what is known from the literature
   interaction; Generalization of learning.                            on individual problem solving: generalization of learning is
                                                                       facilitated by our ability to perceive abstract, deep-level
             Introduction and Background                               similarities, and it can be impeded by the presence of
Games of strategic interaction have successfully been used             superficial, surface similarities (Holyoak & Thagard, 1995).
to model various real-world phenomena. For example, the                   In this paper we present an experiment aimed at studying
game Prisoner’s Dilemma has extensively been used as a                 generalization of learning in games of strategic interaction.
model for real-world conflict and cooperation (Rapoport,               We want to understand when, why, in which direction, and
Guyer, & Gordon, 1976). There has been a recent tendency               under what conditions generalization occurs. We also
toward studying ensembles of games, as most real-world                 present a computational cognitive model as an aid in our
“games” rarely occur in isolation; more often they take                attempt to explain the empirical results and settle any
place either concurrently or in sequence (Bednar, Chen,                potential inconsistencies in the literature.
Xiao Liu, & Page, in press). For instance, when games are                 The next section introduces the experiment and discusses
played in sequence, an effect known as “spillover of                   its results. Then the cognitive model is described and its
precedent” may occur: a precedent of efficient play in a               correspondence with the human data is discussed. The paper
game can be transferred to the next game (e.g., Knez &                 ends with a general conclusion.
Camerer, 2000). We refer to this effect as generalization of
learning in games of strategic interaction. This effect has                                    Experiment
important practical implications. For example, most                    Due to space limitation, only a brief description of the
organizations employ training exercises to develop                     experiment is given here. A more detailed description was
cooperation and trust among their employees. The                       presented elsewhere (Juvina, Saleem, Gonzalez, & Lebiere,
assumption is that what is learned in a very specific, ad-hoc          submitted). We selected two of the most representative
exercise generalizes to organizational life once the training          games of strategic interaction: Prisoner’s Dilemma (PD) and
is over.                                                               the Chicken Game (CG). They are both mixed-motive non-
   Research on what factors cause generalization of learning           zero-sum games that are played repeatedly. Players can
in games of strategic interaction can be summarized as                 choose to maximize short- or long-term payoffs by engaging
follows: (1) Bednar and colleagues (in press) use the                  in cooperation or defection and coordinating their choices
concept of entropy or strategic uncertainly to explain when            with each other. These features give these games the
learned behavior is likely to spillover from one game to               strategic dimension that makes them so relevant to real-
another. They suggest that prevalent strategies in games               world situations (Camerer, 2003). What makes PD and CG
with low entropy are more likely to be used in games with              particularly suitable for this experiment is the presence of
high entropy, but not vice versa (Bednar et al., in press). In         theoretically interesting similarities and differences,
                                                                   521

providing an ideal material for studying generalization of          maintain the relation. A trust relation develops trough
learning. Table 1 presents the payoff matrices of PD and            gradual risk-taking and reciprocation (Cook, Yamagishi,
CG.                                                                 Cheshire, Cooper, Matsuda, & Mashima, 2005). In turn, as
             Table 1: Payoff matrices of PD and CG.                 trust develops, risk is reduced and the trust relation becomes
                                                                    more stable. Our second hypothesis states that participants
    PD        A          B          CG        A          B          develop reciprocal trust throughout the first game, which
     A      -1,-1     10,-10         A     -10,-10     10,-1        facilitates learning of the optimal solution in the second
     B     -10,10       1,1          B      -1,10       1,1         game.
Both PD and CG have two symmetric (win-win and lose-                Participants and Design
lose) and two asymmetric (win-lose and lose-win)                    One hundred and twenty participants were paired with
outcomes. Besides these similarities there are significant          anonymous partners (leading to 60 pairs) and were asked to
differences between the two games. In CG, either of the             play the two games in sequence. The 60 pairs were
asymmetric outcomes is more effective in terms of joint             randomly assigned to two conditions defined by the order in
payoffs than the [1,1] outcome. This is not the case in PD          which the games were played: PD-CG and CG-PD.
where an asymmetric outcome [10,-10] is inferior in terms           Participants played 200 unnumbered rounds of each game.
of joint payoffs to the win-win outcome [1,1]. Mutual               At the end of each game, participants completed a five-item
cooperation in CG can be reached by a strongly optimal              questionnaire assessing: how trustful they were of the
strategy (i.e., alternation of [-1,10] and [10,-1]) or a weakly     opponent; how trustful of them the opponent was; how fair
optimal strategy [1,1]. The optimal strategy in PD                  they thought the opponent’s actions were; how fair the
corresponds to the weakly optimal strategy in CG                    participants’ actions were towards their opponents; and how
numerically, while the strongly optimal strategy of                 satisfied they were with the overall outcome of the game.
alternation in CG shares no surface-level similarities with
the optimal strategy in PD. Thus, although mutual                   Results1 and Discussion
cooperation corresponds to different choices in the two             To study generalization of learning across the two games,
games (i.e., surface-level dissimilarity), they share a deep-       we analyzed the outcomes of a game according to when it
level similarity in the sense that mutual cooperation is, in        was played. We also analyzed the round-by-round dynamics
the long run, superior to competition in both games. This           of these outcomes. The statistical significance of the
provides a perfect test for our first hypothesis stating that       observed effects was tested with the aid of Linear Mixed
individuals who have learned how to find an optimal                 Effects analysis (lmer analysis from the LME4 package in
strategy in one game will be more likely to find an optimal         R). This analysis was preferred instead of the classical
strategy in the next game even if those optimal strategies are      analysis of variance (ANOVA) because the data violated the
different across the two games.                                     assumption of normality.
   In both PD and CG, learning must occur not only at an
individual level but also at a dyad level. If learning occurs       Similarities and differences The frequencies of the most
only in one of the players in a dyad, the outcomes are              relevant outcomes (i.e., the two symmetric ones and an
disastrous for that player, because the best solution also          alternation of the two asymmetric ones) are displayed in
bears the highest risk. For example, if only one player             Figure 1 on a round-by-round basis. The first thing to notice
understands that alternating between the two moves is the           is how different the two games are from each other from a
optimal solution in CG, the outcome for that player can be a        behavioral perspective: the [1,1] outcome increases in PD
sequence of -1 and -10 payoffs. Only if both players                but decreases in CG; alternation is prominent in CG but
understand the value of alternation and are willing to              almost nonexistent in PD; and the mutually destructive
alternate, the result will be a sequence of 10 and -1 payoffs       outcome ([-1,-1] in PD and [-10,-10] in CG) is more
for each player, which in average gives each player a payoff        frequent in PD than in CG. However, in spite of these
of 4.5 points per round. Thus, the context of                       apparent differences, the two games are similar in the sense
interdependence makes unilateral individual learning not            that mutual cooperation emerges as the preferred solution
only useless but also detrimental. The two players must             and it becomes more and more stable over time. These
jointly learn that only a solution that maximizes joint payoff      patterns are in line with previous findings (e.g., Rapoport et
is viable long term. However, this solution carries the most        al., 1976). Given this deep-level similarity, we expect
risk and thus it is potentially unstable in the long term. To       players to be able to generalize their learning of the optimal
ensure that the optimal solution is maintained from one             strategy across the two games, although surface similarities
round to another, there must exist a mechanism that                 might impede this process (Holyoak & Thagard, 1995).
mitigates the risk associated with this solution. It has been       Since we ran the games in both orders (i.e., PD-CG and CG-
suggested that trust relations are self-sustaining once they
have been developed (Hardin, 2002). In situations where
                                                                       1
there are benefits to individuals that can only be generated             Only a summary of the results is provided here as a context for
through mutual trust, each individual has an incentive to           understanding the cognitive model. A more detailed presentation
                                                                    of the results was given in Juvina et al., submitted.
                                                                522

PD), we can also test whether generalization occurs only in                       the observed effects. There was a main effect of order (z =
one direction, from low to high entropy, as suggested by                          2.21, p = 0.027) and a main effect of round (z = -8.171, p <
Bednar and colleagues (in press).                                                 0.001); the interaction between order and round was also
                                                                                  significant (z = -7.196, p < 0.001) indicating that the main
                              PD-CG: Human data
                                                                                  effect of order is larger at the beginning of the game and it
              0.8                                                                 progressively becomes smaller.
                               PD                        CG
                                                                                     In the CG-PD order, if generalization of learning across
                              [1,1]
                              Alternation
                                                        [1,1]
                                                        Alternation
                                                                                  games were driven by surface similarities, one would expect
              0.6
                              [-1,-1]                   [-10,-10]                 the strategy of alternating between the two asymmetrical
                                                                                  outcomes to be attempted in the second game as well, at
                                                                                  least in the beginning of the game. The main effect of order
Frequencies
                                                                                  was non-significant (z = 1.476, p > 0.10), suggesting that the
              0.4
                                                                                  strongly optimal strategy in CG (alternation) was not
                                                                                  transferred as such (based on surface similarities) to PD.
                                                                                  There remains the possibility that the [1,1] outcome was
              0.2                                                                 transferred as such from CG to PD. Even though the [1,1]
                                                                                  outcome is only weakly optimal in CG, it was selected with
                                                                                  relatively high frequency (see Figure 1) and it might have
              0.0                                                                 been considered optimal by some participants. We will
                                                                                  revisit this point in the section on combined effects of
                    0   100                 200   300                 400
                                                                                  surface and deep-level similarities.
                                       Rounds
                                                                                  Generalization Driven by Deep-Level Similarities If
                              CG-PD: Human data
                                                                                  learning across games was driven by deep-level similarities,
              0.8
                                                                                  one would expect learning the optimal strategy in the first
                               CG                                                 game to increase the probability of learning the optimal
                              [1,1]                                               strategy in the second game, even though there is no surface
                              Alternation
                              [-10,-10]                                           similarity between these strategies. These strategies ([1,1] in
              0.6
                                                         PD
                                                                                  PD and alternation in CG) are similar only on an abstract,
                                                                                  deep level: they both aim at maximizing joint payoff in a
                                                        [1,1]
                                                                                  sustainable way, which in these two games is realistically
Frequencies
                                                        Alternation
                                                        [-1,-1]
              0.4                                                                 possible only if the two players make (approximately) equal
                                                                                  payoffs on a long run. On a surface level, these two
                                                                                  strategies are very different. The [1,1] strategy in PD
              0.2                                                                 requires that players make the same move at each trial and
                                                                                  they do not switch to the opposite move. In contrast, the
                                                                                  alternation strategy in CG requires that players make
              0.0
                                                                                  opposite moves at each round and they continuously switch
                                                                                  between the two moves. A LME model with occurrence of
                    0   100                 200   300                 400
                                                                                  the alternation outcome in CG as a dependent variable,
                                       Rounds
                                                                                  order, round and their interaction as fixed factors, and
   Figure 1: Frequencies of the most relevant outcomes in                         participant as a random factor revealed a main effect of
PD and CG by order (PD-CG on top and CG-PD on bottom)                             order (z = -2.014, p = 0.044) indicating a higher level of
  and round averaged across all the human participants.                           alternation when CG was played after PD, a main effect of
                                                                                  round (z = 16.205, p < 0.001) indicating that more and more
Generalization Driven by Surface Similarities If learning                         pairs of participants discovered the alternation strategy as
across games is driven by surface similarities, one would                         the game unfolded, and a significant interaction between
expect the strategy that is learned in the first game to be                       order and round (z = 8.5, p < 0.001) indicating that the
applied in the second game as well, even though it may not                        optimal strategy was learned faster when CG was played
be appropriate for the second game. This is indeed the case                       second. The same analysis was conducted for the occurrence
with regard to the [1,1] outcome in the PD-CG order:                              of the [1,1] outcome in PD and it revealed a main effect of
players learn that [1,1] is long-term optimal in PD and they                      order (z = -4.340, p < 0.001) indicating that more pairs of
are more likely to achieve it in the subsequent CG, even                          participants discovered the optimal strategy in PD when it
though it is only weakly optimal in CG. A LME model with                          was played after CG, a main effect of round (z = 10.149, p <
occurrence of [1,1] as a dependent variable (binomial                             0.001) indicating that more and more pairs of participants
distribution), order, round, and their interaction as fixed                       found the optimal strategy as the game unfolded, and a
factors, and participant as a random factor was used to test                      significant interaction between order and round (z = 12.689,
                                                                            523

p < 0.001) indicating that the optimal strategy reached a          game predicted high levels of trust at T2 (r = 0.67, p < 0.001
ceiling when PD was played after CG, whereas it increased          for CG and r = 0.88, p < 0.001 for PD). As expected, the
continuously when PD was played before CG. These results           level of reciprocal trust increased from T1 to T2 (meanT1 =
supported our first hypothesis. Specifically, learning the         11.8, meanT2 = 14.1, t = -3.247, p = 0.001). These
optimal strategy in the first game increased the probability       correlations between trust and the frequency of mutual
of learning the optimal strategy in the second game, even          cooperation corroborate our second hypothesis. They
though the optimal strategies were different in the two            suggest that generalization of learning driven by deep-level
games. This generalization effect was significant in both          similarity is facilitated by development and maintenance of
directions (PD-CG and CG-PD) suggesting that entropy               reciprocal trust. This finding will be essential for model
(Bednar et al., in press) has little explanatory relevance. If     development.
entropy were the causing factor, generalization would have
only occurred in one direction.                                     A cognitive model of generalization of learning
                                                                   Modeling generalization of learning across games of
Combined Effects of Surface and Deep Similarities In the           strategic interaction provides an opportunity to address
case of deep-level generalization, the main effect of order        some of the ongoing challenges of computational cognitive
was smaller in magnitude for CG (z = -2.014, p = 0.044)            modeling. Three of these challenges are particularly relevant
than for PD (z = -4.340, p < 0.001). It seems as if CG has a       here and are described below as the model is introduced.
stronger impact on PD than vice versa. A possible                  The model is developed in ACT-R and it will be made
explanation for this difference is based on how surface and        freely available to the public on the ACT-R website2.
deep-level similarities combine with each other to drive
generalization of learning across games. They may have             Interdependence
congruent or incongruent effects. Thus, in the PD-CG order,
surface and deep-level similarities act in a divergent,            In games of strategic interaction, players are aware of each
incongruent way: surface similarity makes it more likely           other and their interdependence. In a previous study we
that the [1,1] outcome is selected whereas deep-level              showed that game outcomes were influenced by players’
similarities make it more likely that the alternation outcome      awareness of interdependence. In PD, the more information
is selected. In other words, generalization based on surface       the two players in a dyad had about each other’s options and
similarity interferes with generalization based on deep-level      payoffs the more likely they were to establish and maintain
similarity. In contrast, in the CG-PD order, both types of         mutual cooperation (Martin, Gonzalez, Juvina, & Lebiere,
similarities act in a convergent, congruent way: they both         submitted). Consequently, a cognitive model playing against
increase the probability that the [1,1] outcome is selected.       another cognitive model in a simultaneous choice paradigm
There is no impeding effect of surface similarity on PD            needs to develop an adequate representation of the
because there is no optimal strategy in CG that is similar         opponent. We use instance-based learning (IBL) and
enough to a non-optimal or sub-optimal strategy in PD. The         sequence learning (SL) (Gonzalez, Lerch, & Lebiere, 2003)
impeding and/or enabling effects of surface similarities on        to ensure that the opponent is dynamically represented as
deep-level generalization are revisited in the modeling            the game unfolds. Specifically, at each round in the game an
section.                                                           instance (snapshot of the current situation) is saved in
                                                                   memory. The instance contains the previous moves of the
Reciprocal Trust In addition to game choices, we analyzed          two players and the opponent’s current move. Saved
the debriefing questionnaires that were administered at the        instances are used to develop contextualized expectations
end of each game. Since the answers to these questions were        about the opponent’s moves based on ACT-R’s memory
highly correlated with each other for any one individual           storage and retrieval mechanisms (Anderson, 2007).
participant, we averaged them in one composite variable            Expectations can explain some of the spillovers across
that we call Reciprocal Trust. Since the debriefing questions      games (Devetag, 2005).
were administered twice (at the end of each game) we refer
to them as T1 and T2. We calculated correlations between           Generality
these two trust variables and the variables indicating mutual      Before one attempts to build a model of generalization of
cooperation in the two games. Spearman’s rho coefficient           learning across two games, one needs to have a model that
was used for correlations because the data failed to meet the      is able to account for the human data in both games.
normality assumption. We found that the more frequent              Although by and large cognitive models are task-specific,
mutual cooperation was in the first game the more likely the       there is a growing need to develop more general, task-
players were to rate each other as trustworthy at T1 (r =          independent models and there are a few precedents: Lebiere,
0.75, p < 0.001 for PD and r = 0.42, p < 0.001 for CG). In         Wallach, and West (2000) developed a model of PD that
addition, the more trustworthy players rated each other at         was able to account for human behavior in a number of
T1, the more likely they were to enact mutual cooperation in       other 2X2 games; and Salvucci (under revision) developed a
the second game (r = 0.28, p = 0.03 for CG and r = 0.47, p <       “supermodel” that accounts for human data in seven
0.001 for PD). Finally, mutual cooperation in the second
                                                                      2
                                                                        http://act-r.psy.cmu.edu/
                                                               524

different tasks. We build upon these precedents of generality          As mentioned in the introduction, in both PD and CG the
by developing a single model to account for round-by-round          long-term optimal solution bears the highest risk and, thus,
human data in both PD and CG. We achieve this generality            it is unstable in the absence of reciprocal trust. We indeed
by using instance-based learning for opponent modeling (as          found that self-reported trust increases after game playing
described in the previous section) and reinforcement                and it positively correlates with the optimal outcome. It may
learning for action selection. Both instance-based learning         well be that trust explains the deep-level generalization of
and reinforcement learning are very general learning                learning across games. Players learn to trust each other and
mechanisms that can produce different results depending on          this affects their reward structure.
their input. Specifically, at each round in the game, the              Recent literature on trust (e.g., Castelfranchi & Falcone,
model predicts the opponent’s move based on the                     2010) suggests that trust is essentially a mechanism that
opponent’s past behavior and selects its own move based on          mitigates risk and develops through risk-taking and
the utilities of its own past moves in the current context. The     reciprocity. Inspired by this literature, we added a “trust
input that the model receives as it plays determines the            accumulator” to our model – a variable that increases when
model’s behavior. The input is represented by opponent’s            the opponent makes a cooperative (risky) move and
move, own move, and the payoffs associated with these               decreases when the opponent makes a competitive move. In
moves.                                                              addition, a variable called “willingness to invest in trust”
   An important question is what constitutes the reward from        was necessary to overcome situations in which both players
which the model learns the utilities of its actions (moves).        strongly distrust each other and persist in a mutually
Players may try to maximize their own payoff, the                   destructive outcome, which further erodes their reciprocal
opponent’s payoff, the sum of the two player’s payoffs, the         trust, and so on. In such situations, the empirical data shows
difference, etc. Thus, a large number of reward structures          that players make attempts to develop trust by gradual risk-
can be imagined. A complicating assumption is that the              taking. When these attempts are reciprocated, trust starts to
reward structure might change as the game unfolds                   re-develop. In the absence of reciprocation these attempts
depending on the dynamics of the interaction between the            are discontinued. The willingness to invest in trust increases
two players. This indeed seems to be the case here, as we           with each mutually destructive outcome and decreases with
have realized after a large number of model explorations: no        each attempt to cooperate that is not reciprocated.
single preset reward structure is sufficient to account for the        The variables “trust accumulator” and “willingness to
human data. One could try to computationally explore the            invest in trust” are used to determine the dynamics of the
space of all possible reward structures and their                   reward structure. They both start at zero. When they both
combinations to find the one that best fit the human data,          are zero or negative, the two players act selfishly by trying
but the value of this approach is questionable, because it          to maximize the difference between their own payoff and
may lead to a theoretically opaque solution. Instead, we            the opponent’s payoff. This quickly leads to the mutually
chose to employ a theoretically guided exploration that             destructive outcome, which decreases trust but increases the
drastically reduces the number of possible reward structures        willingness to invest in trust. When the latter is positive, a
and, more importantly, gives us a principled way to describe        player acts selflessly, trying to maximize the opponent’s
the dynamics of players’ motives as the game unfolds (see           payoff. This can lead to mutual cooperation and
its description in the next section).                               development of trust or players may relapse into mutual
                                                                    destruction. When the trust accumulator is positive, a player
Generalization of learning                                          tries to maximize joint payoff and avoid exploitation. Thus,
When the model relies only on the two learning mechanisms           the model switches between three reward functions
described above (i.e., instance-based learning and                  depending on the dynamics of trust between the two players.
reinforcement learning) it is able to only account for the          This mechanism provides a principled solution to the
generalization driven by surface similarities. Thus, the            problem of selecting the right reward structure and in the
opponent is expected to make the same move in a given               same time solves the generalization problem: due to
context as in the previous game. Similarly, an action that          accumulation of trust in the first game, the model employs a
has been rewarded in the first game tends to be selected            reward structure that is conducive to the optimal solution
more often in the second game. It is impossible in this             and thus better performance in the second game.
framework to account for generalization driven by deep-
level similarities. For example, if the opponent used to            Modeling results
repeat move B when it was reciprocated in PD, there is no           A cognitive model incorporating the principles described
reason to switch to alternation between A and B when none           above was developed and fit to the human data presented in
of these moves are reciprocated in CG. Moreover, learning           the previous section. Fitting the model to the human data
within a game may in fact hinder generalization of learning         was done manually by varying a number of parameters (of
across games if surface similarities are incongruent with the       which some are standard in the ACT-R architecture and
optimal solution in the target game. To find a solution to the      others were introduced as part of the trust mechanism) and
deep generalization problem, we need to return to a                 trying to increase correlation (r) and decrease root mean
theoretical and empirical analysis of the two games.                square deviation (RMSD) between model and human data.
                                                                525

The results of the current best model (r = 0.89, RMSD =                          cognitive load) or insufficient (expectations, surface
0.09) are presented in Figure 2.                                                 similarities), while others are essential (deep-level similarity
                                                                                 and reciprocal trust) for generalization of learning in games
                                      PD-CG Model
                                                                                 of strategic interaction.
              0.8
                             PD                      CG
                            [1,1]                   [1,1]
                                                                                                    Acknowledgments
                            Alternation
                            [-1,-1]
                                                    Alternation
                                                    [-10,-10]
                                                                                 This research was supported by the Defense Threat
              0.6
                                                                                 Reduction Agency (DTRA) grant number: HDTRA1-09-1-
                                                                                 0053 to Cleotilde Gonzalez and Christian Lebiere.
Frequencies
              0.4
                                                                                                         References
                                                                                 Anderson, J. R. (2007). How Can the Human Mind Occur in
                                                                                   the Physical Universe? New York: Oxford University
              0.2
                                                                                   Press.
                                                                                 Bednar, J., Chen, Y., Xiao Liu, T., & Page, S.E. (in press).
                                                                                   Behavioral Spillovers and Cognitive Load in Multiple
              0.0
                                                                                   Games: An Experimental Study. Games and Economic
                    0       100            200      300             400            Behavior.
                                          Rounds                                 Camerer, C. F. (2003). Behavioral Game Theory:
                                                                                   Experiments in Strategic Interaction. Princeton, New
                                      CG-PD Model                                  Jersey: Princeton University Press.
                                                                                 Castelfranchi, C., & Falcone, R. (2010). Trust Theory: A
              0.8
                             CG                                                    Socio-Cognitive and Computational Model: John Wiley
                            [1,1]                                                  and Sons.
                            Alternation
                            [-10,-10]                                            Cook, K. S. , Yamagishi, T., Cheshire, C., Cooper, R.,
              0.6
                                                                                   Matsuda, M., & Mashima, R. (2005). Trust Building via
                                                                                   Risk Taking: A Cross-Societal Experiment. Social
                                                                                   Psychology Quarterly, 68(2), 121-142.
Frequencies
              0.4
                                                                                 Devetag, G. (2005). Precedent transfer in coordination
                                                                                   games: An experiment. Economics Letters, 89, 227–232.
                                                                                 Gonzalez, C., Lerch, J. F., & Lebiere, C. (2003). Instance-
              0.2                                            PD
                                                                                   based learning in dynamic decision making. Cognitive
                                                           [1,1]
                                                           Alternation
                                                                                   Science, 27, 591–635.
                                                           [-1,-1]               Hardin, R. (2002). Trust and Trustworthiness. New York:
              0.0                                                                  Russell Sage Foundation.
                    0       100            200      300              400
                                                                                 Holyoak, K. J., & Thagard, P. R. (1995). Mental leaps:
                                          Rounds
                                                                                   Analogy in creative thought. Cambridge, MA: MIT Press.
                                                                                 Juvina, I., Saleem, M., Gonzalez, C., & Lebiere, C.
                        Figure 2: Results of model simulation.                     (submitted). Generalization of learning in conflict games:
                                                                                   Effects of surface and deep level similarities.
Overall, the model matches the trends in the human data                            Organizational Behavior and Human Decision Processes.
reasonably well (compare to Figure 1). More importantly,                         Knez, M., & Camerer, C. (2000). Increasing Cooperation in
the generalization effects are also accounted for.                                 Prisoner’s Dilemmas by Establishing a Precedent of
                                                                                   Efficiency in Coordination Games. Organizational
                         Discussion and Conclusion                                 Behavior and Human Decision Processes, 82, 194-216.
We found that generalization of learning across two games                        Lebiere, C., Wallach, D., & West, R. L. (2000). A memory-
of strategic interaction is driven by deep-level similarities                      based account of the prisoner's dilemma and other 2x2
between the two games. Surface similarities may facilitate                         games. Paper presented at the International Conference on
or hinder generalization depending on whether they are                             Cognitive Modeling.
congruent or incongruent with the optimal solution. We                           Martin, J. M., Gonzalez, C., Juvina, I., Lebiere, C.
used one cognitive model to account for human data in both                         (submitted). Interdependence information and its effects
games. This model helped to explain the observed                                   on cooperation.
generalization effect: reciprocal trust was necessary to                         Rapoport, A., Guyer, M.J., & Gordon, D.G. (1976). The 2X2
mitigate the risk associated with the long-term optimal                            game. Ann Arbor, MI: The University of Michigan Press.
solution. We can conclude that some of the factors                               Salvucci, D.D. (under revision). Integration and reuse in
suggested in the literature are not necessary (entropy,                            cognitive skill acquisition. Cognitive Science.
                                                                           526

