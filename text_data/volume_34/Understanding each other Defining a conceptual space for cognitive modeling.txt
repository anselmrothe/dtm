UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Understanding each other: Defining a conceptual space for cognitive modeling
Permalink
https://escholarship.org/uc/item/1zs440m0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
West, Robert
Leibovitz, David Pierre
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

      Understanding each other: Defining a conceptual space for cognitive modeling
                                           Robert West (robert_west@carleton.ca) &
                                         David Pierre Leibovitz (dpleibovitz@ieee.org)
                                     Institute of Cognitive Science, 2201 DT, Carleton University
                                      1125 Colonel By Drive, Ottawa, Ontario, K1S 5B6 Canada
                               Abstract                               etc.) is misleading and counterproductive. In place of this,
                                                                      we advocate a multidimensional approach to characterize
   Cognitive modeling is a complex endeavor so it is not
   surprising that the goals and intentions of modelers are often     modeling systems along numerous dimensions, including
   misunderstood, even by other modelers. To try to clarify this      the beliefs and motivations of the modeler. Thus in our
   we have attempted to map out the various philosophical and         system, two modelers can use the same computational code
   theoretical commitments that one makes when creating a             but actually have very little in common. Likewise two
   cognitive model or architecture. The goal of this is to avoid      modelers could use very different codes (e.g., the ACT-R
   misunderstandings between the adherents of different               symbolic/subsymbolic code, J. R. Anderson & Lebiere,
   modeling systems and between cognitive modelers and the
   rest of the scientific community.
                                                                      1998; and the NENGO spiking neuron code, Eliasmith &
                                                                      Anderson, 2003) and still be completely on the same page.
   Keywords: cognitive modeling.                                      To illustrate this approach we will use the controversial
                                                                      example of the "symbol" throughout the paper, although
                            Introduction                              each dimension can be applied to any modeling construct.
In the 1990s there was movement to contrast mainstream                Due to limited space we have focused on dimensions that
cognitive modeling, which was labeled as cognitivist, with            we believe are important.
alternative approaches which were asserted to represent a
fundamentally different paradigm. These alternatives                                   AI and Useful fictions
included situated cognition, distributed cognition,                   The strong AI hypothesis says that if the functions of the
dynamicism, embodied cognition and subsumption                        human mind can be correctly simulated on a computer then
architectures. However, Vera and Simon (1993) argued that             there will be no difference between the human mind and the
these theories represented progress and innovation but not            computer mind. It is important to note that this hypothesis is
an alternative approach. They did this by arguing that critics        silent about the level of abstraction or embodiment required
of the mainstream view had mistakenly assumed that the                for success. It could involve high level algorithms realized
practices, strategies, and short cuts of mainstream modelers          as software, or it could involve a brain made of highly
represented their actual philosophical and theoretical                realistic mechanical neurons embodied in a lifelike
commitments. We argue that Vera & Simon’s argument was                humanoid robot and raised as a human infant. Therefore, if
a legitimate response and more generally that philosophical           we apply strong AI to symbols it means that the symbols in
and theoretical commitments cannot be determined solely               a model are a valid way of representing what is taking place
by analyzing systems and practices associated with a                  in the brain. Alternatively, symbols can be viewed as useful
method of modeling. Full understanding requires                       fictions. That is, the brain does not process symbols but
explicating the philosophical and theoretical commitments             there is something about the processing of symbols that is
of the modeler.                                                       analogous to how a brain works and it is therefore useful or
   Today all of the alternatives to mainstream modeling               expedient to model it in this way.
discussed in Vera and Simon (1993) are accepted in the
main stream. That is to say, they are broadly recognized as                                  Metaphysics
important contributions. At this time it would be fair to say
that there is no widely accepted, "main stream" approach to           In mainstream western philosophy there are substances and
modeling. However, despite efforts to understand different            processes that act on the substances. For example, logical
modeling systems as alternative approaches, each with their           formalisms can be used to act on symbolic representations
own strengths (e.g., McClelland, 2009) there are still                about the state of the world. We will refer to this as
numerous attempts to vilify modeling systems by critics               substance philosophy. In process philosophy (see Bickhard,
who do not fully understand the goals and intentions of the           2010; Whitehead & Griffin, 1931) there are no modular
system creators and users. In our view, the idea that a               substances, only interacting processes. What appear to be
modeling system can be dismissed based on a principle or a            substances are temporary emergent properties of ongoing
philosophical argument is, in fact, a philosophical mistake.          processes. According to Quantum Physics, process
   We argue that pigeon holing modeling systems according             philosophy is true for physical objects. For example, the
to broad philosophical and theoretical distinctions (e.g.,            chair across the room is a temporary quantum process not an
cognitivist vs. anti-cognitivist, computational vs. non-              independent object. Likewise, when you close your eyes and
computational, representational vs. anti-representational,
                                                                  2535

remember the chair, that memory is a temporary neural                       Reverse and forward Engineering
process not an object.
                                                                   Reverse engineering involves testing a system, the brain in
   Often psychological and linguistic constructs are
                                                                   this case, and working backwards to discover how it
discussed as if they were actual objects, which leads to
                                                                   functions. Unfortunately, having a model that performs
confusion because it could mean they are meant as objects
                                                                   similarly to a human does not confirm that it is a valid
(i.e., substance philosophy) or that they are meant as a
                                                                   model because other future tests may disconfirm it.
simplification standing in for a process (i.e., process
                                                                     However, modeling can also move forward through
philosophy). For example, the use of symbols in a model
                                                                   forward engineering. Forward engineering involves
could signify that symbols exist, or it could signify that
                                                                   designing a system with the goal of achieving certain
there is a process that acts as though symbols exist. More
                                                                   functions. Therefore, the goal is to achieve the same
generally, a process philosophy view implies that
                                                                   functionality that humans have without worrying about
psychological or linguistic constructs in a model should be
                                                                   doing it in the same way as the brain does. If successful the
regarded as abstract proxies standing in for interacting
                                                                   result would be a system that is roughly isomorphic to how
processes – not informationally encapsulated modules as
                                                                   the brain behaves, but does not give insight into how the
suggested by Fodor (1983). The issue then becomes the
                                                                   brain does it. For example, the use of symbols in a model
relative stability of the processes underlying psychological
                                                                   could be due to the belief that symbols behave
constructs. Process philosophy is silent on this - neural
                                                                   isomorphically to neural representations.
processes giving rise to psychological constructs could be
                                                                     The difference between backward and forward
very stable, resulting in a relatively crisp, well defined
                                                                   engineering is also important when evaluating how a model
constructs; or they could be noisier, resulting in fuzzy and
                                                                   has been evaluated. Generally speaking, attempts to reverse
possibly temporary constructs. Determining this, in our
                                                                   engineer involve careful comparisons to experimental data
opinion, is not a philosophical issue. However, deciding if
                                                                   while attempts to forward engineer involve showing that a
constructs, such as symbols, actually exist is a philosophical
                                                                   model can produce certain functionality. It is important to
issue.
                                                                   note that a modeler may also iterate between reverse and
                                                                   forward engineering.
       Divide and conquer versus unification
The simplicity principle (Chater & Vitányi, 2003) refers to                        Epistemic commitments
the idea that cognitive phenomena are best modeled in the
                                                                   Epistemic commitment refers to the mechanisms used to
simplest way. This goal, related to Occam’s Razor, is not
                                                                   build the model. Specifically, we mean it to refer to a
contentious but becomes less clear when the scope of the
                                                                   commitment to a particular way of understanding and
phenomena is considered. Newell, in his famous (1973)
                                                                   modeling the brain. The debate between proponents of
paper, argued that the phenomena to be explained is the
                                                                   symbol systems and proponents of neural networks is an
whole brain. Newell (1990) distinguished between micro
                                                                   example of an epistemic debate. The idea motivating such
models (i.e., independent models of different phenomena)
                                                                   debates is that it is necessary to first get the way of
and architectural models (i.e., models constrained by the use
                                                                   modeling right, otherwise the resulting models will be
of a cognitive architecture aimed at describing the whole
                                                                   misleading and ultimately dead ends. This issue has fueled a
brain). The goal with micro models is to make them as
                                                                   lot of debate within cognitive science. Examples of different
simple as possible but the goal for models built in an
                                                                   systems are: symbol systems, neural networks, holographic
architecture is more complex. The model should be as
                                                                   systems, dynamic systems, spiking neuron models, Bayesian
simple as possible given the constraints of the architecture
                                                                   networks, logical systems, grammatical systems etc.
but the actual goal is to produce an architecture that is as
                                                                   However, it is also possible to view these as tools rather
simple as possible across numerous models of different
                                                                   than competing theories, in which case the choice of a
phenomena (including neural phenomena, (J. R. Anderson,
                                                                   particular way of modeling would reflect a pragmatic choice
2007a)).
                                                                   rather than a principled one. Another approach is to view
   Therefore, from an architectural point of view, having lots
                                                                   different modeling systems as different lenses for viewing a
of incommensurate micro models is not useful, regardless of
                                                                   phenomena in different ways (McClelland, 2009).
how simple they are individually. One way around this is to
                                                                     Related to this, it is important to note that the word
argue that the micro model represents a distinct
                                                                   architecture is used in two ways. As noted above, it can
cognitive/neural module that encapsulates and operates on a
                                                                   refer to a system meant to be a unified model of the whole
particular kind of information (Fodor, 1983). For example,
                                                                   brain, or it can refer to mechanisms for building models that
by arguing that there is a distinct symbol based language
                                                                   are able to model the whole brain. For example, ACT-R (J.
module, one can ignore issues or problems concerning the
                                                                   R. Anderson, 1993) is an attempt at creating a unified
viability of using symbols to model other functions of the
                                                                   architecture but it is meant to be a hybrid system and
brain. Therefore, the form of a particular model could reflect
                                                                   therefore does not embody an epistemic commitment. ACT-
the goal of creating a unified architecture, of understanding
                                                                   R is often described as a production system but this is
a distinct module, or of creating the simplest model for a
                                                                   incorrect as ACT-R has numerous modules that use
specific phenomenon.
                                                                   numerous mechanisms. The use of a production system
                                                               2536

module to coordinate the other modules in ACT-R is a               paper. A system level occurs when the behavior of a
commitment to a theory about unification; it is not an             complex lower level system can be understood in terms of
epistemic commitment (i.e., a way of understanding the             less complex higher level constructs. For example, in the
whole brain). In contrast, NENGO (Eliasmith & Anderson,            theory of thermodynamics, the complex interactions of
2003) is a system for building spiking neuron models               atomic particles can be understood through higher level
according to a specific theory about spiking neurons, so the       concepts such as heat and pressure. So a systems level is a
use of NENGO can be seen as an epistemic commitment                real thing (in as much as heat and pressure are real things)
(i.e., that the whole brain can be modeled in this way).           but it is important to note that a system level can be weak or
                                                                   strong depending on the relative reduction in complexity
                Ontological commitments                            produced by the emergent level. A weak system level is
Ontological commitment refers to the way a model is                leaky, meaning that it is sometimes affected be system
divided into functional parts and their connectivity. There        levels below it (e.g., Saunders, Kolen, & Pollack, 1994).
are two reasons why ontological commitments are                       The cognitive level is theorized to exist as a systems level
important. The first has to do with creating unified cognitive     above the neural level but there is considerable controversy
architectures. Simply put, a valid cognitive model of the          over whether it exists and if it does, what form does it take?
whole brain requires that the functional parts of the model        The symbol system hypothesis asserted that the cognitive
map onto the functional parts of the brain. Although the           level is based on processing symbols. Likewise, Chomsky
results of experimental psychology are good for testing            (e.g., Chomsky, 1995) argued that for understanding
models, they may be misleading in terms of telling us what         language, symbols could be divorced from the underlying
the parts are because their ontologies are defined primarily       system that produces them. However, it is instructive look at
to make experimentation possible on different psychological        exactly what was meant by, "symbol." For Chomsky a
phenomena. Unfortunately this does not necessary tell us           symbol is a word, but Newell defined a symbol in terms of
what the actual parts are. For example, we remember facts          distal access (Newell, 1990). Distal access refers to using
and we remember episodes and these can be treated                  information that is not local, i.e., information that is
separately for experimental purposes, but we still do not          transported from another part of the brain. The form of the
know if we have separate semantic and declarative memory           information or the way it is transferred is not important,
systems or if they are both products of a single long term         therefore Newell's commitment to symbols is completely
memory system.                                                     different from Chomsky’s commitment to symbols.
   Another very important issue related to ontologies is
cognitive re-use (see M. L. Anderson, 2010). This refers to                             Level of Analysis
whether or not our cognitive ontology corresponds to a             Level of analysis is different from system level. Level of
dedicated neural area. Much of the neural localization work        analysis refers to analyzing a system at a particular level
taking place today explicitly or implicitly assumes that it        (e.g., neural, neural groups, networks, symbols). Using a
does. However, the cognitive re-use hypothesis is that             level of analysis may or may not indicate a belief that the
higher-level cognitive mechanisms and functions can be             level is a systems level. So the use of symbols in a model
created by re-using and recombining lower level cognitive          may indicate a commitment to the symbol system
mechanisms and functions. If this is true then there are two       hypothesis, but it could also occur because that level of
important consequences: (1) specific brain areas are not           analysis is useful, without any commitment to the existence
dedicated to specific cognitive functions, and (2), the            of an actual systems level. Also, it is possible to test a model
ontology that we should be looking for is at a lower level.        constructed at a higher systems level using a lower level of
   Therefore, modelers may believe that the modules of their       analysis if there is a theory about how the lower level is
system correspond to actual cognitive functions in the brain,      related to the higher level. For example, ACT-R models can
and they may further believe that these functions map to           be tested using a neural level of analysis with an fMRI scan
dedicated areas of the brain. But having a module in a             (J. R. Anderson, 2007b). Choice of a level of analysis
model does not necessarily mean that they believe either of        reflects beliefs about the most effective way of testing a
these things. For example, following the cognitive re-use          model.
hypothesis, a module could also represent a function that is
created through the interaction of lower level functions                                  Consciousness
under specific conditions. Symbols, or any other construct,        Explaining consciousness is a special case of the strong AI
can be thought of in either way.                                   issue that deserves its own section. The question is, could a
                                                                   properly constructed cognitive architecture actually have
                        System levels                              conscious experiences. From a strong AI point of view the
Allan Newell (1980) proposed that the brain is constructed         answer is yes, but many people reject this position because
in the way that computers are engineered, according to             they find it hard to imagine. This seems to be due mainly to
system levels. The reason why natural systems would                our subjective experience of qualia.
develop distinct hierarchical levels was developed by Simon           Qualia refer to the various phenomenal feelings of our
(1962) but a discussion of this is beyond the scope of this        conscious experiences. From a modeling point of view
                                                               2537

qualia creates a potential problem because thought, emotion,           more, it can be considered scientific (for a detailed
and different types of perception do not feel the same to us;          discussion see Cooper, 2007).
they feel qualitatively different. However, from a cognitive              It is interesting to note that although some of the criticism
science perspective, and a neuroscience perspective, all               directed at testing models comes from Experimental
qualia arise from information processing that is ultimately            Psychology, Experimental Psychology also fails to follow
realized through the firing of neurons. Since we do not                Popper's model. Specifically, most experiments in
understand what consciousness is or how it creates different           Experimental Psychology test for significant differences
qualia from the same underlying mechanism, most cognitive              predicted by a theory, therefore, falsifying the theory would
models simply ignore the issue or focus on the correlates of           mean showing no significant difference, which would mean
consciousness (e.g., awareness, wakefulness, report ability,           accepting the null hypothesis, which is not allowed in the
etc.).                                                                 ANOVA or t-test statistics that are generally used. Like
   However, the concept of qualia is important for modeling            modeling, theories in Experimental Psychology are
because it cuts across the board and separates the issue of            generally altered and not rejected. In both cases it is possible
how information is processed from how it is subjectively               to construe theories that are falsifiable; it is just not very
experienced. By setting aside the issue of qualia we are               common. The criticism of modeling coming from
implicitly adopting the view that qualia is an epiphenomena;           Experimental Psychology has more to do with statistics.
that is, we can model the brain without considering qualia             Specifically, Experimental Psychology has a clear definition
because qualia has no functional significance (Dennett,                for defining when two conditions are significantly different.
1991). This is very convenient since it allows us to model             In contrast, the goal for a model is to show that it is
all aspects of the brain as information processing and ignore          significantly similar to a set of data and there is not an
or put off the problem of explaining why different brain               agreed upon standard for this (e.g., Roberts & Pashler,
functions feel qualitatively different from each other.                2000), although there are statistical ways to tackle the issue
   Alternatives to understanding consciousness as an                   (e.g., Stewart & West, 2010).
epiphenomena arising from information processing are                      Another issue arises from comparisons with Computer
scarce. Searle (1980) makes his arguments against strong AI            Science or Engineering where it is common to evaluate
by arguing that it leads to absurd consequences or                     algorithms against each other according to some clear
conclusions (the Chinese room is his most famous                       criterion or test set. According to this approach, cognitive
example), but he does not offer an alternative explanation.            models should be compared to see which one explains the
Hameroff & Penrose (1996) argue that normal information                data best. This can be done when the models are specifically
processing is inadequate to model human cognition and                  designed to model the same problem (see Erev et al., 2010
consciousness. They propose that the brain is capable of               for an example). However, it is not commonly done because
quantum computing and therefore a valid simulation would               models are designed with different goals in mind, therefore
require a quantum computer. Although this view is not                  a good test set for one might be an inappropriate or poor test
popular it should be noted as quantum computing is so far              set for another. It all depends on the goals and the
the only scientific alternative to normal computing,                   theoretical framework of the modeler, which is why it is
although, as Penrose concedes, it is still a type of                   important to be clear about these.
information processing. Chalmers (2010) has argued that if
you reject that consciousness arises from information                                            Conclusion
processing, the only option is to adopt some form of                   We have outlined a number of dimensions on which
dualism.                                                               modelers can take different views. Most of them are binary
                                                                       so it is possible to say agree, disagree, or agnostic. This list
                   Philosophy of science                               is not exhaustive, but being aware of where we stand on
Some people define science with Popper’s (1935) notion of              these issues can potentially avoid a lot of misunderstanding
falsifiability. However, although it is in theory possible to          and provide a richer view of the whole modeling enterprise.
falsify cognitive models, it is often the case that the failure           We have tried to be neutral in terms of laying out this list
of a model leads to changes in the model rather than a                 but we acknowledge that some people may feel that some of
rejection of the model. With unified architectures the                 the choices we have presented are invalid. For, example,
problem of falsification is trickier because in order to test          one could argue that there is no such thing as system levels
the architecture, it must be used to build a model of a task,          in the brain. Our point is that we should separate that
therefore, if it fails, it is unclear if the architecture has been     argument from the evaluation of modeling systems that
falsified or just the model. Newell (1990) realized this and           appear to embody systems levels.
argued that Lakatos’ definition of science (1970) was more                Another issue is the relationship between the different
appropriate than Popper's for understanding architectures.             dimensions that we have laid out. People tend to associate
Essentially, Lakatos defines science in terms of making                sets of beliefs with the use of different modeling systems.
progress over time, therefore if an architecture or model is           Possibly some of the dimensions we described are
improved through testing and refinement so that it explains            correlated and logically go together. However, arguments
                                                                       about whether certain dimensions are conceptually related
                                                                   2538

or conceptually independent should be separated from            McClelland, J. L. (2009). The Place of Modeling in
subjective impressions concerning the co-occurrence of            Cognitive Science. Topics in Cognitive Science, 1(1), 11-
dimensions across the users of different modeling systems.        38.
  In order to progress in understanding the various             Newell, A. (1973). You Can’t Play 20 Questions with
cognitive modeling spaces, the impacts of these and other         Nature and Win: Projective Comments on the Papers of
dimensions need to be further deliberated.                        this Symposium. In W. G. Chase (Ed.), Visual
                                                                  Information Processing: Proceedings of the 8th
                        References                                Symposium on Cognition (pp. 283-308). New York:
Anderson, J. R. (1993). Rules of the Mind (p. 336).               Academic Press.
  Lawrence Erlbaum.                                             Newell, A. (1980). Physical Symbol Systems. Cognitive
Anderson, J. R. (2007a). Using Brain Imaging to Guide the         Science, 4(2), 135-183.
  Development of a Cognitive Architecture. In W. D. Gray        Newell, A. (1990). Unified Theories of Cognition (pp. 1-
  (Ed.), Integrated Models of Cognitive Systems (pp. 49-          530). Cambridge, MA: Harvard University Press.
  62). New York, NY: Oxford University Press.                   Popper, K. R. (1935). The logic of scientific discovery
Anderson, J. R. (2007b). How Can the Human Mind Occur             (English tr.). New York, NY: Basic Books.
  in the Physical Universe? Science (p. x-290). Oxford          Roberts, S., & Pashler, H. (2000). How persuasive is a good
  University Press.                                               fit? A comment on theory testing. Psychological Review,
Anderson, J. R., & Lebiere, C. (1998). The atomic                 107(2), 358-367.
  components of thought (p. 504). NJ: Erlbaum.                  Saunders, G. M., Kolen, J. F., & Pollack, J. B. (1994). The
Anderson, M. L. (2010). Neural reuse: A fundamental               Importance of Leaky Levels for Behavior-Based AI. In D.
  organizational principle of the brain. Behavioral and           Cliff, P. Husbands, J.-A. Meyer, & S. W. Wilson (Eds.),
  brain sciences, 33(4), 245-66.                                  From Animals to Animats 3: Proceedings of the Third
Bickhard, M. H. (2010). Does Process Matter? An                   International Conference on Simulation of Adaptive
  Introduction to the Special Issue on Interactivism.             Behavior (Complex Adaptive Systems) SAB94. MIT Press.
  Axiomathes, 21(1), 1-2.                                       Searle, J. R. (1980). Minds, brains, and programs.
Chalmers, D. J. (2010). The Character of Consciousness.           Behavioral and Brain Sciences, 3(3), 417-457.
  Consciousness and Cognition (p. xxvii-596). Oxford            Simon, H. A. (1962). The Architecture of Complexity.
  University Press.                                               Proceedings of the American Philosophical Society,
Chater, N., & Vitányi, P. (2003). Simplicity: a unifying          106(6), 467-482.
  principle in cognitive science? Trends in cognitive           Stewart, T. C., & West, R. L. (2010). Testing for
  sciences, 7(1), 19-22.                                          Equivalence: A Methodology for Computational
Chomsky, N. (1995). Language and Nature. Mind,                    Cognitive Modelling. Journal of Artificial General
  104(413), 1-61.                                                 Intelligence, 2(2), 69-87.
Cooper, R. P. (2007). The Role of Falsification in the          Vera, A. H., & Simon, H. A. (1993). Situated Action: A
  Development of Cognitive Architectures: Insights from a         Symbolic Interpretation. Cognitive Science, 17(1), 7-48.
  Lakatosian Analysis. Cognitive Science, 31(3), 509-533.       Whitehead, A. N., & Griffin, D. R. (1931). Process and
Dennett, D. C. (1991). Consciousness Explained (p. 528).          Reality. Economica, (32), 251.
Eliasmith, C., & Anderson, C. H. (2003). Neural
  Engineering:      Computation,      Representation,   and
  Dynamics in Neurobiological Systems (p. 376).
  Cambridge, MA: MIT Press.
Erev, I., Ert, E., Roth, A. E., Haruvy, E., Herzog, S. M.,
  Hau, R., Hertwig, R., et al. (2010). A choice prediction
  competition: Choices from experience and from
  description. Journal of Behavioral Decision Making,
  23(1), 15-47.
Fodor, J. A. (1983). The Modularity of Mind: An Essay on
  Faculty Psychology (p. 154).
Hameroff, S., & Penrose, R. (1996). Orchestrated reduction
  of quantum coherence in brain microtubules: A model for
  consciousness. Mathematics and Computers in
  Simulation, 40(3-4), 453-480.
Lakatos, I. (1970). Falsification and the methodology of
  scientific research programs. In I. Lakatos & A. Musgrave
  (Eds.), Criticism and the Growth of Knowledge (pp. 91-
  196). Cambridge, UK: Cambridge University Press.
                                                            2539

