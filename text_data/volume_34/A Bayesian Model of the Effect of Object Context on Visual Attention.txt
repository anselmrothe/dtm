UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Bayesian Model of the Effect of Object Context on Visual Attention
Permalink
https://escholarship.org/uc/item/3h51q8kx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Allison, Ben
Keller, Frank
Coco, Moreno I.
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                     University of California

            A Bayesian Model of the Effect of Object Context on Visual Attention
                                                  Ben Allison (ballison@inf.ed.ac.uk)
                                                  Frank Keller (keller@inf.ed.ac.uk)
                                                 Moreno I. Coco (mcoco@inf.ed.ac.uk)
                                            Institute for Language, Cognition and Computation
                                              School of Informatics, University of Edinburgh
                                                10 Crichton Street, Edinburgh EH8 9AB, UK
                               Abstract                                     In this paper, we present a new model of object context
   Research in visual cognition has demonstrated that scene un-          based on a more complex notion of object label co-occurrence
   derstanding is influenced by the contextual properties of ob-         that makes use of latent (i.e., unlabeled and unobserved)
   jects, and a number of computational models have been pro-            scene types: the Latent Scene Type model. This model al-
   posed that capture specific context effects. However, a general
   model that predicts the fit of an arbitrary object with the con-      lows us to exploit the common structure of scenes in order to
   text established by the rest of the scene is until now lacking.       estimate reliable parameters even for infrequently occurring
   In this paper, we explain the contextual fit of objects in visual
   scenes using Bayesian topic models, which we induce from a            objects. We investigate two model variants: the first is La-
   database of annotated images. We evaluate our models firstly          tent Dirichlet Allocation (LDA, Blei et al. 2003), a standard
   on synthetic object intrusion data, and then on eye-tracking          model of word-topic co-occurrence, which we use to capture
   data from a spot-the-difference task and from an object naming
   experiment. For the synthetic data, we find that our models are       object-scene type co-occurrence. The second model variant
   able to detect object intrusions accurately. For the eye-tracking     is formulated as a Bayesian mixture of multinomials, which
   data, we show that context scores derived from our models are         assumes one latent scene type per scene (rather than one per
   associated with fixation latencies on target objects.
                                                                         object, as in LDA).
   Keywords: visual attention; object context; Bayesian model-
   ing; eye-tracking data.                                                  We test both model variants on the task of producing con-
                                                                         text judgments for objects in scenes. We first use a synthetic
                           Introduction                                  data set for evaluation (in this data, context objects have been
Real-world objects are often related to each other and typ-              artificially inserted). In the second evaluation study, we use
ically form a coherent scene. For example, a toothbrush is               our model to mimic the data from an eye tracking experi-
likely to occur with a tube of toothpaste, a mirror, a sink;             ment in which human participants had to spot out-of-context
it is unlikely to occur with a sauce pan, a salt shaker, a               objects. Finally, we demonstrate that our model can predict
cooker. For a given object, it is therefore possible to deter-           fixation latencies in an object naming experiment which in-
mine whether it is in context in a scene (toothbrush in bath-            cluded out-of-context objects.
room), or out of context (toothbrush in kitchen). Experimen-
tal evidence shows that context information facilitates human                                    Related Work
object recognition (Bar, 2004). In visual search tasks, eye fix-         To our knowledge, ours is the first model to attempt to quan-
ations are targeted towards contextually appropriate regions             tify the degree of fit between arbitrary objects in a scene, and
(Torralba et al., 2006), and out-of-context objects attract fixa-        to correlate the predictions of such a model with human be-
tions earlier than in-context objects (Underwood et al., 2008).          havior in scene viewing tasks. However, a number of mod-
   In computer vision, being able to detect out of context ob-           els have been proposed to capture context effects on visual
jects is useful for object labeling. The local detectors stan-           attention; a prominent example is the Contextual Guidance
dardly used for this task only consider the visual features              Model (CGM, Torralba et al. 2006), which combines bottom-
of the pixels within the bounding box of the object of inter-            up saliency with global scene information (scene gist, Oliva
est (Felzenszwalb et al., 2010). Local detectors are therefore           & Torralba 2006). The model is trained on a set of images in
prone to confusing objects that are visually similar (e.g., fork         which the target objects are labeled; from this data a prob-
and toothbrush). This problem can be addressed by comb-                  ability distribution of typical positions of objects is learned.
ing a local detectors with a model of object context, i.e., a            This distribution is conditioned on the scene gist, essentially
model that determines which objects occur together. While                a coarse-grained representation of global image features. Gist
this approach has been shown to increase object labeling                 is a latent variable in the model, comparable to scene type in
performance (Choi et al., 2010; Galleguillos et al., 2008),              our approach. The CGM has been evaluated on eye-tracking
the context models used are simple, typically relying on co-             data from visual search experiments, and can successfully
occurrence statistics over object labels. Furthermore, the con-          predict the scene-type-specific search behavior that partici-
text models used in computer vision are not designed to cap-             pants exhibit. However, the model is not specifically designed
ture human performance (e.g., in visual search). Therefore,              to detect out-of-context objects, and has not been evaluated
these models have not been evaluated on tasks such as detect-            on tasks that require an estimate of the contextual fit of an
ing out-of-context objects.                                              object.
                                                                     1278

   In the computer vision literature, the work closest to ours         describes a distribution over a vector of counts, which we call
in spirit, if not in ultimate task, is that of Choi et al. (2010).     o, such that oi is the count of the i-th object in the current
The authors use a generative model of images features, scene           image (note that for most images, most of these elements will
gist, the set of objects in the image, and their locations, to         be zero).
re-rank the output of a local object detector to respect contex-
                                                                       Latent Dirichlet Allocation There has been much interest
tual interactions between objects, and show an improvement
                                                                       in the use of topic models as descriptive tools, able to infer
over the baseline detector. The co-occurrence model used by
                                                                       structure in collections of documents, or other collections of
Choi et al. (2010) is a fairly simple binary tree of presence
                                                                       discrete entities (Blei et al., 2003; Wang et al., 2009). For
features whose principal purpose is to facilitate inference on
                                                                       the LDA model, the predictive distribution over new sets of
other aspects of the image.
                                                                       object labels is given by:
   The model of object context proposed in this paper is for-
mulated as a topic model. While topic models have been stud-                               Z
                                                                                                                                    !
ied extensively in both the language and vision literature,              pLDA (o|α, β) =       p (θ|α)  ∏ ∑ p (zn |θ) p (ln |zn , β)  dθ
they originate from applications to text, beginning with Latent                                          n zn
Dirichlet Allocation. LDA assumes a document is sampled                                                                                (1)
from a mixture of multinomials, where the multinomial from             where ln is the label for the n-th object in the image, and zn
which words are drawn is sampled once per word, and the                is the (latent) topic assignment for this label—counting the
mixture co-efficients are sampled once per document. A cor-            ln gives o as defined above. The zs are indicators explaining
pus is then a distribution over mixture co-efficients. This ap-        which latent scene type was used to generate the current la-
proach can be adapted fairly straightforwardly for modeling            bel. As in the original paper, α is the Dirichlet prior on θ, and
objects instead of words, and scenes instead of documents.             β is the topic–word probability matrix that gives the probabil-
However, we note that instead of being used as descriptive             ity of each object label in each topic. The above is evaluated
tools to provide insight into collections, in this paper we are        using a particle-filter-inspired Monte Carlo method described
interested in the predictive aspects of a topic model and want         by Wallach et al. (2009).
to test how well they correlate with human scene viewing               Mixture of Multinomials The mixture of multinomials
data. In this respect, while the models we used are standard,          model is defined over the same count vector o as above, but
the purpose for which we use them is novel and we derive               for this model the scene type z is sampled only once per scene.
new metrics to correlate with human behavior.                          The parameters to the model are φ (the mixture coefficients)
   Topic models have been applied to images in the computer            and θ (the parameters for the component multinomials). The
vision literature (Wang et al., 2009; Li et al., 2009), but rather     distribution over the observable variables, o, is:
than describing the sampling of object labels, these models
specify how discrete-valued image patches are sampled (by                                      p (o) = ∑ φz p (o|θz )                  (2)
quantizing continuous image features), and the relation be-                                            z
tween these patches and the labels applied to the image.
                                                                       where the distribution p (o|θz ) is a multinomial parametrized
              Models of Latent Scene Type                              by the vector θz (its components giving the probabilities of
This paper presents a model of context, and by extension con-          each possible label occurring within that component).
textual fit, which rests solely on the set of object labels in the        We explore two variants of this model—the first uses max-
image. The method we employ has two components: a distri-              imum a posteriori (MAP) estimation to fix the parameters to
bution over the set of labels in the scene, and the application        the (approximate) posterior mode—the single best estimate of
of such a model to a continuous measure of how well any ob-            model parameters. This can be done using EM, and we em-
ject fits with that scene. The observation of sets of objects is       ploy uniform priors on both sets of parameters. Conditioned
explained through latent scene types, which can be thought of          on some observations (training data, which we label D), the
as simple clusters of objects which are likely to co-occur. We         maximum likelihood method stipulates:
then use the predictive distribution over new sets of objects,
as derived from our latent scene type models, to determine                                   φ̂, θ̂ = arg max p (D|φ, θ)               (3)
                                                                                                             θ,φ
the fit of target objects to the scenes.                                                                         
                                                                                      pml (o|D) = p o|φ̂, θ̂                           (4)
A Model Of the Probability of a Set of Object Labels
We experiment with two models of the probability of a set              This predictive distribution (4) is what we are interested in:
of object labels, both of which are topic models. Topic mod-           exploring the probability of new scenes given our training
els comprise mixtures over multinomial distributions, where            data.
in this case the multinomial outcomes correspond to object                However, from a computational as well as cognitive per-
labels. The first topic model, Latent Dirichlet Allocation, is         spective, given only limited samples from the process we
fairly standard, while the second one, the mixture of multi-           should feel uneasy about saying with any certainty what the
nomials model, is less commonly used. Each of the models               values of the parameters are. Instead, we suggest that given
                                                                   1279

our experience we have beliefs about what is likely to hap-            That is, the probability of the count vector which includes the
pen, but we retain uncertainty and factor this in to our pre-          new label o0 , normalized by the probability of the context for
dictions. In light of this, we also employ a Bayesian version          all possible objects which could be added to the scene.
of this model, which integrates over the full parameter space             To determine which of o1 and o2 better fits some context o
given our training data D:                                             we can compare p o1 |o and p o2 |o computed as in (6)—
                           Z                                           we may simply interested in which of these is the larger, or
           pbayes (o|D) =     p (φ, θ|D) p (o|φ, θ) dφ, dθ    (5)      perhaps in the ratio between these two quantities. Note that in
                                                                       either case, the normalizing constant can be dropped since it
For the mixture of multinomials model we cannot evaluate               is common to both (this speeds up computation considerably).
this integral in closed form, so we sample mixture models                 Secondly, to determine whether o0 is in context or not, we
from their posterior p (φ, θ|D)—i.e., we retain uncertainty            can compare p (o0 |o) with the quantity obtained by marginal-
about which model best explains our data, and average over             izing out the extra object, namely:
this uncertainty in deriving our predictions. Assuming Dirich-
let priors (in our case, uniform) on φ and θ leads to Dirichlet                          p (onew |o) = ∑ p (on ) p (on |o)            (7)
posteriors over these same parameters, conditioned on assign-                                          on
ments of training observations to latent mixture components;
it is these assignments that we sample. We then evaluate the           where p (on ) is the probability of on occurring in any scene,
p (o|φ, θ) at each of these sampled points; in practice we do          for which we use simply the fraction of all objects across all
not sample different mixture models for each new o we wish             scenes which are on . For this paper, we explore both the de-
to evaluate, but run the sampler once in training and store all        cision problem (is p (o0 |o) > p (onew |o), i.e., is the object in
mixture models sampled. This allows us to simply average               context or not) and the continuous scores derived as above.
over the sampled components for the predictive distribution,
leading to a deterministic evaluation of (5) as simply the mean        Evaluation on Synthetic Out-of-context Objects
of the probability p (o|φ, θ) under the sampled models.                We construct our first test set based on the Spatial Envelope
    As a final note for all these models, while inference tech-        data set (Oliva & Torralba, 2001). Here, the models will be
niques are approximate in all cases, and different between the         used to determine whether an object is in context with re-
MoM and LDA models, we are confident that the particu-                 spect to the rest of a scene, or not (Equation (7)). The im-
lar approximations do not overly sway the models’ ultimate             ages in the data set contain full object annotations, but also
performance. While using heldout probability as the metric             scene type labels. These allow us to construct test data for the
of concern show disparities between different approximations           scenario we are interested in. (Note, however, that this is the
for the LDA model in Wallach et al. (2009), our uses of the            only use of overt scene type labels in this paper; the scene
models are different. Most of the problems we consider are             types in our model are latent.) The data set is annotated using
decision problems where the exact probability is less of a             LabelMe conventions, but does not overlap with the LabelMe
concern than the relative probabilities of the scenes under two        data from which our models are estimated. In terms of ob-
models, and in the final section we are interested in the cor-         jects per scene, there are on the order of ten objects in each
relation between the probabilities and some other continuous           image, and the number of images is reasonably balanced be-
measure which is unlikely to be affected by (relatively) small         tween scene types. We extract scenes which are either rural or
changes due to approximation error.                                    urban (the two top level scene types). We produce frequency
                                                                       counts of objects within these two categories, and compute
Detecting Out-of-context Objects with Scene
                                                                       a χ2 statistic for each to measure the distinctiveness of that
Probability
                                                                       object in that class. We then select the 25 most distinctive ob-
The previous section presented models which have been used             jects for each class which occur in at least ten scenes, and
previously in other fields for describing the co–occurrence of         extract all scenes containing each of these objects. These dis-
entities. We turn in this section to the manipulation of these         tinctive objects are treated as the targets, and the other objects
models to derive quantities which we will correlate with hu-           in the image form the contexts.
man performance.                                                          The original scenes form examples of in-context objects—
    There are two distinct tasks we explore in this paper: which       to produce out-of-context ones, for each scene we replace the
of two objects is more probable given a scene, and whether             in-context target with a randomly selected member of the dis-
a given object belongs to a scene or not. Here, we briefly de-         tinctive list for the other category. This produces a set of just
scribe the use of the models we defined in the previous section        over 26,000 scenes, equally balanced between in- and out-of-
to achieve these tasks.                                                context objects, to use for further experimentation. We divide
    Firstly, the conditional probability of some object (label) in     this into 6,000 scenes for development (model selection and
question o0 given a set of object labels o (where o is the count       parametrization), with the remainder being used for held-out
vector as above) is:                                                   testing. In all cases, the held-out data are unobserved until all
                                    p (o ∪ o0 )                        model parameters are fixed. Table 1 shows examples of the
                   p o0 |o =
                          
                                                              (6)      data we produce.
                               ∑onew p (onew ∪ o)
                                                                   1280

         Target            In/Out Context                                             Context
          stone                   in                      stick:1 stone:1 tree trunk fallen:2 trees:1 ground:2 brushes:1
       buildings                  in               skyscraper:1 building occluded:2 buildings:1 sky:1 skyscraper occluded:1
          road                   out              tree:1 stone:3 river water:1 trees:1 field:1 sky:1 stones:2 rocky mountain:1
                                                window:11 car occluded:2 pot plant occluded:1 sidewalk:1 person occluded:1
       sea water                 out                   arcade:1 palm tree:1 car:1 window occluded:1 person walking:3
                                                        person woman walking:1 traffic light:1 hall:1 building:1 road:1
Table 1: Some examples of the synthetic data—the context is depicted as a sparse vector over the outcomes in the form [la-
bel:count], which is then reduced as appropriate for a trimmed vocabulary
               LDA             ML-MoM               B-MoM            lower probabilities than in-context objects in their data set.
  |T |    500V     1000V     500V     1000V     500V     1000V          Underwood et al.’s study contains 80 pairs of scenes. In one
                                                                     scene in the pair, the target object is in context (congruent, in
   50     0.737    0.747     0.674    0.679     0.896    0.895       the language of that paper) and in the other it is out of context.
  100     0.759    0.801     0.660    0.662     0.897    0.899       (Saliency was also manipulated in the study, but this is not of
                                                                     interest here.) We manually listed the objects in each scene
Table 2: Accuracy (proportion of decisions where the correct         (the contexts are identical between pairs, and two pairs are
determination is made) on the synthetic data                         identical save for their targets). Checking the labels against
                                                                     our LabelMe training data revealed that 25% of target objects
                                                                     were observed in LabelMe, and just over 30% of all objects.
Results Table 2 shows results on the synthetic dataset. The
                                                                     LabelMe contains mainly outdoor scenes, while the experi-
Bayesian Mixture of Multinomials is clearly superior to the
                                                                     mental data set are all indoor scenes, predominantly kitchen,
other two models, and the larger vocabulary size and greater
                                                                     utility room or bathroom scenes, in which the objects have
dimensionality improves this slightly. The LDA model shows
                                                                     been carefully arranged.
greater sensitivity to parametrization than the other two, and
                                                                        We therefore iteratively relabeled the target objects to es-
the maximum likelihood model is considerably worse than
                                                                     tablish a closer match with the LabelMe database, choosing in
the others across all parameter settings. Of particular note is
                                                                     some cases synonyms and in others (direct) hypernyms. This
the maximum likelihood model getting worse as the dimen-
                                                                     was a manual process which relied on linguistic resources
sionality increases; this is a classic result for non-Bayesian
                                                                     such as WordNet. This produced a target coverage rate of just
models, where as the parameter space expands it is less and
                                                                     over 70%, making it possible to use 45 of the 80 scene pairs,
less well summarized by a single point (the mode) and that
                                                                     with each scene having on average approximately 60% of its
mode becomes harder to find.
                                                                     context object appear in the training data (note that this in-
        Modeling Human Experimental Data                             crease was incidental, as we optimized the coverage of the
                                                                     target objects and simply propagated corrections through to
The evaluation study presented in the previous section used          contexts as well so as to reduce the amount of manual engi-
artificially generated data. It showed that the Latent Scene         neering). The selected scenes contain an average of ten ob-
Type Model is highly accurate at detecting out-of-context ob-        jects in total.
jects which have been inserted into a scene. In the present             Given the small size of the test set, we were not able to
study, we validate this result using a data set from an eye          split off a separate development set, and therefore retained
tracking experiment by Underwood et al. (2008). In this ex-          the parameters as set in the previous section on the synthetic
periment, participants had to perform a search task (determine       data set.
whether two scenes are the same or different); in the different-
scene condition, the target object was either out of context or
in context, with saliency being controlled. An example pair of       Results Table 3 shows results on the Underwood et al. data
scenes can be found in Figure 1. The results show that scenes        for the task of detecting which of two possible objects is out
with in-context objects are inspected for longer and received        of context. As in previous sections, we note that the Bayesian
more fixations than scenes with out-of-context objects. Also         version of the mixture of multinomials model performs better
the in-context objects themselves were detected later and re-        than the maximum likelihood version of the model, but given
quired more fixations prior to detection than out-of-context         the small dataset it is not possible to compare the B-MoM and
objects.                                                             LDA models except to say that both are significantly differ-
   We expect our Latent Scene Type Model to capture the              ent from a random (50%) baseline as established by a bino-
behavioral effect of out-of-contextness demonstrated by Un-          mial test. Note that while seeming disappointing initially, the
derwood et al.’s study: out-of-context objects should receive        performance of the models here is limited because the Un-
                                                                 1281

           Method       LDA        ML-MoM       B-MoM
          Accuracy      31/45       24/45        29/45
Table 3: Proportion of scenes in the Underwood et al. (2008)
data where the correct determination was made. The LDA
and B-MoM models are significantly different from a random
(50%) baseline, but not one another
                                                                                             (a) In context target
Figure 1: A pair of example scenes from the eye tracking ex-
periment of Underwood et al. (2008). The target object in the
left hand image is the sock (in context), while in the right
hand image it is the can of soup (out of context)
derwood et al. scenes are staged indoor shots featuring many                                  (b) Out of context
objects that occur infrequently, if at all, in our training data
(only 60% of context objects appeared at all). The next sec-          Figure 2: An example of a scene with an in-context target
tion presents an evaluation where objects are more frequently         (cup) and the same scene with an out-of-context target (fish)
observed.
        Modeling an Object Naming Dataset
The third evaluation of our models used eye-tracking data
from an object naming experiment by Coco et al. (2012).               analysis to investigate how first fixation latency (the depen-
In this study, 24 participants were presented with 28 photo-          dent measure) correlates with model score (our predictor).
realistic scenes and asked to name the five most important            LME is more appropriate than simple correlation because
objects in the scene. In each scene, an object of interest and        there were many other factors considered in the experimen-
two competitors were inserted using Photoshop. The Saliency           tal data which affect the dependent measure, including fre-
(Salient, Non-Salient) and Contextual Fit (In-Context, Out-           quency and saliency of objects in the scene and size of the
of-Context) of the object of interest was manipulated. In con-        objects. Employing LME means we are able to control for
trast to Underwood et al. (2008), this study shows that out-          these factors by including them as covariates in the analysis.
of-context objects are less likely to be named than in-context
objects. Moreover, first fixation latency, i.e., the time to land        On the basis of the experimental data, we expect the model
on a target object for the first time from scene onset, is longer     score to be negatively associated with first fixation latency,
for out-of-context than for in-context objects. A naming task         i.e., the more out-of-context an object is, the longer it takes to
demands a joint evaluation of both linguistic and visual infor-       fixate it. Together with the Score, we include as predictors the
mation, thus even if an out-of-context object might be visu-          Saliency of the object, and the type of Model. As a random
ally more informative, it is linguistically less relevant.            effect, we include Scene. We residualize first fixation latency
   We first evaluate our models on the task of determining            by the area of the object (in pixel square) to reduce the effect
which of two objects is in context, identical to that presented       of area on the dependent measure. We select the final LME
in the previous section. Then, we investigate whether the con-        model by following a forward step-wise procedure, where
textual scores calculated by the models are correlated with the       nested models are compared on the basis of log-likelihood
visual responses observed on the associated objects. We em-           improvement. In the following, we report the coefficients of
ploy linear mixed effects model (LME, Baayen et al. 2008)             the predictors found significant after model selection.
                                                                  1282

            Method       LDA      ML-MoM       B-MoM                  faced by a cognitive process with access to limited data.
           Accuracy     46/56       33/56       50/56                    We showed that the Latent Scene Type models perform
                                                                      well on the task of detecting out-of-context objects in a syn-
Table 4: Proportion of scenes in the Underwood data where             thetic dataset. Furthermore, we successfully applied the mod-
the correct determination was made. The LDA and B-MoM                 els to two eye-tracking datasets, one involving a spot-the-
models are significantly different from a random (50%) base-          difference task, the other involving object-naming. In both
line, but not one another                                             cases, the models were able to successfully detect out-of-
                                                                      context objects, and in the case of the naming data, we also
               LDA              ML-MoM              B-MoM             showed that model scores are associated with first fixation la-
             I        O         I       O         I        O          tencies on a target object (either in or out of context).
   S      0.0106 0.0001 0.0010 0.0010 0.0071 0.0006                                         Acknowledgments
  NS      0.0064 0.0002 0.0010 0.0010 0.0054 0.0012                   The work reported here was funded by the European Re-
                                                                      search Council under award number 203427 “Synchronous
Table 5: Average context scores across the conditions—I is            Linguistic and Visual Processing”.
in context, O out of context, S is the salient condition and
NS is the non-salient condition. ML-MoM scores are in fact                                       References
slightly different to one another, but both contexts are highly       Baayen, R., Davidson, D., & Bates, D. (2008). Mixed-effects mod-
improbable under the model                                               eling with crossed random effects for subjects and items. Journal
                                                                         of Memory and Language, 59, 390-412.
                                                                      Bar, M. (2004). Visual objects in context. Nature Reviews Neuro-
                                                                         science, 5, 617–629.
Results We first present the results for the decision prob-
lem. There are fifty-six decisions to be made (28 pairs of            Blei, D., Ng, A., & Jordan, M. (2003, March). Latent Dirichlet
                                                                         Allocation. Journal of Machine Learning Research, 3, 993–1022.
scenes, each in the salient and non-salient condition), with
                                                                      Choi, M., Lim, J., Torralba, A., & Willsky, A. (2010). Exploiting
the goal being to determine which of the pair is out of con-             hierarchical context on a large database of object categories. In
text. Table 4 shows the results on this task, where we once              Proceedings of cvpr’10 (p. 129-136).
again see that the LDA and Bayesian models are significantly          Coco, M. I., Malcolm, G. L., & Keller, F. (2012). The interplay of
above chance, but given the limited sample size not signif-              bottom-up and top-down mechanisms in visual guidance during
                                                                         object naming. Journal of Vision. (under review)
icantly different from one another. Table 5 shows the mean
context scores across the conditions for each of the three mod-       Felzenszwalb, P., Girshick, R., McAllester, D., & Ramanan, D.
                                                                         (2010, September). Object detection with discriminatively trained
els, where we see that the effects of the models in the decision         part-based models. Pattern Analysis and Machine Intelligence,
problem (Table 4) are equally visible on the continuous scale.           32(9), 1627 -1645.
   When using the LME to check whether model score is a               Galleguillos, C., Rabinovich, A., & Belongie, S. (2008). Object cat-
predictor of first fixation latency, we find a significant effect        egorization using co-occurrence, location and appearance. In Ieee
                                                                         conference on computer vision and pattern recognition (cvpr).
with βScore = −0.1309; p < 0.0001: the more in-context an                Anchorage, AK.
object is, the shorter the latency. We do not find an effect of       Li, L.-J., Socher, R., & Fei-Fei, L. (2009). Towards total scene
Saliency and Model. This result also echoes the experimental             understanding:classification, annotation and segmentation in an
finding obtained by Coco et al. (2012), and shows that the               automatic framework. In Proceedings CVPR’09.
scores generated by our models can capture the patterns in            McCallum, A. (2002). MALLET: A Machine Learning for Language
the eye-movement responses.                                              Toolkit.
                                                                      Oliva, A., & Torralba, A. (2001, May). Modeling the shape of the
                     General Discussion                                  scene: A holistic representation of the spatial envelope. Interna-
                                                                         tional Journal of Computer Vision, 42, 145–175.
This paper introduced the Latent Scene Type models for de-            Oliva, A., & Torralba, A. (2006). Building the gist of a scene: the
scribing the fit of objects to scenes. Our models quantify how           role of global image features in recognition. In Progress in brain
well a target object fits an observed context (the other objects         research (p. 2006).
in the image). Sets of objects are generated by latent scene          Torralba, A., Oliva, A., Castelhano, M., & Henderson, J. M. (2006).
                                                                         Contextual guidance of attention in natural scenes: The role of
types, with scene types representing objects which tend to               global features on object search. Psychological Review, 113(4),
co-occur. We choose a Bayesian formulation for our models,               766–786.
as this is attractive from a cognitive point of view: a cog-          Underwood, G., Templeman, E., Lamming, L., & Foulsham, T.
nitive process operates with finite experience, which means              (2008). Is attention necessary for object identification? evidence
that it has to estimate a model of the world based on a limited          from eye movements during the inspection of real-world scenes.
                                                                         Consciousness and Cognition, 17, 159–170.
sample (in our case of context and objects). Committing to
                                                                      Wallach, H. M., Murray, I., Salakhutdinov, R., & Mimno, D. (2009).
a single parameter setting based on a limited sample is dif-             Evaluation methods for topic models. In Proceedings ICML’09
ficult; it therefore seems more plausible to integrate over the          (pp. 1105–1112). New York, NY, USA: ACM.
full parameter space, which is the hallmark of Bayesian mod-          Wang, C., Blei, D., & Fei-Fei, L. (2009). Simultaneous image clas-
els. The Bayesian approach therefore captures the uncertainty            sification and annotation. In In proceedings CVPR’09.
                                                                  1283

