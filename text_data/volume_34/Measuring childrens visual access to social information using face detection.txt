UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Measuring children's visual access to social information using face detection
Permalink
https://escholarship.org/uc/item/3312b2nr
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Author
Frank, Michael
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                     University of California

      Measuring children’s visual access to social information using face detection
                                                             Michael C. Frank
                                                           mcfrank@stanford.edu
                                                           Department of Psychology
                                                              Stanford University
                               Abstract                                   the child is attending to (Franchak, Kretch, Soska, Babcock,
                                                                          & Adolph, 2010).
   Other people are the most important source of information in
   a child’s life, and one important channel for social information          Of particular interest is the result, reported by Franchak et
   is faces. Faces can convey affective, linguistic, and referential      al. (2010), that 14-month-olds rarely fixated their mother’s
   information through expressions, speech, and eye-gaze. But             face, even when she spoke to them directly. They looked in-
   in order for children to apprehend this information, it must
   be accessible. How much of the time can children actually              stead at her hands or other parts of her body. The authors
   see the faces of the people around them? We use data from              speculated that this result might have been due to the mother’s
   a head-mounted camera, in combination with face-detection              location, usually high above the child. When mothers were
   methods from computer vision, to address this question in a
   scalable, automatic fashion. We develop a detection system             sitting down, their faces were much more visible to their chil-
   using off-the-shelf methods and show that it produces robust           dren. In our current investigation we follow up on this sug-
   results. Data from a single child’s visual experience suggest          gestion, investigating the possibility that the posture of care-
   the possibility of systematic changes in the visibility of faces
   across the first year, possibly due to postural shifts.                givers and the infant’s own posture work together to cause
   Keywords: Social development; face processing; head-                   developmental changes in the accessibility of social informa-
   camera.                                                                tion.
                                                                             The introduction of these new methods mean that for the
                           Introduction                                   first time, we can see what babies are looking at as they in-
Faces are perhaps the most important source of social infor-              teract with—and learn from—the people around them. This
mation for young children. Infants show a preference for                  development opens up many new questions for investigation.
faces and face-like configurations from birth (Johnson, Dz-               Yet work of this type is hindered by the tremendously slow
iurawiec, Ellis, & Morton, 1991; Farroni et al., 2005), and               and resource-intensive task of manually annotating videos,
they will fixate faces to the exclusion of nearly everything              frame by frame. Up until now, only a few research groups
else when attending to complex naturalistic stimuli (Frank,               have grappled with the task of how to analyze the massive
Vul, & Johnson, 2009; Frank, Vul, & Saxe, 2011). By their                 datasets captured using these methods.
first birthday, they are sensitive to facial information about               The current study thus serves two purposes. First, it is de-
emotion (Cohn & Tronick, 1983) and social group (Kelly et                 signed to measure the accessibility of social information—in
al., 2005), and they will readily follow gaze to an attended              the form of faces—to infants. To investigate this question
target (Scaife & Bruner, 1975). As they begin to speak and                across development, we make use of a previously-described
understand language, joint attention becomes a powerful cue               dataset (Aslin, 2009), in which a head-mounted camera
for learning the meanings of words (Baldwin, 1991).                       recorded 2 – 3 hours of the visual experience of a single child
   To extract all of this important information in the natu-              at ages 3, 8, and 12 months (sample frames shown in Figure
ral environment, infants and children must attend to people’s             1). Second, we investigate the possibility of using automated
faces. Nearly all of what we know about children’s atten-                 face detection to measure social information. It might in prin-
tion to—and understanding of—faces comes from tightly-                    ciple be possible to hand-annotate the presence of faces in
controlled lab experiments. In such experiments, the stimuli              each of the million-odd frames in our dataset (such annotation
are typically presented in a very accessible format: at eye-              can be done around 4–8 times slower than real-time, yielding
level, large enough so that all details can be appreciated. How           around 25–50 hours of total annotation time). For any larger
often do children actually see the faces of the people around             study with more participants, annotation costs would quickly
them, though? And how often are the faces large enough to                 become prohibitive. Our study thus was designed to serve as
discern details from?                                                     proof-of-concept for the automated strategy.
   Head-mounted cameras provide a new technique for mea-                     Detection of upright faces in static images is widely con-
suring access to faces during development. While the method               sidered to be a solved problem in computer vision, with the
of placing a miniature camera on the head of an infant or                 work of Viola and Jones (2004) providing a computationally-
young child is still relatively new, a number of investiga-               efficient solution that is now used in a wide variety of systems
tors have begun using it to record children’s first-person per-           and consumer electronics. Nevertheless, the dataset we used
spective (Yoshida & Smith, 2008; Aslin, 2009; Smith, Yu,                  presents a distinct set of challenges for such methods. In what
& Pereira, in press). Some studies have even used head-                   follows, we describe our method for handling these chal-
mounted eye-trackers to measure what part of the visual scene             lenges using a collection of out-of-the-box techniques from
the child is fixating, a good proxy for what parts of the world           computer vision and machine learning. We end by describing
                                                                      342

                   3 months                                8 months                                 12 months
Figure 1: Sample frames showing head-camera data plotted along with face detector data. A separate rectangle is plotted for
each active detector. Frames were selected in which annotations and model predictions matched.
developmental changes in the prevalence and size of faces
in the field of view of the infant we studied. These changes        frame, training the post-processing model that the frame con-
suggest that there may be a number of important factors in-         tained a face would be counterproductive. Instead, our strat-
fluencing the accessibility of social information during early      egy was to create two annotated sets. The first was a training
development.                                                        set that indicated whether, for each frame, the detectors had
                                                                    correctly identified a face. The second was a generalization
                          Methods                                   dataset that indicated whether a face was in fact present in the
                                                                    frame, allowing us to test what proportion of faces our models
Although in principle a single joint detection and tracking
                                                                    identified on a completely independent dataset (different clips
system could be constructed to detect faces in head-camera
                                                                    from the same corpus). In addition, we annotated the child’s
video, in practice such as system would be complex and
                                                                    posture in each video of the corpus. These annotations (along
computationally-intensive. Thus, we pursued a two-step ap-
                                                                    with the details of the dataset) are described below.
proach to face-detection (Figure 2). We first preprocessed
each frame of our data separately using simple but noisy de-        Data and annotation
tectors, which find faces in static images. We then tested a
number of supervised post-processing models on their perfor-        Aslin (2009) head-camera dataset Data for the study con-
mance in picking frames with successful rather than spurious        sisted of videos collected on three days during the infancy
detections.                                                         of a single child, at ages 3, 8, and 12 months. This dataset
   Because of this two-step scheme, conventional annotations        was originally collected by Aslin (2009); the data are de-
of a gold standard training sample (e.g. face/no face) were not     scribed in detail in Cicchino, Aslin, and Rakison (2010). The
maximally effective. If the detectors did not find a face in a      method of collection was a small wireless camera mounted
                                                                343

                                                            Activities                                                                           Postures                   3 months
                150 Frame
            Frame                             Frame                      Frame                                                   150                                        8 months
              1       2                         3                ...       N
                                                                                                                                                                            12 months
                        100                                                                                                      100
              minutes                                                                                                  minutes
             Haar             Haar             Haar
                                                Haar                      Haar
              Haar
                Haar           Haar
                                 Haar             Haar                     Haar
                                                                             Haar
             filter
                  Haar        filter
                                   Haar        filter
                                                    Haar
                                                 filter                   filter
                                                                               Haar
               filter
                 filter         filter
                                  filter           filter                   filter
                                                                              filter
                       50
                   filter           filter           filter                     filter                                            50
                          0                    CRF                                                                                 0
                                                                                                                                       held   lying    sitting
                                                                                                                                                                 standing
                                car    crib
                                               crying   eating
                                                                  held
                                                                         outing
                                                                                  play
                                                                                         reading   walking
                                             sequence
                                               model
Figure 2: Schematic of our face-processing approach. Step 1:                                                         Figure 3: Time spent in each coded posture at each age.
process frames with noisy Haar-style detectors. Step 2: filter
detections with conditional random field (CRF) model.
                                                                                                                   likely to be useful for inferring eye-gaze direction, emotional
on the infant’s head, allowing recording of a large portion of                                                     state, or other social information.
the infant’s visual field. The camera was a Sony 480TVL                                                            Posture annotation We additionally annotated the posture
CCD “bullet” camera, embedded in a headband and wire-                                                              of the child during the videos, in order to use this factor in our
lessly transmitted to a digital video recorder. Videos were                                                        analysis of position and size of detected faces. We attempted
approximately 126, 190, and 140 minutes long for the seg-                                                          to estimate the child’s posture wherever possible, categoriz-
ments collected at 3, 8, and 12 months, respectively. Record-                                                      ing it as lying, sitting, standing, crawling, or being held. Fig-
ings were made while the infant was in a number of different                                                       ure 3 shows descriptive data for this measure. Annotation of
locations, including in the home, on a shopping trip, on a                                                         this measure was somewhat subjective, but inter-rater agree-
walk, and at a playgroup. Due to the variation in activities                                                       ment was relatively high with κ = .72 for five categories.
across ages, the natural statistics of these three samples were
unmatched (likely due to both sampling issues and true dif-                                                        Models
ferences in the distribution of activities across ages); thus we                                                   Although face detection is generally considered to be a solved
will not attempt to compare across activity types.                                                                 problem (Viola & Jones, 2004), face detection in develop-
Annotation of detectors (training set) We annotated a                                                              mental, first-person data presents a number of challenges
sample of videos to provide training data for our models. For                                                      that do not usually occur in static photographs or standard
this annotation effort our goal was to select frames in which                                                      videos. First, faces are often occluded and at odd orientations
the raw face detectors had correctly selected a face (and reject                                                   for children. Second, in our case, the video was transmit-
those for which the detections were incorrect). We classed a                                                       ted wirelessly and contained some artifacts due to the trans-
frame as containing a correct detection if there was at least                                                      mission method. Third, the head-mounted camera was sub-
one detector around the face of a person (thus a frame could                                                       ject to quick movements as the child moved his head, mean-
still contain some spurious detections, though in practice this                                                    ing that many methods applicable for scene segmentation or
was relatively rare). We annotated nine clips of one minute                                                        motion tracking in static-camera applications could not be
each (16k frames). Three minute-long clips were selected for                                                       used here. Our modeling goal in this project was to com-
each age group randomly, with the caveat that they included                                                        bine computationally-inexpensive techniques to address these
some correct face detections in each.                                                                              challenges.
                                                                                                                      Our preprocessing step made use of off-the-shelf Haar-
Annotation of face presence (generalization set) We ad-                                                            style detectors from the OpenCV package (Bradski &
ditionally performed frame-by-frame annotations of whether                                                         Kaehler, 2008). Each frame was processed with four separate
a face was present in the video frame. We selected 3–4 one-                                                        detectors: three full-face and one profile detector. These de-
minute clips at each of the three ages for a total of 11 min-                                                      tectors were noisy, capturing many faces but also spuriously
utes of video at 30 frames per second (20k frames). One-                                                           identifying many background elements as faces as well (e.g.
minute clips were selected randomly, again with the caveat                                                         doorknobs, high contrast windowpanes, see Figure 4). This
that they needed to contain at least some instances of faces.                                                      processing step ran at approx. 10% of real time on a quad-
We counted a frame as containing a face when a face was fully                                                      core machine, taking around 4 days to process all detectors.
visible with no occlusions at three-quarter view or greater                                                           Next, we trained post-processing models to discriminate
(both eyes visible). This stringent annotation criterion was                                                       valid detections from invalid detections, using our detector-
used because occluded or profile-view faces are much less                                                          annotated training set. Our primary model of interest was a
                                                                                                             344

                                                                       Table 1: Model performance on detector-annotated training
                                                                       dataset (“Tr,” 9 minutes, only frames with successful de-
                                                                       tections) and generalization dataset (“Gen,” 11 minutes, all
                                                                       frames with human-visible faces). P = precision, R = recall,
                                                                       F = F-score (harmonic mean of precision and recall).
                                                                                    Tr P   Tr R     Tr F  Gen P     Gen R     Gen F
                                                                          NB         .64    .82     .72     .76      .55       .64
                                                                          HMM        .72    .85     .78     .81      .57       .67
                                                                          CRF        .85    .77     .81     .85      .53       .65
                                                                       Table 2: CRF model performance on generalization training
                                                                       set by age. Prop. faces refers to the proportion of total faces
                                                                       in the gold-standard dataset for that age.
Figure 4: Frames in which CRF model incorrectly predicted                                    3 months    8 months    12 months
that the detectors had correctly identified a face.                           Precision         .92         .89           .33
                                                                              Recall            .60         .48           .35
                                                                              F-score           .73         .62           .34
                                                                              Prop. faces       .46         .45           .07
conditional random field (CRF) model (Lafferty, McCallum,
& Pereira, 2001). CRFs are discriminative sequence mod-                                            Results
els: they take input data of sequences of observations (with
some feature set describing each observation) and return a             Table 1 shows evaluation results for each of the models on
classification of each observation in the sequence. Their key          the two datasets we annotated, the detector-annotated training
difference from feature-based classifiers (e.g. Naive Bayes or         dataset and the gold-standard generalization dataset. (Rather
MaxEnt) is their ability to use sequential information; like-          than using a technique like cross-validation to test generaliza-
wise, their key difference from sequence models (e.g. hidden           tion performance, we report results on both the training data
Markov models) is their ability to incorporate rich featural           and an independent generalization set that was never used
information about each observation. They have been applied             for training). The CRF model performed best on the train-
successfully to a number of tasks including natural language           ing data, capturing a slightly better tradeoff between preci-
processing and computer vision. For this application, we used          sion and recall. The gain in performance was relatively slight
the Matlab CRF toolbox (Schmidt & Swersky, 2008).                      from the HMM to the CRF, indicating that the majority of
                                                                       the value of the CRF was due to the sequential dependencies
   We included two other simpler models for comparison:                enforced by the model. Knowing that a previous frame con-
a Naive Bayes (NB) classifier and a hidden Markov model                tained a successful detection was helpful in deciding whether
(HMM). The classifier made use of exactly the same feature             the current one did as well.
set but considered each frame in isolation (neglecting sequen-
                                                                          When we applied the three models to the generalization
tial dependencies). The HMM considered only the sequence
                                                                       dataset, F-scores were within a small range of one another,
of decisions and the number of detectors that were active.
                                                                       with the HMM outperforming the CRF, perhaps indicating
Thus, the difference in performance between the CRF and
                                                                       some overfitting of feature weights to the training data. Nev-
the classifier provides a rough measure of the contribution of
                                                                       ertheless, the CRF produced the highest precision on the gen-
sequential information (provided by the video), while the dif-
                                                                       eralization dataset. Because our aim was to measure the quan-
ference between CRF and HMM provides an estimate of the
                                                                       tity and spatial distribution of faces at each age, we judged
gain due to adding featural information.
                                                                       precision more valuable than recall and chose the CRF model
   We created a set of binary features to describe the detec-          for our analysis (though we note that results do not change
tions in each frame. These included a separate feature for             meaningfully if the other models are chosen).
whether each detector was active, a feature for each detec-               Performance on the generalization set was highly asym-
tor pair to indicate whether the detector centers fell within a        metric across the three ages, with high precision and recall
certain threshold (5 pixels) of one another, and features for          for the 3-month data, mid-level performance on the 8-month
each detector indicating whether it changed in size or disap-          data, and very low performance on the 12-month data (Table
peared in either the preceding or following frame. We used             2). A number of experiments attempting age-specific training
this feature set to train the models to classify the training data     failed to find major gains in performance by training only on
as containing correct or incorrect detections.                         e.g. 12-month data. There were few faces in the 12-month
                                                                   345

                                                      3 months                  8 months                                    12 months
Figure 5: Heat maps showing probability of finding a face in each location of the camera field for 3, 8, and 12-month-old data.
Dotted lines show the vertical locations of maxima.
                                                                                        months than at 3 months. This shift could potentially be due
      Proportion frames with face detections
                                               0.15
                                                                                        to postural differences: the child was more likely to be held
                                                                                        or lying down at 3 months and more likely to be sitting or
                                                                                        standing at 8 and 12 months. In a sitting or standing position,
                                                0.1
                                                                                        faces tend to be higher in the visual field than when lying
                                                                                        down and looking up over the edge of the crib.
                                                                                           Faces were also different sizes in the older videos. The
                                               0.05
                                                                                        3-month videos had a qualitatively different distribution of
                                                                                        detected face sizes (Figure 7). We cannot completely rule out
                                                                                        the possibility that some of the smaller faces in the 8- and 12-
                                                 0
                                                      3mos       8mos   12mos           month videos were spurious detections. Nevertheless, the rel-
                                                                 Age                    atively similar distribution for each of these (compared with
                                                                                        the drop in precision from 8 to 12 months) suggests that de-
Figure 6: Proportion of faces detected by CRF model at each                             creasing precision of detections was not the only factor here.
age. Error bars show standard error across video clips.                                 Though speculative, a postural explanation for the shift in
                                                                                        size might also be proposed: at older ages, the child was less
                                                                                        likely to be lying or being held close to the face of a care-
                                                                                        giver. Instead, in a seated or standing position, the faces of
data (7% of frames, compared with 46% of frames in the 3-                               others would be further away.
month data), and those that were present were very hard to                                 Our final analysis directly measured size and vertical posi-
detect correctly, perhaps because of their small size. Figure                           tion of faces by posture (due to the limited overlap in postures
4 shows frames in which the CRF model incorrectly reported                              between ages, regression analysis was not possible). The ly-
a face; these typically showed consistent spurious detections                           ing posture, seen only at 3 months, had a much larger face
for some superficially face-like configuration of objects.                              size than the other postures (almost 9% of camera field, as op-
   We evaluated the CRF model on the entire dataset, using                              posed to 2.5% for holding, 3% for sitting, and 4% for stand-
the settings established in training. Congruent with the gen-                           ing). Both lying and being held also had lower average ver-
eralization data, we found very few faces in the 12-month data                          tical positions (.50 and .52 respectively, where 1 was the top
relative to the other two ages. Figure 6 shows the estimated                            of the screen) than sitting and standing (.60 and .57, respec-
proportion of face-containing frames across clips at each age.                          tively).
Nevertheless, we should be cautious in interpreting these re-
sults, due to the relatively small amount of data available in                                             General Discussion
this dataset. It may be the case that these results are skewed                          We investigated the possibility of using automated face de-
due to, e.g., participating in a play-group at 8 months with                            tection techniques to measure the accessibility of social infor-
many children present.                                                                  mation to infants. With head-mounted videos from a single
   Figure 5 shows a heat map of the probability of finding a                            infant at 3, 8, and 12 months, we constructed a discrimina-
face at each location in the camera’s field for each of the three                       tive model of face-detection that made use of inexpensive but
samples.1 Faces were higher in the image plane at 8 and 12                              noisy detectors and a secondary filtering step using a condi-
                                                                                        tional random field model. This approach was relatively suc-
    1 For this and the remaining analyses, we averaged across all de-
tections for each frame, potentially including some noise due to spu-                   the location of correct detections as well as frames in which they
rious detectors in correct frames. Future work should look estimate                     occur.
                                                                                  346

                                                                                              version of this project. This work was supported by a John
                      0.075
                                                                           3 months           Merck Scholars Fellowship.
                                                                           8 month
                                                                           12 months                                  References
Porportion of faces
                       0.05                                                                   Aslin, R. (2009). How infants view natural scenes gathered
                                                                                                from a head-mounted camera. Optometry & Vision Science,
                                                                                                86, 561.
                                                                                              Baldwin, D. (1991). Infants’contribution to the achievement
                      0.025                                                                     of joint reference. Child Development, 875–890.
                                                                                              Bradski, G., & Kaehler, A. (2008). Learning opencv: Com-
                                                                                                puter vision with the opencv library. O’Reilly Media.
                                                                                              Cicchino, J., Aslin, R., & Rakison, D. (2010). Correspon-
                         0
                              0.001         0.007              0.05             0.368           dences between what infants see and know about causal
                                      Percentage of visual field (log scale)                    and self-propelled motion. Cognition.
                                                                                              Cohn, J. F., & Tronick, E. Z. (1983). Three-month-old in-
Figure 7: Smoothed histogram of detected face sizes at 3,                                       fants’ reaction to simulated maternal depression. Child De-
8, and 12 months. Height shows proportion of detections at                                      velopment, 54, 185–193.
each size; horizontal axis is scaled in log proportion of cam-                                Farroni, T., Johnson, M., Menon, E., Zulian, L., Faraguna,
era field. Dashed lines give means.                                                             D., & Csibra, G. (2005). Newborns’ preference for face-
                                                                                                relevant stimuli: Effects of contrast polarity. Proceedings
                                                                                                of the National Academy of Sciences of the United States of
                                                                                                America, 102, 17245.
cessful in picking out correct detections for the dataset as a                                Franchak, J., Kretch, K., Soska, K., Babcock, J., & Adolph,
whole.                                                                                          K. (2010). Head-mounted eye-tracking of infants natural
   The face-detector data revealed a surprising pattern. Faces                                  interactions: A new method.
were far less frequent in the 12-month data (and harder to de-                                Frank, M., Vul, E., & Johnson, S. (2009). Development of
tect, providing a potential caveat to our descriptive results).                                 infants’ attention to faces during the first year. Cognition,
In addition, those faces that were detected in the older part                                   110, 160–170.
of the dataset were both smaller and higher in the visual field                               Frank, M., Vul, E., & Saxe, R. (2011). Measuring the de-
of the infant. These differences seemed related to the distri-                                  velopment of social attention using free-viewing. Infancy,
bution of postures across different ages, and indeed size and                                   1–21.
horizontal position did vary with posture. Nevertheless, fur-                                 Johnson, M., Dziurawiec, S., Ellis, H., & Morton, J. (1991).
ther research (and considerably more data) will be necessary                                    Newborns’ preferential tracking of face-like stimuli and its
to check these conclusions.                                                                     subsequent decline. Cognition, 40, 1–19.
   The speculative picture that emerges is nevertheless con-                                  Kelly, D., Quinn, P., Slater, A., Lee, K., Gibson, A., Smith,
gruent with previous work (Franchak et al., 2010). As chil-                                     M., et al. (2005). Three-month-olds, but not newborns,
dren grow and become more adept at locomotion, they create                                      prefer own-race faces. Developmental Science, 8, F31.
a situation where the faces of others in their environment are                                Lafferty, J. D., McCallum, A., & Pereira, F. C. N. (2001).
further away from them and less visible. While the young in-                                    Conditional random fields: Probabilistic models for seg-
fant is constantly having the faces of others pressed into his,                                 menting and labeling sequence data. In Proc. int’l conf. on
the toddler lives in a world populated by knees.                                                machine learning (icml) (pp. 282–289).
                                                                                              Scaife, M., & Bruner, J. (1975). The capacity for joint visual
   More broadly, the methodological upshot of this work is
                                                                                                attention in the infant. Nature.
that head-camera footage may be an extremely valuable tool
                                                                                              Schmidt, M., & Swersky, K.                  (2008).      Con-
for studying social attention and access to social information
                                                                                                ditional     Random      Field    Toolbox      for   Matlab.
“in the wild.” Nevertheless, this work cannot proceed if hand-
                                                                                                (http://www.di.ens.fr/ mschmidt/Software/crfChain.html)
annotation is the only solution. Computer vision methods that
                                                                                              Smith, L., Yu, C., & Pereira, A. (in press). Not your mother’s
are appropriate for data of this type must be developed, and
                                                                                                view: The dynamics of toddler visual experience. Devel-
this study took a first step in that direction, revealing sugges-
                                                                                                opmental Science.
tive developmental differences.
                                                                                              Viola, P., & Jones, D. H. (2004). Robust real-time face de-
                                       Acknowledgments                                          tection. International Journal of Computer Vision.
                                                                                              Yoshida, H., & Smith, L. (2008). What’s in view for toddlers?
Special thanks to Richard Aslin for generously sharing the                                      using a head camera to study visual experience. Infancy,
primary dataset used in this project. Additional thanks to Ally                                 13, 229–248.
Kraus for help with data organization and annotation, as well
as to Adam Vogel and Evan Rosen for work on an earlier
                                                                                        347

