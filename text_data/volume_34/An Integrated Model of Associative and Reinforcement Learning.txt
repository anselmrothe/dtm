UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Integrated Model of Associative and Reinforcement Learning
Permalink
https://escholarship.org/uc/item/2rq250h1
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Veksler, Vladislav
Myers, Chrsitopher
Gluck, Kevin
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                An Integrated Model of Associative and Reinforcement Learning
                                                Vladislav D. Veksler (vdv718@gmail.com)
                                       Christopher W. Myers (christopher.myers.29@us.af.mil)
                                                Kevin A. Gluck (kevin.gluck@wpafb.af.mil)
                                                       Air Force Research Laboratory
                                                        Wright-Patterson AFB, USA
                               Abstract                                 (Stevenson, 1954; Chun, 2000; Myers, Gray, & Sims, 2012).
                                                                        We demonstrate that model integration improves flexibility
   Any successful attempt at explaining and replicating the com-        and adaptability, provides better predictions of behavioral
   plexity and generality of human and animal learning will re-         data, and produces more efficient behavior in environments
   quire the integration of a variety of learning mechanisms. Here
   we introduce a computational model which integrates asso-            with diverse and dynamic reward structures when compared
   ciative learning and reinforcement learning. We contrast the         to each of the individual models.
   integrated model with associative learning and reinforcement            In the following sections we first provide background on
   learning models in two simulation studies. The first simulation      associative and reinforcement learning theories and models.
   demonstrates performance advantages for the integrated model
   in an environment with a dynamic and diverse reward struc-           Next we describe the integrated model. Finally, two simu-
   ture. The second simulation contrasts the performances of the        lations are presented. Simulation 1 contrasts the associative
   three models in a classic latent learning experiment (Blodgett,      and reinforcement learning models with the integrated model
   1929), demonstrating advantages for the integrated model in          in their ability to efficiently adjust to novel goals and diverse
   predicting and explaining the behavioral data.                       reward structures in a grid-navigation environment. Simu-
   Keywords: Associative Learning, Reinforcement Learning,              lation 2 contrasts the associative and reinforcement learning
   Model Integration, Cognitive Modeling, Cognitive Systems,            models with the integrated model in their ability to predict
   Latent Learning
                                                                        behavioral data from a classic latent learning experiment.
                          Introduction                                  Reinforcement Learning
Integration of computational cognitive models is critical               Reinforcement learning (RL) is a formal model of action se-
for accelerating progress in the field of cognitive modeling            lection where the utility of different actions is learned by
(Gray, 2007a). By means of integrative approaches the field             attending to the reward structure of the environment. It
can begin to predict and explain the robustness and flexibil-           has been used in a wide array of domains, from robotics
ity of human behavior in complex, uncertain, non-stationary             (Peters, Vijayakumar, & Schaal, 2003) and artificial intelli-
environments (or large worlds, Binmore, 2009). Specifically,            gence (Russell & Norvig, 1995) to cognitive architectures (Fu
Choi & Ohlsson (2011) assert that the integration of learning           & Anderson, 2006; Nason & Laird, 2005) and cognitive neu-
mechanisms is essential to improving the predictability and             roscience (Holroyd & Coles, 2002).
explanatory power of cognitive models.                                     Generally speaking, RL works in a trial-and-error fashion
   There is no denying that people are adaptive – for example           – attempting various actions and recording the reward gained
our memories are retrieved based on their recency and fre-              for those actions (for a review see Sutton & Barto, 1998).
quency of use (Anderson & Schooler, 1991), visual search                More formally, given the state that an agent is experiencing,
is adapted to the structure of the the task environment (Shen,          the action most likely to be chosen is the one with the high-
Reingold, & Pomplun, 2000; Myers & Gray, 2010), and prob-               est learned utility, plus or minus some exploratory noise. The
lem solving strategies are adapted with increasing task expe-           utility of any given state-action pair, SA, in turn, is directly
rience (Siegler & Stern, 1998). People’s ability to adapt al-           proportional to the value of the reward, that the agent receives
lows them to persist and thrive in large worlds. If we are to           after SA is executed. Hence, state-action pairs are reinforced
build cognitive models for large worlds, we have to endow               when they result in a reward; and the likelihoods of their fu-
them with human learning mechanisms. Hand-coded knowl-                  ture selection are directly proportional to the values of the
edge engineering results in brittle and expensive models, and           experienced rewards.
is a method that does not scale well beyond simple laboratory              There are several variations on how utility is learned in RL
environments (Gluck, 2010). Our hypothesis is that models               (for an introduction, see Sutton & Barto, 1998). For exam-
may begin to demonstrate human-like flexibility and adaptiv-            ple, Temporal Difference RL (TDRL), a version commonly
ity in large worlds through the integration of multiple human           used to model human behavior (Anderson, 2007; Holroyd &
learning mechanisms.                                                    Coles, 2002), propagates the received reward to past actions.
   In the current paper we present an integrated model of as-           Reward is discounted as a function of time, so that actions
sociative and reinforcement learning, as it is evident that hu-         taken just prior to the reward are strengthened more than ear-
mans are capable of learning both the spatiotemporal con-               lier actions. In this way, TDRL reinforces a sequence of ac-
tingencies and the reward structures of their environment               tions that lead to the reward, rather than just a single state-
                                                                   1078

action pair, helping to obtain a solution in a more efficient           tion. Once this model memorizes the map of the environment,
manner.                                                                 it does not need to learn the reward structure through trial-
   Some RL approaches take into account transitions between             and-error; rather, the utility of each action-path is identified
SA, the resultant reward R, and the utility of the next SA              through spreading activation from the goal.
(SARSA). SARSA models update the utility of the state-                     SNIF-ACT (Fu & Pirolli, 2007) is another model that em-
action pair executed at time t, SA(t), by a function of the             ploys associative rather than reward knowledge for action-
reward that follows it, R(t + 1), combined with a function              selection. SNIF-ACT is a model of human information-
of the utility of the state-action pair that follows it, SA(t + 1).     seeking behavior on the World Wide Web. The World Wide
SARSA models are not as efficient as TDRL, but are guaran-              Web is unpredictable in the sense that there is no way for any
teed to converge on an optimal solution.                                of its users to know what links they will encounter during web
   The Model-based RL approach extends RL by learning the               browsing. The utility of selecting a link in SNIF-ACT is not
structure of the world beyond utilities. The term model in              based on any prior reward, but rather on the semantic associa-
“model-based RL” refers to an agent’s internal representation           tion of a link’s text to the current goal (i.e., information scent).
of the environment, and an agent developed in this frame-               This mechanism allows SNIF-ACT to make non-random de-
work is capable of planning its route before execution. This            cisions in novel situations based on associative knowledge.
is extremely useful when memory and decision cycles are less               A limitation of SNIF-ACT is that it does not learn the asso-
expensive than actions (e.g. robotics).                                 ciation strengths between links and goals, but rather imports
   One of the limitations of RL as a complete model of human            these values from an external source. The Voicu & Schma-
decision-making becomes apparent in environments where                  juk model learns association strengths in a psychologically
goals change. Imagine that on your way to work each day                 implausible manner. The Goal-Proximity Decision-making
you pass a post office. One day you need to mail a letter. At           model (GPD; Veksler, Gray, & Schoelles, 2009) mends this
this point, an RL agent would consider, “let’s try a random             by employing the psychologically-plausible delta learning
action, see how that works.” This is because, by definition,            rule (Rescorla & Wagner, 1972; Widrow & Hoff, 1960) to
RL models make decisions based solely on the learned state-             update association strengths. Like the other two models,
action utilities. If the goal changes, the utilities representing       GPD then estimates the utility of a path based on its associa-
the reward structure from the initial goal become irrelevant at         tion strength to the current goal. Veksler, Gray, & Schoelles
best, or subversive at worst. Humans and animals, of course,            demonstrate that in an environment where goals continue to
will employ their knowledge of the environment (e.g. that               change, GPD is able to replicate human performance and RL
there is a post office on the way to work) to make better-than-         cannot.
chance decisions for achieving new goals (Stevenson, 1954;                 A limitation of AL models is that no reward information
Tolman, 1948; Quartermain & Scott, 1960).                               is learned. In this class of models decisions are based on ex-
   The SARSA and Model-based approaches are major steps                 plicitly specified goals. Associative learning does not help
toward more flexible behavior. The SARSA approach con-                  to understand a diverse reward structure, where some actions
siders the state-action-state transitions when learning utili-          may result in less reward and some in greater reward. Hence,
ties, but stops short of learning these transitions. The Model-         AL models cannot explain why an organism might learn to
based RL approach learns such transitions, but employs them             prefer actions leading to one goal-state over another.
strictly to enable planning. The decision process during the
planning stage, however, is still based on the learned utilities.           Integrating Associative and Reinforcement
Thus, when presented with a new goal a Model-based RL                                                Learning
agent will still begin to plan its route by considering random
                                                                        It is our opinion that AL and RL complement each other.
actions.
                                                                        As discussed above, RL models capture behavior based on
Associative Learning                                                    a given reward structure. However, as agent goals change,
                                                                        so does the reward structure of the environment. Since RL
Another class of decision models relies on associative learn-           fails to capture environmental contingencies beyond the orig-
ing. Associative learning (AL) models focus on acquiring                inal reward structure, it cannot predict the efficiency of hu-
the spatiotemporal contingencies of the environment and em-             man behavior in environments where goals tend to change.
ploying these in action-selection. The utility of any given             Contrariwise, AL models store the the spatiotemporal contin-
choice is estimated as a function of previously experienced             gencies of the environment independent of the reward struc-
spatiotemporal proximity between this choice and the cur-               ture, and are more flexible in adapting to new goals. How-
rent goal. The advantage of this approach over RL is that               ever, in ignoring the reward information in the environment,
the stored knowledge is goal-independent. Whenever a new                association-based models cannot capture people’s sensitivity
goal is given, an AL model can employ its knowledge to make             to the value of reward. In this section we describe how the
informed goal-directed decisions.                                       two learning approaches can be integrated to produce more
   Voicu and Schmajuk (2002) implemented a computational                flexible behavior in environments where the reward structure
model that learns the structure of the environment as a net-            is both diverse and dynamic.
work of adjacent cells. Once a goal is introduced, reward                  Given some agent state, S, and a possible action, A, RL
signal spreads from the goal-cell through this network, such            models learn the utility, u, of the SA state-action pair as di-
that the cells farther from the goal-cell receive less activation       rectly proportional to the reward that has been experienced
than those that are close. Goal-driven behavior in this model           after prior executions of SA, and, in models like TDRL, in-
comprises moving towards the cells with the highest activa-             versely proportional to the length of time between SA and the
                                                                    1079

reward in prior experience. AL models do not learn the utility          Simulation 1: Dynamic & Diverse Reward
of SA, but estimate it based on the strength of association, w,         Structure
between SA and the current goal, G, where w is inversely pro-
                                                                        As pointed out in the Introduction, the strength of RL is in
portional to the length of time (or distance) between SA and G
                                                                        learning a diverse reward structure, where some actions may
in prior experience. From the perspective of what is stored in
                                                                        lead to greater reward than others; AL excels at learning the
model memory, the RL models store the values of u for each
                                                                        environmental structure independent of rewards, such that
state-action pair, SAu, and AL models store the values of w
                                                                        this knowledge may be applied in purposive behavior when-
for each state-action-state transition, SAwS.
                                                                        ever new goals arise. However, large worlds are both di-
   To integrate these two models, we propose that the asso-             verse and dynamic. The following simulation was conducted
ciation strength, w should continue to be recorded as in the            to highlight the conditions under which the RL and AL ap-
AL models, whereas the utility u should be recorded for each            proaches begin to falter, and how an integrated approach ad-
state, S, rather than for each state-action pair SA. Thus, what         dresses these limitations.
will be stored in memory and used for action-selection in the              A 10 × 10 navigation grid was used, where a model’s state
integrated model is both w and u for each state-action-state            was uniquely identified as one of the cells in the grid, and
transition, SAwSu. The strength of association, w, is useful            the model had four possible actions from each cell – to move
as an estimate of the probability that a state might follow a           north, south, east, or west1 . If an illegal move was selected
given state-action pair and the length of time of this transi-          (i.e. a move that would take the model off the grid), the
tion. The utility, u, is useful as an estimate of the reward            model’s state was not changed. For each model run, a model
probability/value to be received after a transition occurs.             was placed in a random cell on the grid. Each time the model
   The integrated model uses the delta learning rule to update          reached a reward state, the model would again be placed in
both utilities and association strengths. For each previously           a random cell on the grid. Before the model began the task
executed state-action pair j and each new state i, the strength         of locating a reward, a reward of 1.0 was placed in a random
of association between j and i, w ji , at current time, n, is in-       cell on the grid. After 4000 steps, the reward was cleared,
creased in the following manner:                                        and placed in a different random cell. Following the next
                                                                        4000 steps (8,000 total steps), the reward was cleared, and
                  ∆w ji (n) = β[ai (n) − w ji (n − 1)]          (1)     rewards of 1.0 and 0.1 were placed in two randomly selected
                                                                        cells. Finally, after the next 4000 steps (12,000 total steps),
   where β is the learning rate parameter, and ai is the acti-          the reward was cleared again, and replaced with rewards of
vation of i (ai = 1 if i is present, else 0). The utility for each      1.0 and 0.1 in two randomly selected cells.
new state i, ui , at current time, n, is increased in the following        The integrated AL+RL model (SAwSu) was compared
manner:                                                                 with Random-walk, AL (GPD), and two RL models –
                                                                        Temporal-Difference RL (TDRL), and Q-learning (Q-RL).
                   ∆ui (n) = α[r(n) − ui (n − 1)]               (2)     As discussed above, TDRL is the version of RL most com-
                                                                        monly used in modeling human/animal behavior. Q-RL is a
   where α is the learning rate parameter, and r(n) is the re-          popular SARSA model that is not as efficient as TDRL, but
ward experienced at time n.                                             is guaranteed to converge on an optimal solution (Sutton &
   At each decision point, the utility of a given state-action          Barto, 1998). The results, averaged over 100 model runs for
pair, j, is calculated as follows:                                      each model type, may be observed Figure 1.
                                                                           AL+RL was the best overall model, averaging a total score
                                                                        of 1328.4 for the entire run in this environment, whereas AL
                     U j = ∑(w ji × ui × δt ) + N               (3)     (GPD), TDRL, Q-RL, and Random models scored 1107.8,
                            ∀i                                          362.2, 237.1, and 66.1, respectively. Q-RL is guaranteed to
                                                                        converge to an optimal solution for any one reward structure
   where δ is a discount parameter (0 < δ < 1), t is the tem-
                                                                        in the environment, but it is too inefficient to find such a so-
poral distance between j and i, and N (exploratory noise) is
                                                                        lution within the 4000 trials allotted in this task (though its
a number drawn randomly from a normal distribution with a
                                                                        efficiency improves after the first 8000 steps, where there is
mean of zero and a standard deviation set to some parameter,
                                                                        more than one goal-state).
σ.
                                                                           TDRL, AL, and AL+RL produce indistinguishable perfor-
   In the following section the SAwSu model is examined in              mance until the first goal change (see Figure 1, first 4000
terms of efficiency and psychological validity within environ-          steps). However, once a new goal is presented, TDRL strug-
ments with diverse and dynamic reward structures.                       gles to relearn the reward-structure of the environment, as
                                                                        all of the state-action utility values need to be relearned
                            Simulations                                 (these may be relearned faster if the exploratory noise was in-
The following subsections compare the integrated AL+RL                  creased, but this would come at the expense of performance,
model (SAwSu) with AL-only and RL-only models. First,                   even for the first goal). Q-RL struggles with the same issue,
the models are evaluated based on the efficiency of finding             as both RL models drop to random-level performance once
rewarding states in a 10 × 10 grid. Second, the models are              a new reward-structure is introduced. In contrast, AL and
evaluated based on the ability to match data from a classic                 1 This is a standard simulation environment for performance ex-
latent learning experiment. A single value for the discount             amination of computational agents, as it aims to represent a generic
parameter (ε = .9) was used for both simulations.                       problem space.
                                                                    1080

                                                                     phenomenon. Such a model would learn the structure of the
                                                                     maze and begin to employ its knowledge immediately once
                                                                     the reward is introduced.
                                                                                              Blodgett 1929 data                                      AL+RL (SAwSu)
                                                                             3.0
                                                                             2.5
                                                                             2.0
                                                                                                                      model$r1
                                                                     Error   1.5
                                                                             1.0
                                                                             0.5
                                                                                           RL (Temporal Difference)                                     AL (GPD)
                                                                             3.0
                                                                                                 model$Trial                                            model$Trial
                                                                             2.5
               Figure 1: Simulation 1 results.                               2.0
                                                                                                                      model$r1
                                                                     Error   1.5
                                                                                                                                             R1
                                                                                                                                             R3
AL+RL can employ all of the associative knowledge that was                   1.0                                                             R7
gathered in the first 4000 steps, and apply it to achieving the
                                                                             0.5
new goal.
                                                                                   1   2     3    4    5      6   7   8          9   1   2        3     4    5      6   7   8   9
   Where AL+RL begins to differ from AL is when the reward                                            Trial                                                 Trial
structure of the environment becomes more diverse. After
8000 steps, there are two rewarding states introduced into the       Figure 2: Maze Performance: Avg. Errors by Trial. Data
environment, one of these having a high value (1.0) and the          adapted from Blodgett, 1929 (top-left) and simulation re-
other having a low value (0.1). The AL+RL model learns the           sults from Reinforcement Learning (bottom-left), Associative
correct reward values of these states. AL, however, cannot           Learning (bottom-right), and integrated (top-right) models.
distinguish between the two types of goals, as it records no
information corresponding to varying reward values.
   In summary, the integration of associative and reinforce-            Interestingly, groups R3 and R7 did not continue to dis-
ment learning results in better performance than could be            play random-level performance until the introduction of the
achieved by either model alone in an environment where the           reward. Rather, these groups displayed a shallow error-
reward structure is dynamic and diverse. The AL+RL model             reduction curve, indicating that there was at least some in-
displays more flexibility than RL in adapting to changing            tention to complete the maze even in the “no-reward” tri-
goals, and more flexibility than AL in adapting to a varying         als (“low-reward” from hereon)2 . An RL model can predict
reward structure.                                                    this phenomenon, producing a shallow learning curve for the
                                                                     “low-reward” trials (R3 until trial 3, R7 until trial 7), and a
Simulation 2: Blodgett, 1929                                         steeper learning curve for the high-reward trials (R1).
                                                                        A model that integrates RL and AL should reproduce both
Latent learning is a classic behavioral paradigm that focuses        (1) the better-than-random level of performance in groups R3
on performance in an environment with a dynamic reward               and R7 prior to the introduction of reward, and (2) the steep
structure, and often involves a diverse reward structure. In         improvements in performance once this reward is introduced.
this paradigm, after having spent some time in an environ-              The integrated AL+RL model (SAwSu) was compared
ment, subjects are presented with some goal. Upon the intro-         with AL (GPD) and RL (TDRL) models. A parameter search
duction of the goal, subjects display a higher level of perfor-      was performed, seeking the model parameters that produced
mance than would be expected if they had not spent any time          the best fit (least sums of square differences) to data in the
in the environment prior to the goal introduction. This phe-         constant-reward (R1) and the “low-reward” (R7, trials 1-7)
nomenon is observable in children, adults, and animals (e.g.         conditions. Three parameters were varied for each model:
Quartermain & Scott, 1960; Stevenson, 1954; Tolman, 1948).           learning rate, amount of exploratory noise (σ), and the per-
   For example, Blodgett (1929) ran three groups of rats in          ceived low-reward (LowR) for finishing the maze on the “low-
a maze-learning experiment. One group (the control) was re-          reward” trials. The learning rate parameter varied for RL was
warded upon reaching the end of the maze on every trial (R1).        the utility-learning constant, α, and for AL and AL+RL it was
The second group began receiving rewards on trial 3 (R3).            the associative-learning constant, β (α remained unvaried for
The third group began receiving rewards on trial 7 (R7). Re-         AL+RL at 1.0).
sults demonstrate that subjects in groups R3 and R7 began               Once the best parameter values were found (RL: α =
to perform at the level of control subjects immediately upon         .4, σ = .08, LowR = .15; AL and AL+RL: β = .2, σ =
the introduction of the reward, producing much steeper error-
reduction slopes in these groups than that of R1 (see Figure 2,         2 We interpret the shallow learning curves as resulting from a low
top-left panel). An associative learning model can predict this      reward, such as being taken out of the maze.
                                                                  1081

  Table 1: Root Mean Square Difference to Blodgett, 1929.           it has three learning and three decision mechanisms, whereas
                                                                    SAwSu has two and one, respectively. Further comparison of
                       Best fit to data      Predicted              the two approaches is warranted.
        Model       R1 R7 [trials 1-7]       R3     R7*                To the best of our knowledge there are no other compu-
        AL+RL 0.21               0.14       0.11 0.15               tational frameworks that learn the reward structure and the
                                                                    spatiotemporal predictions of the environment, and employ
        RL         0.26          0.17       0.19 0.32
                                                                    both in the decision-making process. Frameworks that em-
        AL         0.22          0.56       0.23 0.50               ploy some form of Model-based planning (e.g. Daw, Niv, &
        *Only trials 8 and 9 are predicted.                         Dayan, 2005; Sutton & Barto, 1998) include both AL and RL,
                                                                    but these tend to focus on the trade-off between planning in
                                                                    the head and acting in the world. Associative knowledge in
.05, LowR = .15), the full simulations were executed to get         this class of models is used to enable planning rather than to
model predictions for R3 and for R7 after the introduction of       determine how a path of actions, whether in the head or in the
reward (these conditions were not included during the param-        world, is chosen.
eter search). Results may be observed in Figure 2 and Table 1.         The overall scarcity of decision models that employ AL
As expected, AL and AL+RL produced steeper performance              and RL together is rather surprising given the long history
improvements than RL upon the introduction of the reward            of research on learning in experimental psychology, cogni-
by the experimenter on trials 3 and 7. As expected, RL and          tive science, and artificial intelligence. Ohlsson (e.g. Choi &
AL+RL replicated the shallow error-reduction curves in trials       Ohlsson, 2011) has been promoting the integration of learn-
1-3 for condition R3 and 1-7 for condition R7, and AL did           ing mechanisms, including AL and RL, and Alonso & Mon-
not.                                                                dragón (2006) and Dickinson & Balleine (1993, 1994) call
   AL+RL produced a better overall fit to data than did the         for AL+RL integration. None of these proposals, however,
other two models (see Table 1). The advantages become more          has been implemented as a computational model, and thus
apparent when we focus on the error-reduction after the intro-      cannot be easily contrasted with the SAwSu implementation.
duction of reward. Figure 3 demonstrates model predictions             The Voicu & Schmajuk (2002) model mentioned in the
for error reduction in the R3 group between trials 3 and 5, and     Introduction, does employ AL in action-selection, and even
the R7 group between trials 7 and 9. The AL model predicts          considers variable utility of the goal state in the decision
too high a performance improvement (because the initial per-        phase. However, the Voicu & Schmajuk model does not spec-
formance is underestimated), and the RL model predicts too          ify any way of actually learning state utilities.
low a performance improvement in these trials.                         Earlier versions of the ACT-R integrated cognitive archi-
                                                                    tecture included both RL and AL (see Anderson, 1993; An-
                                                                    derson & Lebiere, 1998). However, according to Ander-
                                                                    son (2001), the particular form of associative learning imple-
                                                                    mented in ACT-R turned out to be “disastrous,” and produced
                                                                    “all sorts of unwanted side effects” (p. 6). Thus, as it stands,
                                                                    the implementation of associative learning in ACT-R 6 has
                                                                    been reduced to a single equation that relates the fan effect
                                                                    to spreading activation. This limits AL to chunks that have a
                                                                    direct symbolic relationship, where associative strengths can
                                                                    only decrease as more knowledge enters the system and the
                                                                    “fan” of associations to each chunk increases.
                                                                       The current effort to integrate AL and RL is in accord with
                                                                    the many calls for the integration of cognitive mechanisms
Figure 3: Error reduction after the introduction of reward in       within a unified computational framework (e.g. Gray, 2007b;
                                                                    Choi & Ohlsson, 2011). However, the current work presents
Blodgett, 1929.
                                                                    the integration of only two learning mechanisms, addressing
                                                                    only some of the complexities of large worlds. In the pursuit
                                                                    of models that can produce persistent, adaptive, and flexible
               Summary and Discussion                               behavior in large worlds, it is required that we address how
In this paper we described how two learning mechanisms              a model like SAwSu might be incorporated into a broader
widely supported in the psychological literature, reinforce-        cognitive architecture such as ACT-R. Further integration of
ment and associative learning, may be integrated. In contrast       AL and RL with other cognitive mechanisms is the necessary
with RL-only and AL-only models, the integrated model,              next step for this research.
SAwSu, was shown to produce more efficient, higher fidelity
behavior in environments where the reward structure is both                             Acknowledgements
diverse and dynamic.
   Gläscher, Daw, Dayan, and O’Doherty (2010) propose an           This research was performed while the author held a National
alternative integration of AL and RL by including a supervi-        Research Council Research Associateship Award with the Air
sory mechanism that learns to arbitrate between AL and RL.          Force Research Laboratory’s Cognitive Models and Agents
This implementation seems less parsimonious than SAwSu –            Branch.
                                                                1082

                         References                               Gray, W. D. (2007a). Composition and control of integrated
                                                                    cognitive systems. In W. D. Gray (Ed.), Integrated models
Alonso, E., & Mondragón, E. (2006). Associative Learning
                                                                    of cognitive systems. New York: Oxford University Press.
  for Reinforcement Learning: where animal learning and
                                                                  Gray, W. D. (Ed.). (2007b). Integrated models of cognitive
  machine learning meet. In Proceedings of the 5th sympo-
                                                                    systems. New York: Oxford University Press.
  sium on adaptive agents and multi-agent systems.
                                                                  Holroyd, C. B., & Coles, M. G. H. (2002). The neural ba-
Anderson, J. R. (1993). Rules of the mind. Hillsdale, NJ:
                                                                    sis. of human error processing: Reinforcement learning,
  Lawrence Erlbaum Associates.
                                                                    dopamine, and the error-related negativity. Psychological
Anderson, J. R. (2001). Activation, Latency, and the Fan
                                                                    Review, 109(4), 679–709.
  Effect. In Eighth annual act-r workshop. Pittsburgh, PA.
                                                                  Myers, C. W., & Gray, W. D. (2010). Visual scan adaptation
Anderson, J. R. (2007). How can the human mind occur in
                                                                    during repeated visual search. Journal of Vision, 8(10).
  the physical universe? Oxford University Press.
                                                                  Myers, C. W., Gray, W. D., & Sims, C. R. (2012). The insis-
Anderson, J. R., & Lebiere, C. (1998). The atomic compo-
                                                                    tence of vision: Why do people look at a salient stimulus
  nents of thought. Mahwah, NJ: Lawrence Erlbaum Asso-
                                                                    when it signals target absence? Visual Cognition, 9(19),
  ciates Publishers.
                                                                    1122–1157.
Anderson, J. R., & Schooler, L. J. (1991). Reflections of
                                                                  Nason, S., & Laird, J. I. (2005). Soar-RL: Integrating rein-
  the Environment in Memory. Psychological Science, 2(6),
                                                                    forcement learning with Soar. Cognitive Systems Research,
  396–408.
                                                                    6, 51–59.
Binmore, K. (2009). Rational Decisions. Princeton Univer-         Peters, J., Vijayakumar, S., & Schaal, S. (2003). Re-
  sity Press.                                                       inforcement Learning for Humanoid Robotics. In Hu-
Blodgett, H. C. (1929). The effect of the introduction of           manoids2003, third ieee-ras international conference on
  reward upon the maze performance of rats. University of           humanoid robots, karlsruhe, germany, sept.29-30.
  California Publications in Psychology.                          Quartermain, D., & Scott, T. H. (1960). Incidental learning
Choi, D., & Ohlsson, S. (2011). Effects of multiple learning        in a simple task. Canadian Journal of Psychology/Revue
  mechanisms in a cognitive architecture. In Proceedings of         Canadienne de Psychologie, 14(3), 175–182.
  the thirty-third annual meeting of the cognitive science so-    Rescorla, R. A., & Wagner, A. R. (1972). A theory of Pavlo-
  ciety.                                                            vian conditioning: Variations in the effectiveness of rein-
Chun, M. M. (2000). Contextual cueing of visual attention.          forcement and nonreinforcement. In P. W. F. Black AH
  Trends in Cognitive Sciences, 4(5), 170–178.                      (Ed.), Classical conditioning ii: Current research and the-
Daw, N. D., Niv, Y., & Dayan, P. (2005). Uncertainty-               ory (pp. 64–99). New York: Appleton Century Crofts.
  based competition between prefrontal and dorsolateral stri-     Russell, S. J., & Norvig, P. (1995). Artificial Intelligence: A
  atal systems for behavioral control. Nature Neuroscience,         Modern Approach. Prentice Hall.
  8(12), 1704–1711.                                               Shen, J., Reingold, E. M., & Pomplun, M. (2000). Soar-RL:
Dickinson, A., & Balleine, B. (1993). Actions and re-               Integrating reinforcement learning with Soar. Perception,
  sponses: The dual psychology of behaviour. Spatial rep-           29, 241–250.
  resentation: Problems in philosophy and psychology. In          Siegler, R. S., & Stern, E. (1998). Conscious and unconscious
  N. Eilan, R. McCarthy, & B. Brewer (Eds.), Spatial rep-           strategy discoveries: A microgenetic analysis. , 127(4),
  resentation: Problems in philosophy and psychology. (pp.          377–397.
  277–293). Oxford University Press.                              Stevenson, H. W. (1954). Latent Learning in Children. Jour-
Dickinson, A., & Balleine, B. (1994, March). Motivational           nal of Experimental Psychology, 47(1), 17–21.
  control of goal-directed action. Animal Learning & Behav-       Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learn-
  ior, 22(1), 1–18.                                                 ing: An Introduction. Cambridge, Massachusetts: The MIT
Fu, W. T., & Anderson, J. R. (2006). From recurrent choice to       Press.
  skilled learning: A reinforcement learning model. Journal       Tolman, E. C. (1948). Cognitive maps in rats and men. Psy-
  of Experimental Psychology: General, 135(2), 184–206.             chological Review, 55(4), 189–208.
Fu, W. T., & Pirolli, P. (2007). SNIF-ACT: A Cognitive            Veksler, V. D., Gray, W. D., & Schoelles, M. J. (2009). Goal-
  Model of User Navigation on the World Wide Web. Human             Proximity Decision Making: Who needs reward anyway?
  Computer Interaction.                                             In 31st annual conference of the cognitive science society.
Gläscher, J., Daw, N., Dayan, P., & O’Doherty, J. (2010).        Voicu, H., & Schmajuk, N. (2002). Latent learning, short-
  States versus rewards: dissociable neural prediction error        cuts and detours: a computational model. Behavioural Pro-
  signals underlying model-based and model-free reinforce-          cesses, 59(2), 67–86.
  ment learning. Neuron, 66(4), 585–595.                          Widrow, B., & Hoff, M. (1960). Adaptive switching circuits.
Gluck, K. (2010). Cognitive architectures for human factors         In 1960 ire wescon convention record (pp. 96–104). New
  in aviation. In E. Salas & D. Maurino (Eds.), Human fac-          York: Institute of Radio Engineers.
  tors in aviation, 2nd edition (pp. 375–400). New York, NY:
  Elsevier.
                                                              1083

