UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Operational Model of Joint Attention - Timing of Gaze Patterns in Interactions between
Humans and a Virtual Human
Permalink
https://escholarship.org/uc/item/4f49f71h
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Pfeifer-Lessmann, Nadine
Pfeifer, Thies
Wachsmuth, Ipke
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                      University of California

                                     An Operational Model of Joint Attention -
   Timing of Gaze Patterns in Interactions between Humans and a Virtual Human
                                Nadine Pfeiffer-Lessmann (nlessman@techfak.uni-bielefeld.de)
                                         Thies Pfeiffer (tpfeiffe@techfak.uni-bielefeld.de)
                                        Ipke Wachsmuth (ipke@techfak.uni-bielefeld.de)
                    Artificial Intelligence Group, Faculty of Technology, Bielefeld University, Bielefeld, Germany
                               Abstract                                     We constructed an operational model of joint atten-
                                                                         tion (Pfeiffer-Lessmann & Wachsmuth, 2009) for our vir-
   Joint attention has been identified as a foundational skill in        tual human Max (Lessmann, Kopp, & Wachsmuth, 2006) to
   human-human interaction. If virtual humans are to engage
   in joint attention, they have to meet the expectations of their       create a more natural and effective interaction partner. The
   human interaction partner and provide interactional signals in        model covers four phases: the initiate-act (1), the respond-act
   a natural way. This requires operational models of joint at-          (2), the feedback phase (3), and the focus-state (4). However,
   tention with precise information on natural gaze timing. We
   substantiate our model of the joint attention process by study-       for Max to apppear believable and to use the same behavior
   ing human-agent interactions in immersive virtual reality and         patterns in the phases as humans do, investigations on time-
   present results on the timing of referential gaze during the ini-     frames, human expectations and insights on how humans ac-
   tiation of joint attention.
                                                                         tually perceive his behavior are indispensable. The topic of
   Keywords: joint attention; virtual humans; social interaction         concrete reaction and duration times of feedback behaviors
                                                                         during the joint attention process has to our knowledge not
                            Introduction                                 been discussed in the area of human-computer interaction yet.
                                                                         The time-frames and expectations of humans for natural in-
Attention has been characterized as an increased aware-                  teractions are central subject of this paper.
ness (Brinck, 2003) and intentionally directed perception
                                                                            In the section to follow, we provide an overview on related
(Tomasello, Carpenter, Call, Behne, & Moll, 2005) and is
                                                                         work covering research on joint attention in human-human
judged to be crucial for goal-directed behavior. Joint atten-
                                                                         interaction and in the area of technical systems. In the sub-
tion builds on attentional processes and has been identified to
                                                                         sequent ”Model” section, a brief summary of our own defini-
be a foundational skill in communication and interaction. The
                                                                         tion of joint attention is provided. Next, we present a study
term joint attention is often used confusably with shared at-
                                                                         in immersive virtual reality concerning the exact timing of
tention. We follow Kaplan and Hafner (2006) and Tomasello
                                                                         the first phase, the initiate-act, of our joint attention model.
et al. (2005) in using the term joint attention for the phe-
                                                                         Thereafter, results are discussed and the paper ends with our
nomenon which presupposes a higher level of interactivity re-
                                                                         conclusions and future work.
quiring intentional behavior and an awareness of the interac-
tion partner. Joint attention can be defined as simultaneously
                                                                                                 Related Work
allocating attention to a target as a consequence of attending
to each other’s attentional states (Deak, Fasel, & Movellan,             Staudte and Crocker (2011) raise the question whether joint-
2001). In contrast, we see shared attention (as well as shared           attention-like behavior is unique to human-human interaction
gaze) as the state in which interactants are just perceiving the         or whether such behaviors can play a similar role in human-
same object simultaneously without further constraints con-              robot interaction. They conclude that their own findings sug-
cerning their mental states or their interaction history.                gest that humans treat artificial interaction partners similar to
   Mundy and Newell (2007) differentiate joint attention be-             humans and that it is therefore valid to investigate joint atten-
haviors into two categories: responses to the bids of others             tion in settings with artificial agents.
and spontaneous initiations. Responding to joint attention                  These artificial agents can consist, on the one hand, of
refers to the ability to follow the direction of gaze and ges-           robots (Deak et al., 2001; Imai, Ono, & Ishiguro, 2003;
tures of others in order to share a reference. On the other              Breazeal et al., 2004; Doniec, Sun, & Scassellati, 2006; Na-
hand, to initiate joint attention humans use gestures and eye            gai, Asada, & Hosoda, 2006; Yu, Schermerhorn, & Scheutz,
contact to direct the attention of others to objects, events, and        2012; Huang & Thomaz, 2011; Staudte & Crocker, 2011)
to themselves.                                                           and, on the other hand, of virtual humans (Peters, Asteri-
   For joint attention, interlocutors have to deliberatively fo-         adis, & Karpouzis, 2009; Zhang, Fricker, & Yu, 2010; Bailly,
cus on the same target while being mutually aware of shar-               Raidt, & Elisei, 2010).
ing their focus of attention (Tomasello et al., 2005; Hobson,               Kaplan and Hafner (2006) point out that research in
2005). To this end, respond and feedback behaviors are nec-              robotics concentrates only on partial and isolated elements
essary. Tasker and Schmidt (2008) argue that to establish joint          of joint attention (e.g. gaze following, simultaneous looking
attention a sequence of behaviors is required which has to               or simple coordinated behavior) covering solely the surface
meet certain time constraints.                                           of the process but not addressing the deeper, more cognitive
                                                                     851

aspects of the problem. The same authors stress that no sys-         interaction patterns occuring naturally as part of joint atten-
tem achieved true joint attention between a robot and a human        tion processes.
or between two robots according to their definition yet. This           In our own approach, we let human participants engage
appears to be still the case, however progress has been made         with our interactive virtual agent Max in an immersive
with respect to investigating joint attention behaviors.             virtual environment. As with human-robotic and human-
   A number of researchers in cognitive science and cognitive        human interactions, the interactants thus share the same three-
robotics use developmental insights as a basis for modeling          dimensional environment and reciprocal interactions are pos-
joint attention showing how a robot can acquire joint attention      sible. However, other than today’s robotic systems, the vir-
behaviors by supervised and unsupervised learning (Deak et           tual agent has (more than) human-like reaction times and is
al., 2001; Nagai et al., 2006; Doniec et al., 2006). However,        controlled by a cognitive architecture which goes from basic
the aspect of intentionality and explicit representation of the      activation processes up to concepts of epistemic modal logics
other’s mental state are not accounted for in these approaches.      to model mutual beliefs which are essential for joint attention.
   Another area of research investigates the impact of artifi-          For human-human interactions, Tasker and Schmidt (2008)
cial agents’ joint attention behavior on humans. Here, real in-      postulate a time frame of 5 s for the addressee of an initiate-
teraction scenarios can be distinguished from humans rating          act to respond appropriately. According to them, the duration
video material. Huang and Thomaz (2011) argue that video-            of the respond-act has to last for at least 3 s in order to estab-
based experiments offer the advantage of studying humans’            lish evidence that the partner’s attention has been captured.
perception of joint attention behaviors without dealing with         The focus-state of joint attention has to last for a minimum
technical challenges of identifying the humans’ behaviors.           of 3 s, too. This duration is in accordance to the results found
According to Staudte and Crocker (2011), it has been shown           by Vaughan et al. (2003) for maintaining focus on the same
that video-based scenarios without true interaction yield simi-      object in an episode of joint engagement.
lar results to live-scenarios and can therefore provide valuable        Mueller-Tomfelde (2007) takes a closer look at the re-
insights into humans’ perceptions and opinions.                      search literature to figure out appropriate time scales for ref-
   Huang and Thomaz (2011) use videos to investigate hu-             erential actions. Since an initiate-act or respond-act could be
mans’ judgements of robots initiating and ensuring joint at-         characterized as such, his results should be highly relevant
tention behavior. Their results suggest that humans overall          for natural time-scales of joint attention behaviors. He argues
preferred robots showing joint attention behavior. Staudte           that since a pointing action includes cognitive aspects, it is
and Crocker (2011) also follow a video-based approach; they          more than a basic movement-primitive and thus more than a
conclude that participants robustly follow the robot’s gaze          basic physical act constrained by the nature of cognitive op-
and use it to anticipate upcoming referents. Bailly et al.           erations at a time period of about a 1/3 of a second. There-
(2010) try to quantify the impact of deictic gaze patterns of        fore, Mueller-Tomfelde (2007) expects an appropriate tem-
their agent. They explicitly instructed participants not to take     poral scale of referential primitives to be greater than 300 ms
the agent’s behavior into account, but the participants were         while being less than the temporal scale of actions of a higher
drastically influenced by the agent’s gaze patterns anyway.          cognitive level with a temporal time window of 2-3 s.
   In a real interaction scenario, Peters et al. (2009) study
how human participants perceive the virtual agent’s simple                            Model of Joint Attention
shared attention behavior of non-verbal cuing and how sub-           The model of joint attention presented here is in agreement
tle changes of this behavior affect the gaze-following of hu-        with the model of Tasker and Schmidt (2008), except that we
man participants. Huang and Thomaz (2011) investigate                do not adopt their time constraints for joint attention. Our
the respond-act of their robot and the resulting impact on           model also meets the requirements of Kaplan and Hafner
a human-robot collaborative task using a task-based metric.          (2006), for a longer discussion see Pfeiffer-Lessmann and
They find that the robot responding to referential foci signifi-     Wachsmuth (2009). However, our model differs in that we
cantly outperforms the one staying focused on the human.             do not require interactants to perform a certain sequence of
   The robot of Breazeal et al. (2004) keeps a representation        behaviors. Instead we define the effects of joint attention be-
of its current focus of attention calculated by saliency values.     haviors on their mental states. Thereby, different behaviors
Additionally, it monitors the human participant’s focus of at-       can be performed counting as joint attention behaviors. How-
tention. It is thereby able to notice when both interactants         ever, to realize a natural interaction partner, we are now inves-
focus on the same object simultaneously. However, the robot          tigating valid joint attention behaviors performed by humans
appears to miss feedback mechanisms on a higher level of in-         to be implemented in our artificial agent.
teractivity covering intentional behavior and the awareness of          We define four phases characterized by the mental states
the interaction partners in the joint attention process.             of the interactants (see Figure 1). In order to engage in joint
   Many researchers investigating the impact of artificial           attention, the interaction partners need to have a certain kind
agents which show joint attention behaviors do not account           of psychological engagement with each other, which can be
for the necessary time courses. As an exception, Yu et al.           described as involving a species of perception as well as a
(2012) try to investigate the exact time course of multi-modal       species of emotional responsiveness (Hobson, 2005). This
                                                                 852

can be defined as the precondition for joint attention. To
establish joint attention, certain behaviors leading to certain
mental states need to take place. The first phase can be de-
scribed as the initiation-phase; one of the interactants per-
forms an initiate-act, which the other interactant can recog-
nize. The second phase can be described as the respond-
phase. Now the addressee of the initiate-act needs to per-
form a respond-act. The third phase is characterized as the
feedback-phase; the interactants affirm that they have rec-
ognized the interaction attempts of their interaction partners.
The forth and last phase consists of the focus-phase; now both
interactants focus on the object of attention and are aware
of the joint attention state (see also Pfeiffer-Lessmann and
Wachsmuth (2009) for a formalized definition of the required                  Figure 2: In the study, the human participant faces the virtual
mental state for joint attention).                                            agent Max in a fully immersive virtual reality environment.
                                                                              Between the two interlocutors is a table with ten objects from
precondition                   Interaction between initiator (I) and          a city planning scenario, which serve as reference objects.
                  I        A   addressee (A), A is able to detect atten-
                               tion focus of I (being aware of other)         The eye gaze and the head movements of the human partic-
                               BELA(ATTi h)     BELA(INTENDi (ATTi h))
                                                                              ipant are tracked and the line of gaze onto the objects in the
                        h
                               A believes I wants to join the attention
                                                                              virtual environment is computed in real-time.
 phase 1           I        A  foci of A and I on target object h
                               (ascribing goal)
             initiate                                                         act to be considered successful. As a first step, we thereby
                         h     BELA(GOALi (ATTA h ATTi h))
                                                                              focus on referential acts via eye gaze.
phase 2                        A adopts the goal of joining the attention
                    I       A  foci and performs a respond-act
                               (adopting goal)
                                                                              Scenario We investigate joint attention in a cooperative in-
               respond                                                        teraction scenario with the virtual human Max, where the hu-
                         h     GOALA(ATTA h ATTi h))                          man interlocutor meets the agent face-to-face in 3D virtual re-
phase 3              I          I shows feedback of recognizing A‘s           ality (see Figure 2). The human’s body movements and gaze
                             A  respond-act
                                (feedback)
                                                                              are picked up by infrared cameras and an eye tracker (Pfeiffer,
                    feedback                                                  2011). This enables Max to follow the human’s head move-
                          h    BELA (BELi (ATTA h))                           ments and gaze in real-time, the two aspects of human joint
phase 4              I          both I and A focus on object h while          attention behavior considered in this study.
                             A  noticing the other is also focusing on h
                                (focus state)
                                                                              Participants
         joint attention h     HAPPENS(<T(h)>A <P(<T(h)>i)>A)
                                                                              Altogether data from 20 participants (10 women, 10 men) has
                                                                              been collected. All participants were students or employees
Figure 1: Phases of the joint attention process - Initiator (I)               of Bielefeld University. The age of the participants was be-
and addressee (A) wear hats according to their roles.                         tween 21 and 45 years, with a mean of 28 years and a SD of
                                                                              5.17.
   The model of Huang and Thomaz (2011) with five steps
shares many features with our model except that we define                     Method
to-be-aware of the interactant as a prerequisite and not a step               The participants were invited to our lab and given a brief in-
in the joint attention process and that they concentrate in their             troduction to the study. At this time, they filled out a short
step 4 on verifying the response of the addressee whereas we                  questionnaire and read the written instructions for the tasks.
lay more emphasis on the required feedback mechanisms be-                     After that, they were equipped with the tracked stereo glasses
tween both interactants.                                                      required for the immersive virtual reality setup. For control-
                                                                              ling the experiment, they were given a Wii Remote to step
   Study on the Timing During the Initiate-Act                                through the trials. After the participants had entered the vir-
While the review of related work has brought up timing data                   tual environment, they had time to get accustomed to the sce-
on the phases 2 to 4 of our model, little has been found on                   nario. Finally, the eye-tracking system was calibrated and the
the internal timing of events during the initiate-act. With the               participants repeated verbally the procedure of the study. Af-
following study, we address the question on the timing of the                 ter all questions had been answered, the trials started.
initiator’s referential act in which she first introduces the tar-               The two possible roles of an interlocutor (initiator or ad-
get of the joint attention process. Additionally, we investigate              dressee), are reflected by the study design: two blocks I and
acceptable response times of the addressee for a referential                  A are repeated, where the human participant is the initiator
                                                                          853

in block I and the addressee in block A. The blocks were re-            ure 3, step 4). Finally, she should press a button as soon as she
peated three times, for the first ten participants in the order         feels that Max should have reacted by then (while we collect
IAIAIA, for the second ten in the order AIAIAI. The tasks               data on βr , the expected maximum response time). Because
within each block are described below. The ten items in block           at this point in time we do not want Max to influence the par-
I and block A had a pre-randomized sequence, which was                  ticipant’s timing behavior, Max does not show any reaction in
static between participants but different for the first, second         response to the participant’s attempts. The whole procedure
and third presentation of the block.                                    is repeated for the remaining objects, until all ten objects have
   After all blocks were completed, the participants were de-           been covered.
briefed. Before departing, all participants received a recom-           A: Dwell Time of Referential Gaze Accepted by Addressee
pense for taking part in the experiment.                                In the second part of the study we reverse the roles of the
I: Dwell Time of Referential Gaze Produced by Initiator                 human interlocutor and the virtual agent Max.
The aim of block I is gathering data about the typical dwell
time of the referential gaze act of an interlocutor when at-                                 step 1         step 2
                                                                                               I       A      I        A
tempting an initiate-act. During the initiate-act, the interlocu-
tor focuses on the target object for a certain amount of time                                                      ?
αr (r for reference) until she checks back by focusing on the
face of the interaction partner for time βr . The total duration                             step 3         step 4
of the initiate-act is αr + βr + 2εr , with εr being the very short                           I        A      I        A
time needed to shift the gaze focus.                                                                                 ?
                     step 1          step 2
                       I       A       I        A
                                                                        Figure 4: The sequence of steps for one item in block A.
                                                                        I=initiator (agent) and A=addressee (human)
                     step 3         step 4                                 Now, it is Max who is performing initiate-acts to achieve
                        I       A      I        A                       joint attention and the human interlocutor observes and eval-
                                                                        uates these attempts. During the initiate-act, Max will stop
                                                                        focusing on the interlocutor and move his gaze focus to the
                                                                        target object for an amount of time from a predefined set
                                                                        ranging from 600 ms to 3000 ms in steps of 600 ms (Figure 4,
Figure 3: The sequence of steps for one item in block I.
                                                                        step 2). These values have been selected to comprise typical
I=initiator (human) and A=addressee (agent)
                                                                        non-communicative gaze durations and the findings on mean
                                                                        durations from the literature. Max will then focus back at
   The interaction scenario described above, with a human               the participant’s face (Figure 4, step 3). The participant is
interlocutor addressing the virtual agent Max, provides the             asked to watch Max’ gaze. Once Max focuses back at the
frame for this task. Max plays the role of an interlocutor,             participant, she has to decide whether Max had intended her
while the participant is instructed to perform an initiate-act          to follow his gaze. If she decides so, she has to press a button
for one of the objects located on a virtual table between the           and gaze at the target object (Figure 4, step 4). If not, she
participant and Max (see Figure 2). In an orientation phase             has to do nothing. After five seconds, Max will automatically
prior to each initiate-act, Max gets blindfolded and the next           continue with the next item.
target object is highlighted with a red arrow (see Figure 3,               During the interaction, one measurement is made. By
step 1). This design has been chosen to make explicitly clear           pressing the button the human participant ascribes Max to
that Max has no prior knowledge of the target object. Once              have performed a valid initiate-act. We then count the dwell
the participant has located the new target object, she has to           time used by Max from the given set as an acceptable dwell
return her gaze to Max and press a button to start the interac-         time for an initiate-act, αa (a for acceptance).
tion. This removes the arrow as well as the blindfold of Max.
Max then gives a short verbal phrase to provide the context                                          Results
of the joint attention act and the human participant can start          In a post-study questionnaire (seven-point Likert scales (1-
her initiate-act. The participant is instructed to use gaze only        7), median score is given here), the participants reported that
to try to direct the attention of Max towards the given target          they felt present in the virtual environment (score 5) and expe-
object (Figure 3, step 2). She should start while focusing on           rienced the agent as being even more present (score 6). The
Max’ face, then attempt an initiate-act by focusing at the tar-         naturalness of the communication with the agent, however,
get object as long as she feels is needed (Figure 3, step 3,            was rated 3 (SD 2). The participants also were able to fully
while we collect data on αr ). She should then interrupt fo-            concentrate on the task (score 6) and were not hindered by the
cusing on the target object and check back at Max’ face (Fig-           devices. Overall, they enjoyed the experience in the virtual
                                                                    854

                     Gaze Durations Produced by Human Initiator                                Gaze Durations Accepted by Human Addressee
               150                       145                                             150
                                130
               100                                 94                                    100
                                                                                                            86
   Frequency                                                                 Frequency
                         69                                                                                          70
               50                                           46                           50                                   48
                                                                                                    40
                                                                                                                                       29
               0                                                                         0
                        600     1200     1800    2400     3000                                     600     1200     1800    2400     3000
                              Duration of Referential Gaze [ms]                                          Duration of Referential Gaze [ms]
Figure 5: Dwell times of referential gaze during initiate-acts            Figure 6: Dwell times of referential gaze during initiate-acts
produced by the human participants in study part I.                       produced by Max in study part A, which have been accepted
                                                                          by the human participant as being intentional.
reality (score 5) and had no difficulties with the task (score 3
rating the difficulty).                                                   used in block A) shows no significant differences (p=0.22,
                                                                          see also Figure 5 and Figure 6).
I: Dwell Time of Produced Referential Gaze
                                                                          Discussion
During block I, 560 initiate-acts were recorded. Overall, the
                                                                          For the presented study we created an immersive virtual envi-
mean dwell time of referential eye gaze (αr ) was 1896.82 ms
                                                                          ronment and let the participants engage in joint attention with
(SD 963.46 ms) and the median was 1796 ms. A histogram
                                                                          a virtual agent to have a realistic but highly controlled exper-
of the durations of the referential gaze is depicted in Fig-
                                                                          imental setup to run our studies on cognitive models of joint
ure 5. During the orientation phase when the participants had
                                                                          attention. The feedback from the participants regarding their
to identify and remember the target object, the mean dwell
                                                                          own experience of presence and the presence of the virtual
time of eye gaze was 1559.58 ms (SD 1029.24 ms) and the
                                                                          agent renders this approach a success.
median was 1390.5 ms. The dwell time during search was
                                                                             With this advanced setup, we aimed at substantiating our
significantly shorter than the dwell time of referential gaze (t-
                                                                          knowledge about the timing of referential gaze within the
Test results in t=5.91 with p=0.001 by 545 DoF, confidence
                                                                          initiate-act. In block I, we found a mean dwell time of refer-
interval 215.75 ms to 430.42 ms).
                                                                          ential gaze towards the target object αr of about 1897 ms. We
   Overall, the mean duration until the human participant                 take the significant differences of the αr from the dwell time
expected a feedback after the production of a referential                 on the same target objects during the orientation phase (when
eye gaze towards the target object (βr ) was 2556.07 ms (SD               the target objects are shown to the participants) as a confir-
1721.06 ms) and the median was 2247.5 ms.                                 mation of the different nature of gaze use in search and in
                                                                          referential gaze. This also shows that the design of the study
A: Dwell Time of Accepted Referential Gaze
                                                                          is plausible to the participants regarding the different interac-
In block A, Max produced altogether 600 initiate-acts with                tion states (orientation phase vs. dialog). Using these timing
referential gaze of different durations (600 ms to 3000 ms in             patterns, Max will learn to arbitrate between gaze search and
600 ms steps). The task of the human participant was to de-               referential gaze in the future.
cide, whether she accepts the gazing behavior as being inten-                If roles are switched and the initiate-act is performed by
tional in that Max wanted to guide her attention to the target            the virtual agent Max, we found that participants accepted
object. The dwell time of accepted referential gaze of the five           the same kind of gaze patterns as natural as they themselves
discrete levels is αa with a median of 1800 ms. The histogram             performed when they had the initiative. This substantiates our
of the accepted dwell times is depicted in Figure 6.                      findings and at the same time emphasizes the high acceptance
   A chi-squared test comparing the accepted dwell times αa               of the virtual agent Max as an interaction partner.
in block A and the dwell times αr for referential gaze used by               A respond-act of the addressee to an initiate-act of the hu-
the participants in block I (discretized to the discrete values           man participant was expected before 2556 ms. This is well
                                                                    855

below the 5 s time frame postulated by Tasker and Schmidt             Imai, M., Ono, T., & Ishiguro, H. (2003). Physical Relation
(2008) based on human-human interactions. However, as our               and Expression: Joint Attention for HumanRobot Interac-
study focused on the dwell times during referential gaze and            tion. IEEE Transactions on Industrial Electronics, 50(4),
Max by design only showed a response when triggered, it                 636-643.
was difficult for the participants to decide this threshold. A        Kaplan, F., & Hafner, V. (2006). The challenges of joint
more thorough investigation of this threshold should use a              attention. Interaction Studies, 7(2), 135-169.
more complex scenario, were, e.g., Max produces respond-              Lessmann, N., Kopp, S., & Wachsmuth, I. (2006). Situated
acts with different delays, similar to the design in block A.           interaction with a virtual human - perception, action, and
                                                                        cognition. In G. Rickheit & I. Wachsmuth (Eds.), Situated
                         Conclusion                                     Communication (p. 287-323). Berlin: Mouton de Gruyter.
The high acceptance of Max as an interaction partner with             Mueller-Tomfelde, C. (2007). Dwell-based pointing in ap-
human-like capabilities and the comparability of our findings           plications of human computer interaction. In Proc. of the
in human-machine interaction with those found in human-                 11th Int. Conf. on Human-Computer Interaction (INTER-
human interaction motivate us to follow this line of research           ACT 2007) (pp. 560–573). Springer Verlag.
further. The 1.9 s dwell time of the referential gaze act is com-     Mundy, P., & Newell, L. (2007). Attention, joint attention,
patible with related findings in human-human interaction. In            and social cognition. Current directions in psychological
next steps, we would substantiate our model of joint attention          science, 16, 269–274.
by incrementally increasing the complexity of the interaction         Nagai, Y., Asada, M., & Hosoda, K. (2006). Learning
scenario until the full process of joint attention can be sim-          for joint attention helped by functional development. Ad-
ulated in real-time in a more natural scenario. This would              vanced Robotics, 20(10), 1165–1181.
also allow us to directly compare joint attention behaviors be-       Peters, C., Asteriadis, S., & Karpouzis, K. (2009). Investigat-
tween human-human and human-agent interactions.                         ing shared attention with a virtual agent using a gaze-based
   Although autonomous behaviors of Max were reduced to                 interface. Journal on Multimodal User Interfaces, Kluwer
a minimum in our controlled setup his naturalness of com-               Academic Publishers, 3(1-2), 119–130.
munication was already rated 3. We believe this rating will           Pfeiffer, T. (2011). Understanding multimodal deixis with
increase significantly when he shows his full range of com-             gaze and gesture in conversational interfaces. Aachen,
municative and joint attention behaviors.                               Germany: Shaker Verlag.
                                                                      Pfeiffer-Lessmann, N., & Wachsmuth, I. (2009). Formal-
                    Acknowledgments                                     izing joint attention in cooperative interaction with a vir-
This research is supported by the Deutsche Forschungsge-                tual human. In B. Mertsching, M. Hund, & Z. Aziz (Eds.),
meinschaft in the SFB 673 Alignment in Communication.                   KI 2009: Advances in Artificial Intelligence (pp. 540–547).
                                                                        Springer Verlag.
                         References                                   Staudte, M., & Crocker, M. W. (2011). Investigating joint
Bailly, G., Raidt, S., & Elisei, F. (2010). Gaze, conversational        attention mechanisms through spoken human-robot inter-
  agents and face-to-face communication. Speech Communi-                action. Cognition, 120(2), 268 – 291.
  cation, 52(6), 598–612.                                             Tasker, S. L., & Schmidt, L. A. (2008). The dual usage prob-
Breazeal, C., Brooks, A., Gray, J., Hoffman, G., Kidd, C.,              lem in the explanations of joint attention and children’s so-
  Lee, H., et al. (2004). Humanoid robots as cooperative                cioemotional development: A reconceptualization. Devel-
  partners for people. Int. J. of Humanoid Robots, 1–34.                opmental Review, 28(3), 263–288.
Brinck, I. (2003). The objects of attention. In Proc. of              Tomasello, M., Carpenter, M., Call, J., Behne, T., & Moll, H.
  ESPP2003, Torino (pp. 1–4).                                           (2005). Understanding and sharing intentions: The origins
Deak, G. O., Fasel, I., & Movellan, J. (2001). The emer-                of cultural cognition. Behavioral and Brain Sciences, 28,
  gence of shared attention: Using robots to test develop-              675-691.
  mental theories. In Proc. of the First Intl. Workshop on            Vaughan, A., Mundy, P., Block, J., Burnette, C., Delgado, C.,
  Epigenetic Robotics, Lund University Cognitive Studies, 85            & Gomez, Y. (2003). Child, caregiver, and temperament
  (p. 95-104).                                                          contributions to infant joint attention. Infancy, 6(6), 603–
Doniec, M. W., Sun, G., & Scassellati, B. (2006). Active                616.
  learning of joint attention. In Proc. of 2006 IEEE-RAS int.         Yu, C., Schermerhorn, P., & Scheutz, M. (2012). Adap-
  conf. on humanoid robots (Humanoids 2006).                            tive eye gaze patterns in interactions with human and arti-
Hobson, R. P. (2005). What Puts the Jointness into Joint                ficial agents. ACM Trans. Interact. Intell. Syst., 1(2), 13:1–
  Attention?      In N. Eilan, C. Hoerl, T. McCormack, &                13:25.
  J. Roessler (Eds.), Joint attention: communication and              Zhang, H., Fricker, D., & Yu, C. (2010). A multimodal real-
  other minds (p. 185-204). Oxford University Press.                    time platform for studying human-avatar interactions. In
Huang, C.-M., & Thomaz, A. (2011). Effects of responding                Proc. of the 10th Int. Conf. on Intelligent Virtual Agents
  to, initiating and ensuring joint attention in human-robot            (pp. 49–56). Springer-Verlag.
  interaction. In RO-MAN 2011 IEEE (pp. 65 –71).
                                                                  856

