UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Deterministic Causal Networks from Observational Data
Permalink
https://escholarship.org/uc/item/2rr1m3sq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Deverett, Ben
Kemp, Charles
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

              Learning Deterministic Causal Networks from Observational Data
                                 Ben Deverett                                               Charles Kemp
                      ben.deverett@mail.mcgill.ca                                           ckemp@cmu.edu
                               McGill University                                      Carnegie Mellon University
                              Abstract                                   networks where the nodes represented population levels of
                                                                         five different species. White’s task proved to be difficult, and
   Previous work suggests that humans find it difficult to learn the
   structure of causal systems given observational data alone. We        performance was poor even when White gave his participants
   show that structure learning is successful when the causal sys-       explicit instructions about how to infer causal structure from
   tems in question are consistent with people’s expectations that       observational data. Here, however, we demonstrate that both
   causal relationships are deterministic and that each pattern of
   observations has a single underlying cause. Our data are well         structures considered by White can be reliably learned in the
   explained by a Bayesian model that incorporates a preference          context of the experimental paradigm that we develop.
   for symmetric structures and a preference for structures that            Given that humans perform well on the structure learning
   make the observed data not only possible but likely.
                                                                         tasks that we consider, it is natural to ask how this perfor-
   Keywords: structure learning, causal learning, Bayesian mod-          mance is achieved. Mayrhofer and Waldmann (2011) pro-
   eling
                                                                         pose that learners rely on a “broken link” heuristic and iden-
   Causal networks have been widely used as models of the                tify the structure that minimizes the number of cases where
mental representations that support causal reasoning. For ex-            a cause is present but an effect is absent. They contrast their
ample, an engineer’s knowledge of the local electricity sys-             heuristic-based approach with Bayesian accounts of structure
tem may take the form of a network where the nodes rep-                  learning that rely on patterns of conditional independence be-
resent power stations and the links in the network represent             tween variables. We propose a Bayesian account that falls in
connections between stations. Causal networks of this kind               between these two alternatives. Like Mayrhofer and Wald-
may be learned in several ways. For example, an intervention             mann, we believe that models which track patterns of con-
at station A that also affects station B provides evidence for a         ditional independence are often too powerful to capture the
directed link between A and B. Networks can also be learned              inferences made by resource-bounded human learners. Un-
via instruction: for example, a senior colleague might tell the          like Mayrhofer and Waldmann, we argue that a Bayesian ap-
engineer that A sends power to B. Here, however, we focus                proach is nevertheless useful for explaining why humans suc-
on whether and how causal networks can be learned from ob-               ceed in the tasks that we consider. In particular, we show that
servational data. For example, the engineer might infer that             human inferences are influenced by two factors that are natu-
A sends power to B after observing that A and B are both in-             rally captured by the prior and the likelihood of a Bayesian
active during some blackouts, that B alone is inactive during            model—a preference for symmetric structures, and a pref-
others, but that A is never the only inactive station.                   erence for structures that explain the observed data without
   A consensus has emerged that causal structure learning                needing to invoke coincidences. We demonstrate that incor-
is difficult or impossible given observational data alone.               porating these factors allows a Bayesian model to account for
For example, Fernbach and Sloman (2009) cite the results                 our data better than an approach that relies on the broken-link
of Steyvers, Tenenbaum, Wagenmakers, and Blum (2003),                    heuristic alone.
Lagnado and Sloman (2004), and White (2006) to support
their claim that “observation of covariation is insufficient for                        Bayesian Structure Learning
most participants to recover causal structure” (p 680). Here             The causal systems that we consider are simple activation net-
we join Mayrhofer and Waldmann (2011) in challenging this                works. Each network can be represented as a graph G which
consensus. We show that people succeed in a structure learn-             may include cycles. Figure 1a shows one such graph and a
ing task when the causal systems under consideration are                 data set D generated over the graph. Each row di in the data
aligned with intuitive expectations about causality. Previous            set D represents an observed pattern of activation—for exam-
studies suggest that people expect causal relationships to be            ple, the first row represents a case where nodes A, C and D are
deterministic (Schulz & Sommerville, 2006; Lu, Yuille, Lilje-            observed to be active and node B is observed to be inactive.
holm, Cheng, & Holyoak, 2008), and expect that any pattern               We will assume that each row di is generated by activating a
of observations tends to be a consequence of a single underly-           randomly chosen node then allowing activation to propagate
ing cause (Lombrozo, 2007). We ask people to reason about                through the network. For example, Figure 1b shows that if A
systems that are consistent with both expectations, and find             is the randomly activated node, the final pattern of activation
that structure learning is reliably achieved under these condi-          will match the first row of matrix D in Figure 1a.
tions.                                                                      The activation networks that we consider have three im-
   A previous study by White (2006) asked participants to                portant properties. First, all causal links are generative, and
learn the structure of deterministic causal systems from obser-          these generative links combine according to an OR function.
vational data alone. The structures involved were five-node              For example, node C in Figure 1a will be active if node A is
                                                                     288

Figure 1: (a) A simple activation network and a data set D generated over the network. Each row of matrix D is an observation
which indicates that some of the nodes in the network are active. (b) The first observation in (a) is generated when node A
spontaneously activates and activation propagates through the network.
active or if node B is active. Second, all causal links are de-       servation pattern produced by activating node n then allowing
terministic. Third, spontaneous activations are rare: at most         activation to propagate through the graph. The prior distribu-
one node in the network can spontaneously activate at any             tion P(n) is uniform, which captures the assumption that all
time, which means that each observed pattern of activation            nodes are equally likely to activate spontaneously. We refer
can be traced back to a single root cause. For example, the           to Equation 2 as the probabilistic likelihood.
spontaneous activation of node A is the root cause of the ac-            The second version of the likelihood term depends only on
tivation pattern in the first row of matrix D in Figure 1a. Our       whether observation di is consistent with G, and will be called
assumptions that causes are rare and have deterministic ef-           the logical likelihood:
fects are conceptually related to the work of Lu et al. (2008)                             
on “sparse and strong” priors for causal learning. Note, how-                                 1 if di is consistent with G
                                                                               P(di |G) =                                          (3)
ever, that our notion of rarity is different from their notion of                             0 otherwise.
sparsity. Their notion captures the expectation that each node
in a causal graph is expected to have at most one strong cause,       Observation di is consistent with G if di can be produced by
but ours captures the idea that each pattern of observations di       activating some node in G and allowing activation to propa-
is expected to have a single underlying cause. For example,           gate through the graph.
the activation network in Figure 1a is inconsistent with their           The first version of the prior P(G) in Equation 1 corre-
notion of sparsity, since A and B are both strong causes of C.        sponds to a uniform distribution over the full space of graphs.
This network, however, is consistent with our notion of rarity        The second version captures a preference for graphs that are
as long as the base rates of A and B are both very low, which         symmetric. Perceptual research has documented a preference
means that at most one of these nodes will spontaneously ac-          for symmetric stimuli, and this preference can be viewed as
tivate at any time.                                                   an instance of a more general preference for stimuli that dis-
   Because the networks we consider may include cycles,               play “good form.” We hypothesized that a graph shows “good
they are different from standard Bayesian networks. If de-            form” if many of its nodes play similar roles. For example,
sired, however, our activation networks can be represented as         nodes A in B in Figure 1a play similar roles, and exchang-
dynamic Bayesian networks where the cycles are unrolled in            ing the labels on these nodes leaves the structure of the graph
time (Rehder & Martin, 2011). For our purposes, however, it           unchanged. The symmetry score of a graph can be formally
will be simplest to work with graphs that may include cycles.         defined as the number of graph automorphisms, or the num-
   Given a data set D generated from an unknown network G,            ber of node permutations that leave the structure of the graph
a probability distribution over the possible networks can be          unchanged. For a given number of nodes, the graph with no
computed using Bayes’ rule:                                           edges and the fully connected graph will share the highest
                                     h            i                   possible symmetry score, because all possible node permuta-
        P(G|D) ∝ P(D|G)P(G) = ∏ P(di |G) P(G),                (1)     tions leave the structure of these graphs unchanged. We used
                                        i                             these symmetry scores to define a prior P(G) ∝ s(G), where
where we have assumed that the rows di in the matrix D are            s(G) is the symmetry score of graph G.
independently generated over the graph. We will consider                 Combining the two likelihoods and the two priors produces
two versions of the prior P(G) and two versions of the likeli-        a total of four different models. The “logical uniform” (LU)
hood term P(di |G).                                                   model produces a posterior distribution P(G|D) that assigns
   The first version of the likelihood term assumes that obser-       equal probability to all graphs G that are consistent with the
vation di resulted from the spontaneous activation of a single        data. The LU model is consistent with the broken link heuris-
node in the graph. We sum over all nodes n that may have              tic described by Mayrhofer and Waldmann (2011), which as-
activated spontaneously:                                              sesses how well graph G accounts for data D by counting
                                                                      the number of times that a parent node is active and a child
                  P(di |G) = ∑ P(di |G, n)P(n).               (2)     node is inactive. In our setting, a graph is deemed consistent
                               n
                                                                      with data D if and only if the graph has a broken link count
P(di |G, n) is either 1 or 0 depending on whether di is the ob-       of zero. When applied to our experimental stimuli, the LU
                                                                  289

model therefore makes identical predictions to a model which         der, followed by two final blocks for the five-node networks
assumes that people choose a graph that minimizes the bro-           (Figure 5). These five-node networks are identical to causal
ken link count, and that people are indifferent among graphs         structures previously studied by White (2006). The observa-
that satisfy this criterion.                                         tions within all blocks were shown in random order.
   Like the LU model, the “probabilistic uniform” (PU)               Materials and Procedure. The nodes in each network ap-
model assigns nonzero probability only to graphs that are            peared as rectangles on screen, and active and inactive nodes
consistent with the data. The PU model, however, allows              had different colors. Participants were told that these rectan-
for cases where a data set D is consistent with two graphs           gles were detectors that “detect a rare type of particle called
but better explained by one graph than the other. Consider           the mu particle.” Participants were told that the detectors
a three-node problem where D includes 6 observations and             were connected by directed satellite links, and that an “active
where each observation indicates that nodes A, B and C are           detector always activates all detectors that it points to.” To re-
all active. The data are consistent with a causal chain where        inforce this information, participants were given an example
A sends an arrow to B and B sends an arrow to C. The data,           like Figure 1 where they observed a single detector activating
however, are not typical of a chain, since the chain hypothe-        and activation subsequently propagating over the network.
sis requires the assumption that A spontaneously activated 6            Participants then worked through the 34 blocks. Within
times in succession, which seems like a big coincidence. The         each block the observations were presented one at a time.
data are also consistent with a fully connected graph, and now       After seeing all observations for a given block, participants
no coincidence must be invoked, since all nodes end up ac-           drew a graph on screen to indicate their best guess about the
tive regardless of which node activates first. As this example       structure of the underlying network and rated their confidence
suggests, comparing the logical models with the probabilistic        in their guess on a seven point scale. To minimize mem-
models will allow us to evaluate whether people’s inferences         ory demands, all observations within a block were retained
depend on probabilistic factors like “degree of coincidence”         on screen after being presented, which means that all ob-
that go beyond consistency with the data.                            servations were visible when participants reached the graph-
   The “logical symmetry” (LS) and “probabilistic symme-             drawing stage. Each previous observation appeared as a panel
try” (PS) models are directly comparable to the LU and PU            with detectors, and every edge that participants added during
models, except that they incorporate a preference for symmet-        the graph drawing stage was simultaneously added to each of
ric graphs. Comparing the symmetry models and the uniform            these panels. This design choice was intended to make it as
models will allow us to evaluate whether people bring a pri-         easy as possible for participants to see whether the graph that
ori expectations about the underlying structure to the task of       they had drawn was consistent with all observations for that
structure learning.                                                  block.
                                                                     Results. We focus first on results for the three-node networks.
            Structure learning experiment                            The first nine panels in Figure 2 show the most popular graphs
We designed an experiment to explore whether humans are              for the nine characteristic data sets, and the remaining panels
capable of learning the structure of an activation network           show results for the blocks with two or fewer observations.
given observational data alone, and to evaluate the four mod-        In each case the most common response is consistent with
els just presented.                                                  the data set, indicating that participants understood the task
Participants. 36 members of the CMU community partici-               and were successfully able to discover causal structure given
pated in exchange for pay or course credit.                          observational data alone. In particular, note that all 36 par-
Design. The experiment included 34 blocks, each of which             ticipants discovered the common effect structure in Figure 2d
included one or more observations generated over an unob-            and the common cause structure in Figure 2f. Steyvers et al.
served network. 32 of the blocks involved networks with              (2003) found that these structures are difficult for learners to
three nodes, and the final two blocks involved networks with         distinguish in a probabilistic setting, but our data suggest that
five nodes. The characteristic data set for a network is a           they are easy to learn in our deterministic setting.
set of observations that result from spontaneous activations            Figure 2 also includes predictions of the PS model, and
of each node in the network. Given any network with three            correlations between the model and the data are shown. Re-
nodes, there are 64 possible graphs, but the characteristic data     sults for all four models across the first 32 blocks of the ex-
sets for these graphs include only 9 qualitatively different         periment are shown in Figure 3. The first correlation in each
types. Representatives of each type are shown in Figures 2a          panel shows the performance of a model across the entire set
through 2i. Among the blocks of three-node networks, these           of blocks, and the correlation in parentheses shows the av-
nine types were each presented twice, making 18 blocks with          erage single-block correlation. The PS model performs best
three observations each. An additional nine blocks with six          overall, suggesting that the probabilistic likelihood and the
observations each were created by including two copies of a          symmetry prior are both required in order to capture human
characteristic data set per block. Five additional blocks each       judgments. A bootstrap analysis indicates that the overall and
had two or fewer observations, and are shown in Figures 2j           average single-block correlations achieved by the PS model
through 2n. These 32 blocks were presented in random or-             are reliably higher than the correlations achieved by the PU
                                                                 290

Figure 2: Participant responses and predictions of the PS model for 14 patterns of observations. The observed data are shown
above the left plot in each panel, and the correlation between model predictions and human responses is shown above the right
plot. The four structures in each plot always include the top two structures chosen by humans and the two most probable
structures according to the model.
                                                               291

                                  Uniform prior           Symmetry prior               (a)       Choice probability       (b)               Uniform prior         Symmetry prior
                              r = 0.89 (0.77)             r = 0.93 (0.91)                    1                                         1                      1
                         1                           1
                                                                                                                      Probabilistic
        Probabilistic
                                                                                         0.5                                          0.5                   0.5
                        0.5                         0.5                                                                likelihood
         likelihood                                                                          0                                         0                      0
                                                                                                   1 2 3 6                                   1 2 3 6               1 2 3 6
                         0                           0
                                                                                                  Confidence
                              0      0.5        1         0     0.5         1
                                                                                             7                                         1                     1
                              r = 0.82 (0.65)             r = 0.88 (0.83)
                         1                           1                                       5                          Logical
                                                                                                                                      0.5                   0.5
                                                                                                                      likelihood
          Logical
                                                                                             3
                        0.5                         0.5
        likelihood
                                                                                             1                                         0                     0
                                                                                                  1 2 3 6                                    1 2 3 6               1 2 3 6
                         0                           0
                              0      0.5        1         0     0.5         1         Figure 4: Inferences after observation ABC is presented one,
                                                                                      two, three or six times. (a) Proportion of structures chosen by
Figure 3: Comparison of the complete set of responses for                             humans that can only generate observation ABC (top); Aver-
the first 32 experiment blocks with the predictions of four                           age confidence ratings (bottom). (b) Probability assigned to
models. The first correlation in each panel shows correlations                        structures that can only generate ABC by four models.
based on the complete set of responses, and the correlation
in parentheses shows the average correlation across the 32
individual experiment blocks.
                                                                                      are sensitive to whether or not a structure is consistent with
                                                                                      an observation, but the number of times that the observation
                                                                                      appears is irrelevant. In contrast, the PU and PS models be-
and LS models (p < 0.01 in all cases).                                                come increasingly confident that the underlying structure can
   The main shortcoming of the logical likelihood is that it                          only generate the observation “ABC.” Figure 4a indicates that
leads to predictions that are too diffuse. The structure pre-                         participants show a similar learning curve, and become in-
ferred by participants is typically one of the most probable                          creasingly confident in their responses as the number of ob-
structures according to the LU model, but the model often as-                         servations increases. Bootstrap analyses indicate that the dif-
signs the same probability to many other structures. For ex-                          ferences between the first and the final bars are statistically
ample, after observing “ABC” three times in succession, the                           significant for both plots in Figure 4a (p < 0.001).
LU model assigns the same probability to all 51 structures                               Figure 5 shows the most popular graphs chosen for the two
that can generate the observation “ABC,” including causal                             five-node blocks. Each set of observations is consistent with
chains over these variables. In contrast, the PU model assigns                        only one structure, and participants were reliably able to dis-
highest probability to the 18 structures that can only generate                       cover these structures. Figure 6 compares our results to those
the observation “ABC.”                                                                reported by White, who found that relatively few participants
   Although the PU model performs better than the LU                                  were able to discover these five-node structures. There are at
model, its predictions are still more diffuse than the human                          least two reasons why these tasks may have produced differ-
responses. As just mentioned, the PU model predicts that 18                           ent results. First, our particle-detector scenario may be more
different structures are equally likely after observing “ABC”                         intuitive than White’s task which required inferences about
three times, but participants overwhelmingly prefer the top                           changes in the populations of species over time. Second,
three structures shown in Figure 2i. The symmetry prior al-                           we asked participants to reason about the five-node structures
lows the PS model to capture this preference: note that the                           following 32 inferences about three-node structures, which
fully connected graph is the most symmetric structure that                            means that practice and familiarity with the task may have
can only generate “ABC,” and the two cycles are the next                              contributed to their performance. Future studies are needed
most symmetric structures that meet this criterion.                                   to isolate the critical differences between these paradigms,
   To further evaluate the difference between the probabilistic                       but for now we can conclude that there are conditions under
and the logical likelihood, we examined the learning curves                           which people reliably discover White’s five-node structures
that result when the same observation is presented multiple                           from observational data alone.
times. The 34 blocks in the experiment include blocks where                              Taken overall our results support two general conclusions.
observation “ABC” is presented once, twice, three times, and                          First, humans succeed at structure learning when causes are
six times. Figure 4b shows model predictions for these four                           strong and when each observation has a single root cause. Be-
blocks, where each bar represents the probability mass as-                            cause our cover story made these conditions quite clear, our
signed to structures that can only generate “ABC.” The learn-                         data suggest that people reason accurately about determinis-
ing curves for the LU and LS models are flat—these models                             tic systems where causes are rare, but not that people sponta-
                                                                                292

                                                                     have a single root cause.
                                                                        Our data are consistent with the recent work of Mayrhofer
                                                                     and Waldmann (2011), who also report positive results for
                                                                     learning from observational data. Mayrhofer and Waldmann
                                                                     (2011) argue that humans succeed at structure learning by
                                                                     relying on simple heuristics, but we found that their “bro-
                                                                     ken link” heuristic accounted less well for our data than a
                                                                     Bayesian model that incorporates a probabilistic likelihood
                                                                     term and a symmetry-based prior. There may be alternative
                                                                     heuristics that can implement the computations required by
                                                                     our Bayesian model, but we believe that any successful ac-
                                                                     count of our data will need to incorporate an a priori prefer-
                                                                     ence for symmetric structures, and a preference for structures
                                                                     that make the observed data not only possible but likely.
Figure 5: Data sets, human responses and model predictions           Acknowledgments. We thank Alan Jern for assistance with
for the final two experiment blocks. All four models make            the experiment. This work was made possible by a training
the same prediction, because in both cases only one structure        program in Neural Computation that was organized by the
is consistent with the observations.                                 Center for the Neural Basis of Cognition and supported by
                                                                     NIH R90 DA023426.
               1                         Current study
                                         White (2006)                                        References
             0.5                                                     Fernbach, P. M., & Sloman, S. A. (2009). Causal learn-
                                                                           ing with local computations. Journal of Experimental
               0                                                           Psychology: Learning, Memory and Cognition, 35(3),
                     S1          S2                                        678–93.
                                                                     Lagnado, D., & Sloman, S. A. (2004). The advantage of
Figure 6: Comparison between our results and results re-                   timely intervention. Journal of Experimental Psychol-
ported by White (2006). The bars show the proportion of                    ogy: Learning, Memory, and Cognition, 30, 856-876.
participants who successfully learned the five-node structures       Lombrozo, T. (2007). Simplicity and probability in causal
S1 and S2.                                                                 explanation. Cognitive Psychology, 55, 232–257.
                                                                     Lu, H., Yuille, A. L., Liljeholm, M., Cheng, P. W., &
neously bring these assumptions to causal learning problems.               Holyoak, K. J. (2008). Bayesian generic priors for
Previous studies, however, suggest that both the determinism               causal learning. Psychological Review, 115(4), 955–
assumption and the rarity assumption may both apply more                   984.
generally (Lu et al., 2008; Lombrozo, 2007)                          Mayrhofer, R., & Waldmann, M. R. (2011). Heuristics
   The second general conclusion is that structure learning in             in covariation-based induction of causal models: suf-
our task cannot be adequately characterized as a search for a              ficiency and necessity priors. In Proceedings of the
structure that is consistent with the observed data. At least              33rd Annual Conference of the Cognitive Science So-
two additional factors play a role: humans are sensitive to                ciety (pp. 3110–3115). Austin, TX: Cognitive Science
whether or not the observed data are typical of a given struc-             Society.
ture, and humans have a priori preferences for certain kinds         Rehder, B., & Martin, J. B. (2011). A generative model
of structures including symmetric structures. The PS model                 of causal cycles. In Proceedings of the 33rd Annual
illustrates that these factors can be captured by the likelihood           Conference of the Cognitive Science Society (pp. 2944–
and prior of a Bayesian model, and demonstrates the value of               2949). Austin, TX: Cognitive Science Society.
the Bayesian approach to structure learning.                         Schulz, L. E., & Sommerville, J. (2006). God does not
                                                                           play dice: causal determinism and children’s infer-
                          Conclusion                                       ences about unobserved causes. Child Development,
Previous studies have found that structure learning from ob-               77(2), 427–442.
servational data is difficult. In contrast, our data suggest         Steyvers, M., Tenenbaum, J. B., Wagenmakers, E. J., &
that humans find structure learning relatively easy in settings            Blum, B. (2003). Inferring causal networks from obser-
where causes act deterministically and where each observa-                 vations and interventions. Cognitive Science, 27, 453-
tion has a single root cause. Future studies can consider relax-           489.
ations of these conditions and explore whether humans still          White, P. A. (2006). How well is causal structure inferred
succeed at structure learning when causes are strong but not               from cooccurrence information? European Journal of
fully deterministic, and when most but not all observations                Cognitive Psychology, 18(3), 454–480.
                                                                 293

