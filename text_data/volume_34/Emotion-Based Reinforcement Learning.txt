UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Emotion-Based Reinforcement Learning
Permalink
https://escholarship.org/uc/item/9cb799vd
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 34(34)
Authors
Ahn, Woo-Young
Ross, Olga
Shin, Yong-Wook
et al.
Publication Date
2012-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                      Emotion-Based Reinforcement Learning
                                              Woo-Young Ahn1 (ahnw@indiana.edu)
                                                   Olga Rass1 (rasso@indiana.edu)
                                            Yong-Wook Shin2 (shaman@amc.seoul.kr)
                                         Jerome R. Busemeyer1 (jbusemey@indiana.edu)
                                          Joshua W. Brown1 (jwmbrown@indiana.edu)
                                          Brian F. O’Donnell1 (bodonnel@indiana.edu)
                                 1 Department   of Psychological and Brain Sciences, Indiana University
                                    2 Department   of Psychiatry, Ulsan University School of Medicine
                               Abstract                                 rather than to maximize expected return. In decision affect
                                                                        theory, our emotional responses (R) are based on obtained
   Studies have shown that counterfactual reasoning can shape
   human decisions. However, there is a gap in the litera-              outcomes, relevant comparisons, and beliefs about the likeli-
   ture between counterfactual choices in description-based and         hood of the outcomes:
   experience-based paradigms. While studies using description-
   based paradigms suggest participants maximize expected sub-                          R    ∝    Chosen Outcome Utility +
   jective emotion, studies using experience-based paradigms as-
   sume that participants learn the values of options and se-                                     Regret / Rejoice +                         (1)
   lect what maximizes expected utility. In this study, we used
   computational modeling to test 1) whether participants make                                    Disappointment / Elation
   emotion-based decisions in experience-based paradigms, and
   2) whether the impact of regret depends on its degree of unex-       All counterfactual terms (regret, rejoice, disappointment, and
   pectedness as suggested by the current regret theory. The re-        elation) are weighted by their unexpectedness. Decision af-
   sults suggest that 1) participants make emotion-based choices
   even in experience-based paradigms, and 2) the impact of re-         fect theory effectively explained various experimental results
   gret is greater when it is expected than when it is unexpected.      (Mellers et al., 1999) and Coricelli et al. (2005) used a mod-
   These results challenge the current theory of regret and suggest     ified version of the theory to examine the neural correlates of
   that reinforcement learning models may need to use counter-
   factual value functions when full information is provided.           regret using description-based paradigms.1
   Keywords: Decision making; Bayesian modeling; mathemat-                 Several studies have examined counterfactual decision-
   ical modeling; regret; reinforcement learning.                       making using experience-based paradigms as well (Lohrenz,
                                                                        McCabe, Camerer, & Montague, 2007; Boorman, Behrens, &
                          Introduction                                  Rushworth, 2011; Hayden, Pearson, & Platt, 2009; Yechiam
                                                                        & Rakow, 2011). Although models used in the studies differ
In our daily lives, we constantly face decisions to make and
                                                                        slightly from each other, all previous studies used reinforce-
assess the costs and benefits of possible options (e.g., “Should
                                                                        ment learning models, which assume that participants learn
I buy a lottery or just buy a snack with this money?”, “Should
                                                                        about chosen and foregone outcomes from trial-by-trial expe-
I buy Apple or Google stock?”). Usually we know only the
                                                                        rience and then choose an option that has the highest expected
outcome of our choices. On rare occasions, we also know
                                                                        value.
what would have happened if we had made different choices
                                                                           This study was developed from this gap in the liter-
(e.g., stock market). Having ‘complete feedback’ (or full in-
                                                                        ature: to explain choice behaviors in description-based
formation) under risk or uncertainty can evoke strong emo-
                                                                        paradigms with full information, researchers have assumed
tions such as regret or disappointment that are triggered by
                                                                        participants would make emotion-based choices. To explain
our capacity to reason counterfactually.
                                                                        choice behaviors in experience-based paradigms, researchers
   The effects of counterfactual reasoning have received much
                                                                        have assumed that participants learn the obtained and fore-
attention, and several theories have been proposed. A grow-
                                                                        gone payoffs and do not make emotion-based choices. We
ing consensus suggests that disappointment and elation are
                                                                        tested whether individuals make emotion-based choices in
elicited by comparison between different states (e.g., “my
                                                                        experience-based paradigms by building computational mod-
grant was not funded...”) whereas regret and rejoice come
                                                                        els for all competing hypotheses. This approach allowed us
from comparison between different choices (e.g., “I should
                                                                        to quantitatively compare hypotheses in a rigorous way.
have married another person...”). Also, the unique aspect of
                                                                           Another aim of the study was to test whether regret
regret is a feeling of responsibility that comes with negative
                                                                        would be weighted by its unexpectedness (i.e., surprising-
outcomes from choices.
                                                                        ness). Mellers et al. (1999) claimed that “...unexpected out-
   Among several theories of counterfactual decision-making,
decision affect theory is regarded as one of the leading models             1 In description-based paradigms, the outcomes of all options and
(Mellers, Schwartz, & Ritov, 1999). Decision affect theory              their probabilities are provided to participants and participants rarely
                                                                        receive feedback. In experience-based paradigms, participants must
assumes that individuals make emotion-based choices and                 learn the outcomes or their probabilities from their personal experi-
want to maximize subjective expected pleasure (or emotion)              ence (Hertwig, Barren, Weber, & Erev, 2004).
                                                                     78

comes have greater emotional impact than expected out-                or smaller points (e.g., 1). The probability of winning larger
comes.” However, how would you feel given the follow-                 points was fixed but unknown, and had to be learned from ex-
ing scenarios? In scenario 1, an Apple employee told you              perience. The payoffs of both chosen and unchosen options
some inside information about Apple, which would increase             were revealed on every trial (“full information”). The loca-
its stock price. You believed that this was 80% reliable, but         tions of the options were fixed within games, but randomized
you did not buy the stock whose price sky-rocketed. In sce-           across games. Participants were encouraged to choose an op-
nario 2, an untrustworthy looking stranger told you the same          tion that would maximize their gain. Payoffs were distributed
information. You believed he was 20% reliabilible, but you            so that the long-term expected values of two options were the
did not buy the stock, whose price sky-rocketed. According            same (see Table 1).
to Mellers et al. (1999), you would experience more regret in
scenario 2. However, we hypothesized that scenario 1 would
generate more regret because of the unique aspect of regret: a        Table 1: The payoff distributions of games 1-4. Note that
feeling of responsibility. Therefore, we predicted that regret        the (long-term) expected values of the safe option (M) and
would be weighted by its expectedness rather than its unex-           the risky option are the same. M: points of the safe option,
pectedness. Mellers, Schwartz, Ho, and Ritov (1997) showed            L: low (smaller) points, H: high (larger) points, %H: the
that a smaller probabilities of disappointment/elation were as-       probability of winning larger points. SD: standard deviation.
sociated with greater emotional response. Although Mellers                                             Risky Option
et al. (1999) claimed that the effect of probability would be                  Game      M
the same with regret/rejoice, no experiment has directly tested                               L     H    %H     Mean        SD
it to our knowledge.                                                              1      12    1   56     0.2        12    22.0
    In sum, we designed our experiment to test the follow-                        2      11    1   26     0.4        11    12.3
ing hypotheses. The first hypothesis proposes that partic-                        3      10    1   16     0.6        10     7.4
ipants will learn the chosen and fictive outcomes, compare                        4       9    1   11     0.8         9     4.0
all available options, and try to maximize their expected re-
turn (“Fictive Learning Alone”). The second hypothesis pro-
                                                                         The timing and presentation of a trial is illustrated in Figure
poses that participants will make emotion-based decisions
                                                                      1. Each trial started with a message (”WAIT”), which was
(i.e., maximize their expected subjective emotion) and their
                                                                      was presented for 1-1.5s. After two options were presented,
regret will be weighted by its unexpectedness (“Original Re-
                                                                      the participant had 2s to select an option by pressing buttons
gret”). The third hypothesis proposes that participants will
                                                                      corresponding in a spatially compatible way to the options.
make emotion-based decisions and will weight their regret
                                                                      The color of the chosen option remained changed for .6s, and
by its expectedness (“Modified Regret”). We designed our
                                                                      the payoffs of both options appeared for 1s.
experiment to test these hypotheses.
                             Method
Participants
Nineteen healthy individuals (7 men, mean age = 23.0,
SD=4.9) participated in the study. Electroencephalography
(EEG) was continuously recorded from the scalp, but EEG
findings are not reported in this paper. Participants were paid
$10/hr for participation and told that they would earn perfor-
mance bonuses based on total points earned during the task.
In reality, all participants received a fixed amount ($5) as their
bonus money (Lejuez et al., 2003). Study procedures were                        Figure 1: Time course of the gambling task.
approved by the Indiana University’s Human Subjects Insti-
tutional Review Board.
                                                                      Computational Modeling
Task                                                                  Three hypotheses (1. Fictive Learning Alone, 2. Original Re-
All participants completed four separate gambling games, the          gret model, 3. Modified Regret model) were implemented
order of which was randomly mixed for each participant. At            as three distinct reinforcement learning models. They uti-
the start of each game, participants were told that each game         lized identical learning (probability learning) and choice rules
was independent of the previous game(s). In each game (90             (softmax), but used different value functions. Due to the spe-
trials/game), participants were asked to choose one of two op-        cific design of the task (only 2 possible payoffs of the risky
tions. One option was a safe option in which participants al-         option in each game), it was assumed that participants would
ways won a fixed amount of points (e.g., 11). The other was a         learn the probability of a larger payoff of the risky option
risky option in which participants won either larger (e.g., 26)       (probability learning). In the delta rule (Rescorla & Wagner,
                                                                   79

1972), the probability of a larger payoff (H) (the risky option)             payoffs are A and B, respectively. We used Equation 1 to cal-
on the next trial t + 1, PrH (t + 1), is updated as follows:                 culate RM(L) (t +1), RM(H) (t +1), RL(M) (t +1), and RH(M) (t +
                                                                             1).4 Following Mellers et al. (1999), we set regret/rejoice and
              PrH (t + 1) = PrH (t) + γ · [Y (t) − PrH (t)]           (2)    disappointment/elation terms to sgn(A − B) · |A − B|α when
                                                                             chosen and unchosen payoffs were A and B.5 We assumed
   Here γ is the learning rate (0 < γ < 1) and Y (t) is the out-             that α is identical for both counterfactual functions and the
come (1 if H, 0 if L) of the current trial t. We assumed no                  chosen outcome utility. Importantly, regret/rejoice or dis-
learning occurred about the safe option because its payoff was               appointment/elation will be weighted by its surprisingness.
always the same (e.g., 11) in a given game. We assumed that                  We used 1 minus its probability as an index of surprisingness
the choice of a risky or safe option did not affect the learning             (e.g., 1-PrH (t + 1)) (Mellers et al., 1999). For example, sup-
rate.2                                                                       pose a participant chooses the safe option (chosen payoff=M
   Action selection was implemented via the Luce choice rule                 and the foregone payoff = H). Then, the expected emotional
(a.k.a. softmax) (Luce, 1959). The inverse temperature pa-                   response can be expressed as RM(H) (t + 1) from Equation 1,
rameter (θ) determines the sensitivity of the choice probabil-               which is equal to M α + (−1) · |M − H|α · (1 − PrH (t + 1)).6
ities to the action values. We employed a trial-independent                  If the participant chooses the risky option and the chosen
choice rule (Yechiam & Ert, 2007), where θ = 3c − 1 (0<                      payoff is L, the expected emotional response is RL(M) (t + 1).
c <5). When c approaches zero, choices become completely                     RL(M) (t + 1) is equal to Lα + (−1) · |L − M|α · (1 − PrL (t +
random (exploratory). When c becomes large, choices be-                      1)) + (−1) · |L − H|α · (1 − PrL (t + 1)). Note that the disap-
come deterministic (exploitive).                                             pointment term was included in this case. RM(L) (t + 1) and
                                                                             RH(M) (t + 1) can be calculated in the same way and these
                                          eθ·QR (t+1)                        terms can be used to calculate action values in Equation 4:
                 PrR (t + 1) =     θ·QR (t+1)
                                                                      (3)
                                 e            + eθ·QS (t+1)
                                                                             QS (t + 1) = RM(H) (t + 1) · PrH (t + 1) + RM(L) (t + 1) · PrL (t + 1) (4)
Here QR (t + 1) and QS (t + 1) are action values of choosing
the risky (R) and safe (S) options on trial t + 1, respectively.             QR (t + 1) = RH(M) (t + 1) · PrH (t + 1) + RL(M) (t + 1) · PrL (t + 1)
PrR (t + 1) is the probability of choosing the risky option on
trial t + 1. Next, we describe differences between three com-                   The computed action values are entered into the softmax
peting models (1. Fictive Learning Alone (FLA), 2. Original                  choice rule in Equation 3 to calculate trial-by-trial probability
Regret model, 3. Modified Regret model).                                     of choosing a risky (or safe) option.
Fictive Learning Alone (FLA) The FLA model assumes                           Modified Regret Model This model is identical to the
that participants compute action values of each option sep-                  Original Regret model except that regret (but not any other
arately, then select an option that would maximize their ex-                 counterfactual comparisons) is weighted by Regret’s expect-
pected return. The action value for the safe option is always                edness. We used regret’s probability as its expectedness (e.g.,
the same on each game, QS (t + 1) = M α (0< α <1.5). In                      PrH (t +1)). Thus, only RM(H) (t +1) and RL(M) (t +1) are dif-
other words, the chosen outcome utility of X points (uX ) was                ferent between two Regret models because participants expe-
set to X α . α is a parameter that governs the shape of the utility          rience rejoice, but no regret for RM(L) (t +1) and RH(M) (t +1).
function. As α goes to zero, the reward sensitivity diminishes.              Summary of Three Competing Models In sum, we com-
The action value of the risky option is the sum of two possi-                pared three different models (specifically, value functions).
ble utilities, weighted by their probabilities. In other words,              The FLA model assumes that participants evaluate two op-
QR (t + 1) = uH · PrH (t + 1) + uL · PrL (t + 1).3 These action              tions separately and choose the option that maximizes their
values are entered into Equation 3 to compute the probability                expected return. The two Regret models assume that par-
of choosing each action on the next trial.                                   ticipants evaluate anticipated emotional responses and maxi-
Original Regret Model In Regret models (both Original                        mize their subjective pleasure. The Regret models, however,
and Modified versions), it is assumed that participants choose               make different assumptions about the role of surprisingness
an option that maximizes their subjective expected pleasure                  when processing regretful outcomes. All three models have
or emotion (Mellers et al., 1999). Thus, action values are the               three free parameters: learning rate (γ), utility shape (α), and
weighted sum of expected emotional responses (R in Equa-                     choice consistency (c). We used hierarchical Bayesian ap-
tion 1), rather than expected utilities.                                     proach to estimate them, which is useful for reliably estimat-
                                                                             ing group and individual parameters (for a review see Lee,
   Here we used the notation that RA(B) (t + 1) is the expected
                                                                             2011).
emotional response on trial (t +1) when chosen and unchosen
                                                                                 4 In all settings, L < M < H (e.g., L=1, M=11, H=26).
    2 We   tried several other versions of learning rules (e.g., separate        5 sgn(x)  = 1 if x > 0, -1 if x < 0.
learning rates for chosen and unchosen options) and choice rules
                                                                                 6 The disappointment/elation term is present only for risky
(e.g., trial-dependent inverse temperature parameter) that are not re-
ported here, but they did not improve model-fits.                            choices. The disappointment/elation term is missing in RM(H) (t + 1)
    3 Pr (t + 1) = 1 − Pr (t + 1), u = H α , and u = Lα .                    because the safe option was chosen, in which there is only one state.
         L                  H           H               L
                                                                          80

                                                                                                                          Game1                                  Game2                                  Game3                                  Game4
                                                                                                                1                                      1                                      1                                      1
               µα          λα     µγ            λγ                µc         λc                                0.9                                    0.9                                    0.9                                    0.9
                                                                                                               0.8                                    0.8                                    0.8                                    0.8
                                                                                                               0.7                                    0.7                                    0.7                                    0.7
                                                                                             % Risky Choices
                                                                                                               0.6                                    0.6                                    0.6                                    0.6
                                                                                                               0.5                                    0.5                                    0.5                                    0.5
                                                                                                               0.4                                    0.4                                    0.4                                    0.4
                    αi                  γi                             ci                                      0.3                                    0.3                                    0.3                                    0.3
                                                                                                               0.2                                    0.2                                    0.2                                    0.2
                                                                                                               0.1                                    0.1                                    0.1                                    0.1
                                                                                                                0                                      0                                      0                                      0
                                                                                                                     15   30    45     60   75   90         15   30    45     60   75   90         15   30    45     60   75   90         15   30    45     60   75   90
                                                                                                                               Trial                                  Trial                                  Trial                                  Trial
        xt,i        ut,i               Qt,i                            θi
                                                                                           Figure 3: The mean proportions of risky choices over trials
                                                     P r[Dt+1 ]
                                                                                           on Games 1-4. The blue solid line indicates the group mean
                                                                                           on each trial and shaded region indicates ±s.e.m. (a moving-
                                                                                           average filter was used).
                                                     Cht+1,i
                                                                                                                                                                              Results
                                              t = 1, . . . , 90
                                                                   i = 1, . . . , N        Behavioral Results
                                                                                           The proportions of risky choices in each game are plotted
Figure 2: Graphical depiction of the hierarchical Bayesian                                 in Figure 3. As seen, participants’ choice behavior varied
analysis for three reinforcement learning model. RA(B)t,i re-                              across games although the expected values of two options
places ut,i for Regret models.                                                             were equated on all games. The mean proportions of risky
                                                                                           choices on games 1-4 were .28, .40, .50, and .68 and the dif-
                                                                                           ferences between games were all significant (games 1 vs 2:
                                                                                           p < .003; games 2 vs 3: p < .004; games 3 vs 4: p < .001).
Graphical Model Implementation - Hierarchical                                                 Next, we examined the effect of chosen feedback, foregone
Bayesian Parameter Estimation                                                              feedback, and the magnitude of their difference (Coricelli et
                                                                                           al., 2005). For this goal, we performed panel logic regression
Figure 2 shows the graphical representation of all three mod-                              using the individual random-effects model. The dependent
els. We modeled the variation in γi , αi , and ci parameters by                            variable was ‘switch’ (1 if switched from the previous trial, 0
assuming they have censored Gaussian distributions across                                  otherwise), and independent variables were the chosen pay-
participants. (e.g., γi ∼ Normal(µγ , λγ )I(0, 1), where µγ and                            offs (or feedback) ( f b), the foregone payoffs ( f gFb), and the
λγ are the mean and precision variables of the Gaussian dis-                               magnitude of their difference (| f b − f gFb|) on the previous
tribution). Mean variables had uniform priors and precision                                trial (T-1). Table 2 shows that participants were more likely
variables had Gamma priors (e.g., λγ ∼ Gamma(.001, .001)).                                 to switch if the chosen feedback was lower (p < 3E-16), the
In Figure 2, clear and shaded shapes indicate latent variables                             foregone feedback was higher (p < 2E-13), and the magni-
and observed variables, respectively. Single and double out-                               tude of the difference was higher (p < .011). These results
lines indicate probabilistic and deterministic functions of in-                            suggest that participants take all three variables into account
put, respectively. Circles and squares indicate continuous                                 when making decisions.
and discrete variables, respectively (Lee, 2008). Vectors xt,i                                To examine the effect of feedback on previous trials, an-
(payoffs) and Cht+1,i (choices) were observed and individ-                                 other panel logistic regression analysis was performed, ex-
ual (γi , αi , ci ) and group parameters (µγ , µα , µc , λγ , λα , λc )                    amining how many previous trials ( f b − f gFb) biased the
were estimated. We used OpenBUGS (Lunn, Spiegelhalter,                                     switch behavior. Figure 4 shows that chosen−foregone pay-
Thomas, & Best, 2009) to perform Bayesian inference. We                                    offs of up to two previous trials significantly influenced the
used 50,000 posterior samples collected following a total of                               switch behavior.
30,000 burn-in samples. Multiple chains were used to check
convergence and R̂ values indicated that Markov chain Monte                                Table 2: Regression analysis (panel logit                                                                                           procedure with in-
Carlo (MCMC) chains converged well with the target poste-                                  dividual random effect). fb: the chosen                                                                                             payoff (feedback),
rior distributions. Given that participants’ choice behavior                               fgFb: the foregone payoff (feedback).
varied across games (see Figure 3), we estimated parameters                                    Variable   Coefficient Std. Error                                                                                                   t                    p
separately for each game (but across all participants within                                  Constant          .3902       .0186                                                                                              21.00                 <3E-16
each game). Ideally, model parameters should remain sta-                                          fb           -.0064       .0009                                                                                              -7.44                 <2E-13
ble across games. Otherwise the model might simply mimic                                         fgFb           .0027       .0007                                                                                               3.71                 <.001
data without providing a coherent theoretical explanation of                                 | fb -fgFb |       .0025       .0010                                                                                               2.53                  .011
choice behavior.
                                                                                      81

                                                                                                                    Model Comparison	

                                       X 10-3
                                       1
                                                                                                                                         p < .005
                                                                                                            4
             Coefficient (fb - fgFb)
                                                                                                                                 p < .05
                                       0
                                                                                                            2
                                       -1         **
                                                                                                            0
                                       -2
                                       -3
                                                                                                   BICBIC
                                                                                                            -2
                                            ***
                                       -4
                                                                                                            -4
                                       -5
                                                                                                            -6
                                            T-1   T-2      T-3        T-4   T-5
                                                        Past trials
                                                                                                                       Regret
                                                                                                                   NoNoFLA
                                                                                                                        regret
                                                                                                                                  Original
                                                                                                                                    Mellers
                                                                                                                                 Orig.         Modifed
                                                                                                                                                    Young
                                                                                                                                       Regret Modified Regret	

                                                                                                                 	
               Regret         Regret
Figure 4: Effects of past outcomes on current choice behavior.
fb: the chosen payoff, fgFb: the foregone payoff.                                      Figure 5: BIC (Bayesian information criterion) scores of
*** p<.0001 ** p<.001.                                                                 three competing models compared to the baseline model.
                                                                                       Note that higher BIC indicates a better model fit. Error bars
                                                                                       indicate ±s.e.m. FLA: Fictive Learning Alone.
Modeling Results
To determine which model best fits our data, we used maxi-
                                                                                       varied greatly across games. In sum, the results of both model
mum likelihood estimation (MLE) methods to fit the model to
                                                                                       fit and parameter consistency indicate that the Modified Re-
each person and game separately, and then used the Bayesian
                                                                                       gret model explains participants’ choice behavior best.
information criterion (BIC) (Schwartz, 1978) to compare the
Bernoulli baseline model, in which the probabilities of two                                                           Discussion
options were equal to the individual’s overall proportion of
each option (the number of free parameters=1) against three                            The goals of this study were to examine: (1) whether par-
models of interest.7 The BIC score is a statistic that combines                        ticipants make emotion-based choices in experience-based
badness of fit with a penalty for the number of parameters. To                         paradigms; (2) whether regret would be weighted by its un-
evaluate the models, we used a BIC change score that mea-                              expectedness or expectedness. The modeling results provided
sures the improvement of the computational model over the                              strong support for the Modified Regret model: the model had
Bernoulli baseline model (BIC change equals the BIC from                               the best model fit and its parameters were the most stable
the baseline model minus the BIC from the cognitive model).                            across games, suggesting it might provide a coherent theo-
Therefore positive BIC changes represent improvement over                              retical account for choice behavior across games. The results
baseline, and the model with the highest BIC change is con-                            provide strong support that participants make emotion-based
sidered the best.                                                                      choices and experience greater regret when it was expected
   Figure 5 shows that the Modified Regret model has the                               rather than when it was unexpected.
best model fit. When tested across participants, the dif-                                 We believe this study is the one of the first attempts to in-
ference was significant (the Modified vs. Original Regret                              corporate emotion-based decisions into reinforcement learn-
models: p < .005, the Modified Regret vs. FLA models:                                  ing. Our findings are consistent with previous studies us-
p < .05). When the descriptive accuracy was assessed by pos-                           ing description-based paradigms that found participants made
terior predictive analysis, the best-fitting model (the Modified                       emotion-based decisions. Our results suggest that reinforce-
Regret model) provided good individual-level model predic-                             ment learning models may need to use value functions that
tions. For example, Figure 6 illustrates a good match between                          can incorporate emotional components. The results are also
the observed data (Figure 6A) and the model’s predictions for                          consistent with the notion that emotions provide a common
a participant’s choices (Figure 6B).                                                   currency on how we make decisions under risk or uncertainty
                                                                                       (Loewenstein, Weber, Hsee, & Welch, 2001; Weber & John-
   Next, we examined whether the parameter values of three
                                                                                       son, 2009).
models would remain stable across games. Again, ideally
                                                                                          We also believe these results need to be tested in other
model parameters should be similar across different games or
                                                                                       experience-based paradigms and to determine their general-
tasks. In Figure 7, all parameters of the models were plotted
                                                                                       izability. Some studies found that Bayesian learning models
across games 1-4. Clearly, the parameters of the Modified
                                                                                       outperformed the delta learning rule (Boorman et al., 2011).
Regret model, which had the best model fit, were the most
                                                                                       Although it is possible that using such a learning model can
stable across games. Note that the utility shape (α) and con-
                                                                                       improve the model fit for all three models, we do not think it
sistency (c) parameters of FLA and Original Regret models
                                                                                       will change the main findings of the current study. In sum, we
   7 We are currently working on comparing models by estimating                        found strong support for the Modified Regret model, which
their Bayes factors (Kruschke, 2011)                                                   challenges the current theory of regret.
                                                                                  82

                                                                                                                                           (B)	
                                                                                                    1.0
                                                                                                                                                                                                                                                     0.8
                                                                                                                                                                                                                                                                   FLA
                                                                                                                                                                                                                                                                   Orig Regret
                                                                                          1.0                                                                                                                                                                      Modified Regret
                                                                                          0.8                                                                                                                                                        0.6
                                                                        % Risky Choices
                                                                                                                                                                                                                                                 γ
                                          (A)	

                                                                                          0.6
                                                                                                                                                                                                                                                     0.4
                                                                                          0.4
                                                                                                                                                                                                                                                     0.2
                                                                                          0.2
                      1
                                                                                          0.0                                                                                                                                                        0.0
  % of Risk Option
                     0.8                                                                          0       10         20             30         40           50         60                                   70       80            90
                                                                                                                                                    Trial
                     0.6                                                                                                                                                                                                                             1.5
                                                                                                      µA = 0.28                                µα = 0.099                                                         µc = 1.327                                       FLA
                     0.4                                                                  4                                                                                                                                                                        Orig Regret
                                                                                                                                                                                                                                                                   Modified Regret
                                                                                                                                                                                  0.0 0.5 1.0 1.5 2.0 2.5
                                                                                                                                    5
                                                                                          3                                                                                                                                                          1.0
                                                                                                                                    4
                     0.2
                                                                        Density                                           Density                                       Density
                                                                                          2
                                                                                                                                    3                                                                                                            α
                      0                                                                                                             2
                           10   20   30   40   50   60   70   80   90                                                                                                                                                                                0.5
                                                                                          1
                                           Trial                                                                                    1
                                                                                          0                                         0
                                                                                                0.0    0.4     0.8                       0.0    0.5       1.0    1.5                                        0.0    1.0       2.0    3.0              0.0
                                                                                                         A                                            α                                                                  c
                                                                                                                                                                                                                                                     3.5
                                                                                                                                                                                                                                                                                        FLA
                                                                                                                                                                                                                                                     3.0                                Orig Regret
Figure 6: Posterior predictive assesment of the Modified                                                                                                                                                                                             2.5                                Modified Regret
Regret model for one participant. (A) The participant’s                                                                                                                                                                                          c
                                                                                                                                                                                                                                                     2.0
                                                                                                                                                                                                                                                     1.5
proportion of risky choices over trials (smoothed with a                                                                                                                                                                                             1.0
moving-average filter) (B) posterior predictive distributions                                                                                                                                                                                        0.5
                                                                                                                                                                                                                                                     0.0
for PrR (t). Small blue squares indicate 50 random samples                                                                                                                                                                                                 Game1                Game2   Game3             Game4
from the posterior predictive distributions. The red solid line
indicates the mean values of the distributions. The partici-                                                                                                                                                                                   Figure 7: Parameter values of three competing models across
pant’s model parameter values are in the bottom figure.                                                                                                                                                                                        games 1-4. Symbols and error bars indicate the means and
                                                                                                                                                                                                                                               standard deviations of the posterior distributions, respec-
                                                                                                                                                                                                                                               tively. FLA: the Fictive Learning Alone model.
                                                         Acknowledgments
This work was supported by the NIMH (R01 MH62150 to
                                                                                                                                                                                                                                               Loewenstein, G., Weber, E., Hsee, C., & Welch, N. (2001).
BFO), NSF (0817965 to JRB), and Indiana University Col-
                                                                                                                                                                                                                                                 Risk as feelings. Psychological Bulletin, 127(2), 267.
lege of Arts and Sciences Dissertation Fellowship (to WYA).
                                                                                                                                                                                                                                               Lohrenz, T., McCabe, K., Camerer, C. F., & Montague, P. R.
                                                                                                                                                                                                                                                 (2007). Neural signature of fictive learning signals in a
                                                                   References                                                                                                                                                                    sequential investment task. Proc. Natl. Acad. Sci. U.S.A.,
Boorman, E. D., Behrens, T. E., & Rushworth, M. F. (2011).                                                                                                                                                                                       104, 9493–9498.
  Counterfactual choice and learning in a neural network                                                                                                                                                                                       Luce, R. D. (1959). Individual choice behavior. New York:
  centered on human lateral frontopolar cortex. PLoS Biol-                                                                                                                                                                                       Wiley.
  ogy, 9(6), e1001093.                                                                                                                                                                                                                         Lunn, D., Spiegelhalter, D., Thomas, A., & Best, N. (2009).
Coricelli, G., Critchley, H. D., Joffily, M., O’Doherty, J. P.,                                                                                                                                                                                  The bugs project: Evolution, critique and future directions.
  Sirigu, A., & Dolan, R. J. (2005). Regret and its avoid-                                                                                                                                                                                       Statistics in Medicine, 28(25), 3049–3067.
  ance: a neuroimaging study of choice behavior. Nature                                                                                                                                                                                        Mellers, B., Schwartz, A., Ho, K., & Ritov, I. (1997). Deci-
  Neuroscience, 8, 1255–1262.                                                                                                                                                                                                                    sion affect theory. Psychological Science, 8(6), 423–429.
Hayden, B. Y., Pearson, J. M., & Platt, M. L. (2009). Fictive                                                                                                                                                                                  Mellers, B., Schwartz, A., & Ritov, I. (1999). Emotion-
  reward signals in the anterior cingulate cortex. Science,                                                                                                                                                                                      based choice. Journal of Experimental Psychology: Gen-
  324, 948–950.                                                                                                                                                                                                                                  eral, 128(3), 332.
Hertwig, R., Barren, G., Weber, E. U., & Erev, I. (2004).                                                                                                                                                                                      Rescorla, R. A., & Wagner, A. R. (1972). A theory
  Decisions from experience and the effect of rare events in                                                                                                                                                                                     of Pavlovian conditioning: variations in the effectiveness
  risky choice. Psychological Science, 15, 534-539.                                                                                                                                                                                              of reinforcement and nonreinforcement (A. H. Black &
Kruschke, J. K. (2011). Doing Bayesian data analysis: A                                                                                                                                                                                          W. F. Prokasy, Eds.). Appleton-Century-Crofts.
  tutorial with R and BUGS. Academic Press / Elsevier.                                                                                                                                                                                         Schwartz, G. (1978). Estimating the dimension of a model.
Lee, M. D. (2008). Three case studies in the Bayesian analy-                                                                                                                                                                                     Annals of Statistics, 5, 461-464.
  sis of cognitive models. Psychonomic Bulletin and Review,                                                                                                                                                                                    Weber, E. U., & Johnson, E. J. (2009). Mindful judgment
  15(1), 1.                                                                                                                                                                                                                                      and decision making. Annual Review of Psychology, 60,
Lee, M. D. (2011). How cognitive modeling can benefit                                                                                                                                                                                            53–85.
  from hierarchical bayesian models. Journal of Mathemati-                                                                                                                                                                                     Yechiam, E., & Ert, E. (2007). Evaluating the reliance on
  cal Psychology, 55(1), 1–7.                                                                                                                                                                                                                    past choices in adaptive learning models. Journal of Math-
Lejuez, C., Aklin, W., Jones, H., Richards, J., Strong, D.,                                                                                                                                                                                      ematical Psychology, 51, 75-84.
  Kahler, C., et al. (2003). The balloon analogue risk task                                                                                                                                                                                    Yechiam, E., & Rakow, T. (2011). The effect of foregone
  (bart) differentiates smokers and nonsmokers. Experimen-                                                                                                                                                                                       outcomes on choices from experience. Experimental Psy-
  tal and Clinical Psychopharmacology, 11(1), 26.                                                                                                                                                                                                chology, 1–13.
                                                                                                                                                                                                                                          83

