UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Automatic generation of naturalistic child-adult interaction data
Permalink
https://escholarship.org/uc/item/2t9414br
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Matusevych, Yevgen
Alishahi, Afra
Vogt, Paul
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                  University of California

                 Automatic generation of naturalistic child–adult interaction data
                                            Yevgen Matusevych (Y.Matusevych@uvt.nl)
                                             Department of Culture Studies, Tilburg University
                                             PO Box 90153, 5000 LE Tilburg, the Netherlands
                                                   Afra Alishahi (A.Alishahi@uvt.nl)
                              Department of Communication and Information Sciences, Tilburg University
                                             PO Box 90153, 5000 LE Tilburg, the Netherlands
                                                      Paul Vogt (P.A.Vogt@uvt.nl)
                              Department of Communication and Information Sciences, Tilburg University
                                             PO Box 90153, 5000 LE Tilburg, the Netherlands
                                Abstract                                might be due to the nature of the process under study (e.g.,
                                                                        learning the meaning of words) or the theoretical framework
   The input to a cognitively plausible model of language acqui-
   sition must have the same information components and statis-         on which the model is based (e.g., construction grammar). In
   tical properties as the child-directed speech. There are collec-     such cases, each utterance must be paired with a representa-
   tions of child-directed utterances (e.g., CHILDES), but a real-      tion of its visual context. Many of the databases in CHILDES
   istic representation of their visual and semantic context is not
   available. We propose three quantitative measures for analyz-        contain video recordings of the interaction sessions, but these
   ing the statistical properties of a manually annotated sample of     recordings are mostly not annotated and hard to use without
   child-adult interaction videos, and compare these against the        preprocessing or manual coding. Some models in fact use a
   scene representations automatically generated from the same
   child-directed utterances, showing that these two datasets are       small set of manually annotated videos as input (e.g., Yu &
   significantly different. To address this problem, we propose         Ballard, 2007; Frank, Tenenbaum, & Fernald, 2013), but this
   an interaction-based framework for generating utterances and         approach is limited in quantity and scalability.
   scenes based on the co-occurrence frequencies collected from
   the annotated videos, and show that the resulting interaction-           A common strategy for dealing with this challenge is to use
   based dataset is comparable to naturalistic data. We use an          artificially generated input: each sentence is constructed by
   existing model of cross-situational word learning as a case
   study for comparing different datasets, and show that only           randomly sampling from a presumed distribution over a list of
   interaction-based data preserve the learning task complexity.        words; the visual context is similarly built by sampling from
   Keywords: Child language acquisition; computational model-           a set of symbols which represent concepts or objects (e.g.,
   ing; child-directed speech; cross-situational word learning.         Siskind, 1996; Niyogi, 2002). To make the data more natu-
                                                                        ralistic, some models select sentences from the transcriptions
                           Introduction                                 of actual child–adult interactions, and build the accompany-
A usage-based approach to language claims that natural lan-             ing scene artificially by assuming a semantic representation
guages in all their complexity can be learned merely from               for each word in the sentence and combining them (Fazly,
the input (or usage) data that is available to human learn-             Alishahi, & Stevenson, 2010).
ers. Computational modeling has been extensively used as                    Generating the visual context automatically based on child-
a methodology for supporting this view: using a dataset                 directed utterances (Utterance-Based Data, or UBD) elimi-
that is statistically similar to child-directed input, a compu-         nates the quantity concern (since manual annotation of the
tational model can show that certain linguistic representa-             surrounding scene is not needed). However, the generated
tions are learnable without domain-specific prior knowledge.            context is different from what the child observes in impor-
Therefore, the input to a cognitively plausible model of lan-           tant ways. In a natural interaction scenario between a child
guage acquisition must have the same information compo-                 and an adult, the surrounding scene is rather consistent or
nents and statistical properties as the natural child-directed          changes minimally (although the attention of the participants
speech (CDS). A careful analysis and reconstruction of such             might move from one set of perceivable objects or actions
data is a prerequisite of developing a model.                           to another). In contrast, in the automatic scene genera-
   Recent decades have seen a significant growth in the                 tion approach the utterance determines the scene, so the vi-
variety and quantity of data collections for studying lan-              sual context can change drastically from one sentence to the
guage. One major resource in this domain is CHILDES                     next. A disproportional variation in visual context or scene
(MacWhinney, 2000), a collection of corpora containing                  can affect language learning; for example, context diversity
recorded interactions of adults with children of different age          has been shown to facilitate cross-situational word learning
and language groups. The interaction transcriptions have                (Kachergis, Yu, & Shiffrin, 2009). Moreover, a UBD ap-
been used in several models of grammar induction from a                 proach guarantees that the relevant meanings for all the words
large text corpus (e.g., Clark, 2001). The problem arises when          in an utterance are included in the constructed scene. Ar-
a learning task demands perceptual and linguistic input. This           tificial noise can be added by post-processing data and ran-
                                                                    2996

domly removing some meaning elements from the scene, but              adults’ and children’s gaze directions, actions, objects or par-
the noise ratio can still be unrealistically low.                     ticipants that the actions were directed at, and utterances. Us-
   UBD also differs from actual exchanges between children            ing this data, we constructed a corpus of child-directed utter-
and their caregivers in that it lacks any interaction-based           ances, each paired with a representation of the accompanying
features. Crucially, utterances and actions directed at the           scene.
learner at each point in time are independent of the learner’s        Scene representation There is no easy way to determine
reaction to previous input data. In reality, the content of           which elements a child perceives as potential referents at a
adult’s utterance often depends on what the child just did or         certain moment of time. In fact, any object, action or event
said (Kishimoto, Shizawa, Yasuda, Hinobayashi, & Minami,              from the natural environment can be occasionally referred to
2007; Chouinard & Clark, 2003). Interaction is suggested              in speech. However, studies suggest that children use certain
to be an essential mechanism of language development (e.g.,           mechanisms and constraints such as referential and salience
MacWhinney, 2010).                                                    cues to focus on relevant aspects of the scene (e.g., Behrend,
   In this paper, we investigate the characteristics of the vi-       1990; Moore, Angelopoulos, & Bennett, 1999). In particular,
sual context in a sample of child–adult interaction sessions,         Yu, Smith, Shen, Pereira, and Smith (2009) show that objects
and compare them to those in an automatically generated               in child’s and parent’s hands dominate the child’s visual field.
one. We show that in every measure, the two contexts are                 In coding the interaction context in the video recordings,
considerably different, and argue that these differences might        we consider two different interpretations for a scene:
have implications for modeling child language learning. We
propose a hybrid approach for generating an input corpus of           active: all the objects that either participant (or both of them)
utterance–scene pairs, where co-occurrence frequencies col-              is acting on or looking at during an utterance, in addition
lected from a sample of manually annotated videos are used               to the actions that (s)he performs (a similar approach was
for generating utterances and visual contexts. Our framework             used by Frank, Goodman, and Tenenbaum (2008)).
not only takes the usage frequencies of the words and objects         all: the full set of visible objects, the action(s) performed
into account, but also includes interaction-based features such          during an utterance and the participants.
as dependence of adult’s utterance on child’s recent behavior.
Finally, as a case study, we use an existing model of word            In addition, a third dataset was automatically generated:
learning (Fazly et al., 2010) to compare the complexity of the
learning task using UBD vs. Interaction-Based Data (IBD,              UBD: Fazly et al. (2010) construct a scene by putting to-
generated by our proposed framework). Our results show                   gether the semantic symbols that correspond to the words
that using UBD for word learning unrealistically simplifies              in the accompanying utterance. Referential uncertainty is
the learning task. Using IBD, in contrast, yields results that           simulated by merging the representations of two consecu-
are closely comparable to the ones based on manually anno-               tive scenes, and pairing them with only one of the utter-
tated scenes from videos of child–adult interaction.                     ances. They include noise into the data by removing the
                                                                         semantic symbol of one word from the scene for 20% of
          Analyzing Utterance-based Input                                the input items. Since we wanted to compare our results
We analyze the cognitive plausibility of UBD by comparing                to those of Fazly et al. (2010), we applied the exact same
its characteristics to a carefully annotated set of video record-        approach to the child-directed utterances that we extracted
ings of child–adult interactions. The details of this data set           from the CASA MILA recordings.
are described below.
                                                                      Measures
Data set
                                                                      To compare the datasets described above, we use three mea-
As part of a larger project to study cross-cultural aspects of        sures: scene stability, noise, and referential certainty.
child-language acquisition (CASA MILA; Vogt and Mastin,
2013a, 2013b), three 13-month-old children from the Nether-           Scene stability As mentioned before, the stability of the
lands were recorded on video. The videos were recorded at             visual scene is one of the main points of deviation between
the children’s homes and involved interactions with one of the        natural interaction settings and the artificially generated in-
parents. The parents were instructed to continue their daily          put. We measure scene stability as the overlap between every
routines and ignore the recordings.                                   pair of consecutive scenes. Since in both cases (the produced
   For each child, we selected an interaction session in a toy        scenes in UBD and the annotated ones in our data set) a scene
playing setting. The video fragments were 8 min 37 sec,               is represented as a set of symbols, we define the overlap be-
8 min 50 sec and 10 min long. We excluded some short                  tween each two sets as the cardinality of their intersection
episodes from the analysis, namely those (1) where the child          divided by the cardinality of their union:
or the adult was not captured properly by the camera, and
(2) where the other parent was present, and the child’s atten-                                               |Si ∩ Si+1 |
                                                                                       overlap(Si , Si+1 ) =
tion was focused on him/her. From the videos we extracted                                                    |Si ∪ Si+1 |
                                                                  2997

Noise We count a word’s usage (or token) in an utterance                                  An interaction-based framework for input
as noisy if its semantic symbol is not included in the scene                                                       generation
representation for that utterance. The total number of noisy
                                                                                     We propose an interaction-based framework for generating
words in an utterance, then, is calculated as
                                                                                     input data which resembles the verbal and non-verbal ex-
                                         |Ui | − |Ui ∩ Si |                          changes between a child and a caregiver. Our model is in-
                        noise(Ui ) =
                                                  |Ui |                              spired by the language game model used to study the evolu-
where Si is the current scene, and Ui is the (correct) mean-                         tion of language (Steels, 1996; Vogt & Haasdijk, 2010). In
ing representation of the current utterance. To avoid making                         this model, agents communicate with each other through ver-
arbitrary decisions about the meaning of abstract or function                        bal and non-verbal behavior. Language game interactions in-
words, we limit our analysis of noise to objects and actions.                        volve a context, and agents communicate about items in this
                                                                                     context, potentially learning associations between words and
Referential certainty We define the referential certainty                            items.
for a scene as                                                                           We simulate the input generation process as a series of in-
                                                 |Ui ∩ Si |                          teractive sessions between two agents, Adult and Child. Each
                          certainty(Si ) =                                           session starts with constructing a visual context (i.e., a col-
                                                    |Si |
                                                                                     lection of objects), followed by a sequence of exchanges be-
Conceptually referential certainty shows what portion of a                           tween the two agents, until one of them leaves or terminates
scene is referred to in the respective utterance. Note that this                     the session. In each turn, Adult performs an action (AdAct)
measure is the opposite of the more commonly used refer-                             while producing an utterance (AdUttr), to which Child re-
ential uncertainty, but it avoids the problem of having zero                         sponds by performing another action (ChAct) and producing
denominators in case the meaning representation of the utter-                        an utterance (ChUttr, implemented as presence or absence of
ance does not overlap with the scene.                                                a verbal reaction).2 The main algorithm can be described as
Results                                                                              follows:
We calculated the above measures for three datasets: child-                              for s ← 1 to number of interaction sessions do
                                                                                              t ← 0;
directed utterances extracted from CASA MILA and paired                                       Context ← setupContext(s);
with two interpretations of the accompanying visual scene                                     repeat
(i.e. active and all), or with UBD-style automatically gen-                                       t ← t + 1;
erated scene representations. Results are shown in Table 1.                                       Situationt ← initialize(Context);
                                                                                                  Situationt ← updateAdult(AdActt−1 , AdUttrt−1 );
         Table 1: Plausibility measures for three datasets                                        Situationt ← updateChild(ChActt−1 , ChUttrt−1 );
                                                                                                  (AdActt , AdUttrt ) ← adultTurn(Situationt );
                                            all        active   UBD
                                                                                                  Situationt ← updateAdult(AdActt , AdUttrt );
            Scene stability               0.916         0.436  0.112                              (ChActt , ChUttrt ) ← childTurn(Situationt );
            Noise                         0.414         0.426  0.099                          until ChActt = ‘leave’ or AdActt = ‘leave’;
            Referential certainty         0.019         0.112  0.602                     end
   The average values provided in the table inform us that                               Each of the main steps in the algorithm are explained in
the all condition differs substantially from the other two in                        more detail below.
terms of scene stability (µ = 0.916 vs. 0.436 and 0.112) and refer-
ential certainty (µ = 0.019 vs. 0.112 and 0.602). For this reason,                   Visual context From the sample data we extracted all the
and taking into account the fact that the standard deviation                         objects that were directly used by adults or children in their
values for the all condition are rather small as compared                            interactions. In each computational simulation, we randomly
to the respective means (σstability = 0.065; σcertainty = 0.032), we elim-           selected a fixed number of objects from the list and added
inate this condition from the analysis.1 To compare the other                        them to the context. Since the size of the visual context may
two conditions, we ran the Mann–Whitney U-test for each                              depend on the interaction domain (e.g., toy playing, book
of the three measures. We found significant differences be-                          reading, etc.), we added it as a parameter to our framework.
tween the annotated data (active condition) and the UBD in                           Actions and action types We compiled two lists of actions,
terms of all three measures: scene stability (Mdn = 0.400 vs. 0.059;                 one for each agent. Actions might take arguments that can
U = 5230, nactive = 274, nUBD = 133, p < .001, r = −.583), noise (Mdn = 0.400 vs.    be an object type or the agents themselves (e.g., take toy or
0.000; U = 8927, nactive = 278, nUBD = 139, p < .001, r = −.466) and referen-        touch child). In order to base our computational model on
tial certainty (Mdn = 0.000 vs. 0.571; U = 3910, nactive = 278, nUBD = 139, p <      more general behavioral patterns rather than on occasional
.001, r = −.690). This demonstrates that UBD may be an easier                        events, we classified agents’ actions into six types, based on
input for the learner than the natural data.
                                                                                          2 Since children in our sample video recordings were too young
    1 The noise values for the active and the all conditions are almost              to talk, we did not gather enough statistical information about their
equal, since the way we interpret a scene has little impact on the                   produced utterances. However, the main concern of our framework
amount of noise in utterances. Due to this fact, for noise we also use               is to create realistic child-directed input, and the child-produced data
only the active condition in the further analysis.                                   is an outcome of the learning model.
                                                                                 2998

the factors that motivate them. These action types are listed                (d) Situationt is updated to include AdActt and its arguments
in Table 2.                                                                 3. Adult’s next utterance is generated:
      Table 2: Action types and their motivating factors                      (a) An utterance type for Adult (AdUttrTypet ) is randomly
   Action type      Motivating factor              Example                        selected, conditioned on Situationt
                     Same person’s            Adultt : [move bag]            (b) An utterance for Adult (AdUttrt ) is randomly selected,
  Continuation
                    previous action          Adultt+1 : [move box]                conditioned on AdUttrTypet and Situationt
                     Other person’s            Childt : [put ball]            (c) Situationt is updated to include AdUttrt
    Reaction
                    previous action          Adultt+1 : [take ball]
                                                                            4. Child’s next action and utterance (ChActt and ChUttrt ) are
                     Same person’s            Adultt : Bumba first
     Result                                                                     generated in the same way as Adult’s.
                     prev. utterance        Adultt+1 : [take Bumba]
   Reaction to       Other person’s             Adultt : The tree
                                                                            A sample interaction session
    utterance        prev. utterance        Childt+1 : [take toy tree]
    Initiating            None                Adultt : [sit down]           We illustrate the interaction process using the following ex-
                                                                            ample (see Table 4).
Utterances and utterance types We compiled a list of ut-                                                    Table 4: A fragment of a generated interaction
terances produced by adults. Some of these contain place-                                                   Context: puzzle, piece-clown, bin, ball, piece-frog
holders which, depending on the context, can be filled with                                                 Turn Agent             Action           Utterance
the labels for the respective actions and their arguments. Sim-                                              1.     Adult       play puzzle              —
ilar to actions, we recognized six utterance types based on                                                  1.     Child play piece-clown           babbling
their motivating factor, as listed in Table 3.                                                               2.     Adult       point puzzle       It fits here.
                                                                                                             2.     Child        touch bin           babbling
    Table 3: Utterance types and their motivating factors                                                    3.     Adult       play puzzle            Yes?
  Utterance
                 Motivating factor               Example
     type                                                                      The example can be interpreted as following. Adult starts
  Accompa-       Same person’s              Adultt : [show ball]            the interaction by playing with a puzzle toy without say-
    nying        current action            Adultt : This is a ball
                                                                            ing anything. Child plays with a clown-shaped puzzle piece
  Continua-      Same person’s             Adultt : Dad the ball?
                                                                            and babbles. Adult points at the puzzle saying It fits here.
     tion        prev. utterance        Adultt+1 : Can dad the ball?
                                                                            However, Child’s attention is distracted by the bin, which
                 Other person’s             Childt : [stand up]
   Reaction                                                                 he touches. He continues babbling. Adult continues playing
                 previous action          Adultt+1 : Gonna walk?
                 Other person’s              Childt : babbling
                                                                            with the puzzle toy, asking Yes?. His question can be inter-
    Answer                                                                  preted either as a support for his previous utterance or as an
                 prev. utterance          Adultt+1 : Yeah, Bumba
   Unknown            None                   Childt : babbling              attempt to clarify the child’s utterance. The interaction goes
                                                                            on in this manner until one of the agents leaves.
 Producing actions and utterances At each step t during a                   Comparing IBD and UBD
 session, the actions and utterances produced by the agents are
 sampled from the frequency distributions collected from the                We used the interaction-based framework for generating a
 annotated videos, each conditioned on the current situation.               dataset. While in UBD scenes were constructed from utter-
 A situation includes all the relevant parameters, including the            ances, in IBD each scene included salient elements, namely,
 current and previous utterances and actions of both agents,                the objects that the agents had in their hands, the agents’ most
 the action arguments, and the visual context. Thanks to these              recent actions and their arguments—in a manner similar to
 parameters, the agents do not produce completely random ac-                the active condition in the Analyzing Utterance-based Input
 tions and utterances, and the interaction process appears to be            section above. Using the same three measures—scene sta-
 logical. (For more details on the estimated probabilities for              bility, noise, and referential certainty—we compare IBD to
 each variable, see Matusevych (2012).)                                     UBD and manually annotated CASA MILA data (both con-
    Each turn in a session consists of the following steps:                 ditions).
1. The current situation (Situationt ) is set to include the vi-
                                                                                          1.0                                          1.0                                              1.0
    sual context (Context), the previous actions (AdActt−1 and
                                                                                Scene Stability
                                                                                                                                       0.8
                                                                                                                                                                            Referential Certainty
    ChActt−1 ) and utterances (AdUttrt−1 and ChUttrt−1 )
                                                                                                                                    Noise
2. Adult’s next action is generated:                                              0.4   0.6     0.8
                                                                                                                                  0.4   0.6
                                                                                                                                                                          0.2    0.4    0.6     0.8
  (a) An action type for Adult (AdActTypet ) is randomly se-                              0.2                                          0.2
      lected, conditioned on Situationt                                                   0.0
                                                                                                      all    active   UBD   IBD
                                                                                                                                       0.0
                                                                                                                                              all   active   UBD   IBD
                                                                                                                                                                                        0.0
                                                                                                                                                                                                      all   active   UBD   IBD
  (b) An action for Adult (AdActt ) is randomly selected, con-
      ditioned on AdActTypet and Situationt                                                 Scene stability                                         Noise                Referential certainty
  (c) Arguments for the action are randomly selected, condi-
      tioned on AdActt                                                                                  Figure 1: Plausibility measures for four datasets
                                                                         2999

   As can be seen in the charts (Figure 1), for each of the                             3. original UBD used by Fazly et al. (2010) and generated
three measures the input data generated by our framework is                                 from the Manchester corpus in the CHILDES database
much closer to the manually annotated data from the inter-                                  (UBD-Manchester);
action videos than UBD. Again, for the reasons specified in                             4. IBD generated by our framework as a result of simulations
the Analysis section, we did not use the all condition in the                               with 19 objects in the environment (which was the average
further analysis. For the other three conditions, the Kruskal–                              context size in the analyzed CASA MILA dataset).3
Wallis H-test showed the significant difference in terms of                              For measuring the learning success at each moment, we used
stability (H(2) = 213.822, p < .001), noise (H(2) = 95.725, p < .001) and                effective ratio calculated as the number of words that the
certainty (H(2) = 289.410, p < .001). To examine the pairwise dif-                       learner has acquired at that time, divided by the number of
ferences between the three groups, we used Mann–Whitney                                  words that she heard so far. The growth of the effective ratio
tests, taking into account Bonferroni correction (which re-                              over time is presented in Figure 2. Note that the size of UBD
sulted in .025 level of significance). The difference be-                                (CASA MILA) set is two times smaller than that of the original
tween active and IBD was not significant in terms of noise                               CASA MILA dataset, because only every other natural utter-
(Mdn = 0.400 vs. 0.333; U = 37897.5, nactive = 278, nIBD = 278, p > .025), and sig-      ance could be included into UBD. For UBD (Manchester) and
nificant with only small effect size in terms of scene stability                         IBD the graphs show values averaged over 10 word learning
(Mdn = 0.400 vs. 0.500; U = 30012, nactive = 274, nIBD = 274, p < .001, r = −.174)       simulations.
and certainty (Mdn = 0.000 vs. 0.000; U = 35002.5, nactive = 278, nIBD = 278, p <
.025, r = −.100).    However, the difference between UBD and
                                                                                                                                  UBD (CASA MILA)                                                           UBD (CASA MILA)
IBD was significant with a large effect size for each mea-                                                                        UBD (Manchester)                                                          UBD (Manchester)
                                                                                            Effective Learning Ratio                                                Effective Learning Ratio
                                                                                                                                  CASA MILA                                                                 CASA MILA
sure: scene stability (Mdn = 0.059 vs. 0.500; U = 2586, nUBD = 133, nIBD =                                                        IBD                                                                       IBD
                                                                                                                       1.0                                                                     1.0
274, p < .001, r = −.699), noise (Mdn = 0.000 vs. 0.333; U = 10008.5, nUBD =
139, nIBD = 278, p < .001, r = −.426) and certainty (Mdn = 0.571 vs. 0.000;
                                                                                                                       0.5                                                                     0.5
U = 2451.5, nUBD = 139, nIBD = 278, p < .001, r = −.760). These results con-
firm that data generated by the proposed framework is more
suitable for training and evaluating cognitive models than                                                             0.0                                                                     0.0
                                                                                                                             0   50   100   150   200   250   300                                    0   1000 2000 3000 4000 5000 6000
UBD. We further investigate this claim by using these dif-                                                                             Input Items                                                              Input Items
ferent data sets in an existing model of word learning.
          Case study: learning word meaning                                             Figure 2: Overall model performance on four different
We used the cross-situational word learning model of Fazly                              datasets for 300 (left) and 6500 (right) utterances
et al. (2010) as a case study for our proposed input genera-                               The graph on the left shows the effective ratio over the
tion framework. Our goal is to show that the complexity of                              course of 300 input items, which is slightly more than the
the learning task depends on the properties of the input data,                          size of the CASA MILA dataset. It is clear from these results
and less realistically generated input can considerably sim-                            that the performance of the word learning model is very sim-
plify the task.                                                                         ilar when it is trained on data collected from CASA MILA
Description of the model                                                                and on data generated by our framework (IBD). In contrast,
                                                                                        the model performs much better when it is trained on any of
The model of Fazly et al. (2010) incrementally learns the
                                                                                        the UBD sets. This difference again suggests that UBD is not
meaning of each word (e.g., play) as a probability distribu-
                                                                                        representative of what young language learners have access
tion over all the possible meaning components, each repre-
                                                                                        to, and a more realistic approach to data generation must be
sented as a unique symbol (e.g., P LAY, BALL). At each mo-
                                                                                        applied. The graph on the right shows the same measure over
ment in time, the model receives a new input item, consisting
                                                                                        the course of 6500 utterances (the size of UBD-Manchester).
of an utterance and its (ambiguous) semantic representation,
                                                                                        The same pattern can be seen: there is a considerably large
which is an unordered set of symbols. The model uses its
                                                                                        gap between the learning curves in UBD and IBD cases. It
previous knowledge of word meanings to align each word
                                                                                        is also clear that in the latter case, the size of the input to the
in the current utterance with the most likely symbols in the
                                                                                        learner does not have to be constrained by the amount of data
current scene representation. It then uses these alignments
                                                                                        available in an existing collection.
to update the meaning of each word by accumulating such
cross-situational evidence over time.                                                                                                  Conclusion and discussion
Model performance on different types of input                                           We manually annotated a small dataset of video recordings
We compared the performance of the word learning model on                               of child–adult interactions and collected various types of co-
four different data sets:                                                                   3 Since one of the main parameters of the framework was the con-
1. the manually annotated portion of CASA MILA (active);                                text size, we also investigated whether the learning process would
                                                                                        vary with the number of objects in the environment, but our manipu-
2. UBD generated from the same data set (UBD-CASA                                       lations did not result in changing the overall learning pattern in terms
   MILA);                                                                               of effective ratio.
                                                                                     3000

occurrence frequencies of utterances, utterance types, accom-           context-free grammars with distributional clustering. In
panying actions and action types, action arguments and par-             Proc. of CoNLL’2000 (pp. 105–112).
ticipants, and other objects available in the visual context. Us-     Fazly, A., Alishahi, A., & Stevenson, S. (2010). A probabilis-
ing three quantitative measures, we compared the character-             tic computational model of cross-situational word learning.
istics of these utterances and their surrounding scenes with            Cognitive Science, 34(6), 1017–1063.
the product of the most realistic existing approach to auto-          Frank, M., Goodman, N., & Tenenbaum, J. (2008). A
matic generation of scene representations (Fazly et al., 2010).         bayesian framework for cross-situational word-learning. In
Our analyses show significant differences between the two               Proc. of NIPS’2008 (pp. 457–464).
datasets, and using an existing model of word learning as a           Frank, M., Tenenbaum, J. B., & Fernald, A. (2013). Social
case study further demonstrates that automatically generated            and discourse contributions to the determination of refer-
utterance-based data simplifies the learning task to an unre-           ence in cross-situational word learning. Language, Learn-
alistic scale. However, manual annotation as an alternative             ing and Development.
approach (e.g., Yu & Ballard, 2007; Frank et al., 2013) is not        Kachergis, G., Yu, C., & Shiffrin, R. M. (2009). Frequency
scalable due to the limited quantity of the data available. The         and contextual diversity effects in cross-situational word
hybrid approach that we propose eliminates these problems:              learning. In Proc. of CogSci’2009 (pp. 2220–2225).
we present an input generation framework which can produce            Kishimoto, T., Shizawa, Y., Yasuda, J., Hinobayashi, T., &
an infinite stream of child–adult interaction data containing           Minami, T. (2007). Do pointing gestures by infants pro-
both linguistic and visual information, whose statistical prop-         voke comments from adults? Infant Behavior and Devel-
erties are closely comparable to those of manually annotated            opment, 30(4), 562–567.
data.                                                                 MacWhinney, B. (2000). The CHILDES project: Tools for
   Any data annotation or generation scheme inevitably in-              analyzing talk (3rd ed.). Lawrence Erlbaum Associates.
corporates assumptions about important components and in-             MacWhinney, B. (2010). Computational models of child
formation cues in language learning, which can be seen as               language learning: An introduction. J. of Child Language,
built-in biases brought to the learning task. However, compu-           37(3), 477–485.
tational models need data and will benefit from any attempt           Matusevych, Y. (2012). Modeling child–adult interaction: A
to make this data more naturalistic.                                    computational study of word learning in context Master’s
                                                                        thesis, Tilburg University, the Netherlands.
   An extension of the proposed framework can potentially
                                                                      Moore, C., Angelopoulos, M., & Bennett, P. (1999). Word
provide certain interaction features such as the participants’
                                                                        learning in the context of referential and salience cues. De-
focus of visual attention and head movement. Such extra fea-
                                                                        velopmental Psychology, 35(1), 60–68.
tures can allow computational models to systematically inves-
                                                                      Niyogi, S. (2002). Bayesian learning at the syntax-semantics
tigate the impact of interaction factors in language learning.
                                                                        interface. In Proc. of CogSci’2002 (pp. 697–702).
   The dataset that we analyzed was limited in size and the           Siskind, J. M. (1996). A computational study of cross-
interaction domain (toy playing). We add a parameter to our             situational techniques for learning word-to-meaning map-
framework to account for potential variation in the size of             pings. Cognition, 61(1-2), 39–91.
the visual context. But humans’ linguistic behavior (e.g., the        Steels, L. (1996). Emergent adaptive lexicons. In From Ani-
structural and pragmatic characteristics of utterances) may             mals to Animats 4: Proc. of the Fourth International Con-
also depend on the domain to some extent (e.g., Choi, 2000).            ference on Simulation of Adaptive Behavior (pp. 562–567).
Therefore, a larger and more diverse collection of interaction        Vogt, P., & Haasdijk, E. (2010). Modeling social learning of
videos will provide a more realistic base for estimating the            language and skills. Artificial Life, 16(4), 289–309.
input generation probabilities in our framework. The larger           Vogt, P., & Mastin, J. D. (2013a). Anchoring social symbol
CASA MILA corpus of interaction data that is currently un-              grounding in children’s interactions. Künstliche Intelligenz,
der development (Vogt & Mastin, 2013a) is one suitable can-             27, 145–151.
didate for such expansion.                                            Vogt, P., & Mastin, J. D. (2013b). Rural and urban differences
                                                                        in language socialization and vocabulary development in
                          References                                    Mozambique. Proc. of CogSci’2013.
                                                                      Yu, C., & Ballard, D. H. (2007). A unified model of early
Behrend, D. A. (1990). Constraints and development: A reply             word learning: Integrating statistical and social cues. Neu-
   to Nelson (1988). Cog. Development, 5(3), 313–330.                   rocomputing, 70(13-15), 2149–2165.
Choi, S. (2000). Caregiver input in English and Korean: Use           Yu, C., Smith, L. B., Shen, H., Pereira, A. F., & Smith,
   of nouns and verbs in book-reading and toy-play contexts.            T. (2009). Active information selection: visual atten-
   J. of Child Lang., 27(1), 69–96.                                     tion through the hands. IEEE Transactions on Autonomous
Chouinard, M. M., & Clark, E. V. (2003). Adult reformula-               Mental Development, 1(2), 141–151.
   tions of child errors as negative evidence. J. of Child Lang.,
   30(3), 637–669.
Clark, A. (2001). Unsupervised induction of stochastic
                                                                  3001

