UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Verifying properties from different emotions produces switching costs: Evidence for coarsegrained language statistics and fine-grained perceptual simulation

Permalink
https://escholarship.org/uc/item/69w9r6fc

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Tillman, Richard
Hutchinson, Sterling
Jordan, Sara
et al.

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Verifying properties from different emotions produces switching costs: Evidence for
coarse-grained language statistics and fine-grained perceptual simulation
Richard Tillman (rntllman@memphis.edu)
Department of Psychology/ Institute for Intelligent Systems, University of Memphis
365 Innovation Drive, Memphis, TN 38152 USA
Sterling Hutchinson (schtchns@memphis.edu)
Department of Psychology/ Institute for Intelligent Systems, University of Memphis
365 Innovation Drive, Memphis, TN 38152 USA
Sara Jordan (sara.jordan@mlh.org)
Department of Rehabilitation Services, Speech Language Pathologist/ Methodist North Hospital,
Methodist LeBonheur Healthcare
3960 New Covington Pike Memphis, TN 38128 USA
Max M. Louwerse (maxlouwerse@gmail.com)
Department of Psychology/ Institute for Intelligent Systems, University of Memphis
365 Innovation Drive, Memphis, TN 38152 USA
Tilburg Centre for Cognition and Communication (TiCC), Tilburg University,
PO Box 90153, 5000 LE, Tilburg, The Netherlands
Abstract
We investigated whether emotions are activated during
comprehension of emotion words. In the first part of the
study, an experiment was conducted in which participants
read sentence pairs each describing an emotional state and
then engaged in a judgment task. Sentences were paired to
either match or mismatch in emotion (happy, sad, or angry).
We predicted that the sentences that mismatch in emotion
produced longer reaction times than those where the emotion
was the same, and that shifts between negative emotions had
less of an impact. In the second part of the study, we
calculated the frequency of first-order co-occurrences of
nouns and adjectives related to happy, sad, and angry
emotional states. This analysis demonstrated emotion words
are more often accompanied by similar emotion words.
Match and mismatch of emotion explained RTs as did
statistical linguistic frequencies of the words. The
combination of these two studies contributes to a growing
body of research that supports the importance of both
symbolic and perceptual processing of emotion.
Keywords: emotion; embodied cognition;
cognition; statistical linguistic frequencies.

symbolic

Introduction
Theories of embodied cognition claim that cognition is
fundamentally based in perceptual experiences. That is,
concepts only become meaningful through comprehenders
mentally reenacting prior physical and perceptual
experiences with the concept in the real world (Barsalou,
1999; Barsalou, Simmons, Barbey, & Wilson, 2003;
Glenberg & Kaschak, 2002; Pecher & Zwaan, 2005; Havas,
Glenberg, & Rinck, 2007; Semin & Smith, 2008). For
instance, Glenberg and Kaschak (2002) proposed the action-

sentence compatibility effect whereby language processing
is facilitated when a congruent response motion is used to
respond to sentences describing motion away from or
towards the body. That is, sentences describing motion
away from the body (e.g., close a drawer) were processed
faster when response motions were also moving away from
the body, and vice versa. These results and findings similar
to these demonstrate that linguistic processing is facilitated
through perceptual-motor information (see Leventhal, 1982
for an overview).
Similar to action related sentences, sentences with
emotional content have also provided support for an
embodied cognition account. Mouilso, Glenberg, Havas,
and Lindeman (2007) found that reading ‘angry’ sentences
resulted in faster movements away from the body and
reading ‘sad’ sentences resulted in faster movements toward
the body. In other words, when people read angry content,
they processed the sentence faster with an aggressive action
toward it, whereas ‘sad’ sentences evoke a withdrawal
action, suggesting that emotional language can affect bodily
responses.
Embodied responses have also been linked to cognition
through the facial feedback hypothesis (Strack, Martin, &
Stepper, 1988; Zajonc, Murphy, & Inglehart, 1989). The
facial feedback hypothesis demonstrates that facial
expressions might influence emotional assessments. For
example, when participants were instructed to smile,
cartoons were perceived as more humorous than when
subjects were not smiling (Strack et al., 1988), showing that
bodily states can affect both judgments and cognition.
Most literature supporting an embodied cognition
account, however, demonstrates evidence without physical
manipulation. For example, Pecher, Zeelenberg, & Barsalou

3551

(2003) found that subjects read sentences describing
features within the same modality faster than sentences
describing features of differing modalities. When
participants read a sentence like apples can be tart followed
by the sentence apples can be sweet (describing the same
gustatory modality) response times were faster for the
second sentence when the second sentence did not describe
a shift in modality, such as is the case when a visual
modality was presented in strawberries can be red or radios
can be loud. The modality of the target words impacted how
those words were perceived. Processing costs incurred from
the mismatched sentences resulted from a perceptual
modality shift, suggesting that perceptual embodied features
indeed impact language processing times.
Recently, the modality switching costs have been
explained by language statistics (Louwerse & Connell,
2011). By computing the word frequencies of the cooccurrences of modality words from a large corpus of
English, Louwerse and Connell were able to identify
modality shifts similar to Pecher et al. (2003). This analysis
was not only applicable to the adjectives (e.g., tart – sweet
being more frequent than tart – red or sweet – red), but also
to concept words (e.g., apples – strawberries being more
frequent than apples – radio or strawberries – radio).
Louwerse and Connell (2011) showed that these frequencies
explained the response times that were attributed to an
embodied cognition account. That is, faster response times
were best explained by language statistics, slower response
times were best explained by perceptual simulations.
Louwerse and Connell’s explanation was that the linguistic
system offers a ’quick and dirty’ shallow heuristic that can
provide good enough performance in cognitive tasks
without recourse to deeper conceptual processing in a
perceptual simulation system. On the other hand, ultimately
concepts are grounded and can be perceptually simulated.
The explanation by Louwerse and Connell can be captured
in the Symbol Interdependency Hypothesis, which proposed
that conceptual processing can be explained by both symbol
and embodied mechanisms (Louwerse, 2007; 2008; 2011).
When we encounter a word, we garner a rough meaning
from its linguistic (symbolic) neighbors using language
statistics, but to fully ground the word, we perceptually
simulate its physical and somatosensory features. Thus,
words can rely on other words to establish a fuzzy sense of
meaning without necessarily always being grounded
themselves. In other words, perceptual information is
encoded in language, such that mental representations are
both perceptual and linguistic. Human beings can rely on
such a linguistic short-cut when processing language in real
time. However, if a deeper meaning or understanding is
needed, grounding the world in perceptual experiences
provides rich sensorimotor information about meaning.
Importantly, language has encoded sensorimotor
information, such that language users can utilize these cues
in cognitive processes.
In short, the Symbol Interdependency Hypothesis
proposes the following: 1) language encodes perceptual

information; 2) language users rely both on language
statistics and perceptual simulation in cognitive processes;
3) the relative dominance of language statistics and
perceptual simulation factors is modified by stimulus type
and task.
Although modality shifts have been shown to support the
Symbol Interdependency Hypothesis (Louwerse & Connell,
2011), the question can be raised whether the finding for
modality shifts can be extended to other semantic domains
that have shown embodiment effects, such as emotions. In
the current study we investigated whether (a) verifying
properties from different emotions for concepts produces
switching costs, similar to the modality shifts; (b) whether
language has encoded the emotions of words, similar to the
modality of words; (c) whether emotion shifts can be
explained by a language statistics account.
To explore these questions we applied Pecher et al.’s
(2003) modality shift paradigm to emotions. Emotional
sentences shifted from happy to sad, sad to happy, happy to
angry, angry to happy, sad to angry, and angry to sad.
According to an embodied cognition account, switches
between emotions should take longer to process than nonswitches
(happy-happy,
sad-sad,
angry-angry).
Alternatively, according to a language statistics account, cooccurrence frequencies of word pairs should be able to
equally account for subject RTs. We thereby made two
hypotheses: (1) as with modality shifts, emotion shifts
would take longer to process than non-shift sentence pairs,
which would be in support of an embodied cognition
account and (2) the same pattern of emotion shift cost would
emerge from language such that emotion words that
matched in valence would co-occur more frequently than
the words that did not match in valence, which would be in
support of a linguistic account.

Experiment 1: Emotion Shift
Method
Participants Thirty-three undergraduate students enrolled
in an introductory psychology course participated for course
credit.
Materials Sixty emotion sentences were created,
following the method described in Pecher et al. (2003) with
each sentence in the format X can be Y. There were 3
experimental types of emotions depicted in the sentences:
angry, happy, and sad. For example, birthdays can be happy
(happy emotion), and insults can be devastating (sad
emotion).
The reason we selected angry, happy, and sad emotions
was motivated by work from Isenhower et al. (2003) who
found that people tend towards more positive states of
emotion. That is, switching from positively valenced to
negatively valenced emotions yields a greater disruption and
requires additional cognitive processing. Further motivation
came from a more recent study by Stein and Sterzer (2012).
In this study, Stein and Sterzer demonstrated that people
identify happy faces more quickly than angry faces. We

3552

therefore selected happy, sad, and angry words, and thus
had one positively valenced emotion (happy) and two
negatively valenced emotions (sad and anger).
Procedure Participants were seated at a computer in a
standard computer lab. The instructions for the experiment
were presented on the screen and read aloud by the
experimenter. Five practice items preceded the experimental
phase to ensure participants understood the task.
Participants saw sentences one at a time in the center of the
screen and then were asked to respond to the question Is the
characteristic true of the items it described? Participants
pressed designated yes or no keys on the keyboard. RT and
accuracy were recorded.

Results
Incorrect responses were not included in the analyses. RT
outliers were defined as 2.5 SD above the mean per subject
per condition and were removed from the analysis. This
removal affected less than 3.6% of the data.
A mixed-effect analysis was conducted on RTs with
emotion shift as the fixed factor and participants and items
as random factors (Baayen, Davidson, & Bates, 2008). The
model was fitted using the restricted maximum likelihood
estimation (REML) for the continuous variable (RT). F-test
denominator degrees of freedom were estimated using the
Kenward-Roger’s degrees of freedom adjustment to reduce
the chances of Type I error (Littell, Stroup, & Freund,
2002). Participants and items were treated as random factors
in the analysis.
For the factor emotion shift no differences were found in
RT, F(1,114) = .431, p = .513. This is somewhat surprising
given that an emotion shift was predicted to increase RTs
akin to the modality shifts. However, when individual
emotion pairs were separated by transition (e.g., happy-sad,
happy-angry), RT differences were obtained with an
emotion shift from happy sentences to sad sentences,
F(1,421) = 30.41, p < .001, with slower RTs for the shift
than no-shift (i.e., a happy sentence followed by a happy
sentence). Also, when shifting from happy sentences to
angry sentences a significant difference was found between
the two conditions, F(1,380) = 20.82, p < .001, there were
slower RTs for the emotion shift sentences than no-shift.
When the sad to angry sentences were compared, again a
difference approaching significance was found between
emotion shift and no-shift conditions, F(1,455) = 5.88, p <
.056, where the shift between sentences yielded longer RTs
than no-shift. In contrast, the comparison of sad to happy
sentences yielded no significant differences between
emotion shift and no-shift sentences, F(1,395) = .02, p <
.89. When switching from angry to happy sentences, a
significant effect was found, F(1,485) = 20.69, p < .001,
again with faster RTs for the emotion shift sentences than
no-shift. Finally, a significant effect was found when
switching from angry to sad sentences, F(1,430) = 5.05, p <
.03, however with faster RTs for the emotion shift sentences
than the no-shift sentences.

In summary, a shift from happy to sad, happy to angry,
sad to angry, and angry to happy yielded significant results,
while the shift from sad to happy was not significant. Figure
1 shows the means and standard deviations for each emotion
shift pair.
Even though no overall effect for emotion shift was
found, patterns for specific emotion transitions did show
shift effects, with specific emotion to emotion shifts
resulting in longer RTs than non-shifts. More specifically,
the shifts from happy to the two negative emotions, shows a
significant increase in RT. The emotion shift from angry to
happy was also significant, but showed a decrease in RT
from angry followed by angry. This is in line with Stein and
Sterzer (2012), who found that people are quicker to
identify happy faces, rather than angry faces. We interpret
this decrease in RT in terms of the nature of the shift. Angry
followed by angry produces the longest RT, while happy
followed by happy produces the shortest RT. As there is a
tendency to prefer to shift toward a more positive state
(Isenhower, Frank, Kay, & Carello, 2010), the reaction
times for the non-shifts reflected this. Moreover, the shift
from angry to happy decreases from its origin (angry
followed by angry), because of the natural tendency to shift
to the more positive state. This is supported by the
significant differences when emotion shifts took place
between angry and happy, angry and sad, and sad and angry.
However, we still are unable to determine whether an
embodiment effect exists for emotion switching, as there
was no overall effect for shifts as there were for Pecher et al.
(2003), but only specific emotion to emotion effects.

Figure 1. Emotion shifts, means, and standard deviations.
** p < .01, * p < .05, n.s. not significant. The means and
standard deviations located at the emotion words indicate no
emotion shift (e.g., a happy sentence followed by a happy
sentence).
To determine whether or not overall shifts for emotions
occurred, we ran a second experiment whereby the

3553

embodiment effect would be enhanced by an embodied
facial feedback paradigm.

Experiment 2: Facial Feedback Hypothesis
In order to determine if emotion switching indeed supports
an embodied cognition account, we examined the effects of
the facial feedback hypothesis (Strack, et al., 1988; Zajonc,
et al., 1989) by assessing the effects different conditions
(frowning or smiling) had on RTs when judging emotion
shift sentences. We hypothesized that neither frowning nor
smiling would produce a significant effect between the
negative emotions (sadness and anger), due to the trend
towards positive (Isenhower et al., 2010). In addition, we
hypothesized that the specific emotion to emotion shifts
found in Experiment 1 would show similar patterns.

sadness and anger; it would stand to reason why there were
no significant differences between these two conditions as
they are both negative emotions and the motor system
necessary for their simulation was already active,
facilitating the effect. Figure 2 shows the means and
standard deviations for each emotion shift pair, the shift
direction, and the no shift means and standard deviations.

Method
Participants Twenty-six undergraduate students enrolled in
an introductory psychology course participated for course
credit.
Materials The same materials were used as in
Experiment 1.
Procedure The procedure was the same as that used in
Experiment 1, with one important addition. Participants
were also randomly assigned to one of two facial feedback
conditions (Strack et al., 1988). In the one condition, the
participants held a pen in their lips (n = 15) to simulate
frowning; in the other, the participants held a pen in their
teeth (n = 11) to simulate smiling.

Figure 2. Emotion shifts, means, and standard deviations
for frowning facial feedback condition. ** p < .01, * p <
.05, n.s. not significant.

Results
As in Experiment 1, emotion shifts did not yield a
significant difference in RT, F(1, 117.27) = .16, p =.70.
Furthermore, there seemed to be no main effect of the facial
feedback conditions, F(2, 78.24) = .73, p = .49. Next, we
investigated the emotion transitions per facial feedback
condition (smiling vs. frowning).
Frowning Facial Feedback When participants held the
pen in their lips to simulate frowning, the shift from happy
to sad was significant as it was in the previous experiment
without the facial feedback task, F(1,236) = 6.69, p = .01,
with higher RTs for the shift sentences than no-shift. The
shift from happy to angry was also significant as found in
the previous experiment, F(1,202) = 8.36, p < .004, with
higher RTs for the shift sentences than no-shift. Also the
shift from angry to happy was significant as previously
found in Experiment 1, F(1,248) 4.31, p < .04, with lower
RT for the shift sentences than no-shift. Again, this is in line
with Isenhower et al. (2010) and Stein and Sterzer (2012), in
that the preference is to shift from a negative state to a
positive state. This is especially true given the fact that
participants were frowning due to the facial feedback task.
The shifts from sad to angry and angry to sad were found to
be non-significant, unlike the findings in Experiment 1.
These results lend support to the facial feedback hypothesis,
in that frowning (pen held in lips) is associated with both

Smiling Facial Feedback When participants held the pen
in their teeth to simulate smiling, the shift from happy to sad
was significant, F(1,168) = 8.98, p < .003, with higher RT
for the shift sentences than no-shift . The shift from happy
to angry was significant, F(1,164) = 15.48, p < .0001, with
higher RTs for the shift sentences than no-shift . The shift
from sad to happy approached significance, F(1,134) = 3.81,
p < .053, with lower RT for the shift sentences than no-shift.
Finally, the shift from angry to happy was also significant,
F(1,179) = 17.84, p < .001, with lower RT for the shift
sentences than no-shift . Again, the decrease in RT for angry
to happy is in accord with Stein and Sterzer (2012). The
shifts from sad to angry and angry to sad were not found to
be significant. Figure 3 shows the means and standard
deviations for each emotion shift pair, the shift direction,
and the no shift means and standard deviations. The main
difference between the smiling condition and the previous
frowning condition is the significant difference found in the
sad to happy shift, which was not found in Experiment 1, or
the frowning facial feedback condition. This difference
supports the findings by Isenhower et al. (2010), in that
since people have a tendency to tend towards a positive
state, which they have in part done by smiling.

3554

emotion shift was present (M = 1.11, SE = .054). This
pattern was also found for just the nouns F(1, 3479) =
148.11, p < .001, with word pairs where there was no
emotion shift (M = 4.29, SE = .08) being more frequent than
word pairs where an emotion shift was present (M = 2.60,
SE = .11). Again, this pattern was found for adjectives, F(1,
3598) = 279.17, p < .001, with word pairs where there was
no emotion shift (M = 2.53, SE = .05) being more frequent
than word pairs where an emotion shift was present (M =
1.00, SE = .07).
In addition, we also compared the log frequencies of each
of the word pairs to the experimental RT over the collapsed
match and mismatch conditions (extracted from
Experiments 1 and 2). Language statistics significantly
predicted RTs, F(1, 113.564) = 34.53, p < .001. However,
language statistics did not predict emotional transitions.
Statistical linguistic frequencies explained RTs of general
emotion shifts, but not RTs of specific emotion transitions.
Figure 3. Emotion shifts, means, and standard deviations
for smiling facial feedback condition. ** p < .01, * p < .05,
n.s. not significant

General Discussion

Corpus Linguistic Study
So far, the results seem to suggest that emotional states can
be based in embodied cognition, as some emotion to
emotion shifts seem to indicate that emotion switching
usually incurs some sort of processing cost. However, this is
not the whole picture, as it does not take into consideration
the linguistic nature of the words. We therefore investigated
whether emotion shifts are encoded in language (Louwerse,
2008; Louwerse & Connell, 2011). To do this we calculated
the frequency of first-order co-occurrences of all the
possible combinations of the nouns and adjectives in the
present study by utilizing the Web 1T 5-gram corpus
(Brants & Franz, 2006). This corpus consists of 1 trillion
word tokens (13,588,391 word types) from 95,119,665,584
sentences. The volume of the corpus allows for an extensive
analysis of patterns in the English language. The frequency
of co-occurrences of the word pairs was computed for
bigrams, trigrams, 4-grams and 5-grams. For instance, the
frequency of the phrase birthdays can be happy {happy,
birthday} was determined by considering these words next
to one another {happy birthday}, with one word in between
{happy w1 birthday}, with two {happy w1 w2 birthday},
three intervening words {happy w1 w2 w3 birthday}, and so
on.
A mixed effects analysis was conducted on the frequency
of co-occurrences of the emotion adjectives and the noun
referents. The independent variable was whether the
emotion words were the same or different emotion, and the
log frequency of the word pair was the dependent variable.
For all possible combinations of both nouns and
adjectives, the log frequency of the co-occurrences were
found to be significant, F(1, 7078) = 212.76, p < .001, with
word pairs where there was no emotion shift (M = 2.08, SE
= .04) being more frequent than word pairs where an

Previous studies have found that two sentences that elicit
a modality shift produce cognitive switching costs,
compared to sentences that describe the same modality
(Pecher et al., 2003). This finding has been reported as
evidence for an embodied cognition account, because the
increased RTs are an indication that comprehenders
perceptually simulate the sentences. Others have shown that
modality is encoded in language. Based on language
statistics, concepts and their features can be categorized in
visual, auditory, olfactory and gustatory modalities
(Louwerse & Connell, 2011). Moreover, when the RTs for
modality shifts were investigated with both language
statistics and perceptual simulation as independent
variables, fast RTs were best explained by language
statistics and slower RTs were best explained by perceptual
simulation. Louwerse and Connell (2011) concluded that
language statistics serves as a coarse-grained system that
serves as a shallow heuristic. Perceptual simulation, on the
other hand, serves deeper conceptual processing. The idea
that language encodes perceptual information and that these
linguistic cues can be used by language users in shallow
comprehension tasks is predicted by the Symbol
Interdependency Hypothesis and supported by various
studies (Louwerse, 2008; Louwerse & Hutchinson, 2012;
Louwerse & Jeuniaux, 2008; 2010).
The current study investigated whether emotion shifts
mimicked the patterns found for previous studies
investigating modality shifts. Even though across three
experiments no general effect was found for shifts, specific
transitions between emotions did yield differences in RTs.
Moreover, evidence was found that language encodes
emotion shifts, and language statistics explained RTs for
these general shifts.
The findings of the current study are supported by the
Symbol Interdependency Hypothesis as well as by findings
reported in other studies. Language statistics explained
coarse-grained emotion shifts. However, language statistics

3555

did not explain fine-grained shifts. On the other hand,
assuming that a perceptual simulation system is responsible
for the other RT differences that were obtained in the two
experiments, the perceptual system did not explain the
coarse-grained differences in general emotion shifts, but did
explain the fine-grained shifts between specific emotions.
These results provide further evidence for the theory that
conceptual processing is both linguistic and embodied,
whereby less precise linguistic processes account for general
patterns in processing, whereas perceptual simulation
provides the fine-tuning.

References
Baayen, R. H., Davidson, D., & Bates, D. (2008). Mixedeffects modeling with crossed random effects for subjects
and items. Journal of Memory and Language, 59, 390412.
Barsalou, L. W. (1999). Perceptual symbol systems,
Behavioral and Brain Sciences, 22, 577–660.
Barsalou, L. W., Simmons, W.K., Barbey, A.K., & Wilson,
C.D. (2003). Grounding conceptual knowledge in
modality-specific systems, Trends in Cognitive Sciences,
7, 84–91.
Brants, T., & Franz, A. (2006). Web 1T 5-gram version 1.
Philadelphia: Linguistic Data Consortium.
Brysbaert, M. (2007). The language-as-ﬁxed-effect fallacy:
Some simple SPSS solutions to a complex problem
(version 2.0). Royal Holloway, University of London.
Technical report.
Ekman, P., & Rosenberg, E. L. (2004). What the face
reveals: basic and applied studies of spontaneous
expression using the facial action coding system (FACS),
New York: Oxford University Press.
Glenberg, A. M. (2007). Language and action: creating
sensible combinations of ideas. In G. Gaskell (Ed.) The
Oxford handbook of psycholinguistics (pp. 361-370).
Oxford, UK: Oxford University Press.
Glenberg, A. M., & Kaschak, M. P. (2002). Grounding
language in action. Psychonomic Bulletin & Review, 9,
558-565.
Havas, D., Glenberg, A. M., & Rinck, M (2007). Emotion
simulation during language comprehension. Psychonomic
Bulletin & Review, 14, 436-441.
Isenhower, R. W., Frank, T. D., Kay, B. A., & Carello, C.
(2010). Capturing and quantifying the dynamics of
valenced emotions. Nonlinear Dynamics Psychology and
Life Sciences, 16(4), 397-427.
Leventhal, H. (1982). A perceptual motor theory of
emotion. Social Science Information, 21, 819-845.
Littell, R. C., Stroup, W. W., & Freund, R. J. (2002). SAS
for Linear Models, 4th Edition. Cary, NC: SAS
Publishing.
Louwerse, M. M. (2007). Symbolic embodied
representations: A case for symbol interdependency. In T.
Landauer, D. McNamara, S. Dennis, & W. Kintsch (Eds.),
Handbook of latent semantic analysis. Mahwan, NJ:
Erlbaum.

Louwerse, M. M. (2008). Embodied relations are encoded in
language. Psychonomic Bulletin & Review, 15, 838-844.
Louwerse, M. M. (2011). Symbol interdependency in
symbolic and embodied cognition. TopiCS, 3, 273-302.
Louwerse, M. M., and Connell, L. (2011). A taste of words:
linguistic context and perceptual simulation predict the
modality of words. Cognitive Science, 35, 381–398.
Louwerse, M. M., & Jeuniaux, P. (2010). The Linguistic
and Embodied Nature of Conceptual Processing.
Cognition, 114, 96-104.
Louwerse, M. M. & Jeuniaux, P. (2008). Language
comprehension is both embodied and symbolic. In M. de
Vega, A. Glenberg, & A. C. Graesser (Eds.), Symbols and
Embodiment: Debates on Meaning and Cognition.
Oxford, England: Oxford University Press.
Mouilso, E., Glenberg, A. M., Havas, D. A., & Lindeman,
L. M. (2007). Differences in action tendencies distinguish
anger and sadness after comprehension of emotional
sentences. Proceedings of the 29th Annual Cognitive
Science Society, 14, 1325-1330.
Pecher, D., & Zwaan, R. (Eds.) (2005). Grounding
cognition: The role of perception and action in memory,
language, and thought. New York: Cambridge University
Press.
Pecher, D., Zeelenberg, R., & Barsalou, L.W. (2003).
Verifying different-modality properties for concepts
produces switching costs. Psychological Science, 14, 119124.
Semin, G. & Smith, E. (Eds.) (2008). Embodied grounding:
Social, cognitive, affective, and neuroscientific
approaches. New York: Cambridge University Press.
Stein, T., & Sterzer, P. (2012). Not just another face in the
crowd: Detecting emotional schematic faces during
continuous flash suppression. Emotion, 12, 988-996.
Strack, F., Martin, L. & Stepper, S. (1988). Inhibiting and
facilitating conditions of the human smile: A
nonobtrusive
test
of
the
facial
feedback
hypothesis. Journal
of
Personality
and
Social
Psychology, 54, 768-777.
Zajonc, R. B., Murphy, S. T., & Inglehart, M. (1989).
Feeling and facial efference: Implications for the vascular
theory of emotion. Psychological Review, 96(3), 395-416.

3556

