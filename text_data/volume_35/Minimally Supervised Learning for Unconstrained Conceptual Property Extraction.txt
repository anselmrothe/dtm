UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Minimally Supervised Learning for Unconstrained Conceptual Property Extraction
Permalink
https://escholarship.org/uc/item/1tb8d62g
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Kelly, Colin
Korhonen, Anna
Devereux, Barry
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

         Minimally Supervised Learning for Unconstrained Conceptual Property
                                                                Extraction
              Colin Kelly (colin.kelly@cl.cam.ac.uk), Anna Korhonen (anna.korhonen@cl.cam.ac.uk)
                                              Computer Laboratory, University of Cambridge
                                           15 JJ Thomson Avenue, Cambridge, CB3 0FD, UK
                                         Barry Devereux (barry@csl.psychol.cam.ac.uk)
                                Centre for Speech, Language, and the Brain, University of Cambridge
                                                Downing Street, Cambridge, CB2 3EB, UK
                             Abstract                                    Table 1: Top ten properties from McRae norms with produc-
                                                                         tion frequencies for knife and pig.
   We present a highly performant, minimally supervised system
   for the challenging task of unconstrained conceptual property                            knife                       pig
   extraction (e.g., banana is fruit, spoon used for eating). Our               is sharp               29    an animal           21
   technique employs lightly supervised support vector machines                 used for cutting       25    lives on farms      20
   to acquire promising features from our corpora (Wikipedia and                is dangerous           14    is pink             20
   UKWAC) and uses those features to anchor the search for plau-                has a handle           14    has a tail          17
   sible unconstrained relations in our corpus. We introduce a                  has a blade            11    has a curly tail    15
   novel backing-off method to find the most likely relation for                a weapon               11    has a snout         12
   each concept/feature pair and produce a number of metrics                    a utensil              9     eaten as bacon      11
   which act as potential indicators of true relations, training our            made of steel          8     oinks               9
   system using a stochastic search algorithm to find the opti-                 is serrated            8     is fat              8
   mal reweighting of these metrics. We also introduce a human                  found in kitchens      8     is dirty            8
   semantic-similarity dataset; our output shows a strong corre-
   lation with human similarity judgements. Both our gold stan-          hope to extract features for a given concept as well as those
   dard comparison and direct human evaluation results improve
   on those of previous approaches, with our human judgements            features’ relationship with that concept; specifically, we aim
   evaluation showing a significant 20 percentage point perfor-          to extract properties in the form of concept relation feature
   mance increase.                                                       triples (e.g., knife used for cutting, pig lives on farm), where
                                                                         both the relation and the feature are unconstrained. Our task
                         Introduction                                    is particularly challenging because while we seek a very spe-
Recent theories in cognitive psychology attest a property-               cific ‘type’ of information (namely, conceptual properties),
based, distributed and componential model of conceptual                  there is an enormous amount of variation across the features
representation for concrete concepts (e.g., elephant, screw-             and relations of properties which exhibit such characteristics.
driver ) in the brain (Farah & McClelland, 1991; Tyler, Moss,               Previous approaches to our specific conceptual property
Durrant-Peatfield, & Levy, 2000; Randall, Moss, Rodd,                    extraction task (Baroni, Murphy, Barbu, & Poesio, 2009; De-
Greer, & Tyler, 2004). To explore the validity of these the-             vereux, Pilkington, Poibeau, & Korhonen, 2009; Kelly, De-
ories, researchers employ real-world knowledge taken from                vereux, & Korhonen, 2010, 2012) have been successful to
property norming studies where human volunteers are asked                varying degrees, however each has suffered from limitations.
to list properties for concepts. McRae, Cree, Seidenberg, and            Baroni et al., for example, did not explicitly offer relations
McNorgan (2005) performed the largest such study to date,                between their extracted concepts and features. The relations
collecting properties for over 500 concrete nouns (we call               extracted by the Devereux et al., system were rather unso-
these the ‘McRae norms’). Some example properties from                   phisticated, with the relation corresponding to the verb found
these norms can be found in Table 1.                                     along the grammatical relation path linking concept to fea-
   However, as has been widely discussed (Murphy, 2002;                  ture. The Kelly et al. (2010) system had reasonable perfor-
McRae et al., 2005), these studies suffer from a number of               mance but was founded on manually constructed rules and
weaknesses. For example, human participants often under-                 relied heavily on WordNet for its feature selection.
report certain properties, even when they are facts presum-                 The system of Kelly et al. (2012) approached this task as
ably known by the volunteers: though all participants are                one of relation classification. The relations generated were
likely to have known that animals have hearts, has heart is              derived directly from its training set; it was therefore unable
not reported as a property for any animal concept. Similarly,            to posit new or unseen relationships between its extracted
is animal is listed as a property of all animals in the norms            concepts and features. We believe their feature output, how-
while breathes is only cited as a property for whale. A re-              ever, was promising and we extend and enhance their feature
lated issue is inconsistency across similar concepts: has legs           extraction method in the first component of our own system.
is listed as a property of leopard but is absent for tiger.                 Our system works by first employing a wealth of lexical,
   Our task is to automatically extract such conceptual repre-           syntactic and semantic machine-learning attributes to train a
sentations from large text corpora using NLP techniques. We              support vector machine for feature-extraction. Unlike other
                                                                     746

approaches, we make heavy use of unlabelled training data,               distance (or margin) to the nearest training data points of each
rendering our system only very lightly supervised. Next, we              class. This plane is subsequently used to classify unseen data
return to our unlabelled corpus to find relations for the ex-            points. SVMs can also be extended to the multi-class case.
tracted features, using a novel, probabilistically motivated                We trained an SVM by constructing paths through each
backing-off technique. In doing so, we are not constrained               sentence’s GR-POS graph from the concept to prospective
by relations found in the McRae norms: our method allows                 features and used the GR path labels, POS tags, relation verb
for the extraction of any relation.                                      instances and path-length as machine learning attributes. We
                                                                         augmented this (mostly syntactic) set of machine learning at-
                              Data                                       tributes to incorporate additional semantic and lexical infor-
Recoded norms                                                            mation: bigrams and concept/feature clusters. 2 The intuition
We used the same set of recoded norms employed by Kelly                  behind this was that similar types of concepts/features (as
et al. (2012) to train our system. This set, containing 510              exhibited by cluster membership) might also exhibit similar
concepts in total, is a coding of an anglicised version1 of the          types of relationships (e.g., ‘tool’ concepts and used for rela-
McRae norms into a uniform concept relation feature for-                 tions); the aim was to enable the SVM to detect the regulari-
mat, where each feature and concept contain one word; the                ties that exist in the relationships between different semantic
relation slot can contain one or more words.                             classes of concepts and features.
                                                                            Every possible attribute across the training set corre-
Corpora                                                                  sponded to a distinct dimension of the vector space. The
We used Wikipedia and the more general UKWAC corpus                      majority of the co-ordinates of the training data points took
(Ferraresi, Zanchetta, Baroni, & Bernardini, 2008), contain-             binary values depending on whether the dimension’s corre-
ing English-language webpages, as corpora. Together these                sponding attribute appeared in the path (except the clustering
offered a suitable balance of general and encyclopaedic text.            and path-length attributes which took integer values). Each
We used the C&C-parser (Clark & Curran, 2007) to extract                 training data point was labelled with its relation (or ‘class’).
grammatical relations (GRs) and part of speech (POS) infor-              Learning instances We applied the SVM Light software3
mation from sentences, allowing us to construct a GR-POS                 (Joachims, 1999) to our learning attributes to extract an SVM
graph for each. We trained our system on the corpora indi-               score (the sum of absolute values of the decision function val-
vidually and in combination.                                             ues, which can be interpreted as a measure of confidence of
Chunking                                                                 the SVM in its classification) for each concept-feature pair.
                                                                         We also calculated log-likelihood (LL) (Dunning, 1993) and
We also used chunked versions of our two corpora. Chunk-
                                                                         pointwise mutual information (PMI) (Church & Hanks, 1990)
ing is a technique which identifies the constituent blocks of
                                                                         statistics across the top 200 returned concept-feature pairs for
a sentence (verb phrase, noun phrase, prepositional phrase,
                                                                         each concept.
etc.). To chunk our corpora, we used the Apache OpenNLP
                                                                            Previous work has ignored a large amount of potentially
1.5 suite (Baldridge, 2005), using the Tokenizer, POS Tagger
                                                                         instructive training data by only examining sentences which
and Chunker tools. The various components of the suite were
                                                                         link entities explicitly found in the training set. However,
trained using models supplied with the OpenNLP package.
                                                                         the use of ‘negative’ information could prove informative
                             Method                                      and therefore we trained on all GR-POS paths linking one
                                                                         of our concepts to any potential feature term4 in each sen-
We trained our system with 466 of the 510 concepts in the an-
                                                                         tence. The size of our training set was 5.52 million instances
glicised McRae set to fix our training parameters and evalu-
                                                                         for the Wikipedia corpus and 20.07 million instances for the
ated with the remaining 44 concepts, those in the ESSLLI ex-
                                                                         UKWAC corpus.5 As we were unaware of the nature of the
pansion set (Baroni, Evert, & Lenci, 2008) (discussed later).
                                                                         relationship between these concept/feature terms, we labelled
Feature derivation                                                       these unknown training paths as unknownrel.
In the first stage we focussed on extracting terms relevant to              Our system was therefore only very lightly supervised:
our concepts in order to generate a promising set of features,           only 6.8% of the UKWAC input and 8.7% of the Wikipedia
similar to those found in our norms.                                     input to the system was labelled with relations drawn from
                                                                         the McRae norms. Consequently, our SVM classified every
Machine learning attributes Support vector machines
                                                                             2 We generated 50 and 150 clusters for the concepts and features
(SVMs) are non-probabilistic binary linear classifiers which
take a set of input data and predict, for each given input,              respectively using hierarchical clustering on WordNet.
                                                                             3 The multi-class implementation, SVM Multiclass (v. 2.20).
which of two possible classes it corresponds to. This works                  4 Potential features were defined as all adjectives and singu-
by plotting training data points in a high-dimensional space             lar/plural nouns in a sentence.
and separating them with a hyperplane which has the largest                  5 Due to memory constraints associated with the very large num-
                                                                         ber of training instances, we were only able to train our UKWAC
    1 See Taylor, Devereux, Acres, Randall, and Tyler (2011) for de-     models on one third of the UKWAC corpus; we selected every third
tails.                                                                   learning pattern for training.
                                                                     747

concept/feature pair into the unknownrel relation class. We          • [NP Mirrors_NNS] [VP are_VBP found_VBN] [PP in_IN]
therefore ignored the relation output from this stage of the            [NP the_DT bedroom_NN] became mirror found in bedroom
                                                                     • [NP Most_JJS cats_NNS] [VP have_VBP] [NP furry_NN
system, instead using the top 200 returned concept/feature              tails_NNS] became cat have tail
pairs ranked by their SVM scores as input to the next stage.         • [NP The_DT microwave_NN] [VP was_VBD running_VBG]
In this way, we were interpreting a higher-rated SVM score              [PP on_IN] [NP electricity_NN] became microwave run
                                                                        on electricity
as a proxy for the likelihood that a feature would have some
kind of relationship with the concept at hand.
                                                                     Relation selection
Relation extraction                                                  The third stage of our system worked by taking each con-
                                                                     cept –feature pair from both the SVM and chunking output,
The underlying hypothesis of our relation extraction stage           and finding the best relation for that pair from the chunking
was that if we found sequences of chunks in our corpus sen-          output to generate a triple. It also assigned to that triple a
tences which were anchored at each end by a known concept            number of metrics relating to its constituent parts, their rela-
and feature (from the previous stage), and those chunks’ la-         tive frequency and association scores.
bels matched the labels of our chunked property norms, then             We assumed that each concept –feature pair had one cor-
we could use the surface text of the chunk(s) between the an-        responding relation. We called the set of extracted triples gen-
chors as the relation in our concept relation feature format.        erated by Stage 2, T (with triples (c, r, f ) ∈ T ) and the set of
Chunk pattern selection To decide which patterns of                  all extracted relations from Stage 2, R. For each concept, we
chunks were likely to be indicative of property norm rela-           also generated a final potential feature set, Fc , which, for a
tions, we turned to our training set. We passed the full text        given concept, was the union of the top 200 features from
of the non-ESSLLI McRae norms through the chunker, and               Stage 1 (ranked by their SVM score) and the top 200 features
manually examined the output for chunk label patterns likely         from Stage 2 (ranked by their frequency in the extracted rela-
to indicate relations.                                               tions, but excluding features which appeared only once).
   Using this output, we created a ruleset for selecting sen-           We defined Concept Feature Frequency (CFF) to be the
tence fragments (chunk sequences) which were similar in              number of times a concept, c, and feature, f , co-occurred
structure to our property norms. We called a sequence of three       across our extracted relations:
labelled chunks a three-chunk, a sequence of four chunks a                               CFF(c, f ) =    ∑ freq(c, r, f )               (1)
four-chunk, etc. We employed the first four most frequent                                                r∈R
label combinations (NP VP NP; NP VP PP NP; NP VP ADJP;                  We also calculated a Distinct Relation Score which mea-
and, NP VP ADVP) to form our ruleset; together these covered         sured the number of distinct relations linking c to f :
95.6% of the three- and four-chunk label patterns generated                    DRS(c, f ) = |Dc, f | for Dc, f = {r : (c, r, f ) ∈ T }  (2)
from our training set. By using the NP VP PP NP-labelled
four-chunks we were able to extract multi-word, prepositional           We next wanted to choose relations for our various con-
verbs (e.g., worn on, used for) as potential relations: previous     cept –feature pairs, (c, f ) ∈ C × Fc . We did this in three steps.
approaches to our task have not attempted this.                      Step 1 For each concept, c, and feature, f , we iterated
Chunk pre-selection We needed to select those chunks                 through all relations relating to that pair and calculated an
most relevant to our relation extraction task. To do this we         Exact Match Score:
passed through our chunked corpus, generating sets of 3 and                         EMS(c, f ) = max{freq(c, r, f ) : r ∈ R}            (3)
4 sequential chunks and pre-selecting those which were rele-
vant to our concepts. Our criterion for relevancy at this stage         If EMS(c, f ) > 0 then we selected as our best relation, r̂,
was that the final term contained within the first chunk, when       the relation corresponding to that score. If there was more
lemmatised, corresponded to a training concept.                      than one relation with the same score, then we chose the least
                                                                     common (i.e., that which had the lowest frequency across all
Chunk to triple conversion Having pre-selected our                   our relations). If EMS(c, f ) = 0 then we left r̂ undefined.
chunks we generated triples from the chunk text. For three-
chunks we did this by simply taking the final term in the first,     Step 2 Our first step only retrieved a relation if there was an
second and third chunks and lemmatising each to give our             exact match amongst our relation extraction output.
concept, relation and feature terms respectively. For four-             If there wasn’t, we took a split approach; given a particular
chunks we followed the same process for the first and fourth         concept, c, and feature, f , we calculated separate probabili-
chunks to yield our concept and feature. To extract the re-          ties across all our relations of c occurring with each relation,
lation we took the final term of the second (VP) chunk and           and of f occurring with each relation. We then calculated for
compounded it with the final term of the third (PP) chunk;           each relation, r, a combined score for the combination of c,
the only exception to this was if the POS of the final term of       r and f by multiplying the constituent probabilities together.
the second chunk was VBG, in which case we lemmatised that           Our pairwise combination score was defined:
term and compounded it with the third chunk’s final term. For                                              freq(c, r, f )
example:
                                                                                        p(c, r) =    ∑   freq(c) · freq(r)
                                                                                                                                       (4a)
                                                                                                    f ∈F
                                                                 748

                                          freq(c, r, f )
                     p(r, f ) =  ∑ freq(r) · freq( f )              (4b)     Table 2: Our best precision, recall and F-scores against the
                                c∈C                                          synonym-expanded ESSLLI norms across our corpora, found
                                                                             using the training β parameters.
                          
                              p(c, r̂) · p(r̂, f ) if r̂ defined
             PCS(c, f ) =                                           (4c)
                             max{p(c, r) · p(r, f ) : r ∈ R}
                                                                                     Relation    Corpus        Prec.     Recall   F
    If we had not already selected a best relation, r̂, then we de-                              Wikipedia    0.1131     0.2265   0.1509
fined it as the relation, r, which corresponded to this pairwise                         With    UKWAC         0.1000    0.2005   0.1335
combination score. Again, if there was more than one relation                                    Combined     0.1214     0.2431   0.1620
                                                                                                 Kelly et al.  0.1238    0.2493   0.1654
with the same score, then we chose the least common.                                             Wikipedia    0.1214     0.2431   0.1620
                                                                                   With (aug.)   UKWAC         0.1048    0.2101   0.1398
Step 3 Our final step assigned relations to concept/feature                                      Combined     0.1298     0.2598   0.1731
pairs which lacked an exact mutually linking relation. This                                      Wikipedia    0.2798     0.5603   0.3732
occurred around 17% of the time and was usually due to both                           Without    UKWAC         0.2560    0.5132   0.3416
                                                                                                 Combined     0.2798     0.5606   0.3733
the concept and feature terms being relatively low frequency.6                                   Kelly et al.  0.2417    0.4847   0.3225
    To achieve this, we backed-off to semantic feature clusters:
we defined f⋆ as the cluster for feature f , and F⋆ as the set of            ten lemmatised properties for each of 44 concepts from the
all feature clusters, and defined our Feature Cluster Score,                 recoded McRae norms, together with a feature expansion set
FCS(c, f⋆ ), analogously to our Pairwise Combination Score,                  generated for each concept relation feature triple. One of
merely substituting all instances of f for f⋆ . Our best relation,           the reasons for using this set is that McRae et al. normalised
r̂, was defined as the relation corresponding to this FCS.                   their features by channelling synonymous properties into a
                                                                             single representation. The ESSLLI set undoes some of these
Reweighting                                                                  normalizations, expanding the feature terms to a set of syn-
In our system’s fourth and final stage we used the metrics                   onyms. In this way, loud, noise and noisy (for example) can
derived above to assign an overall score for each triple using               all be counted as matches against the property is loud. The
a weighting of parameters; we used our training set to derive                relations were not expanded.
the most optimal values for these parameters. We normalised                     Our results can be found in Table 2. We also assessed our
our various metrics so that they all lay between 0 and 1.                    system using the full text of the relations found in the original
    Our relation selection stage had already fixed a relation, r̂,           McRae norms as additional ‘relation synonyms’; these aug-
for each concept and feature. Hence we calculated for each                   mented results can be found under the ‘With (aug.)’ relation
of our triples t = (c, r̂, f ) an overall score:                             heading. We have exceeded the performance of Kelly et al.
   score(t) = βPMI · PMI(t) + βLL · LL(t) + βSVM · SVM(t)                    (2012) (best F-score of 0.1654) with a best overall F-score of
                                                                             0.1731 for the combined corpus.
               + βCFF · CFF(t) + βDRS · DRS(t) + βEMS · EMS(t) (5)
                                                                                We also note that performing these evaluations on the top
               + βPCS · PCS(t) + βFCS · FCS(t)
                                                                             ten properties returned further improved the situation (per-
    We wished to optimise our parameters for superior feature                haps unsurprising since the ESSLLI set contains only ten
F-score performance against our training set. We employed a                  properties per concept); for example, evaluating our top ten
stochastic process to find best-possible values for our training             triples against the relation synonyms set returned a precision
parameters, using a random-restart hill-climbing algorithm,                  of 0.2215 for the combined corpus. Furthermore, the pre-
repeated 1000 times and selecting the output (and β values)                  cision on the combined corpus for the top ten evaluation of
offering the best F-score across these iterations.                           features-only was 0.4409, surpassing Baroni et al. (2009) who
    This process offered a reasonable approximation of the best              offer a best score of 0.239 on the same evaluation.
possible F-scores our system could produce and their corre-
sponding β values; following this process, our best F-scores                 Human-generated semantic similarity
were 0.2739, 0.2803 and 0.2996 for our Wikipedia, UKWAC                      Comparison with the ESSLLI gold standard is still an in-
and combined corpora respectively.                                           complete evaluation: not all conceptual properties for a given
                                                                             concept are contained therein, and lexical variation can mark
                              Evaluation                                     valid relations as wrong. Furthermore, one of the primary ad-
We evaluated our system using gold standard, human                           vantages of our computational approach is its ability to extract
semantic-similarity and direct human evaluations.                            a large number of properties for a given concept. Hence, we
                                                                             introduced an alternative approach to calculate how seman-
Gold standard evaluation                                                     tically meaningful our output was by evaluating the triples’
We began by comparing our top twenty output using the ESS-                   capacity to predict human-rated similarity between words.
LLI gold standard set. This ‘expansion’ set comprises the top                   We asked five native English speakers to rate the similar-
                                                                             ity of 90 concept pairs, where concepts in the pairs were all
     6 Only a small proportion of our triples derived their relations in     drawn from the ESSLLI set. The raters were given instruc-
this way; at this point, in our training sets we had assigned relations
to over 94% of triples from our Wikipedia corpus, and 97% from the           tions explaining the task and then presented with each con-
UKWAC corpus.                                                                cept pair, one by one, a scale of 1 to 7 and asked to rate how
                                                                         749

Table 3: Pearson correlation (r) results with confidence inter-          Table 4: Inter-annotator agreement and judgements for our
vals between our VHuman vector and our similarity vectors V              extraction system applied to our three corpora.
(with dimensionality D and derived from the top n properties)                                          Judge                 %      Kappa
from our system.                                                                 Corpus               A     B       Avg     c/p    (Agree)
  Relation     V               n      D        r        Conf. Int.                          c/p      202 204        203             0.6343
                                                                             Wikipedia      r/w       98    96       97     67.7     (252)
                Wikipedia            654     0.598    [0.446, 0.716]                        c/p      193 204       198.5            0.7398
                UKWAC          20    712     0.629    [0.486, 0.740]            UKWAC                                       66.2
                                                                                            r/w      107    96     101.5             (265)
       With     Combined             692     0.671    [0.539, 0.771]                        c/p      212 216        214             0.7229
                Wikipedia           3585     0.693    [0.568, 0.787]         Combined       r/w       88    84       86     71.3     (266)
                UKWAC         300   3442     0.683    [0.555, 0.780]
                Combined            3380     0.723    [0.606, 0.809]
                Wikipedia            478     0.720    [0.603, 0.807]     (w). Our judges were unaware of the aims of the evaluation.
                UKWAC          20    456     0.754    [0.649, 0.832]     We concatenated their ratings using the methodology of De-
   Without      Combined             475     0.742    [0.632, 0.822]     vereux et al.9 however our instructions reflected the fact that,
                Wikipedia           7324     0.782    [0.685, 0.851]
                UKWAC         300   8698     0.806    [0.719, 0.868]     unlike previous systems, our output contained prepositional
                Combined            8727     0.807    [0.721, 0.869]     relations and we therefore did not wish our volunteers to al-
       With     McRae                410     0.785    [0.691, 0.854]     low for absent prepositions. This evaluation offers an impor-
   Without                           355     0.787    [0.693, 0.855]
                LSA                  300     0.708    [0.586, 0.798]     tant insight into the viability of our method as a property ex-
                                                                         traction system. Our results are in Table 4, and Table 5 shows
similar the two concepts were.                                           a sample of our output and the corresponding judgements.
   To compare our system with these ratings we constructed                  It is clear that our best results were again in the combined
a vector space of dimension D, where D was the number                    corpus, where an impressive 71.3% of our returned triples
of distinct properties across our triples. For each of our 44            were marked as either plausible or correct with a Kappa
concepts, we generated a concept-score vector with non-zero              (Fleiss, 1971) score of 0.7229 indicating substantial agree-
entries by inserting the triple scores, score(t), into their cor-        ment between annotators. This constitutes a major improve-
rect entries in the concept-score vector. We then constructed            ment over Kelly et al. (2012) who evaluated on the same set
a 44 × 44 symmetric pairwise similarity matrix across our                of concepts and whose corresponding score was just 51.1%.
concepts by calculating the cosine similarity between their
concept-score vectors. From this we extracted a similarity                                            Discussion
vector, V , for our 90 pairwise comparisons.                             As the first system to offer viable unconstrained property
   We calculated twelve such matrices (using the top twenty              norm-like extraction, this paper brings research into concep-
and top 300 extracted triples, across three corpora and ex-              tual property extraction to the next level. Our system employs
cluding and including the relation term). We also generated              both full parsing and chunking to extract features and rela-
two such matrices using both the feature-heads and the full              tions respectively and introduces a novel multi-step backing-
text of the McRae property norms, using the norm produc-                 off method for relation selection. Our gold standard perfor-
tion frequencies as entries in each concept’s vector, as well as         mance exceeded that of previous approaches, and our human
comparing our ratings with LSA-predicted (Landauer, Foltz,               evaluation indicated that we have outperformed the system
& Laham, 1998) similarities.7 Our results are in Table 3. 8              of Kelly et al. (2012) by a significant margin. We also intro-
   Our systems’ performance, evaluating with and without re-             duced a semantic similarity evaluation for this task, showing a
lation and when using the top twenty triples, was comparable             strong Pearson correlation of 0.754 with human ratings when
to LSA (correlation 0.708) with average correlations across              employing just 20 extracted properties per concept, with the
our corpora of 0.754 and 0.671 respectively. Including the               correlation rising to 0.807 when using 300 properties. In this
top 300 extracted triples brought our correlations up to 0.807           latter case, the predicted similarities were almost as corre-
and 0.754 respectively, an extremely strong result given that            lated with human judgements as the human judgements are
the average Pearson coefficient of correlation across the five           with each other.
judges (considering all pairwise combinations) was 0.820.                   Potential criticisms of our system include the fact that our
                                                                         chunk to triple conversion process won’t necessarily always
Human evaluation                                                         yield a true reflection of the sentence’s original meaning. It
In our final evaluation, we asked two native English speak-              is, for example, possible for the final chunk to contain ad-
ing human judges to assess the accuracy of our triples. Fol-             jectives which modify the final noun. These could have im-
lowing the methodology of Devereux et al. (2009), we asked               portance from a conceptual representation perspective (e.g.,
them to classify output triples for 15 concepts into four cate-          features such as long neck for giraffe has long neck). Also,
gories: ‘correct’ (c), ‘plausible’ (p), ‘related’ (r) and ‘wrong’        the modifying portion of a chunk may be semantically signifi-
                                                                         cant, altering the final term’s meaning (e.g., a tea bag is quite
    7 300 factors, using the TASA corpus at lsa.colorado.edu.            different from a bag ). It should be possible to have more gen-
    8 The  correlation confidence intervals, calculated using Fisher
transformations (Fisher, 1915), are given at the 95% level of con-           9 i.e. both ‘correct’ and ‘plausible’ triples were counted as cor-
fidence, and two-tailed p < 0.05.                                        rect, while ‘related’ or ‘wrong’ triples were considered incorrect.
                                                                     750

Table 5: Judges’ assessments of the top twenty extracted re-        Devereux, B., Pilkington, N., Poibeau, T., & Korhonen, A.
lation/feature pairs (combined corpus) for two concepts.              (2009). Towards unrestricted, large-scale acquisition of
                        Judge                           Judge         feature-based conceptual representations from corpus data.
          knife         A B                 pig         A B           Research on Language & Computation, 1–34.
  sharpened by hand      c   c    eat piglet            c    p      Dunning, T. (1993). Accurate methods for the statistics
  based on design        c   c    get fat               c    c
  made of steel          c   c    produce pork          r    c        of surprise and coincidence. Computational Linguistics,
  be small               c   p    breed farm            r    r        19(1), 61–74.
  pick on fork           r    r   put into sausage      c    c      Farah, M., & McClelland, J. (1991). A computational model
  be make                p    r   be large              p    p
  crafted from metal     c   c    have baby             c    c        of semantic memory impairment: Modality specificity and
  scaled for use         p   p    be different          p    p        emergent category specificity. Journal of Experimental
  make cut               c   c    stunned through use   r   w         Psychology: General, 120(4), 339–357.
  be sharp               c   c    be bacon              c    r
  be weapon              c   c    be welfare            r    r      Ferraresi, A., Zanchetta, E., Baroni, M., & Bernardini, S.
  have edge              c   c    discover sheep        c    c        (2008). Introducing and evaluating UKWAC, a very large
  have handle            c   c    killed for meat       c    c        web-derived corpus of English. In Proceedings of the 4th
  be serrated            c   c    used for food         c    c
  made of stainless     w r       label cattle          w w           Web as Corpus Workshop (wac-4) – Can we beat Google?
  is for cutting         c   c    be animal             c    c        (pp. 47–54).
  have blade             c   c    shackled by ham       r    r      Fisher, R. (1915). Frequency distribution of the values of the
  be useful              p   c    chew tail             c    c
  be tool                c   c    have disease          c    c        correlation coefficient in samples from an indefinitely large
  be dangerous           c   c    found in guinea       c    c        population. Biometrika, 10(4), 507–521.
                                                                    Fleiss, J. (1971). Measuring nominal scale agreement among
eral chunk to triple extraction (e.g., by using a larger corpus       many raters. Psychological bulletin, 76(5), 378.
to mitigate the sparsity associated with multi-word terms).         Joachims, T. (1999). Making large scale SVM learning prac-
    Finally, a major issue is our lack of comprehensive train-        tical.
ing/testing data; our norms are incomplete insofar as there         Kelly, C., Devereux, B., & Korhonen, A. (2010). Acquiring
were a large number of ‘correct’ properties absent from our           human-like feature-based conceptual representations from
gold standard. In future work we hope to implement large-             corpora. In First Workshop on Computational Neurolin-
scale evaluation of our system’s output (e.g., using Amazon           guistics (p. 61). Association for Computational Linguis-
Turk) which would allow us to rapidly obtain large amounts            tics.
of human-generated feedback. We could then use active-              Kelly, C., Devereux, B., & Korhonen, A. (2012). Semi-
learning to introduce a feedback loop of human-annotation             supervised learning for automatic conceptual property ex-
to better pinpoint inaccurate features or relations. Feedback         traction. In Proceedings of the 3rd Workshop on Cognitive
which strongly indicated that certain properties were unin-           Modeling and Computational Linguistics (pp. 11–20). As-
teresting could prove invaluable in getting even closer to a          sociation for Computational Linguistics.
conceptual structure-like representation of concepts.               Landauer, T., Foltz, P., & Laham, D. (1998). An introduction
                                                                      to latent semantic analysis. Discourse Processes, 25, 259–
                     Acknowledgments                                  284.
                                                                    McRae, K., Cree, G., Seidenberg, M., & McNorgan, C.
This research was supported by EPSRC grant EP/F030061/1
                                                                      (2005). Semantic feature production norms for a large set
and the Royal Society University Research Fellowship, UK.
                                                                      of living and nonliving things. Behavioral Research Meth-
We are grateful to McRae and colleagues for making their
                                                                      ods, Instruments, and Computers, 37, 547–559.
norms publicly available, and to the anonymous reviewers for
                                                                    Murphy, G. (2002). The Big Book of Concepts. The MIT
their comments and feedback.
                                                                      Press.
                          References                                Randall, B., Moss, H., Rodd, J., Greer, M., & Tyler, L.
                                                                      (2004). Distinctiveness and correlation in conceptual struc-
Baldridge, J. (2005). The Apache OpenNLP project.                     ture: Behavioral and computational studies. Journal of Ex-
Baroni, M., Evert, S., & Lenci, A. (Eds.). (2008). ESSLLI             perimental Psychology Learning Memory and Cognition,
   2008 Workshop on Distributional Lexical Semantics.                 30(2), 393–406.
Baroni, M., Murphy, B., Barbu, E., & Poesio, M. (2009).             Taylor, K., Devereux, B., Acres, K., Randall, B., & Tyler,
   Strudel: A corpus-based semantic model based on proper-            L. (2011). Contrasting effects of feature-based statistics
   ties and types. Cognitive Science, 1–33.                           on the categorisation and basic-level identification of visual
Church, K., & Hanks, P. (1990). Word association norms,               objects. Cognition, 122(3), 363-74.
   mutual information, and lexicography. Computational Lin-         Tyler, L., Moss, H., Durrant-Peatfield, M., & Levy, J. (2000).
   guistics, 16(1), 22–29.                                            Conceptual structure and the structure of concepts: A dis-
Clark, S., & Curran, J. (2007, 1). Wide-coverage efficient            tributed account of category-specific deficits. Brain and
   statistical parsing with CCG and log-linear models. Com-           Language, 75(2), 195–231.
   putational Linguistics, 33(4), 493–552.
                                                                751

