UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Developmental and postural changes in children’s visual access to faces
Permalink
https://escholarship.org/uc/item/8gw3v15s
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Frank, Michael
Simmons, Kaia
Yurovsky, Daniel
et al.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

           Developmental and postural changes in children’s visual access to faces
                           Michael C. Frank, Kaia Simmons, Daniel Yurovsky, & Guido Pusiol
                                         {mcfrank,kaias,yurovsky,pusiol}@stanford.edu
                                                           Department of Psychology
                                                               Stanford University
                              Abstract                                     Campos (under review) noted robust correlations between
   The faces of other people are a critical information source             children’s ability to walk and their vocabulary, both recep-
   for young children. During early development, children un-              tive and productive. On the basis of an observational study of
   dergo significant postural and locomotor development, chang-            parent input, they speculated that the emergence of walking
   ing from lying and sitting infants to toddlers who walk inde-
   pendently. We used a head-mounted camera in conjunction                 may change the ability of the child to access social informa-
   with a face-detection system to explore the effects of these            tion (because walking toddlers see more of the social world
   changes on children’s visual access to their caregivers’ faces          than crawling infants). Accessing more social information
   during an in-lab play session. In a cross-sectional sample of
   4–20 month old children, we found substantial changes in face           may in turn allow children to discover word meanings more
   accessibility based on age and posture. These changes may               effectively.
   translate into changes in the accessibility of social information
   during language learning.
                                                                              Recent methodological developments have the potential to
   Keywords: Social development; face processing; head-
   camera.                                                                 provide data that would allow this hypothesis to be tested.
                                                                           The availability of head-mounted cameras and eye-trackers
                          Introduction                                     allows for the measurement of children’s naturalistic envi-
A father offers his young daughter a novel object: a bright                ronment in a way that was not previously possible. Yoshida
yellow feather duster. A few moments after she accepts the                 and Smith (2008) gave the first demonstration of the radi-
toy, he remarks, “Isn’t the zem funny?” Her father may still               cal differences between toddler and adult perspectives on the
be talking about the feather duster, or he may be describing a             social world, with toddlers’ visual field being dominated by
new object. To find out she has access to a simple and reliable            hands and objects much more than that of adults. More re-
method: she can look to his face to infer the direction of his             cent work has used head-mounted eye-tracking methods to
attention.                                                                 measure young toddlers’ fixations (Franchak, Kretch, Soska,
   The ability to follow social signals like eye-gaze is an im-            & Adolph, 2011), also finding that children look relatively
portant part of early social cognition (Scaife & Bruner, 1975)             infrequently at their mothers’ faces in naturalistic play.
and a strong predictor of children’s early language develop-
ment. For example, Brooks and Meltzoff (2005) found that                      These methods are now being applied to understand inputs
children who followed an experimenter’s gaze better before                 to language acquisition. Work by Yu, Smith, and colleagues
their first birthday had larger vocabularies at 18 months. Sim-            suggests that word learning is facilitated when parents and
ilarly, Carpenter, Nagell, and Tomasello (1998) found that                 children create moments in which the visual field is domi-
children’s level of joint engagement (as well as the degree to             nated by a single object (Smith, Yu, & Pereira, 2011; Yu &
which mothers followed the child’s focus of attention in their             Smith, in press). Some data even suggest that young chil-
labeling) predicted vocabulary growth in both language pro-                dren’s restricted viewpoint may be more effective for learn-
duction and comprehension. These studies suggest that chil-                ing words than the comparable adult perspective (Yurovsky,
dren’s social environment plays a powerful supportive role in              Smith, & Yu, in press). Together, this body of evidence
language learning.                                                         suggests that measuring infants’ perspective—and how it
   But at the same time as children are beginning to learn                 changes in motor development—is a critical part of under-
their first words, their view of the world is changing radically           standing early language learning.
(Adolph & Berger, 2007). As speechless infants, they are un-
able to locomote independently. Before their first birthday,                  In the current study we took a developmental approach to
they begin crawling; soon after, they begin to walk indepen-               understanding the relationship between perspective and ac-
dently. Infants’ visual field is subject to the whims of their             cess to social information. We recorded head-camera data
caregivers, but caregivers often place them in positions con-              from a group of infants and children across a broad age range
ducive to joint attention. In contrast, toddlers determine their           as they played with their caregivers during a brief laboratory
own input to a much greater degree, but as a consequence                   visit. We then hand-annotated these data for the child’s pos-
they spend much of their time in a world primarily populated               ture and parents’ naming behavior and used face-detection
by knees. These postural and locomotor changes may have a                  algorithms to measure the frequency of faces in the child’s vi-
profound effect on what children see.                                      sual field. The resulting dataset allows us to analyze changes
   A recent study suggests the possibility of links between                in access to faces according to children’s age, posture, and
motor milestones, social cognition, and language. Walle and                linguistic input.
                                                                       454

                                                                          recording the objects being manipulated by the child, it nev-
                                                                          ertheless allowed us to capture the majority of the faces in the
                                                                          child’s visual field.
                                                                          Procedure
                                                                          After coming to the lab, families were seated in our waiting
                                                                          room where they signed consent documents and where chil-
                                                                          dren were fitted with the headcam. After a short period of
Figure 1: Our light-weight, low-cost head-mounted camera                  play, they were escorted to a playroom in the lab where the
with fisheye lens.                                                        free-play session (the focus of the current study) was con-
                                                                          ducted.
                                                                             In the waiting room, the experimenter placed the headcam
                              Methods                                     on children’s heads after they had time to adjust to the envi-
                                                                          ronment. For children who resisted wearing the headcam, the
Participants                                                              experimenter used distractor techniques (presenting stimulat-
Participants were 20 infants and children (N=4 each at 4,                 ing toys or engaging the children in hand-occupying activi-
8, 12, 16, and 20 months, 9 females total) in an ongoing                  ties) intended to keep children’s focus elsewhere and prevent
large-scale study, recruited from the surrounding community               them from taking off the camera (Yoshida & Smith, 2008).
via state birth records. Participants had no documented dis-              Once the child was wearing the camera comfortably for a pe-
abilities and were reported to hear at least 80% English at               riod of time, child and caregiver (or caregivers: in two cases,
home. Success rates for children wearing the camera for long              there were two adults present) were escorted to the playroom.
enough to initiate the play session varied from 100% at 8                    In the playroom, the experimenter presented the child’s
months to approximately 50% at 20 months.                                 parent with a box containing three labeled pairs of objects,
                                                                          each consisting of a familiar and a novel object (e.g. a ball and
Head-mounted camera                                                       a feather duster, marked as a “zem”). Parents confirmed that
Our head-mounted camera (“headcam”) is composed of a                      the child had not previously seen the novel toys. Parents were
small, inexpensive MD80 model camera attached to a soft                   instructed to play with the object pairs with the child, one at
elastic headband from a camping headlamp. An aftermarket                  a time, “as they typically would” and to use the novel labels
fisheye lens intended for iPhones and other Apple devices is              to refer to the three toys. After giving these instructions, the
attached to increase view angle. The total cost of each cam-              experimenter left the room for a period of approximately 15
era is approximately $60. The camera captures 720x480 pixel               minutes. During this time, a tripod-mounted camera recorded
images at approx. 25 frames per second, and has battery life              video from a corner of the room and the headcam captured
of 60–90 minutes. Without the fisheye lens, the viewing an-               video from the child’s perspective.
gle for the camera is 32◦ horizontal by 24◦ vertical; with the
                                                                          Data Processing and Annotation
fisheye, 64◦ horizontal by 46◦ vertical. The device is pictured
in Figure 1.                                                              All headcam videos were cropped to exclude the period of
   The vertical field of view of the camera was consider-                 entry to the playroom and were automatically synchronized
ably smaller than the child’s approximate vertical field of               with the tripod-mounted videos using FinalCut Pro Software.
view, which—even at 6-7 months—spans around 100–120◦                      The final sample was approx. 5 hours of headcam video (M
in the vertical dimension (Mayer, Fulton, & Cummings,                     = 12 min, range: 2–21 min), for a total of roughly 400,000
1988; Cummings, Van Hof-Van Duin, Mayer, Hansen, & Ful-                   frames.
ton, 1988). We were therefore faced with a choice in the ori-             Posture and Orientation Annotation One major goal of
entation of the camera. If we chose a lower or higher ori-                our study was to understand the relationship between chil-
entation, we would be at risk of truncating either the child’s            dren’s posture and their access to information from the faces
own hands and physically proximate objects, or the faces of               of their caregiver. To investigate this relationship, we cre-
the adults around the child. Yet if we chose the middle ori-              ated a set of annotations for the child’s physical posture (e.g.
entation, we would still be at risk of underestimating the pro-           standing, sitting) and orientation of the caregiver relative to
portion of faces viewed by the child. Thus, for the purposes              the child (e.g. in front of, behind, close, far away). For each
of the current study—measuring visual access to faces—we                  headcam video, a coder used OpenSHAPA software to anno-
chose to orient the camera towards the upper part of the vi-              tate both orientation and posture (Adolph, Gilmore, Freeman,
sual field.1 While this orientation decreased our chances of              Sanderson, & Millman, 2012).
    1 Previous studies have shown that children’s head movements in          Orientation was initially categorized as having the care-
the horizontal dimension are approximated by (though are slightly         giver in front, to the side, or behind the child, and close (de-
lagged by) their head movements (Yoshida & Smith, 2008). Our
own experience with the current apparatus ratifies these conclusions      tical field are less reliable. Hence, these studies may run the risk of
for the horizontal field but suggests that head movements in the ver-     underestimating the proportion of faces actually seen by children.
                                                                      455

         4 months                   8 months                  12 months                  16 months                    20 months
Figure 2: Sample frames from the headcam videos for a child from each age group, selected because they featured successful
face detections (green squares).
fined informally as within arm’s reach) or farther away. Be-
cause of data sparsity, we consolidated this scheme into three           The second algorithm that we evaluated was a semi-
categories: close to the caregiver with the caregiver either in       automated adaptive tracker-by-detection (SAATD). The al-
front or on the side, farther from the caregiver again with care-     gorithm required manual user input (selecting a single face
giver either in front or on the side, and a global category of        example per video) for its initialization, but then needed no
caregiver behind the child. Posture was categorized as being          additional training data. The tracker is based on Kalal, Miko-
held/carried, lying face-up, sitting, prone (crawling or lying),      lajczyk, and Matas (2010) which uses patches in the trajec-
standing, or other. Data from when the child was out of view          tory of an optical-flow based tracker (Lucas & Kanade, 1981)
of the tripod camera was marked as uncodable and excluded             to train and update a face detector. The optical flow tracker
from these annotations.                                               and the face detector work in parallel. If the face detector
                                                                      finds a location in a new frame exhibiting a high similarity
Labeling Annotation We were also interested in the avail-
                                                                      to its stored template, the tracker is re-initialised on that loca-
ability of social information proximate to naming events in
                                                                      tion. Otherwise, the tracker uses the optical flow to decide the
the caregivers’ speech to children. Accordingly, a human
                                                                      location of a face in the new frame. The primary advantage of
coder marked the onset time when the name of any of the six
                                                                      the SAATD algorithm is the use of motion for face detection:
objects in the object set was used. Overall, caregivers pro-
                                                                      Following the movement of the pixels that define a face it is
duced a median of 35 labels in a highly skewed distribution
                                                                      possible for the algorithm to adapt to new morphologies (i.e.
across participants (range: 9 – 131).
                                                                      different face poses).
Face Detection                                                        Detector evaluation To ensure that our evaluation was not
                                                                      biased by the relatively rare appearance of faces in the dataset,
An additional goal of the study was to measure the presence           we annotated two samples, both a random sample from the
of caregivers’ faces in the child’s field of view (as approxi-        data and a sample with a high-density of faces (see Ap-
mated by the headcam). To avoid hand-annotating the size              pendix). We evaluated each algorithm on its precision (hits
and position of faces in every frame of video, we tested two          / hits + false alarms) and recall (hits / hits + misses), as well
face detection systems. Sample frames from the video with             as F-score (the harmonic mean of these two measures). Re-
successful detections are given in Figure 2.                          sults are reported in Table 1.
Face detection algorithms The first algorithm was based                  The HMM model obtained a relatively high level of per-
on freely available computer vision tools (Bradski & Kaehler,         formance for the random subsections, but performed poorly
2008) and is described in depth in our previous work (Frank,          when there was a relatively high density of faces present.
2012). This system had two parts. The first was the appli-            In contrast, SAATD performed well on both samples, giving
cation of a set of four Haar-style face detection filters (Viola      better performance especially in cases where there was partial
& Jones, 2004) to each frame of the videos independently.             occlusion. Our goal in using face-detection algorithms was to
These detectors each provide information about whether a              provide a measurement technique that eliminated tedious and
face is present in the frame as well as size and position             expensive hand-coding and provided acceptable results. We
for any detections. In a second step, these detections are            therefore selected the SAATD model and report detections
then combined via a hidden Markov model (HMM), trained                from this algorithm as an estimate of face presence in all fur-
on hand-annotated data (see Appendix). The HMM model                  ther analyses.
(which performed nearly as well as the more complex and
computationally-intensive Conditional Random Field model                                          Results
used in our previous work) attempted to estimate whether a            We report results from three different sets of analyses. First,
face was truly present in each frame of the videos, using as          we explore developmental changes in posture and orientation
its input the number of Haar detectors that were active in any        in our dataset. Next, we explore how these changes affect ac-
given frame.                                                          cess to faces, as measured using our face-detection algorithm.
                                                                  456

                            0.8                                                                           0.8
                                                                         posture
          Proportion Time                                                               Proportion Time
                            0.6                                           ●   carry                       0.6                                orientation
                                                                                                                ●
                                                                              lie                                                             ●   behind
                            0.4                          ●
                                                                              prone                       0.4                                     close
                                                                              sit                                   ●                             far
                            0.2                                                                           0.2                ●
                                                                                                                                   ●
                                  ●
                                                                              stand
                                                   ●                                                                                    ●
                                          ●
                            0.0                                     ●                                     0.0
                                  4       8       12     16         20                                          4   8       12     16   20
                                              Age (months)                                                              Age (months)
Figure 3: Proportion time in each posture, plotted by child’s age (left panel). Proportion time in each orientation relative to the
caregiver, again plotted by child’s age (right panel). For clarity, the “other” code is not plotted in either figure. Error bars show
standard error of the mean across participants.
                                                                                               since they could not sit independently. In contrast, the 8-
Table 1: Model performance on gold standard generalization
                                                                                               month-olds, who could sit independently, typically sat across
training set dataset. P, R, and F denote precision, recall, and
                                                                                               from their caregiver and saw many faces in both the sitting
F-score for each of the two samples.
                                                                                               and prone postures. The 12-month-olds spent a large amount
                                       High-density                Random                      of time in the prone position (typically crawling after the ball,
        Model                          P    R     F           P       R   F                    for example) and saw almost no faces in that posture. The 16-
        HMM                           .55 .38 .45            .89     .74 .81                   and 20-month-olds saw many faces because they were stand-
        SAATD                         .86 .78 .81            .93     .76 .83                   ing while their parents were sitting, putting their faces at a
                                                                                               relatively similar level.
                                                                                                  Across ages, the carrying and prone postures resulted in the
Finally, we report preliminary results on the accessibility of                                 smallest number of faces seen, while standing and sitting re-
faces during labeling.                                                                         sulted in far more. These postures both presented opportuni-
                                                                                               ties for seeing faces in large part because parents were sitting
Changes in Posture and Orientation                                                             or lying on the floor with children. Although far fewer faces
Our posture coding captured typical developmental mile-                                        were seen when the caregiver was behind the child,2 both the
stones (Figure 3, left). Overall, sitting was the most common                                  close and far positions resulted in approximately equal pro-
posture for interactions in the caregiver play session. The                                    portions of face detections.
youngest infants in our sample mostly sat (with parental as-
sistance), but also lay down and were carried a significant pro-                               Access to Faces During Labeling
portion of the time. The 12-month-olds were the only group
                                                                                               Our final analysis concerned the accessibility of caregivers’
who spent a large amount of time crawling, and the 16- and
                                                                                               faces during labeling events. Franchak et al. (2011) found
20-month-olds sat and stood in equal parts.
                                                                                               that referential speech was marginally more likely to draw
   Similarly, our coding of orientation revealed some signifi-
                                                                                               toddlers’ attention to mothers’ faces. We were similarly in-
cant developmental changes (Figure 3, right). Younger chil-
                                                                                               terested in whether looking at faces occurred during label-
dren more frequently had the caregiver behind them, often
                                                                                               ing. Accordingly, we used the labeling annotations for each
because the caregiver was supporting the child’s sitting pos-
                                                                                               child to identify the 2s before and after each labeling event.
ture (for the 4-month-olds especially). In contrast, the 12–20
                                                                                               We then computed the proportion face detections within this
month olds were able to locomote independently and so were
                                                                                               window across ages.
able to spend more time further from the caregiver.
                                                                                                  The overall pattern of face accessibility closely mirrored
Access to Faces                                                                                the base rates shown in Figure 4. Although this general pat-
We next investigated the effects of the child’s posture and ori-                               tern in itself is important in assessing developmental access to
entation on the presence and size of the caregiver’s face in the                               social information, in the current analysis we were interested
visual field. Figure 4 shows the proportion of frames with a                                   in whether there was differential access to faces around label-
positive face detection, plotted by the child’s age, posture,                                  ing instances. We thus computed difference scores between
and orientation relative to the caregiver.                                                     the baseline face detection rate and the rate of face detections
   Overall, there were very large differences in access to faces                                   2 Since orientation was coded via body posture, faces seen while
across age. The 4-month-olds saw almost no faces—their par-                                    the caregiver was behind the child were due to children looking over
ents were behind them most of the time, supporting them                                        their shoulder.
                                                                                      457

                              0.30                                                                                                                   0.30                              ●                                             0.30                ●         ●
                                                                                                                                                                                                                                                          ●
 Proportion Face Detections                                                                                             Proportion Face Detections                                                      Proportion Face Detections
                                                                                                               ●
                                                                                                                                                                                              ●
                                                                                                                                                                                              ●                                                          ●
                              0.25                                                                        ●                                          0.25                               ●
                                                                                                                                                                                                                                     0.25                 ●
                                                                                   ●
                                                                                                                                                                                                                                                                    ●
                              0.20                                                                                                                   0.20                              ●                                             0.20                           ●
                                                                                                                                                                                               ●
                                                                                                              ●                                                                                ●
                              0.15                                                 ●                                                                 0.15                                                                            0.15     ●
                                                                                                              ●                                                                                                                                          ●
                                                                                   ●                                                                                                           ●                                                         ●
                                                                                  ●                                                                                           ●         ●                                                                 ●
                                                                                                      ●                                                                                                                                                  ●
                              0.10                                                                                                                   0.10                     ●        ●
                                                                                                                                                                                                                                     0.10                ●
                                                                                                                                                                                                                                                         ●         ●
                                                                                                                                                                              ●                                                                          ●         ●
                                                                                                      ●                                                                                ●                                                                 ●
                                                                                           ●                  ●●                                                                       ●       ●
                                                                                                          ●                                                                                    ●
                                                                                                                                                                                               ●
                                                                                                                                                                                               ●                                                                   ●●
                              0.05                                                                                                                   0.05                     ●
                                                                                                                                                                                        ●
                                                                                                                                                                                                                                     0.05     ●
                                                                                                                                                                                                                                              ●           ●
                                                                                                      ●                                                              ●       ●          ●     ●
                                                                         ●         ●        ●●                                                                       ●                                                                        ●●          ●
                                                                                                                                                                              ●                                                                ●                   ●
                                                                                                                                                                                                                                                                   ●
                                                                        ●
                                                                        ●●                  ●                                                                        ●
                                                                                                                                                                     ●       ●●
                                                                                                                                                                             ●                ●
                                                                                                                                                                                              ●
                                                                                                                                                                                              ●                                               ●
                                                                                                                                                                                                                                              ●●
                                                                                                                                                                                                                                               ●         ●
                              0.00                                                           ●                                                       0.00                    ●
                                                                                                                                                                             ●●
                                                                                                                                                                              ●        ●
                                                                                                                                                                                       ●      ●●                                     0.00     ●●
                                                                                                                                                                                                                                               ●         ●
                                                                        4         8        12         16      20                                                    carry   prone    stand    sit                                           behind     close       far
                                                                                       Age (months)                                                                           Posture                                                                Orientation
Figure 4: Proportion face detections, split by age group (left panel), posture (middle panel), and caregiver’s orientation (right
panel). We omit the lying face-up posture due to data sparsity. Black points show individual participants and are jittered slightly
on the horizontal, red lines show means and 95% confidence intervals.
                                                                                                                                                                                    method for examining social access during language learning.
                                                                       0.08
                              Face detections relative to base rate
                                                                              ●
                                                                       0.06
                                                                                                                                                                                                      General Discussion
                                                                                                                                                                ●
                                                                              ●                                    ●
                                                                                                                                                                                    Using a head-mounted camera, we explored the relationship
                                                                       0.04                                                                                  ●
                                                                              ●                                                                             ●                       between infants’ postural and locomotor development and
                                                                                                                   ●                                                                their visual access to social information. The use of auto-
                                                                       0.02   ●                  ●
                                                                                                 ●                 ●                                        ●
                                                                                                                                                                                    mated annotation tools from computer vision allowed us to
                                                                       0.00
                                                                                                 ●
                                                                                                 ●
                                                                                                                   ●                                                                measure the prevalence of caregivers’ faces in their children’s
                                                                              ●
                                                                                                 ●
                                                                                                                                                                                    visual field. We found systematic differences in the visual
                                                                      −0.02                                                                                                         accessibility of faces based on posture, orientation, and age,
                                                                                                                   ●
                                                                                                                                                                                    as well as hints of differences in language-related changes in
                                                                              8                  12                16                                       20                      visual access. While these results remain preliminary given
                                                                                                     Age (Months)                                                                   the size of our developmental sample, this work nevertheless
                                                                                                                                                                                    provides an important proof-of-concept that computer vision
                                                                                                                                                                                    techniques can be used as a measurement method in the de-
Figure 5: Proportion of faces detected in a 4s window of                                                                                                                            velopmental context.
time centered around labeling events, plotted by age group                                                                                                                             The measures developed here have broad applicability to
and whether the word was familiar or novel. Error bars show                                                                                                                         the study of individual and cultural differences. Since the
95% confidence intervals. 4-month-olds are omitted due to                                                                                                                           physical circumstances of child rearing vary widely across
the limited number of total face detections for this group.                                                                                                                         households and across cultures, there may be important and
                                                                                                                                                                                    predictable differences in children’s visual experience. As
                                                                                                                                                                                    suggested by the correlations between walking and vocabu-
                                                                                                                                                                                    lary development (Walle & Campos, under review), postural
in labeling windows for each participant. Figure 5 shows the                                                                                                                        development may have substantial downstream consequences
results of this analysis.                                                                                                                                                           for language. For example, shifts in how infants are placed in
   Although any conclusion must remain extremely tentative                                                                                                                          particular postures by strollers or carriers (Zeedyk, 2008) or
because of the small sample, we nevertheless saw an increase                                                                                                                        how their motor development is encouraged by parent prac-
in label-related face access for the 20-month-olds. This dif-                                                                                                                       tices (Bril & Sabatier, 1986) may lead to differences in social
ference was robust across a variety of window sizes from 1–6                                                                                                                        input which in turn affect their language learning. Since our
s. (8-month-olds were more variable but similarly showed                                                                                                                            variant of the headcam method is both inexpensive and highly
some trend towards greater face access during naming.) We                                                                                                                           portable, we have been able to deploy it in children’s homes
cannot yet make inferences about the source of these differ-                                                                                                                        with some success; it may thus be a valuable tool for investi-
ences: They could be could be caused by children, caregivers,                                                                                                                       gating differences in child-rearing practices.
or a combination of the two. Nevertheless, these results con-                                                                                                                          A deep body of work uses children’s linguistic input—
verge with previous work and suggest that, in combination                                                                                                                           measured using audio recordings—to understand the
with face detection techniques, the headcam may be a viable                                                                                                                         learning mechanisms underlying vocabulary acquisition
                                                                                                                                                                             458

(Huttenlocher, Haight, Bryk, Seltzer, & Lyons, 1991; Hart         Kalal, Z., Mikolajczyk, K., & Matas, J. (2010). Forward-
& Risley, 1995; Fernald, Perfors, & Marchman, 2006).                backward error: Automatic detection of tracking failures.
There have been some important initial successes in using           In 20th International Conference on Pattern Recognition
visual input to predict language uptake (Yu & Smith, in             (pp. 2756–2759).
press). Nevertheless, we have a long way to go before             Lucas, B., & Kanade, T. (1981). An iterative image regis-
our knowledge about children’s visual input parallels our           tration technique with an application to stereo vision. In
understanding of their linguistic environment. Coming to            Proceedings of the 7th International Joint Conference on
such an understanding will require the creation of both             Artificial intelligence.
corpus resources and automated tools such as those we have        Mayer, D., Fulton, A., & Cummings, M. (1988). Visual fields
begun to develop here.                                              of infants assessed with a new perimetric technique. Inves-
                                                                    tigative Ophthalmology & Visual Science, 29(3), 452–459.
                   Acknowledgments                                Scaife, M., & Bruner, J. (1975). The capacity for joint visual
Thanks to Ally Kraus, Kathy Woo, Aditi Maliwal, and other           attention in the infant. Nature, 253, 265-266.
members of the Language and Cognition Lab for help in re-         Smith, L. B., Yu, C., & Pereira, A. F. (2011). Not your
cruitment, data collection, and annotation. This research was       mother’s view: The dynamics of toddler visual experience.
supported by a John Merck Scholars grant to MCF.                    Developmental Science, 14, 9-17.
                                                                  Viola, P., & Jones, D. H. (2004). Robust real-time face de-
                        References                                  tection. International Journal of Computer Vision.
Adolph, K., & Berger, S. (2007). Motor development. In            Walle, E. A., & Campos, J. J. (under review). Infant language
  Handbook of child psychology. Wiley Online Library.               development is related to the acquisition of walking.
Adolph, K., Gilmore, R., Freeman, C., Sanderson, P., & Mill-      Yoshida, H., & Smith, L. (2008). What’s in view for toddlers?
  man, D. (2012). Toward open behavioral science. Psycho-           using a head camera to study visual experience. Infancy,
  logical Inquiry, 23(3), 244–247.                                  13, 229–248.
Bradski, G., & Kaehler, A. (2008). Learning OpenCV: Com-          Yu, C., & Smith, L. B. (in press). Embodied attention and
  puter vision with the OpenCV library. O’Reilly Media.             word learning by toddlers. Cognition.
Bril, B., & Sabatier, C. (1986). The cultural context of mo-      Yurovsky, D., Smith, L., & Yu, C. (in press). Statistical word
  tor development: Postural manipulations in the daily life         learning at scale: The baby’s view is better. Developmental
  of Bambara babies (Mali). International Journal of Behav-         Science.
  ioral Development, 9(4), 439–453.                               Zeedyk, M. (2008). What’s life in a baby buggy like?: The im-
Brooks, R., & Meltzoff, A. (2005). The development of gaze          pact of buggy orientation on parent-infant interaction and
  following and its relation to language. Developmental Sci-        infant stress (Tech. Rep.). London, UK: National Literacy
  ence, 8(6), 535–543.                                              Trust.
Carpenter, M., Nagell, K., & Tomasello, M. (1998). Social
  cognition, joint attention, and communicative competence                Appendix: Face presence annotation
  from 9 to 15 months of age. Monographs of the Society for       We selected 1 minute of interaction for each age group, di-
  Research in Child Development, 63(4).                           vided evenly across the four dyads at that age. For each
Cummings, M., Van Hof-Van Duin, J., Mayer, D., Hansen,            dyad, we divided the recorded video into contiguous 1 s seg-
  R., & Fulton, A. (1988). Visual fields of young children.       ments and selected 16 in accordance with two criteria. First,
  Behavioural and Brain Research, 29(1), 7–16.                    8 of these segments were selected by choosing the parts of
Fernald, A., Perfors, A., & Marchman, V. (2006). Picking          the videos highest in face detection (high density sample).
  up speed in understanding: Speech processing efficiency         To be fair to both algorithms, half of this was chosen from
  and vocabulary growth across the 2nd year. Developmental        the segments with the most HMM detections and half were
  Psychology, 42(1), 98–116.                                      chosen from the segments with the most SAATD detections.
Franchak, J., Kretch, K., Soska, K., & Adolph, K. (2011).         The remaining segments were chosen by randomly sampling
  Head-mounted eye tracking: A new method to describe in-         from segments not yet selected for coding (random sample).
  fant looking. Child Development.                                Segments were annotated frame-by-frame by a human coder,
Frank, M. C. (2012). Measuring children’s visual access to        who marked each frame as containing a face if at least half
  social information using face detection. In Proceedings of      of the face was in the child’s view. Detector output for each
  the 33nd Annual Meeting of the Cognitive Science Society.       of these frames was then compared to this gold standard. A
Hart, B., & Risley, T. (1995). Meaningful differences in the      detection was counted as correct if it overlapped a face with
  everyday experience of young American children. Balti-          half of its total area. The HMM training sample was selected
  more, MD: Brookes Publishing Company.                           via the same method as this gold standard sample, but used
Huttenlocher, J., Haight, W., Bryk, A., Seltzer, M., & Lyons,     separate set of video segments.
  T. (1991). Early vocabulary growth: Relation to language
  input and gender. Developmental Psychology, 27(2), 236–
  248.
                                                              459

