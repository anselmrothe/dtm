UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Multi-attribute Linear Ballistic Accumulator Model of Decision-making

Permalink
https://escholarship.org/uc/item/0bq5q666

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Trueblood, Jennifer
Brown, Scott
Heathcote, Andrew

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

The Multi-attribute Linear Ballistic Accumulator Model of Decision-making
Jennifer S. Trueblood (jstruebl@uci.edu)
Department of Cognitive Sciences, University of California, Irvine
Irvine, CA 92697 USA

Scott D. Brown and Andrew Heathcote
{scott.brown, andrew.heathcote}@newcastle.edu.au
School of Psychology, University of Newcastle, Australia
Callaghan, NSW 2308 AU
Abstract

greater preference for X when AX is included in the choice set
{X,Y } as compared to when AY is included (and vice versa
for Y ). Mathematically, the attraction effect occurs when
Pr[X|{X,Y, AX }] > Pr[X|{X,Y, AY }] and Pr[Y |{X,Y, AX }] <
Pr[Y |{X,Y, AY }].

Context effects - preference changes depending on the availability of other options - have wide ranging implications across
applied and theoretical domains, and have driven the development of new dynamic models of multi-attribute and multialternative choice. We propose the Multi-attribute Linear Ballistic Accumulator (MLBA), a new dynamic model that provides a quantitative account of the co-occurrence of three
context effects - attraction, similarity, and compromise - not
only in traditional paradigms involving choices among hedonic stimuli but also of recent demonstrations of these effects
with non-hedonic stimuli. The MLBA model has analytical
solutions making it computationally easier to apply than previous dynamic models.
Keywords: Decision-making, multi-alternative choice, preference reversal, context effects, dynamic models

CX

Attribute 2

SX

Introduction

X

AX
Y
AY

Individuals are often faced with the problem of choosing a
single option from a large set of possible alternatives where
the options have several features. For example, when purchasing a new cell phone, there are numerous phones from
which to choose and each phone has many different features.
A robust finding in the choice behavior literature is that preferences are subject to “context effects”. That is, preferences
for existing alternatives can be influenced or even reversed by
the addition of new alternatives. For example, an initial preference for a cheap, low quality cell phone over an expensive,
high quality phone could be reversed when a third expensive
phone of low quality is also considered.
Three important context effects are the attraction (Huber,
Payne, & Puto, 1982), similarity (Tversky, 1972), and compromise (Simonson, 1989) effects. The standard experiment for the effects involves choices among three alternatives
which each have two attributes. For example, three different cell phones with attributes of price and quality. Figure 1
graphically represents the positions of various options within
a two dimensional space defined by two attribute values.

SY
CY

Attribute 1

Figure 1: Various options plotted in a two dimensional attribute space. Preferences between X and Y can be affected
by the presence of other options.

Three context effects
The attraction effect refers to the enhancement of an option
through the inclusion of a similar but slightly inferior decoy alternative. For the choice set {X,Y }, let AX and AY
be similar to X and Y respectively but slightly inferior to
each. For example, X might be a cheap, low quality cell
phone and AX might be the same quality as X but more
expensive. The attraction effect occurs when people show

The similarity effects refers to the enhancement of a dissimilar option when two similar options compete with one
another. For the choice set {X,Y }, let SX and SY be similar and competitive to X and Y respectively. For example, if X is a cheap, low quality cell phone, then SX might
be a little more expensive and have slightly higher quality than X. The similarity effect occurs when people show
greater preference for the dissimilar option Y when SX is included in the choice set {X,Y } as compared to when SY is included (and vice versa for X). Mathematically, the similarity
effect occurs when Pr[X|{X,Y, SX }] < Pr[X|{X,Y, SY }] and
Pr[Y |{X,Y, SX }] > Pr[Y |{X,Y, SY }].
The compromise effect refers to the enhancement of an option when it is presented as a compromise between two other
alternatives. For the choice set {X,Y }, let CX and CY be
extreme options that make X and Y take the middle ground
respectively. For example, if X is a cheap, low quality cell
phone, then CX might be drastically cheaper and extremely

3581

lower quality than X. The compromise effect occurs when
people show greater preference for X when CX is included
in the choice set {X,Y } as compared to when CY is included
(and vice versa for Y ). Mathematically, the compromise effect occurs when Pr[X|{X,Y,CX }] > Pr[X|{X,Y,CY }] and
Pr[Y |{X,Y,CX }] < Pr[Y |{X,Y,CY }].
The three context effects are theoretically important because they violated the simple scalability property (Krantz,
1964; Tversky, 1972) which is a property of most utility models of choice including Luce’s (1959) ratio of
strengths model. To show a violation, consider the attraction effect. According to simple scalability, the inequality
Pr[X|{X,Y, AX }] > Pr[X|{X,Y, AY }] implies that the strength
of AX is less than the strength of AY . However, the inequality
Pr[Y |{X,Y, AX }] < Pr[Y |{X,Y, AY }] implies that the strength
of AY is less than the strength of AX . Both statements obviously cannot be true so the property is violated. The similarity
and compromise effects produce similar violations.

Dynamic models of context effects
Because utility models cannot account for context effects due
to violations of simple scalability, researchers have turned to
dynamic models to explain the effects. There are two predominate dynamic models of the effects: multi-alternative decision field theory (MDFT) (Roe, Busemeyer, & Townsend,
2001) and the leaky competing accumulators (LCA) model
(Usher & McClelland, 2004). Even though these models have
provided great insight into multi-alternative choice, they are
not without flaws. First, both models require time intensive
simulations for fitting data with internally controlled stopping
times (the experimental procedure commonly used in context
effects tasks in which participants control when they make
decisions as opposed to an experimenter controlled deadline).
Thus, it is difficult to fit the models to human data and evaluations of the models have relied on qualitative analyses such
as showing that all three effects can be obtained using a single
set of parameters. There has not been a quantitative comparison of the models and it remains unknown whether or not
they can account for human data.
Further, the LCA model assumes that the attraction and
compromise effects are the result of loss aversion. The loss
aversion assumption seems reasonable for situations where
the options have hedonic attributes such as consumer products with attributes of price and quality. However, there is
recent evidence that context effects are a general feature of
choice behavior and not specific to options with hedonic attributes. Trueblood (2012) demonstrated the three effects
in an inference paradigm involving scenarios about criminal suspects. In these experiments, subjects were asked to
infer which suspect out of a set of three was most likely
to have committed a crime based on eye-witness evidence.
Trueblood, Brown, Heathcote, and Busemeyer (in press) also
showed the three effects in a simple perceptual task where
subjects were asked to select the largest rectangle out of a
set of three. Choplin and Hummel (2005) found the attrac-

tion effect with ovals and line segments in a similarity judgment paradigm and Tsetsos, Usher, and McClelland (2011)
obtained the similarity effect using time-varing psychophysical stimuli. These experiments all suggest that the effects are
not due to loss aversion because there is no notion of gains or
losses along the attributes.
This paper introduces a new dynamic model, the multiattribute linear ballistic accumulator (MLBA) model, to account for context effects in multi-alternative choice. The
MLBA model is easier to fit to data than MDFT and the LCA
model because of its computational tractability. Also, it does
not rely on loss aversion to explain the effects and thus can be
applied to both hedonic and non-hedonic choices.

Precursors to the MLBA model
The MLBA model is an extension of the linear ballistic accumulator (LBA) model (Brown & Heathcote, 2008) that takes
into account multiple attributes of options. The LBA models choice and response times with independent accumulators
that race to a threshold χ. Each accumulator corresponds to
a different option and the accumulator that first reaches the
threshold is selected. Within a single trial, the accumulators
are both linear and deterministic leading to mathematically
tractable solutions for choice and response times. Each accumulator starts at a randomly determined amount of evidence
selected from a uniform distribution [0, A]. The accumulators
increase at speeds defined by the drift rates which are drawn
from a normal distribution on each trial. Typically, the normal
distributions have freely-estimated mean values d1 , d2 , d3 ...
and a common standard deviation s = 1.

Weights

Valences

Contrasts

Preferences

X

X

X

Y

Y

Y

Z

Z

Z

P

Attentional
Selection

Q

Input
Preprocessing

Lateral Inhibition

Figure 2: Connectionist network interpretation of MDFT.
The MLBA model also incorporates some of the cognitive
mechanisms used in MDFT. MDFT assumes an individual’s
preferences are determined by a series of comparisons and
evaluations of the alternatives that evolve across time. The
preferences continue to evolve until one of the preference
states, associated with one of the options, reaches a threshold and is selected. Preference states are determined by va-

3582

lences for each option and lateral inhibition among the options. The valences are constructed from three components:
subjective values, stochastic attention weights, and a comparison mechanism. The strength of the lateral inhibition is determined by the distance between two options in an “indifference/dominance” space (Hotaling, Busemeyer, & Li, 2010).
MDFT can be interpreted as a connectionist network (Roe
et al., 2001; Busemeyer & Johnson, 2004), as illustrated in
Figure 2. At each moment in time, attention can be allocated
to attribute P (e.g., price) or attribute Q (e.g., quality), as illustrated in the first layer. The second layer shows each option
weighted by the attributes. This layer instantiates the comparison mechanism and projects valences to the third layer. The
valences are subject to the distance-dependent lateral inhibition process in the fourth layer, which outputs preferences.

The MLBA model
The MLBA model adds to the LBA model by explicitly specifying how drift rates arise from the evaluation of attributes. It
is assumed that mean drift rates are an increasing function
of the valences of the options, where valences are defined
in a similar manner to MDFT. Specifically, valences are determined by three components: subjective values, attention
weights, and a comparison mechanism.
In determining the mean drift rates, each option is associated with a valence that represents the advantages or disadvantages of the option. For a set of options such as {X,Y, Z},
the valences can be described by the vector V = [vX , vY , vZ ]0 .
Let P and Q be attributes where Pi and Qi denote the value
of option i on each dimension. It is assumed that decisionmakers evaluate the subjective value of each option on each
attribute producing the matrix of evaluations:


mP1 mQ1
M = mP2 mQ2  .
(1)
mP3 mQ3
In MDFT, the values mPi and mQi represent an individual’s
subjective evaluation of the attributes. However, the exact
form of this evaluation is not given. In the MLBA model,
we constrain this form. We assume that the psychological
mPi and mQi values result from a local rescaling (i.e., within
a single trial) of the experimentally defined attribute values,
Pi and Qi . Wedell (1991) argued that context effects should
be considered as local rather than global phenomena. Also,
Gonzalez-Vallejo (2002) postulated a local rescaling of options in her proportional difference model.
There are at least three ways the local rescaling can be
implemented. One possible rescaling arises from dividing
the experimental values by the smallest values on attributes
P and Q for a given set of options so that mPi = Pi /min(P)
and mQi = Qi /min(Q). Another possibility is dividing the experimental values by the largest values on attributes P and
Q for a given set of options so that mPi = Pi /max(P) and
mQi = Qi /max(Q). The third option is to divide the experimental values by the average value of the attributes P and Q

for a given set of options so that mPi = Pi / 31 (P1 + P2 + P3 ) and
mQi = Qi / 13 (Q1 + Q2 + Q3 ).
As in MDFT, the second component of the valence vector
is the attention weights. We assume the decision-maker allocates a certain amount of attention to each attribute. The
attention weight wP represents the amount of attention allocated to the P attribute across the trial and wQ represents
the amount of attention allocated to the Q attribute across the
trial. It is further assumed that wQ = 1 − wP . The two attention weights are used to define the attention vector:

W = wP

0
wQ .

(2)

The third component of the valence vector is a comparison
mechanism. Like MDFT, this comparison process determines
the relative advantage or disadvantage of each option on each
attribute. The comparison process can be represented by a
contrast matrix:


1
− 12 α12 − 12 α13
(3)
C = − 21 α21
1
− 12 α23 
1
1
1
− 2 α31 − 2 α32
Using this matrix, which assumes that αi j = α ji , we can define the valence vector in a similar manner as in MDFT by
the matrix product
V = CMW.

(4)

The weights αi j in the contrast matrix are defined by
the indifference/dominance distance function developed by
Hotaling et al. (2010) to determine the strength of the lateral inhibition in MDFT. Consider a pair of options (Pi , Qi )
and (Pj , Q j ). Define the distance between these two options
as (∆P, ∆Q) = (Pi − Pj , Qi − Q j ). These distances are then
mapped to the corresponding coordinates
in the indifference
√
and dominance space: (∆I, ∆D) = 1 2 · ((∆Q − ∆P), (∆Q +
∆P)) where ∆I is the difference along the indifference dimension and ∆D is the difference along the dominance dimension.
Using these coordinates, the distance function that weights
changes more in the dominance dimension than the indifference dimension is defined as
q
Disti j = (∆I)2 + β · (∆D)2
(5)
where β ≥ 1. The distances are converted into similarities by
the Gaussian function:
Si j = exp(−φ · Disti j ).

(6)

Using the similarities, we define the α parameters as dissimilarity measures:
αi j = 1 − Si j .
(7)
This mapping allows for options that are dissimilar to be
weighted more in the comparison process. Finally, the valences are mapped into mean drift rates using a logistic function:

3583

10
.
di =
1 + exp(−γ · vi )

(8)

In total, the model uses four free parameters to define the
mean drift rates: the attention weight wP , the dominance
weight β, the similarity parameter φ, and the logistic parameter γ. The model also has a starting point parameter A, a
threshold parameter χ and a drift rate noise parameter s which
are fixed when only modeling choice probabilities. For the
current application, the noise parameter is assumed to the be
the same for each accumulator. This assumption could be relaxed in other circumstances.
As with MDFT, the MLBA model can also be interpreted
as a connectionist network as illustrated in Figure 3. For a
given trial, a certain amount of attention can be allocated to
attribute P and to attribute Q as illustrated in the first layer of
the network. The second layer shows each option weighted
by the attributes. This layer applies the contrast process and
projects valences into the third layer. The valences are transformed into mean drift rates by a logistic function (f) in the
fourth layer. Unlike MDFT, there is no lateral inhibition.

Weights

Valences

Contrasts

X

Preferences

X

X

P

f
Y

Y

Y

Z

Z

Z

Q

Input
Preprocessing

Independent
Channels

Figure 3: Connectionist network interpretation of MLBA.
The MLBA model accounts for the similarity effect
through the local rescaling of the attribute values used to determine the M matrix. This local rescaling process results
in more favorable subjective values for the dissimilar option.
The model accounts for the attraction and compromise effects
through the comparison process captured by the C matrix.

Combined Inference Experiment
Context effects are typically tested using different groups of
subjects for different effects. Thus, it remains an open question whether the attraction, compromise, and similarity effects can be found within a single experiment using the same
subjects. This experiment investigates whether all three effects can be observed within the same experiment using the
inference paradigm developed by Trueblood (2012). Data

from this experiment will be used in the subsequent section
to compare MDFT and the MLBA model.
In Trueblood (2012), the three effects were tested in separate experiments using an inference paradigm involving decisions about criminal suspects. The experiments tested how
people infer which suspect out of a set of three is most likely
to have committed a crime based on two pieces of evidence.
The evidence was described as strength ratings from two different eyewitness testimonies where the ratings ranged from 0
to 100 with a rating of 0 corresponding to very weak evidence
for guilt and a rating of 100 corresponding to very strong evidence for guilt. In these crime scenarios, the suspects represent the different choice options and the eyewitness testimonies represent the two attributes in a similar manner as a
consumer product with attributes of quality and price.

Method
Sixty-eight undergraduate students from Indiana University
participated for course credit. Participants were told they
would see three suspects of a crime on each trial and were instructed to select the suspect that seemed most likely to have
committed the crime based on the strengths of two different
eyewitness testimonies. Participants were also told that the
testimonies of both eyewitnesses were equally valid and important and that the strengths of the testimonies were equated.
Participants did not receive feedback.
The suspects and eye-witness strengths were presented in
a table format with different suspects in different rows. In the
attraction effect experiment, for example, participants might
have seen strength ratings of 63 (eyewitness 1) and 33 (eyewitness 2) for the suspect in row one, strength ratings of 32
(eyewitness 1) and 64 (eyewitness 2) for the suspect in row
two, and strength ratings of 61 (eyewitness 1) and 31 (eyewitness 2) for the suspect in row three. In this example, the
third suspect acts as the dominated decoy for the first suspect.
Each participant completed 240 trials which were divided
into three blocks of 80 trials. The three blocks were used to
test the three effects and were randomized across participants.
Within each block, participants saw 40 trials testing one of the
effects and 40 filler trials. The presentation order of the trials
within each block was randomized. Filler trials where one
alternative was clearly superior were used to assess accuracy
throughout the experiment. The trials used to test for context
effects were subdivided so that the decoy was placed near one
alternative for some trials and near the other alternative for
other trials. For example, the attraction effect was analyzed
by comparing the choice sets {X,Y, AX } and {X,Y, AY } as
illustrated in Figure 1. The similarity effect was tested in
two regions of the attribute space using a total of four ternary
choice sets as in Trueblood (2012).

Results
For data analyses, three participants were removed because
their accuracy was two standard deviations lower than the average accuracy on the filler trials. Figure 4 shows the mean
choice probabilities for focal and non-focal alternatives in the

3584

attraction, similarity, and compromise effect trials. For the
similarity effect, the focal option refers to the dissimilar alternative because this is the one enhanced by the decoy. For
the attraction effect trials, the choice probability for the focal alternative (M = 0.548) was significantly larger than the
choice probability for the non-focal alternative (M = 0.419)
(t(64) = 3.141 , p = 0.002). The similarity trials also showed
that across all four choice sets the choice probabilities were
significantly larger for focal options (M = 0.429) than nonfocal options (M= 0.362)(t(64) = 2.578, p = 0.012). For
the compromise effect, the choice probability for compromise alternatives (M = 0.466) was significantly larger than
the choice probability for extreme alternatives (M = 0.407)
(t(64) = 2.172, p = 0.034).

by minimizing the sum of squared error (SSE) between the
model predictions and the data. When fitting mean probabilities, the SSE will approximate the maximum likelihood
estimate. Table 1 gives the mean squared error (MSE) and
the R2 values for the models.
Table 1: MSE and R2 values for MDFT and three versions of
MLBA.
Model
MDFT
MLBA (minimum rescaling)
MLBA (maximum rescaling)
MLBA (average rescaling)

MSE
0.037
0.030
0.016
0.029

R2
0.251
0.400
0.684
0.414

Comparing MDFT and MLBA
We fit MDFT and the MLBA model to the average choice
probabilities across subjects from the combined inference experiment discussed above. We did not fit individual choice
probabilities because there were not enough data from each
subject. The two models were fit to choice probabilities only
rather than choice probabilities and response times. In previous literature, multi-alternative choice models have only been
analyzed with respect to choice probabilities. Future work
could use response time measures to further test the models.
Because MDFT does not have analytical solutions for internally controlled stopping times, fitting the model requires
computationally intensive simulations. To avoid this computational difficulty an approximate method developed by
Hotaling et al. (2010) was used, where we fit the model using an externally controlled stopping procedure with a large
stopping time. We fit the model by allowing four parameters
to vary freely. One of the parameters determines the attention
weight in the first layer of the model shown in Figure 2. The
other three parameters are used in calculating the strength of
the lateral inhibition. We fixed the decision time to the large
value, t = 1001 used by Hotaling et al. (2010) and the withintrial variability parameter to 1. Because the attribute values
for the experiment were associated with eyewitness testimony
strengths ranging from 0 to 100, we used the attribute values
divided by 10 for the subjective values.
The MLBA model was fit by numerically integrating over
decision times as discussed in Hawkins et al. (submitted). For
the MLBA model, we allowed the four parameters used to define the mean drift rates to vary freely and fixed the starting
point parameter to A = 1, the threshold parameter to χ = 2,
and the drift rate noise parameter to s = 1. We fit three versions of the model corresponding to the three possible local
rescalings: minimum, maximum, and average.
We fit a total of 24 choice probabilities arising from the
eight ternary choice sets used in the experiment. The attraction and compromise effects each involved two ternary choice
sets corresponding to the two possible locations of the decoy alternative. There were four ternary choice sets used for
the similarity effect as described above. The models were fit

The MLBA model using the maximum rescaling is able
to account for about 68% of the variability in the data as indicated by the R2 value. Substantially poorer performance
was obtained for the MLBA model with minimum and average rescaling, although both still performed much better
than the MDFT model. Future work could examine the differences between the three rescalings in more detail. We doubt
MDFT’s poor fits are due to the externally controlled stopping time procedure, as Hotaling et al. (2010) found that the
externally controlled stopping time model with long stopping
times produced essentially the same results as internally controlled stopping times.

Discussion
Multi-alternative choice models such as MDFT and the LCA
model have provided great insight into choice behavior, but
these models have some drawbacks. They both required time
intensive stimulations to fit data. Further the LCA model
uses loss aversion to explain the attraction and compromise
effects. The assumption that asymmetry between losses and
gains is the underlying cause of these effects is problematic
because the effects arise in paradigms were there are no losses
or gains. The MLBA model overcomes these problems and
provides a new psychological theory of context effects.
The MLBA model consists of two components: a frontend process that compares options along their attributes and
a back-end process that determines the probability that a particular option will be selected. The back-end process is the
LBA model developed by Brown and Heathcote (2008). This
paper develops the front-end attribute processing component.
The coupling of front-end and back-end processes is not new.
The SAMBA model (Brown, Marley, Donkin, & Heathcote,
2008) of choice and response times in absolute identification
tasks proposes a front end to the Ballistic Accumulator model
(Brown & Heathcote, 2005). We suggest this pairing can be
viewed as the front-end process modulating action selection
in the back-end process. Models incorporating such modulation are common in the neurophysiological literature. For
example, Frank (2005) developed a model in which the striatum modulates motor actions and working memory updating

3585

Focal
Non−focal

0.6

0.5

0.4

0.55
Choice Probability

Choice Probability

Choice Probability

Focal
Non−focal
0.5

0.4

0.3

Compromise
Extreme

0.5
0.45
0.4
0.35

0.3

Attraction Effect

0.2

Similarity Effect

0.3

Compromise Effect

Figure 4: Mean choice probabilities with error bars showing the standard error of the mean for the attraction, similarity, and
compromise effects from the combined inference experiment.
in frontal cortex. We do not argue that our model can be
mapped to specific brain regions, but speculate that the gating
of actions by a front-end process could have a neural explanation.

Acknowledgments
Jennifer Trueblood was supported by the IGERT program in
Dynamics of Brain-Body-Environment Systems at Indiana
University. Scott Brown and Andrew Heathcote were supported by ARC fellowships. The authors would also like to
thank Jerome Busemeyer for his helpful comments.

References
Brown, S. D., & Heathcote, A. (2005). A ballistic model
of choice response time. Psychological Review, 112, 117128.
Brown, S. D., & Heathcote, A. (2008). The simplest compete
model of choice response time: Linear ballistic accumulation. Cognitive Psychology, 57, 153-178.
Brown, S. D., Marley, A. A. J., Donkin, C., & Heathcote, A.
(2008). An integrated model of choices and response times
in absolute identification. Psychological Review, 115(2),
396-425.
Busemeyer, J. R., & Johnson, J. G. (2004). Computational
models of decision making. In D. Koehler & N. Harvey (Eds.), Handbook of judgment and decision making
(p. 133-154). Blackwell Publishing Co.
Choplin, J., & Hummel, J. (2005). Comparison-induced decoy effects. Memory & Cognition, 33, 332343.
Frank, M. J. (2005). Dynamic dopamine modulation in the
basal ganglia: a neurocomputational account of cognitive
deficits in medicated and non-medicated parkinsonism. J
Cogn Neurosci, 17, 51-72.
Gonzalez-Vallejo, C. (2002). Making trade-offs: A probabilistic and context-sensitive model of choice behavior.
Psychological Review, 109, 137-155.
Hawkins, G. E., Marley, A. A. J., Heathcote, A., Flynn, T. N.,
Louviere, J. J., & Brown, S. D. (submitted). Integrating
cognitive process and descriptive models of attitudes and
preferences.

Hotaling, J. M., Busemeyer, J. R., & Li, J. (2010). Theoretical
developments in decision field theory: A comment on k.
tsetsos, n. chater, and m. usher. Psychological Review, 117,
1294-1298.
Huber, J., Payne, J. W., & Puto, C. (1982). Adding asymmetrically dominated alternatives: Violations of regularity and
the similarity hypothesis. Journal of Consumer Research,
9, 90-98.
Krantz, D. H. (1964). The scaling of small and large color
differences. (doctoral dissertation, university of pennsylvania). Ann Arbor, Michigan: University Microfilms. No.
65-5777.
Luce, R. D. (1959). Individual choice behavior: A theoretical
analysis. New York: Wiley.
Roe, R. M., Busemeyer, J. R., & Townsend, J. T. (2001).
Multialternative decision field theory: A dynamic connectionist model of decision making. Psychological Review,
108, 370-392.
Simonson, I. (1989). Choice based on reasons: The case of
attraction and compromise effects. Journal of Consumer
Research, 16, 158-174.
Trueblood, J. S. (2012). Multi-alternative context effects obtained using an inference task. Psychonomic Bulletin &
Review, 19 (5), 962-968.
Trueblood, J. S., Brown, S. D., Heathcote, A., & Busemeyer,
J. R. (in press). Not just for consumers: Context effects are
fundamental to decision-making. Psychological Science.
Tsetsos, K., Usher, M., & McClelland, J. (2011). Testing
multi-alternative decision models with non-stationary evidence. Frontiers in Neuroscience, 5, 1-18.
Tversky, A. (1972). Elimination by aspects: a theory of
choice. Psychological Review, 79, 281-299.
Usher, M., & McClelland, J. L. (2004). Loss aversion and
inhibition in dynamical models of multialternative choice.
Psychological Review, 111, 757-769.
Wedell, D. H. (1991). Distinguishing among models of contextually induced preference reversals. Journal of Experimental Psychology: Learning, Memory, and Cognition, 17,
767-778.

3586

