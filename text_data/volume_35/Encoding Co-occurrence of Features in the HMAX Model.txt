UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Encoding Co-occurrence of Features in the HMAX Model
Permalink
https://escholarship.org/uc/item/2pb1874q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Jalali, Sepehr
Tan, Cheston
Lim, Joo-Hwee
et al.
Publication Date
2013-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                      Encoding Co-occurrence of Features in the HMAX Model
                                                  Sepehr Jalali (tmssj@nus.edu.sg)
                                            Cheston Tan (cheston-tan@i2r.a-star.edu.sg)
                                             Joo-Hwee Lim (joohwee@i2r.a-star.edu.sg)
                                              Jo-Yew Tham (jytham@i2r.a-star.edu.sg)
                                               Sim-Heng Ong (eleongsh@nus.edu.sg)
                                              Paul James Seekings (mmrl@nus.edu.sg)
                                              Elizabeth A. Taylor (tmsohe@nus.edu.sg)
                                           National University of Singapore, Singapore 119077
                                     Institute for Infocomm Research, A*STAR, Singapore 138632
                              Abstract                                 visual cortex of cats (Finn & Ferster, 2007; Lampl, Ferster,
                                                                       Poggio, & Riesenhuber, 2004), as well as in the higher vi-
   We introduce a method for encoding co-occurrence of features
   in the HMAX model of visual recognition, and conduct a series       sual areas of monkeys, such as areas V 4 (Gawne & Martin,
   of experiments to investigate the contribution of co-occurrence     2002) and IT (Sato, 1989). Importantly, however, each of
   towards better recognition performance. We show that classi-        these studies also showed evidence for “Average” pooling oc-
   fication accuracy is increased by adding a higher-order layer
   to the HMAX processing hierarchy, whereby co-occurrence             curring, which can be interpreted as encoding the mean oc-
   of features is encoded as a new dictionary of features. We          currence frequency of features.
   show that concatenation of mean pooling, max pooling and               Beyond just being tuned to the statistics of feature occur-
   co-occurrence information results in better classification re-
   sults on three datasets (Caltech101, a subset of Caltech256,        rences, there is strong evidence that the primate visual sys-
   and TMSI Underwater Images). Overall, we show that incor-           tem is also tuned to co-occurrence statistics. This refers to
   porating co-occurrence statistics into a biologically-inspired      either the joint or conditional probabilities of two (or more)
   model of visual recognition provides a boost in classification
   performance above that produced by incorporating occurrence         features occurring together within images belonging to a cer-
   statistics alone.                                                   tain object category or across categories. Since a “feature”
   Keywords: computer vision; HMAX; biologically inspired;             is not always a precisely defined concept, how can the co-
   co-occurrence statistics; visual cortex; image classification.      occurrence of two features be distinguished from the occur-
                                                                       rence of a single feature that happens to be comprised of two
                          Introduction                                 simpler features? To make this distinction unambiguous, ex-
Certain categories of visual stimuli can be characterized by           periments were designed such that the elementary features are
the co-occurrence of multiple features. For example, images            visually distinct, due to explicit segmentation, due to spatial
of cars frequently contain wheels, doors and windows. These            separation, or from the task context. We term such features,
co-occurring features do not occur in rigid configurations.            which are the result of sensitivity to co-occurrence, as “co-
Even for a rigid object, 3D rotations can result in inter-feature      occurrence features”.
distances changing when projected as 2D images. However,                  In some sense, mid-level features themselves can be con-
co-occurring features are generally found close to each other.         sidered as co-occurrence features, with their elementary fea-
Using faces as an example, the exact distances between facial          tures being simple orientation-sensitive filters (corresponding
features (e.g. eyes, nose, mouth) vary from person to person,          to orientation-sensitive neurons in the primary visual cortex).
but these features are always relatively near to each other.           Since lines, curves and contours are ubiquitous in images,
   Can this particular property be exploited to achieve bet-           the presence of a short line segment of a certain orientation
ter visual recognition performance? This question cannot               strongly predicts that the orientation of a neighboring line
be cleanly answered through behavioral experiments unless              segment will be similar. This is particularly so if the rela-
brain cells encoding co-occurrence can somehow be “turned              tive position of that neighboring line segment is such that the
off”; computational modeling may be a better approach. In              two line segments have the possibility of being collinear.
this paper, as a proof-of-concept, we modify the biologically-            Our focus here is on high-level features whose elemen-
inspired HMAX model of visual recognition (Riesenhuber                 tary features are more complex than simple oriented filters.
& Poggio, 1999) to encode co-occurrence statistics that are            These high-level features approach the level of semantic ob-
learnt from a training set of images, and we show that recog-          ject parts or possibly even objects themselves. In the rest of
nition performance does indeed improve.                                this section, we will review the experimental evidence that the
                                                                       primate visual system develops sensitivity to such high-level
                          Background                                   co-occurrence features.
There is evidence for Max spatial pooling (finding the max-               In the field known as visual statistical learning (VSL), it
imum among a set of inputs from a local spatial region) oc-            has clearly been shown that adult humans develop sensitiv-
curring at multiple levels in the visual system in the primary         ity to co-occurrence statistics in images (Fiser & Aslin, 2001;
                                                                   2644

Aslin & Newport, 2012). In a ground-breaking study by Fiser           (2002); Sakai and Miyashita (1991). However, there are limi-
and Aslin (2002) it was shown that 9-month-old infants al-            tations, such as the presence of noise, limited recording time,
ready developed sensitivity to visual co-occurrence statistics.       and the ability to record from at most a few hundred neurons.
   There is also an abundance of evidence from monkeys                   Beyond just “being sensitive” to co-occurrence statistics,
that their visual systems develop sensitivity to co-occurrence        what are the characteristics of such sensitivity? It is specific
statistics. Miyashita (1988) and Sakai and Miyashita (1991),          to spatial configuration, such as the relative position of the
monkeys were trained to recognize pairs of stimuli, in a              elementary features (Hirabayashi & Miyashita, 2005). In ad-
paradigm known as paired-associate learning. Neurons were             dition, this sensitivity is reflected not in strength of neural
found that were sensitive to such trained stimulus pairs, but         responses per se, but rather in the selectivity for co-occurring
not other stimulus pairs. The pairings were arbitrary, making         features relative to non-co-occurring features (Baker et al.,
the likelihood that such neurons had already possessed such           2002).
sensitivity vanishingly small. More recently, Hirabayashi and            One special case of sensitivity to co-occurrence of fea-
Miyashita (2005) found that populations of IT neurons are             tures is that of faces. The elementary features are seman-
sensitive to feature configuration within objects.                    tic face parts such as the eyes, nose and mouth. It is very
   Direct evidence for sensitivity to co-occurrence (over             well-established that humans and monkeys are sensitive to the
and above sensitivity to occurrence) was found by Baker,              combination and relative configuration of face parts. Specif-
Behrmann, and Olson (2002). Monkeys were trained to dis-              ically, any change to the normal configuration of the face
criminate between objects that were each composed of two              leads to reduced neural responses and poorer recognition ac-
distinct parts linked by a line, forming “baton” objects. Com-        curacy. One manifestation of this is the Face Inversion Effect
pared to untrained objects, selectivity for trained objects was       (FIE), whereby inverted faces are much more poorly recog-
enhanced. This was for both the individual parts, as well as          nized than upright faces (Yin, 1969). Faces with the parts
the combined “baton” objects. Crucially, selectivity for the          in scrambled configurations are also poorly recognized. Fur-
two parts together (i.e. the whole object) was greater than the       thermore, the sensitivity to co-occurrence seems to be un-
combined (summed) selectivity for each individual part.               avoidable. In what is known as the Composite Face Effect,
   Under what conditions does sensitivity to co-occurrence            people are sensitive to the bottom halves of faces, even when
develop? In human adults, this is an implicit process that            they are explicitly instructed to ignore them during a discrim-
develops without awareness of the co-occurrence statistics,           ination task (Young, Hellawell, & Hay, 1987).
using a “cover task” or even through mere exposure (Turk-                Generally, such sensitivity requires normal visual experi-
Browne, Jungé, & Scholl, 2005; Turk-Browne, Scholl, Chun,            ence during infancy in order to develop (Le Grand, Mond-
& Johnson, 2009; Aslin & Newport, 2012). This is also true            loch, Maurer, & Brent, 2004). It also develops quickly, reach-
for human infants (Fiser & Aslin, 2002; Aslin & Newport,              ing adults levels (at least qualitatively) by age 4 (Heering,
2012). In monkeys, most work has been done using active               Houthuys, & Rossion, 2007); this is consistent with the no-
task learning. This is so that the neural selectivity for trained     tion that passive exposure is sufficient for co-occurrence sen-
objects can be compared to the control set of untrained ob-           sitivity to develop (see above). Evidence for sensitivity to
jects. Since neural selectivity is enhanced for features that         co-occurrence for face parts has also been found at the level
are diagnostic for active task learning (Sigala & Logothetis,         of single neurons. Freiwald, Tsao, and Livingstone (2009)
2002), passive viewing may not be sufficient to produce selec-        found that in one of the brain regions that respond selectiv-
tivity that is large enough to be statistically significant when      ity to faces, neurons on average responded to combinations
measured from electrode recordings.                                   of two to three face parts, rather than individual parts. Co-
   How has sensitivity to co-occurrence been measured ex-             occurrences have been studied in a series of experiments such
perimentally? The methods have generally been constrained             as Edelman, Yang, Hiles, and Intrator (2002).
by the nature of the subjects. Adult human subjects have gen-            Use of co-occurrences of features for creating more com-
erally been tested behaviorally, i.e. through their explicit re-      plex features in Fidler, Boben, and Leonardis (2008) shows an
sponses (usually simple ‘yes/no’ tests). More recently, fMRI          improvement in classification accuracy, and bag-of-features
has been shown to be able to detect co-occurrence sensitiv-           approaches show improvements in classification results us-
ity (Turk-Browne et al., 2009). In human infants, due to              ing frequency of patches in the images in (Fei-Fei & Perona,
their inability to understand or respond explicitly to verbal         2005). Co-occurrence information can be used to find part-
instruction, experiments have been constrained to using tests         part and part-whole relations of features of different recep-
for novelty detection that are ubiquitous for infants. In mon-        tive field sizes. If a feature is occurring too often in a class
keys, due to the ability to conduct invasive experiments that         (and not likewise in other classes), it is more likely to be a
are not possible with humans, scientists have conducted elec-         discriminant feature in that class and if two features are co-
trophysiological experiments (i.e. using electrodes to record         occurring in a class often in a neighborhood, they may be part
the responses of individual neurons). Such experiments al-            of a more complex feature and can have a part-part relation-
low for a detailed, “close-up” analysis of the effects of co-         ship and they might be more related to the object rather than
occurrence at the level of individual neurons e.g. Baker et al.       the background (unless the background is also repetitive, e.g.
                                                                  2645

sky in airplane images). Also, if there exist features of differ-
ent sizes and they co-occur in the same position on different
scales they are likely to have a part-whole relationship.
                       HMAX Model
The HMAX model (Riesenhuber & Poggio, 1999) simulates
the feed-forward path of the visual cortex. This model is used
to find a good trade-off between invariance and selectivity.
S1 cells provide selectivity by responding to oriented filters
and C1 cells provide invariance by pooling over neighboring
scales and positions. We use the HMAX model presented in
Mutch and Lowe (2008) in the first three layers (S1, C1 and
S2). Here we have a brief review on this model and show our           Figure 1: In HMAX, the max on the columns is taken as the
modifications to it.                                                  response for creating C2 output vector. In contrast, histogram
   In this implementation, an image is fed into the structure         approaches based on SIFT methods use the frequency of fea-
and 10 different scales of the image are created as inputs to         ture occurrence, i.e. the normalized sum of the max values on
S1 layer. Gabor filters in 12 orientations are created as S1          the rows.
layer filters:
                                                                         The values of R are stored as S2 layer. The distance of
                            (X 2 + γ2Y 2 )
                                                     
                                                   2π
         G(x, y) = exp −                     cos      X   .   (1)     each sample from each training image with each entry on the
                                 2σ2               λ                  dictionary is calculated and a local max is taken in C2 layer
   where X = x cos θ − y sin θ and Y = x sin θ + y cos θ. The         in ±1scale and ±10% spatial neighborhood (despite a global
values of x and y vary between -5 and 5, and θ varies between         max in Serre et al. (Serre et al., 2005)). These C2 features
0 and π. The parameters γ (aspect ratio), σ (effective width),        are sent to the SVM for training. For testing images the same
and λ (wavelength) are all taken from Serre, Wolf, and Poggio         hierarchical procedure is repeated. In (Mutch & Lowe, 2008)
(2005) and are set to 0.3, 4.5, and 5.6 respectively.                 sparse prototypes are calculated and the maximum response
   A fixed size of Gabor filters is implemented on different          from all directions for each window is taken and SVM nor-
scales of the images where the smaller edge of the biggest            mals method (Mladenić, Brank, Grobelnik, & Milic-Frayling,
image is set to 140 pixels while maintaining the aspect ratio         2004) is used to select the features with higher weights. In
(the image pyramid of 10 scales created each layer by a factor        this approach, SVM is run a few times, and each time fea-
of 21/4 smaller than the last using bicubic interpolation). The       tures with lower weights are dropped. In this HMAX imple-
response of a patch of pixels X to a particular S1 filter G is        mentation, once S2 features are calculated, the C2 layer is
given by:                                                             calculated as:
                                   ∑ Xi Gi                                              C2(n) = max(Vkn ) for ∀k ∈ M
                       R(x, y) = q                            (2)
                                                                                                       for n = 1, ..., N          (4)
                                      ∑ Xi2
   These outputs are sent to the C1 layer, which performs a lo-          As can be seen in Figure 1 in conventional HMAX ap-
cal 3D max operation on both scale (±1) and position (3 × 3           proaches, the max on the columns is taken as the value for
neighborhood) of the filter responses. The output of this layer       C2 either in a local neighborhood of each feature or globally.
is a pyramid consisted of between 500-2000 different patches          Since taking the max in a local neighborhood (in ±1 scale and
of size 4 × 4, 8 × 8, 12 × 12 and 16 × 16 in 8 scales depending       ±10%spatial neighborhood) is shown to improve the perfor-
on the size of the input image. In this level one or two sam-         mance by about 5% in Caltech101 dataset in Mutch and Lowe
ples are randomly sampled from each training image (from              (2008), in our experiments we also use a local neighborhood
random scales and positions) and a dictionary of features of          for calculating the responses. We also eliminate the local in-
size 4096 is created. This dictionary is then made sparse by          hibition in S2 level proposed in Mutch and Lowe (2008) as
selecting the highest response from each orientation and set-         it increased the performance. Once a feature belongs to the
ting the rest to 0.                                                   first or last scale in the pyramid, we extend the neighborhood
   The response of a patch of C1 units X to a particular S2           to two neighboring scales. Same method is used for features
feature/prototype P (a dictionary feature), of size n × n, is         which fall in the borders of each scale, and +20% or −20%
given by a Gaussian radial basis function:                            of their neighborhood is used for comparisons.
                                                                         If we take the sum of the values on rows in Figure 1 and
                                                                      normalize them, these are “HMean” features, which are also
                                     k X − P k2
                                                
                 R(X, P) = exp −                              (3)     biologically-inspired, and significantly improve classification
                                          σ2                          results when concatenated with HMAX features (Jalali, Lim,
                                                                  2646

Tham, & Ong, 2012). HMean is equivalent to the feature
occurrence frequency in “bag-of-features” methods.
        Encoding Co-occurrence of Features
For each class, we first find the value and index of the most-
frequently occurring features (MOF). The next step is to en-
code the co-occurrence of these features as can be seen in
Figure 1. For every class, we calculate the co-occurrence of
the most frequent features and store it as a S3 dictionary fea-
ture. Hence a new dictionary of features is added to the model
which is composed of #MOF × #MOF entries for each class,
where #MOF was set as 20. In this dictionary of features, the
value of each dictionary feature is calculated as:
                                          k Si − S j k2
                                                       
          C3(i, j) = C2(i)C2( j) exp −                          (5)
                                               σ2
where Sn represents the spatial position of the C2 feature and
σ = 0.5.
   This dictionary encodes the value of co-occurrence of ev-
ery pair of features selected for each class. Hence we will
have NN dictionaries where NN stands for the number of cat-
egories in the classification task. These dictionaries are con-
catenated to create the C2 dictionary of features. In the train-
ing and test phases, the respective feature to each dictionary
feature is found (the most similar feature in every image) and
the similarity of the values in dictionary of features are cal-               Figure 2: Diagram of model processing hierarchy.
culated for every image. This results in a #MOF × #MOF×
NN feature as the C3 feature and it is concatenated to C2 fea-
ture vector and sent to the classifier for classification. The          size) from 13 categories. Example images from this dataset
extended model for encoding the co-occurrence of features is            are shown in Figure 3. We used 30 images per category for
shown in Figure 2.                                                      training, and the rest for testing.
                                                                           Results are shown in Table 1. For all images, only intensity
                   Experimental Results                                 (luminance) information was used. All results were derived
We evaluated our co-occurrence model on the Caltech101                  using 8 random train/test splits. For all three datasets, the
dataset (Fei-Fei, Fergus, & Perona, 2004). The model was                combination of HMAX and co-occurrence features gave bet-
trained on 30 images per category (standard for this dataset;           ter results (classification accuracy) than either type of feature
see Mutch and Lowe (2008)), and tested on all the other im-             alone (Caltech101: 59.3% vs. 54.7% vs. 57.7%; Caltech256:
ages. We also used the Caltech256 dataset (Griffin, Holub,              64.4% vs. 60.2% vs. 48.6%; Underwater Images: 98.7%
& Perona, 2007), because it allows for more images per cat-             vs. 92.9% vs. 92.2%). Since co-occurrence features were
egory than Caltech101. In particular, we considered only the            derived from the co-occurrence of HMean features, we also
14 (out of 256) categories which had 200 or more images.                compared which of these two feature types (co-occurrence
We trained the model on 150 images (so that there would be              vs. HMean) gave better results when combined with HMAX.
at least 50 images for testing), and tested on the rest. We also        Again, for all three datasets, combining co-occurrence fea-
examined classification accuracy as a function of number of             tures with HMAX produced better results than combining
training images for Caltech256. This was motivated by the               HMean with HMAX (Caltech101: 59.3% vs. 58.9%; Cal-
concern that co-occurrence features could require more data             tech256: 64.4% vs. 61.3%; Underwater Images: 98.7% vs.
for reliable co-occurrence statistics to be extracted, before the       98.3%). Furthermore, for all datasets, the combination of all
advantage of co-occurrence could be properly manifested.                three feature types was better than just HMAX and HMean
   We also evaluated the performance of our model on a new              together (Caltech101: 60.1% vs. 58.9%; Caltech256: 64.1%
dataset consisting of images of underwater targets. The main            vs. 61.3%; Underwater Images: 99.0% vs. 98.3%).
challenge with underwater images is the existence of particles             We also examined the effect of disregarding spatial dis-
that limit the visibility in unclear waters and results in scatter-     tance (i.e. the exponential in Eq. 5). As seen in Table 1, for all
ing, reflection and absorption of light, and the differential ab-       datasets, results were better when spatial distance was taken
sorption of light of different wavelengths by water itself. This        into account (Caltech101: 57.7% vs. 55.1%; Caltech256:
dataset consists of 1664 images (roughly 740 × 420 pixels in            48.6% vs. 44.2%; Underwater Images: 92.2% vs. 83.3%).
                                                                    2647

                                                                        Figure 4: Classification accuracy on Caltech256 as a function
                                                                        of number of training images.
 Figure 3: Examples from TMSI Underwater Images dataset.
                                                                                                 Discussion
                                                                        In this paper, we showed that combining co-occurrence fea-
Table 1: Classification performance on the Caltech101, Cal-             tures with regular HMAX features leads to better classifica-
tech256 (subset – see text for details), and TMSI Underwater Images     tion performance than using either feature type alone. Fur-
datasets.                                                               thermore, adding co-occurrence features to HMAX increases
                                                                        performance more than adding occurrence features. The three
  Method                 Caltech101 Caltech256         Underwater       types of features encode different information, and therefore
                                           (subset)    Images           the combination of all three feature types gave the best overall
  HMAX                   54.7              60.2        92.9             performance. For co-occurrence, the spatial distance between
  Co-occurrence          55.1              44.2        83.3             the two co-occurring features also contributes to better perfor-
  (no distance)                                                         mance. In this work, we focused solely on HMAX. However,
  Co-occurrence          57.7              48.6        92.2             in future work, our co-occurrence method can be applied to
  HMAX +                 59.3              64.4        98.7             other vision algorithms.
  Co-occurrence                                                            In preliminary experiments not reported here, we exper-
  HMAX + HMean           58.9              61.3        98.3             imented with creating co-occurrence features from HMAX
  HMAX + HMean           60.1              64.1        99.0             features (rather than HMean features, as done in this paper).
  + Co-occurrence                                                       However, this resulted in either a drop in performance or no
                                                                        change. This will be investigated further in future work.
                                                                           Fig. 4 suggests that the performance boost from using co-
                                                                        occurrence may be limited by the number of training images.
   In order to evaluate the effect of number of training im-            More detailed investigation is limited by the relatively small
ages for the creation of co-occurrence features, we trained             number of images per category in these datasets. Further in-
the model with varying numbers of training images per cat-              vestigation may require utilizing or creating larger datasets.
egory. As shown in Figure 4, the performance boost when                    Another prospect for further improvement is to encode co-
adding co-occurrence features was greatest when using 150               occurrence of more than two features. However, besides
training images. However, for fewer than 150 training im-               possibly requiring even more training data than two-feature
ages, the boost from adding co-occurrence features is unreli-           co-occurrence, there may be diminishing returns for such
able. Nonetheless, looking at just HMAX alone, performance              “higher-order” co-occurrences. This is because relatively
seems to asymptote at 150 training images, but for the com-             fewer classes will have the underlying visual structure that
bination of HMAX and co-occurrence features, performance                will benefit from encoding such co-occurrences.
seems to increase roughly linearly with the number of train-               In this paper, the choice of features for encoding co-
ing images. While empirically, co-occurrence may help per-              occurrence was based on their frequency. Choosing discrim-
formance in all datasets, similar analyses (i.e. performance            inative (rather than frequent) features for co-occurrence en-
boost as a function of number of training images) for the other         coding may be a more direct approach to maximizing classi-
2 datasets may not be meaningful, since the maximum num-                fication performance. To choose discriminative features, one
ber of training images is only 30 per category.                         approach is to train the SVM several times and remove fea-
                                                                    2648

tures with low weights, as in Mutch and Lowe (2008), or to            depending on the feature configuration within a whole ob-
simply use features with mean response values that differ the         ject. Journal of Neuroscience, 25(44), 10299–307.
most between different classes.                                     Jalali, S., Lim, J., Tham, J., & Ong, S. (2012). Clustering and
                                                                      use of spatial and frequency information in a biologically
                          References                                  inspired approach to image classification. In International
                                                                      Joint Conference on Neural Networks (pp. 1–8).
Aslin, R. N., & Newport, E. L. (2012). Statistical Learning:
                                                                    Lampl, I., Ferster, D., Poggio, T., & Riesenhuber, M. (2004).
  From Acquiring Specific Items to Forming General Rules.
                                                                      Intracellular measurements of spatial integration and the
  Current Directions in Psychological Science, 21(3), 170–
                                                                      MAX operation in complex cells of the cat primary visual
  176.
                                                                      cortex. Journal of Neurophysiology, 92(5), 2704–13.
Baker, C. I., Behrmann, M., & Olson, C. R. (2002). Impact of
                                                                    Le Grand, R., Mondloch, C. J., Maurer, D., & Brent, H. P.
  learning on representation of parts and wholes in monkey
                                                                      (2004). Impairment in holistic face processing following
  inferotemporal cortex. Nature Neuroscience, 5(11), 1210–
                                                                      early visual deprivation. Psychological Science, 15(11),
  6.
                                                                      762–8.
Edelman, S., Yang, H., Hiles, B., & Intrator, N. (2002). Prob-
                                                                    Miyashita, Y. (1988). Neuronal correlate of visual associative
  abilistic principles in unsupervised learning of visual struc-
                                                                      long-term memory in the primate temporal cortex. Nature,
  ture: human data and a model. Advances in Neural Infor-
                                                                      335, 817–20.
  mation Processing Systems, 1, 19–26.
                                                                    Mladenić, D., Brank, J., Grobelnik, M., & Milic-Frayling, N.
Fei-Fei, L., Fergus, R., & Perona, P. (2004). Learning gen-
                                                                      (2004). Feature selection using linear classifier weights:
  erative visual models from few training examples: an in-
                                                                      interaction with classification models. In Proceedings of
  cremental Bayesian approach tested on 101 object cate-
                                                                      the 27th Annual International ACM SIGIR Conference on
  gories. IEEE. CVPR 2004. In Workshop on generative-
                                                                      Research and Development in Information Retrieval (pp.
  model based vision (Vol. 2).
                                                                      234–241).
Fei-Fei, L., & Perona, P. (2005). A bayesian hierarchi-             Mutch, J., & Lowe, D. (2008). Object class recognition and
  cal model for learning natural scene categories. In IEEE            localization using sparse features with limited receptive
  Conference on Computer Vision and Pattern Recognition               fields. International Journal of Computer Vision, 80(1),
  (Vol. 2, pp. 524–531).                                              45–57.
Fidler, S., Boben, M., & Leonardis, A. (2008). Similarity-          Riesenhuber, M., & Poggio, T. (1999). Hierarchical models
  based cross-layered hierarchical representation for object          of object recognition in cortex. Nature Neuroscience, 2,
  categorization. In IEEE Conference on Computer Vision               1019–1025.
  and Pattern Recognition (pp. 1–8).                                Sakai, K., & Miyashita, Y. (1991). Neural organization for
Finn, I. M., & Ferster, D. (2007). Computational diversity            the long-term memory of paired associates. Nature, 354,
  in complex cells of cat primary visual cortex. Journal of           152–5.
  Neuroscience, 27(36), 9638–48.                                    Sato, T. (1989). Interactions of visual stimuli in the recep-
Fiser, J., & Aslin, R. N. (2001). Unsupervised statisti-              tive fields of inferior temporal neurons in awake macaques.
  cal learning of higher-order spatial structures from visual         Experimental Brain Research, 77(1), 23–30.
  scenes. Psychological Science, 12(6), 499–504.                    Serre, T., Wolf, L., & Poggio, T. (2005). Object recognition
Fiser, J., & Aslin, R. N. (2002). Statistical learning of new         with features inspired by visual cortex. In IEEE Conference
  visual feature combinations by infants. Proceedings of the          on Computer Vision and Pattern Recognition (Vol. 2, pp.
  National Academy of Sciences of the United States of Amer-          994–1000).
  ica, 99(24), 15822–6.                                             Sigala, N., & Logothetis, N. K. (2002). Visual categorization
Freiwald, W. A., Tsao, D. Y., & Livingstone, M. S. (2009).            shapes feature selectivity in the primate temporal cortex.
  A face feature space in the macaque temporal lobe. Nature           Nature, 415, 318–20.
  Neuroscience, 12(9), 1187–96.                                     Turk-Browne, N. B., Jungé, J., & Scholl, B. J. (2005). The
Gawne, T. J., & Martin, J. M. (2002). Responses of Primate            automaticity of visual statistical learning. Journal of Ex-
  Visual Cortical V4 Neurons to Simultaneously Presented              perimental Psychology: General, 134(4), 552–64.
  Stimuli. Journal of Neurophysiology, 88(3), 1128–1135.            Turk-Browne, N. B., Scholl, B. J., Chun, M. M., & Johnson,
Griffin, G., Holub, A., & Perona, P. (2007). Caltech-256              M. K. (2009). Neural evidence of statistical learning: ef-
  object category dataset (Tech. Rep. No. 7694). California           ficient detection of visual regularities without awareness.
  Institute of Technology.                                            Journal of Cognitive Neuroscience, 21(10), 1934–45.
Heering, A. de, Houthuys, S., & Rossion, B. (2007). Holistic        Yin, R. K. (1969). Looking at upside-down faces. Journal of
  face processing is mature at 4 years of age: evidence from          Experimental Psychology, 81(1), 141–145.
  the composite face effect. Journal of Experimental Child          Young, A. W., Hellawell, D., & Hay, D. C. (1987). Configu-
  Psychology, 96(1), 57–70.                                           rational information in face perception. Perception, 16(6),
Hirabayashi, T., & Miyashita, Y. (2005). Dynamically mod-             747–59.
  ulated spike correlation in monkey inferior temporal cortex
                                                                2649

