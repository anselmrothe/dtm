UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Comparing Model Comparison Methods
Permalink
https://escholarship.org/uc/item/4352z3vc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Schultheis, Holger
Singhaniya, Ankit
Chaplot, Devendra Singh
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                      Comparing Model Comparison Methods
                                     Holger Schultheis (schulth@informatik.uni-bremen.de)
                     Cognitive Systems, University of Bremen, Enrique-Schmidt-Str. 5, 28359 Bremen, Germany
                                                               Ankit Singhaniya
                                 Computer Science and Engineering, NIT Nagpur, Nagpur 440010, India
                                                           Devendra Singh Chaplot
                                Computer Science and Engineering, IIT Bombay, Mumbai 400076, India
                               Abstract                                   faced with a situation that requires comparing models regard-
   Comparison of the ability of different computational cogni-            ing their ability for simulating human behavior, modelers are
   tive models to simulate empirical data should ideally take into        often faced with the problem that it is unclear which model
   account the complexity of the compared models. Although                comparison methods could reasonably and should ideally be
   several comparison methods are available that are meant to
   achieve this, little information on the differential strengths and     employed in a given situation.
   weaknesses of these methods is available. In this contribu-               In this contribution we present the results of a systematic
   tion we present the results of a systematic comparison of 5            comparison of 5 model comparison methods. The methods
   model comparison methods. Employing model recovery sim-
   ulations, the methods are examined with respect to their ability       are examined with respect to their ability to select the model
   to identify the model that actually generated the data across 3        that actually generated the data across 3 pairs of models and a
   pairs of models and a number of comparison situations. The             number of contextual variations (e.g., tightness of fits, amount
   simulations reveal several interesting aspects of the considered
   methods such as, for instance, the fact that in certain situa-         of noise in the data). The obtained results highlight impor-
   tions methods perform worse than model comparison neglect-             tant properties of the different comparison methods. Together
   ing model complexity. Based on the identified method charac-           with the fact that all 5 considered methods are general in the
   teristics, we derive a preliminary recommendation on when to
   use which of the 5 methods.                                            sense that they place no restrictions on the type of models that
   Keywords: computational cognitive models, model compari-               can be compared, these results are, we believe, conducive to
   son, model mimicry, model generalization                               increasing the frequency with which more sophisticated com-
When computationally modeling cognition, often several dif-               parison methods instead of the naı̈ve approach will be em-
ferent models are available or conceivable as explanations for            ployed for model evaluation and comparison.
the cognitive ability in question. In such a situation, the aim              The remainder of this article is structured as follows. First,
is to select the best of these candidate models according to              we list and briefly describe all considered methods. Second,
a set of criteria. Among others (e.g., falsifiability or inter-           the employed models, contextual variations, and procedu-
pretability) the extent to which the different models are able            ral details of the method comparison are described. Subse-
to simulate observed human behavior is usually considered a               quently, comparison results are presented and discussed be-
key criterion for selecting from the candidate models.                    fore we conclude our considerations and highlight topics for
   A naı̈ve approach to gauge the models’ ability to simulate             future work.
the existing observations is to fit each model to the available
data and choose the model that provides the tightest fit as indi-
                                                                                                     Methods
cated, for instance, by the models’ Root Mean Squared Error               The 5 methods we compared are the bootstrap, the bootstrap
(RMSE). Such an approach is problematic, because it does                  with standard error (SE) and confidence interval (CI), the
not take into account the complexity of the compared mod-                 data-uninformed parametric bootstrap cross-fitting method,
els. As a result, there is a tendency for overfitting and for             henceforth called cross-fitting method (CM), the simple hold-
selecting more complex models even if simpler models pro-                 out, and the prediction error difference method (PED). Each
vide the better explanation of the considered cognitive ability           of these was applied to 3 pairs of models and will be described
(Pitt & Myung, 2002).                                                     in turn below.
   Several methods taking into account model complexity
have been proposed to avoid the pitfalls of the naı̈ve ap-                Bootstrap
proach (see Shiffrin, Lee, Kim, & Wagenmakers, 2008, for                  Given a set of n observations, the bootstrap method of model
an overview). However, common use of such more sophisti-                  comparison proceeds as follows (see Efron & Tibshirani,
cated model comparison methods is partly hampered by the                  1993, for an overview of bootstrapping procedures). First,
fact that many properties of the different methods are in-                an arbitrary but fixed number B of bootstrap samples is gen-
sufficiently investigated. Only very few studies (e.g., Co-               erated. A bootstrap sample is a set of n data points ran-
hen, Sanborn, & Shiffrin, 2008) have systematically exam-                 domly drawn with replacement from the n original obser-
ined different comparison methods with respect to their dif-              vations. Due to sampling with replacement, most bootstrap
ferential advantages and disadvantages. Consequently, when                samples will contain only a subset of all original observa-
                                                                      1294

tion (but some of these more than once). Second, each of                 Given a set of observations, these two distributions can be
the to-be-compared models is fitted to each bootstrap sample.        utilized to decide which of the two models provides the bet-
Third, for each bootstrap sample, the fitted models are used to      ter account of the observations. Both models are fitted to the
predict those data points that were not in the bootstrap sam-        observations and the difference in the models’ GOFs are com-
ple and the deviation of the predictions from the original data      puted. If the resulting difference is classified to more likely
points is measured (e.g., by the mean squared error). Fourth,        come from the distribution resulting from data generated from
the measures of deviation are combined for each model across         model 1, model 1 is assumed to be more appropriate; other-
all bootstrap samples to obtain an overall measure for the pre-      wise the model 2 is assumed to be more appropriate.
                  ¯ of each model. The model that has the
diction error (Err)                                                      Based on the results reported in Schultheis and Singhaniya
          ¯
lowest Err is assumed to be the best approximation to the            (accepted), we employed a variant of the k-Nearest Neighbor
process that actually generated the n original data points.          algorithm (k = 10) for classification. The runtime complexity
   Due to the randomness in generating the bootstrap samples         of the CM is O(NDS ∗ f itCost).
as well as the noise that is likely included in the original ob-
              ¯ only constitutes an estimate of the models’
servations, Err                                                      Simple Hold-Out
true prediction error. Accordingly, the model showing the            This method gauges the to-be-compared models by repeat-
lowest Err¯ may do so because of chance and not because it           edly splitting the set of available n observations into a train-
is the best model. Knowing the variability, that is, the SE, of      ing and test set. For each of these splits, both models are
the error estimates can potentially help alleviating this prob-      fitted to the respective training set. The fitted models are then
lem. Given the standard error, CIs on the true prediction er-        used to generate predictions for the data points in the test set
ror can be derived. If the CIs of the models’ error estimates        and the corresponding prediction error is determined. Ac-
do not overlap, one may conclude with more confidence—               cordingly, using I different splits results in I prediction error
depending on the confidence level employed to construct the          values for each of the two models. The model that has the
intervals—that the model with the lower Err  ¯ in fact provides      lower median prediction error is selected as the more appro-
the better approximation to the process that generated the n         priate model. The runtime complexity of the simple hold-out
original data points.                                                method is O(I ∗ f itCost).
   In our simulations we assess both the bootstrap considering
the SE and the bootstrap not considering the SE for deciding         PED
which of the two models is more appropriate. We construct            Similar to the simple hold-out the PED (van de Wiel,
the CIs by (a) computing the SE as proposed in Efron and             Berkhof, & van Wieringen, 2009) employs I splits of the orig-
Tibshirani (1997), (b) employing a confidence level of 99%,          inal data set into training and test set to compare models. For
and (c) assuming that the prediction error estimates are dis-        both models the prediction error is computed for each point
tributed approximately normal. The runtime complexity of             in the test set after fitting the models to the corresponding
both bootstrap variants is O(B ∗ f itCost), where B is the num-      training set. Subsequently, pairwise differences between pre-
ber of bootstrap samples and f itCost is the time complexity         diction errors for model 1 and model 2 are calculated. These
of estimating model parameters.                                      signed error differences are subjected to signed rank tests to
                                                                     derive the probability of the observed distribution of signed
CM                                                                   ranks under the null hypothesis that the models do not differ
The CM was proposed by Wagenmakers, Ratcliff, Gomez,                 in predictive accuracy.
and Iverson (2004) as a way to assess to what extent two                 Thus, the PED yields I probability values. If the median
models are able to mimic each other’s behavior. Since model          of these values is below or equal to a pre-specified signif-
complexity and the ability to mimic other models are often           icance level α, the models are assumed to be significantly
related, the obtained mimicry information potentially allows         different in their predictive accuracy and the model with the
reducing the bias towards selecting more complex models.             smaller prediction error is assumed to be the more appropri-
   The following steps are involved in the CM: First, for one        ate model. In our simulations we used the Wilcoxon signed
of the models (say, model 1) a certain number, NDS, of sets          rank test with α = 0.05. The runtime complexity of the PED
of parameter values are randomly drawn from the feasible             is O(I ∗ f itCost).
range of the model’s free parameters. Second, model 1 is
used to generate NDS data sets employing each of the NDS             Method Properties
parameter value sets, respectively. Third, both models are fit-      The procedural details of the methods described above imply
ted to each of the NDS data sets yielding NDS measures of            a number of (differences in) crucial properties of the methods
goodness of fit (GOF, e.g., the mean squared error) for both         regarding model comparison.
models. Fourth, the pairwise GOF differences are computed                First, the methods apply different criteria for judging the
for all datasets. By repeating these four steps for the second       suitability of the compared models for a given data set. Both
model (model 2), one obtains two distributions of GOF dif-           bootstrap variants, the PED, and the simple hold-out judge
ferences, one for data generated from model 1 and one for            the models based on their ability to generalize to new data
data generated from model 2.                                         points, that is, these methods attempt to optimize what has
                                                                 1295

been called the generalization criterion (Busemeyer & Wang,                recalls from a binomial distribution assuming a certain num-
2000). In contrast, the CM has been argued to be optimal ”un-              ber learned items (NL). Fourth, this set of numbers of suc-
der the validation criterion of selecting the generating model”            cessful recalls was treated as if it was a set of empirical ob-
(Cohen et al., 2008, p. 698). Since our simulations check the              servations for which to identify the most appropriate model.
methods ability to recover the generating model, they test the             Accordingly, the comparison method in question was applied
conjecture of (Cohen et al., 2008) or, more generally, examine             as described above to the model pair and the set of observa-
to what extent methods employing different criteria perform                tions. Fifth, which (if any) of the two compared models was
(dis)similarly in model recovery.                                          found to be more appropriate was noted. This procedure was
   Second, only the bootstrap without SE and the simple hold-              repeated R = 100 times for each model in each model pair.
out method can straightforwardly be extended to the simulta-               Across all model pairs and methods the measure to assess
neous comparison of more than two models. All other meth-                  model fits and prediction error was always the mean squared
ods are (currently) restricted to comparing pairs of models.               error and the models were fit using a variant of the Metropolis
   Third, the bootstrap with SE and the PED are the only                   algorithm (Madras, 2002).
methods that explicitly take into account the statistical vari-               Following this general procedure, our simulations varied 5
ability and reliability during comparison. This renders these              factors that potentially impact the performance of the com-
methods potentially superior to the other methods, because                 parison methods. Besides allowing to assess the importance
statistically reliable decisions between models can be as-                 of each of these factors for method performance, factor vari-
sumed to be more accurate. On the other hand this property                 ation ensured a more general view on the methods accuracy
comes with the potential disadvantage that no decision may                 in model recovery, that is, a view that is not specific to only
be possible in certain situations1 . Accordingly, the overall              one particular combination of factor levels. The considered
quality of the bootstrap with SE and the PED will depend on                factors are tightness of fit, strength of noise, number of data
the precise tradeoff between how accurately a decision be-                 points, number of samples, and split ration and are described
tween models can be taken and the number of situations in                  in the following.
which a decision is reached.
                                                                           Tightness of fit Fitting a model to a set of observations is a
                            Approach                                       specific instance of a general type of optimization problems:
Three hypothetical models of memory decay, M1, M2, and                     Find the optimal set of parameter values for the given obser-
M3, were used to assess the model comparison methods.                      vations. It is well known that one is rarely guaranteed to find
Each of these models predicts the probability of recall in                 the optimum in such optimization problems. Thus, model
dependance on the time t that has passed since the to-be-                  fits may often be suboptimal to greater or lesser extent. This
remembered items have been learned. The models are defined                 raises the question how susceptible the different comparison
by the following formulas (see Pitt & Myung, 2002):                        methods are to suboptimal model fits. To investigate this, we
                                                                           considered 3 levels of tightness of fits by varying how thor-
             M1 : (1 + t)−a , a ∈ [0, 2]                                   oughly the Metropolis algorithm searches the models’ param-
                                                                           eter space. More precisely, we varied the number of sets of
             M2 : (b + t)−a , a ∈ [0, 2], b ∈ [1, 2]
                                                                           parameters that were sampled (called swaps) for model fit-
             M3 : (1 + bt)−a , a ∈ [0, 2], b ∈ [0, 2]                      ting, using swaps = 100, 1000, and 10000. Simulations look-
                                                                           ing at the rates for recovering the generating model when fit-
Note that M1 is nested in both M2 and M3, but nesting is                   ting to the probabilities directly (i.e., looking at model behav-
different in the two cases. Since, furthermore, M2 and M3                  ior without adding sampling noise) corroborated that these
are not nested, the three models allowed to examine the com-               numbers of swaps realized increasingly accurate model fits.
parison methods regarding their ability to cope with different
types of nesting as well as non-nested models.
                                                                           Strength of noise Since the only noise in the data is sam-
   Each method was applied to all three possible pairs of mod-
                                                                           pling noise, the amount of noise in the data is determined
els, M1 vs. M2, M1 vs. M3, and M2 vs. M3 using the follow-
                                                                           exclusively by the number of learned items: The higher NL
ing general procedure. Given one of the three models, first,
                                                                           is the lower is the influence of sampling noise. Accord-
a set of parameter values was randomly drawn according to a
                                                                           ingly, employing NL = 5, 50, and 1000 allowed to examine
uniform distribution from the range of parameter values spec-
                                                                           the methods’ capability to cope with noisy data.
ified above. Second, probabilities for this set of parameter
values were generated from the model. Third, these probabil-
ities were used to randomly sample the number of successful                Number of data points The information about the process
                                                                           that has generated a set of data can be assumed to increase
    1 Some may also consider this a strong point of the methods, since     with the number of available observations in the data set. To
the methods make explicit if too little information is available for a     what extent the different methods require few or many data
reliable decision. Yet, assuming that modelers often need to take a
decision based on a set of available data, an equivocal comparison         points for performing well was explored by varying the num-
outcome is disadvantageous.                                                ber of data points (NDP). Levels of NDP = 5, 20, and 100
                                                                       1296

were employed and the corresponding data points were gen-
erated for t distributed equidistantly in the range [0.1, 8.1].                                        M1−M2
                                                                                           240
Number of samples All of the methods come with a pa-                                       220
rameter that controls the amount of resources that are in-
                                                                                           200
vested for model comparison. For PED and simple hold-
out this parameter is the number of splits that are considered                             180
                                                                             Performance
(I), for both bootstrap variants this parameter is the number
of bootstrap samples (B), and for the CM this parameter is                                 160
the number of GOF difference samples (NDS) each GOF dif-
                                                                                           140
ference distribution consists of. By using I = 10, 100, 1000,                                                                 AIC
                                                                                                                              BSSE
B = 100, 1000, and NDS = 100, 1000 we gauged the models                                    120                                SHO
resource-performance trade-offs.                                                                                              CM
                                                                                           100                                BS
                                                                                                                              SR
                                                                                                                              PED
Split ratio Application of the PED and the simple hold-out                                  80
                                                                                                 1st    2nd             3rd
requires splitting the set of observations into training and test                                      Quartile
sets and the relative sizes of the two sets is potentially crucial
                                                                                                       M1−M3
for comparison performance. If the training set is too small,                              240
insufficient information about the generating process may be
                                                                                           220
available. If the training set is too large, the danger of over
fitting may arise and the test set may become too small to                                 200
obtain a reliable estimate of generalization performance. In
our simulations we investigated splits with Q = 0.2, 0.4, and                              180
                                                                             Performance
0.6, where Q indicates the fraction of the original observa-
                                                                                           160
tions that are used for the training set.
                                                                                           140
To assess the methods’ ability to outperform less elaborate
approaches to model comparison, our simulations comprise                                   120
the Akaike Information Criterion (AIC, Akaike, 1973) as the
sixth method and a seventh method that we term simple re-                                  100
covery. Following the same general procedure as described
                                                                                            80
above, simple recovery compares models by only consider-                                         1st    2nd             3rd
                                                                                                       Quartile
ing the GOF of each model on the given data set: The model
that provides the tighter fit is assumed to be the more appro-                                         M2−M3
                                                                                           240
priate model. Simple recovery and AIC simulations involve
the same variations of the factors tightness of fit, strength of                           220
noise, and number of data points as employed for the 5 more
                                                                                           200
sophisticated methods.
                                                                                           180
                            Results
                                                                             Performance
To characterize the methods’ performance we computed, for                                  160
each method, model pair, and situation, the sum of the per-                                140
centages of cases in which both (a) a clear decision between
the two models of pair could be taken and (b) the actually                                 120
generating model was correctly recovered. If, for example,
                                                                                           100
for the model pair M1-M2, M1 was correctly recovered 90%
of the time and M2 was correctly recovered 43% of the time,                                 80
                                                                                                 1st    2nd             3rd
the performance measure was computed to be 90 + 43 = 133.                                              Quartile
Similarly, for BSSE and PED the percentages of cases where
no model could be recovered with certainty was computed as
                                                                         Figure 1: Quartiles of performance for the three considered
the sum of the percentages of such cases for each of the two
                                                                         model pairs and the seven considered methods. AIC = Akaike
compared models.2 . From the such obtained values the first,
                                                                         Information Criterion, BSSE = bootstrap with standard error,
   2 Given  this procedure, BSSE and PED sometimes show both             SHO = simple hold-out, CM = cross-fitting method, BS =
high performance and high percentages of situations where no model       bootstrap, SR = simple recovery, PED = PED method.
was recovered. Such a pattern indicates that the method in question
only rarely recovered any model, but if it did, it was accurate
                                                                      1297

second (median), and third quartiles (and associated stan-
dard errors) were determined for each method and model pair                                                                  M1−M2
across all situations. Figure 1 and Figure 2 display the quar-                                                   200
                                                                         # Cases Where No Decision Is Possible
tiles for the different methods.
   As is evident from Figure 1, there are marked performance                                                     150
differences between model pairs and comparison methods.
                                                                                                                             M1−M3
As one may have expected, the nested model pairs generally
                                                                                                                 200
prove more difficult than the non-nested model pair, with M1-
M3 being even more difficult than M1-M2. It is mainly in the
nested pairs that the less elaborate methods, AIC and simple                                                     150
recovery perform worse than all of the 5 more elaborate meth-                                                                M2−M3
ods. Of the 5 more elaborate methods, PED, simple hold-out,                                                      200
and BSSE generally outperform BS and CM. In sum, PED,                                                                                   BSSE
simple hold-out, and BSSE tend to perform best, AIC and                                                                                 PED
simple recovery perform worst, and CM and BS show inter-                                                         150
                                                                                                                       1st    2nd       3rd
mediate performance, but are only better than AIC and simple                                                                 Quartile
recovery for nested model pairs. As Figure 2 shows, the su-
perior performance of PED and BSSE comes at the cost of a
                                                                      Figure 2: Quartiles of the number of cases for which BSSE
substantial number of cases in which the two methods do not
                                                                      and PED do not allow to take a decision.
allow to take a clear decision for one or the other model.
   Several aspects of this pattern of results seem noteworthy.
In contrast to the assumption that the CM is optimal for re-          the nesting model. Put differently, for loose fits, the CM and
covering the generating model (Cohen et al., 2008), the CM            the bootstrap with SE tend to erroneously favor the simpler
performs comparatively bad. On average, the CM is only bet-           model; a problem that is mitigated when using tighter fits.
ter than SR for nested models, and generally worse in avoid-          The remaining three methods are largely insensitive to tight-
ing misclassifications than the PED, BSSE, and the simple             ness of fits indicating that, for these methods, it may not be
hold-out. In fact, given its comparative simplicity, the simple       the absolute but the relative tightness of fit that matters.
hold-out performs remarkably well. While providing a deci-
sion for 100% of the cases, these decision are correct in more
than 90% of the cases on average. This set of results also            Strength of noise Not surprisingly, all methods get con-
provides further evidence for dissimilarity in model recovery         sistently better with decreasing strength of noise. Further-
performance depending on whether a generalization criterion           more, all methods encounter severe difficulties with the high-
or a model recovery criterion is instantiated by the employed         est noise level (NL = 5) that leads to near chance performance
comparison method. Comparing the simple hold-out and CM               for most model pairs and methods. The methods differ, how-
indicates performance differences depending on which crite-           ever, regarding the level of noise from which they start to
rion is used and, more interestingly, that a method using the         show good or very good performance. While the simple hold-
generalization criterion can outperform a method using the            out, the bootstrap with SE and the PED achieve high accuracy
recovery criterion in model recovery.                                 already for NL = 50, the CM and the bootstrap tend to do so
   In addition to the results across all factor combinations,         only for NL = 1000.
considering the impact each factor has on method perfor-
mance yields a number of interesting insights.
                                                                      Number of data points Although all methods but the AIC
                                                                      tend to improve with an increase in the number of data points,
Tightness of fit Across all methods, the influence of the             there are marked differences with respect to the strength of
tightness of fit (if present at all) is only considerable be-         the influence of this factor. The PED and the bootstrap with
tween loose fits (swaps = 100) and moderate to tight fits             SE are impacted severely by the number of data points im-
(swaps = 1000 and 10000). In simple recovery, the tendency            proving considerably – especially for nested models – with
to select the more complex models increases with tightness            an increase from NDP = 5 to NDP = 20 as well as from
of fits such that for moderate and tight fits the nesting model       NDP = 20 to NDP = 100 both regarding accuracy and the
is selected more often even if the nested model generated the         percentage of decision that can be made. The other four meth-
data. Except for pair M1-M3, performance of AIC increases             ods are much less sensitive to NDP levels, but exhibit a ten-
considerably with tighter fits. In comparison, the bootstrap          dency for a reduction in erroneously selecting a nested model
with SE and the CM, exhibit less (but still noticeable) sus-          when the data was generated from a nesting model. Interest-
ceptibility to tightness of fit in the sense that with tighter fits   ingly, the performance of the AIC drops with increasing NDP
for nested model pairs the overall correct recovery rate in-          due to an increased tendency to erroneously pick the nested
creases by selectively increasing the correct recovery rate of        model.
                                                                  1298

Number of samples Effects of increasing the number of                two models need to be compared and / or if it is important to
samples are mixed across the methods. This factor has virtu-         reach a decision on which of the compared models to select.
ally no effect on the bootstrap. Yet, for the bootstrap with SE         Although this initial assessment already highlights impor-
increasing the number of samples leads to a decrease in the          tant properties of the comparison methods, it is best viewed
percentage of cases in which a decision can be made and to           as a first glimpse on the methods’ characteristics. Further
a tendency to more often select the simpler of the two com-          research considering a range of different (types of) models
pared models. Both PED and simple hold-out perform better            is required to provide a more comprehensive picture of the
with increased I, but this trend is largely due to the difference    strengths and weaknesses of available comparison methods.
between I = 10 and I = 100. Similar to the bootstrap with            Besides taking up this task we intend to explore modifica-
SE, the PED allows (slightly) fewer decision with increasing         tions of the CM, PED, and bootstrap with SE that renders
I. The CM exhibits a shift towards more often selecting the          them applicable to comparing more than two models in our
more complex model with increased numbers of samples.                future work.
                                                                                          Acknowledgments
Split ratio The split ratio has only little impact on the per-
                                                                     The authors gratefully acknowledge support by the Ger-
formance of the PED and the simple hold-out. While the
                                                                     man Academic Exchange Service (DAAD) and the Ger-
number of cases that cannot be decided by the PED slightly
                                                                     man Research Foundation (DFG) through the project R1-
increases with an increase in Q, the accuracy remains gener-
                                                                     [ImageSpace], SFB/TR 8 Spatial Cognition
ally high. Only for comparing M1 and M3 do higher values
of Q lead to pronounced performance decrements. Similarly,                                     References
the simple hold-out becomes slightly but consistently worse          Akaike, H. (1973). Information theory and an extension of
in correctly recovering the nested model in the two nested              the maximum likelihood principle. In B. N. Petrov (Ed.),
model pairs with an increase in Q.                                      Proceedings of the Second International Symposium on In-
                                                                        formation Theory (p. 267 - 281).
                         Conclusion                                  Busemeyer, J. R., & Wang, Y.-M. (2000). Model compar-
Our simulation studies revealed a number of interesting prop-           isons and model selections based on generalization crite-
erties of the considered comparison methods. First, methods             rion methodology. Journal of Mathematical Psychology,
employing a generalization criterion for model comparison               44, 171-189.
(e.g., simple hold-out) can outperform methods supposedly            Cohen, A. L., Sanborn, A. N., & Shiffrin, R. M. (2008).
optimal for model recovery (the CM) in model recovery. Sec-             Model evaluation using grouped or individual data. Psy-
ond, although all 5 considered methods can substantially im-            chonomic Bulletin & Review, 15, 692-712.
prove on less elaborate approaches (as instantiated by the AIC       Efron, B., & Tibshirani, R. J. (1993). An introduction to the
and the simple recovery method), the less elaborate methods             bootstrap. New York: Chapman & Hall.
may perform better under certain conditions. Thus, whether           Efron, B., & Tibshirani, R. J. (1997). Improvements on cross-
the use of one of the examined methods is advantageous will             validation: The .632+ bootstrap method. J Am Stat Assoc,
depend on the precise nature of the model comparison situa-             92, 548-560.
tion at hand (e.g., how many data points are available and how       Madras, N. N. (2002). Lectures on monte carlo methods.
noisy the data is). Third, the considered methods differ no-            Providence, Rhode Island: American Mathematical Soci-
ticeably in the degree to which their performance depends on            ety.
the characteristics of the comparison situation. The compara-        Pitt, M. A., & Myung, J. (2002). When a good fit can be bad.
tively low quartiles of the bootstrap and the CM indicates that         TRENDS in Cognitive Sciences, 6, 421-425.
these methods outperform the less elaborate approaches only          Schultheis, H., & Singhaniya, A. (accepted). Decision crite-
in comparatively few particular settings. Fourth, the highest           ria for model comparison using cross-fitting. In 22nd An-
accuracies were achieved by the PED, but this method allows             nual Conference on Behavior Representation in Modeling
decisions about which of the compared models is more ap-                & Simulation (BRiMS 2013).
propriate only in very few cases. Furthermore, performance           Shiffrin, R. M., Lee, M. D., Kim, W., & Wagenmakers, E.-J.
of the PED breaks down if only few data points are available.           (2008). A survey of model evaluation approaches with a tu-
Fifth, despite its comparable simplicity, the simple hold-out           torial on hierarchical bayesian methods. Cognitive Science,
method achieves high accuracies while allowing to select one            32, 1248-1284.
of the models in 100% of all cases. In addition, the simple          van de Wiel, M. A., Berkhof, J., & van Wieringen, W. N.
hold-out is the only method that can be easily extended to              (2009). Testing the prediction error difference between two
comparing more than two models.                                         predictors. Biostatistics, 10, 550 - 560.
   Against this background our results suggest to employ the         Wagenmakers, E.-J., Ratcliff, R., Gomez, P., & Iverson, G. J.
PED if only pairs of models have to be compared and if ac-              (2004). Assessing model mimicry using the parametric
curacy is more important than being able to reach a decision.           bootstrap. Journal of Mathematical Psychology, 48, 28-
The simple hold-out appears to be a good choice if more than            50.
                                                                 1299

