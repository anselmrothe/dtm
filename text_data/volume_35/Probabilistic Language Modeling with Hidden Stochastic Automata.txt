UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Probabilistic Language Modeling with Hidden Stochastic Automata
Permalink
https://escholarship.org/uc/item/8mg8t7jm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Author
Andrews, Mark
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

    Probabilistic Language Modeling with Hidden Stochastic Automata
                                                       Mark Andrews
                                                     Division of Psychology
                                                 Nottingham-Trent University
                                                               and
                                        Division of Psychology and Language Sciences
                                                   University College London
                           Abstract                                man, 2004), computer vision (e.g., Oliver, Rosario, &
   In this paper, we introduce a novel dynamical Bayesian          Pentland, 2000), machine learning (e.g., Bishop, 2006,
   network model for probabilistic language modeling. We           2013), expert systems (e.g., Lauritzen & Spiegelhalter,
   refer to this as the Hidden Stochastic Automaton. This          1988; Pearl, 1988), information retrieval (e.g., Salakhut-
   model, while based on a generalization of the Hid-
   den Markov model, has qualitatively greater generative          dinov & Hinton, 2009), and in cognitive science (see,
   power than either the Hidden Markov model itself or             e.g., Chater & Oaksford, 2008; Griffiths, Chater, Kemp,
   any of its existing variants and generalizations. This al-      Perfors, & Tenenbaum, 2010, for overviews).
   lows the Hidden Stochastic Automaton to be used as a
   probabilistic model of natural languages in a way that is          Despite their breadth of appeal, graphical models have
   not possible with existing dynamical Bayesian networks.         had a rather limited role as language models, if by lan-
   Its relevance to Cognitive Science is primarily as a com-       guage models we specifically mean generative models of
   putational — in the Marr (1982) sense of the term —
   model of cognition, but potentially also as a model of          language. There are at least two important reasons for
   resource bounded cognitive processing, and as a model           this. On the one hand, stochastic grammars can not, in
   of the implementation of computation in physical dy-            general, be represented as graphical models. (In some
   namical systems.
                                                                   cases, notably stochastic regular grammars, the termi-
   A probabilistic language model is a hypothetical gen-           nal and nonterminal variables of the grammar can be
erative model of a language, where a language is defined           identified with vertices of a directed Markovian graph.
most generally as a set of strings concatenated out of             For the super-regular grammars, however, this is not
a finite set of symbols. By far the most widely used               the case and the variables of the grammar can not be
formalisms for specifying probabilistic language mod-              identified with the vertices of any fixed graph). On the
els are stochastic grammars, which are symbol rewrit-              other hand, the most widely used graphical models for
ing rules with accompanying probabilities. The use of              sequential probabilisitic modeling, including the Hidden
grammars is motivated by the fact that human languages             Markov model and its extensions, are limited in their
are structurally complex, with properties that place               generative power to the regular languages (i.e. the Type-
them between the so-called context-free and context-               3 languages in the Chomsky hierarchy). In other words,
sensitive formal languages (see, e.g., Chomsky, 1956,              graphical models have had a relatively limited role as
1963; Shieber, 1985), and formal grammars are compu-               language models because the most widely used proba-
tationally universal in the sense that they can generate           bilistic models that have sufficient generative power to
any recursively enumerable set (see, e.g., Hopcroft, Mot-          model human languages can not be represented as graph-
wani, & Ullman, 2001).                                             ical models, and the most widely used graphical models
   By contrast to the case of language modeling, in                for sequential structures do not have sufficient generative
probabilistic modeling more generally, the most widely             power to model natural languages.
used formalism for specifying probabilistic models is the             There is, however, no inherent limitation to the gen-
graphical model (see, e.g., Koller & Friedman, 2009; Jor-          erative power of graphical models. In this paper, we
dan, 2004). Graphical models are directed or undirected            introduce a graphical model, specifically a dynamical
graphs whose vertices are identified with random vari-             Bayesian network, whose generative power is equivalent
ables and whose edges indicate conditional dependen-               to that of an arbitrary stochastic grammar. This model,
cies. The appeal of graphical models is their flexibility          that we will refer to as the Hidden Stochastic Automa-
to represent complex relationships between large num-              ton, is based on a novel generalization of the widely used
bers of variables, and their graph-theoretic properties            Hidden Markov model. As such, it retains many of the
that afford general and computationally efficient algo-            appealing characteristics of the Hidden Markov model
rithms for probabilistic inference, whether exactly or             while extending its generative power.
approximately by, for example, Monte Carlo methods.
As a result, graphical models have effectively become                        Hidden Stochastic Automata
a graph-based modeling language for developing and ex-             To introduce the Hidden Stochastic Automaton (HSA),
tending probabilistic models. They have had widespread             it is necessary to first briefly describe the Hidden Markov
application in fields such as bioinformatics (e.g., Fried-         model (HMM). Given a set of J independent discrete
                                                              1750

valued sequences w1 , w2 . . . wj . . . wJ , where the jth              be done by replacing the single valued xji in the HMM
sequence is wj = wj1 , wj2 . . . wji . . . wjnj , the genera-           by a variable sized array or vector. In other words, while
tive model assumed by the HMM treats each wji as                        in the HMM, each state variable is xji ∈ {1 . . . K}, this
drawn from one of K discrete probability distributions                  may be generalized to xji ∈ {1 . . . K}∗ . Here ∗ indicates
φ1 , φ2 . . . φk . . . φK over a finite vocabulary of length V .        Kleene star, or the union of all concatenations of the el-
Which distribution is chosen for wji is determined by                   ements from {1 . . . K} and {∅}. This change clearly in-
the value of the unobserved variable xji ∈ {1 . . . K} that             creases the cardinality of the state space to a countably
corresponds to wji . For all j, each xj1 , xj2 . . . xji . . . xjnj     infinite set. Importantly, however, as we will elaborate,
is a first-order Markov chain, with initial distribution π              if the set of operations that can increase or decrease the
and a K × K transition matrix θ. More formally, the                     state-vector are limited to a finite set, and if the the con-
HMM assumes that for all j,                                             ditional dependencies on this state-vector are limited to
                                                                        a finite range of elements, then inference in this general-
     wji |xji , φ ∼ Categorical(wji |φxji )         1 ≤ i ≤ nj ,
                                                                        ized model is almost identical in kind to inference in the
           xji |π ∼ Categorical(xji |π)                     i = 1,      standard HMM.
   xji |xji−1 , θ ∼ Categorical(xji |θxji−1 )       1 < i ≤ nj .
                                                                           For reasons that will be made clear, we will collectively
The graphical model for the HMM is shown below.                         refer to generalizations of the HMM using a state-vector
                                                                        as Hidden Stochastic Automata (HSA). For the purposes
                                  φ                                     of this paper, however, we will mostly concentrate on one
                                                                        specific form of the HSA. For simplicity, we will also refer
                                                                        to this particular case of the model as the HSA, with the
            wj1      wj2   ···   wji     ···      wjn
                                                     j                  understanding that it is but one of many variants based
                                                                        on the same principles.
                                                                           Just as with the HMM, the HSA is a generative
            xj1      xj2         xji              xjn
                           ···           ···         j
                                                                        model of discrete valued sequences. It assumes that
                                              j ∈ {1. . .J}
                                                                        each variable wji in the sequence of observations wj =
                                                                        wj1 , wj2 . . . wji . . . wjnj is drawn from one of (H + 1) × K
                                                                        discrete probability distributions φ01 , φ02 . . . φhk . . . φHK
             π                       θ                                  over a length V vocabulary. Which of these (H + 1) × K
                                                                        distributions is chosen is determined by the values of two
   Figure 1: The graphical model or dynamical                           latent or unobserved state variables that correspond to
   Bayesian network for the Hidden Markov model.                        wji . On the one hand, there is a standard finite state
   The shaded nodes indicate the observed variables.                    variable xji ∈ {1 . . . K}. On the other hand, there is
   For simplicity, we have omitted the priors on φ, π                   an additional state-vector variable zji ∈ {1 . . . H}∗ , with
   and θ.                                                               wji being conditional on only the first element of zji , if
                                                                        zji 6= ∅. In other words, wji is sampled from φ[zji              1 ,x ] ,
                                                                                                                                             ji
                                                                                   1
   Precisely because graphical models naturally afford                  where zji       indicates the value of the first element of the
generalizations and extensions, the HMM has lead to                     state-vector zji , or else 0 when zji = ∅.
many variants. Most notably, these include the mixed
memory HMM (Saul & Jordan, 1999), the coupled HMM                          For all j, the sequence (xj1 , zji ), (xj2 , zji ) . . .
(Brand, Oliver, & Pentland, 1997), the factorial HMM                    (xji , zji ) . . . (xjnj , zji ) is a first-order Markov chain
(Ghahramani & Jordan, 1997), and the hierarchical                       of coupled state variables. The distribution over xj1
HMM (Fine, Singer, & Tishby, 1998). These extensions                    is given by the K valued distribution π, and the
are often based on introducing additional chains of la-                 value of zj1 is deterministically set to zj1 = ∅. For
tent variables with varying degrees of conditional inde-                1 < i ≤ nj , both xji and zji are conditional on xji−1
pendence between them. Despite the evident value of                     and, if zji 6= ∅, the first element of zji . The value of
these models, they do not qualitatively alter the formal                xji is determined by sampling from the K dimensional
generative complexity of the underlying model. In all of                probability distribution specified by θ[zji      1
                                                                                                                           −1 ,xji−1 ]
                                                                                                                                       , where
these extensions, the sequences generated are equivalent                θ is a (H + 1) × K × K stochastic transition matrix,
                                                                                 1
to regular or Type-3 formal languages                                   and zji−1       is as above. The value of zji is determined
                                                                        by applying one of H + 1 different operations to
From HMM’s to Hidden Stochastic                                         zji−1 , specifically prepending zji−1 by one symbol from
Automata                                                                {1 . . . H} or removing the first element from zji−1 . For
It is possible, however, to generalize the HMM in such a                example, if σ1 σ2 σ3 (with each σl ∈ {1 . . . H}) is the
way that its generative complexity is increased. This can               value of the state-vector zji−1 , a possible sequence of
                                                                    1751

operations and their effect on the state-vector could be                         Generative Power of Hidden Stochastic
                                 remove                                          Automata
           zji−1 = σ1 σ2 σ3 −−−−→ zji = σ2 σ3 ,
                                                                                 The generative power of the HSA model (as shown in
                                 prepend 3
                zji = σ2 σ3 −−−−−−→ zji+1 = 3σ2 σ3 ,                             Figure 2) relative to that of the standard HMM (as
                                 prepend 2                                       shown in Figure 1) arises from the fact that the state-
            zji+1 = 3σ2 σ3 −−−−−−→ zji+2 = 23σ2 σ3 .                             space of the state-vector zji , namely {1 . . . H}∗ , is a
                                                                                 countably infinite set yet the conditional relationships to
Which of these H + 1 operations is applied is deter-                             and from zji are finitely specifiable. The consequences
mined by sampling from the H + 1 dimensional prob-                               of this can be better appreciated by reference to discrete
                                                1
ability distribution specified by Ω[zji−1            , xji−1 ], where Ω          automata of the kind that form the foundations of theo-
is a (H + 1) × K × (H + 1) stochastic transition matrix.                         retical computer science (see, e.g., Hopcroft et al., 2001).
   More formally, the probabilistic generative model de-                            As we have described it, the state-vector zji is iden-
fined by this HSA is, for i ≤ i ≤ nj ,                                           tical to a pushdown stack with a symbol set {1 . . . H}.
                                                                                 Prepending an element to the state-vector is equivalent
          wji |xji , zji , φ ∼ Categorical(wji |φ[zji   1 ,x ] ),
                                                              ji
                                                                                 to a push operation, while removing the first element is
                                                                                 a pop operation. Assuming known values for Ω, which
and for i = 1,
                                                                                 operation is applied to zji is dependent only on the value
             xji |π ∼ Categorical(xji |π),          zji = ∅,                     of the finite state variable xji−1 and the first element or
                                                                                 head of zji−1 . Likewise, assuming known values for θ,
and for 1 < i ≤ nj ,                                                             the value taken by xji is also dependent only on xji−1
                                                                                 and the head of zji−1 . In other words, the HSA model
      xji |xji−1 , zji−1 , θ ∼ Categorical(xji |θ[zji   1
                                                          −1 ,xji−1 ]
                                                                      ),         described above is equivalent to a stochastic generative
                                                                                 version of a pushdown stack automaton.
         zji |uji−1 , zji−1 = O[uji−1 ] (zji−1 ),
                                                                                    If we allow a greater variety of operations on the state-
   uji−1 |xji−1 , zji−1 , Ω ∼ Categorical(uji−1 |Ω[zji     1
                                                              −1 ,xji−1 ]
                                                                          ).     vector than just prepending or removing symbols from
                                                                                 the left, the computational power of the HSA can be
Here, we use the auxilary variable uji to refer to the                           beyond that of a generative pushdown stack automaton.
operation applied to zji , and O is the set of (H + 1)                           For example, if
functions that map zji to zji+1 when these operations are                                                  σ1 σ2 σ̇3 σ4 σ5 σ6
applied. In other words, this makes clear that the value
of zji+1 is a deterministic function of zji when the value of                    is the value of the state-vector, we may treat an arbi-
uji is known, but this value is stochastically conditional                       trary element — in this cases σ3 — as its head. If we
on xji and zji . In terms of the original variables, the                         allow for the appending of new elements to the left or
graphical model for the HSA is as follows:                                       the right of the head, or for the deleting of the element
                                                                                 at the head, followed by the moving of the head pointer
                                         φ                                       to the left or right, then this state-vector is equivalent
                                                                                 to a two-way memory tape. As before, assuming known
                                                                                 values for Ω, which of the operations is applied to the
       wj1        wj2        ···        wji      ···             wjn
                                                                    j
                                                                                 state-vector zji is again dependent only on the value of
                                                                                 the finite state variable xji−1 and the head of zji−1 . Like-
                                                                                 wise, as before, assuming known values for θ, the value
                                                                                 taken by xji is also dependent only on xji−1 and the head
       xj1        xj2                   xji                      xjn
                             ···                 ···                j            element of zji−1 . As such, with these changes the HSA is
                                                                                 now equivalent to a stochastic generative version of the
                                                                                 Turing machine.
       zj1        zj2        ···        zji      ···             zjnj
                                                                                 Inference
                                                            j ∈ {1. . .J}
                                                                                 As is clear from Figure 2, only the variables w = {wj1 . . .
                                                                                 wji . . . wjnj }Jj=1 are observed. In general, therefore, the
        π          Ω                              θ                              problem of inference in the HSA is the problem of infer-
                                                                                 ring the joint posterior
   Figure 2: The graphical model or dynamical Bayesian                                             P(θ, φ, Ω, π, x, z|w, α, β, γ, ν),
   network for the Hidden Stochastic Automaton. As
   with Figure 1, shaded nodes indicate observed vari-                           where x and z are the set of finite state and state-vectors
   ables and we have omitted the priors on φ, π, θ and                           variables, and α, β, γ, ν are the Dirichlet priors for θ, φ,
   Ω.                                                                            Ω, π, respectively.
                                                                             1752

   The procedure for inference that we will follow is to                    takes the value of 1 is k− = k = k+ and takes the value
use a collapsed Gibbs sampler to draw samples from the                      of zero otherwise. Likewise, δk− ,k takes the value of 1
posterior                                                                   when k− = k, and takes the value of 0 otherwise. The
                         P(x, z|w, α, β, γ, ν),                             terms a, b and c are the sums of α, β, γ, respectively.
                                                                               For the case of the posterior distribution of the state-
that integrates over the values of θ, φ, Ω, π. This Gibbs
                                                                            vector, it is sufficient to infer the distribution over op-
sampler is identical in nature to the collapsed sampler
                                                                            erations applied to it. As mentioned, the value of the
used in Andrews and Vigliocco (2010) for the case of a
                                                                            state-vector zji is deterministic function of zji−1 when
hierarchical mixture of Hidden Markov models.
                                                                            the operation uji−1 is known. The posterior distribution
   For all j ∈ {1 . . . J} and i ∈ {1 . . . nj }, the Gibbs sam-
                                                                            over uji is given by
pler iteratively draws samples from the posterior over
xji and over zji , conditioned on sampled values for all
                                                                               P(uji |wji , zji , x¬ji , w¬ji , z¬ji , α, β, γ) ∝
remaining variables.                                                                "Z
   The posterior distribution over xji , conditioned on
                                                                                ×         P(wji+1 . . . wjnj |xji+1 . . . xjnj , zji+1 . . . zjnj , φ)
known values for all the other variables is1
                                                                                                                             #
P(xji |wji , zji , x¬ji , w¬ji , z¬ji , α, β, γ) ∝
 Z                                                                                       P(φ|w¬ji  ~ , x¬ji~ , z¬ji ~ , β)dφ
    P(wji |xji , zji , φ)P(φ|w¬ji , x¬ji , z¬ji , β)dφ ×                            "Z
 Z
    P(zji+1 |xji , zji , Ω)P(Ω|x¬ji , z¬ji , γ)dΩ ×                             ×         P(xji+1 . . . xjnj |xji . . . xjnj−1 , zji . . . zjnj−1 , θ)
Z                                                                                                                      #
   P(xji+1 |xji , zji , θ)P(xji |xji−1 , zji−1 , θ)P(θ|x¬ji , z¬ji , α)dθ.               P(θ|x¬ji ~ , z¬ji
                                                                                                         ~ , β)dθ
This leads to the following closed form:                                        × P(zji+1 . . . zjnj |uji , zji )
                                                                                    Z
 P(xji = k|wji , zji , x¬ji , w¬ji , z¬ji , α, β, γ)                            × P(uji |xji , zji , Ω)P(Ω|x¬ji           ~ , z¬ji
                                                                                                                                 ~ , γ)dΩ,
                     ¬ji
                   Shkv   + βv      Q¬ji
                                       hkq + γq
              ∝       ¬ji
                                 ×                                          where we see that because a change to the operation uji
                    Shk·   +b        Q¬ji
                                        hk· + c
                      ¬ji
                                                                            deterministically changes the values of zji+1 . . . zjnj , the
                   (Rhkk   +
                              + δk− ,k,k+ + αk+ )(Rh¬ji − k− k
                                                               + αk )       likelihood terms for the uji variable include the variables
              ×                                                       .
                                     ¬ji
                                  Rhk·   + δk− ,k + a                       wji+1 . . . wji+1 and xji+1 . . . xji+1 2 . In the above, the nota-
                                                                            tion ¬ji, ~ e.g., in x ~ , indicates the exclusion of variables
                                                                                                      ¬ji
Here, we are assuming that the value of the observed                        ji . . . jnj . This distribution leads to the closed form
variable at ji is v, the value of head of the state-vector
at ji is h, its value at ji−1 is h− , the value of the finite                         P(uji = q|wji , zji , x¬ji , w¬ji , z¬ji , α, β, γ) ∝
state variable at ji−1 is k− and its value at ji+1 is k+ .                            Q                     QShkv q
                                                                                                                       −1 ¬ji ~
The S, Q and R are rank-3 arrays of frequencies, with                                             q
                                                                                         {hkv : Shkv  >0}       s=0       Shkv + βv + s
                                                                                                                   q                          ×
the superscript of ¬ji indicating that they are based on                                 Q
                                                                                                    q
                                                                                                            QShk·      −1 ¬ji ~
                                                                                                                          Shk· + b + s
                                                  ¬ji                                       {hk : Shk· >0}      s=0
excluding variables at ji. As such, Shkv              is the number
                                                                                                            QQqhkq −1 ¬ji      ~
of times the observed variable has a value of v when
                                                                                      Q
                                                                                                   q
                                                                                         {hkq : Qhkq >0}        s=0       Qhkq + γq + s
the finite state variables has the value k and the head                                 Q                    QQqhk· −1 ¬ji     ~
                                                                                                                                              ×
(e.g., the first) element of state-vector takes the value of                                        q
                                                                                           {hk : Qhk· >0}        s=0      Q  hk· +   c +  s
                       ¬ji
h ∈ {0 . . . H}, Qhkq        is the number of times that state-                       Q                    QRhkl  q
                                                                                                                       −1 ¬ji~
vector operation q occurs whenever the head element                                               q
                                                                                         {hkl : Rhkl  >0}      s=0       Rhkl + αl + s
                                                                                                            QRhk· q
                                                                                                                       −1 ¬ji ~
                                                                                                                                            .
of the state-vector takes the value of k and the finite                                 Q
                                                                                                  q                       R      + a  + s
                                                      ¬ji                                 {hk : R     >0}      s=0          hk·
state variable takes the value of k, and Rhkk             +
                                                             gives the                            hk·
number of times the finite state variable takes the value                                 ~
                                                                                        ¬ji        ~                 ~
k+ whenever its value at the previous index is k and                        Here, Shkv       , Q¬  ji             ¬ji
                                                                                                 hkq and Rhkl have the same meaning as
                                                                                ~
                                                                              ¬ji         ~             ~
the value of the head of the state-vector at the previous                   Shkv   , Q¬   ji           ¬ji
                                                                                        hkq and Rhkl with the difference being that the
index is h. The dot in place of the third index, e.g.,                      frequencies are calculated excluding variables at the in-
  ¬ji                                                                                                                                    q
Shk·  , indicates a sum over the index. The term δk− ,k,k+                  dices ij . . . jnj . By contrast, the arrays Shkv                 , Qqhkq and
                                                                              q
   1                                                                        Rhkl     are the frequencies of the co-occurrences the values
     We will provide the conditional distributions for values
of xji and zji where 1 < i < nj . The distributions for the
                                                                                2
cases where i = 1 and i = nj require minor modifications,                         In graphical model terms, the variables wji+1 . . . wji+1 ,
which we will omit here in the interests in space.                          xji+1 . . . xji+1 and zji+1 . . . zjnj are all children of uji .
                                                                        1753

Figure 3: Strings generated by the probabilistic context-free grammar S → 0S1 (p = 0.66), S → 01 (p = 0.34) were
used as observed data in a HSA. Shown above are samples of the binary strings generated by the HSA model on
the basis of estimates of the parameters φ, θ, Ω and π after 3, 5, 10, 20, 50 and 100 iterations of the Gibbs sampler.
The dark shade codes the value of 1. It is evident that by over 50 iterations, the HSA has inferred the correct
generative model of the probabilistic language. By 100 iterations, it is only generating strings from the language
L = {0n 1n : n ≥ 0}.
of the variables after operation q is applied to the state-     models, we have argued, have effectively become a graph-
vector zji+1 and the changes to the subsequent state-           based modeling language for developing and extending
vectors are deterministically applied.                          probabilistic models. They have had a remarkable in-
                                                                fluence on the progress of probabilistic modeling in a
Demonstration                                                   wide variety of fields, including cognitive science. It is
We demonstrate inference of a language from data by             notable, therefore, that graphical models have had a rel-
using the textbook example of a simple context-free lan-        atively limited role in the probabilistic modeling of nat-
guage, namely L = {0n 1n : n ≥ 0}. We can generate              ural language. The obvious reason for this is due to the
strings from a probabilistic version of this language us-       structurally complex nature of natural languages. While
ing the probabilistic context-free grammar                      this structure is modeled well by probabilistic grammars,
                                                                grammars can not, in general, be represented by graph-
                  S → 0S1,     p = 0.66,                        ical models. By contrast, the graphical models most
                     → 01,     p = 0.34.                        widely used for modeling sequential data do not have
                                                                the structural complexity necessary for modeling natu-
We sample J = 25 strings from this language and use             ral language.
them as the data w = w1 , w2 . . . wj . . . wJ for a HSA
model of the kind described.                                       We have introduced the HSA as a dynamical Bayesian
   Using the collapsed Gibbs sampler, we can sample             network model that is capable of modeling structurally
from the posterior over the finite state and state-vector       complex sequences. Its principal relevance to cognitive
trajectories conditional on w. From these, we may then          science is therefore as a computational model of cogni-
draw sample estimates of φ, θ, Ω and π. Shown in Fig-           tion, where by computational model we specifically mean
ure 3 are strings generated by the HSA model with pa-           the Marr (1982) sense of the term: a model of the ab-
rameters φ, θ, Ω and π as estimated after, from left to         stract nature of problem being faced and of its ratio-
right, 3, 5, 10, 20, 50 and 100 iterations of the Gibbs         nal solution. However, the HSA model is potentially
sampler.                                                        as relevant as a model of the resource limited practice,
                                                                or possibly even the physical implementation, of cogni-
       Relevance for Cognitive Science                          tion. For example, the HSA is an incremental state-
Our initial motivation for the HSA model was put in             space model, where inference is naturally modeled by
terms of the computational advantages of graphical mod-         the kind of sequential Monte Carlo methods, particu-
els as formalisms for probabilistic modeling. Graphical         larly particle filters, that have been advocated by, for
                                                            1754

example, Griffiths, Vul, and Sanborn (2012); Sanborn,                       els of Cognition: Exploring Representations and
Griffiths, and Navarro (2010); Levy, Reali, and Griffiths                   Inductive Biases. Trends in Cognitive Sciences,
(2009) as models of memory and time constrained ap-                         14 (8), 357-364.
proximations to rational computational models. On the                Griffiths, T. L., Vul, E., & Sanborn, A. N. (2012).
other hand, from the point of view of physical implemen-                    Bridging Levels of Analysis for Probabilistic Mod-
tation, the state-vector of the HSA can be represented                      els of Cognition. Current Directions in Psycholog-
naturally by a real-valued variable. If the state-vector is                 ical Science, 21 (4), 26-268.
σ1 σ2 . . . σi . . P
                   . σn , this can be represented exactly by the     Hopcroft, J., Motwani, R., & Ullman, J. (2001). Intro-
                      n
real number i=1 σi (H+1)−i and the operations applied                       duction to automata theory, languages and compu-
to the state vector correspond to real-valued functions.                    tation (2nd ed.). Addison Wesley.
For example, if the state-vector is binary, prepending a             Jordan, M. I. (2004). Graphical models. Statistical
σ ∈ {0, 1} to σ1 σ2 . . . σi . . . σn is identical to multiplying           Science, 19 (1), 140-155.
   n         −i
                  by 12 and adding σ2 . By treating the finite
P
   i=1 σi 2                                                          Koller, D., & Friedman, N. (2009). Probabilistic graphi-
state variable as another real number, this allows us to                    cal models: Principles and techniques. Cambridge,
represent the HSA exactly as a stochastic nonlinear dy-                     MA: MIT Press.
namical system that is directly comparable to a recur-               Lauritzen, S., & Spiegelhalter, D. (1988). Local Com-
rent neural network (see, e.g., Tabor, 2000, for related                    putations With Probabilities On Graphical Struc-
discussion).                                                                tures And Their Application To Expert Systems.
                                                                            Journal Of The Royal Statistical Society Series B-
                             References                                     Methodological , 50 (2), 157-224.
                                                                     Levy, R., Reali, F., & Griffiths, T. L. (2009). Modeling
Andrews, M., & Vigliocco, G. (2010). The Hidden
                                                                            the Effects of Memory on Human Online Sentence
       Markov Topics Model: A Probabilistic Model of
                                                                            Processing with Particle Filters. In Advances in
       Semantic Representation. Topics in Cognitive Sci-
                                                                            Neural Information Processing Systems (Vol. 21,
       ence, 2 , 101-113.
                                                                            p. 937-944).
Bishop, C. M. (2006). Pattern Recognition and Machine
                                                                     Marr, D. (1982). Vision. New York, NY: W. H. Freeman
       Learning. New York, NY: Springer.
                                                                            & Company.
Bishop, C. M. (2013, Feb 13). Model-Based Machine                    Oliver, N., Rosario, B., & Pentland, A. (2000, Aug).
       Learning. Philosophical Transactions of the Royal                    A Bayesian Computer Vision System for Model-
       Society A - Mathematical Physical and Engineer-                      ing Human Interactions. IEEE Transactions on
       ing Sciences, 371 (1984).                                            Pattern Analysis and Machine Intelligence, 22 (8),
Brand, M., Oliver, N., & Pentland, A. (1997). Cou-                          831-843.
       pled Hidden Markov Models for Complex Action                  Pearl, J. (1988). Probabilistic reasoning in intelligent
       Recognition. In 1997 IEEE Computer Society Con-                      systems: Networks of plausible inference. Morgan
       ference On Computer Vision And Pattern Recog-                        Kaufmann.
       nition, Proceedings (p. 994-999).                             Salakhutdinov, R., & Hinton, G. (2009, Jul). Seman-
Chater, N., & Oaksford, M. (2008). The Probabilistic                        tic Hashing. International Journal of Approximate
       Mind. Oxford, UK: Oxford University Press.                           Reasoning, 50 (7), 969-978.
Chomsky, N. (1956). Three models for the description                 Sanborn, A. N., Griffiths, T. L., & Navarro, D. J. (2010).
       of language. Institute of Radio Engineers Transac-                   Rational approximations to rational models: Al-
       tions on Information Theory, 2 , 113-124.                            ternative algorithms for category learning. Psy-
Chomsky, N. (1963). Formal properties of grammars. In                       chological Review , 117 , 1144-1167.
       R. D. Luce, R. R. Bush, & E. Galanter (Eds.),                 Saul, L., & Jordan, M. (1999, Oct). Mixed Memory
       Handbook of mathematical psychology (Vol. 2,                         Markov Models: Decomposing Complex Stochastic
       p. 323-418). New York and London: John Wiley                         Processes as Mixtures of Simpler Ones. Machine
       and Sons, Inc.                                                       Learning, 37 (1), 75-87.
Fine, S., Singer, Y., & Tishby, N. (1998). The Hierar-               Shieber, S. M. (1985). Evidence Against the Context-
       chical Hidden Markov Model: Analysis and Appli-                      Freeness of Natural-Language. Linguistics And
       cations. Machine Learning, 32 (1), 41–62.                            Philosophy, 8 (3), 333-343.
Friedman, N. (2004, Feb 6). Inferring Cellular Net-                  Tabor, W. (2000). Fractal encoding of context-free gram-
       works using Probabilistic Graphical Models. Sci-                     mars in connectionist networks. Expert Systems,
       ence, 303 (5659), 799-805.                                           17 (1), 41-56.
Ghahramani, Z., & Jordan, M. (1997). Factorial Hidden
       Markov Models. Machine Learning, 29 , 245-273.
Griffiths, T. L., Chater, N., Kemp, C., Perfors, A., &
       Tenenbaum, J. B. (2010, Aug). Probabilistic Mod-
                                                                 1755

