UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Spontaneous Analogy by Piggybacking on a Perceptual System
Permalink
https://escholarship.org/uc/item/6mj558xs
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Pickett, Marc
Aha, David
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                  Spontaneous Analogy by Piggybacking on a Perceptual System
                                   Marc Pickett                                                 David W. Aha
                         NRC/NRL Postdoctoral Fellow                   Navy Center for Applied Research in Artificial Intelligence
                              Washington, DC 20375                   Naval Research Laboratory (Code 5510); Washington, DC 20375
                      marc.pickett.ctr@nrl.navy.mil                                        david.aha@nrl.navy.mil
                                Abstract                                          Source
   Most computational models of analogy assume they are given a
   delineated source domain and often a specified target domain.
   These systems do not address how analogs can be isolated from
   large domains and spontaneously retrieved from long-term
   memory, a process we call spontaneous analogy. We present a
   system that represents relational structures as feature bags. Us-
   ing this representation, our system leverages perceptual algo-
   rithms to automatically create an ontology of relational struc-
   tures and to efficiently retrieve analogs for new relational struc-
   tures from long-term memory. We provide a demonstration of
   our approach that takes a set of unsegmented stories, constructs
                                                                                   Target
   an ontology of analogical schemas (corresponding to plot de-                                                  Pterodactyls!    Canyon
   vices), and uses this ontology to efficiently find analogs within
   new stories, yielding significant time-savings over linear ana-                     (a) Mapping                (b) Spontaneous Retrieval
   log retrieval at a small accuracy cost.
                                                                            Figure 1: An analog of Analogical Mapping vs. Sponta-
                 1 Spontaneous Analogy                                      neous Analogy. In Analogical Mapping (a), we are given an
In our day-to-day experience, we often generate analogies                   explicit source and target, free from interfering context. In
spontaneously (Wharton, Holyoak, & Lange, 1996; Clement,                    spontaneous analogy (b), the analogs are spontaneously re-
1987). That is, with no explicit prodding, we conjure up                    trieved from long-term memory.
analogs to aspects of our current situation. For example, while
reading a story, we may recognize a plot device that is anal-
                                                                               The process of spontaneous analogy shares some proper-
ogous to one used in another story that we read long ago.
                                                                            ties with low-level perception, as exemplified in Figure 1.
The shared plot device may be a small part of each story, it
                                                                            Within seconds of being presented with a visual image of a
is usually not explicitly delineated for us or presented in iso-
                                                                            pterodactyl flying over a canyon, one can typically describe
lation from the rest of the story, and we may recognize the
                                                                            the image using the word “pterodactyl”, even if one has had
analogy of the plot device even if the general plots of the two
                                                                            no special explicit recent priming for this concept, indeed
stories are not analogous. Somehow, we segment out the plot
                                                                            even if one has not consciously thought about pterodactyls for
device and retrieve the analog1 from another story in long-
                                                                            several years. For us to produce the word “pterodactyl”, we
dormant memory. Spontaneous analogy is the process of ef-
                                                                            must segment the pterodactyl from the canyon and retrieve the
ficiently retrieving an analog from long-term memory given
                                                                            “pterodactyl” concept from the thousands of concepts stored
an unsegmented source domain such that part of the source
                                                                            in memory. We must have learned the “pterodactyl” concept
shares structural similarity with the analog, though they might
                                                                            to begin with from unsegmented images. This perceptual pro-
not share surface similarity. This process differs from stan-
                                                                            cess is robust to noise: The pterodactyl in the image could be
dard models of analogy, which are given a delineated source
                                                                            partially occluded, ill-lit, oddly colored, or even drawn as a
concept, and often a target concept. Given a pair of analogs,
                                                                            cartoon, and we are still able to correctly identify this shape
analogical mapping is relatively straightforward. The more
                                                                            (to a certain point). Likewise, many details of the plot devices
difficult problem is finding the analogs to begin with. As
                                                                            from the above story example could be altered or obfuscated,
Chalmers, French, and Hofstadter (1992) argue “when the
                                                                            but this analogy would degrade gracefully.
program’s discovery of the correspondences between the two
                                                                               Our primary technical contribution in this paper is an algo-
situations is a direct result of its being explicitly given the
                                                                            rithm called Spontol2 that solves the problem of spontaneous
appropriate structures to work with, its victory in finding the
                                                                            analogy: efficient parsing, storage, and retrieval of analogs
analogy becomes somewhat hollow”.
                                                                            from long-term memory. That is, given a corpus of many large
    1 In our terminology, an analog is substructure of a domain that        unsegmented relational structures, Spontol discovers analog-
is structurally similar to a substructure of another domain, and an         ical schemas that are useful for characterizing the corpus and
analogical schema is a generalization of an analog. For example,
an input domain might be the entire story of Romeo & Juliet, an             efficiently retrieves analogs given a new structure. E.g., given
analog would be the part of the story where Romeo kills Tybalt,             a set of narratives in predicate form, Spontol discovers plot
who killed Romeo’s friend, Mercutio (like in Hamlet where Ham-
let kills Claudius, who killed Hamlet’s father), and an analogical              2 Spontol is short for “spontaneous analogy using the Ontol on-
schema would be the generalized plot device of a “revenge killing”.         tology learning and inference algorithm”.
                                                                       3229

devices and analogs between the stories. We know of no prior         Both these systems are also limited in that they are unable to
work that scales to this task when the number of narratives          exploit partial analogical schemas. That is, a partial overlap in
and statements per narrative are both in the hundreds.               these systems’ vectors does not correspond to a common sub-
   In the remainder of this paper, we describe related work          graph in the corresponding structures. These systems stand in
(Section 2), give background on perceptual systems (Sec-             contrast to Spontol, which is able to represent larger struc-
tion 3), describe the Spontol algorithm, which transforms the        tures and efficiently find common substructures.
problem of spontaneous analogy into a “perceptual” problem
(Section 4), demonstrate Spontol’s performance on a story                     3 Background: Perceptual Systems
database (Section 5), discuss implications and shortcomings          Spontol transforms relational structures into feature bags so
of Spontol, and conclude (Section 6).                                that their surface similarity corresponds to the structural sim-
                                                                     ilarity of the relational structures. After Spontol has made
                     2   Related Work                                this transformation, the problem of spontaneous analogy is
                                                                     reduced to the problem of feature overlap, and any of several
There has been earlier work on the problem of analogy in             existing “perceptual” systems can be used to find and exploit
the absence of explicitly segmented domains. The COWARD              patterns in feature vectors. Our implementation of Spontol
system (Baldwin & Goldstone, 2007) addresses this prob-              uses a model inspired by the human sensory cortices (audi-
lem by searching for mappings within a large graph, essen-           tory, visual, tactile) called Ontol (Pickett, 2011). Ontol is a
tially searching for isomorphic subgraphs. SUBDUE (Holder,           pair of algorithms, both of which are given “sensor” inputs
Cook, & Djoko, 1994) compresses large graphs by breaking             (fixed-length, real-valued non-negative vectors). The first al-
them into repeated subgraphs, but is limited in that its out-        gorithm constructs an ontology that concisely encodes the in-
put must be a strict hierarchy, and would be unable to dis-          puts. For example, given a set of vectors representing visual
cover the lattice structure of the concepts in Figure 2. Nauty       windows from natural images, Ontol produces a feature hi-
(McKay, 1981) uses a number of heuristics to efficiently de-         erarchy loosely modeled on that seen in the visual cortex.
termine whether one graph is a subgraph of another, but this         The second algorithm takes as input an ontology (produced
must be given source and target graphs to begin with. We             by the first algorithm) and a new vector, and parses the vector.
can also apply The Chunker (described in Section 3) to fea-          That is, it produces as output the new vector encoded in the
ture bag graphlet kernels (Shervashidze, Vishwanathan, Petri,        higher-level features of the ontology. In addition to “bottom-
Mehlhorn, & Borgwardt, 2009), which are related to Spon-             up” parsing, the second algorithm also makes “top-down”
tol’s transform T in that both represent partial graphs, but this    predictions about any unspecified values in the vector.
earlier work applies only for cases where there is one kind of           Ontol is ignorant of the modality of its input. That is, Ontol
entity, one kind of relation, and only binary relations, while       is given no information about what sensory organ is produc-
Spontol works for multiple kinds of entities and relations, in-      ing its inputs. Because of this ignorance, we are able to lever-
cluding relations of large arity.                                    age Ontol to find patterns in abstract “sensory” inputs that are
   The MAC phase of MAC/FAC (Forbus, Gentner, & Law,                 actually encodings of relational structures.
1995) bears some relation to our spontaneous analog retrieval.
MAC uses vectors of content, such as the number of nodes             Ontology Learning
and edges in a graph, as a heuristic for analog retrieval. How-      Ontol’s ontology formation algorithm, called The Chunker,
ever, in cases where the subgraph in question is a part of a         seeks to find concepts (or chunks) that allow for concise char-
much larger graph, the heuristics that MAC uses are drowned          acterization of vectors. Since chunks themselves are vectors,
out by the larger graph. Likewise, ARCS (Thagard, Holyoak,           The Chunker is applied recursively to create an ontology. In
Nelson, & Gochfeld, 1990) also assumes that analogs have             essence, this algorithm is similar to the recursive block pur-
been delineated (i.e., it matches an entire source domain,           suit algorithm described by Si and Zhu (2011) in that both
rather than a substructure). SEQL (Kuehne, Forbus, Gentner,          search for large frequently occurring sets of features. The
& Quinn, 2000) generalizes relational concepts, but doesn’t          Chunker differs in that it allows for multiple inheritance,
build a hierarchical ontology of analogical schemas.                 while recursive block pursuit creates only strict tree struc-
   There has been some work on representing structures as            tures. In Section 4, we show the importance of this prop-
feature vectors. For example, Holographic Reduced Repre-             erty for finding multiple analogical schemas within a single
sentations have been used to implement Vector Symbolic               relational structure. For simplicity, we describe the discrete
Architectures in which there is a correlation between vec-           binary version of The Chunker algorithm (chunk(B), which
tor overlap and structural similarity (Gayler, Levy, & Bod,          takes as input a set B of feature bags and produces an ontol-
2009). This work is limited in that it requires vectors of length    ogy Ω) provided by Pickett (2011), but this can be modified
10,000 to represent very small graphs (≤ 10 nodes), and only         for continuous vectors. In this version, each vector is treated
represents binary relations of a single type, so this approach       as a set, with a value of 1 for feature f signifying inclusion of
is not directly extendable to relational structures such as the       f in the set, and a value of 0 signifying exclusion.
stories in our demonstration. This is also a limitation for the          The Chunker searches for intersections among existing fea-
system proposed by Rachkovskij, Kussul, and Baidyk (2012).           ture bags and proposes these as candidates for new concepts.
                                                                 3230

Each candidate is evaluated by how much it would compress                   Although the parsing problem is NP-complete, a sin-
the ontology, then the best candidate is selected and added             gle bottom-up pass can be performed in logarithmic time
to the set of feature bags, and the process is repeated until no        (Pickett, 2011). Importantly, Ontol examines only a small
candidates are found that further reduce the description length         subset of the concepts and instances while parsing. This
of the ontology. Figure 2 shows the ontology constructed by             means that, when judging concept similarity, Ontol does not
this algorithm when applied to an animal dataset, where the             need to compare each of its n nodes. This property is impor-
“sensory percepts” are features for each animal3.                       tant for spontaneous analog retrieval (described below).
                                                                                       4 Analogy as Perception
                                                                        We now describe a method for transforming relational struc-
                                                                        tures into sparse feature vectors (or feature bags) such that
                                                                        the problem of analog retrieval is reduced to the problem of
                                                                        percept parsing. An example of this process is shown for the
                                                                        Sour Grapes fable in Figure 5. For this process, we rely on
                                                                        a transform T (described below) that takes a small relational
                                                                        structure and converts it into a feature bag (exemplified in
                                                                        Figure 5(c)). The size of relational structure is limited for T
                                                                        because T ’s runtime is quadratic in the size of the structure.
                                                                        We view this limitation as acceptable because people gener-
                                                                        ally cannot keep all the details of an entire lengthy novel (or
                                                                        all the workings of a car engine) in working memory. Gen-
                                                                        erally, people focus on some aspect of the novel, or some
                                                                        abstracted summary of the novel (or engine). Therefore, we
                                                                        break each large relational structure into multiple overlapping
                                                                        windows. A window is a small set of connected statements,
                                                                        where two statements are connected if they share at least one
                                                                        argument. Spontol exploits a principle akin to one used by
                                                                        the HMax model of the visual cortex (Riesenhuber & Poggio,
                                                                        1999): as the number of windows for a relational structure
                                                                        increases, the probability decreases that another structure has
Figure 2: The Zoo Ontology with some instances. Instances               the same windows without being isomorphic to the first.
are individual animals shown on the left, and base features                 The process for building an ontology of analogical schemas
are on the right. Black nodes in the middle correspond to               from large relational structures, called Spontol-Build, is de-
higher-level features. The concept that corresponds to “fish”           scribed in Figure 3. This algorithm extracts numWindows
is marked. Inhibitory links are shown as dark circles.                  windows from each large relational structure and transforms
                                                                        them into feature bags (exemplified in Figure 5(d)) and
                                                                        chunks these feature bags to create an ontology of windows
Parsing and Prediction                                                  called windowOntology. Spontol-Build then re-encodes the
                                                                        windows by parsing them using this ontology, and re-encodes
Given an ontology and a new instance, Ontol’s parse(b, Ω)
                                                                        the larger structures (from which the windows came) as a fea-
algorithm characterizes the feature bag instance b using the
                                                                        ture bag of the parsed windows. Finally, Spontol-Build runs
higher-level features in the ontology Ω. For example, given
                                                                        another pass of chunking on the re-encoded structures to gen-
a new animal (a goldfish) that doesn’t breathe, has fins, has
                                                                        erate the schema ontology.
no feathers, and is domestic, Ontol will parse the animal as
                                                                            The process of spontaneous analog retrieval, called
an instance of the fish concept, with the exception that it is
                                                                        Spontol-Retrieve, is given in Figure 4. When given a new re-
domestic. If Ontol is given no other information about the
                                                                        lational structure s, we encode s by extracting windows from
animal, it will also perform top-down inference, and unfold
                                                                        it, parsing these using the windowOntology, then parsing the
the fish concept to predict that the new instance has eggs,
                                                                        feature bag representation using the schemaOntology. This
no hair, has a tail, etc.. This latter step is called “top-down
                                                                        yields a set of schemas that are contained in s.
prediction”. Ontol searches for the parse that minimizes the
description length of the instance. In our goldfish example,            Transforming Small Relational Structures
the “raw” description of the goldfish consists of 4 elements,           Here, we describe an operation T , which transforms a (small)
while the “compressed” description has only 2 elements.                 relational structure into a feature bag. In our demonstra-
    3 A full description and implementation of The Chunker, as well     tion, we assume that the relational structure is described
as source code for our demonstration of Spontol can be downloaded       in predicate logic, but our approach is not limited to this
at http://marcpickett.com/src/analogyDemo.tgz.                          representation. We consider a relational structure to be a
                                                                    3231

                                                                       “A fox wanted some grapes, but could not get them. This caused
       Figure 3: Spontol’s Ontology Learning Algorithm                 him to decide that the grapes were sour, though the grapes
 // Creates an ontology of schemas given a set of structures S.        weren’t. Likewise, men often blame their failures on their cir-
 // numWindows is the number of windows to grab per structure.         cumstances, when the real reason is that they are incapable.”
 // windowSize is the number of statements per window.                                            (a) English (for clarity)
 define Spontol-Build (S, numWindows, windowSize)
     // Randomly grab windows from each structure,                      fox Of3Fox       cause m34 m33               sameAs f36 (sour Of3Grapes)
     // and transform them into feature bag form.                       false f36        grapes Of3Grapes            sameAs f35 (decide Of3Fox f36)
                                                                        cause f34 f35    incapable Of3Men            sameAs f34 (get Of3Fox Of3Grapes)
     foreach s ∈ S ; for i = 1, · · · , numWindows                      false f34        decide Of3Fox f36           sameAs m34 (incapable Of3Men)
         let ws,i = grabConnectedStatements      (s, windowSize)        men Of3Men       sameAs m33 (fail Of3Men)    blameFor Of3Men concCircum m33
                                                                       fail Of3Men      want Of3Fox Of3Grapes       circumstances concCircum
         add T ws,i to allWindows
     // Run The Chunker to generate the window ontology                              (b) Predicate Form (Spontol’s actual input)
     windowOntology = chunk (allWindows)
     // Re-encode each structure using the reduced-size windows.                                                      blameFor1=blameFor3.fail1
                                                                                                                      circumstances1=blameFor2
     foreach s ∈ S ; for i = 1, · · · , numWindows                                                                   fail1=blameFor3.fail1
                                                                            blameFor Of3Men concCircum m33
         add parse T ws,i , windowOntology to bigWindowss                    sameAs m33 (fail Of3Men)
                                                                                                                      fail1=blameFor1
                                                                                                                T     incapable1=blameFor3.fail1
     // Run The Chunker to generate the schema ontology.
     schemaOntology = chunk (bigWindows)
                                                                             fail Of3Men
                                                                             circumstances concCircum
                                                                             men Of3Men
                                                                             incapable Of3Men
                                                                                                             ⇒        incapable1=blameFor1
                                                                                                                      incapable1=fail1
                                                                                                                      men1=blameFor3.fail1
     return schemaOntology, windowOntology                                                                            men1=blameFor1
                                                                                                                      men1=fail1
                                                                                                                      men1=incapable1
                                                                                              (c) Transforming a Window
    Figure 4: Spontol’s Spontaneous Analogy Algorithm
 // Finds analogical schemas for relational structure s.                                                       cause2.fail1=blameFor3.fail1
                                                                                 blameFor1=blameFor3.fail1     blameFor1=blameFor3.fail1
 // schemaOntology is the schema ontology.                                       circumstances1=blameFor2      blameFor1=cause2.fail1
 // windowOntology is the window ontology.                                       fail1=blameFor3.fail1         cause2=blameFor3
                                                                                 fail1=blameFor1               fail1=blameFor3.fail1
 // numWindows is the number of windows to grab per structure.                   incapable1=blameFor3.fail1    fail1=cause2.fail1
 // windowSize is the number of statements per window.                           incapable1=blameFor1          fail1=blameFor1
                                                                                 incapable1=fail1              men1=blameFor3.fail1
 define Spontol-Retrieve (s, · · · , windowSize)                                 men1=blameFor3.fail1          men1=cause2.fail1
     // Randomly grab windows from s,                                            men1=blameFor1                men1=blameFor1
                                                                                 men1=fail1                    men1=fail1
     // transform them into feature bag form,                                    men1=incapable1
     // and parse them using the window ontology.
                                                                                                                blameFor1=blameFor3.fail1
     for i = 1, · · · , numWindows                                               false1.sour1=decide2.sour1     fail1=blameFor3.fail1
         wi = grabConnectedStatements (s, windowSize)                            decide1=cause2.decide1
                                                                                 decide2=cause2.decide2
                                                                                                                fail1=blameFor1
                                                                                                                incapable1=blameFor3.fail1
         add parse (T (wi ) , windowOntology) to bags                            false1=cause2.decide2          incapable1=blameFor1
     // Parse bags , the bag representation of s                                 false1=decide2                 incapable1=fail1
     relevantSchemas = parse (bags , schemaOntology)                                          ..                men1=blameFor3.fail1
                                                                                                                men1=blameFor1
     return relevantSchemas
                                                                                               ..               men1=fail1
                                                                                                                             ..
                                                                                                                men1=incapable1
                                                                                                .                             .
set of relational statements, where each statement is either                              (d) Many Transformed Windows
a relation (of fixed arity) with its arguments, or the spe-
cial relation sameAs, which uses the syntax sameAs <name>            Figure 5: Transforming the Sour Grapes Story. We show
(<relation> <arg1> <arg2> ...). The sameAs relation                  the transformation of Sour Grapes from predicate form to fea-
allows for statements about statements. E.g., the statements         ture bag form. For clarity, we show an English paraphrase of
in Figure 5(b) encode (among other things) that “a fox de-           the story (a), though the input to Spontol has already been
cides that the grapes are sour”.                                     encoded in the predicate form shown in (b), which shows the
    Given a small relational structure s (. 10 statements),          story as a set of 18 statements. In (c), we show a window w
T transforms s into a feature bag using a variant of con-            from the story and its feature bag transform T (w). Finally,
junctive coding. That is, T breaks each statement into a             the story is represented as many transformed windows (d).
set of roles and fillers. For example, the statement want
Of3Fox Of3Grapes has two roles and fillers, namely the
two arguments of the want relation. So T breaks this state-          ments). Given a set of roles and fillers, T then chains the
ment into want1=Of3Fox and want2=Of3Grapes, where                    fillers to get filler equalities. For example, if we have that
want2 means the 2nd argument of want (i.e., the “wanted”).           decide1=Of3Fox and want1=Of3Fox, then chaining gives
T then creates one large set of all the roles and their              us decide1=want1. Chaining is essential for recognizing
fillers. If there are multiple instances of a relation, it           structural similarity between relational structures, and allows
gives them an arbitrary lettering (e.g., wantB1=Of3Fox).             us to side-step a criticism of conjunctive coding and ten-
T makes a special case for the sameAs relation. In this              sor products: that the code for wantB1=Of3Fox may have
case, T uses a dot operator to replace the intermediate              no overlap with the code for want1=Of3Fox (Hummel et
variable. For example, the statements sameAs f35 (decide             al., 2004). Chaining introduces the code for wantB1=want1,
Of3Fox f36) and sameAs f36 (sour Of3Grapes) would                    which makes the similarity apparent when searching for
yield decide2.sour1=Of3Grapes. The dot operator allows               analogs (these “chained” features are a core difference be-
T to encode nested statements (i.e., statements about state-         tween MAC’s content vectors and our feature bags). After
                                                                 3232

chaining the roles and fillers, T treats each of these role-filler
bindings as an atomic feature. Note that, when we treat roles
and fillers as atomic features, Ontol doesn’t recognize over-
lap among feature bags unless they share exactly the same
feature. For example, the atomic feature wantB1=Of3Fox has
no more resemblance to want1=Of3Fox for Ontol than it does
for any other feature. Also note that the ordering of the roles
in each feature is arbitrary but consistent (T uses reverse al-
phabetical order), so there is a men1=incapable1 feature, but
not an incapable1=men1 feature. The left side of Figure 5(c)
shows a window taken from the sour grapes story from Figure
5(b). On the right side is the feature bag transform of this set
of 6 statements, consisting of 11 atoms.
                     5 Demonstration
We applied Spontol to a database of 126 stories provided by
Thagard et al. (1990). These include 100 fables and 26 plays
all encoded in a predicate format, where each story is a set
of unsorted statements. An example story in predicate form is
shown in Figure 5(b). Note that although the predicates and
arguments have English names, our algorithm treats all these
as gensyms except for the special sameAs relation. In this en-
coding, the smallest story had 5 statements, while the largest
had 124 statements, with an average of 39.5 statements.
   We ran Spontol-Build on these stories using numWindows =
100 and windowSize = 20 which produced an ontology of sto-
ries, part of which is shown in Figure 6. In this figure we see a     Figure 6: Part of the ontology Spontol learned from the
“Double Suicide” analogical schema found in both Romeo &              story dataset. As in the Zoo Ontology in Figure 2, black
Juliet and in Julius Caesar. In the former, Romeo thinks that         ovals represent higher level concepts. The “raw” features
Juliet is dead, which causes him to kill himself. Juliet, who         (corresponding to the white ovals in Figure 2) are omitted
is actually alive, finds that Romeo has died, which causes her        due to space limitations. Instead, we show the outgoing edges
to kill herself. Likewise, in Julius Caesar, Cassius kills him-       from each black oval. While in the Zoo Ontology, the higher
self after hearing of Titinius’s death. Titinius, who is actually     level concepts correspond to shared surface features, in this
alive, sees Cassius’s corpse, and kills himself. The largest          figure, high level concepts correspond to shared structural
schema found (in terms of number of outgoing edges) was               features, or analogical schemas. For example, the highlighted
that shared by Romeo & Juliet and West Side Story, which are          oval on the right represents a Double Suicide schema, which
both stories about lovers from rival groups. The latter doesn’t       happens in both Romeo & Juliet and in Julius Caesar.
inherit from the Double Suicide schema because Maria (the
analog of Juliet), doesn’t die in the story, and, Tony (Romeo’s
analog) meets his death by murder, not suicide. Some of the           the 100 training stories. Whereas MAC/FAC returns entire
schemas found were quite general. For example, the oval on            stories, Spontol-Retrieve returns analogical schemas (just as
the lower right with 6 incoming edges and 3 outgoing edges            a visual system would return a generic “pterodactyl” con-
corresponds to the schema of “a single event has two signifi-         cept rather than specific instances of pterodactyls). For com-
cant effects”. And the oval above the Double Suicide oval cor-        parison, we modify Spontol-Retrieve to return the set of in-
responds to the schema of “killing to avenge another killing”.        stances that inherit from relevantSchemas, rather than just the
   Spontol-Retrieve uses this schema ontology to efficiently          schemas.
retrieve schemas for a new story, which can be used to make
inferences about the new story in a manner analogous to                    Table 1: Speed/Accuracy Comparison of Spontol
the “goldfish” example from Section 3. To evaluate the ef-                                    Accuracy       Average # Comparisons
ficiency of Spontol-Retrieve, we randomly split our story                MAC/FAC        100.00% ± .00%       100.00 ± .00
dataset into 100 training stories and 26 testing stories. We             Spontol          95.45% ± .62%       15.43 ± .20
then used an ontology learned from the training set, and mea-
sured the number of comparisons needed to retrieve schemas
(during parse) for the testing set. We compare this approach             Results are shown in Table 1, averaged over 100 trials. We
to MAC/FAC, which, during the MAC phase, visits each of               show accuracy (and standard error) for both systems mea-
                                                                  3233

sured as the percentage of stories correctly retrieved, where                                  References
a story was determined to be correct if it was retrieved by          Baldwin, D., & Goldstone, R. L. (2007). Finding Analogies
MAC/FAC. Spontol effectively improves on a linear (in the              Within Systems: Constraints on Unsegmented Matching. In
number of structures) case-by-case comparison to an “in-               WrkShp. on Analogies: Integrating Multiple Cog. Abilities.
dexed” logarithmic-time look-up at a slight cost of accu-            Chalmers, D. J., French, R. M., & Hofstadter, D. (1992).
racy. Therefore, Spontol requires orders of magnitude fewer            High-level Perception, Representation, and Analogy: A
comparisons than MAC/FAC, or any linear look-up algo-                  Critique of Artificial Intelligence Methodology. J. Exp.
rithm (for a survey, see (Rachkovskij et al., 2012)). For larger       Theor. Artif. Intell., 4(3), 185-211.
datasets, we hypothesize that these differences will be even         Clement, J. (1987). Generation of Spontaneous Analogies
more pronounced. Although each comparison by both MAC                  by Students Solving Science Problems. In Thinking Across
and Spontol-Retrieve is a fast vector operation, for very large        Cultures (p. 303-308).
datasets (e.g., 109 relational structures), even a linear num-       Forbus, K., Gentner, D., & Law, K. (1995). MAC/FAC: A
ber of vector operations becomes impractical. In future work,          Model of Similarity-based Retrieval. Cog. Sci., 19(2).
we will test these systems on a broader range of relational          Gayler, R., Levy, S., & Bod, R. (2009). A Distributed Ba-
datasets to help elucidate the conditions under which Spontol          sis for Analogical Mapping. In New Frontiers in Analogy
yields high accuracy and very-low retrieval cost.                      Research; Proc. of 2nd Intern. Analogy Conf. (Vol. 9).
                                                                     Holder, L., Cook, D., & Djoko, S. (1994). Substructure Dis-
                        6 Conclusion                                   covery in the SUBDUE System. In Workshop on Knowl-
The chief contribution of this paper is a demonstration of a           edge Discovery in Databases.
system, Spontol, that is able to solve the problem of sponta-        Hummel, J. E., & Holyoak, K. J. (2005, June). Relational
neous analogy. That is, we have demonstrated how Spontol               Reasoning in a Neurally Plausible Cognitive Architecture:
can efficiently store and retrieve analogs without the need of         An Overview of the LISA Project. Current Directions in
human delineation of schemas.                                          Psychological Science, 14(3), 153–157.
                                                                     Hummel, J. E., Holyoak, K. J., Green, C., Doumas, L. A. A.,
   Our representation also offers a new solution for the bind-
                                                                       Devnich, D., Kittur, A., et al. (2004). A Solution to the
ing problem for long-term (static) memory that allows for ef-
                                                                       Binding Problem for Compositional Connectionism. In
ficient analog retrieval in the absence of explicitly segmented
                                                                       AAAI Fall Symp. on Comp. Connectionism in Cog. Sci.
domains. The binding problem asks how we can meaning-
                                                                     Kuehne, S., Forbus, K., Gentner, D., & Quinn, B. (2000).
fully represent bindings between roles and fillers. Most solu-
                                                                       SEQL: Category Learning as Progressive Abstraction Us-
tions to the binding problem in connectionism do so in terms
                                                                       ing Structure Mapping. In Proceedings of the 22nd Annual
of temporal synchronicity (e.g., LISA (Hummel & Holyoak,
                                                                       Meeting of the Cognitive Science Society.
2005)). Temporal synchronicity only works for knowledge in
                                                                     McKay, B. (1981). Practical Graph Isomorphism. Congres-
working memory, and these models typically address storage
                                                                       sus Numerantium, 30, 45-87.
in long-term memory by relying on some form of conjunc-
                                                                     Mountcastle, V. (1978). An Organizing Principle for Cerebral
tive coding or tensor products. Though these systems fail to
                                                                       Function: The Unit Model and the Distributed System.
address how relational structures can be efficiently retrieved
                                                                     Pickett, M. (2011). Towards Relational Concept Formation
from long-term memory, we hypothesize that a working-
                                                                       From Undifferentiated Sensor Data. Doctoral dissertation,
memory system, such as LISA, is necessary for the “chain-
                                                                       University of Maryland Baltimore County.
ing” process on which our system relies.
                                                                     Rachkovskij, D., Kussul, E., & Baidyk, T. (2012). Build-
   Spontol may offer evidence in support of a uniform “sub-
                                                                       ing a World Model with Structure-Sensitive Sparse Binary
strate” of intelligence (Mountcastle, 1978). In particular,            Distributed Representations. Bio. Inspired Cog. Archs..
we’ve shown how a system that was designed to process per-
                                                                     Riesenhuber, M., & Poggio, T. (1999). Hierarchical Mod-
ceptual data (Ontol) can be leveraged to process “symbolic”
                                                                       els of Object Recognition in Cortex. Nature Neuroscience,
data (i.e., relational structures). This may provide insight into      2(11), 1019–1025.
how species capable of higher-order cognition might have
                                                                     Shervashidze, N., Vishwanathan, S., Petri, T., Mehlhorn, K.,
evolved from species capable of only low-level perception.
                                                                       & Borgwardt, K. (2009). Efficient Graphlet Kernels for
   Although Spontol addresses some outstanding problems                Large Graph Comparison. In Int. Conf. on AI & Stats.
in Computational Analogy, there is still ample room for fu-          Si, Z., & Zhu, S. (2011). Unsupervised Learning of Stochas-
ture work. Our implementation for characterizing a relational          tic AND-OR Templates for Object Modeling. In IEEE Int.
structure as a set of windows might not scale well to very             Conf. on Computer Vision Workshops (pp. 648–655).
large structures without some modifications. An open prob-           Thagard, P., Holyoak, K., Nelson, G., & Gochfeld, D. (1990).
lem is how windows might be managed in a sensible way.                 Analog Retrieval by Constraint Satisfaction. Artificial In-
Spontol currently uses “bags of windows” for medium-sized              telligence, 46(3), 259–310.
structures. We propose extending Spontol by allowing hier-           Wharton, C., Holyoak, K., & Lange, T. (1996). Remote Ana-
archies of progressively higher-order bags to represent larger         logical Reminding. Memory & Cognition, 24(5), 629–643.
structures (e.g., bags of bags of bags of windows).
                                                                 3234

