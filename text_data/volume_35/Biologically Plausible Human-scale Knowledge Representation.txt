UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Biologically Plausible, Human-scale Knowledge Representation
Permalink
https://escholarship.org/uc/item/8kf312gz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Crawford, Eric
Gingerich, Matthew
Eliasmith, Chris
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                  Biologically Plausible, Human-scale Knowledge Representation
                                             Eric Crawford (e2crawfo@uwaterloo.ca)
                       Centre for Theoretical Neuroscience, University of Waterloo, Waterloo, ON, N2L 3G1
                                              Matthew Gingerich (majugi@cs.ubc.ca)
                                        University of British Columbia, Vancouver, BC, V6T 1Z4
                                            Chris Eliasmith (celiasmith@uwaterloo.ca)
                       Centre for Theoretical Neuroscience, University of Waterloo, Waterloo, ON, N2L 3G1
                              Abstract                                      In this paper, we briefly review past connectionist ap-
                                                                         proaches to addressing the problem of representing structure,
   Several approaches to implementing symbol-like represen-              and discuss recent criticisms of those approaches which sug-
   tations in neurally plausible models have been proposed.
   These approaches include binding through synchrony (Shas-             gest that they will not scale. We then present a new ap-
   tri & Ajjanagadde, 1993), mesh binding (van Der Velde &               proach that we have developed that allows for the represen-
   de Kamps, 2006), and conjunctive binding (Smolensky, 1990;            tation and manipulation of large-scale structured representa-
   Plate, 2003). Recent theoretical work has suggested that most
   of these methods will not scale well – that is, they cannot en-       tions in anatomically and physiologically plausible models
   code structured representations that use any of the tens of thou-     of brain function. In past work we have provided theoreti-
   sands of terms in the adult lexicon without making implausible        cal arguments suggesting that this approach will scale better
   resource assumptions (Stewart & Eliasmith, 2011; Eliasmith,
   in press). Here we present an approach that will scale appro-         than others (Stewart & Eliasmith, 2011). Here, our focus is
   priately, and which is based on neurally implementing a type          on empirically demonstrating that claim. We do so by en-
   of Vector Symbolic Architecture (VSA). Specifically, we con-          coding the central structural relations in WordNet into neural
   struct a spiking neural network composed of about 2.5 million
   neurons that employs a VSA to encode and decode the main              representations in a spiking network. We present the results
   lexical relations in WordNet, a semantic network containing           of three experiments showing that 1) this information can be
   over 100,000 concepts (Fellbaum, 1998). We experimentally             decoded for arbitrary lexical items, 2) lexical hierarchies of
   demonstrate the capabilities of our model by measuring its per-
   formance on three tasks which test its ability to accurately tra-     any depth within WordNet are successfully represented, and
   verse the WordNet hierarchy, as well as to decode sentences           3) these lexical representations can be combined to represent
   employing any WordNet term while preserving the original              structured sentences with the same methods.
   lexical structure. We argue that these results show that our
   approach is uniquely well-suited to providing a biologically
   plausible, human-scale account of the structured representa-                               Past Approaches
   tions that underwrite cognition.
                                                                         There have been many approaches to representing structure in
   Keywords: knowledge representation; biologically plausible;
   scaling; neural; vector symbolic                                      connectionist networks. We consider three of the most suc-
                                                                         cessful: 1) binding through synchrony; 2) mesh binding; and
                                                                         3) conjunctive binding.
                          Introduction                                      The suggestion that structured cognitive representations
One of the central challenges for contemporary cognitive                 could be constructed using binding through synchrony (Shas-
modelling is scaling. As Jeff Hinton remarked in his address             tri & Ajjanagadde, 1993) was imported into cognitive mod-
to the Cognitive Science Society, “In the Hitchhiker’s Guide             elling from the earlier hypothesis that feature binding in vi-
to the Galaxy, a fearsome intergalactic battle fleet is acciden-         sion can be accounted for by the synchronization of neu-
tally eaten by a small dog due to a terrible miscalculation of           rons in visual cortex (von der Malsburg, 1981). Recently,
scale. I think that a similar fate awaits most of the models             this approach has seen a revival in the DORA architecture
proposed by Cognitive Scientists” (Hinton, 2010). This ob-               (Doumas et al., 2008) and its variants, which focus on repre-
servation can be taken as a challenge for cognitive modellers:           senting structures for analogical reasoning. In these models,
Will the principles demonstrated in a small-scale cognitive              the temporal relationships between connectionist nodes are
model scale up to the complexity of a human-sized cogni-                 employed to represent structured relations. Structured rep-
tive system? This scaling problem has often been thought to              resentations (e.g. bigger(Fido, Sarah)) are constructed out
be a special challenge for biologically inspired approaches to           of four levels of representation, where nodes in higher lev-
cognitive modelling (Jackendoff, 2002). This is because the              els represent more complex structures via their connections
basic principles employed in such models often do not allow              to nodes in lower layers. As has been argued in more detail
for a straightforward characterization of structured represen-           elsewhere, this kind of representational scheme will not scale
tations, despite the ubiquity of such representations in cog-            well (Stewart & Eliasmith, 2011; Eliasmith, in press) because
nitive behaviour. This same concern is not as immediate for              the number of nodes needed to support arbitrary structured
symbolicist approaches which typically take structured repre-            representations over even small vocabularies (e.g. 6000 lex-
sentations to be primitive (Anderson, 2007).                             ical items) is larger than the number of neurons in the brain.
                                                                     412

Notably, this is not an issue with the use of synchrony per          man’s vocabulary. Hierarchical scaling refers to being able to
se, but rather with the the way binding has been mapped to           encode the depth of grammatical and lexical relations found
network nodes. However, it has also been suggested that syn-         in adult humans. Any method that claims to provide appro-
chrony itself will not scale well to binding complex structures      priate scaling will have to demonstrate success along both of
(Stewart & Eliasmith, 2011; O’Reilly & Munakata, 2000).              these dimensions. The approach that we adopt in this work
    A different approach to structured representation has been       employs a neural implementation of a VSA which uses cir-
taken by van Der Velde & de Kamps (2006) in their work               cular convolution for binding. The purpose of this paper is to
on the Neural Blackboard Architecture (NBA). To avoid the            demonstrate empirically that our approach successfully meets
exponential growth in resources, the NBA employs “neu-               both of these challenges.
ral assemblies.” These assemblies are temporarily bound
to particular symbols using a mesh grid of neural circuits
                                                                             Theoretical Approach and Methods
(e.g. bind(noun1, Fido)). Larger structures are then built           WordNet
by binding these assemblies to roles using a gating circuit          The target of our efforts – the human-scale knowledge base
(e.g. gate(agent1, bind(noun1, Fido))). Neural assemblies            that we will be encoding – is WordNet, a manually con-
that bind roles and their gated word assemblies are used to          structed lexical database of the English language (Fellbaum,
define higher level structure assemblies which can be used           1998). WordNet’s design is intended to reflect the organiza-
to represent sentential structures. The use of temporary bind-       tion of concepts in a psychologically plausible way using a
ing in this manner significantly reduces the resource demands        handful of common relationships. In WordNet words are di-
of this approach compared to DORA. However, as argued in             vided into ”synsets” or synonym sets of words that have the
Stewart & Eliasmith (2011) and demonstrated in more detail           same meaning. Words that have multiple meanings are listed
in Eliasmith (in press), just to represent simple sentences of       in multiple synsets, so the fundamental unit in WordNet is
the form relation(agent, theme) from a vocabulary of 60,000          not a word but a word sense. Each synset is linked to other
terms, this approach requires about 480 cm2 of cortex, ap-           synsets by relations, of which there are several types. The
proximately one fifth of total cortical area. This is much           two relation types that are of the most interest are hypernymy
larger than known language areas which account for both rep-         and holonymy. A hypernym of a concept is the general type
resentation and processing of linguistic terms. Consequently,        of the concept (i.e. dog has the hypernym canine); a holonym
while the NBA has improved scalability compared to DORA,             of a concept is something that that concept is a part of (i.e.
it remains implausible.                                              lock has the holonym door). These relations are explicitly
    The final approach we consider is the class of propos-           encoded in the lexicon we employ. The inverse of the hy-
als broadly called conjunctive coding approaches, or, more           pernym and holonym relations are also implicitly included in
recently, Vector Symbolic Architectures (VSAs; (Gayler,              our encoding, although we do not test their extraction as this
2003)). In general, these approaches propose some kind of            requires more complex control of signal flow that is beyond
nonlinear vector operation to bind two vectors together. The         our present scope. The depiction of lexical relations found in
earliest and perhaps best known such approach is that pro-           WordNet is slightly simplified, though it is sufficient for our
posed by Smolensky (1990), which employs the tensor prod-            purposes; a complete description of the simplifications made
uct as the binding operation. The model presented in this            can be found in Fellbaum (1998).
paper employs a VSA which uses circular convolution as                  Each synset can be defined in terms of its relationships with
the binding operation (after Plate (2003)). The crucial dif-         other synsets. This means that a term such as dog can be
ference between using the tensor product vs. using circu-            defined as:
lar convolution is that for an n-dimensional vector, a tensor
binding results in an n2 -dimensional vector, whereas the cir-                   dog = isA(canine) and partOf(pack)               (1)
cular convolution binding results in an n-dimensional vector.        We will make extensive use of this type of representation in
This computational difference results in severe scaling differ-      our model. We think of the relations in (1) as belonging to the
ences when considering possible biological implementations.          dog synset, and pack and canine as the targets of the relations.
In particular, tensor products scale exponentially poorly as
the depth of the structure increases. For example, Eliasmith         Vector Symbolic Architectures
(in press) shows that encoding a sentence where lexical items        VSAs in general provide a means for representing struc-
have hierarchical relations of depth two or greater (e.g. Sarah      tured knowledge using high-dimensional vectors. This makes
isA(person isA(mammal))) will require approximately 625              VSAs amenable to neural implementation using the Neu-
cm2 of cortex. Again, this is significantly larger than relevant     ral Engineering Framework, which shows how to systemat-
language areas.                                                      ically use populations of spiking neurons to represent vectors
    The above considerations suggest two main challenges             and functions thereof (Eliasmith & Anderson, 2003). In a
for connectionist implementations of structured representa-          VSA, each symbol to be represented is randomly assigned a
tions: lexical scaling and hierarchical scaling. Lexical scal-       high-dimensional vector. Two core operations are provided,
ing means having a lexicon that is as large as an adult hu-          each of which takes two vectors as input and returns a third.
                                                                 413

Binding (~) two vectors returns a third vector that is disim-          Architecture (Eliasmith, in press) which was used to create
ilar to both of the original vectors. The superposition (+)            Spaun, currently the world’s largest functional brain model,
of two vectors returns a third vector that is similar to both          which is able to account for several perceptual, motor and
of the original vectors. Here we employ a close relative of            cognitive behaviours (Eliasmith et al., 2012).
Holographic Reduced Representations (Plate, 2003), a type                 Now, given a semantic pointer representing a synset, we
of VSA in which binding is implemented via circular convo-             can traverse the connections between that synset and related
lution, superposition is implemented via vector addition and           synsets by dereferencing its semantic pointer with the ID-
vectors representing symbols are randomly chosen from the              vector of a relation type, obtaining a noisy version of the ID-
D-dimensional unit sphere. The circular convolution returns            vector of the target of the relation, as demonstrated in (3).
a vector which has the same dimension as the two input vec-               After this initial dereferencing, we must still determine
tors, which is a significant improvement over the tensor prod-         how to remove the noise from the vector returned by the
uct VSA discussed in the Past Approaches section.                      dereferencing operation. Looking again at equation (3), we
   We can use these operations to encode graph-like struc-             see that dog ~ isA is similar to canineID since it consists of
tures such as WordNet. First we fix a dimension D for our              canineID superposed with a noise vector, and is dissimilar to
vectors (D=512 in our model). Then each WordNet synset                 the rest of the ID-vectors (packID , isA, partOf) since they
and each relation type is assigned a random vector on the D-           are related to dog ~ isA via the binding operation. A cleanup
dimensional unit sphere called an ID-vector. Each synset is            memory, which returns the vector in a vocabulary which is
also assigned a second D-dimensional vector, built-up using            most similar to a given input vector, is a potential solution to
the VSA operations, which stores the structural information            this denoising problem. However, we want our model to be
about the synset. To construct this vector, for each relation          able to traverse the full WordNet hierarchy, not just a single
belonging to a particular synset, we bind the ID-vector for            relation, and ID-vectors alone contain no structural informa-
the relation type to the ID-vector for the target of the relation.     tion. We need to have some way to move from the ID-vector
We then superpose the results from all the relations. The fol-         for a synset to its semantic pointer. We can perform both the
lowing equation demonstrates this process for the dog synset:          denoising and mapping to semantic pointer in one step by us-
                                                                       ing an associative cleanup memory rather than a pure cleanup
           dog = isA ~ canineID + partOf ~ packID              (2)     memory. In short, we take the noisy ID-vector returned by
                                                                       the dereferencing operation and feed it into an associative
where all variables on the right-hand side are ID-vectors. We          cleanup memory mapping each synset’s ID-vector to its se-
have disambiguated the two vectors assigned to a synset by             mantic pointer, thus obtaining a clean semantic pointer which
denoting the ID-vector with the ID subscript. What makes               can then be used in further traversals.
(2) useful is that dog preserves information about its con-
stituents; we can use a third operation, dereferencing, to de-         Neural Representation and Computation
termine what a given vector is bound to in dog. The deref-             Thus far we have described VSAs and how they can be used
erencing operation is performed by binding dog with the in-            to encode structural knowledge such as WordNet, but have
verse of the given vector. As an example, imagine we want to           not yet said anything of how to implement them in neurons.
extract the synset that the dog synset is related to via the isA       For this purpose we turn to the Neural Engineering Frame-
relation type. We bind dog with isA, the inverse of the isA            work (NEF), a set of methods for building biologically plau-
vector:                                                                sible models using principles for neural representation, com-
                                                                       putation and dynamics (Eliasmith & Anderson, 2003). The
      dog ~ isA                                                        central idea behind the NEF is that a group of spiking neu-
       = (isA ~ canineID + partOf ~ packID ) ~ isA                     rons can represent vectors over time, and that connections
                                                                       between groups of neurons can compute functions on those
       = isA ~ canineID ~ isA + partOf ~ packID ~ isA                  vectors. More precisely, a group of neurons represents any
       ≈ canineID + partOf ~ packID ~ isA                      (3)     of a set of vectors, that is, a vector space. The NEF provides
                                                                       a set of methods for determining what the connections need
Equation (3) shows that dog ~ isA is canineID superposed               to be to compute a given function on the vector space repre-
with another vector which can effectively be regarded as               sented by a group of neurons. Suppose we wish to compute
noise. All that remains is to remove that noise, and we will           the function y = f(x), where vector space x is represented in
discuss methods for doing so below.                                    population A, and vector space y is represented in population
   We call vectors constructed in the manner of dog in (2) se-         B. To do so, the NEF assumes that each neuron in A and B has
mantic pointers because they are a compressed representation           a “preferred direction vector.” The preferred direction vector
of their constituents, and preserve similarity (i.e. semantic)         is the vector (i.e. direction in the vector space) for which that
relations in their compressed form. In addition, the derefer-          neuron will fire most strongly. Consequently, the spiking ac-
encing operation is similar to the dereferencing of pointers           tivity of every neuron in a population A can be written
in programming languages. Semantic pointers have a wide
range of uses; indeed, they are central to the Semantic Pointer                            ai (x) = G[ αi ei x + Jbias ]             (4)
                                                                   414

where ai is the ith neuron in the population, G is the spik-         Neural Associative Memory
ing neural nonlinearity, αi is the gain of the neuron, ei is the     There are several ways in which associative memories can be
preferred direction (or encoding) vector, and Jbias is a bias        implemented (see (Lansner, 2009) for a review). Recently,
current to account for background activity of the neuron. The        (Stewart et al., 2010) used the NEF to construct an efficient,
elements in the square brackets determine the current flowing        fast autoassociative (a.k.a. cleanup) memory out of spiking
into the cell, which then drives the spiking of the chosen sin-      neurons, and this approach can be trivially extended to con-
gle cell model G. For computational efficiency, we employ a          struct an associative memory. Moreover, they demonstrate
leaky integrate-and-fire (LIF) neuron model, though the NEF          that this approach significantly outperforms a linear associa-
can be applied for arbitrary neuron models. Equation (4) is          tor, a direct function approximator and a standard multi-layer
referred to as an encoding equation because it describes how         perceptron. However, that paper only considers lexicons up
a vector space, in this case x, is encoded into neural spikes.       to 10,000 items, and does not discuss any actual lexical pro-
The NEF assumes a least-squares optimal linear decoding to           cessing, as is our focus here.
reconstruct x or any nonlinear function thereof, f(x). Thus,
                                f                                       Given a noisy version of an ID-vector as input, we want
we must find the decoders di , such that                             our associative memory to output a clean version of the corre-
                     1
                       Z
                                              f                      sponding semantic pointer. A simple algorithm that achieves
                 E=       [ f (x) − ∑ ai (x)di ]2 dx         (5)     this is to take the dot product of the input vector with each of
                     2                i
                                                                     the ID-vectors in the vocabulary, threshold these values (set
is minimized. Finding the decoders in this manner then pro-          to 0 all values below some fixed threshold), multiply each se-
vides us with a way to estimate any vector f(x) given the ac-        mantic pointer vector by its corresponding thresholded value,
tivities from the encoding equation. We can write this as the        and add all the resultant vectors together to obtain a single D-
decoding equation:                                                   dimensional vector. If the input vector is only similar to one
                                            f                        of the ID-vectors, then all of the dot products will be thresh-
                       fd(x) = ∑ ai (x)di                    (6)     olded except for one and the output vector will be equal to the
                                  i
                                                                     correct semantic pointer.
where N is the number of neurons in the group and fd  (x) is the        We can use the NEF to implement this algorithm in spiking
estimate of f(x) where x is the input driving the neurons. Re-       neurons as follows. Assign each synset a small (∼20) popula-
call that our purpose in defining the representation of a vector     tion of neurons. Then we set the preferred direction vector of
space in a neural population is to use it to compute a function      each neuron equal to the ID-vector for the synset that the neu-
between two populations. If we define the encoding and de-           ron is assigned to. Equation (4) shows the activities of each
coding for groups A and B using equations (4) and (6), we can        population can be seen as encoding the similarity between the
substitute the decoding of A into the encoding of B, thereby         input vector and the population’s assigned ID-vector. To de-
deriving connection weights. In addition, if the function we         termine the weight matrices between these populations and
wish to compute is linear, we can include the relevant lin-          the output population, we first minimize equation (5) with f
ear operator in the weight equation. The weight equation for         set to a thresholding function to find optimal decoders, and
computing any combination of linear and nonlinear functions          then substitute these into equation (7) with L set to semantic
is then:                                                             pointer of the population’s assigned synset. Thus, the output
                                    f
                                                                     of a population with ID-vector e and semantic pointer s is a
                         ωi j = di α j Le j                  (7)     neural reconstruction of threshold(xT e) · s. Summing the out-
where i indexes the neurons in group A and j indexes the             put of all the association populations is implicitly performed
neurons in B, f is any nonlinear function and L is any DB x          by the dendrites of the neurons in the output population.
DA linear operator, where DA and DB are the dimensionalities
of the two vector spaces.
                                                                                               The Model
   It is worth noting that these representations and computa-        The core of the model is a network of spiking neurons, con-
tions can be implemented to any desired precision, by adding         structed using the techniques outlined above, which, given
enough neurons. Specifically, the root mean-squared-error            a semantic pointer corresponding to a WordNet synset and
goes down as 1/N (Eliasmith & Anderson, 2003). One of the            a query vector corresponding to a relation type, returns the
main concerns of this paper is to demonstrate that the opera-        semantic pointer corresponding to the target of the relation.
tions required for representing human-scale lexical structure        This network can be used to traverse the WordNet hierarchy
can be done with a reasonable number of neurons.                     by running it recursively, with the output of the last run used
   It is straightforward to use the NEF to create networks of        as input on the next run. The tasks of moving the output into
spiking neurons for computing the inverse and circular con-          the input, controlling which relation goes into the query vec-
volution operations (Eliasmith, 2005). However, neurally im-         tor population, etc, are not investigated here as they are pe-
plementing an associative memory requires a specific appli-          ripheral to our central concern of representing human-scale
cation of these methods, which we will outline in the next           structured knowledge in a biologically plausible manner.
section.                                                                A schematic diagram of the model is depicted in Fig-
                                                                 415

Figure 1: The network of spiking neurons that traverses the WordNet graph. Assume S = R ~ TID +U ~VID where R, TID , U,
and VID are all ID-vectors and T is the semantic pointer corresponding to TID . All nodes represent neural populations.
ure 1. The nodes correspond to populations of spiking neu-          network returns the semantic pointer for canine. To be con-
rons which represent and manipulate 512-dimensional vec-            sidered correct, the returned vector must have a larger dot
tors. All neurons employ the leaky integrate-and-fire neu-          product with the correct semantic pointer than with any incor-
ron model. Each time the model was run to perform a single          rect semantic pointer in the vocabulary, and this dot product
edge traversal, it was simulated for 100 ms with a simulation       must pass a threshold of 0.7. We ran 20 runs, each of which
timestep of 1 ms, after which the vector represented by the         consisted of 100 trials, amounting to 2000 edge traversals.
Output population was taken to be the output of the model.
The Binding node, which performs a circular convolution be-         Experiment 2: Hierarchy Traversal
tween two vectors, contains 51,400 neurons, and each of the
other 4 nodes outside the associative memory contain 25,600         This experiment is designed to test the model’s ability to tra-
neurons. The associative memory contains 117,659 popula-            verse the network to arbitrary depth. To that end, we use the
tions, one for each WordNet synset, with 20 neurons each,           model to answer the following question: given two synsets
resulting in a grand total of 2,506,980 neurons. This is equiv-     and a relation type, can the second synset be reached from the
alent to approximately 14.7 mm2 of cortex, much smaller             first synset solely by following links of the specified type? We
than previous neural approaches to structured representation,       present the model with the semantic pointer corresponding to
which required on the order of 500 cm2 cortex (as there are         the first synset as well as the ID-vector for the given rela-
about 170,000 neurons per mm2 ; (Eliasmith, in press)).             tion type. Then we run the model, and compare the output
                                                                    vector to the semantic pointer for the second synset. If they
                  Experimental Results                              are the same (their normalized dot product is above a fixed
We performed three experiments on the model to test different       threshold), then the model responds with a Yes. If not, we
aspects of its performance. For each experiment, a trial con-       feed the output vector back into the model as the new seman-
sists of using the network to answer a single question about        tic pointer and run the model again. This process is repeated
the WordNet graph (the question is different for each exper-        until the model returns a vector with a norm below a fixed
iment). A run consists of a group of trials. For each experi-       threshold. If the model reaches this point, it responds with a
ment we perform some number of runs, calculate the perfor-          No. Here it is especially important that the decoded semantic
mance on each run as the percentage of trials on which the          pointer be very similar to the correct semantic pointer since
model answered correctly, and report the mean performance           we recursively use the output, and large errors would build
over all the runs. We employ a bootstrapping method to ob-          up with successive edge traversals. Our tests were performed
tain 95% confidence intervals on the mean performance. This         using only the isA relation type as it is the most prominent in
data can be seen in Table 1. The model was perfectly success-       WordNet and permits the deepest traversals. We ran 20 runs,
ful on Experiment 1, and nearly so on Experiments 2 and 3.          each consisting of 20 positive examples (the second synset
                                                                    could be reached in the actual WordNet graph), and 20 nega-
Experiment 1: Decoding Accuracy
                                                                    tive examples.
This test investigates how many of the 117,659 concepts in
the WordNet can be accurately decoded. For this experiment,                          Table 1: Experimental Results
we present the model with a semantic pointer corresponding                Experiment                % correct        95% CI
to a randomly chosen synset and an ID-vector corresponding                                                        lower upper
to a relation type, and see if the model returns the semantic             1. Decoding Accuracy         100.0      100.0 100.0
pointer corresponding to the target of that relation. For exam-           2. Hierarchy Traversal        95.5      94.3     96.9
ple, we might present the network with the semantic pointer               3. Sentence Encoding          99.6      99.3     99.8
for dog and the ID-vector for the relation isA and see if the
                                                                416

Experiment 3: Sentence Encoding                                      Eliasmith, C., Stewart, T. C., Choo, X., Bekolay, T., DeWolf,
The final experiment we performed was designed to con-                 T., Tang, Y., et al. (2012). A large-scale model of the
firm that this method of knowledge representation is flexi-            functioning brain. Science, 338(6111), 1202-1205.
ble enough to allow concepts to bind to arbitrary roles while        Fellbaum, C. (1998). Wordnet: an electronic lexical
still encoding the thousands of relationships between them-            database. Cambridge, Massachusetts: MIT Press.
selves. To this end, we test whether the network can accu-           Gayler, R. W. (2003). Vector Symbolic Architectures an-
rately decode a crude approximation of a sentence, consist-            swer Jackendoff’s challenges for cognitive neuroscience.
ing of synsets bound to one of six different roles. To build a         In P. Slezak (Ed.), Iccs/ascs international conference on
sentence, we randomly choose roles for inclusion, each with            cognitive science (pp. 133–138).
a different probability, and then synsets are randomly cho-          Hinton, G. (2010). Where do features come from? In Out-
sen to fill the selected roles. Each role type is assigned an          standing questions in cognitive science: A symposium hon-
ID-vector, in the same way that ID-vectors are assigned to re-         oring ten years of the david e. rumelhart prize in cognitive
lation types. A semantic pointer for the sentence is created by        science. Cognitive Science Society.
binding synset ID-vectors to role ID-vectors in the usual way.       Jackendoff, R. (2002). Foundations of language: Brain,
We then present the network with the semantic pointer for the          meaning, grammar, evolution. Oxford University Press.
sentence and the ID-vector for a role, and see if the vector it
                                                                     Lansner, A. (2009, March). Associative memory models:
outputs is the same as the semantic pointer for the concept
                                                                       from the cell-assembly theory to biophysically detailed
filling that role in that sentence. To determine the correct-
                                                                       cortex simulations. Trends in neurosciences, 32(3), 178–
ness of a particular decoding, we used the same criteria as in
                                                                       86.
Experiment 1. We ran 20 runs, each of which consisted of
constructing 30 sentences and asking the model about each            O’Reilly, R. C., & Munakata, Y. (2000). Computational Ex-
role therein, amounting to 2411 edge traversals.                       plorations in Cognitive Neuroscience: Understanding the
                                                                       Mind by Simulating the Brain (1st ed.). The MIT Press.
                           Conclusion                                  Paperback.
These empirical results demonstrate that our spiking neural          Plate, T. A. (2003). Holographic reduced representations.
network can accurately represent structured knowledge rep-             Stanford, CA: CSLI Publication.
resentations approaching the scale of those found in an adult        Shastri, L., & Ajjanagadde, V. (1993). From simple associa-
human. Moreover, our’s is the only approach with neural re-            tions to systematic reasoning: A connectionist representa-
source requirements that fall within the range of biological           tion of rules, variables, and dynamic bindings. Behavioral
plausibility.                                                          and Brain Sciences, 16, 417–494.
                                                                     Smolensky, P. (1990). Tensor product variable binding and
                     Acknowledgments                                   the representation of symbolic structures in connectionist
Funding for this work was provided by the National Science             systems. Artificial Intelligence, 46, 159–217.
and Engineering Research Council of Canada, Canada Re-               Stewart, T., & Eliasmith, C. (2011). Compositionality and bi-
search Chairs, the Canadian Foundation for Innovation and              ologically plausible models. In W. Hinzen, E. Machery, &
the Ontario Innovation Trust.                                          M. Werning (Eds.), Oxford handbook of compositionality.
                                                                       Oxford University Press.
                           References                                Stewart, T., Tang, Y., & Eliasmith, C. (2010). A biologically
Anderson, J. R. (2007). How can the human mind occur in                realistic cleanup memory: Autoassociation in spiking neu-
    the physical universe? Oxford University Press.                    rons. Cognitive Systems Research.
Doumas, L. A. A., Hummel, J. E., & Sandhofer, C. M. (2008).          van Der Velde, F., & de Kamps, M. (2006). Neural black-
    A theory of the discovery and predication of relational con-       board architectures of combinatorial structures in cogni-
    cepts. Psychological Review, 115, 1–43.                            tion. Behavioral and Brain Sciences, 29(29), 37–108.
Eliasmith, C. (2005). Cognition with neurons: A large-scale,         von der Malsburg, C. (1981). The Correlation Theory of
    biologically realistic model of the Wason task. In G. Bara,        Brain Function (Vol. Internal R; Tech. Rep.). Abteilung
    L. Barsalou, & M. Bucciarelli (Eds.), Proceedings of the           fiir Neurobiologie, MPI for Biophysical Chemistry.
    27th annual meeting of the cognitive science society (pp.
    624–630).
Eliasmith, C. (in press). How to build a brain: A neural ar-
    chitecture for biological cognition. New York, NY: Oxford
    University Press.
Eliasmith, C., & Anderson, C. H. (2003). Neural engineer-
    ing: Computation, representation and dynamics in neuro-
    biological systems. Cambridge, MA: MIT Press.
                                                                 417

