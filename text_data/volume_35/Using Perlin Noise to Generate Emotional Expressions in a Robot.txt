UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Using Perlin Noise to Generate Emotional Expressions in a Robot

Permalink
https://escholarship.org/uc/item/4qv84958

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Beck, Aryel
Hiolle, Antoine
Cañamero, Lola

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Using Perlin Noise to Generate Emotional Expressions in a Robot
Aryel Beck (aryelbeck@gmail.com)
Antoine Hiolle (a.hiolle@herts.ac.uk)
Lola Cañamero (l.canamero@herts.ac.uk)
Embodied Emotion, Cognition and (Inter-)Action Lab, School of Computer Science & STRI
University of Hertfordshire, College Lane, Hatfield, Herts AL10 9AB, UK
the same time lively and flexible enough to be believable and
engaging.

Abstract
The development of social robots that convey emotion with
their bodies—instead of or in conjunction with their faces—
is an increasingly active research topic in the field of humanrobot interaction (HRI). Rather than focusing either on postural or on dynamics aspects of bodily expression in isolation,
we present a model and an empirical study where we combine
both elements and produce expressive behaviors by adding dynamic elements (in the form of Perlin noise) to a subset of
static postures prototypical of basic emotions, with the aim of
creating expressions easily understandable by children and at
the same time lively and flexible enough to be believable and
engaging. Results show that the noise increases the recognition
rate of the emotions portrayed by the robot.

Affect Space
This study is part of our research investigating the elaboration of an Affect Space for the generation of emotional body
language to be displayed by robots. It builds on an Affect
Space that was generated using key poses (Beck, Cañamero,
& Bard, 2010; Beck, Hiolle, Mazel, & Cañamero, 2010). In
the context of this paper, a key pose is a posture modeled after
an actor performance so that it clearly describes the emotion
displayed.

Keywords: Bodily emotional expression; human-robot interaction; affective robotics; Perlin noise.

Static features

Introduction
Echoing the importance of emotional expression in social
interaction and communication among humans, the development of expressive robots that can interact with us in a
human-oriented way is nowadays a very active research topic
in the field of human-robot interaction (HRI). Interest in using
robot’s bodies for emotional expression is rapidly increasing.
This is partly due to two main factors. On the one hand, an increasing corpus of research in psychology and neuroscience
(e.g., (Wallbott, 1998; De Gelder, 2006; Avizer, Trope, &
Todorov, 2012)) is emphasizing the role of the body in conveying emotion-specific information rather than merely nonspecific information related to intensity as it was previously
thought. On the other hand, the fact that a number of robotic platforms currently available have complex bodies with
a high number of degrees of freedom and/or good motion capabilities, but do not necessarily have articulated faces—that is
the case in Nao1 , the robot that we have used in this study.
While researchers typically focus either on the use of
expressive postural elements or on expressive aspects of
movement (Coulson, 2008)—see (Kleinsmith & BianchiBerthouze, 2012) for a survey—the combination of both aspects has not received as much attention in robotics. In the
study resented here, we combine both elements and produce
expressive behaviors by adding dynamic elements to a subset
of static postures prototypical of basic emotions. Our underlying motivation from the point of view of HRI2 , as part of
the European project ALIZ-E (www.aliz-e.org), was to create
a set of expressions easily understandable by children and at
1 www.aldebaran-robotics.com.
2 See (Cañamero, 2002, 2008) for discussions of design issues
regarding expressive robots for HRI.

In animation, one of the standard methods for creating convincing and believable displays relies on expressive key poses
rather than body language in motion (Thomas & Johnston,
1995; Vala, Paiva, & Rui Gomes, 2008). Taking inspiration from this method, in previous work (Beck, Cañamero,
& Bard, 2010; Beck, Hiolle, et al., 2010) we used static key
poses as a basis to produce expressive animated behaviors in a
humanoid robot. This method presents the advantage of permitting to investigate and model independently postural and
motion-related expressive elements. This approach is also
consistent with research on affective body expression suggesting that form and movement information are processed
by separate pathways in the brain (Kleinsmith & BianchiBerthouze, 2012). The key poses that we used are consistent
with the static features3 in (Kleinsmith, Bianchi-Berthouze,
& Steed, 2011).
Our initial experiments (Beck, Cañamero, & Bard, 2010)
showed that it is possible to successfully convey emotions
using static key poses displayed by a Nao humanoid robot.
Based on these results, we started to develop a continuous
Affect Space for our robot by “blending” key poses to generate new expressions (Beck, Hiolle, et al., 2010). The resulting
system maps static key poses into a continuous dimensional
model of emotion. Empirical results regarding the interpretation of the static key poses generated by this Affect Space can
be found in (Beck, Hiolle, et al., 2010). While some of the expressions were clearly recognized, our results also show that
some of the generated key poses are ambiguous and do not
convey a clear emotion. In addition, feedback from people interacting with the robot indicated that they found it too static,
which might have a negative impact on the perception on the
3 In particular, the collar joint angle was also found to be salient
to the expression of emotion through body posture.

1845

robot and hence on the interaction. This led us to hypothesize that the addition of dynamic aspects to the key poses could
greatly improve the understanding and believability of the expressions.

This mapping was chosen, rather than directly using the
speed of the motors, due to constraints imposed by our
robot. However, it should be noted that the actual velocity of the movement also depends on the amplitude of the
noise, since the time is kept constant but the amplitude varies. Based on the existing literature, we expected that this
parameter would have a significant effect on the perception of the emotion as it is related to Quantity of Motion
(Camurri, Mazzarino, & Volpe, 2003), Speed (Roether,
Omlor, Christensen, & Giese, 2009; Bernhardt, 2010) and
Activation (Wallbott, 1998; Hartmann, Mancini, Buisine,
& Pelachaud, 2005).

Animating Emotional Key Poses Using Perlin Noise
To endow the key poses with a dynamic dimension, we added Perlin noise4 (Perlin, 1990) to them. In animation, Perlin
noise—a coherent noise that is highly controllable—is a wellknown tool used to procedurally generate movements and increase the lifelikeness of animations. It presents the advantages of being simple and computationally cheap, which are
important factors for implementation on a robotic platform.
Moreover, the parameters used to generate it can be modulated, resulting in different types of animations. Perlin noise
can be used to modify movement but also to create different
types of non-repetitive and “idle” behaviors, as well as to generate textures. In robotics, Perlin noise and similar methods
have also been used, applied to joint angles, to increase the
lifelikeness of robot movements and to generate idle behaviors (Snibbe, Scheeff, & Rahardja, 1999; Ishiguro, 2005).
Going beyond standard practice, in the work reported in
this paper we have used Perlin noise to generate all the movements of the robot, rather to simply modify existing trajectories. The addition of Perlin noise values to the current joint
angles produces a Perlin noise-based animation for the current pose of the robot. Although this step has not been validated with formal perceptual studies, the movements generated
have been successfully used as idle behavior in empirical interaction studies with children carried as part of the ALIZ-E
project (Nalin et al., 2012).

The Experiment
To assess the potential of using Perlin noise to express emotions in robots, we designed a study to investigate the relation between characteristics of the movements generated using Perlin noise and the perceived emotion.
Independent Variables: Three independent variables
were manipulated: Emotional Key Pose, Velocity and Jerkiness.
• Key Pose had five different values that corresponded to the
different emotions tested.
• Velocity had three levels and described how fast the robot
moved.

Using Perlin Noise to Express Emotions
Following a “deep” approach to emotion modeling
(Cañamero, 2008), affective expression in our robot is
driven by the dynamics of the internal “affective state” of
the robot in its interaction with the world. Consequently,
movements produced by Perlin noise can be modulated by
the internal state of the robot and used as a tool to express
emotions. This novel use of Perlin noise can potentially be
a powerful tool to create more subtle expressions in robots,
since it permits to procedurally create non-repetitive body
movements that convey different emotions or nuances of the
same emotion. Another advantage of our approach is that
such expression would not be limited to a single platform
and could be reused across different robots—both humanoid
and non-humanoid.
One of the main challenges posed by the use of Perlin noise
to express emotions is to find a mapping between the parameters used to generate the noise and the emotion to be conveyed.
In our model, we used the following mappings:
• Velocity was mapped to the time taken by the robot to
move, i.e., the shorter the time the higher the velocity.
4 See http://freespace.virgin.net/hugo.elias/models/
m perlin.htm for a description of the method used.

• Jerkiness was introduced by applying random variations
to the duration parameter, slightly modifying the interval
of update of the joint angle. The literature suggests that
jerkiness has a strong effect on the expression of emotion (Hartmann et al., 2005; Lee, Park, & Nam, 2007;
Bernhardt, 2010).

• Jerkiness had two levels. In the Jerky condition, the velocity of each movement (generated using Perlin noise) was
multiplied by a random value between 0.5 and 1.5 ensuring that the mean of the velocity remained the same but
introducing variation of speed during the animation. In the
Regular (non-Jerky) condition, the speed (given by the Velocity condition) remained constant throughout the whole
animation.
This resulted in 35(5 Key Poses ∗ 3Velocity ∗ 2 Jerk + 5 static)
animations tested.
Dependent Variables: Perception of emotion was defined
in terms of Emotional Label, Valence and Arousal.

Participants
20 Participants were recruited, mostly members of staff of the
University of Hertfordshire (9 females and 11 males) ranging
in age from 18 to 55 (M=29.31, SD=11.93).

Apparatus
Five key poses were selected from previous studies (Figure
1): two positive, two negative and one neutral that had been

1846

Figure 1: The five key poses (from left to right: sadness, anger, neutral, pride, happiness)

Table 1: Recognition rate of the Key Poses with and without
added movements
Emotion
Sadness
Anger
Pride
Happiness
Neutral

Recognition Rate Static
84%
42%
63%
79%
84%

Recog. Rate with Movement
100%
68%
74%
95%
74%

Best Condition
Slow Regular
Fast Regular
Medium Regular
Fast Jerky
Medium Regular

Figure 2: Effect of Changing the Key Pose on Valence
recognized well above chance level in previous studies (Beck,
Cañamero, & Bard, 2010; Beck, Hiolle, et al., 2010). To ensure stability, the robot was sitting and only the joint angles
of the upper body were modified while changing key pose.
The animations were generated by adding Perlin noise to the
joints of the upper body (as described above).

Procedure
The same experimenter tested all participants individually.
Once each participant had given consent at the beginning of
their session, they were given standardised explanation regarding the questionnaire that they were expected to answer
and were instructed to imagine that the robot was reacting to
something. In this context, Valence was defined as the extent to which this “something” was positive or negative, and
Arousal was defined as the level of energy (low to high energy).
After confirming that they understood all the questions,
participants watched and assessed the 35 animations. Each
animation was displayed only once in a randomized order
different for each participant. A distance was introduced to
avoid having the same pose coming twice in a row. Each
time, the robot took a pose and displayed an animation during 15 seconds and returned to a non-expressive key pose (a
second neutral pose) until the participant answered. For each
animation, participants were asked to describe the animation
using their own terms and eventually choose an emotion label from a list of six emotions. The list was comprised of
Anger, Sadness, Fear, Neutral, Pride, Happiness and Excitement. Participants completed ratings of Valence and Arousal
on a 10-point Lickert scale. After all the poses had been assessed, participants were fully debriefed. Each session lasted
approximately 30 minutes.

Results
Since this experiment uses a modified set of key poses (unlike
in the test of the static key poses, here the robot is sitting), it
was necessary to validate the material created for this study.

Validation of the Sitting Key Poses
Recognition rates showed that it was possible for participants
to correctly identify the different static key poses far above
chance level (Chance level would be 17%). Thus, it was possible for participants to identify the static key poses displayed
(Table 1).

As part of the validation of the material, a two-ways (static
vs. highest recognition rate) Repeated Measures ANOVA
was conducted on the total Number of Correct Interpretations
comparing the static display and the highest recognition rate
with movement for each emotion. This was done to check
that it was possible to increase the recognition rate by adding
movements generated with Perlin noise in at least one condition for the different key poses. The results show that this
was the case (F(1, 18) = 9.08, p < 0.01, η2 = 0.33). Table IV
also highlights the recognition rates as well as the conditions
in which the highest recognition rates were obtained.
In the following sections, the data was analysed using 5(Key Pose)*3(Velocity)*2(Jerkiness) Repeated Measures
Anovas on the dependent variables. It should be noted that
since they do not have a Jerkiness condition, the static poses
were not included in these tests.

Effect of Changing the Key Pose Displayed
Effect on the Number of Correct Interpretations As
expected, Key Pose had a significant effect on the
Number of Correct Interpretations (F(4, 72) = 6.89, p <
0.01, partial η2 = 0.99). This indicates that overall, when
displayed with movements, the key poses were not all equally
well recognized. Post-Hoc tests (Least Significant Difference) showed that the poses for Sadness and Pride were recognized better than the others (p < 0.01).
Effect on Valence Key Pose had a significant effect on
Valence (F(4, 72) = 33.26, p < 0.01, partial η2 = 0.65).
Post-hoc tests (Least significant Difference) showed that the
pose for Sadness was perceived as more negative than the
rest of the poses (p < 0.01 for all of them). The key pose
for Anger was perceived as more negative than Happiness

1847

Figure 3: Effect of Changing the Key Pose on Arousal

Table 2: Effect of Velocity and Jerkiness on Interpretation per
Key Pose
Key Pose
Sadness

Anger

Neutral

Pride

Happiness

(p < 0.01) and Pride (p < 0.01). There was however no significant difference between Anger and Neutral (p = 0.29).
Pride was perceived as significantly more positive than the
rest of the key pose (p < 0.05 for all of them). Happiness
was perceived as significantly more positive than Sadness
(p < 0.01), Anger (p < 0.01) and Neutral (p < 0.05) (Figure
2)
These results indicate that participants’ perception of
Valence was affected by the Key Pose being displayed. Overall, negative key poses were interpreted as such and positive
key poses were interpreted as positive (Figure 2).
Effect on Arousal Key Pose had a significant effect on
Arousal (F(4, 72) = 13.29, p < 0.01, partial η2 = 0.42).
Post-Hoc tests(Least Significant Difference) showed that
Sadness was perceived as less aroused than Anger (p < 0.01),
Pride (p < 0.01), and Happiness (p < 0.05). There was
no significant difference between Sadness and Neutral (p =
0.21). Anger was perceived as more aroused than Neutral
(p < 0.01). However, there was no significant difference with
Happiness (p = 0.26) and Pride (p = 0.37). Pride was perceived as less aroused than neutral (p < 0.01). There was a
trend toward Pride being perceived as less aroused than Happiness (p = 0.06).
These results indicate that perception of Arousal was affected by the key pose being displayed (Figure 3).

Effect of Velocity
Effect on Interpretation Velocity had a significant effect on the number of correct interpretation (F(2, 36) =
11.02, p < 0.01, Partial η2 = 0.98). This effect was further
investigated while looking at the interactions between the dependent variables.
Effect on Valence Although it did not reach significance,
there was a trend of Velocity affecting Valence (F(2, 36) =
3.14, p = 0.06, partial η2 = 0.15). Post-Hoc tests (Least Significant Difference) showed that there was a trend of Slow
movement perceived as less positive than Fast (p = 0.07).
There was no difference between the Slow and Medium con-

Effect of Velocity
F(2, 34) = 5.34, p < 0.05, η2 = 0.24
Slow > Medium(p < 0.05)
Slow > Fast(p < 0.01)
Medium = Fast(p = 0.31)
F(2, 34) = 6.21, p < 0.01, η2 = 12.43
Fast > Medium(p < 0.05)
Fast > Slow(p < 0.01)
Medium = Slow(p = 0.45)
F(2, 36) = 48.69, p < 0.01, η2 = 0.73)
Slow > Fast(p < 0.01)
Medium > Fast(p < 0.01)
Slow = Medium(p = 0.1)
F(2, 36) = 17.95, p < 0.01η2 = 0.50
Slow > Fast(p < 0.01)
Medium > Fast(p < 0.01)
Slow = Fast(p = 0.19)
F(2, 36) = 5.36, p < 0.01, η2 = 0.23)
Fast > Slow(p < 0.01)
Fast = Medium(p = 0.09)
Medium = Slow(p = 0.17)

Effect of Jerkiness
F(1, 17) = 11.73, p < 0.01, η2 = 0.41
Regular > Jerki(p < 0.01)
F(1, 18) = 0.79, p = 0.39, η2 = 0.04

F(1, 18) = 0.00, p = 1, η2 = 0.00

F(1, 18) = 1.09, p = 0.31, η2 = 0.06

F(1, 18) = 1.20, p = 0.29, η2 = 0.06

ditions (p = 0.34). The Medium condition was perceived as
significantly less positive than the Fast condition (p < 0.05).
These results indicate that the fast movement condition was
perceived as more positive than the other two.
Effect on Arousal Velocity had a significant effect on
Arousal (F(2, 36) = 93.60, p < 0.01, partial η2 = 0.84).
Post-Hoc tests (Least Significant Difference) showed that the
Slow condition was perceived as less aroused than the Medium condition (p < 0.01) which in turn was perceived as
less aroused than the Fast condition (p < 0.01).
These results indicate that overall the faster the movement
is, the more aroused the expression is perceived.

Effect of Jerkiness
Effect on Interpretation There was a trend of Jerky being
more correctly interpreted than the same display in the Regular condition (F(1, 18) = 4.21, p = 0.55, partial η2 = 0.49).
This was further explored while considering the interactions
between the dependent variables.
Effect on Valence Jerkiness had no significant effect
on Valence (F(1, 18) = 0.26, p = 0.62, partial η2 = 0.01).
These results indicate that overall, participants’ perception of
Valence was not affected by the Jerkiness of the movements.
Effect on Arousal Jerkiness had a significant effect on
Arousal (F(1, 18) = 27.51, p < 0.01, partial η2 = 0.60).
Post-Hoc tests showed that the ”Jerky” condition was perceived as more aroused than the Regular one (p < 0.01).

Interaction between the independent variables
Interpretation There was an interaction between Key Pose
and Velocity of movements over the Number of Correct Interpretation (F(8, 144) = 13.15, p < 0.01, partial η2 = 1). Similarly, there was an interaction between Key Pose and Jerkiness (F(4, 72) = 2.54, p < 0.05, partial η2 = 0.69). This indicates that the interpretation of emotion depended both on
the Key Pose being displayed, on the Velocity of movement
and on the Jerkiness. This was further investigated using repeated measures ANOVAs on the different Key Pose and Ve-

1848

ation available (Key Pose, Velocity and Jerkiness) was used to
rate Arousal.

Table 3: Effect of Velocity on Valence per Key Pose Displayed
Key Pose
Sadness
Anger
Neutral
Pride
Happiness

Repeated Anovas
F(2, 36) = 0.43, p = 0.65, partial η2 = 0.02
F(2, 36) = 1.46, p = 0.25, partial η2 = 0.08
F(2, 36) = 0.86, p = 0.43, partial η2 = 0.05
F(2, 36) = 1.57, p = 0.22, partial η2 = 0.08
F(2, 36) = 10.24, p < 0.01, partial η2 = 0.36
Fast > Slow(p < 0.01)
Fast > Medium(p < 0.01)
Medium = Slow(p = 0.33)

locity conditions (Table 2). Table 2 shows that the highest recognition rate for Sadness was with Slow and Regular movements, for Anger, it was with Fast movements (no effect of
jerkiness), neutral was better interpreted with Slow and Medium speed. Pride was better interpreted at Slow and medium
speed. For Happiness, it was with Fast and Medium speed.
Valence There was a significant interaction between Velocity and Key Pose on Valence (F(8, 144) = 5.85, p <
0.05, partial η2 = 0.11). This indicates that the effect of Velocity depends on the Key Pose. This was therefore investigated
in details using 3(Velocity) Repeated Measure Anovas on the
different Key Pose individually (Table 3).
Arousal There was a significant interaction between Key
Pose and Velocity on Arousal (F(8, 144) = 5.81, p <
0.01, partial η2 = 0.24). Repeated Measures Anovas were
therefore conducted on the different Key Pose conditions separately. The results of these showed that the pattern were constant for all of them and that the Fast condition was perceived
as more aroused than the Medium condition (p < 0.01 for all
the Key Poses) which in turn was perceived as more Aroused
than the Slow condition (p < 0.01 for all the Key Pose).

Discussion
Valence and Arousal As expected, Key Pose had a strong
effect on Valence and Arousal. More precisely, the perceived
Valence and Arousal were consistent with the respective positions of each Key Pose within the Affect Space (Figures 2
and 3). Moreover, Velocity had a marginal effect on Valence.
However, the interactions between Velocity and Key Pose suggest that the difference in Valence was due to the key pose for
happiness (Table 3) as it was found that for all the other key
poses, Velocity had no effect on Valence. Similarly, Jerkiness did not affect the perceived Valence of the display. This
is consistent with existing results in psychology which suggest that Arousal is a formless cue that relates directly to the
movement kinematics while Valence seems to be related to
the relations between the different limb segments (Pollick,
Paterson, Bruderlin, & Sanford, 2001).
However, both Velocity and Jerkiness were found to increase the perception of Arousal. Taken together, the results
suggest that the perceived Valence depended on the Key Pose
displayed without taking into account the different dynamic
conditions. In contrast, the perceived Arousal depended on all
three dependent variables. Hence, participants relied only on
the body posture to assess Valence. However, all the inform-

Interpretation Participants were able to correctly identify
the different static key poses. Whilst the recognition rate for
Anger was lower than for the other key poses, it was still
above chance level. This low recognition rate could be due
to the modification done to the material as the robot was
sitting down. The key pose was better recognised in previous experiment with the robot standing up (Beck, Stevens,
Bard, & Cañamero, 2012) and the lack of significant difference between the key poses for anger and neutral on Valence
that was found in this study could be due to the key pose for
anger being misinterpreted in most of the conditions. This
will have to be investigated in future work.
Moreover, when compared with static poses, the recognition rates for the display with movements clearly show that
adding appropriate dynamic elements improves significantly
the expressivity of the key pose (Table 1). Although it was not
possible to capture this statistically, Velocity seems to have a
consistent effect on interpretation. For instance, the key pose
for sadness was interpreted as sad in slow motion (resulting
in the very high recognition rate in this condition); however,
as the Velocity increased, it shifted toward anger and frustration. This is consistent with the results found with regards to
the effect of Velocity on Valence (Table 3) which show that,
with the exception of happiness, Velocity had not effect on
Valence. Thus, these shifts in interpretation can be explained
by the effect of Velocity on Arousal. In other words, a negative expression, remains negative, but its level of Arousal
increases along with Velocity shifting from sadness to anger
and frustration. The interpretations of the key poses were affected by the Velocity and the Jerkiness of the movements.
More precisely, the dependence between Key Pose and Velocity with regards to the interpretation shows the importance
of matching the Velocity and the Jerkiness of movements to
the Key Pose in order to express specific areas of the Affect
Space. The drop in recognition for Sadness in the Jerky condition suggests the importance of regular movement for this
expression.
Even though pride was correctly labeled, the rating of
Arousal was higher than what could have been expected. This
was also the case in (Beck, Cañamero, & Bard, 2010) and
could be due to this specific posture. It could also be related
to the physical aspect of the Nao robot, as the arm joints are
very salient in this key pose.
Limitations and Future Work It is important to highlight
that the key poses used for this study are prototypical and
were intentionally selected to be expressive. This is appropriate and beneficial for the development of an expressive
system. However, it is likely that the use of prototypical
expressions had an effect on the results found in this study.
Moreover, the Jerkiness condition could have been implemented by manipulating the number of Harmonics and the
Frequency of the noise. This could result in different visual

1849

results with different effects on the perception of emotion. It
should also be noted that Perlin noise does not capture the
relationship that exists between the rotation of one joint and
another. This may result in unrealistic animations (Egges &
Magnenat-Thalmann, 2005). Although this did not seem to
be the case in this study as the material was carefully checked,
it could still have affected the results.
This study did not consider the effect of context on the
perception of the body language displayed. However, it can
be argued that interpretation of emotion is context dependent
and that changing the context could change the perception of
the expressions generated by this Affect Space. On the other
hand, work on facial expressions of emotion has shown that
at least for a few basic emotions, context is not necessary to
identify the expressed emotion. In other words, the expression of an emotion is to a certain extent independent from the
context, as evidenced by the widespread use of FACS. Similarly, the high recognition rates obtained in this study suggest
that these expressions could convey the intended emotion in
different contexts. However, people’s reaction to the emotional expression are likely to differ. This will be investigated
as part of the ALIZ-E project.

Acknowledgments
This work was funded by the ALIZ-E FP7 European Project
(Grant 248116). The opinions expressed are solely the authors’ and not necessarily those of the consortium.

References
Avizer, H., Trope, Y., & Todorov, A. (2012). Body cues,
not facial expressions, discriminate between intense positive and negative emotions. Science, 338, 1225-1229.
Beck, A., Cañamero, L., & Bard, K. (2010, sept.). Towards an
affect space for robots to display emotional body language.
In Ro-man, 2010 ieee (p. 464 -469).
Beck, A., Hiolle, A., Mazel, A., & Cañamero, L. (2010).
Interpretation of emotional body language displayed by robots. In Proc. of the 3rd int. workshop on affective interaction in natural environments (pp. 37–42). Firenze, Italy.
Beck, A., Stevens, B., Bard, K. A., & Cañamero, L. (2012,
March). Emotional body language displayed by artificial
agents. ACM Trans.. Interact. Intell. Syst., 2(1), 2:1–2:29.
Bernhardt, D. (2010). Emotion inference from human
body. Unpublished doctoral dissertation, University of
Cambridge, Computer Laboratory.
Camurri, A., Mazzarino, B., & Volpe, G. (2003). Analysis
of expressive gesture: The eyesweb expressive gesture processing library. Gesture-Based Communication in HumanComputer Interaction, 460-467. (LNAI)
Cañamero, L. (2002). Playing the emotion game with
feelix. In K. Dautenhahn, A. Bond, L. Cañamero, & B. Edmonds (Eds.), Socially intelligent agents (Vol. 3, p. 69-76).
Springer US.
Cañamero, L. (2008). Animating affective robots for social
interaction. In L. Cañamero & R. Aylett (Eds.), Animating
expressive characters for social interaction (p. 103-121).

Coulson, M.
(2008).
Expressing emotion through
body movement: A component process approach. In
L. Cañamero & R. Aylett (Eds.), Animating expressive
characters for social interaction (p. 7186). John Benjamins
Publishing Company.
De Gelder, B. (2006). Toward the neurobiology of emotional body language. Nature Reviews, Neuroscience, 7,
242-249.
Egges, A., & Magnenat-Thalmann, N. (2005). Emotional
communicative body animation for multiple characters. In
Proc. of the first int. workshop on crowd simulation (pp.
31–40). Lausanne, Switzerland.
Hartmann, B., Mancini, M., Buisine, S., & Pelachaud, C.
(2005). Design and evaluation of expressive gesture synthesis for embodied conversational agents. In Proc. 4th int.
joint conf. on autonomous agents and multiagent systems
(pp. 1095–1096). The Netherlands.
Ishiguro, H. (2005). Android science: Toward a new crossdisciplinary framework. In Cogsci-2005 worskshop: Toward social mechanisms of android science (p. 1-6).
Kleinsmith, A., & Bianchi-Berthouze, N. (2012). Affective body expression perception and recognition: A survey.
IEEE Trans. on Affective Computing, 99(PrePrints).
Kleinsmith, A., Bianchi-Berthouze, N., & Steed, A. (2011,
August). Automatic recognition of non-acted affective postures. Trans.. Sys. Man Cyber. Part B, 41(4), 1027–1038.
Lee, J.-H., Park, J.-Y., & Nam, T.-J. (2007). Emotional interaction through physical movement. In Proc. of the 12th int.
conf. on human-computer interaction (pp. 401–410). Berlin, Heidelberg: Springer-Verlag.
Nalin, M., Baroni, I., Kruijff-Korbayov, I., Cañamero, L.,
Lewis, M., Beck, A., et al. (2012). Childrens adaptation
in long-term interactions with a humanoid robot. In Proc.
of ro-man12.
Perlin, K. (1990). Noise and turbulence. Available from
http://www.mrl.nyu.edu/˜ perlin/doc/oscar.html
Pollick, F., Paterson, H., Bruderlin, A., & Sanford, A. (2001).
Perceiving affect from arm movement. Cognition, 82(2),
B51–B61.
Roether, C. L., Omlor, L., Christensen, A., & Giese, M. A.
(2009). Critical features for the perception of emotion from
gait. Vision, 9(6).
Snibbe, S., Scheeff, M., & Rahardja, K. (1999). A layered
architecture for lifelike robotic motion. In Proc. of the 9th
int. conference on advanced robotics.
Thomas, F., & Johnston, O. (1995). The illusion of life. NewYork: Abbeville Press.
Vala, M., Paiva, A., & Rui Gomes, M. (2008). Effective bodies for affective interaction. In L. Cañamero & R. Aylett
(Eds.), Animating expressive characters for social interaction (p. 87-101).
Wallbott, H. G. (1998). Bodily expression of emotion.
European J. of Social Psychology, 28(6), 879–896.

1850

