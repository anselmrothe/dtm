UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A neural network model of working memory for episodes
Permalink
https://escholarship.org/uc/item/4j8185kx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Takac, Martin
Knott, Alistair
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                        A neural network model of working memory for episodes
                                               Martin Takac (takac@ii.fmph.uniba.sk)
                                      Centre for Cognitive Science, Comenius University, Slovakia
                                                 Alistair Knott (alik@cs.otago.ac.nz)
                                 Department of Computer Science, University of Otago, New Zealand
                               Abstract                                 mechanism is likely to predate language, since apes are able
   We present a neural network model of the storage of episode
                                                                        to store episodes in long-term memory (see e.g. Schwartz
   representations in working memory (WM). Our key idea is that         and Evans, 2001). One interesting possibility is that evolu-
   episodes are encoded in WM as prepared sensorimotor rou-             tion found a new use for the buffering mechanism in linguistic
   tines: i.e. as prepared sequences of attentional and motor oper-     communication (see Knott, 2012; Takac et al., 2012). In this
   ations. Our network reproduces several experimental findings
   about the representation of prepared sequences in prefrontal         paper we present a connectionist model of WM storage which
   cortex. Interpreted as a model of WM episode representations,        supports not only language processing, but also the prelin-
   it has useful applications in an account of long-term memory         guistic role of the episodic buffer mediating transmission of
   for episodes and in accounts of sentence processing.
                                                                        episode representations to the hippocampus.
   Keywords: working memory, episodic buffer, neural network
   models of language
                                                                             WM episode representations as prepared
   Introduction: working memory for episodes                                              sensorimotor routines
The classical model of working memory (WM; Baddeley                     Our model is founded on the assumption that WM episodes
and Hitch, 1974) posits two representational media: one                 provide an interface between the sensorimotor (SM) mecha-
for visual material (the visuospatial sketchpad) and one for            nisms through which episodes are apprehended and the hip-
phonological material (the phonological buffer). Baddeley               pocampal structures in which they are stored. On this as-
(2000) revised the model to include a third medium, hold-               sumption, we expect the structure of WM episode represen-
ing semantic material—specifically, semantic representations            tations to reflect both the structure of SM processes and the
of episodes—called the ‘episodic buffer’. This medium                   structure of hippocampal representations. A strong common-
stores semantic representations of actions, or events, or sta-          ality in the structures of these two domains is sequential or-
tive propositions. Our paper is about the episodic buffer.              ganisation.
   Baddeley argues for the episodic buffer on several grounds.             SM processing is strongly sequential at certain timescales,
Some relate to models of language processing. When a sen-               because it involves sequential deployments of the agent’s sen-
tence is being generated, the message which it is to express            sory and motor apparatus. (For instance, saccades deploy the
is standardly assumed to be maintained in the speaker’s WM              agent’s fovea sequentially to targets in the world.) Ballard
(see e.g. Levelt, 1989). When a sentence is being interpreted,          et al. (1997) propose that SM processing is organised into
several theorists envisage a set of competing episode repre-            sequentially structured routines, whose atomic elements are
sentations being activated in the hearer’s WM, with one of              discrete sensory or motor actions. These actions are termed
these eventually being chosen as the winner (see e.g. May-              deictic operations, and a sequence of such actions is termed
berry and Miikkulainen, 2008). In each case we must as-                 a deictic routine. Through a case study of episodes involv-
sume a WM medium which stores semantic episode repre-                   ing reach-to-grasp actions, Knott (2012) argues that the SM
sentations. Baddeley (2000) postulates bidirectional links be-          processes through which concrete episodes are apprehended
tween the episodic buffer and the phonological buffer, to sup-          take the form of sequentially structured deictic routines.
port sentence-processing tasks. But in fact his primary argu-              The hippocampus stores associations between stimuli of
ment for the episodic buffer has nothing to do with language            many different kinds. But an emerging idea is that it is spe-
processing. This argument concerns the neural mechanisms                cially good at storing associations between sequentially struc-
through which episodes are stored in long-term memory. The              tured items (Wallenstein et al., 1998). One recent finding
long-term neural storage of an episode is widely agreed to              which strongly supports this idea is that the hippocampus
involve the hippocampus: specifically, the creation of links            actively replays sequences of representations evoked during
between hippocampal assemblies representing the various se-             SM experience (see e.g. Lee and Wilson, 2002). The key
mantic components of the episode. But associations between              result is that sequences of hippocampal place cells activated
hippocampal assemblies can only be learned if they are active           when a rat navigates a maze are replayed later when the rat
almost simultaneously, within around 100ms of one another               is asleep. (Sequences are replayed at much higher speeds,
(Abraham et al., 2002). Experiencing an episode often takes             perhaps consistent with the hippocampus’ natural dynam-
much longer than this. So we must envisage that episode rep-            ics.) Since episodes are apprehended through well-defined
resentations are initially buffered in WM, and only relayed             sequences of SM operations, and sequences appear to be a
to the hippocampus when they are complete. This buffering               natural unit of storage in the hippocampus, an interesting pos-
                                                                    1432

sibility is that WM episodes are also stored as sequences. Our        action, but also an operation to inhibit itself, so the next-most
model of WM episodes basically implements this idea.                  active assembly wins the competition at the next time point
   We make two main proposals. First, we propose that a con-          (see Rhodes et al., 2004). In competitive queueing, the rep-
crete episode is stored in WM as the sequence of SM opera-            resentation of a prepared sequence is destructively updated in
tions through which it was experienced. We suggest that the           the medium in which competition occurs. We will call the se-
order of SM operations in a deictic routine implicitly identi-        quence representations in this medium ‘dynamic’. However,
fies the roles played by participants in the observed episode.        there is also evidence that prepared sequences are represented
Specifically, the object attended to first plays the role of the      in a WM medium which is not destructively updated when a
‘proto-agent’: the entity which is most agentlike, animate            sequence is replayed. Perhaps most obviously, a given pre-
or active (Dowty, 1991), and the object attended to next is           pared sequence can be executed several times: each time, the
the ‘proto-patient’. This idea is motivated in detail in Knott        sequence representation in the dynamic medium must some-
(2012). Second, we propose that the sequence of SM opera-             how be restored from some more enduring medium. We will
tions is stored as a prepared deictic routine: i.e. as a prepared     call representations in the enduring medium ‘static’.
sequence of attentional and motor operations. Humans (in-                There is also evidence that a monkey can represent mul-
deed all primates) can prepare complex sequences of sensory           tiple alternative prepared sequences in dlPFC, in a medium
and/or motor operations. If episodes are stored as prepared           which allows competition between candidate sequences and
SM sequences, then there is a natural model of how they are           the selection of a winner. This evidence comes from a
transmitted to the hippocampus: they are simply replayed, at          study by Averbeck et al. (2006), in which monkeys were
a speed commensurate with the associative learning mecha-             trained to perform two sequences in response to two cues.
nism in the hippocampus. Naturally, in replay mode the pre-           Each day different cues were chosen to represent the two se-
pared attentional and motor operations are simulated rather           quences. Halfway through the day, the mapping from cues
than actually executed. (In fact, this proposal about the for-        to sequences was reversed, so the monkeys had to gradually
mat of WM episode representations can be seen as a way of             learn the new mapping. During this period, dlPFC assemblies
implementing ‘simulationist’ accounts of semantic represen-           could be identified representing each prepared sequence, and
tations; see e.g. Barsalou, 2008.) In summary: in our pro-            the relative activation of the two assemblies after presenta-
posal episodes are experienced as sequences, stored in WM             tion of a cue could be used to predict the sequence which the
as prepared sequences, and then replayed to the hippocampus           monkey actually performed.
where they are stored more permanently as sequences.                     In summary, the prefrontal mechanism implementing se-
                                                                      quence preparation appears to involve four distinct media.
      Representation of prepared sequences in                         There is a medium holding representations of individual op-
                      prefrontal cortex                               erations in a sequence, which encodes the context in which
                                                                      they appear. There is a medium holding distributed repre-
A bonus of the above model of WM episodes is that the neu-            sentations of whole sequences, in assemblies whose compo-
ral mechanisms supporting preparation of SM sequences have            nents encode individual actions, whose order is determined
been extensively studied, in single-cell recording experiments        by their level of activation. Sequence representations in this
in monkeys. The principal mechanisms supporting sequence              medium are destructively updated when a prepared sequence
preparation are in dorsolateral prefrontal cortex (dlPFC; see         is executed. But there is also a medium holding sequence
e.g. Barone and Joseph, 1989; Averbeck et al., 2002). Several         representations which are not destroyed. Finally there is a
schemes for encoding prepared sequences have been found.              medium in which alternative candidate sequence representa-
In one scheme, individual neurons encode specific move-               tions are active in parallel and compete with one another. If
ments in particular contexts. For instance, Barone and Joseph         episodes are stored in WM as prepared SM sequences, then
(1989) found neurons which were active when a monkey pre-             this mechanism would allow for WM episodes to be stored
pared movement A, but only when it was followed by another            and replayed, and also for alternative WM episodes to com-
movement B. In another scheme, neurons encode individ-                pete amongst one another, with the winner being selected.
ual movements, and their position in the prepared sequence
is given by their activation levels. For instance, in a monkey             A network for storing and selecting WM
preparing a sequence of three movements A B and C, Aver-
beck et al. (2002) found neurons representing each prepared
                                                                                                 episodes
action which were active in parallel, with the neuron encoding        In this section we introduce a neural network which imple-
A most active and that encoding C least active. Interestingly,        ments the sequence-preparation mechanism described above.
when the prepared sequence is executed, neurons encoding              One part of the network allows the storage and replay of expe-
specific actions are inhibited just after their associated action     rienced sequences in WM. However, another part of the net-
is produced. Averbeck et al.’s (2002) findings strongly sup-          work learns about commonly-occurring sequences, so it can
port a ‘competitive queueing’ model of sequence preparation,          make predictions about how a sequence being experienced
in which PFC assemblies encoding different actions compete            will be completed, and or about which sequences are asso-
against one another, with the winner triggering the associated        ciated with reward for the agent. (We envisage the network
                                                                  1433

being used to control the process of ‘experiencing an episode’            area (see below) to a signal-encoding SOM. This SOM has
both when the experiencer is acting and when he is watching               recurrent connections, as described in Strickert and Hammer
an external episode.)                                                     (2005): it takes as an additional input a context vector com-
   Our key aim for the network is that it learns the kind of rep-         bining the weight and the context vector of the winner at the
resentations of prepared sequences which are found in mon-                previous time point. When trained on a sequence of inputs, a
key PFC, as discussed above. However, there are also two                  recurrent SOM organises itself so that individual units encode
other design criteria. Firstly, there should be a medium in               signals occurring in particular sequential contexts, very much
which candidate SM operations compete with one another at                 like the PFC units identified by Barone and Joseph (1989).
every stage in the execution of a sequence. At any point, the                Units in the signal-encoding SOM represent signals in a
operation which an agent executes is dictated partly by what              localist way, so that alternative signals compete with one an-
is planned or expected, but also partly by bottom-up stim-                other. The winning signal at each time step is copied to
uli. We want a medium which allows competition between                    an area which is isomorphic with the recurrent SOM called
alternative operations from both these sources. Secondly, in              the dynamic episodic buffer. This area accumulates rep-
the medium holding alternative possible SM sequences, there               resentations of each signal in an input sequence, with the
must be no scope for binding errors, whereby an item belong-              first signal represented most strongly and subsequent sig-
ing to one sequence is falsely identified as part of a different          nals being stored with decreasing activation, as in the pre-
sequence. Given that this medium must represent multiple                  frontal area studied by Averbeck et al (2002). When an input
sequences simultaneously, this is a difficult requirement. To             sequence is encoded in the dynamic episodic buffer, it can
address both these criteria, a key design decision is to use              be replayed immediately by iteratively sending the dynamic
self-organising maps (SOMs; Kohonen, 1982). A SOM is a                    episodic buffer’s most active unit to the signal-encoding SOM
two-dimensional map of units fully connected to a layer of                (via the ‘WTA’ link) and then inhibiting this winning unit. To
input units. When presented with training inputs, it learns to            support repeated execution of a sequence, it can be stored in
represent input vectors as localist units in the map, but also            a static episodic buffer, which has the same structure as the
learns to represent similar inputs in similar regions of the              dynamic one, and later reloaded.
map. It thus encodes similarities between its training input                 At the highest level in the network there is another SOM
vectors even though it represents these in a localist scheme.             called the candidate episodes buffer. This area encodes the
   The architecture of our network is shown in Figure 1. The              distributed representations in the dynamic episodic buffer as
                                                                          localist units. During training it learns to represent episodes
                                                                          with similar encodings in the dynamic episodic buffer in
                                               candidate
                                               episodes                   neighbouring positions in the SOM. At every time point dur-
                                               buffer
                                                                          ing presentation of a sequence this area represents a proba-
             dynamic                                      static
                                                                          bility distribution over complete episodes. (If the network is
         episodic buffer                              episodic buffer     being used to control the agent’s own actions, this distribu-
                                                                          tion represents action sequences which lead to reward; if it
                                       store                              is being used to support observation of external episodes, it
                                       load                               represents likely action sequences.) The distribution changes
                                        WTA                               as new items arrive in the sequence and become encoded in
                                                                          the dynamic episodic buffer.
                                                                             The winning unit in the candidate episodes buffer provides
         signal-encoding SOM
                                                                          top-down activation to the static episodic buffer, through
                                                                          weights which are copies of those delivering input to the can-
                                                                          didate episodes buffer. Since the winning unit always encodes
                                                         Δt
                                                                          a complete episode, the static episodic buffer likewise always
                          aggregate SM signal context                     encodes a complete episode, but in the same distributed for-
                                                                          mat used by the dynamic episodic buffer. During presenta-
                            input SM signal
                                                                          tion of a sequence, activity in the static episodic buffer is fed
                                                                          back to the signal-encoding SOM. This top-down input, when
              Figure 1: Architecture of the network                       combined with the current context representation, produces a
                                                                          pattern of activity biased towards a representation of the next
network takes as input a sequence of SM signals at successive             SM signal. The pattern is passed back to the aggregate SM
time points, evoked in the input SM signal area. Input SM                 signal area at the next time point. Thus the aggregate area
signals can be thought of as representing either the agent’s              receives both bottom-up inputs from the input SM signal and
own actions (attentional or motor) or external stimuli in the             top-down ones from the static episodic buffer.
world (objects or observed actions).                                         We conclude by reporting some details of the network ar-
   SM input signals are fed through an aggregate SM signal                chitecture. Different SM operations are encoded in the ‘input
                                                                      1434

                                                                                               sequence fragment: DOG BALL
SM signal’ layer with 1-hot localist coding, i.e. one unit for                    activity reconstructed sequence
each possible SM operation. The ‘aggregate SM signal layer’                       0.30     DOG BALL PUSH
                                                                                  0.27     DOG BALL SEE
is isomorphic with the input layer. The signal-encoding SOM                       0.27     DOG BALL GRAB
is a 2-dimensional Merge SOM (Strickert and Hammer, 2005)                         0.26     DOG BALL KICK
                                                                                  0.25     DOG BALL HIT
with 400 units (α = 0.4, β = 0.5, constant learning rate 0.1                               sequence fragment: DOG BALL CAUSE
and Gaussian neighbourhood decreasing from 10 to 0.5).                            0.33     DOG BALL CAUSE GO
                                                                                  0.32     DOG BALL CAUSE STOP
   The static and dynamic episodic buffers are both isomor-                       0.32     * DOG BALL CAUSE GO CAT BALL CAT CAUSE GO
phic with the signal-encoding SOM, i.e. have 400 units each.                      0.29     DOG BALL CAUSE HIDE DOG NEAR
                                                                                  0.29     DOG BALL CAUSE HIDE MAN UNDER
Experiencing a sequence of SM operations creates a tempo-
ral pattern of active units in the signal-encoding SOM. This         Table 1: Probability distributions of episodes predicted in the
pattern is recorded in the dynamic episodic buffer as a ‘trace’      candidate episodes buffer from two initial sequences. (The
of the isomorphic units with exponentially decaying activity         asterisk denotes an ‘ill-formed’ episode representation.)
(the nth unit in the sequence has activity δn−1 where δ = 0.8
and all unused units have zero activity). To prevent confusion
of elements in the trace, we force the signal-encoding SOM           work in three ways (all tests were repeated for the 10 different
to select a new winner in each step of the sequence (i.e. win-       simulation runs and averaged).
ners from previous steps of this sequence are excluded from
competition). After completing the whole sequence, the 400-          Immediate serial recall The basic requirement for our net-
dimensional vector representing its trace serves as a training       work is that it can store and replay individual behavioural
input to the candidate episodes buffer, which is a standard          sequences. This capability relies on interactions between
SOM with 900 units, constant learning rate 0.9 and Gaussian          the signal-encoding SOM and the dynamic episodic buffer.
neighbourhood decreasing from 10 to 0.5.                             We presented the trained network with 200 sequences of in-
   Once a winner is selected in the candidate episodes buffer,       put signals: 100 taken from the training data and 100 new
activity is propagated back through the network, a process we        ones not seen before. Each sequence was coded in the dy-
call ‘top-down reconstruction’. This process uses the prop-          namic episodic buffer; then the signal-encoding SOM’s con-
erty of SOMs that the memory of each unit is in its weights.         text was reset and the winning unit in the dynamic buffer
During reconstruction, the weights of the winning unit in the        was iteratively sent to the SOM and then inhibited. 99.4%
candidate episodes buffer are copied back to the static and          (SD=0.49%) of training sequences were correctly replayed,
then dynamic episodic buffer. Destructive iterative updating         and 98.6% (SD=0.92%) of unseen sequences.
of the dynamic episodic buffer causes a temporal sequence
of activations of units in the signal-encoding SOM, which in         Predicted completions of sequences The network is also
turn project their weight vectors back to the aggregate SM           designed to generate top-down predictions about sequences,
signal layer where they represent top-down expectations.             through activity in the candidate episodes buffer. The pre-
                                                                     diction is actually a retrieval of the most similar past episode
                Experiments and results                              as remembered in the weights of this buffer. The weights of
Training We trained the model on sequences of SM oper-               the winning candidate are copied to the static episodic buffer
ations, representing the SM routines through which different         and replayed in the signal-encoding SOM where they gener-
episodes are experienced. The SM sequences were built from           ate top-down biases for SM elements. To test this ability, we
35 SM operations, e.g. MAN SNEEZE (intransitive episode),            exposed the trained network to the 500 sequences encoun-
MAN CUP GRAB (transitive), MAN WALK HOUSE INTO (in-                  tered during training element by element, and examined the
transitive with PP complement), MAN CUP CAUSE BREAK                  prediction about the possible completion of the sequence. At
(simple causative) and DOG BONE CAUSE ROLL TABLE UN -                the beginning of an exposure, after seeing a short fragment of
DER (causative with PP). (For detailed justification of the or-      an episode, its completion is inherently ambiguous, as there
derings in these sequences, see Knott, 2012.) We repeated            may be many possible continuations (see Table 1). Later the
each simulation 10 times with different random initializa-           number of candidates narrows down and the prediction can be
tions of connection weights in the model and different train-        more accurate.1 We can evaluate the retrieval from fragments
ing sets (stochastically generated by the same set of transcrip-     of an episode of various lengths, up to complete episodes.
tion rules). Each training set consisted of 500 sequences, out       The results are summarized in Table 2.
of which on average 13.1 were of length 2, 86.4 of length               Note also that the network is not confused by sequences
3, 126.1 of length 4 and 274.4 of length 6. Sequences could          containing duplicate items. A regular competitive queueing
contain duplicates: in all, 19.1% of sequences contained two         network has problems representing duplicate items, because
copies of a single signal and 0.9% contained 3. The training         after the first instance of the item is presented it is inhibited
took 200 epochs; in each epoch the training sequences were           in the competitive medium. But since the dynamic episodic
presented in random order and the Merge SOM context was                 1 The average fragment length necessary to predict the whole se-
reset after each sequence. After training we tested the net-         quence correctly was 77.4% (SD=0.7%).
                                                                 1435

     Fragment length  0-25%   25-50%   50-75% 75-100%   100%
     Matches (avg)    0.0%    0.1%     26.0%  92.0%     94.2%
                                                                                         Summary and discussion
     Matches (SD)     0.0     0.1      1.2    2.8       2.9             This paper contains two main proposals. Most concretely, we
                                                                        propose a network model of WM for behavioural sequences.
Table 2: Percentage of correct sequence completions from                We also propose a more far-reaching idea: that episodes
fragments of different relative length.                                 are represented in semantic WM as prepared behavioural se-
                             MAN 3
                                                                        quences. Specifically, we propose our model of prepared se-
                                                                        quences as a model of the episodic buffer argued for cogently
            MAN 1
                     MAN 5                                              by Baddeley (2000). We now assess these proposals.
            MAN 4            MAN 2   MAN 6
                                                                        WM for sequences There are numerous network models
Figure 2: Position of the winning unit in the signal-encoding           of WM for sequences. However, most of these are explic-
SOM for occurrences of the SM signal MAN in six different               itly models of phonological WM. We follow Baddeley (2000)
contexts. (Only a fragment of the 20x20 SOM is shown.)                  in distinguishing between phonological WM and WM for
                                                                        episodes. This means our model does not directly compete
buffer receives inputs from the signal-encoding SOM where               with the best-known models of WM for sequences, for in-
we forced a unique winner selection, different instances of a           stance Burgess and Hitch (1999). It does not have to repro-
given input are represented differently, and it does not suffer         duce the classic effects found in immediate recall of phono-
from this problem. To verify this, we tested the prediction             logical sequences, such as primacy and recency effects. Em-
on a set consisting of 95 sequences with 2 repeating elements           pirically, our focus is on modelling the neural sequence-
and 5 sequences with 3 repeating elements and the results               preparation mechanisms found in monkeys, which it does
were similar to those presented above (the average success in           quite successfully. There are some computational models
prediction from fragments of more than 75% of the sequence              which propose the same mechanism both for phonological
length was 91.8% (SD=3.5%).                                             WM and prepared action sequences—in particular Rhodes et
                                                                        al. (2004). We certainly envisage similarities between the
                                                                        mechanisms subserving these tasks. (In particular they both
Relation to neural activation data As discussed above,
                                                                        appear to involve competitive queueing.) But our suggestion
PFC stores prepared sequences in several different ways. We
                                                                        is that they are separate, although, as Baddeley suggests, there
examined the properties of representations in the trained net-
                                                                        are links between them, which support sentence processing.
work to see how they corresponded to representations identi-
                                                                        We will discuss some ideas about these links below.
fied in monkey PFC.
   Some PFC units encode individual operations in a prepared
sequence in a way which takes into account their sequential             Episode representation As a model of representation of
context (see e.g. Barone and Joseph, 1989). Inspecting units            episodes in WM, our network is just a first step. An ob-
in the signal-encoding SOM shows that they have this prop-              vious issue for discussion is our localist representation of
erty. We presented the trained signal-encoding SOM with                 episodes in the candidate episodes buffer. Since episode rep-
five input sequences featuring six instances of the signal MAN          resentations can have other episode representations nested
in different serial positions. The SOM unit which represents            within them, it is clearly infeasible to have a single assem-
MAN is different in each case, as shown in Figure 2.                    bly in this medium for each possible episode. However, we
   Some PFC units encode individual operations in a prepared            should distinguish episode representations from sentence rep-
sequence in a format where relative activation levels indi-             resentations. Our conception of epsiodes as stored SM se-
cates the serial order in which operations will be executed             quences means that there are several kinds of nestedness in
(see Averbeck et al., 2002). Of these units, some have ac-              sentences which we do not have to model declaratively. For
tivity which changes dynamically during execution of a pre-             instance, to model The dog [which chased Mary] barked we
pared sequence, being maximal before execution of the action            can initially rehearse just the matrix episode The dog barked:
they encode and being inhibited thereafter. Others are invari-          when dog is activated we can temporarily evoke the subordi-
ant during execution of a planned sequence. Units in the dy-            nate episode The dog chased Mary in the candidate episodes
namic episodic buffer have the former property, and units in            buffer, so it can be rehearsed, and then inhibit it, so the matrix
the static episodic buffer have the latter property.                    episode once again becomes dominant. This device of inter-
   Finally, some areas of PFC provide a medium in which al-             rupting processing is not available to schemes which repre-
ternative prepared sequences can compete against one another            sent episodes declaratively in a static pattern of neural activ-
(Averbeck et al., 2006). The candidate episodes buffer acts as          ity: we see this as a strong advantage of representing episodes
such a medium. Table 1 shows the five most active candidates            as sequences. Sequentially structured episode representations
in the candidate episodes buffer as a response to the presen-           also permit an interesting representation of nested sentential
tation of DOG BALL and DOG BALL CAUSE fragments.2                       complements; see Caza and Knott (2012).
    2 Candidates   were determined by top-down reconstruction, i.e.     replayed as a temporal sequence in the aggregate SM signal layer.
                                                                    1436

Sentence processing As regards sentence processing, the                prefrontal cortex during dynamic selection of action se-
network can be extended in several interesting directions.             quences. Nature Neurosceience, 9(2), 276–282.
These all enlarge on Baddeley’s (2000) proposal that sentence        Baddeley, A. (2000). The episodic buffer: A new component
processing involves interactions between two separate WM               of working memory? TICS, 4(11), 417–423.
buffers, one for phonological material and one for episodes.         Baddeley, A. and Hitch, G. (1974). Working memory. In
   There is a natural way of extending the network to sup-             G. Bower, editor, The psychology of Learning and Motiva-
port sentence generation. A detailed model of sentence gen-            tion, pages 48–79. Academic Press.
eration incorporating the current model of WM episodes is            Ballard, D., Hayhoe, M., Pook, P., and Rao, R. (1997). Deic-
given in Takac et al. (2012). In this model, generating a              tic codes for the embodiment of cognition. Behavioral and
sentence involves replaying a WM episode stored as a pre-              Brain Sciences, 20(4), 723–767.
pared sequence, in a special mode where SM signals can trig-
                                                                     Barone, P. and Joseph, J.-P. (1989). Prefrontal cortex and spa-
ger learned articulatory motor plans. During this replay pro-
                                                                       tial sequencing in macaque monkey. Experimental Brain
cess, an interesting mixture of sustained and transient signals
                                                                       Research, 78, 447–464.
is evoked: in particular, there are tonically active represen-
tations of each action in the planned sequence in the static         Barsalou, L. (2008). Grounded cognition. Annual Review of
episodic buffer throughout the replay process. These tonic             Psychology, 59, 617–645.
representations permit a neat account of the extended syntac-        Burgess, N. and Hitch, G. (1999). Memory for serial order:
tic domain of verbs. Verbs can appear at various different             A network model of the phonological loop and its timing.
positions in the structure of a clause, and they can carry in-         Psychological Review, 106, 551–581.
flections signalling agreement with arguments at distant po-         Caza, G. and Knott, A. (2012). Pragmatic bootstrapping: A
sitions in the clause (for instance subjects). The neural basis        neural network model of vocabulary acquisition. Language
for this non-locality is currently a complete mystery. But if          Learning and Development, 8, 1–23.
sentences are produced by replaying a prepared SM routine,           Dowty, D. (1991). Thematic proto-roles and argument selec-
and if verbs and their inflections are produced from planned           tion. Language, 67(3), 547–619.
motor and attentional action representations which are toni-         Knott, A. (2012). Sensorimotor Cognition and Natural Lan-
cally active during replay, we have a promising explanation            guage Syntax. MIT Press, Cambridge, MA.
of this non-locality: the semantic representations from which
                                                                     Kohonen, T. (1982). Self-organized formation of topolog-
inflected verbs are generated are active throughout the gener-
                                                                       ically correct feature maps. Biological Cybernetics, 43,
ation process, and can be produced at any time.
                                                                       59–69.
   The WM episode network also has interesting uses in mod-
els of sentence interpretation. Neural models of sentence in-        Lee, A. and Wilson, M. (2002). Memory of sequential experi-
terpretation take sequences of words as input, and use various         ence in the hippocampus during slow wave sleep. Neuron,
types of recurrent network to produce output semantic repre-           36, 1183–1194.
sentations. Such a network could deliver episode representa-         Levelt, W. (1989). Speaking: From Intention to Articulation.
tions directly to the candidate episodes buffer. After training,       MIT Press, Cambridge, MA.
this buffer would activate a distribution of possible sentence       Mayberry, M. and Miikkulainen, R. (2008). Incremental non-
meanings and a winner could be picked. In our network, this            monotonic sentence interpretation through semantic self-
winner could then be simulated as a SM sequence, in line               organization. Technical Report AI08-12, Department of
with embodied theories of meaning.                                     Computer Sciences, The University of Texas at Austin.
                                                                     Rhodes, B., Bullock, D., Verwey, W., Averbeck, B., and
                    Acknowledgements                                   Page, M. (2004). Learning and production of movement
This research was supported by a VEGA grant (1/0439/11)                sequences: Behavioral, neurophysiological, and modeling
and a SAIA travel grant (both for Martin Takac). We are                perspectives. Human Movement Science, 23, 699–746.
grateful to Lubica Benuskova for helpful discussions.                Schwartz, B. and Evans, S. (2001). Episodic memory in pri-
                                                                       mates. American Journal of Primatology, 55(2), 71–85.
                         References                                  Strickert, M. and Hammer, B. (2005). Merge SOM for tem-
                                                                       poral data. Neurocomputing, 64, 39–71.
Abraham, W., Logan, B., Greenwood, J., and Dragunow, M.
   (2002). Induction and experience-dependent consolidation          Takac, M., Benuskova, L., and Knott, A. (2012). Mapping
   of stable long-term potentiation lasting months in the hip-         sensorimotor sequences to word sequences: A connection-
   pocampus. Journal of Neuroscience, 22, 9626–9634.                   ist model of language acquisition and sentence generation.
                                                                       Cognition, 125, 288–308.
Averbeck, B., Chafee, M., Crowe, D., and Georgopoulos, A.
                                                                     Wallenstein, G., Eichenbaum, H., and Hasselmo, M. (1998).
   (2002). Parallel processing of serial movements in pre-
                                                                       The hippocampus as an associator of discontiguous events.
   frontal cortex. PNAS, 99(20), 13172–13177.
                                                                       Trends in Neurosciences, 21, 317–323.
Averbeck, B., Sohn, J., and Lee, D. (2006).         Activity in
                                                                 1437

