UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Is Lexical Access Driven by Temporal Order or Perceptual Salience? Evidence from British
Sign Language
Permalink
https://escholarship.org/uc/item/45k7h1vf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Thompson, Robin L.
Vinson, David P.
Fox, Neil
et al.
Publication Date
2013-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

Is Lexical Access Driven by Temporal Order or Perceptual Salience? Evidence from
                                                     British Sign Language
                                        Robin L. Thompson (robin.thompson@ucl.ac.uk)
                                                David P. Vinson (d.vinson@ucl.ac.uk)
                                                    Neil Fox (neil.fox@ucl.ac.uk)
                                             Gabriella Vigliocco (g.vigliocco@ucl.ac.uk)
         Deafness, Cognition and Language Research Centre, Department of Cognitive, Perceptual and Brain Sciences
                              University College London, 26 Bedford Way, London, WC1H 0AP, UK
                             Abstract                                    For spoken languages, it is generally uncontroversial that
                                                                       information is processed almost immediately as it comes in
   While processing spoken language, people look towards
  relevant objects, and the time course of their gaze(s) can           (e.g., Rayner & Clifton, 2009). Such incremental moment-
  inform us about online language processing (Tanenhaus et al,         by-moment language processing is likely necessary to keep
  1995). Here, we investigate lexical recognition in British Sign      up with the incredibly fast rate of speech input (estimated to
  Language (BSL) using a visual world paradigm, the first such         be between 150-190 words per minute, Marslen-Wilson,
  study using a signed language. Comprehension of spoken               1973). However, during incremental processing listeners,
  words and signs could be driven by temporal constraints              processing even a single word, are faced with many possible
  regardless of modality (“first in, first processed”), or by          alternatives that match the current acoustic-phonetic input.
  perceptual salience which differs for speech (auditorialy            Empirical evidence suggests that instead of waiting until
  perceived) and sign (visually perceived). Deaf BSL signers
                                                                       temporary ambiguities are resolved, partial activation of
  looked more often to semantically related distracter pictures
  than to unrelated pictures, replicating studies using                possible words (i.e., lexical competitors) that match current
  acoustically-presented speech. For phonologically related            phonological information proceeds, with potential words
  pictures, gaze increased only for those sharing visually salient     being eliminated across time as more information becomes
  phonological features (i.e., location and movement features).        available (e.g., McClelland and Elman, 1986; Gaskell &
  Results are discussed in the context of language processing in       Marslen-Wilson, 1997).
  different modalities. Overall, we conclude that lexical                 Evidence for incremental activation of lexical competitors
  processing for both speech and sign is likely driven by              during spoken language processing comes from the “visual
  perceptual salience and that potential differences in processing     world” paradigm (language presented simultaneously with
  emerge from differences between visual and auditory systems.
                                                                       related pictures; Allopena, Magnuson, & Tanenhaus, 1998;
                                                                       Altman & Kamide, 2004; Huettig & Altmann, 2005; Yee &
  Keywords: lexical access; sign           language;    semantics,     Sedivy, 2006). For example, in Allopena et al. (1998),
  phonology, visual world; modality                                    subjects heard an utterance like “Pick up the beaker” while
                                                                       viewing a display with four pictures including: 1) an object
                         Introduction                                  matching the noun (the target; e.g. “beaker”), 2) an object
General theories of language processing have developed on              with a name beginning with the same phoneme (e.g.
the basis of extensive data from spoken, but not signed                “beetle”), 3) an object with a name sharing the same rhyme
languages, making it impossible to tease apart those aspects           (e.g., “speaker”) and, 4) an unrelated object (e.g., carriage).
of language processing that are truly general from those               The probability of fixating the target and onset competitor
dependent on the oral-aural language modality. While                   were identical immediately after word onset (when the two
spoken language processing happens through aural                       could not be distinguished from each other), and fixations to
perception of sounds, sign language processing occurs                  these picture types were higher than fixations to the rhyme
through visual perception which allows for more                        or unrelated competitors. Immediately after reaching a
simultaneous input of information; spoken languages make               phoneme differentiating the target and onset competitor, the
use of mouth and vocal tract, while signed languages use               probability of fixating the target rose sharply while the
slower manual articulators (hands, as well as eyes, mouth              probability of fixating the related competitor fell. A weaker,
and body). An understanding of the processing differences              but significant effect was also observed for rhyme
that arise from these differing language modalities is critical        competitors compared to unrelated competitors, indicating
for understanding the interaction of language processing               that activation is not restricted to words sharing onsets but is
with other cognitive systems such as perception and action.            continuous (see for example McClelland and Elman, 1986).
Here we take advantage of these physical differences in                   A question of interest, then, is why words that share
language processing for signed languages compared to                   onsets make the strongest lexical competitors. One
spoken languages to investigate the nature of lexical                  possibility is that strong activation of onset competitors
processing and lexical access.                                         compared to word rhymes is due to temporal considerations:
                                                                       i.e., word onsets occur earlier in time. This view about the
                                                                   1450

activation of onset competitors can be called a ‘first in, first       used a picture sign interference task (subjects named a
processed’ account. However, onsets also tend to be salient,           picture in ASL while trying to ignore a superimposed image
particularly in languages such as English (used in the                 of a related distracter) and found that distracter signs sharing
majority of visual world studies) in which stress has the              both movement and location with the target sign resulted in
effect of lengthening the first syllable as well as adding both        significant facilitation effects at all stimulus onset
intensity and pitch change: all of which serve to make the             asynchronies (-130, 0, 130 ms), while signs sharing
first part of a word more salient. Evidence that stress is             handshape and location, or handshape and movement
important to lexical access comes from Reinisch, Jesse, and            features did not affect picture naming.
McQueen (2010). In a visual world study they found that
participants use lexical stress information to direct eye gaze
such that upon hearing a word with initial stress (e.g.,
octopus) fixations on printed target words with first-syllable
stress (e.g., octopus) were more frequent than fixations on
differently stressed competitors (e.g., October, with stress
on the second syllable). Thus, an alternate account of the
strong activation of onset competitors observed in visual
world studies is that word onsets are the most auditorily
salient part of a word and that auditory salience drives
lexical access for processing efficiency. However, because
spoken word onsets tend to be both temporally early and
auditorily salient, it is difficult to tease apart these alternate
accounts based on previous studies.
   Interestingly, unlike spoken words, for visually processed
signs there is evidence that the phonological features that
form the onset of a sign (i.e., the first features to be formed
as a sign moves through time) may not coincide with the
most visually salient features (i.e., the features that can be
seen most easily, for example, under visually noisy
conditions). Just as in spoken languages, signed languages
have sub-lexical units (phonological features) that combine
in rule-governed ways to form words/signs. Signs are made
up of phonological features from three major parameters
(handshape, movement, and location [place of articulation];
see Sandler & Lillo-Martin, 2006 for discussion, and Figure
1 for examples of signs sharing these features). In terms of
sign onsets, results from early gating studies (single frame
presentation of a sign, with subsequent presentations
increasing in length; Grosjean, 1981, Emmorey & Corina,
1990) suggest that handshape and location features are                    Figure 1: Examples of phonological minimal pairs in
recognized first across time. In Emmorey & Corina (1990)               BSL. Top: car and robot share location and movement (up
subjects’ initial responses tended to share the handshape and          and down) parameters, but differ in handshape. Middle:
location of the target sign but differed in movement                   saxophone and computer share handshape and movement
features. Once the movement of the sign was identified, the            (finger wiggle) features, but differ in location. Bottom:
target sign also tended to be identified. The authors suggest          mouse and nose share handshape and location features, but
that lexical recognition in a signed language is a two-stage           differ in movement (mouse, with a twisting movement and
process such that handshape and location are identified                nose with a tapping movement).
almost from the start of the sign (i.e., form the onset of the
sign) followed by movement which coincides with sign                      While location features are available early in sign
recognition.                                                           perception, movement features only emerge later and are
   In terms of sign salience, Corina & Hildebrandt (2002)              therefore crucial in teasing apart whether lexical access (at
used a sign similarity judgement task and found that                   least for signs) is driven by temporal constraints or by
subjects preferred to pair non-signs with other non-signs              perceptual salience. If temporal constraints drive lexical
sharing location and movement features more frequently                 access in sign, signers should pay attention to handshape
than pairing non-signs with matching handshape and                     and location features which are available at the start of a
location features, or handshape and movement features,                 sign and ignore movement features which emerge later.
suggesting that they are paying attention to these feature             Movement features have been argued to be the most
pairings. Further support for the salience of movement and             sonorous or salient part of a sign (Perlmutter, 1992). Thus,
location features is found in Corina & Knapp (2006) who
                                                                   1451

if perceptual salience is instead key to lexical access, then       Absent” trials (n=28), three unrelated distractor pictures
signers may pay attention to movement features.                     with no semantic, phonological or visual relationship to the
   Here we investigate lexical recognition in BSL using a           target sign were presented along with a related distracter
visual world paradigm and asking whether or not the nature          picture. Related distracter pictures had signs that were either
of access in a dynamic visual language is also incremental          semantically (e.g., target: banana, distracter: strawberry,
and graded with alternate possible words considered                 target: zipper, distracter: button) or phonologically related to
simultaneously over the time-course of processingFurther,           the target. Phonologically related pictures were minimal
we consider the nature of activation of lexical competitors         pairs that shared two out of three parameters (see Figure 1
(if any) and whether sign access supports a first in, first         for examples). Semantically related distracter pictures were
processed pattern, or a pattern driven by visual salience.          not phonologically related to the target, and phonologically
   We include two critical conditions. First, a semantic            related pictures were not semantically related.
condition will determine if a visual world paradigm can be a
successful methodology using sign language which must be
presented visually. Previously subjects have been found to
look towards semantically related competitor pictures
during spoken language visual world studies (Huettig, &
Altmann, 2005). Here we explore whether eye movements
are drawn to a semantically related object in a signed visual
world in the absence of a phonological or visual
relationship. The Visual World paradigm has never been
used with sign language stimuli and a semantic condition
(see methods) serves as our test case, under the assumption
that semantic relationships should hold regardless of
language. If the visual world methodology is successful
with BSL, we should expect subjects to look more                    Figure 2: Example of a single trial. Areas of interest for
frequently to distracter pictures that are semantically related     gaze analyses were set directly around the (250x250 pixels)
to a given target sign. Secondly, we examine the nature of          pictures and the (320x240 pixels) video.
sign recognition in real time using pictures that have
phonologically related signs. If temporal information is            Procedure
most important, and signers process information primarily
                                                                    After giving consent to participate, subjects were presented
through sequential, incremental, first-in first-processed
                                                                    with video-recorded instructions in BSL (signed by N.F., a
order, then signs sharing handshape and location features
                                                                    native BSL signer) and invited to ask clarification questions.
should be particularly salient for them. Alternatively, if
                                                                    Subjects were then fitted with a head-mounted eye-tracker
perceptual salience is more relevant then signers may
                                                                    (SR Research, EyeLink II) and initial calibration was
instead look more frequently to distracters that share
                                                                    performed (9 fixation points). Subjects were seated 50 cm
movement and location features.
                                                                    from the monitor with the tracker positioned in front of the
                                                                    right eye. There were four practice trials before the
                            Method                                  experiment began. Another calibration check was performed
                                                                    after these practice items and then again after every 36 trials
Subjects                                                            (the final set had only 35 trials), at which time subjects took
24 Deaf signers (13 women, 11 men, mean age 34.8) were              a self-paced break (total 107 trials, 3 sets). Additionally,
recruited from deaf communities in England and took part in         drift correction on a single centrally located fixation point
the experiment. Of these, eleven were native signers (born          was performed at the start of each trial. Responses were
to deaf signing parents), four began signing by the age of          recorded using a hand-held joypad with buttons that can be
five (early signers) and 9 learned BSL after age five. All          located tactilely without the need to look at keys. The entire
subjects use BSL as their preferred and primary language.           experiment (with instructions and calibration) took
                                                                    approximately 20 minutes to complete. In order to ensure
Materials                                                           that all pictures were familiar to the subjects as well as to
                                                                    obtain naming data, subjects named all of the pictures used
For each trial, four pictures of objects were presented
                                                                    in the visual world experiment before we began.
simultaneously with a centrally located video clip (see
                                                                      The location of the pictures was balanced so that each
Figure 2). In each video clip, a native BSL signer produced
                                                                    picture type (related distracter, unrelated distracter [filler])
the carrier phrase, “I see…”, followed by the target sign.
                                                                    occurred a roughly equal number of times in each location
Subjects were asked to indicate (with button press, “yes” or
                                                                    within a given condition. Additionally, we created two sets
“no”) as quickly and accurately as possible whether the
                                                                    of stimuli such that half the subjects saw any one picture in
target BSL sign matched one of the pictures. “Target
                                                                    one location and half of the subjects saw it in a different
Present” trials (n= 79) in which a picture of the target sign
                                                                    location. The order of trial presentation was randomized
was present constituted our fillers. On critical “Target
                                                                1452

throughout. Pictures were presented simultaneously with the         target sign was not yet produced during the early period,
sign video.                                                         gaze could not yet be informed by the target. The late period
                                                                    was defined as the period from the start of the target sign
                            Results                                 until the button was pressed. Gaze during the late period
                                                                    should provide information about processing of the target
  First we analyzed signs produced during picture naming            sign.
to ensure that signs for target and related pictures in the           In the first set of gaze analyses across the different
phonological conditions were indeed phonologically related          pictures, we tested whether subjects looked longer at related
in subjects’ lexicons. Individual trials were excluded when         pictures than unrelated pictures in the late time period, once
subjects produced a sign (for either target or related              the meaning of the target sign could be processed. We
pictures) that did not have the intended phonological               conducted hierarchical linear regressions, treating subjects
relationship (6%). Error trials, in which participants              and target signs as random effects, including picture
mistakenly indicated that the target sign matched a picture         relatedness (considering only related vs. unrelated pictures)
on the screen (12.4%) were also excluded from analyses of           and time period (early vs. late) as predictors, and dwell time
response latencies and eye gaze. The number of trials by            (in milliseconds) as the dependent measure. Separate
condition along with average correct response latencies and         models were fit for each relatedness condition (semantic,
percent of correct answers across different conditions are          location-movement, handshape-movement, handshape-
reported in Table 1. A one-way repeated measures ANOVA              location)2. Across all conditions there was a main effect of
by subjects revealed no significant differences for accuracy        time period indicating longer gaze overall in the late period:
between conditions: F(3,69)=1.686, p=.178. However, a               semantic (95% CI [183.7, 221.1], pMCMC <.001); location-
significant difference was found between conditions for             movement (95% CI [134.0, 199.3], pMCMC <.001);
response latencies (F(3,66)1=3.202, p=.029). Post-hoc tests         handshape-movement (95% CI [135.8, 190.0], pMCMC
revealed that responses were slower for handshape-                  <.001); and handshape-location (95% CI [121.3, 172.4],
movement trials than other conditions.                              pMCMC <.001). The main effect of picture relatedness was
                                                                    not significant in any of the four conditions (all pMCMC >.67).
Table 1. Average correct response time (standard deviation            The crucial effect is the interaction between picture
by subjects in brackets) and percent correct as a function of       relatedness (related vs. unrelated) and time period (early vs.
relatedness type. Sem: related picture sharing a semantic           late) on dwell times, as increased looks to related pictures
relationship to the target; HS-MV: related picture sharing          should only start to occur once the target sign is being
the handshape and movement of the target sign; LOC-MV:              produced. For semantic trials, the picture by time period
sharing the location and movement of the target sign; HS-           interaction was significant (95% CI of relative increase for
LOC: sharing both the handshape and location of the target          related pictures in the late period [63.5, 107.9], pMCMC
sign.                                                               <.001) reflecting longer gaze to related compared to
                                                                    unrelated pictures in the later time period (that is, after the
                                                                    carrier phrase was complete and the target sign was being
                     Items         RT(SD)            %Correct
                                                                    produced). A significant interaction of picture by time
     SEM             n=11        2792 (462)           88.3
                                                                    period was also observed in location-movement trials (95%
     LOC-MV          n=5         2730 (421)           91.1
                                                                    CI of relative increase for related pictures in the late period
     HS-MV           n=6         2887 (421)           87.9
                                                                    [20.9, 96.6], pMCMC =.001), again reflecting longer gaze to
     HS-LOC          n=6         2662 (446)           83.2
                                                                    related than unrelated pictures in the later time period when
                                                                    information about the target sign becomes available.
  Five areas of interest within each time period were               However, for the other two phonological conditions
identified: the location of the signer in the middle of the         (handshape-movement           and     handshape-location)       the
screen (displayed as video), and one corresponding to each          interaction of picture and time period did not reach
of the pictures displayed (coded as target, related, unrelated      significance (both pMCMC >.3).
and matching in size to the actual pictures). The dependent           We next conducted a Wilcoxon signed-rank test
measure of interest was dwell time (summed gaze duration            comparing looks to related and unrelated distracter pictures
in a given area, measured in milliseconds). Not surprisingly,       to explore possible differences in gaze across time,
across all trial types, gaze was primarily directed to the          beginning at target sign onset. Cumulative fixations,
signer in the video (M=86.9%). This led to fewer looks              analyzed as arcsine transformations, were grouped into
towards pictures than would be expected in a study with
auditory stimuli, so we started with a broad analysis.                 2
                                                                          We fit separate models for the different phonological
Specifically, for each trial, we identified two time windows.       relatedness conditions because a combined model revealed a
The early period, began at the start of the trial and ended         significant interaction between relatedness, time period and type of
when the carrier phrase "I see…" finished. Because the              phonological relation. Using location-movement as a reference
                                                                    condition, the 95% CI of the change in relatedness × time period
                                                                    interaction coefficient was (11.8, 103.8) for handshape-movement
   1
       Reduced df is due to empty cells               for  some     (pMCMC =.040), and (-1.7, 94.3) for handshape-location (pMCMC
participant/condition combinations in this analysis.                =.064).
                                                                1453

100ms bins starting from 400 ms after the target onset and             In the semantic condition, subjects looked at semantically
continuing until 1000 ms (see Figure 3 for time course              related distracter pictures more frequently than unrelated
plots). 100ms bins were used to ensure the presence of              pictures, the first time such findings have been demonstrated
sufficient fixations to each area of interest during each time      for a signed language. This result is predicted under the
period for statistical analyses. Additionally, analyses began       view that activation of semantically related lexical
at 400ms after the start of the target period because during        competitors should not be affected by the modality in which
the first 300 milliseconds of the target period across all          a language occurs. The results from the semantic condition
trials, subjects fixated the sign video almost exclusively. For     reveal that despite the need for split visual attention to both
semantic trials, cumulative gaze toward related pictures            visual linguistic stimuli and pictures related to that stimuli,
differed significantly from the unrelated pictures across all       it is possible to investigate sign language processing using
bins from 400-1000ms (range of Z from -2.20 to -3.59, all           visual world and related paradigms.
p<.03). This same pattern of results was observed for                 The results from the three phonological conditions pairing
location-movement trials (range of Z from -2.31 to -3.63, all       different phonological parameters produced differing
p<.02). There was no difference between related and                 results. Phonological competitors that shared information
unrelated picture gaze for handshape-location trials across         occurring at the onset of the sign (handshape and location
all bins (all p> .24). However, there were significantly more       features) did not draw more looks either at the onset of the
looks to related pictures compared to unrelated pictures for        period in which the target sign was produced (as evidenced
the handshape-movement condition, but this difference was           by our analysis of the time period from 400-1000ms after
found only from 800ms-1000ms (p<.05, between 800-                   the target sign onset) or during the entire time period from
100ms; all p>.2 up to 800 ms).                                      the target sign onset until a button press decision was made
                                                                    (the late time period). This finding suggests that onsets may
                                                                    not be as relevant to sign language processing as has been
                                                                    suggested for spoken language processing (e.g., Gaskell &
                                                                    Marslen-Wilson, 1997).
                                                                      Crucially, in the location-movement condition, subjects
                                                                    looked significantly more toward the phonologically related
                                                                    picture than unrelated pictures in the late time period. This
                                                                    finding parallels the Corina and Knapp (2006) study that
                                                                    found effects only for signs sharing location and movement
                                                                    features. Further, for location-movement trials, looks to the
                                                                    related and unrelated pictures differed significantly from
                                                                    400ms after the onset of the target sign, a time comparable
                                                                    with that found in spoken language studies (e.g., Allopena et
                                                                    al, 1998). Finally, competitor pictures that shared handshape
                                                                    and movement features with the target did not draw more
                                                                    looks than unrelated pictures during the late time period.
Figure 3: Time course of eye gaze from onset of the target          However, there was a significant, but short-lived difference
sign for 1000ms for target-absent trials across the four            such that looks to related and unrelated pictures differed
conditions (from left to right: semantic, location-movement,        between 800-1000ms after the start of a target sign. Because
handshape-location, handshape-movement).                            a difference between related and unrelated pictures was not
                                                                    observed in the overall late period analyses we conclude
                                                                    that, while subjects may be aware of the phonological
                          Discussion                                similarity of signs sharing handshape and movement
   Overall, we found both semantic and phonological effects         features, they are likely not making use of these feature
during online processing of BSL using a visual world                pairs during online processing. Instead, looks to related
paradigm. Once information about the target sign became             pictures occurring between 800 and 1000 ms (relatively late
available, subjects looked at related pictures longer than          after the onset of the target sign) appears to be a post-lexical
unrelated pictures during the semantic condition. During the        effect in which subjects consider alternate competitor
production of the target sign, related pictures also attracted      pictures before determining that the target picture is not
more looks than unrelated pictures for one phonological             shown. Crucially, the pattern of gaze in the handshape-
condition (location-movement) but not for the others                movement condition differs from the location-movement
(handshape-location           and        handshape-movement).       condition for which gaze to the related competitor picture is
Importantly, in the early period of each trial (i.e. before the     early and consistent. Thus, we suggest that gaze toward
target sign was produced), there was no difference in gaze          related competitors in location-movement trials is indicative
patterns to the different picture types (related and unrelated)     of active online lexical processing as has been found in
confirming that the results are not driven by visual                spoken language studies.
characteristics of the related pictures.                              In the introduction, we offered two explainations for why
                                                                    onsets play a special role in auditory lexical access (i.e.,
                                                                1454

either temporal constraints or salience). In terms of sign            now you don’t: Mediating the mapping between language
language processing, if temporal constraints are driving              and the visual world. In J. Henderson & F. Ferreira (Eds.),
lexical access, then signers in our study should have paid            The interface of language, vision, and action. New York:
attention to handshape and location features because these            Psychology Press.
features are available at the start of a sign. Instead, the sign     Corina, D. P., & Knapp, H. (2006). Lexical retrieval in
language results suggest that sign onsets are not similarly           American Sign Language production. In L. M. Goldstein,
privileged to spoken word onsets, which in turn suggests              D. H. Whalen, & C. T. Best (eds.). Papers in laboratory
that lexical processing is not temporally driven.                     phonology. Berlin: Mouton de Gruyter, 213–240.
  Alternatively, the data support a view under which                 Emmorey, K., & Corina, D. (1990). Lexical recognition in
perceptual salience drives sign access. Specifically, our data        sign language:       Effects of phonetic structure and
show that signers pay attention to movement features which            morphology. Perceptual and Motor Skills, 71: 1227-1252.
are visually salient, but which occur relatively late in sign        Emmorey, K. (2002). Language, cognition, and the brain:
production: only trials with related distracters that share           Insights from sign language research. Lawrence Erlbaum
movement features show differences between looks to                   and Associates, Mahwah, NJ.
unrelated filler pictures and related competitor pictures.           Gaskell, M.G. & Marslen-Wilson, W.D. (1997). Integrating
Further, the pairing of location-movement features appears            form and meaning: A distributed model of speech percep-
to be of greatest importance during online processing.                tion. Language and Cognitive Processes, 12: 613-656.
  The nature of acoustically perceived speech in languages           Grosjean, F. (1981). Sign and word recognition: A first
such as English makes it impossible to determine why word             comparison. Sign Language Studies, 32: 195-219.
onsets seem to have privileged status in lexical access:             Corina, D. P., & Hildebrandt, U. (2002). Psycholinguistic
either due to temporal characteristics (perceived first) or           investigations of phonological structure in American Sign
perceptual salience. Investigating signed languages such as           Language. In R. P. Meier, K. Cormier & D. Quinto-Pozos
BSL allows us to clearly tease these apart, because the most          (Eds.), Modality and Structure in Signed and Spoken
salient perceptual properties (e.g. movement) are not                 Languages. Cambridge: Cambridge University Press.
available at the onset but only become available later. Thus,        Huettig, F., Altmann, G. (2005). Word meaning and the
increased looks towards related distracter pictures in the            control of eye fixation: semantic competitor effects and
location-movement condition may provide insight, not only             the visual world paradigm. Cognition, 96(1): B23-B32.
into the nature of online sign processing but online speech          Marslen-Wilson,W.D. (1973). Linguistic structure and
processing as well. It is important to note that there is no a        speech shadowing at very short latencies. Nature,
priory reason to assume that (visual) signs and (auditory)            244:522–523.
words will be processed similarly and therefore different            McClelland, J.L. & Elman, J. (1986). The TRACE model of
strategies might be used.                                             speech perception. Cognitive Psychology, 18:1–86.
  Overall, the results here reveal important characteristics of      Perlmutter, David M. (1992): Sonority and syllable structure
lexical access concerning the role of lexical variables               in American Sign Language. Linguistic Inquiry, 23, 3,
(semantic condition) and relative time course of access to            407-442.
different phonological parameters (phonological condition)           Rayner, K. & Clifton C. Jr. (2009). Language processing in
for sign language processing. More broadly, our current               reading and speech perception is fast and incremental:
understanding of language processing is intimately tied to            Implications for event related potential research.
oral-aural modality of spoken languages. The current work             Biological Psychology, 80: 4-9.
clearly shows that language processing interacts with                Reinisch, E., Jesse, A., & McQueen, J. M. (2010). Early use
modality, and that the key to lexical access for both signed          of phonetic information in spoken word recognition:
and spoken languages may be perceptual saliency, instead of           Lexical stress drives eye-movements immediately. Quar-
temporal recency.                                                     terly Journal of Experimental Psychology, 63(4), 772-783.
                                                                     Sandler, W. & Lillo-Martin, D. (2006). Sign Language and
Acknowledgments                                                       Linguistic Universals. Cambridge: Cambridge Univ.Press.
   This work was supported by the Economic and Social                Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K.
Research Council of Great Britain (Grant RES-620-28-                  M., & Sedivy, J. C. (1995). Integration of visual and
6001), Deafness, Cognition and Language Research Centre               linguistic information in spoken language comprehension.
(DCAL).                                                               Science, 268(5217), 1632-1634.
                                                                     Yee, E. & Sedivy, J.C. (2006). Eye movements to pictures
                                                                      reveal transient semantic activation during spoken word
References
                                                                      recognition. Journal of Experimental Psychology:
Allopena, P. D., Magnuson, J. S., & Tanenhaus, M. K.                  Learning, Memory & Cognition, 32: 1-14.
  (1998). Tracking the time course of spoken word                    Vinson, D.P., Cormier, K., Denmark, T., Schembri, A. &
  recognition using eye movements: Evidence for                       Vigliocco, G. (2008). The British Sign Language (BSL)
  continuous mapping models. Journal of Memory and                    norms for age of acquisition, familiarity and iconicity.
  Language, 38, 419- 439.                                             Behavior Research Methods, 40: 1079-87.
Altmann, G. T. M., & Kamide, Y. (2004). Now you see it,
                                                                 1455

