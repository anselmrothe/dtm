UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Social Affordances and Using Them for Planning
Permalink
https://escholarship.org/uc/item/9cj412wg
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Uyanik, Kadir F.
Calskan, Yigit
Bozcuoglu, Asil Kaan
et al.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                      Learning Social Affordances and Using Them for Planning
       Kadir Firat Uyanik, Yigit Caliskan, Asil Kaan Bozcuoglu, Onur Yuruten, Sinan Kalkan, Erol Sahin
                                       {kadir, yigit, asil, oyuruten, skalkan, erol}@ceng.metu.edu.tr
                               KOVAN Research Lab, Dept. of Computer Eng., METU, Ankara, Turkey
                              Abstract                                     (Ugur, Şahin, & Oztop, 2009), liftability of objects (Dag, Atil,
                                                                           Kalkan, & Sahin, 2010) and showed how one can make multi-
   This study extends the learning and use of affordances on
   robots on two fronts. First, we use the very same affordance            step plans using the learned affordances.
   learning framework that was used for learning the affordances              In this paper, we argue that robots can use the very same
   of inanimate things to learn social affordances, that is affor-         framework to learn what a human may afford. Moreover, we
   dances whose existence requires the presence of humans. Sec-
   ond, we use the learned affordances for making multi-step               enhance our prior study on multi-step planning (Ugur et al.,
   plans. Specifically, an iCub humanoid platform is equipped              2009) via a new form of prototypes for effect representation.
   with a perceptual system to sense objects placed on a table, as            Specifically, we equipped the humanoid robot iCub with
   well as the presence and state of humans in the environment,
   and a behavioral repertoire that consisted of simple object ma-         a perceptual system to sense tabletop objects, as well as the
   nipulations as well as voice behaviors that are uttered simple          presence and state of humans in the environment, and a be-
   verbs. After interacting with objects and humans, the robot             havioral repertoire that consisted of simple object manipu-
   learns a set of affordances with which it can make multi-step
   plans towards achieving a demonstrated goal.                            lations and voice behaviors that uttered simple verbs. After
                                                                           interacting with objects and humans, we show that the robot
                          Introduction                                     is able to learn a set of affordances with which it can make
                                                                           multi-step plans towards achieving a demonstrated goal.
Motor competences of robots operating in our environments,
is likely to remain inferior to ours on most fronts in the near            Related Work
future. In order to complete tasks that require competences
beyond their abilities, the robots will need need to interact              The notion of affordances provides a perspective that puts the
with humans in the environment towards compensating these                  focus on the interaction (rather than the agent or the environ-
deficiencies. The inspiration for our study comes from ba-                 ment) and was formalized as a relation a between an entity
bies and small children who can compensate the lack of their               or environment e, a behavior b and the effect f of behavior b
physical competences through the use of adults via social in-              on e (Şahin, Çakmak, Doğar, Uğur, & Üçoluk, 2007; Monte-
teraction . For instance, for a child, the reachability of a candy         sano, Lopes, Bernardino, & Santos-Victor, 2008):
on a high shelf becomes possible only in the presence of an
adult, as long as he can “manipulate” him properly using his                                          a = (e, b, f ),                            (1)
social behaviors.
                                                                           For example, a behavior bli f t that produces an effect fli f ted on
   In this paper, we extend an affordance framework proposed
                                                                           an object ecup forms an affordance relation (ecup , bli f t , fli f ted ).
for robots towards learning interactions with inanimate ob-
                                                                           Note that an agent would require more of such relations on
jects, to learning interactions with humans. The notion of af-
                                                                           different objects and behaviors to learn more general affor-
fordances, proposed by Gibson (Gibson, 1986), emphasized
                                                                           dance relations.
the interaction between the agent and the environment, as op-
                                                                              The studies on learning and use of affordances have
posed to the agent or the environment only, and provided a
                                                                           mostly been confined to inanimate things, such as objects
unifying frameworks for the study.
                                                                           (Fitzpatrick, Metta, Natale, Rao, & Sandini, 2003; Detry,
 Contribution                                                              Kraft, Buch, Kruger, & Piater, 2010; Atil, Dag, Kalkan,
This study extends the learning and use of affordances on                  & Şahin, 2010; Dag et al., 2010) and tools (Sinapov &
robots on two fronts. First, we use the very same affordance               Stoytchev, 2008; Stoytchev, 2008) that the robot can interact
learning framework that was used for learning the affordances              with. In these studies, the robot interacts with the environ-
of inanimate things to learn social affordances1 (viz. affor-              ment through a set of actions, and learns to perceptually de-
dances whose existence requires the presence of humans).                   tect and actualize them. Moreover, with the exception of few
Second, we use learned affordances to make multi-step plans.               studies (Ugur et al., 2009; Williams & Breazeal, 2012), the
   In our earlier studies, we had proposed a framework that                robots were only able to perceive the immediate affordances
allowed the robot to learn affordances such as traversabil-                which can be actualized with a single-step action plan.
ity of an environment (Ugur & Şahin, 2010) or graspability                   Formalizations, such as 1, are proved to be practical with
                                                                           successful applications in navigation (Ugur & Şahin, 2010),
    1 We would like to note that the term, social affordances has been     and manipulation (Fitzpatrick et al., 2003; Detry et al., 2010;
used in different contexts, e.g., for the possibilities emerging from      Ugur et al., 2009; Ugur, Oztop, & Şahin, 2011), conceptu-
social networks (Wellman et al., 2003), or the affordances of an en-
vironment and properties of people that facilitate social interaction      alization and language (Atil et al., 2010; Dag et al., 2010;
in a group of people (Kreijns & Kirschner, 2001).                          Yürüten et al., 2012), and vision (Dag et al., 2010). However,
                                                                       3604

in these studies, the environment is limited to objects only, ex-
cluding the possible diversities or use-cases that might arise
due to the existence of humans in addition to the objects.
   Human-assistance has been incorporated in (Montesano et
al., 2008; Dag et al., 2010; Ugur, Oztop, & Şahin, 2011) us-
ing the same affordance formalization (1) to learn object af-
fordances by imitation and emulation. However, the role of
the human is limited to teaching affordances as part of the
training phase, and he is out of the loop during execution of
actions for possible assistance in creating a certain effect in
the environment to extend robot’s motor capacities.                                     (a)                            (b)
   Robot’s motor capacities are extended by learning the af-
fordances of tools in (Sinapov & Stoytchev, 2008; Stoytchev,          Figure 1: Visualization of the interaction environment. (a)
2008). However, these studies are focused on learning af-             Robot is on the left, and the human is on the right. (b) Object
fordances of tools while the objects are kept fixed, hence the        related features. From top left to bottom right: raw RGBD
affordances of objects themselves couldn’t be captured.               point cloud of an object on a table, table and extracted table-
   In most of the HRI or social robotics studies, the robots are      top object with its oriented bounding box and id, surface nor-
intended to collaborate with their human partners and they            mals, min curvatures, max curvatures, shape indices. [Figure
are “active learners” that learn from their partners the cor-         best viewed in color]
rect way to execute and sequence actions for achieving a goal
(see, e.g., (Fong, Thorpe, & Baur, 2003; Breazeal, 2004; We-
ber, 2008; Cakmak, DePalma, Arriaga, & Thomaz, 2010) for              environment is represented as a feature vector containing the
a review). This way, one can teach a robot to learn compli-           following features:
cated sequences of actions (e.g., dancing with a human part-          Surface features are surface normals (azimuth and zenith),
ner (Kosuge & Hirata, 2004)) using several mechanisms like            principal curvatures (min and max), and shape indices. They
scaffolding (Ugur, Celikkanat, Sahin, Nagai, & Oztop, 2011;           are represented as a 20-bin histogram in addition to the min,
Saunders, Nehaniv, Dautenhahn, & Alissandrakis, 2007) or              max, mean, standard deviation and variance information.
demonstration (Pastor, Hoffmann, Asfour, & Schaal, 2009;
Argall, Chernova, Veloso, & Browning, 2009; Akgun, Cak-               Spatial features are bounding box pose (x, y, z, theta),
mak, Jiang, & Thomaz, 2012). Similarly, affordances are also          bounding box dimensions (x, y, z), and object presence.
utilized in planning (Ugur, Oztop, & Şahin, 2011) over action        Social features are human torso pose (x, y, z, roll, pitch,
possibilities, but human is not a part of the plan. However, in       yaw), human gaze direction (roll, pitch, yaw), and human
(Williams & Breazeal, 2012), humans are important part of             presence, all with respect to robot’s own coordinate system
the plan, yet their participation is limited with the experiment      shown as coordinate axis in Fig. 1a.
scenario, and participants are priorly acknowledged about the
type of assistance they are going to provide to the robot.
   For a similar goal, affordances (called “interpersonal affor-
dances”) that emerge from coordinated joint actions of two
robots are investigated (Richardson, Marsh, & Baron, 2007;
Marsh, Richardson, & Schmidt, 2009); e.g., two robots learn-
ing the interpersonal affordance of lifting a table which, oth-       Figure 2: The interaction objects. From left to right; balls,
erwise, is liftable by neither of them. Our approach differs          boxes, cylinders, mugs, and irregular objects.
from these studies in the sense that the human is seen as part
of the environment (with no special status) and uses the very
same framework to learn social affordances as the physical            Behaviors and Effect Representation
affordances of objects.
                                                                      The robot is equipped with six behaviors (push-left, push-
                    Research Platform                                 right, push-forward, pull, top-grasp, side-grasp) and some
                                                                      voice behaviors (“pass me”, “hello”, “come”, “sit down”,
Perception and Environment Representation                             “stand up”, “bye”, “push right”, “push left”, “take”).
iCub perceives its environment through two Kinect cameras                Effects –in their raw form- are computed as the difference
(K1 and K2). K1 is used to extract table and tabletop objects.        between the final and the initial state of the environment (viz.
K2 –accompanied with a motion capture system (Visualeyez              difference between the feature vectors representing the envi-
II VZ4000)- is used to detect human’s body posture and gaze           ronment before and after the behavior performance).
direction. For gaze direction detection, participants are pro-           Effects are supervisedly matched to an effect category cho-
vided with a hat with active LEDs on top. Overall interaction         sen from a set of effects such as grasped, knocked, no-
                                                                  3605

change2 , disappeared, moved right, moved left, moved for-               effect prototype is needed and we use the Mahalanobis dis-
ward, pulled, sat down, stood up, got attention, got closer.             tance, which is calculated by taking the mean µEi of first ef-
   Effect categories are compactly represented as a vector               fect cluster Ei (if the first Ei is an effect instance, we take the
of “0”, “+”, “−”, and “∗” to represent changes in certain                effect instance as µEi ) and using the second effect cluster’s E j
feature value as unchanged (mean close to zero, small vari-              mean µE j and variance σE j :
ance); consistently increased (positive mean, small variance),
consistently decreased (negative mean, small variance); and
inconsistently changed (large variance), respectively. This                                     r                   T                            
                                                                                                           +,−,0                          +,−,0
prototype-based effect representation is claimed to corre-                       d(Ei , E j ) =    µEi − f proto,E j    S−1
                                                                                                                         j   µ  E  i −  f proto,E j        (2)
spond to verb concepts in our earlier studies (Atil et al., 2010).
For extracting the prototypes for each effect cluster, we ana-
lyze the mean and variance values for each element of the
                                                                         where S j is the covariance matrix of the second effect cluster
features in the cluster. Specifically, we apply unsupervised
                                                                         E j . In computing the Mahalanobis distance, the features
clustering (RGNG, (Qin & Suganthan, 2004)) on the mean-
                                                                         marked inconsistent in the prototype are disregarded (denoted
variance space. RGNG finds four clusters naturally formed.                     +,−,0
                                                                         by f proto,E   for the effect prototype f proto,Ei of an effect cluster
From the obtained effect consistencies, we determine the pro-                         i
                                                                         Ei ), as those correspond to an unpredictable/inconsistent
totype of each effect cluster.
                                                                         change in the feature elements.
                          Methodology                                    Finding the effects Planning toward achieving the goal is
Data Collection                                                          found using a breadth-first tree search. Starting with the ini-
                                                                         tial state, we construct a tree such that it contains all the possi-
We used 35 objects (Fig. 2) that are chosen to be in dif-
                                                                         ble effect sequences with length n (empirically chosen as 3).
ferent colors, and shape complexities (from primitive cubes,
                                                                         The plan is made as the goal is matched with the predicted
spheres, cylinders to mugs, wine glasses, coke cans etc.), eas-
                                                                         states after applying a sequence of behaviors.
ily identified as “cylinder”, “ball”, “cup”, “box”, while some
of them had irregular shapes to show generalization ability of               In the first step of future state                Initial State
the system.                                                              calculation (Fig. 3), the current
   We had iCub interact with objects and with humans by us-              state of the object is fed to the
ing all of the behaviors precoded in its behavior repertoire.            trained SVM for each behavior.                          Predict-bi
In order to collect social interaction data, we have worked              Then, the predicted effect’s pro-                                      SVM for bi
with 10 participants of different genders (4 female, 6 male),            totype is determined. The mean
ages (20-40) and professions (4 undergrad, 2 grad students,              value of this effect is added to the
4 researchers with non-CS background). They were asked                   initial features, with the excep-
to respond naturally to a random sequence of voice behaviors             tion of the inconsistent features,
enacted by iCub. Some of the voice behaviors were accompa-               and the predicted future state is                                           +
nied by simple movements (nodding head, waving arm, etc.).               found.         After this application,
                                                                         the predicted future state can be                        Predicted
Affordance Learning                                                                                                               after-state
                                                                         compared with other states; but the
We collected 413 triplets of (e, b, f ) (Eqn. 1) for object in-          inconsistent features of the applied
teractions and 150 triplets for human interactions, and used             effect (denoted as black columns
them to train a Support Vector Machine (SVM) classifier for              in predicted after-state) is excluded              Figure 3: Future state
each behavior to predict the effect label given an entity. Dur-          from the comparison calculations.                  prediction.
ing training, we normalized the feature vectors with respect
to the range of values each feature can take.                            Application of effects Given the object, we can obtain from
                                                                         the trained SVMs the behavior that can achieve a desired ef-
Planning with Forward Chaining                                           fect with the highest probability. Thus, we obtain the be-
Since we trained SVMs for predicting the effect of each be-              haviors required for each step in the planned effect sequence,
havior on an object, iCub can do forward chaining to find the            forming a sequence of behaviors. If the obtained effect at any
set of behaviors leading to a goal state. Since the effect labels        step in the behavior sequence does not match with the expec-
are represented by effect prototypes, the similarity between             tation, then the planning restarts. Fig. 3 and 4 respectively
the goal state (which is an effect instance) and the predicted           exemplify how a sequence of effect prototypes for reaching a
                                                                         desired effect is sought and how a behavior that produces an
    2 The no-change label denotes that the applied behavior did not      effect on an object is found. The system executes the planned
produce any notable change on the object. For example, when iCub         behavior sequence. If, at any step, the predicted effect is not
asks a participant to stand up who was already standing, the partic-
ipant would not change his position. This yields negligibly small        achieved (including overshoots or undershoots), the planning
changes in the feature vector.                                           restarts from the current object state.
                                                                     3606

                                                                     the participants would leave the scene. However, participants
                          Initial
                          (t = 0)                                    mostly kept their positions and responded by waving back.
        Predict-b0      Predict-b1      Predict-b2
                                                                               1
                                                                              0.5
         Object          Object           Object     Predicted          E1     0
                                                                             −0.5
          t=1             t=1              t=1       States                   −1
                                                                               1
         b = {b0}
                                                                                                     Feature Index
                         b = {b1}         b = {b2}   After                    0.5
                                                     Step 1
                                                                               0
                                                                        E2   −0.5
                                                                              −1
                                                                               1                     Feature Index
                                                                              0.5
          Predict-b0    Predict-b1     Predict-b2                       E3     0
                                                                             −0.5
                                                                              −1
                                                     Predicted
                                                                               1                     Feature Index
           Object        Object         Object                                0.5
                                                     States             E4     0
            t=2            t=2            t=2                                −0.5
                                                     After
          b = {b1,b0}   b = {b1,b1}
                                                                              −1
                                       b = {b1,b2}                             1
                                                     Step 2
                                                                                                     Feature Index
                                                                              0.5
                                                                        E5     0
                                                                             −0.5
                                                                              −1
Figure 4: A simple depiction of how the planning is per-                               F1   F2                       F3
formed using the combinations of effects in the repertoire. At
each step, the prediction block described in Fig. 3 is applied       Figure 5: Some of the results obtained from unsupervised
for each behavior. Once a future state close enough to goal          clustering of feature mean and variances. Vertical axis repre-
state is obtained, the search is terminated.                         sents some of the effects (E1: moved-right, E2: moved-left,
                                                                     E3: moved-forward, E4: pulled and E5: disappeared) and
                                                                     the horizontal axis represents some of the features detected
                             Results                                 as consistently increasing (red upwards arrow) or decreasing
                                                                     (blue downwards arrow) in the given effects (F1: position-x,
Social Affordance Learning                                           F2: position-y, F3: human presence))
Fig. 5 shows some of the effect prototypes that lead us to
interesting observations. In the first place, some effects can
apparently be produced both by social and non-social behav-
iors. An obvious example is “say push to your left” and push-
right (causing the moved right effect most of the time). Fur-
thermore, we observe that in some cases, social behaviors can
be a better option for goal emulation. For instance, when the
object is far enough from the robot, the predicted effect for
                                                                     Figure 6: Some different reactions by experiment participants
pull behavior is no change; whose effect prototype has only
                                                                     when the robot uses the “pass me” voice behavior.
*’s and 0’s (features with inconsistent change and negligible
change), whereas the predicted effect for “pass me” behav-
ior is pulled, the effect whose prototype denotes consistent            Both social and non-social behaviors contribute to these
decrease in object’s distance to the robot (x-position). In em-      results. For example, pulled can be produced both from pull
ulating a goal to pull this object towards itself, Eq. 2 yields      and “pass me” behaviors. Note that some of the features,
that pulled effect brings the object closer to the goal, hence       which were inconsistently changed (marked with star) or neg-
iCub chooses to ask a human to pass the object.                      ligibly changed (marked with circle), grouped into one col-
                                                                     umn for brevity.
   The effects got attention and got closer turned out to be
ambiguous effect labels - their corresponding clusters did not       Social Affordances and Multi-step Planning
have any consistently increasing or decreasing features. This
                                                                     We demonstrate social affordances in three scenarios:
might also be related with our feature set. Similar results were
observed for the effects clustered as sat down and stood up,         1- Multi-step planning without human presence In this
although they were unambiguously interpreted by the partic-          scenario, the object is placed in front of iCub as the initial
ipants. The amount of standing and sitting of our experiment         position, and the target position is shown with red circles
participants has had a high variance. The participants had           (Fig. 7a). After initial and final positions are shown to iCub,
two major interpretations for the “pass me” behavior: they           it plans without a human present in the environment; i.e., it
either (i) pushed the object towards the robot (causing pulled       cannot make use of “social affordances”. According to the
effect) or (ii) tried to pass it to robot’s hand (Fig. 6). Similar   plan, the effect sequence is determined as moved forward,
response was also observed when the voice behavior “take             moved left, moved forward. After a successful push-forward
this” was applied: while most of the participants took the ob-       behavior, the object is moved-forward (Fig. 7b), then with a
ject and removed it from the scene (causing the disappeared          push-left behavior, the object reaches close to the target posi-
effect), some of the participants just dragged the object to-        tion (Fig. 7c). Appropriate behaviors to end up with the last
wards themselves (causing moved forward effect). We were             moved-forward effect may have been push-forward behavior
expecting that when iCub enacted the voice behavior “bye”,           or “pass me” voice behavior. Since there is no human across
                                                                 3607

the table and because the object is too far to be moved forward        a new effect sequence of moved left, moved forward. This
to its target position, iCub figures out that it is impossible for     re-planned effect sequence is achieved by using push-left be-
the object to reach its final position (Fig. 7d) and stops at this     havior (Fig. 9f) and “take” voice behavior (Fig. 9g) and
stage.                                                                 object reaches its target position (Fig. 9h).
                                                                              (a)              (b)             (c)               (d)
        (a)              (b)             (c)             (d)
Figure 7: Scenario #1: The robot cannot reach the target po-
sition and cannot fulfill the goal due to absence of a human.
2- Multi-step planning with a human - using “pass me”
voice behavior This scenario demonstrates a case for suc-
cessful planning. As the initial position, the object is placed               (e)              (f)             (g)               (h)
closer to the human and the target position is shown with a red
circle (Fig. 8a). After planning, the effect sequence pulled,          Figure 9: Scenario #3: The robot can use human’s affor-
pulled, moved left is determined to reach the target position.         dances when stuck, by using the with “take” voice behavior.
For the first pulled effect, since the object is placed far from
iCub and with the contribution of human presence, “pass me”               From these 3 different scenarios, we conclude as follows:
voice behavior has the highest probability and is executed             (i) After iCub executes a behavior, if it observes an unex-
(Fig. 8b). For the remaining pulled and moved-left effects,            pected effect, it re-plans. (ii) iCub executes its behaviors
pull (Fig. 8c) and push-left (Fig. 8d) behaviors are executed          by planning (and re-planning if necessary) until the object
respectively. As a result, each planned effect is achieved and         reaches the target position or iCub decides that it is impossi-
the object reaches its target position (Fig. 8e).                      ble for the object to reach the target position. (iii) If there is a
                                                                       human, iCub may benefit from the affordances offered by the
                                                                       human to get a desired effect. (iv) If there is no human and
                                                                       the desired effect requires a human, iCub can realize that it is
                                                                       impossible for the object to reach the target.
                                                                                                 Conclusion
                                                                       In this paper, we used the very same affordance learning
    (a)           (b)           (c)            (d)           (e)       framework developed for discovering the affordances of inan-
                                                                       imate things to learn social affordances, that is affordances
Figure 8: Scenario #2: The robot can use human’s affor-                whose existence require the presence of humans. We demon-
dances when stuck, by using the “pass me” voice behavior.              strated that our humanoid robot can interact with objects and
                                                                       with humans (using simple verbal communication) and from
3- Multi-step planning with a human - using “take” voice               these interactions, it can learn what the objects as well as the
behavior This scenario shows a demonstration in which                  humans afford . Moreover, we showed that the robot can ask
iCub finds a valid plan but because of a behavior which re-            for human assistance whenever it is required while executing
sults with an unexpected effect, iCub re-plans. For this sce-          multi-step plans to satisfy demonstrated/given goals.
nario, the object is placed close to iCub and the target position         Our approach towards learning the social affordances is
is shown with a red circle (Fig. 9a). The planner finds out            in line with the findings that affordances at different lev-
the required effect sequence as moved forward, moved right,            els (intra-level and inter-level) share the same intrinsic con-
moved forward. The first two effects are achieved using the            straints and organizations (e.g., (Richardson et al., 2007)).
push-forward (Fig. 9b) and then push-right behavior (Fig.
9c). For the last effect, push-forward behavior is executed.                               Acknowledgement
However, instead of a moved-forward effect, moved-right ef-            This work is partially funded by the EU project ROSSI (FP7-
fect occurs (Fig. 9d). Because of this unexpected effect, iCub         ICT-216125) and by TÜBİTAK through project 109E033.
needs a re-planning (Fig. 9e). This re-planning results with           The authors Kadir F. Uyanik, Onur Yuruten and Asil K.
                                                                   3608

Bozcuoglu acknowledges the support of Turkish Science and               from demonstration. In Robotics and automation, 2009.
Technology Council through the 2210 scholarship.                        icra’09. ieee international conference on (pp. 763–768).
Figures in the result section are adapted from (Mutlu, 2009)         Qin, A. K., & Suganthan, P. N. (2004). Robust growing
with author’s permission.                                               neural gas algorithm with application in cluster analysis.
                                                                        Neural Networks, 17(8-9).
                         References                                  Richardson, M., Marsh, K., & Baron, R. (2007). Judg-
Akgun, B., Cakmak, M., Jiang, K., & Thomaz, A. L. (2012).               ing and actualizing intrapersonal and interpersonal affor-
  Keyframe-based learning from demonstration. Interna-                  dances. Journal of experimental psychology: Human Per-
  tional Journal of Social Robotics, 4(4), 343–355.                     ception and Performance, 33(4), 845.
                                                                     Şahin, E., Çakmak, M., Doğar, M., Uğur, E., & Üçoluk, G.
Argall, B., Chernova, S., Veloso, M., & Browning, B. (2009).
                                                                        (2007). To afford or not to afford: A new formalization of
  A survey of robot learning from demonstration. Robotics
                                                                        affordances toward affordance-based robot control. Adap-
  and Autonomous Systems, 57(5), 469–483.
                                                                        tive Behavior, 15(4), 447–472.
Atil, I., Dag, N., Kalkan, S., & Şahin, E. (2010). Affordances
                                                                     Saunders, J., Nehaniv, C., Dautenhahn, K., & Alissandrakis,
  and emergence of concepts. In Epigenetic robotics.
                                                                        A. (2007). Self-imitation and environmental scaffolding for
Breazeal, C. (2004). Social interactions in hri: the robot view.
                                                                        robot teaching. International Journal of Advanced Robotic
  Systems, Man, and Cybernetics, Part C: Applications and
                                                                        Systems, 4(1), 109–124.
  Reviews, IEEE Transactions on, 34(2), 181–186.
                                                                     Sinapov, J., & Stoytchev, A. (2008, aug.). Detecting the
Cakmak, M., DePalma, N., Arriaga, R., & Thomaz, A.
                                                                        functional similarities between tools using a hierarchical
  (2010). Exploiting social partners in robot learning. Au-
                                                                        representation of outcomes. In Development and learning,
  tonomous Robots, 29(3), 309–329.
                                                                        2008. icdl 2008. 7th ieee international conference on (p. 91
Dag, N., Atil, I., Kalkan, S., & Sahin, E. (2010). Learning
                                                                        -96).
  affordances for categorizing objects and their properties. In
                                                                     Stoytchev, A. (2008). Learning the affordances of tools using
  In 20th int. conference on pattern recognition (pp. 3089–
                                                                        a behavior-grounded approach. Towards Affordance-Based
  3092).
                                                                        Robot Control, 140–158.
Detry, R., Kraft, D., Buch, A., Kruger, N., & Piater, J. (2010,      Ugur, E., Celikkanat, H., Sahin, E., Nagai, Y., & Oztop, E.
  may). Refining grasp affordance models by experience.                 (2011). Learning to grasp with parental scaffolding. In
  In Robotics and automation (icra), 2010 ieee international            Humanoid robots (humanoids), 2011 11th ieee-ras inter-
  conference on (p. 2287 -2293).                                        national conference on (pp. 480–486).
Fitzpatrick, P., Metta, G., Natale, L., Rao, S., & Sandini, G.       Ugur, E., & Şahin, E. (2010). Traversability: A case study
  (2003). Learning about objects through action - initial steps         for learning and perceiving affordances in robots. Adaptive
  towards artificial cognition. In Ieee international confer-           Behavior, 18(3-4), 258-284.
  ence on robotics and automation (icra’03) (p. 3140-3145).          Ugur, E., Şahin, E., & Oztop, E. (2009). Affordance learn-
Fong, T., Thorpe, C., & Baur, C. (2003). Collaboration, di-             ing from range data for multi-step planning. Int. Conf. on
  alogue, human-robot interaction. Robotics Research, 255–              Epigenetic Robotics.
  266.                                                               Ugur, E., Oztop, E., & Şahin, E. (2011). Goal emulation
Gibson, J. (1986). The ecological approach to visual percep-            and planning in perceptual space using learned affordances.
  tion. Lawrence Erlbaum.                                               Robotics and Autonomous Systems, 59(7-8).
Kosuge, K., & Hirata, Y. (2004). Human-robot interaction.            Weber, J. (2008). Human-robot interaction. Handbook of Re-
  In Robotics and biomimetics, 2004. robio 2004. ieee inter-            search on Computer Mediated Communication. Hershey,
  national conference on (pp. 8–11).                                    PA: Information Science Reference.
Kreijns, K., & Kirschner, P. (2001). The social affordances of       Wellman, B., Quan-Haase, A., Boase, J., Chen, W., Hampton,
  computer-supported collaborative learning environments.               K., Dı́az, I., et al. (2003). The social affordances of the in-
  In Frontiers in education conference (Vol. 1).                        ternet for networked individualism. Journal of Computer-
Marsh, K., Richardson, M., & Schmidt, R. (2009). Social                 Mediated Communication.
  connection through joint action and interpersonal coordi-          Williams, K., & Breazeal, C. (2012). A reasoning archi-
  nation. Topics in Cognitive Science, 1(2), 320–339.                   tecture for human-robot joint tasks using physics-, social-,
Montesano, L., Lopes, M., Bernardino, A., & Santos-Victor,              and capability-based logic. In Intelligent robots and sys-
  J. (2008). Learning object affordances: From sensory–                 tems (iros), 2012 ieee/rsj international conference on (pp.
  motor coordination to imitation. Robotics, IEEE Transac-              664–671).
  tions on, 24(1), 15–26.                                            Yürüten, O., Uyanık, K. F., Çalışkan, Y., Bozcuoğlu, A. K.,
Mutlu, B. (2009). Designing gaze behavior for humanlike                 Şahin, E., & Kalkan, S. (2012). Learning adjectives and
  robots. Unpublished doctoral dissertation, Pittsburgh, PA,            nouns from affordances on the icub humanoid robot. In
  USA. (AAI3367045)                                                     From animals to animats 12 (pp. 330–340). Springer.
Pastor, P., Hoffmann, H., Asfour, T., & Schaal, S. (2009).
  Learning and generalization of motor skills by learning
                                                                 3609

