UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Visual Recognition using a Combination of Shape and Color Features
Permalink
https://escholarship.org/uc/item/0094h82q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Jalali, Sepehr
Tan, Cheston
Lim, Joo-Hwee
et al.
Publication Date
2013-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

            Visual Recognition using a Combination of Shape and Color Features
                                                    Sepehr Jalali (tmssj@nus.edu.sg)
                                            Cheston Tan (cheston-tan@i2r.a-star.edu.sg)
                                             Joo-Hwee Lim (joohwee@i2r.a-star.edu.sg)
                                              Jo-Yew Tham (jytham@i2r.a-star.edu.sg)
                                                 Sim-Heng Ong (eleongsh@nus.edu.sg)
                                              Paul James Seekings (mmrl@nus.edu.sg)
                                              Elizabeth A. Taylor (tmsohe@nus.edu.sg)
                                           National University of Singapore, Singapore 119077
                                      Institute for Infocomm Research, A*STAR,Singapore 138632
                              Abstract                                   discrimination and matching is continuous (Palmer, 1999).
   We develop and implement a new approach to utilizing color
                                                                         Interestingly, people of different races (Boynton & Olson,
   information for object and scene recognition that is inspired         1987), as well as chimpanzees (Matuzawa, 1985), organize
   by the characteristics of color- and object-selective neurons in      colors into the same basic color categories, such as red, blue,
   the high-level inferotemporal cortex of the primate visual sys-       yellow, green.
   tem. In our hierarchical model, we introduce a new dictionary
   of features representing visual information as quantized color           More importantly for object and scene recognition, the cat-
   blobs that preserve coarse, relative spatial information. We run      egorical recognition of color suggests that, if color informa-
   this model on several datasets such as Caltech101, Outdoor
   Scenes and Underwater Images. The combination of our color            tion is incorporated into object and scene classification, then
   features with (grayscale) shape features leads to significant in-     fine-grained color information (e.g. precisely specified hue)
   creases in performance over shape or color features alone. Us-        may not be necessary. For example, a beach scene might be
   ing our model, performance is significantly higher than using
   color naively, i.e. concatenating the channels of various color       recognized from the blue (sky and sea) and brown (sand) re-
   spaces. This indicates that usage of color information per se is      gions. It may not be important exactly how blue the sky/sea
   not enough to produce good performance, and that it is specifi-       or how brown the sand grains are. In fact, it may be important
   cally our biologically-inspired approach to color that results in
   significant improvement.                                              to disregard such details in order to perform classification that
                                                                         is tolerant to variations in lighting, and so on.
   Keywords: Visual recognition; Color; HMAX; Biologically
   inspired; Visual cortex; Image classification                            In addition, the coarse relative spatial position of such color
                                                                         regions may be important. A blue region above a yellow-
                          Introduction                                   brown region might suggest a beach scene. If the relative
Many models are inspired by the hierarchical organization of             positions are reversed, then the image is probably not a beach
the visual cortex, such as Fukushima (1980) and Riesenhuber              scene (or might be an upside-down one). Not only is the de-
and Poggio (1999). Most of these models focus on grayscale               tailed spatial information unnecessary, it may be crucial to
information and ignore color information. While the broad                discard it and only retain coarse spatial information, since the
use of color information in the primate visual system is                 exact spatial relations will depend on factors such as the pre-
well-known, the details are still under active investigation             cise shape of the beach and the camera angle.
(Conway et al., 2010). Nonetheless, in this paper, we attempt               Overall, our model can be loosely described as perform-
to utilize what is currently known about the use of color to en-         ing object and scene classification by reducing a given image
hance object and scene recognition by computer algorithms.               to a “coarse arrangement of categorical color blobs”, similar
In this paper we utilize the HMAX model (Riesenhuber &                   to the idea of spatial aggregation of visual keywords (Lim,
Poggio, 1999), but this approach can be extended to other                1999), but with realization on the HMAX model. This is
computational models.                                                    different from approaches that utilize color information in a
   In our experiments, we use the HMAX model                             low-level fashion, although the two types of approaches are
(Riesenhuber & Poggio, 1999) in concatenation with                       not mutually exclusive. Crucially, our biologically-inspired
our color model in order to evaluate the use of both shape               approach outperforms the naive use of color, where an image
and color. HMAX is a biologically-inspired model which                   is decomposed into separate color channels that are processed
focuses on the shape processing capabilities of the ventral              independently until the final classifier stage.
visual pathway, and has been used to perform classification
tasks (Serre, Wolf, Bileschi, Riesenhuber, & Poggio, 2007).                                      Related Work
   We focus on extending the model by modelling the high-                First, we go beyond the intuitive motivation for our approach
level usage of color by incorporating insights from cognitive            and review the biological evidence that the primate visual
psychology and neuroscience. The broad intuitive inspiration             system utilizes color information in a manner that is broadly
for our model follows from the fact that colors are recognized           consistent with our model. Specifically, we review studies of
categorically just as object classes are, even though color              color processing in the high-level visual area of the primate
                                                                     2638

brain known as infero-temporal cortex (IT), which is com-          ply it independently to the R, G and B channels, and then
monly associated with invariant object recognition.                concatenate the features from all 3 channels just before the
   In the broadest terms, IT is known to play an important         final classifier stage. Most algorithms are variants of this
role in color discrimination. A majority of IT neurons are         basic idea, either using some other color space, or fusing
color-selective (Desimone, Schein, Moran, & Ungerleider,           the channels before the classifier stage (usually at the dictio-
1985) and two independent studies estimated this proportion        nary or keyword learning stage). For example, SIFT features
to be roughly 70% (Komatsu, Ideura, Kaji, & Yamane, 1992;          can be computed separately for each channel in HSV color
Edwards, Xiao, Keysers, Földiák, & Perrett, 2003). Con-          space (Bosch, Zisserman, & Muñoz, 2008), while Brown and
trary to the theory that color processing occurs after more        Susstrunk (2011) do this for RGB space, along with an NIR
rapid luminance-only processing, no evidence was found that        (near infra-red) channel. Besides SIFT features, other algo-
colored images evoke responses that are delayed relative to        rithms use (non-orientation based) histograms in the HSV
achromatic images (Edwards et al., 2003). More direct ev-          (Tang, Miller, Singh, & Abbeel, 2012), Gaussian opponent
idence for the role of IT comes from findings that color           color (Burghouts & Geusebroek, 2009), normalized RGB or
discrimination is severely disrupted by lesions (Heywood,          opponent color spaces (Gevers & Stokman, 2004). What
Shields, & Cowey, 1988) or cooling (Horel, 1994).                  these algorithms have in common is that in terms of the bi-
   Color-selective neurons in IT are found in clusters, sug-       ology of color vision, they correspond to at most the level of
gesting that they may form a segregated and independent            color-opponent cells in the primary visual cortex, the lowest
processing network (Conway, Moeller, & Tsao, 2007). As             level in the hierarchically-organized visual cortex.
further evidence of this, one color cluster in IT received
projections from a color cluster from another part of IT,                                  CQ-HMAX
suggesting that these clusters of color-processing neurons         In this section, we describe our new biologically-inspired
form reciprocally-connected modules within a distributed           model, CQ-HMAX (Color Quantization Hierarchical Max),
network (Banno, Ichinohe, Rockland, & Komatsu, 2011).              which uses color information in a hierarchical organization
   IT neurons are selective for both hue and saturation            of simple and complex cells. HMAX is a hierarchical model
(Komatsu, 1993). Different cells have different preferred          which uses Gabor filters to find simple and complex shapes
hues, and as a population, the cells’ preferred color spans        in the images. Our model has a similar hierarchical structure.
most of the color space (Conway et al., 2007). The colors          However, we use color quantization cores and not Gabor fil-
for which IT neurons are selective for tend to correspond to       ters, hence our model encodes color information. When com-
the basic color names (Komatsu, 1998). Komatsu (1998) pro-         bined with HMAX, the overall model includes both color and
posed that IT has templates corresponding to color categories      shape information.
and may be involved in determining color category by find-            Our color model has a hierarchical structure of simple and
ing the best match over these categories. More recently, the       complex cells, as can be seen in Fig. 1. We first introduce the
distribution of color-selective neurons found in IT seems to       model briefly, followed by a more detailed description of each
correspond to the three to four most basic colors (Stoughton       layer. An image pyramid is created in YIQ color space. The
& Conway, 2008). The largest peaks align with red, green,          Y channel represents luminance information, while the I and
and blue, in order of size of peak, with a smaller peak cor-       Q channels represent chrominance information. The pyramid
responding to yellow. These peaks roughly correspond to            has 10 scales, with each neighboring scale different by a ratio
colors perceived by humans. Prior to this, neural representa-      of 1/(21/4 ). In order to evaluate the use of color informa-
tion of such unique hues (Hurvich, 1981) had not been found        tion in our model, we determined that the YIQ color space
(Valberg, 2001). Note that in the low-level primary visual         produced the best results in comparison with HSV and RGB
cortex, the axes defined by cone opponency should more ac-         color spaces. A set of representative values from each color
curately be denoted bluish-red/cyan and lavender/lime oppo-        channel is selected as color cores and used to find the best
nency (Stoughton & Conway, 2008), rather than red-green            matching unit to each individual pixel value in the pyramid.
and blue-yellow opponency.                                         The S1 layer is created on 10 scales indicating the index of the
   Finally, the region of IT where color-selective neurons are     best matching YIQ core to each pixel in the image pyramids.
found is coarsely retinotopic (Yasuda, Banno, & Komatsu,           At the C1 layer, a local max pooling is computed over ±10%
2010), meaning that spatial information is maintained in a         spatial neighborhoods of approximately 6×6 on ±1 neighbor
coarse manner, rather than completely discarded or main-           scales to find the most frequent color core in each neighbor-
tained with high fidelity. Overall, these studies are broadly      hood. A dictionary of features is sampled randomly from the
consistent with our proposed “coarse arrangement of categor-       C1 layer of images. The distance of each dictionary feature
ical color blobs” model of high-level color processing in the      to all patches in a neighborhood of that dictionary feature is
primate visual system.                                             calculated to create the S2 layer and the best response to each
   In contrast, most computer vision algorithms utilize color      dictionary feature in each image is chosen as the C2 layer to
information in a relatively low-level manner. The simplest         be fed to the SVM layer for classification. We describe each
color extension of a non-color algorithm would be to ap-           layer in more detail below.
                                                               2639

                         Figure 1: The CQ-HMAX model and the processing of an example beach image.
S1 Layer and Quantization Cores                                      I = (0.4, 0.47, 0.55, 0.63, 0.7), Q = (0.4, 0.47, 0.5, 0.53, 0.6).
The input images are first converted into YIQ color space and        Using these values results in better classification performance
a pyramid of 10 scales with a ratio of 21/4 is created, with         than using the full range [0, 1] in each YIQ channel. The out-
the first scale having the shorter side set to 140 pixels, main-     puts at the S1 layer are the index values (i.e. 1,2,...,125) of
taining the aspect ratio of the original image. This image           the best-matching color core for each element in the image
pyramid is then used as the input to the S1 layer. A series          pyramid.
of YIQ quantized “color cores” over YIQ channels are cre-
                                                                    C1 Layer
ated to be used as filters for this layer. We experimented with
different numbers of quantization values per color channel,          The C1 layer provides local invariance to position and scale
and chose 5 per channel as the optimal number (which re-             as it pools nearby S1 units, and as a result, subsamples S1
sults in 5 × 5 × 5 = 125 cores). In order to choose the opti-        to reduce the number of units. The S1 pyramid is convolved
mal cores, 500 images were randomly selected and the color           with a 3D max filter to set the C1 layer size of the bottom
range of these images in YIQ color space was calculated after        of the pyramid to 25 × 25 and the highest layer of the pyra-
normalization to the range [0, 1]. The values of YIQ channel         mid to 5 × 5 accordingly. The max is calculated over ±10%
are mostly in the range [0, 1], [0.4, 0.7] and [0.4, 0.6] respec-    spatial neighborhood on ±1 neighbor scales in the middle of
tively. These ranges were selected and divided into 5 bins.          the pyramid and −2 on the highest level and +2 on the low-
The quantized values of Y, I and Q after normalization to [0, 1]     est layer of the pyramid (hence it is called a 3D max, as it
were therefore chosen as follows: Y = (0, 0.25, 0.5, 0.75, 1),       takes the max over 2D spatial distribution and over ±1 scale).
                                                                 2640

This layer provides a model for V 1 complex cells. Fig. 1 also         outputs a vector of the same size as the dictionary of features.
shows an example image of S1 and C1 layer. S1 and C1 layers            We chose different sizes for the dictionary of features and
have a distribution of quantization cores from coarse to fine.         in most cases a dictionary of size 10000 was chosen which
The higher layers of the S1 pyramid are taken from smaller             results in slightly better performances than smaller sizes of
scales of the images in the input pyramid and respectively the         about 1000 dimensions.
higher levels of C1 layer are computed by taking a 3D max
over higher levels of S1 layer. As can be seen in Fig. 1, the          Classification Layer
higher levels of the pyramid in the S1 and C1 layers represent         The C2 vectors are classified using a multi-class one-versus-
less detailed information from the image. All levels in the            rest linear kernel support vector machine. The algorithm used
C1 intermediate layer are used for sampling a dictionary of            to train the classifier is weighted regularized least-squares af-
features.                                                              ter the data is sphered and the mean and variance of each
                                                                       dimension are normalized to zero and one respectively as in
Dictionary of Features and Distance Table                              Mutch and Lowe (2008).
Once the C1 layer is created, sampling is performed by cen-
tering patches of size 4 × 4 at random positions and scales us-        Use of HMAX for Encoding Shape Information
ing a normalized random number generator function. A dis-              For shape information, we used the HMAX model implemen-
tance table is created to store the actual weighted Euclidean          tation of Mutch and Lowe (2008). In HMAX, the maximum
distances of the indices from YIQ quantization cores. Since            response of the S2 layer is chosen as the C2 layer to be fed
the values of the Y channel are normally distributed between           to the classifier. An N-dimensional vector is calculated as the
[0, 1], but the values of I and Q channels fall in the approxi-        output of the C2 layer, where each element is the maximum
mate range of [−0.6, +0.6] and [−0.5, +0.5] respectively, and          response (everywhere in the image in Serre, Oliva, and Pog-
as in most of the images the actual values of these two latter         gio (2007) and in a spatial neighborhood of each dictionary
channels fall between [−0.1, +0.2] and [−0.1, +0.1] (before            feature in Mutch and Lowe (2008)) over image patches for
normalization to [0, 1]) we weighed the distances to have an           each dictionary feature where N is the number of features in
equal effect in the distance calculation. The distance table           the dictionary.
                                                                                 j
weights are calculated as:                                                Let Vi be the response of the image patch pi to the dictio-
                                                                       nary feature d j calculated using Eq. 2. The response of the
                                 p
         DistanceTable(i, j) = D(1) + γD(2) + βD(3)
                                                                       C2 layer is calculated as:
       Where D(k) = (Y IQCore(i, k) −Y IQCore( j, k))2         (1)
                                                                                                          j
                                                                                         C2( j) = max(Vi ) for ∀i ∈ M
with γ = 3.3 and β = 5. In Jalali, Lim, Ong, and Tham (2010)
and Jalali, Lim, Tham, and Ong (2012) various clustering                                                for j = 1, ..., N            (3)
methods in the creation of the dictionary of features were im-
                                                                       where M is the number of valid patches in each image and
plemented and it is shown that by use of random sampling in
                                                                       N is the size of the dictionary of features. This is consistent
HMAX model, relatively good results can be achieved with
                                                                       with the recent HMAX models (Mutch & Lowe, 2008; Serre,
a lower computational cost in comparison with clustering of
                                                                       Oliva, & Poggio, 2007; Jalali et al., 2010; Theriault, Thome,
features.
                                                                       & Cord, 2011).
S2 Layer
                                                                                          Experimental Results
Once the dictionary of features and the distance table are cre-
ated, each entry in the dictionary of features is used as a filter     First we examine the naive use of color by computing vari-
to be convolved on C1 patches of size 4 × 4 on the neighbor            ous color spaces (RGB, HSV, YIQ) on the Caltech101 dataset
scales of the dictionary feature in the pyramid. The responses         (Fei-Fei, Fergus, & Perona, 2004) and compare the results
V (d, p) of each dictionary feature, d to all of the neighbor          with grayscale images. The Caltech 101 dataset, includes
patches of the same size in ±1 scale and ±10% in position, p           101 classes of objects plus a background category. Each class
are calculated using a Euclidean distance equation as:                 contains between 31 to 800 color images of different sizes.
                                                                       The size of each image is approximately 300 × 200 pixels on
                                                                       average. We used 30 randomly chosen images for training
                                    k d − p k2
                                              
                 V (d, p) = exp −                              (2)     from each class and the rest of the images were used in the
                                      2σ2 α                            test phase. We first divide the images into three channels and
                                                                       feed them to the unmodified HMAX (Mutch & Lowe, 2008)
where d is a feature in the dictionary and p is a patch in the
                                                                       directly and evaluate the classification performance.
image C1 pyramid. σ and α are set to 0.5 and 1 respectively
as in Mutch, Knoblich, and Poggio (2010).                                 As can be seen in Table 1, the use of three different chan-
                                                                       nels and concatenating the C2 vectors of all channels to the
C2 Layer                                                               SVM provides only marginal improvement. Since the YIQ
Once the S2 layer is generated, the maximum values for each            color space gives the best overall results, we use this color
patch in the dictionary are taken as the C2 output. This layer         space in our color model. In the rest of this section, we
                                                                   2641

        Color Component                Caltech101    Scenes            on color images and HMAX on grayscale images. As seen in
        Y channel (i.e. gray scale)    54.65         71.48
        I channel                      35.20         54.62             Table 2, the classification accuracy increases when color and
        Q channel                      26.86         50.75             shape information are combined.
        YIQ channels concatenated      55.06         72.66
        RGB channels concatenated      26.53         73.81
        HSV channels concatenated      31.32         73.69                                      Conclusions
                                                                       In this paper, we introduced a new biologically-inspired ap-
Table 1: Results (percentage accuracy) for the naive use of            proach to image classification which uses color in a man-
various color channels and color spaces.                               ner consistent with high-level visual cortex processing by in-
                                                                       corporating insights from cognitive psychology and neuro-
evaluated our model on three datasets: Caltech101, Outdoor             science. We ran this model on several datasets such as Cal-
Scenes and Underwater Images.                                          tech101, Outdoor Scenes and Underwater Images. The com-
                                                                       bination of our color features with (grayscale) shape features
Caltech101 Dataset                                                     led to significant increases in performance over shape or color
The results of using CQ-HMAX on Caltech 101 are shown in               features alone. Using our model, performance is significantly
Table 2. All experiments are performed 8 times on random               higher than using color naively, i.e. concatenating the chan-
splits of training and test sets and the average performance is        nels of various color spaces.
reported. As can be seen, the use of our color model in this              Currently, our model quantizes the YIQ color space into
dataset does not outperform HMAX. However, when the C2                 cubed-shaped “color cores” at the S1 layer. Following the
features of the color model are concatenated with C2 features          work of Shahbaz Khan et al. (2012) and Van De Weijer and
of HMAX, the classification results are improved by more               Schmid (2006), learning the color values that correspond to
than 6% over HMAX alone. HMAX is a computationally                     semantic color names such as “orange”, “brown”, could also
expensive model as Gabor filter responses over different ori-          further improve performance. Alternatively, color cores can
entations in S1 layer are calculated. However, CQ-HMAX                 be learnt through unsupervised clustering, in which more fre-
is relatively faster than HMAX as it performs a quantization           quent colors in each dataset are chosen as color cores.
with 125 cores in the S1 layer instead of Gabor filters.                  Our model emulates color processing in the high-level IT
                                                                       cortex. Interestingly, the combination of our features with
                                                                       those of Zhang, Barhomi, and Serre (2012) – a biologically-
      Model                      Caltech101   Scenes    UWI
      HMAX (i.e. shape)          54.65        71.48     92.93          inspired model that emulates the lower-level cortex – results
      CQ-HMAX (i.e. color)       38.11        69.21     94.03          in classification performance as good (or better) than the
      CQ-HMAX + HMAX             61.09        78.97     96.23          state-of-the-art on several benchmark datasets (Jalali, Tan,
                                                                       Lim, Tham, & Ong, 2013).
Table 2: Results (percentage accuracy) on the Caltech101,
Outdoor Scenes and Underwater Images (UWI) datasets.
                                                                                                References
Outdoor Scenes Dataset                                                 Banno, T., Ichinohe, N., Rockland, K. S., & Komatsu,
This dataset contains 8 outdoor scene categories: coast,                  H. (2011). Reciprocal connectivity of identified color-
mountain, forest, open country, street, inside city, tall build-          processing modules in the monkey inferior temporal cor-
ings and highways (Oliva & Torralba, 2001). There are 2600                tex. Cerebral Cortex, 21(6), 1295–310.
color images of size 256 × 256 pixels. We used 100 random              Bosch, A., Zisserman, A., & Muñoz, X. (2008). Scene classi-
images per category for training and the rest (236 on average)            fication using a hybrid generative/discriminative approach.
for testing. As can be seen in Table 2, the combination of                IEEE Transactions on Pattern Analysis and Machine Intel-
shape and color significantly improves performance.                       ligence, 30(4), 712–27.
                                                                       Boynton, R. M., & Olson, C. X. (1987). Locating basic colors
Underwater Images Dataset                                                 in the OSA space. Color Research & Application, 12(2),
We also evaluated CQ-HMAX on the Underwater Images                        94–105.
dataset (Jalali, Tan, Lim, Tham, Ong, Seekings, & Taylor,              Brown, M., & Susstrunk, S. (2011). Multi-spectral SIFT for
2013). This dataset is made of 1664 images of around 740                  scene category recognition. In IEEE Conference on Com-
x 420 pixels from 13 different categories. We used 30 ran-                puter Vision and Pattern Recognition (CVPR) (pp. 177–
domly selected images per category for training and the rest              184). IEEE.
for testing. These underwater images contain small objects of          Burghouts, G. J., & Geusebroek, J.-M. (2009). Performance
various shapes and color against a varied seabed background.              evaluation of local colour invariants. Computer Vision and
The main challenge with these images is in light absorption               Image Understanding, 113(1), 48–62.
by the water, and the existence of particles that limit visibility     Conway, B. R., Chatterjee, S., Field, G. D., Horwitz, G. D.,
and result in scattering and reflection of light. In this exper-          Johnson, E. N., Koida, K., et al. (2010). Advances in color
iment, we created a set of images using both grayscale and                science: from retina to behavior. Journal of Neuroscience,
color cameras and compared the performance of CQ-HMAX                     30(45), 14955–63.
                                                                   2642

Conway, B. R., Moeller, S., & Tsao, D. Y. (2007). Special-          Lim, J. (1999). Learning visual keywords for content-based
  ized color modules in macaque extrastriate cortex. Neuron,          retrieval. In International conference on multimedia com-
  56(3), 560–73.                                                      puting and systems (Vol. 2, pp. 169–173).
Desimone, R., Schein, S. J., Moran, J., & Ungerleider, L. G.        Matuzawa, T. (1985). Colour naming and classification in a
  (1985). Contour, color and shape analysis beyond the stri-          chimpanzee. Journal of Human Evolution, 14, 283 – 291.
  ate cortex. Vision Research, 25(3), 441–52.                       Mutch, J., Knoblich, U., & Poggio, T. (2010). CNS: a GPU-
Edwards, R., Xiao, D., Keysers, C., Földiák, P., & Perrett,         based framework for simulating cortically-organized net-
  D. (2003). Color sensitivity of cells responsive to complex         works (Tech. Rep. No. MIT-CSAIL-TR-2010-013 / CBCL-
  stimuli in the temporal cortex. Journal of Neurophysiology,         286). Cambridge, MA: Massachusetts Institute of Technol-
  90(2), 1245–56.                                                     ogy.
Fei-Fei, L., Fergus, R., & Perona, P. (2004). Learning              Mutch, J., & Lowe, D. (2008). Object class recognition and
  generative visual models from few training examples: an             localization using sparse features with limited receptive
  incremental Bayesian approach tested on 101 object cat-             fields. International Journal of Computer Vision, 80(1),
  egories. In IEEE CVPR 2004 Workshop on Generative-                  45–57.
  Model Based Vision (Vol. 2).                                      Oliva, A., & Torralba, A. (2001). Modeling the shape of
Fukushima, K. (1980). Neocognitron: A self-organizing                 the scene: A holistic representation of the spatial envelope.
  neural network model for a mechanism of pattern recogni-            International Journal of Computer Vision, 42(3), 145–175.
  tion unaffected by shift in position. Biological Cybernetics,     Palmer, S. E. (1999). Vision science: Photons to phenomenol-
  36(4), 193–202.                                                     ogy. Cambridge, MA: MIT Press.
Gevers, T., & Stokman, H. (2004). Robust histogram                  Riesenhuber, M., & Poggio, T. (1999). Hierarchical models
  construction from color invariants for object recognition.          of object recognition in cortex. Nature Neuroscience, 2,
  IEEE Transactions on Pattern Analysis and Machine Intel-            1019–1025.
  ligence, 26(1), 113–118.                                          Serre, T., Oliva, A., & Poggio, T. (2007). A feedforward
Heywood, C. A., Shields, C., & Cowey, A. (1988). The                  architecture accounts for rapid categorization. Proceedings
  involvement of the temporal lobes in colour discrimination.         of the National Academy of Sciences, 104(15), 6424.
  Experimental Brain Research., 71(2), 437–41.                      Serre, T., Wolf, L., Bileschi, S., Riesenhuber, M., & Pog-
Horel, J. A. (1994). Retrieval of color and form during sup-          gio, T. (2007). Robust object recognition with cortex-like
  pression of temporal cortex with cold. Behavioural Brain            mechanisms. Pattern Analysis and Machine Intelligence,
  Research, 65(2), 165–72.                                            IEEE Transactions on, 29(3), 411–426.
                                                                    Shahbaz Khan, F., Anwer, R. M., Weijer, J. Van de, Bag-
Hurvich, L. M. (1981). Color Vision. Sutherland, MA.:
                                                                      danov, A. D., Vanrell, M., & Lopez, A. M. (2012). Color
  Sinauer Associates Inc.
                                                                      attributes for object detection. In Conference on Computer
Jalali, S., Lim, J., Ong, S., & Tham, J. (2010). Dictionary of
                                                                      Vision and Pattern Recognition (CVPR) (pp. 3306–3313).
  features in a biologically inspired approach to image classi-
                                                                    Stoughton, C. M., & Conway, B. R. (2008). Neural basis for
  fication. In Internationl Conference on Neural Information
                                                                      unique hues. Current Biology, 18(16), R698–9.
  Processing (ICONIP) (pp. 541–548). Springer.
                                                                    Tang, J., Miller, S., Singh, A., & Abbeel, P. (2012). A tex-
Jalali, S., Lim, J., Tham, J., & Ong, S. (2012). Clustering and
                                                                      tured object recognition pipeline for color and depth image
  use of spatial and frequency information in a biologically
                                                                      data. In IEEE International Conference on Robotics and
  inspired approach to image classification. In International
                                                                      Automation (ICRA) (pp. 3467–3474).
  Joint Conference on Neural Networks (pp. 1–8).
                                                                    Theriault, C., Thome, N., & Cord, M. (2011). HMAX-S:
Jalali, S., Tan, C., Lim, J., Tham, J., & Ong, S. (2013). CQ-         deep scale representation for biologically inspired image
  HMAX: A New Biologically Inspired Color Approach to                 classification. In International Conference on Image Pro-
  Image Classification. (Manuscript under preparation).               cessing (pp. 3–6).
Jalali, S., Tan, C., Lim, J., Tham, J., Ong, S., Seekings, P.,      Valberg, A. (2001). Unique hues: an old problem for a new
  et al. (2013). Encoding Co-occurrence of Features in the            generation. Vision Research, 41(13), 1645–57.
  HMAX Model. In Proceedings of the Annual Conference               Van De Weijer, J., & Schmid, C. (2006). Coloring local fea-
  of the Cognitive Science Society.                                   ture extraction. European Conference on Computer Vision
Komatsu, H. (1993). Neural coding of color and form in                (ECCV), 334–348.
  the inferior temporal cortex of the monkey. Biomedical            Yasuda, M., Banno, T., & Komatsu, H. (2010). Color selec-
  Research, 14, 7–13.                                                 tivity of neurons in the posterior inferior temporal cortex of
Komatsu, H. (1998). Mechanisms of central color vision.               the macaque monkey. Cerebral Cortex, 20(7), 1630–46.
  Current Opinion in Neurobiology, 8(4), 503–8.                     Zhang, J., Barhomi, Y., & Serre, T. (2012). A new bi-
Komatsu, H., Ideura, Y., Kaji, S., & Yamane, S. (1992). Color         ologically inspired color image descriptor. In European
  selectivity of neurons in the inferior temporal cortex of the       Conference on Computer Vision (Vol. 7576, pp. 312–324).
  awake macaque monkey. Journal of Neuroscience, 12(2),               Springer.
  408–24.
                                                                2643

