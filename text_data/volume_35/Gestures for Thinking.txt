UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Gestures for Thinking
Permalink
https://escholarship.org/uc/item/0zk7z5h9
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Jamalian, Azadeh
Giardino, Valeria
Tversky, Barbara
Publication Date
2013-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                                                  Gestures for Thinking
                                         Azadeh Jamalian (aj2334@columbia.edu)
                                   Teachers College, Columbia University, 525 W. 120th Street
                                                     New York, NY 10027 USA
                                        Valeria Giardino (Valeria.Giardino@ens.fr)
                                  Institut Jean Nicod, Ecole Normale Supérieure, 29, rue d'Ulm,
                                                             F-75005 Paris
                                         Barbara Tversky (btversky@stanford.edu)
                                   Teachers College, Columbia University, 525 W. 120th Street
                                                     New York, NY 10027 USA
                            Abstract                                 comprehended and learned felicitously, yielding an
  Can our gestures help us think, and, if so, how? Previous
                                                                     integrated external model of the information that can be
  work suggests that they can. Here, students, alone in a room,      inspected and mentally manipulated (e. g., Tversky, 2011).
  studied descriptions of environments for later tests of            Gestures are crude, and as such almost necessarily abstract.
  knowledge. The majority of participants spontaneously              They can also create integrated external models. In
  gestured while reading the descriptions, and most of those         explaining complex environments or scientific systems,
  also gestured while answering true-false questions. They did       people produced a coordinated and integrated series of
  not gesture proportionately more time for environments with        gestures that modeled the spaces of environment (Emmorey,
  many landmarks than for environments with few. Their
  gestures laid out the environments, primarily using points to      Tversky, and Taylor, 2000), family trees (Enfield, 2003),
  places and lines for paths. Descriptions and questions             and scientific processes (Kang, Tversky, and Black, 2013)
  accompanied by gestures were remembered more accurately.           to be learned.
  Participants rarely looked at their hands. Gestures seem to           People gesture to explain spatial environments to others,
  promote learning by establishing embodied representations of       creating external models with their hands. Will they do so
  the environments.                                                  for themselves, as aids to comprehension and memory?
  Keywords: Gesture; embodiment; spatial representation;             Here, we investigate whether people, alone in a room
  spatial memory; route/survey perspectives; navigation.             studying descriptions of complex environments will gesture
                                                                     for themselves. If so, what is the nature of their gestures?
                        Introduction                                 And does gesturing help them learn and remember the
Gestures serve many ends and have many forms. People                 environments?
gesture in communications to others, but also for                       Gesturing could help learning and memory indirectly by
themselves, that is, they gesture to think (Goldin-Meadow,           off-loading memory to another modality. Gestures have
2003; McNeill, 1992). Gestures for thinking help thinking in         been shown to be effective in off-loading memory during
different ways. They help people find words (Krauss &                explanations (Goldin-Meadow, et al, 2001). But gestures
Hadar, 2001). They offload memory (Cook, Yip, & Goldin-              could also help learning and memory in direct ways, by
Meadow, 2012; Goldin-Meadow, Nusbaum, Kelly, &                       constructing an external model of the environment to be
Wagner, 2001). They help people perform mental rotation              learned. Half the environments participants studied had 4
(Chu & Kita, 2008; Wexler, Kosslyn, & Berthoz, 1998;                 landmarks and half had 8 landmarks; the latter should put
Wohlschlager & Wohlschlager 1998). They help people                  greater stress on working memory (e. g., Jonides, Lewis,
count (Carlson, Avraamides, Cary, & Strasberg, 2007).                Nee, Lustig, Berman, and Moore, 2008). If the primary role
  Gestures are actions in space, and as such, can readily            of gestures is to offload working memory, participants
represent spatial structures and spatial actions. In fact,           should gesture more when studying descriptions with more
gestures help people solve spatial problems (Kessell &               landmarks. If the primary role of gestures is to construct a
Tversky, 2006; Schwartz & Black, 1996). Interestingly, in            model of the environment, much like a diagram, then there
solving spatial problems, gestures can serve much like               is little reason to expect more gesturing for the
diagrams. When given paper and pencil during problem                 environments with more landmarks. Gestures can reflect
solving, one group diagrammed the same spatial problems              mental representations (e. g., Alibali, Bassok, Olseth, Syc,
that another group gestured to solve (Kessell & Tversky,             and Goldin-Meadow, 1999). Description perspective was
2006). Diagrams also offload memory, but they serve                  manipulated because route and survey descriptions yield
cognition in many other ways. Creating a good diagram                different mental representations early (but not late) in
entails extracting the crucial information and structuring it        learning (Lee and Tversky, 2005).
to represent a problem to be solved or information to be
                                                                 645

                           Method                                 Design. Each participant read four descriptions, one with 4
                                                                  landmarks and one with 8 landmarks from each perspective.
Participants. 48 (28 female, 20 male), primarily graduate
                                                                  The specific environment for each condition was chosen
students from Columbia University, were paid to participate
                                                                  from the set of three outdoor environments and three indoor
in the study. Participants were native English speakers or
                                                                  environments. All variables, size, perspective, environment,
have graduated from an English speaking high school.
                                                                  order were counter-balanced and appeared equally often
Descriptions. The environments had 4 or 8 landmarks.
                                                                  across participants.
There were three outdoor environments, Etna City,
                                                                  True-false Questions. Verbatim and inference statements
Chinatown, and the Financial district, and three indoor
                                                                  were designed for each description, 10 for the 8 landmark
environments, a spa, an electronics show, and a grocery
                                                                  environments and 6 for the 4 landmark environments. For
store. There were 8 landmarks and 4 landmarks versions of
                                                                  the 8 landmark environments, there were 2 statements taken
each of these.
                                                                  verbatim from the text with the same perspective, 2
   There were also versions of each environment from route
                                                                  statements taken verbatim from the text with the other
(R) or survey (S) perspectives. A route perspective takes an
                                                                  perspective, and 6 inference statements, 3 route, and 3
imaginary traveler, you, through an environment describing
                                                                  survey. For the 4 landmark environments, there were a total
the turns and landmarks with respect to “you” in terms of
                                                                  of 6 statements: 1 verbatim from the route perspective, 1
your left, right, front, and back. A survey perspective takes
                                                                  verbatim from the survey perspective, 2 inference from a
an overview of an environment and describes landmarks
                                                                  route perspective, and 2 inference from a survey
with respect to each other in terms of north-south-east-west.
                                                                  perspective. Inference statements could be verified from
The route descriptions always began with cardinal directions
                                                                  information provided in the descriptions. Half of the
so that participants could answer questions from a survey
                                                                  statements were true and the other half was false. The
perspective. The descriptions and the environments were
                                                                  statements were presented in a random order for each
based on earlier work (Taylor & Tversky, 1992).
                                                                  participant. Table 2 shows examples of true/false statements
   The average length of the 8R descriptions was 141 words,
                                                                  for Etna.
of the 8S descriptions, 127 words, of the 4R descriptions, 69
words, and of the 4S descriptions, 72 words. Table 1 shows
                                                                            Table 2: Examples of true/false statements
an example of a description of an outdoor environment with
4 landmarks from a survey perspective, and of an indoor
environment with 8 landmarks from a route perspective.                                 Verbatim                 Inference
                                                                     Route      Going east on River       From Mountain Rd,
              Table 1: Examples of descriptions                                 Highway, at the           turn right on River
                                                                                intersection with         Highway and you
  Example 1: 4S outdoor environment                                             Mountain Rd, you will     will have the Gas
  Etna is a charming town nestled in an attractive valley,                      find a gas station on     Station on your
  entered on River Highway. River Highway runs east-                            your left.                right.
  west at the southern edge of the town of Etna. Toward             Survey      North of the gas          South of Maple Ave
  the eastern border, River Highway intersects with                             station, Mountain         to the west of
  Mountain Rd, which runs north of it. At the northwest                         Road will intersect       Mountain Rd is the
  corner of the intersection is a gas station. North of the                     with Maple Ave,           Gas Station.
  gas station, Mountain Road will intersect with Maple                          which runs east.
  Ave, which runs west.
                                                                  Procedure. Participants first signed a consent form,
  Example 2: 8R indoor environment                                assenting to participating in the experiment and to being
  Rock Creek Center is a showcase for new electronic              videotaped. They were additionally asked for permission to
  devices. Enter Rock Creek Center from the east side of          show their videos in presentations of the research. They then
  the building near the southeast corner. As you enter, you       completed a paper version of the Mental Rotation Task
  see, on the left wall, a Bulletin Board. Past the Bulletin      (Vandenberg & Kuse, 1978), a common test of spatial
  Board, on your right is the Video Camera room and on            ability.
  your left is the Office stretching to the corner of the            Participants were seated in front of a Mac OS X 10.7, as
  building. Past the office you are forced to turn right and      shown in Figure 1. Video records of the computer screen
  you will find the Cafeteria on your left stretching to the      and front views of participants were captured with
  corner of the building. After the Cafeteria, you are forced     Silverback© software, and participants’ side views with a
  to turn right and you will find a large room with Mobile        videocam. The experimenter explained the procedure to
  Phones on your left. On your right you will see the             each participant: “In this study you will be asked to read 4
  Televisions room. At the end of the hallway, turn right         text descriptions of environments. After reading each
  and you will find the Laptop Center on your left. Past the      description, your memory for the information in the text will
  Laptop Center, you will return to the entrance on your          be tested. You will start with a practice text description.
  left.                                                           Throughout the study, you will not have access to a
                                                              646

keyboard and will send commands to the computer with                 a third coder. One coder coded the remaining videos,
your voice.” The participants responded verbally, saying             discussing uncertain cases with the second coder.
“next”, “yes”, or “no” when appropriate, to advance from             Qualitative coding of the gestures is ongoing, but it is clear
screen to screen. Their responses were analyzed by the Mac           that gestures indicating places, primarily points, and
speech recognition program and used to advance screens               indicating connections between places, drawing lines or
and record responses. This left participants’ hands free to          placing the edge of a hand, predominate. Most gestures were
gesture, on or off the table.                                        performed on the table, but some were in the air (see
   Participants first had a practice trial. The first screen         Figures 1 and 2).
explained the task: “You will be asked to read the                   Gesture at study. Seventy-three percent of participants (35
description of an environment as practice. Once you are              out of 48) gestured at least once for at least one description
done reading the description say aloud “Next”. After the             during study. Twelve participants (25%) gestured for all
description you will be asked to judge the truth of some             four descriptions, 7 gestured for three, 10 gestured for two,
statements about the environment. You may take as much as            and 6 for only one. Notably, number of landmarks in the
time you need.” Then participants read a description of an           environments (4 vs. 8) did not influence whether
amusement park. The complete description was on the                  participants gestured at study, 𝝌2(1, N= 48)= 1.132, p=
screen. Participants were free to read the practice and              0.289. Similarly, neither perspective (route vs. survey),
experimental descriptions as long as they liked. Immediately         𝝌2(1, N= 48)= .023, p= 0.879, order (1st, 2nd, 3rd, or 4th),
after reading the description, participants were presented
with 4 true/false questions, one on each screen. They said           𝝌2(3, N= 48)= 1.687, p= 0.171, or MRT score , F (1, 185)=
“yes” for true and “no” for false. After the practice trial, the     0.089, p= 0.765, influenced gesturing at study.
experimenter answered any questions the participant had,                For each participant, the percentage of time gesturing
and then left the room.                                              while studying was computed. Neither spatial ability F(1,
   Participants then proceeded through the experiment,               45)= 0.357, p= 0.553) nor gender (F(1, 45)= 0.505, p=
reading each of the four descriptions and answering the              0.481) affected the percent of time gesturing
corresponding true/false questions after each.                       Gesture at test. Sixty-five percent of participants (31 out of
                                                                     48) gestured at least once when verifying the true/false
                                                                     statements. Table 3 shows number of statements for which
                                                                     participants gestured both when studying and answering,
                                                                     only when studying, only when answering, or not at all, out
                                                                     of the total of 1526 statements (excluding 10 cases in which
                                                                     participants’ answers were missing).
  Figure 1: Experimental Setup. Participant gesturing while
                     studying description.
                           Results
Coding. Two trained coders, coded 10 of 48 videos for
gesturing while studying, Kappa = 0.76 (p < 0.001), for
length of time spent gesturing while studying, t(39)= 0.244,           Figure 2. Participant gesturing while answering question.
p= 0.809, for looking at their hands while gesturing while
studying, Kappa = 0.56 (p < 0.001), for studying time,                    Table 3: Number of questions and gesture behavior
t(39)= 1.402, p= 0.169, for gesturing while verifying
statements, Kappa = 0.90 (p < 0.001), for looking at their             Gesturing                      Frequency       Percentage
hands while gesturing in verifying statements, Kappa = 0.44            Both at study and when            547            35.8%
(p < 0.001), and for length of time to verify statements,              verifying
t(359)= 0.120, p= 0.90. Any movement of hands or fingers,              Only at study                     220            14.4%
excluding beat gestures, was coded as gesturing. Any glance            Only when verifying                21             1.4%
at hands while gesturing was coded as looking. The coded               None                              738            48.4%
duration of the gesture included active movements and
periods when individuals left their hands still on the table or         As evident from Table 3, participants were far more likely
in mid-air in a certain position and form. Times were coded          to gesture to verify statements for the descriptions they
from the Silverback© videos of the screen and by using               gestured at study. Only 1.4% of the questions received
ELAN software. In cases of disagreement coders consulted
                                                                 647

gestures at verification that had not received gestures at              The effects of gesturing at verification were analyzed
study.                                                                separately. Participants were more likely to be accurate
   Moreover, for 85% of the descriptions accompanied by               verifying statements when they gestured (M= 0.814, SD=
gesture, at least one question was also accompanied by                0.23), than when they did not (M= 0.757, SD= 0.29), F(1,
gesture. Participants, who did not gesture at all while               1515)= 5.325, p= 0.038 < 0.05. As before, accuracy
studying the descriptions did not gesture when answering              increased with spatial ability, F(1, 1515)= 10.191, p=0.001
questions. Specifically, 27% of participants (13 out of 48)           < 0.01, and was affected by statement category in the same
did not gesture either at study or at verification.                   ways as the previous analysis, F(1, 1515)= 17.084, p <
  Overall, neither the environment’s perspective (survey vs.          0.001.
route), 𝝌2(1, N= 48)= .743, p= 0.389, nor question
perspective, 𝝌2(1, N= 48)= .264, p= 0.608, nor number of                                       0.85
landmarks (4 vs. 8), 𝝌 (1, N= 48)= .028, p= 0.868, nor type
                                   2
                                                                            Average Accuracy
of statement (verbatim vs. inference), 𝝌2(1, N= 48) = .439,                                     0.8
p= 0.508, nor MRT scores, F(1, 1520) = 0.899, p= 0.343
influenced whether participants gestured at verification.
   In short, most participants gestured while studying and                                     0.75
verifying and most who gestured at verification had also
gestured at study. Neither spatial ability nor length nor
perspective of the descriptions or questions affected whether                                   0.7
participants gestured.                                                                                Gesture When   No Gesture When
Accuracy. As evident in Figure 3, when participants had                                                 Verifying       Verifying
gestured at study, they were more likely to be accurate at
testing (M= 0.821, SD = 0.29) than when they had not                          Figure 4: Accuracy by gesturing at verification.
gestured at study (M= 0.743, SD = 0.30), F(1, 1517) =
8.249, p=0.004 < 0.01. Not surprisingly, accuracy was                    To examine the effects of gesture at study and gesture at
higher for the 4 landmark environments (M= 0.810, SD=                 response, participants were divided into 4 groups: gesture at
0.24) than for the 8 landmark environments (M= 0.760, SD=             both, gesture only at study, gesture only at response, no
0.28), F(1, 1517)= 6.561, p= 0.011 < 0.05. Accuracy                   gesture. Gesture behavior had an effect on accuracy, F(3,
improved with spatial ability, F(1, 1517)= 10.210, p= 0.001           1494)= 3.593, p= 0.013 < 0.05. Post-hoc analyses showed
< 0.01 but the correlation between accuracy and spatial               that participants were more accurate at testing when they
ability was low and not significant. Accuracy varied with             had gestured both at study and verification (M= 0.780, SD=
kind of statement, F(1, 1517)= 7.182, p < 0.001. Replicating          0.27), than when they did not gesture at all (M= 0.705, SD=
Taylor and Tversky (1992), post-hoc analyses showed that              0.32), t(1494)= 2.491, p= 0.013 < 0.05. Similarly, they were
verbatim statements (M= 0.838, SD= 0.21) were more                    more accurate when they only gestured at study (M=0.816,
accurate than inference statements (M= 0.720, SD= 0.31),              SD=0.23), than when they did not gesture at all, t(1494)=
t(1513)= 3.809, p < 0.01, and that for inference statements,          2.655, p= 0.008 < 0.01. However, there was not a significant
there was no advantage for statements in the perspective of           improvement for gesture only at response (M= 0.811,
reading (same perspective (M = 0.727, SD = 0.30); other               SD=0.25) than for no gesture, t(1494)= 0.333, p= 0.739; this
perspective (M = 0.718, SD = 0.31), t(1513) =0.311, p=                could be due to the severely limited number of cases in
0.756), indicating that memory representations were                   which participants only gestured at response (See Table 3).
perspective-free.                                                        To make sure that the advantage of gesturing was not
                                                                      because the better learners gestured, comparisons were done
                                                                      within participants who gestured when studying two or three
                      0.85                                            descriptions, but not all descriptions. For those who
   Average Accuracy
                                                                      gestured sometimes, accuracy was higher when they
                       0.8
                                                                      gestured at study (M= 0.762, SD = 0.29) than when they did
                      0.75                                            not (M= 0.677, SD = 0.35), F(1, 513) = 3.938, p= 0.048 <
                                                                      0.05. Similarly, they were more accurate verifying
                       0.7
                                                                      statements when they gestured (M= .764, SD= 0.29) than
                      0.65                                            when they did not gesture (M= 0.628, SD= 0.35), F(1,
                             Gesture at Study   No Gesture at         513)= 3.910, p= 0.049 < 0.05. So, gesturing itself helps - it
                                                   Study              is not just that those who tend to gesture also remember
                                                                      better.
     Figure 3. Accuracy by gesturing at study. Error bars             Studying Times. As expected, participants took longer to
                  represent standard error                            study the longer descriptions with 8 landmarks (M=
                                                                      112.14sec, SD= 28.43) than the shorter ones with 4
                                                                      landmarks (M= 56.57sec, SD=28.43), F(1, 187)= 94.104, p
                                                                648

< 0.001. Gesturing did not influence study time, F(1, 187)=                                      descriptions accompanied by gestures were remembered
1.212, p= 0.272. Similarly, neither spatial ability, F(1, 187)=                                  better than those that were not, and the questions that were
2.198, p= 0.140, nor text perspective, F(1, 187)= 0.101, p=                                      accompanied by gestures were answered more accurately
0.752, affected study times.                                                                     than those that were not. The advantage of gesturing on
Verification Times. Figure 5 shows that gesture behavior                                         memory cannot be explained as the better participants both
influenced verification time, F(3, 1441)= 3.431, p= 0.016 <                                      gestured and remembered better. Even within those
0.05. Post-hoc Bonferroni comparisons showed that                                                participants who frequently gestured, gesturing at study and
participants were faster to verify statements when they had                                      at responding improved memory. Gestures modeled the
only gestured at study (M= 8.95 sec, SD= 2.61) than when                                         structures of the environments, pointing to places and
they had not gestured at all (M= 10.35 sec, SD= 4.16), p <                                       outlining paths between places. Except on rare occasions,
0.001. By contrast, answering took longer when participants                                      participants did not look at their hands as they gestured,
only gestured at verification (M= 15.65 sec, SD= 6.19) than                                      suggesting that it is the actions per se that serve
when they only gestured at study, p=0.004 < 0.01. There                                          comprehension and learning, rather than the visual
was no difference on verification time when participants                                         accompaniments. Overall, spontaneous gesturing at learning
gestured both at study and at verification (M= 11.47 sec,                                        and spontaneous gesturing at memory retrieval promoted
SD= 3.88), compared to when they did not gesture at all.                                         learning. Gestures appeared to improve learning by
Spatial ability, perspective, and size of environment did not                                    establishing embodied representations of the structures of
effect verification times. Thus gesture at study decreased                                       the environments and appear to improve memory by
verification time while gesture at responding increased                                          redintegrating the queried parts of the environments.
verification time, and in cases when they gestured both at                                          In addition to providing embodied representations of the
study and at verification, the two effects cancelled each                                        environments, gestures might also have served to offload
other.                                                                                           memory, as in previous research (e. g., Cook, et al., 2012;
                                                                                                 Goldin-Meadow, et al., 2001), just as diagrams offload
                                                                                                 memory. However, the proportion of study time gesturing
                           18                                                                    did not increase as memory load increased from light to
                           16                                                                    heavy. Thus, the role of gesture in lightening memory load
Verification Times (sec)
                           14                                                                    appears to be less important for comprehending and learning
                           12                                                                    complex environments than other features of gestures,
                           10                                                                    notably, creating embodied representations.
                            8                                                                       Gestures are actions, and thereby provide an additional
                                                                                                 code beyond the verbal code participants read. Multiple
                            6
                                                                                                 codes in multiple modalities are known to promote memory
                            4
                                                                                                 (e. g., Paivio, 1986). Motor codes in particular augment
                            2                                                                    memory (e. g. Engelkamp & Zimmer, 1994; Hommel,
                            0                                                                    Musseler, Aschersleben, & Prinz, 2001) but the cases that
                                   No Gesture   Gesture at    Gesture at Gesture at Both
                                                Study Only   Verification  Study and             have been studied have primarily been cases where the
                                                                Only      Verification           memory was for the action per se. In the present case, the
                                                                                                 actions served memory not for the actions but rather for
                                Figure 5: Verification time by gesture behavior                  what the actions represented.
                                                                                                    Actions, like diagrams and words, can represent, that is,
Did participants look at their hands while gesturing? For                                        they can stand for something other than themselves.
the most part, participants did not look at their hands while                                    Certainly for the case of words but also for the case of
gesturing; they looked at their hands for 35.8% of the texts                                     diagrams, representation seems to be their primary function.
during reading but they were typically brief glances. Out of                                     Not so for actions. Actions can represent, but they are
the 35 participants who gestured at least once when reading                                      primarily used for the ordinary (and extraordinary) tasks of
texts, 15 never looked at their hands. At verification,                                          life, manipulating objects and navigating environments.
participants looked at their hands for less than 10% of the                                      Gestures are a special class of actions that serve to represent
statements they gestured while verifying. Out of the 31                                          rather than to act on or in the world. Similar to diagrams,
participants who gestured for at least one of the statements,                                    gestures can represent more directly than purely symbolic
16 never looked at their hands.                                                                  words; they bear some resemblance to what they represent
                                                                                                 (e. g., Tversky, 2011).
                                                Discussion                                          Like diagrams, gestures can use space to represent ideas
Participants, alone in a room, read descriptions of a variety                                    that are spatial or metaphorically spatial (e. g., Enfield,
of complex environments that they were to learn for later                                        2003; Emmorey, et al., 2000; Tversky, 2011; Tversky,
questions. While they were studying, most of them gestured                                       Heiser, Lee, & Daniel, 2009). Like diagrams, gestures are
at least once, and the majority gestured for most of the                                         spatial and visual. However, it seems that the spatial and
descriptions, in the absence of any communication. The                                           action components of representational gestures serve
                                                                                           649

comprehension and memory rather than the visual.                   Jonides, J., Lewis, R. L., Nee, D. E., Lustig, C. A., Berman,
Participants rarely looked at their hands. Researchers in art,       M. G., and Moore, K. S. (2008). The mind and brain of
sketching, and design refer to drawing as gesture.                   short-term memory. Annual Review of Psychology, 19,
Blindfolded architects gesture copiously as they design, and         193-224.
they cannot see either their gestures or their designs.            Kang, S., Tversky, B. & Black, J. B. (2013) Gesture and
Nevertheless, their designs equal those they create without          speech in explanations to experts and novices. Submitted.
blindfolds (Bilda and Gero, 2006). Together, these findings        Kessell, A. M. & Tversky, B. (2006). Using gestures and
suggest that some of the benefits of gesturing to those who          diagrams to think and talk about insight problems. In R.
gesture may be the embodiment of thought into action.                Sun and N. Miyake (Eds) Proceedings of the 28th Annual
                                                                     Conference of the Cognitive Science Society. P. 2528.
                    Acknowledgments                                Krauss R. M. & Hadar U. (2001). The role of speech-related
The authors are indebted to NSF IIS-0725223, NSF IIS-                arm/hand gestures in word retrieval. In R. Campbell & L.
0855995 and NSF IIS-0905417 for supporting parts of the              Messing, (Eds.) Gesture, speech, and sign. Pp. 93-116.
research and manuscript preparation. Valeria Giardino is             Oxford: Oxford University Press.
grateful to The Italian Academy for Advanced Studies in            Lee, P. U. and Tversky, B. (2005). Interplay between visual
America at Columbia University for supporting her visit.             and spatial: the effect of landmark descriptions on
                                                                     comprehension of route/survey spatial descriptions.
                                                                     Spatial cognition and computation, 5, 163-185.
                        References                                 McNeill, D. (1992). Hand and mind: What gestures reveal
Alibali, M. W., Bassok, M., Olseth, K. L., Syc, S. E., &             about thought. Chicago: University of Chicago Press.
   Goldin-Meadow, S. (1999). Illuminating mental                   Paivio, A (1986). Mental representations: a dual coding
   representations through speech and gesture. Psychological         approach. Oxford: Oxford University Press.
   Science, 10, 327-333.                                           Schwartz, D. L., & Black, J. B. (1996). Shuttling between
Bilda, Z and Gero, J.S. (2006) Reasoning with internal and           depictive models and abstract rules: Induction and
   external representations: A case study with expert                fallback. Cognitive Science, 20, 457– 497.
   architects. In R Sun (ed), CogSci/ICCS 2006, Lawrence           Taylor, H. A., & Tversky, B. (1992). Spatial mental models
   Erlbaum, pp. 1020- 1026.                                          derived from survey and route descriptions. Journal of
Carlson, R. A., Avraamides, M. N., Cary, M., & Strasberg,            Memory and Language, 31, 261-282.
   S. (2007). What do the hands externalize in simple              Tversky, B. (2011). Visualizing thought. Topics in
   arithmetic? Journal of Experimental Psychology:                   Cognitive Science. 3, 499-535.
   Learning, Memory, and Cognition, 33(4), 747-756.                Tversky, B., Heiser, J., Lee, P. and Daniel, M.P. (2009).
Chu, M., & Kita, S. (2008). Spontaneous gestures during              Explanations in gesture, diagram, and word. In K. R.
   mental rotation tasks: Insights into the microdevelopment         Coventry, T. Tenbrink, & J. A. Bateman (Editors), Spatial
   of the motor strategy. Journal of Experimental                    language and dialogue. Pp. 119-131. Oxford: Oxford
   Psychology: General, 137, 706–723.                                University Press.
Cook, S. W., Yip, T. & Goldin-Meadow, S. (2012).                   Vandenberg, S. G. and Kuse, A. R. (1978). Mental
   Gestures, but not meaningless movements, lighten                  rotations: A group test of three-dimensional spatial
   working memory load when explaining math. Language                visualization. Perceptual Motor Skills, 47, 599-604.
   and Cognitive Processes, 27, 594-610.                           Wexler, M., Kosslyn, S. M., & Berthoz, A. (1998). Motor
Emmorey, K., Tversky, B., & Taylor, H. (2000). Using                 processes in mental rotation. Cognition, 68, 77–94.
   space to describe space: Perspective in speech, sign, and       Wohlschläger, A., & Wohlschläger, A. (1998). Mental and
   gesture. Spatial Cognition and Computation, 2, 157-180.           manual rotatation. Journal of Experimental Psychology:
Enfield, N. (2003). Producing and editing diagrams using             Human Perception and Performance, 24, 397–412.
   co-speech gesture: Spatializing non-spatial relations in
   explanations of kinship in Laos. Journal of Linguistic
   Anthropology, 13, 17-50.
Engelkamp, J. and Zimmer, H. D. (1994). Human memory:
   A multimodal approach. Seattle: Hogref and Huber.
Goldin-Meadow, S. (2003). Hearing gesture: How our
   hands help us think, Cambridge, Mass.: The Belknap
   Press.
Goldin-Meadow, S., Nusbaum, H., Kelly, S. D., & Wagner,
   S. (2001). Explaining math: Gesturing lightens the load.
   Psychological Science, 12, 516-522.
Hommel, B., Musseler, J., Aschersleben, G., & Prinz, W.
   (2001). The theory of event coding (TEC): A framework
   for perception and action planning. Behavioral & Brain
   Sciences, 24, 849-937.
                                                               650

