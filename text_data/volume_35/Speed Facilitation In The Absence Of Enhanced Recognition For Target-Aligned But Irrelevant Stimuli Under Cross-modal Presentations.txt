UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Speed Facilitation In The Absence Of Enhanced Recognition For Target-Aligned But
Irrelevant Stimuli Under Cross-modal Presentations

Permalink
https://escholarship.org/uc/item/1z1591mq

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Dewald, ANdrew
Sinnett, Scott

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Speed Facilitation In The Absence Of Enhanced Recognition For Target-Aligned
But Irrelevant Stimuli Under Cross-modal Presentations
Andrew D. Dewald (adewald@hawaii.edu)
Department of Psychology, University of Hawaii at Manoa
2530 Dole Street, Honolulu, HI 96822
Scott Sinnett (ssinnett@hawaii.edu)
Department of Psychology, University of Hawaii at Manoa
2530 Dole Street, Honolulu, HI 96822
Abstract
An ignored stimulus is later recognized at enhanced levels if it had
previously been aligned with a target from a separate task. This has
been demonstrated using both visual and auditory presentations.
Here we extend these findings to multisensory conditions.
Participants were required to detect immediate repetitions in a
sound or picture stream while ignoring superimposed words
presented in the opposite modality (either written or spoken,
respectively), and then underwent a surprise recognition test for
these words. Contrary to the previous unisensory examples
(Dewald, Sinnett, & Doumas, in press; Dewald & Sinnett, 2012), a
significant difference between recognition rates for target-aligned
and non-aligned words was not observed. However, a highly
significant difference in response latency was observed, with
target-aligned words being responded to much more quickly. This
finding was robust and observed when the surprise test was
presented in either the visual or auditory modalities, as well as
across modalities.
Key words: Attention, Multimodal Presentation, Response
latency, Cross-modal processing.

Introduction
Investigations of the relationship between attention and
perception have demonstrated significant learning
enhancements for certain stimuli in the absence of focused
attention (Seitz & Watanabe, 2003, 2005; Watanabe, Náñez,
& Sasaki, 2001). However, in order to observer these
enhancements a number of compulsory prerequisite
conditions were required. These included extended exposure
rates of unattended stimuli (a random dot motion display)
that were presented below threshold (a subset of dots moved
coherently and subliminally) and also temporally aligned
with a target from an attended secondary task. Under such
conditions, enhanced learning performance was observed
for the unattended stimuli in later motion discrimination
tasks (see, Seitz & Watanabe, 2003, 2005; Watanabe et al.,
2001). Curiously however, when presenting the same type
of stimuli (coherent motion) under the same conditions, but
at levels that are easily perceptible (i.e., suprathreshold), the
aforementioned learning enhancements vanish (Tsushima,
Sasaki, & Watanabe, 2006; Tsushima, Seitz, & Watanabe
2008). Thus, it appears that the relationship between
whether or not learning enhancements occur for irrelevant

stimuli is dependent on whether the initial presentation is
sub- or suprathreshold. It is important to note that the
investigations that collectively posit this idea have
exclusively used random-dot, coherent motion displays
(Seitz & Watanabe, 2003, 2005; Watanabe et al., 2001;
Tsushima et al., 2006; Tsushima et al., 2008). A natural
ensuing question, therefore, would be whether these
findings apply to stimuli that arguably demand a higher
level of processing?
Directly addressing this question, Dewald, Sinnett, and
Doumas (in press) adapted Seitz and Watanabe’s (2003,
2005, see also, Watanabe et al., 2001) motion detection task
to include a high-level irrelevant semantic stimulus (words)
in an inattentional blindness paradigm (see Rees et al., 1999
for a similar example of the paradigm). Specifically,
participants were required to respond to immediate picture
repetitions in a stream of serially presented line drawings,
while at the same time ignore a simultaneously presented
stream of superimposed words. The irrelevant word stream
contained a single, unchanging word aligned with the
presence of an immediate picture repetition (i.e., targetaligned) as well as seven additional words that were
superimposed over the non-repeated pictures (non-aligned;
i.e., analogous to exposure frequencies used by Seitz &
Watanabe, 2003). The findings demonstrated that, despite
attention being directed away from the task-irrelevant items
(i.e., the words), subsequent recognition of these previously
irrelevant items was nevertheless enhanced. Critically, this
enhancement only occurred for words that had been
presented simultaneously with a task-target in the previous
task (i.e., target-aligned) when compared to non-aligned
irrelevant words.
Similar enhancements for target-aligned stimuli have
been observed when measuring recognition performance for
irrelevant pictures that had appeared with targets (geometric
shapes) in a separate task (e.g., the attentional boost effect;
see Swallow & Jiang, 2010). Collectively, the findings by
both Dewald et al. (in press) and Swallow and Jiang seem to
paint a different picture than what was described earlier.
That is, explicit presentations lead to an enhancement in
recognition performance for previously target-aligned items.
This is the exact opposite of the inhibited performance
observed when explicit motion presentations were used as
the irrelevant stimulus (see Tsushima et al., 2006; Tsushima

2183

et al., 2008). Dewald et al. argue that the high saliency of
the irrelevant stimuli (i.e., written words rather than a lower
level stimulus) likely underpins the difference in findings,
assuming that the previous requisite condition of
simultaneous presentation is met.
Regardless of the direction of learning effects, the critical
component appears to whether the irrelevant stimulus is
temporally aligned with a task-relevant target in a previous
task (Seitz & Watanabe, 2003). As these investigations have
largely been conducted only in the visual modality, it is
important to extend these findings to other sensory
modalities in order to determine whether they extend
beyond the visual domain. Our recent work (Dewald &
Sinnett, 2012) recently explored this very question by
presenting an analogous paradigm using spoken words and
sounds (i.e., rather than pictures). A facilitation for targetaligned irrelevant stimuli was observed. Interestingly
however, the enhanced performance occurred only when the
surprise recognition task was presented in either the same
modality as the initial presentation (audition) or across
modalities (i.e., audiovisual presentations).
Despite vision being the dominant sense in humans
(Chandra, Robinson, & Sinnett, 2011; Colavita, 1974;
Posner, Snyder, & Davidson, 1980; Sinnett, Spence, &
Soto-Faraco, 2007), it is clear that the human perceptual
experience is multisensory in nature. Thus, it is important to
explore if the learning effects for irrelevant stimuli within
the same sensory modality extend across modalities, as this
will further inform how information is processed as a
consequence of attentional allocation both within, and
across modalities. Generally, performance improves when
comparing multisensory to unisensory presentations (see for
example Duncan, Martens, & Ward, 1997; Sinnett et al.,
2006; Toro, Soto-Faraco, & Sinnett, 2005; Wickens, 1984).
The enhanced recognition performance for cross-modal
presentations, when compared to unimodal presentations,
can be explained by numerous findings that suggest that the
capacity of the attentional system is increased if a
demanding unisensory task is divided across multiple
sensory modalities (i.e., multiple resources theory, see
Wickens, 1984). For instance, Sinnett et al. (2006)
demonstrated that under multimodal presentations,
inattentional blindness for words was ameliorated (i.e.,
perception improved) when compared with unimodal
conditions, regardless of the modality of word presentation
(see also Toro et al., 2005 for a similar example involving
statistical learning). These findings seem to provide support
for an attentional system that is segregated, such that each
sensory modality has access to individualized attentional
resources (Wickens, 1984, see also Duncan et al., 1997 for
an example using the attentional blink)
In the present investigation, we extend unimodal
examples of learning enhancements for task irrelevant but
target-aligned stimuli, to multimodal presentations. As
increased performance has been observed for such
presentations (see Duncan et al., 1997; Sinnett et al., 2006),
we would expect an overall increase in recognition

performance for both target-aligned and non-aligned items if
they are presented in a separate sensory modality from a
temporally aligned task-relevant target (e.g., more
attentional resources will be available for non-aligned
words). Of particular interest is whether or not the
comparatively higher scores for target-aligned words will
persist under cross-modal presentations. Interestingly, this
could possibly jettison the enhancement associated with
target-alignment if performance for non-aligned words
increases substantially (i.e., a ceiling effect). We presented
participants with multisensory visual and auditory streams
(adapted from those used in the unimodal conditions in
Dewald et al., in press and Dewald & Sinnett, 2012). This
resulted in one of the streams including spoken words with
distracting pictures, and the other having written words with
distracting sounds. The task was to respond to repetitions in
the target stream (i.e., sounds or pictures) and then to
subsequently recognize as many words that had been
previously presented (i.e., ignored) in the repetition
detection task.
The present study also investigates the nature of the
surprise recognition task. With the exception of our
previous work in the auditory modality (Dewald & Sinnett,
2012), all research involving this paradigm has presented
the recognition task in the visual modality, regardless of
whatever modality it was presented in during the repetition
detection task. As irrelevant stimuli in the exposure portion
of the experiment will be presented in either the auditory or
visual sensory modalities, it is necessary to examine if
subsequent recognition of these items is affected by whether
presentation is in a congruent modality. Our previous work
(Dewald & Sinnett, 2012) did precisely this and
systematically manipulated the modality of presentation
between exposure and recognition tests. Not surprisingly,
when irrelevant items were presented for recognition in the
same modality as the exposure (i.e., both visually or both
auditorally), learning effects were observed. However, when
irrelevant stimuli were presented for recognition in an
incongruent modality from their initial exposure, learning
enhancements failed to surface for irrelevant items that had
been temporally aligned with task-relevant targets (Dewald
& Sinnett, 2012). Lastly, cross-modal presentations lead to
the greatest magnitude of enhancement for the previously
aligned words in the surprise recognition test. This latter
outcome dovetails with previous investigations of
attentional allocation across sensory modalities in
perceptual and recognition tasks, suggesting that crossmodal presentations generally lead to superior performance
when compared to unimodal presentations (Dewald &
Sinnett, 2011; Duncan et al., 1997; Sinnett et al, 2006; Toro
et al., 2005). Accordingly, in the present experiment we also
presented the surprise recognition tests in the same or
different sensory modality, or across modalities. If primary
and secondary task modality congruence is a factor as it was
in Dewald and Sinnett (2012), then we expect improved
results for congruent matchings vs. incongruent matchings
between exposure and recognition tasks, and potentially an

2184

additional enhancement for multimodal presentations
(simultaneous visual and auditory presentation of the
stimulus in the recognition test) given that performance is
generally enhanced for multisensory presentations (see
Driver & Spence, 2004). Note, these modality specific
enhancements were only seen for target-aligned items.

Method
Participants. Seventy-four participants (n=74) were
recruited from the University of Hawai’i at Manoa in
exchange for course credit. A total of 46 participants were
assigned to the visual words and sounds condition and a
total of 28 participants assigned to the auditory words and
pictures condition. The uneven distribution of participants
across all conditions was a consequence of convenience
sampling. Participants were naïve to the experiment and
had normal or corrected to normal vision and hearing.
Written informed consent was obtained before participation
in the experiment occurred.
Materials. The exact same stimuli and design to create
streams were used here as in Dewald et al. (in press, for
visual stimuli) and Dewald and Sinnett (2012, for auditory
stimuli) except now with multimodal presentations (i.e.,
pictures presented with spoken words or sounds presented
with written words).
Attending to pictures with spoken words. A total of 50
pictures were selected from the Snodgrass and Vanderwart
(1980) picture database. Each of the pictures (on average 5
to 10 cm, rotated +/-30 degrees from upright so as to ensure
difficulty) was combined with eight one-to-two syllable,
high frequency English words (average length of five
letters; range 4–6) selected from the MRC psycholinguistic
database (Wilson, 1988). The overall average frequency of
the eight selected words was 361 per million, ranging
between 135 and 782. For the auditory presentation of the
words, a native English speaker’s voice was recorded
reading the list of selected words three times. Three blind
listeners chose the best exemplar of each spoken word, with
a fourth listener deciding which one was best in the event of
a tie. The selected recordings were edited using sound
editing software so that all items were the same presentation
length (350 ms) and average amplitude.
A stream of 960 picture-spoken word concatenated items
was created, with repeated pictures acting as task relevanttargets. The presentation stream was broken into eight
blocks of trials (120 each) in which an immediate picture
repetition occurred on average one out of every eight trials,
equating to 15 task-relevant target repetitions per block, for
a total of 120 trials of exposure to a task-relevant target (and
specific word, see below). Only eight total words were
superimposed over the 960 pictures. Note then that all word
types (aligned or non-aligned) were presented in equal
proportions (120 times each). This was done to parallel the
number of different motions used in Watanabe et al, (2001;
see also Seitz & Watanabe, 2003, 2005), so as to expose the

participants to an unchanging, single, irrelevant word,
although also having seven additional irrelevant words all
exposed at the same frequency. The same single word was
always temporally aligned with the presentation of an
immediately repeated picture target. The presentation was
pseudorandomized so that on average one out of every eight
trials was an immediate picture repetition (and, therefore,
the presentation of the same superimposed task-irrelevant
target word). Only one superimposed word was aligned with
all of the immediately repeated pictures for each participant.
Attending to sounds with written words. The exact same
procedure as above was employed but now with sounds,
instead of pictures, serving as the task-targets, and visually
presented words as the irrelevant stimuli. The sound stimuli
were extracted from a database of 100 familiar sounds and
were also edited to 350 ms and similar average amplitude
(see Sinnett et al., 2006). All other aspects were identical to
the previous condition (pictures and spoken words).
Surprise recognition task: For both conditions, a surprise
recognition test for the presented words was administered
after the completion of the repetition detection task. The test
consisted of a total of sixteen words (i.e., half came from the
previously presented words, while the other half consisted
of foil words that had never been presented before, average
frequency of 236 per million with a range of 165-399. The
word recognition tasks were randomized and presented by
DMDX
software
(http://www.u.arixona.edu/jforster/dmdx.htm) one at a time,
in either the visual or auditory modality, or across
modalities. For the visual presentation the words were
written in bold, capitalized letters in Arial font at a size of
24 points, and remained on the screen until a response was
made. For auditory presentations the words were spoken
just as they were in the initial repetition detection task.
Cross-modal presentations involved the written word on the
screen with the spoken word presented simultaneously.

Procedure
Participants were required to attend to the sound (or
picture) stream (i.e., they were explicitly instructed to
ignore
the
simultaneously
presented,
overlaid
written/spoken words) and respond to immediate repetitions
by pressing the ‘G’ key on the keyboard of the computer.
Each item in the sound-word (or picture-word) presentation
was presented for 350 ms with a 150-ms inter-stimulus
interval (ISI; silence) for a stimulus onset asynchrony
(SOA) of 500 ms. Before the first experimental block, a
training block of eight trials was given and repeated until
participants were familiar and comfortable with the task.
Immediately after the repetition detection task, the surprise
word recognition test was administered to all participants
(modality type of surprise task dependent on condition).
Participants were instructed to press the “B” key if they had
heard the word during the repetition detection task or,
instead, the “V” key if they had not heard the word before.

2185

Results
Target detection accuracy in the repetition detection
task. Overall performance accuracy (across all conditions)
of immediate target repetition detection revealed that
participants were successful at detecting target repetitions in
the primary task, (72% hit rate vs. 28% miss rate, t(73)=
14.67, p<.001).

Overall recognition accuracy. Across all conditions,
participants were accurate in recognizing the unattended
words (both target-aligned and non-aligned) displayed
during the repetition detection task at better than chance
levels (86.1% SE = 1.47, t (73) = 17.35, p < .001). A threefactor mixed design ANOVA was used to analyze overall
(across all conditions) recognition performance for all
words. Surprise test modality (auditory, visual, or crossmodal) and exposure modality (visual words vs. auditory
words) were between-subjects factors, and target alignment
(target-aligned or non-aligned) was a within-subjects factor.
There were no main effects for target alignment (F (1, 68) =
.217, p = .643), exposure modality in the primary task (F
(1, 68) = 2.68, p = .08), or surprise test modality (F (2, 68)
= .548, p = .580). A planed comparison further
demonstrated that target-aligned and non-aligned words
were recognized at statistically indistinguishable rates,
across all conditions (target-aligned: 89.1.0%, SE = 3.06;
non-aligned: 83.8%, SE = 2.15, t(68) = 1.30, p = .195,
Figure 1). Given these null results, no further analyses of
recognition performance were conducted.

aligned or non-aligned) as a within-subjects factor. A main
effect of target alignment confirmed that overall, the speed
of responding to words was significantly faster for targetaligned words (787.8 ms, SE = 21.8) when compared to
non-aligned words (1378.2 ms, SE=80.2) (F (1, 68) =
52.44,p = .001) (see Figure 2). No main effects were
observed for surprise test modality (F (1, 68) = .298, p =
.587) or exposure modality (F (2,68) = 1.80, p = .173), nor
were any interactions significant except for the three-way
interaction (F (2, 68) = 3.58, p = .03). To further explore
this interaction, further ANOVAs of response speed for each
condition were conducted.

Figure 2. Response latencies pooled across all
conditions for Target-Aligned (black bar) and Non-Aligned
(grey bar) words.

Attending to sounds with written words. A two factor

Figure 1. Recognition percentages pooled across all
conditions for Target-aligned words (black bar) and NonAligned (grey bar) words.

Overall recognition speed. To explore if response latency
to the words was modulated by target alignment or the
modality of presentation of the surprise task, the same threefactor mixed design ANOVA was conducted as above, with
surprise test modality (auditory, visual, or cross-modal) and
exposure modality (visual words vs. auditory words) as
between-subjects factors, and target alignment (target-

mixed design ANOVA was conducted for response latencies
with surprise test modality as a between subjects factor and
target-alignment as a within-subjects factor. A main effect
of target alignment (F (1, 43) = 34.97, p < .001) was
observed, demonstrating that participants responded more
quickly to target aligned (812.2 ms, SE = 29.3) when
compared with non-aligned words (1371 ms, SE = 93.8).
There was no main effect for surprise test modality (F(2, 43)
= .237, p = .790) nor was there a significant interaction (F(2,
43) = 1.28, p = .286. Planned comparisons also confirmed
that when examining response latency in the surprise word
recognition task, target-aligned words were responded to
significantly faster than non-aligned words in all conditions
(Visual Presentation: Target-aligned: 789.2 ms, SE=38.4 vs.
Non-aligned: 1293.7, SE = 217.4, t (16) = 2.92, p = .02;
Auditory Presentation: Target-aligned: 717.8 ms, SE = 63.4,
vs. Non-aligned: 1519.2 ms , SE = 38.4, t (11) = 5.31, p =
.001; Multimodal Presentation: Target-aligned: 901.9 ms,
SE = 46.1 vs. 1320 ms, SE = 72.7, (t (16) = 4.86, p = .001).
Further confirming the non-significant interaction, there
were no significant differences in performance between
conditions (all p > .58).

Attending to pictures with spoken words. The same two
factor mixed design ANOVA was conducted for response

2186

latencies with surprise test modality as a between subjects
factor, and target-alignment as a within-subjects factor.
Again, a main effect for target alignment (F(1, 25) = 19.57,
p < .001) was observed, demonstrating once more that
participants responded more quickly to target aligned (747.8
ms, SE = 30.9) when compared with non-aligned words
(1404.5 ms, SE = 147.2). while no main effect was observed
for the modality of the recognition test (F(2, 25) = 1.51, p =
.239). The interaction also failed to reach levels of
significance (F(2, 25) = 2.81, p = .079). Planned
comparisons also confirmed this in each modality
presentation in the surprise recognition task (Visual
Presentation: Target-aligned: 659.7 ms, SE = 67.8 vs. Nonaligned: 1833.9, SE = 368.9, t (8) = 2.84, p = .02; , Auditory
Presentation: Target-aligned: 819.8 ms, SE = 44.5 vs. Nonaligned: 1159.4 ms, SE = 116.5, t (9) = 3.10, p = .01;
Multimodal Presentation: Target-aligned: 755.9 ms, SE =
35.5 vs. 1247.3 ms, SE = 200.9, t (8) = 2.42, p = .04).
Despite the marginal interaction, there were no significant
differences in performance between conditions (all p > .05)..

Discussion
There are a number of outcomes that necessitate discussion,
as the present findings strengthen the understanding of how
unattended information is processed when it appears
simultaneously with an attended target, especially when
considering the multimodal exposures used here.
Specifically, the findings exhibit that both presentation
types (pictures with auditory words, or sounds with visually
presented words) lead to learning effects, exemplified by
high recognition rates in the surprise task, despite attention
not being directed to the words. This is similar to analogous
paradigms using only unimodal visual (Dewald et al, in
press) or auditory (Dewald & Sinnett, 2012) presentations.
However, both of these unimodal studies indicated
enhanced recognition rates for target-aligned words when
compared with non-aligned words. This was not the case
with cross-modal presentations, as observed here. That is,
although the recognition rates for the unattended stimuli
were high, there was no difference between target-aligned
and non-aligned items.
The lack of a significant difference in recognition rates
based on target alignment is likely due to the cross-modal
presentations used here. It is possible that the division of the
task could have permitted additional attentional resources to
focus on processing all of the words, as shown by the high
recognition rates for non-aligned words here (overall 84%).
While it is difficult to statistically compare this rate to our
previous studies (already published), it is worth noting that,
in analogous but unimodal paradigms, performance for nonaligned words was much lower in either the visual (68%,
Dewald et al., in press) or auditory modality (59%, Dewald
& Sinnett, 2012). Thus, it appears that by presenting the
repetition detection task across modalities, additional
resources were available that potentially enabled the
processing of irrelevant stimuli, resulting in arguably near

ceiling recognition rates for both aligned and non-aligned
words. This dovetails well with other research
demonstrating enhanced performance under multimodal
conditions (Duncan et al., 1997; Sinnett et al., 2006),
possibly indicating a segregation of attentional resources
across modalities (Wickens, 1984).
Despite the lack of a recognition difference between
target-aligned and non-aligned items, the former were
responded to significantly faster, regardless of the modality
of presentation (target-aligned: 787.8 ms vs. non-aligned:
1378.2 ms), suggesting alignment did play a role. That is, it
is possible that there was improved learning of words that
were temporally aligned with a task-relevant target,
indicated by response latencies to target-aligned words
being faster in all three recognition conditions (visual,
auditory, audiovisual). This is an intriguing finding as it
indicates a conceivable enhancement for target-aligned
material without explicit awareness, as there were no
differences in recognition performance. Although, it should
be acknowledged that recognition performance might have
been at ceiling levels and therefore masked any possible
improvement for target-aligned words. Regardless, this
finding warrants discussion, as well as further research.
Indeed, of the many studies published on this topic (see,
Dewald et al, in press, Dewald et al., 2011; Dewald &
Sinnett, 2012; Rees et al, 1999; Sinnett et al, 2006; Swallow
& Jiang, 2010; Tipper & Driver, 1988) the present
experiment is the first to use response latency as a potential
measure of enhancement for target-aligned material.
Also of key interest here, is that we did not observe an
interaction in performance between target-alignment and the
modality of the surprise test, as was observed by Dewald
and Sinnett (2012). Across all conditions, regardless of the
congruency between presentation and recognition task, there
was no significant difference between target-aligned and
non-aligned words. Accordingly, this suggests that, at least
in the present case, under multimodal presentation, the
modality of presentation does not need to match exposure
and test conditions. This could be a byproduct of the overall
enhanced recognition performance seen after cross-modal
presentations. A more systematic approach manipulating
presentation (unimodal vs. cross-modal) and surprise test
(congruent, incongruent, cross-modal) is required before
ruling out that this factor is unnecessary.
Collectively, the present findings provide insight into
how irrelevant information is processed when it is presented
simultaneously with an attended target across sensory
modalities. If certain prerequisite conditions are met,
unattended stimuli can be perceived and affect behavior,
perhaps even below levels of conscious awareness.
Additionally, although a significant difference was not
observed here, future research should consider the
congruency of modality presentation in both exposure and
testing conditions.

2187

References
Broadbent, D.E. (1958). Perception and communication.
London: Pergamon Press.
Dewald, A.D., Sinnett, S., & Doumas, L.A.A. (in press). A
Window of Perception When Diverting Attention?
Enhancing Recognition for Explicitly Presented,
Unattended, and Irrelevant Stimuli by Target
Alignment. Journal of Experimental Psychology:
Human Perception and Performance, 1(39)
Dewald, A.D., & Sinnett, S. (2012). An inhibited
recognition performance for explicitly presented
target-aligned irrelevant stimuli in the auditory
modality. Proceedings of the Twenty-Ninth Annual
Conference of the Cognitive Psychology Society
Dewald, A.D., Sinnett, S., & Doumas, L.A.A. (2011).
Conditions of directed attention inhibit recognition
performance for explicitly presented target-aligned
irrelevant stimuli. Acta Psychologica, 138, 6067.
Dewald, A.D., Sinnett, S., & Doumas, L.A.A. (2010). The
inhibition and facilitation of stimuli can be
modulated by the focus of direct attention.
Proceedings of the Twenty-Eight Annual
Conference of the Cognitive Psychology Society
Driver, J., & Spence, C. (2004). Cross-modal spatial
attention: Evidence from human performance.
In C. Spence & J. Driver (Eds.), Cross-modal space
and cross-modal attention. Oxford, UK: Oxford
University Press.
Duncan J, Martens S, Ward R (1997), Restricted attentional
capacity within but not between sensory
modalities. Nature 387(6635):808-10
Lin, J.Y., Pype, A.D., Murray, & Boynton, G.M. (2010).
Enhanced memory for scenes presented at
relevant points in time. PLoS Biol, 8(3), E1000337.
Lupker, S. J. (1984). Semantic priming without association:
A second look. Journal of Verbal Learning and
Verbal Behavior, 23, 709–733.
Rees, G., Russell, C., Frith, C. D., & Driver, J. (1999).
Inattentional blindness versus inattentional
amnesia for fixated but ignored words. Science,
286, 2504-2507.
Seitz, A. R., Kim, R., & Shams, L. (2006). Sound facilitates
visual learning. Current Biology, 16, 1422-1427.
Seitz, A. R. & Watanabe, T. (2003). Psychophysics: Is
subliminal learning really passive? Nature, 422,
36.
Seitz, A. R. & Watanabe, T. (2005). A unified model for
perceptual learning. Trends in Cognitive
Science, 9 (7), 329-334.

Seitz, A. R. & Watanabe, T. (2008). Is task-irrelevant
learning really task-irrelevant? PLoS ONE 3(11):
e3792
Sinnett, S., Costa, A., & Soto-Faraco, S. (2006).
Manipulating inattentional blindness within and
across sensory modalities. Quarterly Journal of
experimental Psychology, 59(8), 1425-1442
Snodgrass, J. G., & Vanderwart, M. (1980). A standardized
set of 260 pictures: Norms
for name
agreement, image agreement, familiarity, and
visual complexity.
Journal of Experimental
Psychology: Human Learning and Memory, 6, 1
74–215.
Spence, C., & Squire, S. (2003). Multisensory integration:
Maintaining the perception of synchrony. Current
Biology, 13, R519-R521.
Spence, C., & Driver, J. (1998). Cross-modal links in spatial
attention. Pilosophican Transactions of the Royal
Society :Biological Sciences, 352(1373), 13191331.
Swallow K. M., & Jiang, Y. V. (2010). The attentional
boost effect: Transient increases in attention to
one task enhance performance in a second task.
Cognition, 115, 118-132.
Tshushima, Y., Sasaki, Y., & Watanabe, T. (2006). Greater
disruption due to failure of inhibitory
control on an ambiguous distractor. Science, 314,
1786-1788.
Tsushima, Y., Seitz, A. R., & Watanabe, T. (2008). Taskirrelevant learning occurs only
when the
irrelevant feature is weak. Current Biology,18(12),
516-517.
Watanabe, T., Náñez,Y., & Sasak, S. (2001). Perceptual
learning without perception. Nature, 413, 844–
848.
Wickens, C. D. (1984). “Processing resources in attention",
in R. Parasuraman & D.R. Davies (Eds.), Varieties
of attention, (pp. 63–102). New York: Academic
Press.
Wilson, M. D. (1988). The MRC psycholinguistic database:
Machine readable dictionary, version 2.
Behavioural Research Methods, Instruments and
Computers, 20, 6-11.

2188

