UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
General and Efficient Cognitive Model Discovery Using a Simulated Student
Permalink
https://escholarship.org/uc/item/267572ft
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Li, Nan
Stampfer, Eliane
Cohen, William
et al.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

      General and Efficient Cognitive Model Discovery Using a Simulated Student
                                                       Nan Li (nli1@cs.cmu.edu)
                                               Eliane Stampfer (estampfe@cs.cmu.edu)
                                               William W. Cohen (wcohen@cs.cmu.edu)
                                          Kenneth R. Koedinger (koedinger@cs.cmu.edu)
                                         School of Computer Science , Carnegie Mellon University
                                                5000 Forbes Ave., Pittsburgh PA 15213 USA
                               Abstract                                 important instructional details may still be overlooked. Au-
                                                                        tomated search methods such as Learning Factor Analysis
   In order to better understand how humans acquire knowledge,
   one of the essential goals in cognitive science is to build a        (LFA) (Cen, Koedinger, & Junker, 2006) are more objective:
   cognitive model of human learning. Moreover, a cognitive             the algorithm searches through the space of human-provided
   model that better matches student behavior will often yield bet-     factors to find a cognitive model that best matches with hu-
   ter instruction in intelligent tutoring systems. However, man-
   ual construction of such cognitive models is time consuming,         man data. Although automated search methods have found
   and requires domain expertise. Further, manually-constructed         better models than manual construction, the quality of the dis-
   models may still miss distinctions in learning which are impor-      covered model depends on the quality of the human-provided
   tant for instruction. Our prior work proposed an approach that
   finds cognitive models using a state-of-the-art learning agent,      factors. If there is a better model that can not be expressed by
   SimStudent, and we demonstrated that, for algebra learning,          known factors, LFA will not be able to uncover it.
   the agent can find a better cognitive model than human experts.         In Li, Matsuda, Cohen, and Koedinger (2011), we have
   To ensure the generality of that proposed approach, we further
   apply it to three domains: algebra, stoichiometry, and frac-         proposed to use the state-of-the-art learning agent, SimStu-
   tion addition. To evaluate the quality of the cognitive models       dent (Matsuda, Lee, Cohen, & Koedinger, 2009), to auto-
   discovered, we measured how well the cognitive models fit to         matically discover cognitive models without depending on
   student learning curve data. In two of those domains, SimStu-
   dent directly discovers a cognitive model that predicts human        human-provided factors. SimStudent learns skill knowledge
   student behavior better than the human-generated model. In           from demonstration and problem solving experience. Each
   fraction addition, SimStudent supported discovery of a better        skill SimStudent acquires corresponds to a KC in the cogni-
   cognitive model in combination with another automated cog-
   nitive model discovery method.                                       tive model. To demonstrate the generality of this approach,
   Keywords: cognitive model, machine learning, simulated stu-          we present evaluations of the SimStudent-generated models
   dent                                                                 in three domains: algebra, stoichiometry, and fraction addi-
                                                                        tion. We validate the quality of the cognitive models using hu-
                           Introduction                                 man student data as in Koedinger and MacLaren (1997). In-
One of the fundamental goals in cognitive science is to un-             stead of matching with performance data, we use the discov-
derstand human knowledge acquisition. A cognitive model of              ered cognitive model to predict human learning curve data.
human learning that fits data would be a significant achieve-           Experimental results show that for algebra and stoichiometry,
ment. This goal also complements with another goal in edu-              SimStudent directly finds a better cognitive model than hu-
cation, which is to provide individualized instruction based            mans. For fraction addition, SimStudent results assist LFA
on students’ abilities, learning styes, etc. Cognitive mod-             in finding a better cognitive model than a domain expert. We
els provide intelligent tutoring systems with useful informa-           have also carried out an in-depth study using Focused Bene-
tion on the learning task difficulties and transfer of learning         fits Investigation (FBI) (Koedinger, McLaughlin, & Stamper,
among similar problems. A better cognitive model often leads            2012) to better understand this machine learning approach,
to more effective tutoring. A cognitive model is a system that          and discussed possible ways of further improvements.
can solve problems in the various ways human students can.
One common way of representing a cognitive model is a set                            A Brief Review of SimStudent
of knowledge components (KC) (Koedinger & McLaughlin,                   SimStudent is an intelligent agent that inductively learns
2010). The set of KCs includes the component skills, con-               skills to solve problems from demonstrated solutions and
cepts, or percepts that a student must learn to be successful           from problem solving experience. It is a realization of pro-
on the target tasks. For example, a KC “divide” in algebra              gramming by demonstration (Lau & Weld, 1998) using a
encodes how to proceed given problems of the form Nv = N                variation of the version space algorithm (Mitchell, 1982), in-
(e.g., −3x = 6), where N stands for a number, and v stands              ductive logic programming (Muggleton & Raedt, 1994), and
for a variable.                                                         iterative-deepening depth-first search as underlying learning
   Nevertheless, manual construction of cognitive models re-            techniques. For more details, please refer to Matsuda et al.
mains time consuming and error prone. Traditional ways                  (2009). Recently, in order to build a more human-like in-
to construct cognitive models include structured interviews,            telligent agent, we have developed a model of representation
think-aloud protocols, and rational analysis. Manual con-               learning, and integrated it into SimStudent’s skill acquisition
struction of cognitive models requires domain expertise, and            mechanism.
                                                                    894

 • Original:                        • Extended:                           the last row that SimStudent entered input.
 • Skill divide (e.g. -3x = 6)      • Skill divide (e.g. -3x = 6)
 • Perceptual information:          • Perceptual information:
                                                                             The second part of the learning mechanism is a precondi-
    • Left side (-3x)                  • Left side (-3, -3x)
                                                                          tion (i.e., “when”) learner, which acquires the descriptions of
    • Right side (6)                   • Right side (6)                   desired situations in applying the skill. The learner is given
 • Precondition:                    • Precondition:                       a set of feature predicates to get a basic understanding of the
    • Not has-constant-term            • Not has-constant-term            problem. Each predicate is a boolean function that describes
      (-3x)                              (-3x)
                                                                          relations among objects in the domain (e.g. (has-coefficient
 • Operator sequence:               • Operator sequence:
    • Get coefficient (-3) of left     • Get coefficient (-3) of left
                                                                          −3x)). The precondition learner utilizes FOIL (Quinlan,
      side (-3x)                         side (-3x)                       1990), an inductive logic programming system that learns
    • Divide both sides with the       • Divide both sides with the       Horn clauses from both positive and negative examples. The
      coefficient (-3)                   coefficient (-3)                 learning process is general to specific, where the precondi-
Figure 1: Original and extended production rules for divide               tion learner starts by considering all situations applicable, and
in a readable format.                                                     then gradually narrows down the condition based on negative
                                                                          examples. The precondition acquired for the example skill
Tutoring Strategy                                                         divide is (not (has-constant ?var-left)), which returns true if
To learn, SimStudent interacts with a tutor (human or auto-               the left side does not have a constant.
mated). Given a problem, if SimStudent does not know how
                                                                             The last component is the operator sequence (i.e., “how”)
to solve it, it will ask the tutor to demonstrate a next step. If
                                                                          learner. The learner is given a set of operator functions as
SimStudent knows how to proceed, it will propose the next
                                                                          prior knowledge. Operator functions specify (ideally) basic
step, and ask for feedback from the tutor. For each demon-
                                                                          manipulations (e.g. (add 1 2), (get-coefficient −3x)) that Sim-
strated step, the tutor specifies a tuple of hselection, action,
                                                                          Student can apply to the problem. Given all of the demon-
inputi (SAI tuple) for a skill along with a skill label (e.g., di-
                                                                          strated steps, the learning mechanism searches for the shortest
vide). For instance, a demonstrated step for skill “divide” is
                                                                          operator sequence that could explain all of the records, using
h(−3x, 6), input text, (divide − 3)i.
                                                                          iterative-deepening depth-first search. For example, given a
   SimStudent learns skills as production rules. The left side
                                                                          demonstrated step h(−3x, 6), (divide −3)i, the shortest ex-
of Figure 1 shows an example production rule for skill di-
                                                                          planation sequence is (bind ?coef (get-coefficient ?left-var))
vide in a readable format. A production rule shows “where”
                                                                          (bind ?output (divide ?coef)).
to look for useful information (i.e., perceptual information),
“when” to apply the skill (i.e., precondition), and “how” to                 There are two groups of operator functions, domain-
proceed (i.e., operator sequence). To illustrate, consider the            independent operator functions and domain-specific opera-
rule on the left side of Figure 1. It states that given an equa-          tor functions. Domain-independent operator functions (e.g.
tion (e.g., -3x = 6), if the left side does not have a constant           (add 1 2)) are basic skills applicable across multiple do-
term, then first get the coefficient of the left side (-3), and           mains. Human students often have knowledge of these simple
divide both sides by that coefficient.                                    skills prior to class. Domain-specific operator functions (e.g.
   Each skill corresponds to a KC. During training, the tutor             (add-term 5x − 2 5), (get-coefficient −3x)) are more compli-
can provide an initial set of KCs to SimStudent by labeling               cated skills that human students may not know before class.
each demonstrated step with a skill name. The label given to              Thus providing such operators to SimStudent may produce
SimStudent in the example production rule (i.e., the left side            learning behavior that is distinctly different from human stu-
of Figure 1) is “divide”. If no such initial KC is known, the tu-         dents (Matsuda et al., 2009). As we will explain in the next
tor can simply label all of the steps with the same skill name.           subsection, by integrating representation learning with skill
SimStudent’s learning algorithm will automatically create the             learning, we can reduce or remove SimStudent’s dependency
cognitive model as needed.                                                on domain-specific operator functions.
                                                                             Finally, let’s talk about how the KCs are discovered. Sim-
Skill Learning                                                            Student starts with a given set of skill labels associated with
SimStudent has three learning components - each acquires                  demonstrated steps. SimStudent tries to learn one rule for
one part of the production rules. The first component is a                each label. It will fail when the perceptual information learner
perceptual information (i.e., “where”) learner that acquires              cannot find one path that covers all demonstrated steps, or the
the path to identify the useful information from its environ-             operator sequence learner cannot find one operator function
ment. In our case, the environment is a graphical user inter-             sequence that explains all records. In that case, SimStudent
face, but it could also be a physical world or an educational             learns a disjunctive rule just for the last record. This effec-
game. The elements in the environment are organized in a                  tively splits the examples into two clusters. Later, for each
tree structure. SimStudent learns perception by moving from               new record, SimStudent tries to acquire a rule for each of the
specific to general. That is, SimStudent tries to find the least          clusters with the new record, and stops whenever it success-
general path in the perceptual hierarchy that covers all of the           fully learns a rule with one of the clusters. If the new record
selections in the demonstrated steps. In the example skill ”di-           cannot be added to any of the existing clusters, SimStudent
vide,” the left and right sides of the equation can be found in           creates another new cluster. By the end of learning, the set of
                                                                      895

                                                                     approach, in this paper, we tested SimStudent in three do-
               Expression                     Expression             mains, algebra, stoichiometry, and fraction addition.
                                                                     Method
       SignedNumber Variable           MinusSign       Variable
                                                                     In each domain, we compared the SimStudent model with the
                                                                     best human-generated model available, made by domain ex-
   MinusSign Number                                                  perts. To generate the SimStudent model, SimStudent was
                                                                     trained by interacting with automated tutors that simulate the
                                                                     automated tutors used by human students in the studies. The
                   3        x                              x
                                                                     video demonstration in the original study was not used in
         Figure 2: Different parse trees for -3x and -x.             training SimStudent. SimStudent was trained on problems
                                                                     used by humans students. Then, for each step a human stu-
clusters defines a new cognitive model.                              dent performed, we assigned the applicable production rule
                                                                     as the KC associated with that step. In cases where there
Integrating Representation Learning with Skill                       was no applicable production rule, we coded the step using
Learning                                                             the human-generated cognitive model. Each time a student
                                                                     encounters a step using some KC is considered as an oppor-
As we can see, the prior knowledge given to SimStudent
                                                                     tunity for that student to show mastery of that KC.
(e.g., the perceptual hierarchy, the operator functions) largely
affects the cognitive model it discovers. The more knowl-                To evaluate the quality of the cognitive model, we mea-
edge engineering needed, the less human-like SimStudent is.          sured how well the cognitive model fits with human student
Therefore, to get a better cognitive model, we need to reduce        data using the Additive Factor Model (AFM) (Cen et al.,
the amount of knowledge engineering required in construct-           2006) to validate the coded steps. AFM is an instance of
ing SimStudent. Previous studies (Chase & Simon, 1973)               logistic regression that predicts the probability of a student
have shown that one of the key differences between experts           making an error on the next step given each student, each
and novices is their different representations of the world.         KC, and the KC by opportunity interaction as independent
Recently, we have extended SimStudent to support represen-           variables.
tation learning, and integrated it into skill learning. It has                    pi j
                                                                             ln          = θi + ∑ βk Qk j + ∑ Qk j (γk Nik ).
been shown that by integrating representation learning and                      1 − pi j        k           k
skill learning, we can automatically learn the tree-structured
representation of the problem, and reduce or remove the need         Where:
of domain-specific operator functions.
                                                                     i represents a student i.
   The representation learner extends a grammar induction
technique to acquire a probabilistic context-free grammar            j represents a step j.
(pCFG) for the problems based on a set of observations (e.g.,        k represents a skill or KC k.
−3x, 2x + 5). To integrate representation learning with skill
learning, we extend the perceptual hierarchy to further in-          pi j is the probability that student i would be correct on step
clude the most probable parse trees from the learned pCFG                j.
in the contents of the leaf nodes. For example, the left side        θi is the coefficient for proficiency of student i.
of Figure 2 is a subtree for parsing “−3x” and is connected          βk is coefficient for difficulty of the skill or KC k
to the node associated with −3x in the perceptual hierarchy.
This subtree ensures that the coefficient −3 is explicitly rep-      Qk j is the Q-matrix cell for step j using skill k.
resented in the perceptual hierarchy. Then, the perceptual in-       γk is the coefficient for the learning rate of skill k;
formation learner and the operator function sequence learner         Nik is the number of practice opportunities student i has had
determine how to extract the coefficient from the perceptual             on the skill k;
hierarchy. This path for identifying the coefficient is added to
the perceptual information part of the production rules (See         Domains
Figure 1, right side). Then, the operator function sequence
                                                                     We carried out our study in three domains: algebra, stoi-
part no longer needs the domain-specific operator function
                                                                     chiometry, and fraction addition. The domains as well as the
“get-coefficient”. For more details, please refer to Li, Cohen,
                                                                     setup in each domain vary from one to another. This ensures
and Koedinger (2012).
                                                                     that the experiment tests the generality of the proposed ap-
                                                                     proach.
           Cognitive Model Discovery Study                               In algebra, we analyzed data from 71 students who used an
In Li et al. (2011), we demonstrated the effectiveness of us-        Carnegie Learning Algebra I Tutor unit on equation solving.
ing SimStudent to discover cognitive models in an algebra            The students were typical students at a vocational-technical
domain. In order to evaluate the generality of the proposed          school in a rural/suburban area outside of Pittsburgh, PA. A
                                                                 896

                          Table 1: AIC on SimStudent-Generated models and Human-Generated Models.
                                                       Human-Generated              SimStudent-Discovered
                                                       Model                        Model
                         Algebra                       6534.07                      6448.1
                         Stoichiometry                 17380.9                      17218.5
                         Fraction Addition             2112.82                      2202.02
total of 19,683 transactions between the students and the Al-        −v = N. This is caused by the different parse trees for Nv and
gebra Tutor were recorded, where each transaction represents         -v as shown in Figure 2. Due to this split, the SimStudent-
an attempt or inquiry made by the student, and the feedback          generated model predicts a higher error rate on problems
given by the tutor. We selected 40 problems that were used to        of the form −v = N than problems of the form Nv = N.
teach students as the training set for SimStudent.                   It matches with human student error rates better than the
   The stoichiometry dataset contains data from 3 studies.           human-generated model, which does not differentiate prob-
510 high school and college students participated in the stud-       lems of these two forms.
ies, and generated 172,060 transactions. Instructional videos           In stoichiometry, instead of finding splits of existing KCs,
on stoichiometry were intermingled with the problems. In-            SimStudent discovers new KCs that overlap with the origi-
structional materials were provided via the Internet. It took        nal KCs. There are three basic sets of skills in this domain.
students from 1.5 hours to 6.5 hours to complete the study. 8        Within each set, the human-generated KCs are assigned based
problems in this study were used in training SimStudent.             on the location of the input, while the SimStudent-discovered
   In fraction addition, we analyzed data from 24 students           KCs are associated with the goals of the input. Hence, sup-
who used an intelligent tutoring system as part of a larger          pose in two different problems, there are two inputs at the
study. Approximately half of the students were recruited             same location in the interface. If they are associated with
from local schools. Students were given immediate correct-           different goals, the human-generated model will not differen-
ness feedback on each step, and were offered on-demand text          tiate them, while the SimStudent-discovered model will put
hints. Each interaction was logged through Datashop, and             them into two KCs. This indicates that SimStudent not only
the 24 students yielded 4558 transactions. SimStudent was            splits existing KCs, but also discovers totally different KCs.
tutored with 20 problems from this study.                               The fraction addition problem set consists of three types
                                                                     of problems in increasing difficulty: 1) addends have equal
Measurements                                                         denominators; 2) the denominator of one addend is a mul-
We used Akaike Information Criterion (AIC) and a 10-fold             tiple of the other; 3) addends have unrelated denominators.
cross validation (CV) to test how well the generated model           The human-generated model differentiates these three types
predicts the correctness of human student behavior. AIC mea-         of problems in calculating the common denominators and the
sures the fit to student data and penalizes over-fitting. We did     scaled numerators, and ends up having six KCs. SimStudent,
not use BIC (Bayesian Information Criterion) as the fit met-         however, associates all of the numerator scaling steps with
ric, because based on past analysis across multiple DataShop         one KC and associates the common denominator calculations
datasets, it has been shown that AIC is a better predictor of        with two KCs. In other words, in this domain, SimStudent
cross validation than BIC is. The cross validation was per-          partially recovered three out of six KCs, but did not further
formed over ten folds with the constraint that each of the           split them into six KCs. SimStudent did discover the other
training sets must have data points for each student and KC.         three KCs, but eventually removed them when they were su-
We reported the root mean-squared error (RMSE) averaged              perseded by more generalized rules. This bias towards more
over ten test sets.                                                  general production rules over specific ones regardless of com-
                                                                     putational cost appears to be a limitation of SimStudent as
Results                                                              a cognitive model. Perhaps if we had let SimStudent keep
As shown in Table 1 and Table 2, in algebra and stoichiom-           a utility function for each production rule and retrieve them
etry, the SimStudent-discovered models that have lower               based on the computational cost, last retrieval time, and cor-
AICs and RMSEs (p < 0.001) than the human-generated                  rectness, SimStudent may have arrived at all six KCs in the
models. This means the SimStudent models better match                human-generated model.
the data (without over-fitting). However, in fraction addi-
tion, the human-generated model performs better than the             FBI Analysis and LFA on Fraction Addition
SimStudent-discovered ones.                                          The differences of AIC and RMSE between the models are
   A closer look at the models reveals that in algebra, the          small. This is partially because the difference between the
SimStudent-discovered model splits some of the KCs in the            models is small. FBI, a recently developed technique, is de-
human-generated model into finer grain sizes. For exam-              signed to analyze which of these differences improves the
ple, SimStudent creates two KCs for division, one for prob-          model, and by how much. We applied FBI to the SimStudent
lems of the form Nv = N, and one for problems of the form            and human-generated models in each domain to determine
                                                                 897

                     Table 2: CV RMSE on SimStudent-Generated models and Human-Generated Models.
                                                     Human-Generated                 SimStudent-Discovered
                                                     Model                           Model
                        Algebra                      0.4024                          0.3999
                        Stoichiometry                0.3501                          0.3488
                        Fraction Addition            0.3232                          0.3343
why the SimStudent models are better in two of the three             they focused on assessing self-explanation instead of student
cases. In the analysis, we set the human-generated models            learning. Additionally, there has been considerable work on
as the base.                                                         comparing the quality of alternative cognitive models. LFA
   FBI shows that in algebra, splitting “divide” reduces the         automatically discovers cognitive models, but is limited to the
RMSE of those steps by 1.02%. Further, splitting subtraction         space of the human-provided factors. Other works such as
and addition decreases the RMSE of those steps by 3.78%              Pavlik, Cen, and Koedinger (2009); Villano (1992) are less
and 3.10%, respectively. This also indicates that SimStudent         dependent on human labeling, but the models generated may
is able to discover KCs of finer grain sizes that match with         be hard to interpret. In contrast, the SimStudent approach has
human data well.                                                     the benefit that the acquired production rules have a precise
   The stoichiometry results are different. SimStudent dis-          and usually straightforward interpretation.
covered new KCs that were not part of any existing KCs.                 Other systems (e.g., Tatsuoka, 1983; Barnes, 2005) use a
Given the 40 KCs in the human-generated model, SimStu-               Q-matrix to find knowledge structure from student response
dent improved 26 of them. The biggest improvement is on              data. Baffes and Mooney (1996) apply theory refinement to
skill molecular weight (4.60%), since there are sometimes            the problem of modeling incorrect student behavior. In ad-
more than one skill applicable to the same step. The human-          dition, some research (e.g., Langley & Ohlsson, 1984) uses
generated model misses the additional skill, while the Sim-          artificial intelligent techniques to construct models that ex-
Student model successfully captures both skills.                     plain student’s behavior in math domains. Besides SimStu-
   As described previously, SimStudent did not differenti-           dent, there has also been considerable research on models
ate the numerator-scaling and common-denominator steps by            of high-level learning (e.g., Laird, Rosenbloom, & Newell,
problem type. This hurts the RMSE of the associated KCs in           1986; Anderson, 1993; Taatgen & Lee, 2003; Sun, 2007;
the SimStudent-generated model. Nevertheless, SimStudent             Tenenbaum & Griffiths, 2001; Schmid & Kitzelmann, 2011).
considers finding the common denominator to be a different           Other research on creating simulated students (e.g., Chan &
KC than copying it to the second converted addend. This              Chou, 1997) is also closely related to our work. Nevertheless,
split decreases by 7.43% for problems with unrelated denom-          none of the above approaches focused on modeling how rep-
inators, and and by 0.12% for denominator steps of problems          resentation learning affects skill learning. Moreover, none of
where one addend denominator was a multiple of the other.            them compared the system with human learning curve data.
   Given the above results, we carried out a third study on          To the best of our knowledge, our work is the first combina-
fraction addition to test that whether the new KCs created           tion of the two whereby we use cognitive model evaluation
by SimStudent can be used to discover better cognitive mod-          techniques to assess the quality of a simulated learner, and
els. We used LFA to discover cognitive models given two              demonstrate it across multiple domains.
sets of factors. The baseline LFA model was generated based
on the factors (KCs) in the human-generated model. The                                         Conclusion
other LFA model was discovered using both the factors (KCs)          In this paper, we evaluated the generality of an automatic cog-
in the human-generated model and those in the SimStudent-            nitive model discovery method, and carried out an in-depth
generated model. Both LFA models were better than the orig-          analysis to better understand the proposed approach. To avoid
inal human-generated model in terms of AIC and RMSE.                 over-generalization of KCs, we would like to further extend
Moreover, the LFA model using both human-generated and               the skill learning component to maintain utilities associated
SimStudent-generated factors had better AIC (2061.4) and             with each production rule. Further, we plan to investigate dis-
RMSE (0.3189) than the baseline LFA model (AIC: 2111.96,             covery of cognitive models for individual students, to provide
RMSE 0.3226). In other words, with the help of SimStudent,           more personalized learning. Finally, we plan to further inte-
LFA discovered better models of human students.                      grate the perceptual learning component into skill learning,
                                                                     so that the representation acquired by the learner is refined
                       Related Work                                  during the process learning.
The objective of this paper is to evaluate the generality of the        In the study, we show that the integration of the repre-
proposed cognitive model discovery approach. Conati and              sentation learning component into skill learning is key to
VanLehn (1999) also applied machine learning techniques              the success of SimStudent in discovering cognitive mod-
to generate cognitive models that fit with human data, but           els. Results indicate that in two out of three domains,
                                                                 898

SimStudent-generated models are better predictors of hu-                   ing in soar: The anatomy of a general learning mecha-
man students’ learning performance than human-coded mod-                   nism. Machine Learning, 1, 11–46.
els. For the third domain, when given the SimStudent- and            Langley, P., & Ohlsson, S. (1984). Automated cognitive
human-generated KCs, LFA finds a better model than the                     modeling. In Proceedings of the fourth national con-
human-generated one. A closer analysis shows that SimStu-                  ference on artificial intelligence (p. 193-197). Austin,
dent is able to split existing KCs into finer grain sizes, dis-            TX: Morgan Kaufmann.
cover new KCs, and uncover expert blind spots.                       Lau, T., & Weld, D. S. (1998). Programming by demon-
                                                                           stration: An inductive learning formulation. In Pro-
                    Acknowledgments                                        ceedings of the 1999 international conference on intel-
The authors would like to thank Hui Cheng for running the                  ligence user interfaces (pp. 145–152).
experiments. This work was supported by the Pittsburgh Sci-          Li, N., Cohen, W. W., & Koedinger, K. R. (2012). Efficient
ence of Learning Center, NSF Grant #SBE-0836012.                           cross-domain learning of complex skills. In Proceed-
                                                                           ings of the eleventh international conference on intelli-
                         References                                        gent tutoring systems (pp. 493–498). Berlin: Springer-
                                                                           Verlag.
Anderson, J. R. (1993). Rules of the Mind. Hillsdale, New            Li, N., Matsuda, N., Cohen, W. W., & Koedinger, K. R.
      Jersey: Lawrence Erlbaum Associates.                                 (2011). A machine learning approach for automatic
Baffes, P., & Mooney, R. (1996). Refinement-based stu-                     student model discovery. In Proceedings of the 4th
      dent modeling and automated bug library construction.                international conference on educational data mining,
      Journal of Artificial Intelligence in Education, 7(1),               eindhoven (p. 31-40).
      75–116.                                                        Matsuda, N., Lee, A., Cohen, W. W., & Koedinger, K. R.
Barnes, T. (2005). The Q-matrix method: Mining student re-                 (2009). A computational model of how learner errors
      sponse data for knowledge. In Proceedings aaai work-                 arise from weak prior knowledge. In Proceedings of
      shop educational data mining (pp. 1–8). Pittsburgh,                  conference of the cognitive science society.
      PA.                                                            Mitchell, T. (1982). Generalization as search. Artificial In-
Cen, H., Koedinger, K., & Junker, B. (2006). Learning fac-                 telligence, 18(2), 203–226.
      tors analysis - a general method for cognitive model           Muggleton, S., & Raedt, L. de. (1994). Inductive logic pro-
      evaluation and improvement. In Proceedings of the 8th                gramming: Theory and methods. Journal of Logic Pro-
      international conference on intelligent tutoring systems             gramming, 19, 629–679.
      (pp. 164–175).                                                 Pavlik, P. I., Cen, H., & Koedinger, K. R. (2009). Learn-
Chan, T.-W., & Chou, C.-Y. (1997). Exploring the design                    ing Factors Transfer Analysis: Using Learning Curve
      of computer supports for reciprocal tutoring. Interna-               Analysis to Automatically Generate Domain Models.
      tional Journal of Artificial Intelligence in Education, 8,           In Proceedings of 2nd international conference on ed-
      1–29.                                                                ucational data mining (pp. 121–130).
Chase, W. G., & Simon, H. A. (1973, January). Perception             Quinlan, J. R. (1990). Learning logical definitions from rela-
      in chess. Cognitive Psychology, 4(1), 55–81.                         tions. Mach. Learn., 5(3), 239–266.
Conati, C., & VanLehn, K. (1999). A student model to as-             Schmid, U., & Kitzelmann, E. (2011, September). Inductive
      sess self-explanation while learning from examples. In               rule learning on the knowledge level. Cognitive System
      Proceedings of the seventh international conference on               Research, 12(3-4), 237–248.
      user modeling (pp. 303–305). Secaucus, NJ: Springer-           Sun, R. (2007, September). Cognitive social simulation in-
      Verlag New York, Inc.                                                corporating cognitive architectures. IEEE Intelligent
Koedinger, K. R., & MacLaren, B. A. (1997). Implicit                       Systems, 22(5), 33–39.
      strategies and errors in an improved model of early            Taatgen, N. A., & Lee, F. J. (2003). Production compilation:
      algebra problem solving. In Proceedings of the nine-                 A simple mechanism to model complex skill acquisi-
      teenth annual conference of the cognitive science soci-              tion. Human Factors, 45(1), 61–75.
      ety (p. 382-387). Hillsdale, NJ: Erlbaum.                      Tatsuoka, K. K. (1983). Rule space: An approach for deal-
Koedinger, K. R., & McLaughlin, E. A. (2010). Seeing                       ing with misconceptions based on item response the-
      language learning inside the math: Cognitive analysis                ory. Journal of Educational Measurement, 345-354.
      yields transfer. In Proceedings of the 32nd annual con-        Tenenbaum, J. B., & Griffiths, T. L. (2001). Generaliza-
      ference of the cognitive science society (pp. 471–476).              tion, similarity, and bayesian inference. Behavioral and
      Austin, TX.                                                          Brain Sciences, 24, 629–640.
Koedinger, K. R., McLaughlin, E. A., & Stamper, J. C.                Villano, M. (1992). Probabilistic student models: Bayesian
      (2012). Automated student model improvement. In                      belief networks and knowledge space theory. In Pro-
      Proceedings of the 5th international conference on ed-               ceedings of the 2nd international conference on intelli-
      ucational data mining (p. 17-24). Chania, Greece.                    gent tutoring systems (p. 491-498). Heidelberg.
Laird, J. E., Rosenbloom, P. S., & Newell, A. (1986). Chunk-
                                                                 899

