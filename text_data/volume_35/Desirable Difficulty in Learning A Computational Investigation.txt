UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Desirable Difficulty in Learning: A Computational Investigation
Permalink
https://escholarship.org/uc/item/3k28w8xp
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Nematzadeh, Aida
Fazly, Afsaneh
Stevenson, Suzanne
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

                  Desirable Difficulty in Learning: A Computational Investigation
                                   Aida Nematzadeh, Afsaneh Fazly, and Suzanne Stevenson
                                                      Department of Computer Science
                                                             University of Toronto
                                                  {aida,afsaneh,suzanne}@cs.toronto.edu
                               Abstract                                   learning mechanisms, and the variations in the input condi-
                                                                          tions, that might explain these findings. We first introduce our
   Certain difficulties of a word learning situation can promote          computational model of cross-situational word learning, and
   long-term learning, and thus are referred to as “desirable diffi-
   culties”. We use a computational modelling approach to exam-           then explain and analyze the experimental data and results
   ine the possible explanatory factors of the observed patterns in       of Vlach and Sandhofer (2010) in the context of our model.
   a cross-situational word learning experiment. Our results sug-         Finally, we describe the way we simulate these experiments
   gest that the within-trial ambiguity and the presentation du-
   ration of each trial in addition to other distributional charac-       using our model, and how this enables us to examine the role
   teristics of the input (experimental stimuli) may explain these        of several different factors in the observed pattern of word
   results. Our findings also emphasize the role of computational         learning.
   modelling in understanding empirical results.
                                                                                         The Computational Model
                           Introduction
                                                                          In this section, we present our computational model of word
One of the important questions in language acquisition is                 learning that was first published in Nematzadeh, Fazly, and
how people learn the mappings between words and their                     Stevenson (2012a). Our model builds on the word learning
meanings (Quine, 1960). A number of mechanisms and ap-                    model of Fazly, Alishahi, and Stevenson (2010), which takes
proaches have been proposed in an attempt to address this                 an incremental approach in learning probabilistic associations
question (e.g., Tomasello, 1992; Pinker, 1989). A widely-                 between words and their meanings. In Nematzadeh et al., we
discussed mechanism is cross-situational learning, in which               integrated new functionality into this model to capture for-
people learn word meanings by gathering evidence from var-                getting (i.e., an effect of memory) and attention to novelty.
ious exposures of words in different situations. Recent word              Our proposed model accounts for several observed patterns
learning experiments also confirm that both adults and chil-              of the spacing effect in children and adults, in which exper-
dren keep track of cross–situational statistics across individ-           imental subjects learn presented items better when they are
ually ambiguous learning trials, and infer the correct word–              spaced apart in time, than when they are shown in immediate
meaning mappings even in highly ambiguous conditions (Yu                  succession. We provide a brief overview of the model before
& Smith, 2007; Smith & Yu, 2008). These experiments have                  turning to modelling of other kinds of “desirable difficulties.”
gained popularity in recent years (e.g., Yurovsky & Yu, 2008;
Vlach, Sandhofer, & Kornell, 2008), and provide opportuni-                Learning from an Input Pair
ties for further investigating the observed patterns in natural-          Our model learns about the meaning of words by incremen-
istic word learning.                                                      tally processing a corpus that contains a sequence of utter-
   One interesting aspect of word learning that can be stud-              ances paired with a semantic representation of a scene, which
ied in such experiments, is its interaction with other cognitive          is the hypothetical perception of a learner upon hearing the
processes such as memory and attention. An example is the                 utterance. Each input to the model pairs a set of words (the
experiments of Vlach et al. (2008) on children, which exam-               representation of the utterance) with a set of semantic features
ine the spacing effect, i.e., the observation that people gen-            (the representation of the scene), as in:
erally learn better when the presentations of the items to be                  Utterance: { she, drinks, milk }
learned are distributed (spaced) over a period of time. This                   Scene: { ANIMATE, PERSON, FEMALE, CONSUME,
and other similar patterns in human learning are referred to                     DRINK , SUBSTANCE, FOOD , DAIRY- PRODUCT }
as “desirable difficulties”: Although a more difficult learning           We create corpora drawn from child-directed speech, in
situation may hinder short-term recall of learned material, it            which lemmatized, transcribed utterances are paired with ar-
may promote long-term retention.                                          tificially generated semantics, based on WordNet or other se-
   In this work, we use a computational model to shed light on            mantic featural representations of the entities and actions cor-
one such case of an observed “desirable difficulty” in cross-             responding to the words. In the experiments here on novel
situational word learning, studied by Vlach and Sandhofer                 word learning, nonce words are paired with these naturalis-
(2010). Notably, Vlach and Sandhofer (2010) attribute their               tic semantic representations, in which features corresponding
findings to desirable difficulties in learning, but do not pro-           to meaning properties are probabilistically associated with a
vide an explanation of why and how the sort of difficulty they            word.
focus on facilitates long-term retention of the learned words.                When processing an input pair, the model bootstraps its
Computational modelling enables us to investigate the precise             current knowledge of word meanings to hypothesize the
                                                                     1073

strength of association between the words in the current input         a decay over time. At each time t, the strength of association
and the meaning features in the current scene. These prob-             of a word and a feature is formulated as:
abilistic alignments between the words and features of the                                                           at 0 (w, f )
current input are then used to update the model’s knowledge                            assoct ( f , w) = ∑                    da 0
                                                                                                                                    (3)
of word meanings.                                                                                              t0  (t − t 0 )     t
   More formally, for each word, the model learns a mean-
                                                                       where t 0 is the time at which the alignment at 0 is calculated,
ing probability, which is a probability distribution over all
                                                                       and dat 0 is the decay rate associated with this alignment. We
possible semantic features. The model starts with uniform
                                                                       note that our formulation of assoc is inspired by the ACT-R
meaning probabilities for all words; i.e., before processing
                                                                       model of memory (Anderson & Lebiere, 1998), in which the
any input, all features are equally likely for every word. At
                                                                       sum of individual memory strengthenings for an item deter-
each time step t, the model processes an input pair and cal-
                                                                       mines the item’s activation. We assume that stronger align-
culates an alignment score, at (w, f ), between each word w
                                                                       ments should be more entrenched in memory and thus decay
and semantic feature f in the input pair. This alignment score
                                                                       more slowly than weaker alignments. Thus, each alignment
reflects how strongly the w– f pair are associated at time t,
                                                                       undergoes a decay which is dependent on the strength of the
by considering two sources of information: (1) the meaning
                                                                       alignment:
probabilities of all the words in the utterance Ut (represent-
ing the knowledge of the model of word meanings up to that                                                       d
point), and (2) the novelty of words, capturing the attention a                                dat 0 =                              (4)
                                                                                                          at 0 (w, f )
learner might pay to the novel words compared to the familiar
words (explained below). The alignment score is formulated             where d is a constant parameter. Note that the alignments
as:                                                                    between a word and different features may be forgotten at
                                                                       different rates.
                           pt ( f |w)
          at (w, f ) =                 ∗ noveltyt (w)          (1)        This association score is then normalized using a smoothed
                        ∑ pt ( f |w0 )                                 version of the following to yield the meaning probability of
                       w0 ∈Ut
                                                                       that feature f for that word w at time t:
where pt ( f |w) is the probability of f being part of the mean-
                                                                                                          assoct ( f , w)
ing of word w at time t, right before processing the input pair,                      pt ( f |w) =                                  (5)
and noveltyt (w) is a multiplicative attentional factor.                                               ∑ assoct ( f 0 , w)
                                                                                                     f 0 ∈M
   This factor, noveltyt (w), taps into empirical studies on at-
tention showing that people attend to novel items in a learning        where M is the set of all observed meaning features.
scenario more than other items, leading to improved learn-
ing of those items (e.g., Snyder, Blank, & Marsolek, 2008;                   Desirable Difficulties in Word Learning
MacPherson & Moore, 2010; Horst, Samuelson, Kucker, &
                                                                       Vlach and Sandhofer (2010) — henceforth V&S — explore
McMurray, 2011). In the word learning scenario, this corre-
                                                                       the factors involved in “desirable difficulty” through a set of
sponds to a person focusing on determining the meaning of
                                                                       (now standard) cross-situational word learning experiments
novel words. We model this observation by incorporating the
                                                                       on adults, varying the presentation and testing conditions.
multiplicative noveltyt (w) in the above formula, providing an
                                                                       In each N × N trial, subjects see some number N of novel
increase in word–feature association for a more novel word.
                                                                       objects on a computer screen, while hearing N novel words
The noveltyt (w) measures the degree of novelty of a word
                                                                       (in arbitrary order) that label the displayed objects; see Fig-
as a simple inverse function of recency: The more recently a
                                                                       ure 1. In testing, subjects hear a single word, and are asked
word w has been observed by the model (tlast w ), the less novel
                                                                       to select the corresponding object from a display of 4 ob-
it appears to the model at the current time t:
                                                                       jects. Across three presentation conditions, the total num-
             noveltyt (w) = 1 − recency(t,tlast w )            (2)     ber of word–object pairs, and the number of times each is
                                                                       seen, are held constant, while there is increasing within-trial
where recency(t,tlast w ) is inversely proportional to the differ-     ambiguity — i.e., the number of possible pairings between
ence between t and tlast w . We set novelty(w) to be 1 for the         the words and the objects within a single presentation: 2 × 2,
first exposure of the word.                                            3×3, and 4×4. Furthermore, participants were tested at each
                                                                       of three times: immediately after training, 30 minutes after,
Accumulating Evidence over Time                                        and one week after.
The model keeps track of the accumulation of all the align-               V&S find that in the immediate testing condition, as ex-
ment scores of all word–feature pairs, and uses these scores to        pected, the number of correctly learned pairs decreases as the
update the meaning probabilities of the words. These align-            within-trial ambiguity increases. That is, the participants per-
ment scores reflect the model’s knowledge of the associations          formed the best in the 2 × 2 condition and the worst in 4 × 4
between words and various potential meanings. To simulate              (Figure 2). However, when tested after 30 minutes of delay,
the effect of forgetting in memory, these alignments undergo           there was no significant difference between the performance
                                                                   1074

                                                                       the number of trials between the two presentations of a word–
                                                                       object pair).
                                                                          Computational modelling can be used as a tool to study
                                                                       the necessity and the interaction of these three factors (the
                                                                       within-trial ambiguity, the presentation time of each trial, and
                                                                       the average spacing interval) in a cross-situational learning
                                                                       scenario. In our model, the increase in within-trial ambi-
                                                                       guity results in more competition among the possible align-
                                                                       ments since there are more words and meanings to potentially
                                                                       align; this results in lower association scores and therefore
                                                                       decreased performance in word learning. We argue that the
                                                                       second factor, the presentation duration, is related to forget-
                                                                       ting. In the following section (Methodology), we will explain
 Figure 1: Example stimuli from 2 × 2 condition taken from V&S.        how we incorporate differences in the presentation duration
                                                                       into our model. The third factor (the spacing interval) relates
of the participants in the 2 × 2 and the 3 × 3 conditions, while       to the interaction of forgetting and attention to novelty in the
4 × 4 still had the worst performance. Interestingly, in testing       model: As the spacing interval becomes larger, the amount of
after one week, the participants performed better in the 3 × 3         forgetting increases, resulting in lower association scores be-
than the 2 × 2 condition. (Again, 4 × 4 still had the worst            tween words and features; however, the novelty of words and
performance.) In summary, what should be the “easiest” con-            consequently their association scores increases as the spacing
dition (2 × 2) has the best performance in immediate testing,          interval gets larger. Thus, varying the spacing interval affects
but a more difficult condition (3 × 3) has better performance          the performance of the model (see Nematzadeh et al., 2012a
one week later.                                                        for more details). We use our model to study the interaction
                                                                       of these three factors, with the goal of providing a more pre-
                                                                       cise explanation for the desirable difficulty observed in the
                                                                       experiments of V&S. Next, we explain our methodology, in-
                                                                       cluding our input generation, and the simulation of the V&S
                                                                       experiments.
                                                                                                 Methodology
                                                                       Input Generation
                                                                       To generate the input stimuli for our model, we need to pair
                                                                       words with a meaning representation that corresponds to the
                                                                       depiction of the corresponding object in the experimental sit-
            Figure 2: The results of V&S’s experiment.                 uation of Figure 1. To do so, we draw on the input-generation
   V&S relate their findings to “desirable difficulties” in            lexicon of Nematzadeh, Fazly, and Stevenson (2012b), which
learning: they argue that the difficulty of a learning situation       was previously used to automatically annotate corpora of
might hinder immediate performance, but promote longer                 child-directed utterances with meaning features correspond-
term performance. However, they do not discuss why the per-            ing to the words in those utterances. Here, we use the lexicon
formance of the 4 × 4 condition is the worst compared to the           to provide a source of naturalistic meaning representations
other conditions for all testing intervals. That is, why is the        (“novel object descriptions”) for a set of “novel” words (i.e.,
level of difficulty in 3 × 3 desired, but is not so for 4 × 4.         the words in the input stimuli are unknown to the model, as
Moreover, they do not explain why and how difficulty can               in the experiments we are modeling).
boost learning in the long term in this learning scenario.                The true meaning of each word in the lexicon, tm(w), is
   We observe that, in the V&S experiments, the 2 × 2 condi-           a vector of semantic features and their assigned scores or
tion has more learning trials, each of which is seen for less          weights (Figure 3).1 When a word is used in an input trial, its
time, than in the 3 × 3 condition (and similarly for 3 × 3             meaning features are probabilistically sampled from tm(w)
compared to 4 × 4). This occurs because the total number               according to the weight of each feature in the lexical entry
of word–object pairs, the number of times each is seen, and            of the word. This probabilistic sampling captures our intu-
the total presentation time of the full set of items, are all held     ition that a participant, when faced with a trial in the cross-
constant across the three presentation conditions. We can thus         situational experiment of Figure 1, will grasp some features
identify three factors that differ across the V&S conditions,          of the novel objects but not necessarily all. Each trial of the
each of which may contribute to the observed pattern: (1) the          input is then composed of a set of N words (2, 3, or 4 words,
within-trial ambiguity, (2) the presentation duration of each              1 We note that this lexicon is only used in input generation and
trial, and (3) the average spacing interval (where spacing is          evaluation, and not in the learning of the model.
                                                                   1075

depending on the condition), paired with a set of features            of time after training: immediately after processing the last
which is the union of the N sets of meaning features sampled          input (time = t), at t + 30, and at t + 350. These times were
for each of the words in that trial.                                  chosen to loosely reflect the three time intervals in V&S’s
                                                                      experiments. We will use the labels “no delay”, “brief delay”,
         apple: { FOOD:1, SOLID:.72, PRODUCE:.63,                     and “lengthy delay”, to refer to these timings in describing
                 EDIBLE - FRUIT :.32, PLANT- PART:.22,
                 PHYSICAL - ENTITY :.17, WHOLE :.06, · · · }          our results.
                                                                         To evaluate the performance of the model at each testing
     Figure 3: True meaning features & probabilities for apple.
                                                                      point, we measure how well each word is acquired by com-
                                                                      paring its learned meaning lm(w) – a vector holding the val-
   To produce a full set of experimental trials, we first convert     ues of the meaning probability (Eqn. (5)) – to its true meaning
the exact stimuli of V&S to the format of our input. That is,         tm(w) from the input-generation lexicon (see Figure 3):
in their stimuli, we replace each word with a specific word
from our lexicon, and each object with the probabilistically-                         acq(w) = sim(lm(w),tm(w))                     (6)
generated meaning representation for its corresponding word
(as explained above). The precise combination of corre-               where sim is the cosine similarity between the two meaning
sponding word/object pairs in each trial, and the order of the        vectors, tm(w) and lm(w). The higher acq(w) is, the more
trials, are exactly the same as in the V&S stimuli. We refer to       similar lm(w) and tm(w) are. We use the average acq score
this data as the input of V&S.                                        at time t of all the words in the input to reflect the overall
   The V&S input includes 18 novel word–object pairs, each            learning of the model at that time.
of which occurs 6 times, resulting in 54, 36, and 27 trials
in the 2 × 2, 3 × 3, and 4 × 4 conditions, respectively. We                                      Results
note that the V&S input, as a specific set of stimuli, might          We first examine the behavior of our model when trained on
have particular spacing properties that contribute to their re-       the V&S input, and then compare these with results on our
sults. Thus we also randomly generate input stimuli in order          randomly generated stimuli.
to evaluate the effect of arbitrary variation in the precise pre-
sentation order of the word/object pairs. We randomly gen-            The Input of V&S
erate 20 sets of input stimuli for each condition, keeping the        The results of training and evaluating our model on the V&S
number of pairs, their frequency, and the number of trials the        input are presented in Figure 4. We see the same interesting
same as in the V&S input. We use the same novel words                 pattern as found in V&S (shown in Figure 2) for the 2 × 2 and
that we used in generating V&S data, and randomly generate            the 3 × 3 conditions. That is, 2 × 2 is better with no delay, but
their meaning representations as explained. The result is that        similar with brief delay and worse with lengthy delay, even
we can experiment both with the precise data of V&S, as well          though 3 × 3 is “harder” due to its higher degree of within-
as 20 randomly generated sets of input stimuli with the same          trial ambiguity. Unlike the V&S results, 3 × 3 and 4 × 4 are
basic properties.                                                     similar for all delays.
Modeling of the Presentation Duration
One aspect of the V&S experimental conditions that we can-
not directly replicate in our model is the presentation dura-
tion of each trial in a stimulus set. Recall that because of the
various properties of the stimuli, the individual trials in each
of the three conditions (2 × 2, 3 × 3, and 4 × 4) have differ-
ent presentation durations. Our model does not have a no-
tion of “presentation duration” — it simply processes each
input as it receives it. Thus to simulate these differences,
different degrees of forgetting decays are used in the model
(see Eqn. (4)). The intuition is that subjects forget faster in
a condition with a shorter presentation duration, since they
have less time to absorb the stimuli in each trial. The forget-
ting decay is thus set to a larger value in the 2 × 2 condition
(where the presentation time is the smallest), and successively
                                                                      Figure 4: Average acq score of words (from the model) given
smaller in each of the 3 × 3 and 4 × 4 conditions.
                                                                      the three conditions and three time intervals similar to the
Simulation of the V&S Experiments                                     V&S experiments.
We train our model by presenting the set of inputs for a given           We consider these findings in the context of the discussed
condition, where it learns incrementally in response to each          factors of presentation duration, within-trial ambiguity, and
trial. Similarly to V&S, we evaluate our model at three points        average spacing of items, which we proposed might explain
                                                                  1076

the desirable difficulty in learning. The differences in presen-        We train our model on the randomly-generated inputs (with
tation duration (shortest for 2×2 and longest for 4×4) entails       different average spacing intervals) for all four N × N condi-
that, generally, the learning in the 2 × 2 condition should de-      tions. To evaluate the performance of the model, the average
cline most steeply over time, and learning in the 4 × 4 should       acq score of words for all 20 sets of inputs within a single
decline least steeply: i.e., for each set of same-coloured bars      N × N condition are averaged (see Figure 5). We can see that
in Figure 4, we expect learning to decrease over time, and           when tested with “no delay”, the 2 × 2, 3 × 3, and 4 × 4 con-
more rapidly for lower values of N in the N × N conditions.          ditions have similar scores. Moreover, we can see a pattern
We see this predicted behaviour with our model, which results        similar to V&S’s experiments: the 3 × 3 and 4 × 4 conditions
from our modeling of presentation duration with an inversely         have the best results after the “lengthy delay”. We also ob-
proportional decay rate (i.e., the shorter the presentation du-      serve that by increasing difficulty in the 6 × 6 condition (due
ration, the greater the degree of forgetting).                       to the high within-trial ambiguity), the model produces a pat-
   It is expected that in the absence of other factors, increas-     tern similar to the pattern observed in the 4 × 4 condition in
ing within-trial ambiguity from the 2 × 2 to the 4 × 4 condi-        V&S’s experiments. This confirms our hypothesis that for
tions results in a decline in average acq score, since greater       our model, the 4 × 4 condition is not “hard” enough to result
ambiguity should lead to decreased learning. However, in             in a steep decline over time intervals as in the V&S’s results.
our model, the presentation duration also plays a role. Sim-
ilar to results of V&S, we see the decline pattern in the “no
delay” condition, and in the “brief delay” condition (albeit
with less difference), due to the increased competition for
word–meaning alignments that occurs with a higher number
of items in a trial (see Figure 4). However, we do not see this
pattern in the lengthy delay condition.
   To summarize, our results are similar to those of V&S,
who found that while the 2 × 2 condition led to best learn-
ing when tested immediately, it led to poorer performance
than the 3 × 3 condition given a lengthy delay before testing
— a pattern V&S attribute to the “desirable difficulty”. It
seems that these factors of presentation duration and within-
trial ambiguity may interact, such that the steep decline in
performance in subsequent testing in the 2×2 condition more          Figure 5: Average acq score of words (from the model) given
than offsets the advantage it has from the lesser within-trial       the four conditions and the three time intervals, averaged over
ambiguity.                                                           20 sets of stimuli.
   In the experiments of V&S, the performance in the 4 × 4
condition is always worse than the two other conditions.                However, we also see that, in contrast to V&S’s results (and
However, our model produces very similar results for the             our model’s performance on the V&S data), the 2 × 2 condi-
3 × 3 and the 4 × 4 conditions. Also, the role of the spacing        tion with no delay fails to show better learning than the other
interval is not clear in these results. The problem is that by       conditions.
just considering one set of stimuli within each N × N condi-            To better understand this difference between the two sets
tion (each of which has a set spacing of items), we do not have      of results, we look more closely at the scores of the individ-
a variation of the average spacing interval that is independent      ual randomly-generated stimuli sets. We find that there is a
of the presentation duration and the within-trial ambiguity.         notable difference in the average acq score across the 20 in-
We turn to these issues in the next subsection.                      put files for the 2 × 2 condition, which shows its maximum
                                                                     value of 0.76 for the V&S’s data, while the minimum is 0.50.
Randomly Generated Input                                             This suggests that the characteristics of the particular input
We observed that the performance of the model in the 3 × 3           (as a result of varying the average spacing interval) may be
and 4 × 4 conditions on the V&S input is very similar. We            responsible for some of the observed patterns in the V&S’s
also investigate a condition here with higher within-trial am-       results.
biguity to see if such a condition might be “hard” enough               We were interested to understand why the V&S data has
for the model (because of the higher within-trial ambiguity)         the maximum score, especially since there was a sizable gap
so that it results in a similar patten to the 4 × 4 condition in     between the score of this input and the next best score among
V&S. As with the others, we generate 20 sets of input stim-          the randomly-generated inputs (of 0.64). In an attempt to
uli for this 6 × 6 condition, using 18 word-object pairs, each       identify the factor behind this variation, we measured vari-
of which occurs 6 times, producing 18 trials. Thus the gener-        ous statistics for each input set, such as the following: (1) the
ated input stimuli for the four conditions allows us to examine      average spacing interval of words, which has been shown to
both the role of average spacing interval, and the impact of a       affect learning both in people (Vlach et al., 2008) and in our
more difficult condition with higher within-trial ambiguity.         model (Nematzadeh et al., 2012a); (2) the average time since
                                                                 1077

the last occurrence of words, that impacts the amount of for-         are also grateful for the financial support from NSERC of
getting that occurs; and (3) the average context familiarity of       Canada, and University of Toronto.
words (that is, the familiarity of the words that occur with
a word in an utterance), a factor that has been noted to af-                                    References
fect word learning (see, e.g., Fazly, Ahmadi-Fakhr, Alishahi,         Anderson, J. R., & Lebiere, C. (1998). The atomic compo-
& Stevenson, 2010). However, we found that none of these                nents of thought. Lawrence Erlbaum Associates.
measures explain the variation of the scores in all the inputs.       Fazly, A., Ahmadi-Fakhr, F., Alishahi, A., & Stevenson, S.
Future research is needed to fully understand the impact of             (2010). Cross-situational learning of low frequency words:
the properties these measures tap into, and whether they may            The role of context familiarity and age of exposure. Proc.
(individually or in combination) contribute to explaining the           of CogSci, 10.
pattern of the results.                                               Fazly, A., Alishahi, A., & Stevenson, S. (2010). A probabilis-
                                                                        tic computational model of cross-situational word learning.
                            Summary                                     Cognitive Science, 34(6), 1017–1063.
The “desirable difficulty” of a learning condition can pro-           Horst, J. S., Samuelson, L. K., Kucker, S. C., & McMurray,
mote the long term retention of the learned items. We have              B. (2011). Whats new? children prefer novelty in referent
used a computational model to investigate the possible factors          selection. Cognition, 118(2), 234 - 244.
behind one such case of a “desirable difficulty” in a cross-          MacPherson, A. C., & Moore, C. (2010). Understanding
situational word learning experiment (Vlach & Sandhofer,                interest in the second year of life. Infancy, 15(3), 324–335.
2010). Notably, the experimental results were not clearly             Miller, G. (1956). The magical number seven, plus or minus
pointing to the factors causing the patterns observed in the            two: some limits on our capacity for processing informa-
performance of the human participants. Using a computa-                 tion. Psychological review, 63(2), 81.
tional model, we have suggested that an interaction between           Nematzadeh, A., Fazly, A., & Stevenson, S. (2012a). A com-
two factors (the within-trial ambiguity of the learning trials,         putational model of memory, attention, and word learning.
and the presentation duration of each trial) might explain the          In Proceedings of the 3rd workshop on cognitive modeling
observed patterns. In addition, our results point to other dis-         and computational linguistics. Association for Computa-
tributional characteristics of the input (experimental stimuli)         tional Linguistics.
that might have an impact on the performance of the learner.          Nematzadeh, A., Fazly, A., & Stevenson, S. (2012b). Inter-
These findings illustrate the role of computational modelling,          action of word learning and semantic category formation in
not only in explaining observed human behaviour, but also               late talking. In Proceedings of the 34th Annual Conference
in fully understanding the factors involved in a phenomenon.            of the Cognitive Science Society.
There are several factors involved in a cross-situational word        Pinker, S. (1989). Learnability and cognition: The acquisi-
learning experiment, such as the contextual familiarity of              tion of argument structure. Cambridge, Mass.: MIT Press.
words, and the average spacing interval of words. Our find-           Quine, W. V. O. (1960). Word and object. MIT Press.
ings signify the importance of controlling for these factors in       Smith, L., & Yu, C. (2008). Infants rapidly learn word-
order to understand the reasons behind the observed patterns.           referent mappings via cross-situational statistics. Cogni-
But it is difficult do so in human experiments because the fac-         tion, 106(3), 1558–1568.
tors can interact in complex ways.                                    Snyder, K. A., Blank, M. P., & Marsolek, C. J. (2008). What
   Our work is an initial attempt at shedding light on the in-          form of memory underlies novelty preferences? Psycho-
teraction of memory, attention and word learning, and under-            logical Bulletin and Review, 15(2), 315 - 321.
standing “desirable difficulty” in learning. Other factors (e.g.,     Tomasello, M. (1992). The social bases of language acquisi-
working memory) might play a role in the performance of                 tion. Social development, 1(1), 67–87.
people as well. For example, because the number of items              Vlach, H. A., & Sandhofer, C. M. (2010). Desirable difficul-
that people can store in their working memory is limited                ties in cross-situational word learning. In Proceedings of
(Miller, 1956), the participants might store more trials in their       the 32nd annual conference of the cognitive science soci-
working memory in the 2 × 2 condition, compared with the                ety.
other conditions. The participants might use this information         Vlach, H. A., Sandhofer, C. M., & Kornell, N. (2008, Oc-
of the multiple trials (in their working memory) to make infer-         tober 0). The Spacing Effect in Children’s Memory and
ences about word–object mappings that repeat in successive              Category Induction. Cognition, 109(1), 163–167.
trials. One future direction would be to incorporate a working        Yu, C., & Smith, L. (2007). Rapid word learning under un-
memory module into our word learning model, and examine                 certainty via cross-situational statistics. Psychological Sci-
the impact of such inferences in a cross-situational learning           ence, 18(5), 414–420.
scenario.                                                             Yurovsky, D., & Yu, C. (2008). Mutual exclusivity in crosssi-
                                                                        tuational statistical learning. In Proceedings of the 30th an-
                     Acknowledgements                                   nual conference of the cognitive science society (pp. 715–
We would like to thank Haley Vlach for valuable discussion,             720).
and for providing us with the stimuli of her experiments. We
                                                                  1078

