UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Emotional Speech Processing and Language Knowledge
Permalink
https://escholarship.org/uc/item/9d02095k
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Frye, Conor
Creel, Sarah
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                         Emotional Speech Processing and Language Knowledge
                                             Conor I. Frye (cifrye@cogsci.ucsd.edu)
                       Department of Cognitive Science, UC San Diego, 9500 Gilman Dr. M/S 0515
                                                       La Jolla, CA 92093 USA
                                             Sarah C. Creel (creel@cogsci.ucsd.edu)
                       Department of Cognitive Science, UC San Diego, 9500 Gilman Dr. M/S 0515
                                                       La Jolla, CA 92093 USA
                              Abstract                                Language-specific recognition of vocal affect
   How does language knowledge affect processing of                      One likely set of cues that listeners use to identify vocal
   paralinguistic information—vocal properties that are not           emotion is prosody: pitch and timing information. Happy
   directly related to understanding words? This study                speech, for instance, typically has more variable pitch and
   investigates links between a listener’s native language, any       volume, higher overall pitch level, and a faster speaking
   other languages they may have experience in, and the ability       rate, whereas sad speech sounds exhibit lower average pitch,
   to identify vocal emotional information in those languages.
                                                                      attenuated loudness and pitch variation, and a slower pace
   The study focuses on two particular classes of languages:
   those with lexical tone, such as Mandarin Chinese, which use       of speech (Morton & Trehub, 2001). Previous work
   pitch properties to distinguish otherwise-identical words; and     demonstrates that humans use paralinguistic cues during
   those without lexical tone, such as English. English listeners     speech to alert co-communicators to their current emotional
   and bilingual Mandarin-English listeners listened to sentences     state (Kehrein, 2002). However, this ability to attribute
   and categorized the emotional content of English and               certain paralinguistic cues to particular emotional states may
   Mandarin sentences. Half of the sentences were presented           not be fully present at birth, but may require learning
   normally; the other half were low-pass filtered to remove all
   but prosodic cues (pitch and timing). English listeners were
                                                                      through lengthy exposure to one’s native language.
   better at identifying emotions in English sentences, while            One indication of the learned nature of paralinguistic
   bilinguals were equally good at identifying emotions in both       processing is that children experience difficulty in
   languages. This indicates better overall emotion recognition       identifying vocal emotional cues (Morton & Trehub, 2001);
   from prosody alone for listeners more familiar with a              for instance, 6-year-olds who hear “my mommy gave me a
   language. It may point to a connection between tone language       treat” with “sad” emotional prosody will report that the
   experience and augmented paralinguistic processing                 speaker sounded happy, suggesting that they are still
   capabilities.
                                                                      learning the mapping between particular speech patterns and
   Keywords: speech perception; paralinguistic perception;            emotional states. Further research suggests that these
   voice; language background; individual differences;                learned aspects may be language-specific (Thompson &
   bilingualism                                                       Balkwill, 2006), though those authors do not pinpoint
                                                                      particular cues that may be relevant, nor do they offer a
                          Introduction                                hypothesis as to what level of fluency one needs to access
   Spoken language as a medium is not just a symbol system            the learned aspects of emotional speech. In a related area,
of discrete speech sounds; it is also replete with cues to the        speaker recognition shows some language specificity in
talker’s identity, region of origin, and emotional state.             infants (Johnson et al., 2011) and adults (Bregman & Creel,
Although much research has been devoted to understanding              2012). If encoding of vocal emotional information works
how exposure to a language affects speech sound                       similarly to encoding of voices, then good emotional
identification (Kuhl, 1994), almost no one has asked how              recognition within a language may be dependent on lengthy
language knowledge affects processing of paralinguistic               language experience, and may not generalize to emotion
information—vocal properties that are not directly related to         recognition in other languages.
understanding words like speech rate and pitch changes (see
Thompson & Ballkwill, 2006, for an exception). Emotion in             General ability to recognize vocal emotion
the voice is thought to be conveyed by these paralinguistic              On the other hand, there is evidence that expertise in
cues. Though differing languages seem to use similar vocal            processing the cues that communicate vocal emotion may
acoustic cues for the “basic emotions” in non-speech                  generalize widely across domains. This implies that better
vocalizations such as laughter and crying, it is not clear how        attention to or encoding of pitch for another purpose or in
readily listeners perceive these emotional cues cross-                another domain may lead to better perception of vocal
linguistically when only presented with the auditory signal           emotion. For example, certain types of languages have been
(Sauter et al., 2010).                                                claimed to boost pitch perception abilities: Speakers of tone
                                                                      languages such as Mandarin are better at making relative
                                                                  2333

pitch distinctions in musical stimuli (Pfordresher & Brown,       speakers of English, and the other four (2 male, 2 female)
2009). Conversely, musicians are better than non-musicians        were native speakers of Mandarin Chinese. The speakers’
at brain encoding of linguistic pitch changes (Wong et al,        ages ranged from 19 to 26, with a mean age of 21.75.
2007). These studies suggest that facility with pitch                Sentences spanned six different emotional states: anger,
processing generalizes even across domains. This implies          disgust, fear, happiness, sadness, and surprise. Sentence
that good linguistic pitch processing should facilitate pitch     semantic content was created to elicit the intended emotion,
processing generally, which would thereby facilitate              to make the task of emotional speech production more
perception of pitch-related vocal-emotional information,          naturalistic for our speakers. The 16 sentences for each
even outside one’s native language. The novel prediction for      emotion were originally written in English and translated to
vocal affect detection is that, over and above language-          Mandarin Chinese. Each sentence contained five syllables in
specific knowledge, tone-language speakers may excel at           the English version. The translation was retranslated
perceiving vocal-emotional information due to their               separately by all four Mandarin speakers to ensure a good
language background.                                              content match with the original English sentences.
                                                                     Sentences were recorded in a sound-attenuated booth and
The current study                                                 saved as .wav files. Files were edited so that each sentence
  The current study focuses on two hypotheses about               had its own sound file. Two types of sound files were
processing of vocal affect. First, the language-specificity       created for each sentence. One type of stimulus (Figure 1a)
hypothesis suggests that listeners are best at identifying        reflected the original recording, complete with naturalistic
vocal affect in their native language, due to lengthy             emotional sentence semantic content. The other (Figure 1b)
perceptual learning of vocal correlates of emotional states       was low-pass filtered at 500 Hz using Praat software
specific only to that language. This also assumes any second      (Boersma & Weenink, 2011). Low-pass filtering removes
language that the speakers are fluent or near-fluent at will      high-frequency information including cues to consonants
also experience this emotional state comprehension.               and vowels, while retaining low-frequency information in
Crucially, a listener who is not a fluent speaker of a            the speech signal. The result is a muffled, unintelligible
language will have difficulty identifying emotion in that         sound that preserves fundamental frequency variability,
language relative to fluent speakers. Second, the tone-           including lexical tone (Mandarin only), prosody, and speech
language benefit hypothesis posits that listeners with a          rate. This manipulation allowed for the measurement of
history of speaking tone languages will show good                 prosody recognition without the confound of language-
identification of vocal affect even in non-native languages       specific semantic content. Each file was set to an average
because tone languages generally facilitate listeners’            loudness of 60 decibels.
processing of pitch information, and pitch information is
one important cue to affect.                                      (a)
  To investigate how language background affects
emotional speech processing, we asked speakers of
Mandarin Chinese (a tonal language) who also spoke
English, and speakers of American English (a non-tonal
language) to identify emotion in utterances produced in            (b)
Mandarin and English. Participants’ abilities to identify
emotions in their own language were compared to their
ability to identify emotions in languages unfamiliar to them.
                          Methods
                                                                   Figure 1. Spectrogram of an (a) unfiltered and (2) low-pass
Participants                                                                         filtered angry sentence.
Thirty-six undergraduates from the University of California,
                                                                  Procedure
San Diego participated in this study for class credit.
Eighteen of the participants were native English speakers         Each participant listened to 384 total sentences,
who did not speak Mandarin Chinese, and the remaining             counterbalanced across two conditions so no participant
eighteen were native speakers of Mandarin Chinese who             heard the same sentence from the same speaker or in the
also spoke English fluently as a second language. English         same language or in the same filter condition twice. Of the
was a second language for each of the 18 Mandarin                 384 sentences, half each were English and Mandarin;
speaking participants, who acquired English at a mean age         crossed with this, half were unfiltered and half were filtered.
of 8 years (range: 0-17).                                         Participants were asked to identify the emotional state of
                                                                  each sentence as it was presented through Sennheiser HD
Stimuli                                                           280 Pro headphones. The computer monitor displayed the
                                                                  six possible emotions and a number that corresponded to
Eight speakers recorded 96 sentences each in their native
                                                                  each emotion. Participants pressed the number key that they
language. Four speakers (2 male, 2 female) were native
                                                              2334

thought matched the emotion of the sentence. Perceived                  Therefore, we broke the data out into filtered and
emotional responses and reaction times were recorded in              unfiltered data to better highlight interactions at that level.
Matlab using the Psychtoolbox3 (Brainard, 1997; Pelli,               For unfiltered sentences, with full naturalistic verbal
1997).                                                               content, there was a significant interaction between Listener
                                                                     Language and Stimulus Language (F(1,34)=325.58,
                            Results                                  p<.0001), indicating that participants were better able to
                                                                     comprehend emotional affect in languages they spoke
                                                                     highly fluently, which is supportive of our language-
For the current experiment, the language-specificity
                                                                     specificity hypothesis. This result is also important as a
hypothesis and the tone-language hypothesis make similar
                                                                     control for the stimuli used, and demonstrated that the
predictions regarding the participants due to the fact that our
                                                                     sentences provided ample emotional content clues. When
tone language speakers were fluent bilinguals. If listeners
                                                                     presented with unfiltered speech, English speakers were
showed native language specificity of emotion recognition,
                                                                     significantly more accurate with English speech
then accuracy should be higher for English listeners in
                                                                     (t(17)=63.706, p>.0001) whereas Mandarin speakers were
English. Mandarin-English bilinguals should perform
                                                                     equally proficient at identifying emotional affect in both
equally well in both languages—either due to knowledge of
                                                                     languages (t(17)=.891, p=.385) as would be expected from
both languages, or due to enhanced abilities as tone-
                                                                     their language background.
language speakers. This may vary by degree of language
                                                                        Considering the filtered stimuli, which contained only
fluency, as assessed by age of English acquisition.
                                                                     prosodic cues, we again found an interaction of Listener
Importantly, this pattern should still hold for filtered speech,
                                                                     Language and Stimulus Language (F(1,34)=46.278,
which crucially does not contain any semantic language
                                                                     p<.0001), indicating that even when there was a lack of
information. That is, if listeners are using language-specific
                                                                     verbal information, participants were significantly more
acoustic cues to vocal emotion, they should still be more
                                                                     capable to parse emotional affect when presented with
accurate at recognizing emotion in a familiar language even
                                                                     languages they spoke fluently. Considering each listener
when lexical cues are removed.
                                                                     language group individually, English speakers were
   To test this hypothesis, we performed a mixed ANOVA
                                                                     significantly more accurate in identifying the intended
on recognition accuracy with Listener Language (English,
                                                                     emotion when given filtered English speech than filtered
Mandarin) as a between-participants variable, and Stimulus
                                                                     Mandarin speech (t(17)=9.949, p<.001). Mandarin speakers,
Language (English, Mandarin) and Intelligibility (unfiltered,
                                                                     however, showed good performance on filtered speech in
filtered) as within-participants variables. The three-way
                                                                     both languages with no significant differences in accuracy
interaction between these variables was significant
                                                                     (t(17)=1.923, p=.0714).
(F(1,34)=121.70, p<.0001). This interaction qualified all
lower-level effects and interactions.
                                                                        Figure 3. Accuracy for filtered (prosody only) sentences
                                                                         with standard errors. The dotted line represents chance
    Figure 2. Accuracy for unfiltered sentences with standard                                  performance
     errors. The dotted line represents chance performance
                                                                 2335

   Finally, we assessed whether English performance was           monolingual listeners perform better in identifying vocal
affected by degree of English fluency by calculating the          emotion from prosodic cues in their native language.
correlation between bilinguals’ English accuracy on filtered      Listeners familiar with two languages (Mandarin and
speech and their age of English acquisition. This correlation     English) performed comparably in both languages. It is
was not significant (r(16) = -.2176, p = .3856), suggesting       possible more subtle effects were also present in the data
that somewhat surprisingly, age of second language                regarding the bilingual speakers’ performances. For
acquisition was unrelated to ability to process semantic-         instance, if the emotional-speech processing capabilities are
information-free speech.                                          affected by the “sensitive period” demonstrated for
                                                                  phonology, then early-exposed speakers of a second
                         Discussion                               language should show native-level abilities in emotional
Previous work has demonstrated a link between tonal               speech processing, and those who acquired their second
language background and enhancements of abilities in other        language later should not. When a Pearson’s product-
perceptual domains such as music (Pfordresher & Brown,            moment correlation was run on the data, however, the
2009), but the link between language background and vocal         correlation suggested no link between a subject’s age of
emotion had been underexplored. The current study                 acquisition and performance on filtered speech processing.
explored the relationship between language background and         If age of acquisition does not play a correlational role, some
vocal affect identification, focusing on hypotheses of a          other aspect of the subjects’ background must be dictating
language-specific benefit and a tone-language benefit in          their abilities.
vocal affect identification.                                         This leaves the possibility that a general benefit for tone-
   In support of these hypotheses, we showed that listeners       language speakers better accounts for the results. The
are better at discerning emotional content in speech for all      current results, although consistent with a language specific
languages they have achieved fluency in, even when high-          (or familiar-language) benefit, are also consistent with the
frequency lexical cues are removed. This is important,            tone-language benefit hypothesis; speakers of tonal
because the lack of high frequency information removes any        languages performed equally well at identifying vocal affect
clues as to the specific language being presented. This           in both languages presented. However, the data described
means that all performances on filtered speech represent the      here cannot distinguish whether bilinguals’ good
participant’s processing of the low frequency emotional           performance in both languages resulted from their tone-
pitches and tonal changes without the influence of any clues      language background, or simply being fluent in both
to the actual language. This implies that any responses are       languages present in the stimuli. Further study will present
based entirely on pitch processing, and any benefits can          these subject groups with tonal and non-tonal languages
only come from validation of one of the two hypotheses            with which they are not familiar.
presented. The data demonstrate, specifically, that English          If there is a general tone-language advantage, Mandarin
monolinguals identified emotions more accurately in               speakers should outperform English speakers on all
English than in Mandarin, whereas Mandarin-English                unfamiliar languages. This would indicate that a tonal
bilinguals showed equivalent performance in both                  language background affords the speaker with a type of
languages. This is consistent with the language-specificity       emotional prosody processing training, and that there are
hypothesis: that listeners are better at discerning cues to       aspects of tonal languages that improve the processing
emotion in their native language. However, due to the             capabilities of individuals in emotional speech. This might
design of the current study, it is also consistent with the       be true if there are universal pitch characteristics that can be
tone-language facilitation hypothesis: that tone-language         found in all languages, but are very subtle and difficult to
speakers, due to lengthy experience attending to fine-            pick up on if the listener does not have sufficient training in
grained pitch characteristics of language, have a general         either that specific language or excellent awareness in pitch
advantage at recognizing vocal emotion. That is, Mandarin         perception in general.
listeners performed at above-chance levels in both                   There could even be a specific tone-language advantage,
languages because they are tone-language speakers, not only       such that tone-language speakers would outperform non-
because they also speak English. This would predict that          tone language speakers only for unfamiliar tone languages.
Mandarin listeners would also be superior at emotion              This data pattern might hold if tone languages use devices to
recognition in a completely unfamiliar language, a                convey vocal emotion that are similar across a range of tone
hypothesis we are currently testing. Further study is             languages, but different than non-tone languages. We are
currently being performed to address these design                 currently testing these possibilities regarding the tone-
limitations, and will alleviate the current confound of the       language advantage with English speakers and Mandarin-
tonal language speaking subjects being fluent in the              English bilinguals and four languages (English; Mandarin;
languages of all the presented stimuli                            Dutch [unfamiliar non-tone language]; Vietnamese
   Nevertheless, in the current study, discussion of the          [unfamiliar tone language]).
implications of both hypotheses is warranted by the data. In         The present data demonstrate that there is emotional
regards to the first hypothesis, the language-specific            affect information present even without higher frequency
hypothesis, our data are consistent with the expectation that     information. It also provides evidence that speakers are
                                                              2336

capable of picking up on this information without relying on     Thompson, W. F., & Matsunaga, R. I. E. (2004).
distinct linguistic information in languages that they are         Recognition of emotion in Japanese, Western, and
familiar with. The current study provides a tantalizing peek       Hindustani music by Japanese listeners 1, 46(4), 337–349.
into the emotional affect processing provided by language        Thompson, W., Schellenberg, E. G., & Husain, G. (2006).
background, and with further study already in process,             Perceiving Prosody in Speech. Annals of the New York
moves our understanding of vocal emotional affect                  Academy of Sciences, 999, 530–532.
processing forward.                                              Thompson, W. F., Schellenberg, E. G., & Husain, G.
                                                                   (2004). Decoding speech prosody: do music lessons help?
                   Acknowledgements                                Emotion (Washington, D.C.), 4(1), 46–64.
   CIF was supported by the Center for Research in
Language at UC San Diego, and SCC was supported by an
NSF CAREER Award (BCS-1057080). We would like to
acknowledge the help of undergraduate research assistants
Sheryl Soo, Christina Hwang, and Ben Howard, who were
instrumental in developing language stimuli and collecting
data on participant’s language background.
                        References
Balkwill, L., & Thompson, W. (1999). A cross-cultural
   investigation of the perception of emotion in music:
   Psychophysical and cultural cues. Music perception,
   17(1), 43–64.
Bregman, M. R., & Creel, S. C. (2012). Learning to
   recognize unfamiliar voices: the role of language
   familiarity and music experience. In N. Miyake, D.
   Peebles, & R. P. Cooper (Eds.), Proceedings of the 34th
   Annual Conference of the Cognitive Science Society.
   Austin, TX: Cognitive Science Society.
Johnson, E. K., Westrek, E., Nazzi, T., & Cutler, A. (2011).
   Infant ability to tell voices apart rests on language
   experience. Developmental Science, 5, 1002-1011.
Kehrein, R. (2002). The prosody of authentic emotions.
   Proc. Speech Prosody Conf (pp. 423–426).
Kuhl, P. K. (1994). Learning and representation in speech
   and language. Current Opinion in Neurobiology, 4, 812-
   822.
Ley, R. G., & Bryden, M. P. (1982). A dissociation of right
   and left hemispheric effects for recognizing emotional
   tone and verbal content. Brain and cognition, 1(1), 3–9.
Morton, J. B., & Trehub, S. E. (2001). Children’s
   Understanding of Emotion in Speech. Child Development,
   72(5), 834-843.
Pfordresher, P. Q., & Brown, S. (2009). Enhanced
   production and perception of musical pitch in tone
   language     speakers.     Attention,   Perception,      &
   Psychophysics, 71(7), 1385–1398. Springer.
Sauter, D. A, Eisner, F., Ekman, P., & Scott, S. K. (2010).
   Cross-cultural recognition of basic emotions through
   nonverbal emotional vocalizations. Proceedings of the
   National Academy of Sciences of the United States of
   America, 107(7), 2408-12.
Slevc, L. R., & Miyake, A. (2006). Individual differences in
   second-language proficiency: does musical ability matter?
   Psychological Science, 17(8), 675-81.
Thompson, W. F., & Balkwill, L.-L. (2006). Decoding
   speech prosody in five languages. Semiotica, 2006(158),
   407–424. Walter de Gruyter.
                                                             2337

