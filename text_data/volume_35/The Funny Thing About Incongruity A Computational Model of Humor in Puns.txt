UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Funny Thing About Incongruity: A Computational Model of Humor in Puns
Permalink
https://escholarship.org/uc/item/04j190sw
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Kao, Justine T.
Levy, Roger
Goodman, Noah D.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

             The Funny Thing About Incongruity: A Computational Model of Humor in Puns
Justine T. Kao1 (justinek@stanford.edu), Roger Levy2 (rlevy@ucsd.edu), Noah D. Goodman1 (ngoodman@stanford.edu)
                        1 Department   of Psychology, Stanford University. 2 Department of Linguistics, UC San Diego.
                                  Abstract                                     Language understanding in general, and particularly hu-
                                                                            mor, relies on rich commonsense knowledge and discourse
       Researchers showed the robot ten puns, hoping that one of
       them would make it laugh. Unfortunately, no pun in ten did.          understanding. To somewhat limit the scope of our task, we
                                                                            focus on applying formalizations of incongruity to a subset of
       What makes something funny? Humor theorists posit that               linguistic humor: puns. Writer and philosopher Henri Berg-
       incongruity—perceiving a situation from different viewpoints         son defined a pun as “a sentence or utterance in which two
       and finding the resulting interpretations to be incompatible—
       contributes to sensations of mirth. In this paper, we use a com-     ideas are expressed, and we are confronted with only one se-
       putational model of sentence comprehension to formalize in-          ries of words.” This highlights the fact that one sentence must
       congruity and test its relationship to humor in puns. By com-        evoke two different interpretations in order to be a pun, which
       bining a noisy channel model of language comprehension and
       standard information theoretic measures, we derive two dimen-        aligns with the concept of incongruity as a requisite of humor.
       sions of incongruity—ambiguity of meaning and distinctive-              We develop our model on homophone puns—puns contain-
       ness of viewpoints—and use them to predict humans’ judg-             ing words that sound identical to other words in the English
       ments of funniness. Results showed that both ambiguity and
       distinctiveness are significant predictors of humor. Addition-       language—because the space of possible interpretations of
       ally, our model automatically identifies specific features of a      a homophone pun is relatively constrained and well-defined.
       pun that make it amusing. We thus show how a probabilistic           An example helps to illustrate:
       model of sentence comprehension can help explain essential
       features of the complex phenomenon of linguistic humor.
                                                                               “The magician got so mad he pulled his hare out.”
       Keywords: Humor; language understanding; probabilistic
       models                                                               This sentence allows for two interpretations:
                              Introduction                                (a) The magician got so mad he performed the trick of pulling
    Humor plays an essential role in human interactions: it has                a rabbit out of his hat.
    important positive effects on children’s development (Frank           (b) The magician got so mad he (idiomatically) pulled out the
    & McGhee, 1989), success in the work place (Duncan et                      hair on his head.
    al., 1990), coping with illness and traumatic events (Gelkopf
    & Kreitler, 1996), and marital satisfaction (Ziv & Gadish,              If the comprehender interprets the word “hare” as itself, he
    1989). Indeed, in a study on gender differences in desired              will arrive at interpretation (a); if he interprets the word as
    characteristics of relationship partners, both men and women            its homophone “hair,” he will arrive at interpretation (b). The
    rated sense of humor as more important than physical attrac-            sentence-level differences between interpretations (a) and (b)
    tiveness and earning potential (Stewart et al., 2000). In this          can thus be approximated by the two interpretations of the
    paper, we are interested in understanding how this fundamen-            observed word “hare.” In general, distinct interpretations of a
    tal and ubiquitous phenomenon works from the perspective of             homophone pun hinges on one phonetically ambiguous word,
    cognitive science. What makes something funny? How might                allowing the two lexical forms of the homophone word to
    defining characteristics of humor shed light on the ways in             stand in for competing interpretations of the entire sentence.
    which the mind processes and evaluates information?                        Critically, even though the example we gave was a writ-
       A leading theory of humor posits that incongruity—                   ten pun and the reader sees the word “hare” explicitly on the
    perceiving a situation from different viewpoints and finding            page, the “hair” interpretation is still present and even salient
    the resulting interpretations to be incompatible—contributes            in the context of the sentence. Here we explore the idea that
    to sensations of mirth (Veale, 2004; Forabosco, 1992; Mar-              puns such as these arise and are funny when they are due to
    tin, 2007; Hurley et al., 2011); an idea that dates to Kant’s           noisy-channel processing. Noisy channel models of sentence
    theories about laughter and the sublime (Veatch, 1998). Al-             processing posit that language comprehension is a rational
    though there is disagreement about whether incongruity alone            process that incorporates uncertainty about surface input to
    is sufficient, most theorists accept that incongruity is neces-         arrive at sentence-level interpretations that are globally coher-
    sary for producing humor: as Veale (2004) states, “Of the               ent (Levy, 2008; Levy et al., 2009). Comprehenders can thus
    few sweeping generalizations one can make about humor that              consider multiple word-level interpretations (“viewpoints”)
    are neither controversial or trivially false, one is surely that        to arrive at more than one interpretation of a sentence, each
    humor is a phenomenon that relies on incongruity.” However,             coherent but potentially incongruous with each other. The
    definitions of incongruity are often ambiguous and difficult to         notion of incongruity thus fits naturally into a noisy channel
    operationalize in empirical research. In this paper, we use a           model of sentence comprehension.
    computational model of language understanding to formalize                 Our purposes for developing a formal model of linguistic
    a notion of incongruity and test its relationship to humor.             humor are two-fold. First, we wish to formalize the concept
                                                                        728

of incongruity and test assumptions adopted by leading the-
                                                                                                               m
ories in humor research. Secondly, we aim to show that a
noisy channel of language processing allows for flexible con-
text selection and sentence comprehension that gives rise to
sophisticated linguistic and social meaning such as humor.                                     w1        w2        h          wn
                                 Model
Incongruity is a property of the interpretations derived from a                                f1        f2        fh         fn
sentence. In order to formalize incongruity, we first describe
a probabilistic model of sentence comprehension. Our model
                                                                              Figure 1: Generative model of a sentence. Each word wi is
aims to infer the topic of a sentence (a coarse representation
                                                                              generated based on the sentence topic m if the indicator vari-
of its meaning) from the observed words. Unlike previous
                                                                              able fi puts it in semantic focus; otherwise it is generated as
such models, however, we take a noisy channel approach,
                                                                              noise (from a unigram distribution).
assuming that the comprehender maintains uncertainty over
which words reflect the sentence topic and which are noise.
From this model we derive two quantities that may contribute                     Using the above generative model, we can infer the joint
to humor: ambiguity and distinctiveness. Intuitively, if the re-              probability distribution P(m, ~f |~w) of the sentence topic m and
sulting interpretation is unambiguous, then no incongruity ex-                the indicator variables ~f that determine whether each word is
ists and the sentence is unlikely to be funny. However, since                 in semantic focus. This distribution can be factorized into:
many ambiguous sentences are not funny (e.g. “I went to the
bank”), ambiguity alone is insufficient. This is because the in-                               P(m, ~f |~w) = P(m|~w)P(~f |m,~w)             (1)
terpretations of such sentences are not supported by distinct
topical subsets of the sentence (or “viewpoints”). In other                   The two terms on the right-hand side are the basis for our
words, there must be a set of words in the sentence that sup-                 derivations of measures for ambiguity and distinctiveness re-
port one interpretation and a set that supports the other, and                spectively. Ambiguity means the presence of two similarly
these two sets must be different or “distinct” from each other                likely interpretations and can be quantified as a summary of
in order to evoke a sense of incongruity.                                     the distribution P(m|~w). Distinctiveness measures the degree
    Assume our sentence is composed of a vector of content                    to which two interpretations are supported by “distinct” view-
words ~w = {w1 , . . . , wi , h, wi+1 , . . . , wn }, including a phonet-     points of the sentence, which we represent as the divergence
ically ambiguous word h. We will use a simple generative                      between sets of words that are in semantic focus given the
model for ~w (see Figure 1): given the latent sentence topic                  two values of m; it can be quantified as a summary of the dis-
m, each word is generated independently by first deciding if                  tribution P(~f |m,~w). Together, these two measures constitute
it reflects the topic (the indicator variable fi ). If so it is sam-          our formalization of incongruity.
pled based on semantic relevance to m; if not it is sampled                   Ambiguity Let M denote the distribution P(m|~w), a bino-
from a fixed unigram prior over words. We thus view the sen-                  mial distribution over the two meaning values m1 and m2
tence as a mixture of topical and non-topical words. Similar                  given the observed words. If the entropy of this distribution
approaches have been used in generative models of language                    is low, this means that the probability mass is concentrated
to account for words that provide non-semantic information,                   on only one meaning, and the alternative meaning is unlikely
such as topic models that incorporate syntax (Griffiths et al.,               given the observed words. If entropy is high, this means that
2005). Our model is motivated by the important role that se-                  the probability mass is more evenly distributed among m1 and
mantic priming plays in lexical disambiguation during sen-                    m1 , and the two interpretations are similarly likely given the
tence processing (Seidenberg et al., 1982; Burke & Yee,                       contexts. The entropy of P(m|~w) is thus a natural measure of
1984); while ignoring the other non-semantic factors of in-                   the degree of ambiguity present in a sentence. We compute
terpretation (which may also be important).                                   P(m|~w) as follows:
    We make the simplifying assumption that the plausible
candidate topics m of the sentence correspond to the poten-                              P(m|~w) = ∑ P(m, ~f |~w)                            (2)
tial interpretations of the homophone word h, which are con-                                          ~f
strained by phonetic similarity to two alternatives, m1 and
                                                                                                  ∝ ∑ P(~w|m, ~f )P(m)P(~f )                 (3)
m2 . For example, in the magician pun described above, h                                             ~f
is the phonetically ambiguous target word “hare,” and m1 and                                                                       
m2 are the candidate interpretations hare and hair. The two                                                       ~
                                                                                                  = ∑ P(m)P( f ) ∏ P(wi |m, fi )             (4)
potential topics of the sentence can be identified by the two                                         ~f               i
interpretations hare and hair. This assumption reduces the
ill-defined space of sentence meanings to the simple proxy of                 We approximate P(m) as the unigram frequency of the words
alternate spellings for phonetically ambiguous words.                         that represent m. For example, P(m = hare) is approximated
                                                                          729

as P(m = “hare”). We also assume a uniform probability that           changing the homophone word itself) so that the sentence is
each words is in focus—hence P(~f ) is a constant. As for             still grammatical but is no longer a pun. This resulted in sen-
P(m|~w), note that it is driven in part by the semantic relation-     tences that differed from the pun sentences by one word each.
ship between m and ~w and in part by the prior probability of            The 130 non-pun sentences were chosen to match each pun
m, which we approximate using the unigram probability of              sentence on its ambiguous word as well as the alternative ho-
the words m1 and m2 . From the generative model,                      mophone. The sentences were taken from an online version
                              (                                       of Heinle’s Newbury House Dictionary of American English
                                P(wi ),     if f = 0                  (http://nhd.heinle.com/). We selected sample sentences
              P(wi |m, fi ) =                                         included in the definition of the homophone word. This de-
                                P(wi |m), if f = 1
                                                                      sign ensured that puns, de-punned, and non-pun sentences all
Once we derive P(m|~w), we then compute its entropy as a              contain the same set of phonetically ambiguous words. Ta-
measure of ambiguity.                                                 ble 1 shows example sentences from each category.
Distinctiveness We next turn to the distribution over focus             Type         Example
sets, given sentence topic. This may be computed as follows:            Pun          The magician got so mad he pulled his hare out.
                                                                        De-pun       The professor got so mad he pulled his hare out.
                P(~f |m,~w) ∝ P(~w|m, ~f )P(~f |m)            (5)       Non-pun      The hare ran rapidly across the field.
                                                                        Non-pun      Some people have lots of hair on their heads.
Since ~f and m are independent, P(~f |m) = P(~f ).
   Let F1 denote the distribution P( f |m1 ,~w) and F2 denote the             Table 1: Example sentences from each category
distribution P( f |m2 ,~w). F1 and F2 represent the distributions
over semantic focus sets assuming the sentence topic m1 and
m2 , respectively. We use a symmetrized Kullback-Leibler              Human ratings of semantic relatedness
divergence score DKL (F1 ||F2 ) + DKL (F2 ||F1 ) to measure the       As described in the model section, computing our measures
distance between F1 and F2 . This score measures how “dis-            requires the prior probabilities of meanings P(m) (approxi-
tinct” the semantic focus sets are given m1 and m2 . A low KL         mated as the unigram probabilities of the words that denote
score would indicate that meanings m1 and m2 are supported            the meanings), the prior probabilities of words P(w), and the
by similar subsets of the sentence; a high KL score would in-         conditional probabilities of each word in the sentence given
dicate that m1 and m2 are supported by distinct subsets of the        a meaning P(w|m). While we computed P(w) and P(m) di-
sentence.                                                             rectly from the Google Web unigram corpus, P(w|m) is dif-
                                                                      ficult to obtain through traditional topic models trained on
                           Evaluation                                 corpora due to data sparsity. However, since each meaning
By generating a large corpus of sentences involving the same          we consider has a single word as proxy, we may approximate
words and measuring subjective funniness of each sentence             P(w|m) using an empirical measure of the semantic related-
we can evaluate the contribution of each of our quantitative          ness between w and m, denoted R(c, m). We use R(c, m) as
measures, ambiguity and distinctiveness, to humor. We eval-           a proxy for point wise mutual information between c and m,
uate our model and measures on a set of 235 sentences, con-           defined as follows:
sisting of 65 puns, 40 “de-punned” control sentences that are
                                                                                       P(w, m)           P(w|m)
matched with a subset of the puns, and 130 non-pun control            R(w, m) = log               = log          = log P(w|m)−log P(w)
sentences that match the puns in containing the same phonet-                          P(w)P(m)            P(w)
ically ambiguous words.                                               We assume that human ratings of relatedness between two
Materials                                                             words R0 (w, m) approximate true relatedness up to an additive
                                                                      constant z. With the proper substitutions and transformations,
We selected 40 pun sentences from a large collection of puns
                                                                                                       0
on a website called Pun of the Day, which contains over one                              P(w|m) = eR (w,m)+z P(w)                   (6)
thousand puns. Puns were selected such that the ambiguous
item is a single phonetically ambiguous word, and no two                 To obtain R0 (w, m) for each of the words w in the stimuli
puns in the collection have the same ambiguous item. To ob-           sentences, we recruited 200 subjects on Amazon’s Mechani-
tain more homophone pun items, a research assistant gener-            cal Turk to rate distinct word pairs on their semantic related-
ated an additional 25 pun sentences based on a separate list          ness. Since it is difficult to obtain the relatedness rating of a
of homophone words.                                                   word with itself, we used a free parameter r and fit it to data.
   We constructed 40 sentences to be minimally different              Function words were removed from each of the sentences in
from the pun sentences that we collected from “Pun of the             our dataset, and the remaining words were paired with each of
Day,” which we will call de-punned sentences. A second re-            the interpretations of the homophone sequence (e.g., for the
search assistant who was blind to the hypothesis was asked            pun in Figure 1, “magician” and “hare” is a legitimate word
to replace one word in each of the pun sentences (without             pair, as well as “magician” and “hair”). This resulted in 1460
                                                                  730

     (a) Relatedness of each word with candidate meanings             (b) Average relatedness              (c) Average funniness ratings
Figure 2: (a) In the example pun (top), two candidate meanings of h are each more related to a subset of the content words. In
the non-pun, only one candidate meaning is more related. (b) Content words are similarly related to both candidate meanings
in puns; more related to alternative meanings in de-puns; more related to observed meanings in non-pun. (c) Funniness varies
across the sentence types in a pattern that reflects the balance of relatedness to candidate meanings shown in (b).
distinct word pairs. Each subject saw 146 pairs of words in                                   Estimate   Std. Error    p value
random order and were asked to rate how related each word                 Intercept            −0.699         0.180    < 0.0001
pair is using a scale from 1 to 10. The average split-half cor-           Ambiguity              1.338        0.245    < 0.0001
relation of the relatedness ratings was 0.916, indicating that            Distinctiveness        0.183        0.053    < 0.0001
semantic relatedness was a reliable measure.
   Figure 2(a) shows the relatedness of each content word            Table 2: Regression coefficients using ambiguity and distinc-
with the two homophone interpretations for two example sen-          tiveness to predict funniness ratings
tences. In the top sentence, which is a pun, the word “magi-
cian” is rated as significantly more related to “hare” than it
                                                                     for the sentence types, and rated each sentence on funni-
is to “hair”, while the word “pulled” is rated as significantly
                                                                     ness and correctness. The average split-half correlation of
more related to “hair” than it is to “hare.” In the bottom sen-
                                                                     funniness ratings was 0.83. Figure 2(c) shows the aver-
tence, which is a non-pun, all words except the neutral word
                                                                     age funniness ratings of puns, de-punned, and non-pun sen-
“through” are more related to the word “hare” than to “hair.”
                                                                     tences. Pun sentences are rated as significantly funnier than
   Figure 2(b) shows the average relatedness ratings of words        de-punned sentences, and de-punned sentences are rated as
and the two homophone interpretations across the three types         significantly funnier than non-pun sentences (F(2, 232) =
of sentences. In pun sentences, the average relatedness of           415.3, p < 0.0001). Figure 2 (b) and Figure 2 (c) together
words to the two homophone interpretations are not signif-           suggest that the balance of relatedness between the two inter-
icantly different. In the de-punned sentences, the average           pretations is a predictor of funniness.
relatedness of words to the alternative meaning is signifi-
cantly higher than to the observed meaning. In the non-pun                                       Results
sentences, the average relatedness of words to the observed          Following the derivations described in the model section and
meaning is significantly higher than to the alternative mean-        using the relatedness measures described above, we com-
ing. These analyses suggest that relatedness ratings for the         puted an ambiguity and distinctiveness value for each of the
two candidate meanings capture the presence or absence of            235 sentences. Our model has two free parameters—the ad-
multiple interpretations in a sentence. It further supports our      ditive constant z in equation (6) and the constant r that indi-
model’s prediction that ambiguity of meaning and the dis-            cates the relatedness of a word with itself —which we opti-
tinctiveness of supporting context words can help distinguish        mized using R2 in the linear regression summarized in Ta-
among the three types of sentences.                                  ble 2. As predicted, ambiguity differs significantly across
                                                                     sentence types (F(2, 232) = 25.42, p < 0.0001) and corre-
Human Ratings of Funniness
                                                                     lates significantly with human ratings of funniness across
We obtained funniness ratings of the 235 sentences from              the 235 sentences (r = 0.33, p < 0.0001). Furthermore, dis-
100 subjects on Amazon’s Mechanical Turk. Each subject               tinctiveness scores differ significantly across sentence types
read roughly 60 sentences in random order, counterbalanced           as well (F(2, 232) = 5.76, p < 0.005) and correlates signifi-
                                                                 731

   m1      m2     Type        Sentence and Semantic Focus Sets                                            Amb.      Disj.    Funniness
                  Pun         The magician got so mad he pulled his hare out.                             0.570     3.405         1.714
                  De-pun      The professor got so mad he pulled his hare out.                            0.575     2.698         0.328
  hare     hair
                  Non-pun     The hare ran rapidly through the fields.                                    0.055     2.791       −0.400
                  Non-pun     Most people have lots of hair on their heads.                             2.76E −5    3.920       −0.343
                  Pun         It was an emotional wedding. Even the cake was in tiers.                    0.333     3.424         1.541
                  De-pun      It was an emotional wedding. Even the mother-in-law was in tiers.           0.693     2.916         0.057
  tiers   tears
                  Non-pun     Boxes are stacked in tiers in the warehouse.                                0.018     3.203       −0.560
                  Non-pun     Tears ran down her cheeks as she watched a sad movie.                     1.73E −5    4.397       −0.569
Table 3: Semantic focus sets, ambiguity/disjointedness scores, and funniness ratings for two groups of sentences. Words in
red are in semantic focus with m1 ; green with m2 ; blue with both. Semantic focus sets for all sentences can be found at
http://www.stanford.edu/˜justinek/Pun/focusSets.html
Figure 3: Standard error ellipses of ambiguity and distinctive-     Figure 4: Funniness contours smoothed using a 2-D Loess re-
ness across sentence types. Puns score higher on ambiguity          gression with ambiguity and disjointedness measures as pre-
and distinctiveness; de-puns are less supported by distinct fo-     dictors. Sentences become funnier as they move to high am-
cus sets; non-puns have low ambiguity.                              biguity and distinctiveness space.
cantly with human ratings of funniness (r = 0.21, p < 0.005).       lowest on ambiguity with moderate distinctiveness measures.
   A linear regression showed that both ambiguity and dis-             Figure 4 shows the funniness contours in the two-
tinctiveness are significant predictors of funniness. Together,     dimensional ambiguity-distinctiveness space smoothed using
the two predictors capture a modest but significant amount          a 2-D Loess regression. Not only do the three types of sen-
of the variance in funniness ratings (F(2, 232) = 20.86, R2 =       tences differ along the two dimensions, but sentences be-
0.145, p < 0.001; see Table 2). Using both ambiguity and            come funnier as they increase in ambiguity and distinctive-
distinctiveness as dimensions that formalize incongruity, we        ness. These results suggest that our measures of incongruity
can distinguish among puns, non-puns, and de-punned sen-            capture an important aspect of humor in pun sentences.
tences, as shown in Figure 3. Figure 3 shows the stan-                 Beyond predicting the funniness of a sentence, our model
dard error ellipses for each of the three sentence types in         can also tell us which particular features of a pun make it
the two-dimensional space of ambiguity and distinctiveness.         amusing. By finding the most likely semantic focus sets ~f
Although there is a fair amount of noise in the predictors          given each latent meaning variable m and the observed words,
(likely due to simplifying assumptions, the need to use em-         we can identify words in a funny sentence that are critical to
pirical measures of relatedness, and the inherent complexity        producing incongruity and humor. Table 3 shows the most
of humor) we see that pun sentences tend to cluster at a space      likely semantic focus sets given each meaning for two groups
with higher ambiguity and distinctiveness. While de-punned          of sentences. Sentences in each group contain the same pair
sentences are also high on ambiguity (e.g. it is ambiguous          of candidate meanings for the target word h. However, they
whether the word “hare” in “The professor got so mad he             differ in measures of ambiguity, distinctiveness, and funni-
pulled his hare out” should be interpreted as hair), they tend      ness. Words in the most likely focus sets given m1 are in red;
to have lower distinctiveness measures. Non-puns score the          words in the most likely focus sets given m2 are in green; and
                                                                732

words in the most likely focus sets of both meanings are in         our work contributes to research in humor theory, computa-
dark blue. We observe that visually, the two pun sentences          tional humor, and language understanding, with the aim to
(which are significantly funnier) have more distinctive and         one day understand what makes us laugh and build robots
balanced sets of focus words for each meaning than other            that appreciate the wonders of word play.
sentences in their groups. De-punned sentences tend to have
fewer words in support of m1 , and non-pun sentences tend to                               Acknowledgments
have no words in support of the interpretation that was not         This material is based upon work supported by the National
observed. Moreover, imagine if you were asked to explain            Science Foundation Graduate Research Fellowship to JTK
why the two pun sentences are funny. The colorful words in          and a John S. McDonnell Foundation Scholar Award to NDG.
each pun sentence—for example, the fact that magicians tend         We thank Stu Melton and Mia Polansky for helping prepare
to perform magic tricks with hares, and people tend to be de-       part of the materials used in this paper.
scribed as pulling out their hair when angry—are what one
might intuitively use to explain why the sentence is a pun.                                     References
Our model thus provides a natural way of not only formaliz-         Binsted, K. (1996). Machine humour: An implemented model of
                                                                       puns.
ing incongruity and using it to predict when a sentence is a        Burke, D., & Yee, P. (1984). Semantic priming during sentence pro-
pun, but also to explain what aspects of a pun make it funny.          cessing by young and older adults. Developmental Psychology,
                                                                       20(5), 903.
                                                                    Duncan, W., Smeltzer, L., & Leap, T. (1990). Humor and work:
                         Discussion                                    Applications of joking behavior to management. Journal of Man-
                                                                       agement, 16(2), 255–278.
Researchers in artificial intelligence have argued that given       Forabosco, G. (1992). Cognitive aspects of the humor process: The
the importance of humor in human communication, comput-                concept of incongruity. Humor: International Journal of Humor
ers need to generate and detect humor in order to interact with        Research; Humor: International Journal of Humor Research.
                                                                    Frank, M., & McGhee, P. (1989). Humor and children’s develop-
humans more effectively (Mihalcea & Strapparava, 2006).                ment: A guide to practical applications. Routledge.
However, most work in computational humor has focused ei-           Gelkopf, M., & Kreitler, S. (1996). Is humor only fun, an alterna-
ther on joke-specific templates and schemata (Binsted, 1996;           tive cure or magic? the cognitive therapeutic potential of humor.
                                                                       Journal of Cognitive Psychotherapy, 10(4), 235–254.
Kiddon & Brun, 2011) or surface linguistic features that pre-       Griffiths, T., Steyvers, M., Blei, D., & Tenenbaum, J. (2005). In-
dict humorous intent (Mihalcea & Strapparava, 2006; Reyes              tegrating topics and syntax. Advances in neural information pro-
et al., 2010). Our work moves beyond these approaches and              cessing systems, 17, 537–544.
                                                                    Hurley, M., Dennett, D., & Adams, R. (2011). Inside jokes: Using
directly utilizes a model of sentence comprehension to derive          humor to reverse-engineer the mind. Mit Pr.
theory-driven measures of humor.                                    Kiddon, C., & Brun, Y. (2011). Thats what she said: double enten-
                                                                       dre identification. In Proceedings of the 49th annual meeting of
   While the measures we developed account for a signifi-              the association for computational linguistics: Human language
cant amount of variance in funniness ratings, there are several        technologies (pp. 89–94).
ways to improve our model of language in order to more accu-        Levy, R. (2008). A noisy-channel model of rational human sentence
                                                                       comprehension under uncertain input. In Proceedings of the con-
rately capture the subtleties of linguistic humor. By making           ference on empirical methods in natural language processing (pp.
the simplifying assumption that semantic association drives            234–243).
sentence comprehension, we disregarded the sequential struc-        Levy, R., Bicknell, K., Slattery, T., & Rayner, K. (2009). Eye move-
                                                                       ment evidence that readers maintain and act on uncertainty about
ture of language that is often important for understanding a           past linguistic input. Proceedings of the National Academy of Sci-
pun. For example, “The actors had one great movie after an-            ences, 106(50), 21086–21090.
other. They were on a role.” scores high on funniness but low       Martin, R. (2007). The psychology of humor: An integrative ap-
                                                                       proach. Academic Press.
on our measures because it leverages the idiomatic expression       Mihalcea, R., & Strapparava, C. (2006). Learning to laugh (auto-
“on a roll” to boost the interpretation roll. Since our bag-of-        matically): Computational models for humor recognition. Com-
words model does not account for word sequences, the mea-              putational Intelligence, 22(2), 126–142.
                                                                    Reyes, A., Buscaldi, D., & Rosso, P. (2010). The impact of seman-
sures we derive fail to fully capture the incongruity of many          tic and morphosyntactic ambiguity on automatic humour recog-
pun sentences that contain idiomatic expressions. In future            nition. In H. Horacek, E. Mtais, R. Muoz, & M. Wolska (Eds.),
work, we aim to incorporate information about the sequential           Natural language processing and information systems (Vol. 5723,
                                                                       p. 130-141). Springer Berlin / Heidelberg.
structure of a sentence to further improve our language model       Seidenberg, M., Tanenhaus, M., Leiman, J., & Bienkowski, M.
and measures of incongruity.                                           (1982). Automatic access of the meanings of ambiguous words in
   In this paper, we showed how a basic model of sentence              context: Some limitations of knowledge-based processing. Cog-
                                                                       nitive psychology, 14(4), 489–537.
comprehension can illuminate incongruous sentence interpre-         Stewart, S., Stinnett, H., & Rosenfeld, L. (2000). Sex differences
tations with rich social and linguistic meaning. Although our          in desired characteristics of short-term and long-term relationship
task in this paper is limited in scope, we believe that it rep-        partners. Journal of Social and Personal Relationships, 17(6),
                                                                       843–853.
resents a step towards developing models of language that           Veale, T. (2004). Incongruity in humor: Root cause or epiphe-
can explain complex phenomena such as humor. From the                  nomenon? Humor-International Journal of Humor Research,
perspective of language understanding, such phenomena can              17(4), 419–428.
                                                                    Veatch, T. (1998). A theory of humor. Humor, 11, 161–215.
serve as probes for developing models of language that ac-          Ziv, A., & Gadish, O. (1989). Humor and marital satisfaction. The
count for the subtleties of linguistic behavior. We hope that          journal of social psychology, 129(6), 759–768.
                                                                733

