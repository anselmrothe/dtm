UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Match Me if You Can: How Smart Choices are Fueled by Competition
Permalink
https://escholarship.org/uc/item/62t0j19r
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Schulze, Christin
Van Ravezwaaij, Don
Newell, Ben R.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                                      Match Me if You Can:
                                 How Smart Choices are Fueled by Competition
                                            Christin Schulze (c.schulze@unsw.edu.au)
                                         School of Psychology, University of New South Wales
                                                        Sydney, NSW 2052 Australia
                                  Don van Ravenzwaaij (d.vanravenzwaaij@unsw.edu.au)
                                         School of Psychology, University of New South Wales
                                                        Sydney, NSW 2052 Australia
                                             Ben R. Newell (ben.newell@ unsw.edu.au)
                                         School of Psychology, University of New South Wales
                                                        Sydney, NSW 2052 Australia
                              Abstract                                   2000). In a typical setup a decision maker repeatedly has
   In a world of limited resources, scarcity and rivalry are central
                                                                         two choice options available, one of which is the correct
   challenges for decision makers. We examine choice behavior            choice with greater probability than its alternative, e.g.
   in competitive probability learning environments that                 p(A1) = .7 and p(A2) = .3, and correct predictions are
   reinforce one of two strategies. The optimality of a strategy is      rewarded with monetary payoffs. Assuming the outcome
   dependent on the behavior of a computerized opponent: if the          probabilities are stationary and irrespective of prior events
   opponent mimics participant choices, probability matching is          or subjects‚Äô behavior, A1 is the superior choice option
   optimal; if the opponent is indifferent, probability maximizing       throughout and, following an initial period of probability
   is optimal. We observed accurate asymptotic strategy use in
   both conditions suggesting participants were sensitive to the
                                                                         learning, should be chosen exclusively. By contrast, the
   differences in opponent behavior. Moreover, the results               frequently observed probability (over-) matching tendency
   emphasize that ‚Äòirrational‚Äô probability matching can be               results in inferior prediction accuracies and payoffs and is
   adaptive once such competitive pressures are taken into               therefore considered fallacious within context-independent
   account. The application of reinforcement learning models to          interpretations of rational choice behavior (Vulkan, 2000).
   the data suggests that computational conceptualizations of
   opponent behavior are critical to account for the observed            Probability Matching in Competitive Environments
   divergence in strategy adoption.
                                                                         What seems irrational in individualized context-free
   Keywords: Decision making; Probability matching;                      environments, however, can be optimal in ecologically more
   Reinforcement       learning;     Evolutionary      psychology;
   Mathematical modeling                                                 valid situations that take prospective social interactions into
                                                                         account (Gallistel, 1990; Gigerenzer, 2000). That is to say,
                          Introduction                                   when decision makers seek to exploit limited resources
                                                                         under natural circumstances (e.g. forage for food or make
Competition is a pervasive characteristic of the world ‚Äì                 money), they are rarely alone but typically in fierce
plants compete for light, water and pollination; animals are             competition for the exploitation of these resources with
in continual competition for territory, food and mating; and             other agents. The more individual agents then choose the
even as humans we are constantly competing in sports, for                seemingly richest resource, the smaller each one‚Äôs share. In
social standing and companionship. Considering the                       nature, this situation cannot remain stable as natural
ubiquity of competitive pressures in virtually all aspects of            selection would favor those agents who sometimes chose
our lives, their crucial impact on the development of                    options with potentially scarce resources that are exploitable
adaptive decision strategies in a broad range of contexts                under less competition (Gallistel, 1990).
may seem self-evident. And yet, prior research has                          Following this line of argument, it has been suggested that
concentrated on assessing the rationality of numerous choice             agents should distribute their choices among resources
phenomena primarily by focusing on individual decision                   relative to their reward potentials, i.e. adopt a probability
makers in social isolation. Consequently, observed choice                matching strategy, to create an equilibrated evolutionary
inconsistencies are frequently dismissed as suboptimal with              stable situation that does not give rise to conditions
little or no regard for their adaptive potential in ecologically         selecting against it (Gallistel, 1990). Evidence for such
valid settings (see e.g., Todd & Gigerenzer, 2007).                      behavior has been provided by experiments that studied
   One such extensively studied choice anomaly is the                    groups of animals in the wild, e.g. foraging behavior of
tendency to proportionately match choices to outcome                     ducks on a lake (Harper, 1982) and fish in a tank (Godin &
probabilities in repeated binary decisions, a phenomenon                 Keenleyside, 1984).
known as probability matching (for a review see Vulkan,
                                                                     3372

   Our aim was to examine the role of competitive pressures          include assumptions regarding three main components (see
for the facilitation of optimal decision making in simple            e.g., Sutton, 1998): a utility function that specifies the goal
binary human choice contexts. Specifically, we wanted to             of the learning problem; a learning rule which establishes
assess potential benefits of probability matching under the          propensities for each choice option; and a choice rule
premise that competitive conditions reinforce its superiority.       defining the course of action given current propensities.
Following the logic of natural foraging situations, we               Here, we examine two learning models postulating different
designed a choice environment in which each decision                 conceptualizations of the utility formation process.
maker competes against a computerized opponent for the
exploitation of a monetary resource that an indifferent              Utility Function In a learning environment where an
‚Äònature‚Äô repeatedly places at one of two choice options with         agent‚Äôs primary goal is maximization of total payoffs, the
stable probabilities. When both agents converge on the same          utility of a choice is typically directly specified by its
choice, potential rewards are split evenly between them.             associated monetary reward (e.g., Rieskamp & Otto, 2006):
   In this competitive context, the success of any strategy
                                                                          ùë¢ùë° (ùëñ) = ùëüùë° (ùëñ),                                        (1)
largely depends on the behavior of the opponent. Under the
assumption that the competitor is attentive towards the              where ùë¢ùë° (ùëñ) corresponds to the utility of the monetary gains
decision maker‚Äôs choice behavior and imitates her course of          ùëüùë° (ùëñ) associated with choice i on trial t, namely, in our task,
action, probability matching is an optimal strategy. The             0, 2 or 4 cents for no, split and full payoffs (see below). The
prevalence of aggregative behavior in a broad range of               focus on monetary gains for the evaluation of choice utilities
natural group settings, e.g. flocking behavior of birds,             has left systematic investigations of a wider range of factors
shoaling of fish, swarming of insects and herd behavior of           potentially influencing this important model component
land animals (Allee, 1978), suggests that a strategy-                largely unexplored (with few notable exceptions, e.g.,
mirroring opponent creates competitive conditions closely            Janssen & Gray, 2012; Singh, Lewis, & Barto, 2009). This
in line with real life ecological pressures. Thus, in one            is the case even though various additional motivational
experimental condition each opponent‚Äôs choice probabilities          sources of utility are conceivable: e.g. avoidance of
are close imitations of participant behavior which renders           boredom associated with repetitive tasks (Keren &
probability matching optimal (see appendix). In a second             Wagenaar, 1985) or task completion time (Gray, Sims, Fu,
condition (between-subjects), each participant is paired with        & Schoelles, 2006). Relating this prevalent negligence to
a computer opponent who is indifferent towards subjects‚Äô             the competitive task employed here, we argue that
choices thereby making exclusive preference for the more             describing utilities in terms of monetary rewards only
profitable resource (i.e. probability maximizing) the optimal        confounds two discrete learning goals vital in this context,
strategy. This is the case because sporadic choices by the           namely, correctly assessing the profitability of an option and
participant to the lesser option will not tempt this indifferent     attending to the competitor‚Äôs choices. In fact, monetary
opponent to replicate deviant behavior but will merely result        based utilities understate the crucial role differential causes
in relinquished earning potential for the participant.               of opponent behavior play when subjects face an imitative
   By manipulating opponent behavior as described, we                vs. an indifferent competitor. That is to say, different
created two competitive choice environments that differed            opponent strategies necessitate divergent learning goals: if
solely in the extent to which participants had influence on          an opponent is identified as attentive, deciding on a course
their competitors‚Äô behavior. Thus, we can assess the role of         of action requires consideration of ways to influence and
the qualitative nature of competition for the facilitation of        outsmart that other agent; if, on the other hand, the
adaptive decision making. Given the availability of                  competitor is indifferent, the impact of opposing actions on
sufficient feedback (Shanks, Tunney, & McCarthy, 2002),              one‚Äôs own decisions should be strongly discounted.
we predicted that choices will converge on the respective               Incorporating these aspects into the learning model we
optimal strategy in both environments as learning                    propose a utility function that disentangles the two learning
progresses, i.e. probability matching when competing                 goals present in our task and allows direct estimation of the
against a mimicking opponent and probability maximizing              importance decision makers attribute to the choice strategies
when encountering an indifferent opponent.                           they observe in their competitors compared to the
                                                                     importance they ascribe to making accurate choices:
Models of Learning under Competition
                                                                          ùë¢ùë° (ùëñ) = [ùõΩ ‚àô ùëîùë° (ùëñ)] + [(1 ‚àí ùõΩ) ‚àô ùë†ùë° (ùëñ)].             (2)
To shed more light on the nature of underlying learning
processes within such competitive environments, we discuss           Here, the utility ùë¢ùë° (ùëñ) of a choice, is expressed as the
the applicability of reinforcement learning models proposed          weighted sum of its accuracy ùëîùë° (ùëñ) (0 for incorrect and 1 for
for similar choice settings, e.g. learning in experimental           correct guesses) and the choice of the competitor ùë†ùë° (ùëñ) (-1
games (Erev & Roth, 1998), learning in the Iowa Gambling             for converging choices and 1 for incongruent choices) on
task (Yechiam & Busemeyer, 2005) and strategy selection              any given trial. The additional free parameter Œ≤ determines
learning (Rieskamp & Otto, 2006), to our experiments and             the weight a subject assigns to choosing the correct option
outline potential adaptions of such models to account for the        as compared to outsmarting their competitor in terms of
competitive pressures examined here. Such models typically           choosing the opposite line of action. For Œ≤ = 1 subjects
                                                                 3373

value the accuracy of their choices only, whereas for Œ≤ = .5         Design
the importance of correct choices and outwitting the                 We employed a 2 x 5 mixed model design with opponent
competitor are weighted equally. We predict that balancing           type (mimicry or indifferent) as between-subjects factor and
these two requirements of the task would be more                     trial block (five blocks of 100 trials each) as within-subjects
pronounced when facing an imitative competitor, thus                 factor. The dependent measure was the proportion of
MŒ≤,indifferent > MŒ≤,mimicry, and that learning models considering    choices to the more probable choice option. For the mimicry
these differential challenges of the task would account for          group, the choice sequence of each opponent was computed
the data more thoroughly.                                            one step ahead by equating the opponent‚Äôs choice
                                                                     probabilities on each trial with the choice proportions the
Updating and Choice Rule Adjustment of propensities                  subject had displayed during the past ten trials. For example,
follows a delta learning rule commonly employed in similar           when a participant chose the more probable option on 7 out
decision tasks (e.g., Yechiam & Busemeyer, 2005):                    of the past 10 trials, her opponent‚Äôs probability of choosing
    ùëûùë° (ùëñ) = ùëûùë°‚àí1 (ùëñ) + ùõº[ùë¢ùë° (ùëñ) ‚àí ùëûùë°‚àí1 (ùëñ)].                 (3)    the same option on the subsequent trial was .7. 1 This
                                                                     algorithm creates opponent behavior that probabilistically
Here, initial propensities towards both options are assumed          mimics participants‚Äô choices.
to equal zero and are then gradually updated in increments              By contrast, the choice sequence of each opponent for
of the learning rate Œ± based on the prediction error in              subjects in the indifferent condition was computed
brackets. As outcomes are mutually exclusive in our task,            irrespective of participants‚Äô choices. Instead, each subject
propensities for both options are updated simultaneously             played against an opponent whose set of choice
regardless of the actual choice on any given trial. An agent‚Äôs       probabilities simply repeated those of an opponent
probability of choosing either option is determined by these         encountered by another subject in the mimicry condition.
propensities following an exponential ‚Äòsoftmax‚Äô choice rule:
                      ùëí ùúÉ‚àôùëûùë° (ùëñ)                                     Procedure
    ùëùùë° (ùëñ) =                           , ùúÉ = 310‚àôùëê ‚àí 1,       (4)
                ùëí ùúÉ‚àôùëûùë° (ùëó) +ùëí ùúÉ‚àôùëûùë° (ùëñ)                               Subjects were asked to predict which of two light bulbs
where the sensitivity parameter Œ∏ governs the precision with         would illuminate over a series of trials while attempting to
which the preferred option is chosen. If Œ∏ = 0, decisions are        earn as much money as possible. Instructions indicated that
made at random, i.e. ùëùùë° (ùëñ) = ùëùùë° (ùëó) = .5, whereas large             the lighting sequence was random, i.e. no pattern or system
sensitivity parameter values (ùúÉ ‚Üí ‚àû) correspond to strictly          existed which made it possible to correctly predict the
deterministic choices to the option with the higher                  outcome throughout, and that the outcome probabilities of
propensity. Following Yechiam & Ert (2007), an                       both choice options remained constant during the entire
exponential transformation of Œ∏ was employed to allow                experiment. Additionally, participants in both conditions
variation of choice sensitivities between random guessing            were informed that a computerized opponent with learning
(for ùúÉ ‚âà 0) to fully deterministic (for ùúÉ > 700) within              abilities such as their own and no initial information about
narrow bounds of c, which denotes the sensitivity constant           the lighting frequencies was monitoring their choices and
constrained between 0 and 1.                                         adapting to their skill level. On each trial, predictions were
                                                                     made simultaneously by both participant and opponent and
                                     Method                          followed by feedback about the other agent‚Äôs choice and the
                                                                     outcome, i.e. one light bulb lit up.
Participants                                                            Upon completion of every block of 100 trials a self-paced
                                                                     pause interrupted the experiment during which block
Fifty (35 female) undergraduate students from the                    feedback was provided and a short message reminded
University of New South Wales (mean age 18.9, SD = 1.2               participants that the lighting sequence was random. Subjects
years) participated in this experiment in return for course          were told: ‚ÄúIn this game you earned X$. Using an optimal
credit and performance based monetary compensation.                  strategy you could have earned at least Y$.‚Äù, where X
                                                                     represented the actual earnings of that block and Y was
Decision Task                                                        computed by an optimizing algorithm (Shanks et al., 2002).
A standard probability learning paradigm involving repeated          This algorithm was set to probability matching in the
binary decisions with mutually exclusive outcomes over 500           mimicry opponent condition and probability maximizing in
choice trials was employed. Choice alternatives were                 the indifferent opponent condition while taking both agents‚Äô
represented by two light bulbs displayed on a computer               actual predictions during that trial block into account.
screen and programmed to illuminate with probabilities of .7         Additional incentives to improve performance on the
and .3, counterbalanced across participants for left and right       following block were provided by informing participants
choice options. Correct predictions were rewarded with               that reaching optimal performance (¬± three cents) would
4 cents (1 AUD = .95 USD). Choices were made while
competing against a computerized opponent and when both                 1
agents converged on the correct response, the payoff was                  During the first ten trials of the experiment, each opponent
                                                                     randomly adopted one of three possible initial strategies: random
evenly split between them, i.e. each agent received 2 cents.
                                                                     response, probability matching, or probability maximizing.
                                                                 3374

double their payoff, whereas suboptimal performance would            Bayes
                                                                   ùëùH0 = .00) 2, with predictions closer to the respective
result in halved earnings on the subsequent trial block.           optimal response strategy in the last compared to the first
                                                                   block of 100 trials for both groups. In the mimicry
Parameter Estimation and Model Evaluation                          condition, subjects‚Äô choice behavior accurately approached
We estimated parameters for each individual separately             optimal probability matching towards the last trial block
based on the models‚Äô accuracy in predicting the observed           (M = .76), whereas an indifferent competitor elicited
choice sequence one step ahead for each trial. That is, all        decisions more in line with a probability maximizing
models generate trial-by-trial choice probabilities for both       strategy (M = .90). This adaptive divergence of learning
response alternatives on the basis of subjects‚Äô prior              processes is emphasized by a significant main effect of
decisions, their associated payoffs and the respective             competitor type across all trial blocks (F(1, 48) = 11.7,
model‚Äôs parameter values. Employing maximum likelihood                                      Bayes
                                                                   p = .001, ∆ûp2 = .195, ùëùH0 = .03). The competition type by
estimation we searched for the set of parameters that              trial block interaction did not reach statistical significance,
maximized the summed log-likelihood of the predicted               although the Bayesian evidence was ambiguous
choice probabilities across trials given each participant‚Äôs                                                             Bayes
                                                                   (F(2.37, 113.8) = 2.77, p = .058, ∆ûp2 = .055, ùëùH0 = .66).
observed responses with an iterative particle swarm
optimization (Kennedy & Eberhart, 1995). For each
individual, optimization proceeded iteratively with a total of
24 particles, 23 of which started at random positions while
the final particle started at the best parameter combination
from the previous iteration. Optimization terminated once
the model fit did not improve further for at least five
successive iterations. The following parameter bounds
constrained the optimization process: ùõº ‚àà [0,1] for the
learning parameter, ùëê ‚àà [0,1] for the transformed sensitivity
Œ∏, and ùõΩ ‚àà [0,1] for the additional outsmarting parameter.
   The final fit of each learning model was compared to a
baseline statistical model which assumes constant and
statistically independent choice probabilities across trials
(see e.g., Yechiam & Busemeyer, 2005), and hence,
accounts for the data without presuming any learning. The
stationary probability of choosing the more probable option
pooled across all trials (p1, p2 = 1 - p1) is the only free
parameter in this baseline model and, to account for               Figure 1: Mean ¬± standard error proportion of choices to the
divergent model complexities, both learning models are                more probable option averaged across trials and subjects.
evaluated by comparing differences in Bayesian Information
Criterion (BIC; Schwarz, 1978) statistics between learning            Similar adaptive differences in choice behavior were also
and baseline model (see e.g., Yechiam & Busemeyer, 2005).          observed at an individual level, with high proportions of
If a learning model is superior to the statistical baseline        subjects in both conditions adopting the respective adequate
model, i.e. accurately describes how subjects adapt their          rather than suboptimal strategy by the final trial block.
choice behavior over time, positive ‚àÜBIC values result from           In sum, we have demonstrated that subjects are sensitive
this model evaluation.                                             towards their competitors‚Äô decision strategies and modify
                                                                   their choices accordingly. The underlying psychological
                           Results                                 processes that lead to this adaptive divergence in strategy
                                                                   use, however, remain elusive from the behavioral data.
Behavioral Data                                                    Thus, we now turn to a computational modeling analysis to
                                                                   illuminate the determinants of emergent optimal choice
The mean proportion of choices to the more probable choice         behavior within competitive environments more holistically.
option for each block of 100 trials averaged across
participants in the two experimental conditions is displayed       Modeling Data
in Figure 1. For inferential statistics, we conducted Bayesian
analyses in addition to conventional methods of hypothesis         The parameter estimates and ‚àÜBIC values for the proposed
testing to quantify evidence in favor of the null and              learning models are compared in Table 1. The first learning
alternative hypotheses (Wagenmakers, 2007). We assume              model we examined defined decision utilities solely based
equal plausibility for the null and alternative hypotheses a       on their associated monetary payoff and, judging by its
priori and report the posterior probability for the null              2
                          Bayes                                         Mauchly‚Äôs test indicated that the assumption of sphericity had
hypothesis, denoted as ùëùH0 , associated with each effect. A        been violated (œá2(9) = 63.0, p < .001), therefore degrees of freedom
mixed model ANOVA revealed a significant main effect of            were corrected for both conventional and Bayesian analyses using
trial block (F(2.37,113.8) = 27.9, p < .001, ∆ûp2 = .367,           Greenhouse-Geisser estimates of sphericity (Œµ = .593).
                                                               3375

  Table 1: Mean and standard deviations (in parentheses) of parameter estimates and the difference in Bayesian Information
                                    Criterion (ŒîBIC) between statistical baseline and specified model.
                                                 Learning (Œ±)             Sensitivity (c)         Outsmarting (Œ≤)
                                                                                                                            ŒîBIC
                                             mimic   indifferent      mimic indifferent        mimic indifferent
  Monetary utility function                    .07        .02          .16          .29
                                                                                                  -           -          17.8 (31.3)
  ùë¢t(ùëñ) = ùëüt                                  (.21)      (.04)        (.20)        (.29)
  Competition utility function                 .08        .07          .43          .28          .85        .97*
                                                                                                                         18.7 (33.7)
  ùë¢ùë° (ùëñ) = [ùõΩ ‚àô ùëîùë° (ùëñ)] + [(1 ‚àí ùõΩ) ‚àô ùë†ùë° (ùëñ)]  (.21)      (.14)        (.38)        (.22)        (.24)      (.05)
*p < .05
clearly positive average ‚àÜBIC score, accounts considerably              binary prediction tasks, probability matching needed to be
better for the observed choice behavior than the stationary             dismissed as an inferior strategy. By contrast, the presence
baseline model despite greater complexity. However, this                of an imitative opponent necessitates response allocations
basic utility model does not permit differentiation between             proportional to outcome probabilities in order to maximize
the learning processes that lead to divergent choice behavior           payoffs. In this context, we observed an adaptive tendency
in the examined competitive environments, because                       towards probability matching ‚Äì i.e. probability maximizing
estimated individual model parameters did not differ                    was correctly rejected as an inferior strategy.
significantly between experimental groups, although the                    What drives this adaptive divergence of strategy adoption
Bayesian evidence was ambiguous (t(26.0) = 1.21, p = .238,              in these two competitive contexts? Our evaluation of
  Bayes
ùëùH0 = .71 for learning rates and t(42.6) = -1.87, p = .068,             learning models suggests that the observed adaptiveness of
  Bayes                                                                 choice behavior largely resulted from differing learning
ùëùH0 = .51 for sensitivity constants).
                                                                        goals with respect to opponent behavior: imitative
   The second learning model proposed above disentangles
                                                                        competitors require consideration for strategies that
the two learning goals of choosing accurately, yet
                                                                        influence and outsmart these agents, whereas indifferent
outsmarting the competitor by introducing an additional free
                                                                        opponents necessitate disregard for their choices when
parameter, Œ≤. The differential requirements of the two
                                                                        deciding on one‚Äôs own course of action. Thus,
competitive environments are well represented by this
                                                                        conceptualizing opponent behavior as a key factor in the
additional outsmarting parameter, which was significantly
                                                                        evaluation of choice utilities that is traded off against the
smaller in the mimicry condition, indicating a tradeoff
                                                                        desire to choose accurately disentangles these divergent
between betting on the more probable option and deviating
                                                                        requirements while providing a good approximation of
from the opposing choice behavior, compared to the
                                                                        observed behavior. Yet, when modeling individual data, the
indifferent group, where opponent choices were to be
                                                       Bayes            additional outsmarting parameter for each decision maker
disregarded (t(26.4) = -2.44, p = .022, ùëùH0 = .27).                     increased the complexity of the model beyond its
Parameter estimates for learning rate and sensitivity                   explanatory potential as indicated by the ‚àÜBIC score
constant, again, did not differ between conditions, although            comparisons. Omitting the computational representation of
the Bayesian evidence was ambiguous (t(48) = .220,                      opponent behavior from the model, however, resulted in
                 Bayes
p = .827, ùëùH0 = .82 and t(38.6) = 1.65, p = .107,                       parameter estimates that gave little indication of the
  Bayes
ùëùH0 = .59, respectively). Although the more elaborate                   underlying learning processes prompting decision makers to
utility evaluation model sheds light on the processes                   respond adaptively to qualitatively different competitive
underlying the observed divergence in choice behavior, the              pressures. At best, within this simpler model, divergent
added complexity results in ‚àÜBIC statistics not significantly           environmental requirements were somewhat reflected in
better than those of the simpler utility model (t(49) = -.613,          marginally decreased sensitivities for evaluated choice
             Bayes                                                      propensities in the mimicry competitor condition, i.e.
p = .543, ùëùH0 = .88). Thus, despite the conceptual promise
                                                                        adoption of optimal probability matching is explained in
and excellent parameter fit of the more complex model,
                                                                        terms of greater randomness in subject‚Äôs choice behavior.
overall, the simple monetary utility model is to be preferred
                                                                        Attributing the observed adaptiveness of strategy use in both
for its parsimony.
                                                                        contexts to differences in choice rule precision appears,
                                                                        however, conceptually implausible, because under the
                             Discussion                                 influence of an imitative competitor, participants are not less
Qualitatively different competitive pressures in a binary               sensitive towards monetary rewards per se. On the contrary,
prediction task result in adaptively divergent choice                   we suggest that it is the added requisite to outmaneuver the
behavior on aggregate and individual learning levels. Under             opposing agent that fuels optimal matching in this context.
the influence of an indifferent opponent, resources should                 Consequently, to account for core learning processes that
and were found to be exploited without consideration for the            drive adaptive choice behavior within these competitive
other agent‚Äôs preferences, i.e. much like in classic individual         environments, an additional representation of opponent
                                                                  3376

behavior is conceptually essential. To remedy potential          Schwarz, G. (1978). Estimating the Dimension of a Model.
disadvantages of added model complexity an interesting             Annals of Statistics, 6(2), 461‚Äì464.
avenue for future research is to explore the suitability of      Shanks, D. R., Tunney, R. J., & McCarthy, J. D. (2002). A
hierarchical parameter estimation techniques, which may            re-examination of probability matching and rational
highlight the benefits of including an outsmarting parameter       choice. Journal of Behavioral Decision Making, 15(3),
without introducing the downsides of overly complex                233‚Äì250.
models. The take-home message from this study is that
                                                                 Singh, S., Lewis, R., & Barto, A. G. (2009). Where Do
learning to choose under uncertainty can indeed be steered
                                                                   Rewards Come From? In N. Taatgen & H. van Rijn
by competition and thus proceed adaptively in situations
                                                                   (Eds.), Proceedings of the 31st Annual Meeting of the
where probability maximizing or matching is optimal.
                                                                   Cognitive Science Society (pp. 2601‚Äì2606). Austin, TX:
                                                                   Cognitive Science Society.
                   Acknowledgments
                                                                 Sutton, R. S. (1998). Introduction to reinforcement learning.
The authors would like to thank J√∂rg Rieskamp and Lael             Cambridge, Mass: MIT Press.
Schooler for insightful suggestions on developing a model
of competitive reinforcement learning. This research was         Todd, P. M., & Gigerenzer, G. (2007). Environments That
supported by Australian Research Council grants to BRN             Make Us Smart. Current Directions in Psychological
(DP110100797; FT110100151), and a University of New                Science, 16(3), 167‚Äì171.
South Wales International Postgraduate award (to CS).            Vulkan, N. (2000). An Economist's Perspective on
                                                                   Probability Matching. Journal of Economic Surveys,
                        References                                 14(1), 101‚Äì118.
Allee, W. C. (1978). Animal aggregations: A study in             Wagenmakers, E.-J. (2007). A practical solution to the
  general sociology. New York: AMS Press.                          pervasive problems of p values. Psychonomic Bulletin &
                                                                   Review, 14(5), 779-804.
Erev, I., & Roth, A. E. (1998). Predicting How People Play
  Games: Reinforcement Learning in Experimental Games            Yechiam, E., & Busemeyer, J. (2005). Comparison of basic
  with Unique, Mixed Strategy Equilibria. The American             assumptions embedded in learning models for experience-
  Economic Review, 88(4), 848‚Äì881.                                 based decision making. Psychonomic Bulletin & Review,
                                                                   12, 387‚Äì402.
Gallistel, C. R. (1990). The organization of learning (1st
  MIT Press paperback). Cambridge, Mass: MIT Press.              Yechiam, E., & Ert, E. (2007). Evaluating the reliance on
                                                                   past choices in adaptive learning models. Journal of
Gigerenzer, G. (2000). Adaptive thinking. Oxford: Oxford
                                                                   Mathematical Psychology, 51(2), 75‚Äì84.
  University Press.
Godin, J.-G. J., & Keenleyside, M. H. A. (1984). Foraging                                  Appendix
  on patchily distributed prey by a cichlid fish (Teleostei,
  Cichlidae): A test of the ideal free distribution theory.      Expected reward proportions are defined as the weighted
  Animal Behaviour, 32(1), 120‚Äì131.                              average of all possible outcomes resulting from nature‚Äôs
                                                                 move and both agents‚Äô choices. When two decision makers
Gray, W. D., Sims, C. R., Fu, W.-T., & Schoelles, M. J.          follow the same course of action, i.e. one imitates the other,
  (2006). The soft constraints hypothesis: A rational            the choice probabilities of both agents are identical. Given
  analysis approach to resource allocation for interactive       identical choice probabilities, the agents can either converge
  behavior. Psychological Review, 113(3), 461‚Äì482.               or diverge on a choice option, and thus expected rewards
Harper, D. G. C. (1982). Competitive foraging in mallards:       can be broken down into split and full payoffs while their
  ‚ÄúIdeal free‚Äô ducks. Animal Behaviour, 30(2), 575‚Äì584.          sum amounts to the total expected payoff proportion:
Janssen, C. P., & Gray, W. D. (2012). When, What, and               ùëüùëÜùëùùëôùëñùë° = (ùëùùëê (ùêª)2 ‚àô .7 + ùëùùëê (ùêø)2 ‚àô .3)‚ÅÑ2                    (5)
  How Much to Reward in Reinforcement Learning-Based
  Models of Cognition. Cognitive Science, 36(2), 333‚Äì358.           ùëüùêπùë¢ùëôùëô = (ùëùùëê1 (ùêª) ‚àô ùëùùëê2 (ùêø) ‚àô .7) + (ùëùùëê1 (ùêø) ‚àô ùëùùëê2 (ùêª) ‚àô .3) (6)
Kennedy, J., & Eberhart, R. (1995). Particle swarm               For each decision maker, split reward proportions are
  optimization. In IEEE International Conference on              computed as the joint probability of both agents choosing
  Neural Networks Proceedings (Vol. 4, pp. 1942‚Äì1948).           the same option (pc(i)2) weighted by the outcome
Keren, G. B., & Wagenaar, W. A. (1985). On the                   contingencies (here, .7 and .3) and split by two; whereas full
  psychology of playing blackjack: Normative and                 reward proportions can be expressed as the joint probability
  descriptive considerations with implications for decision      of both players choosing different options (pc1(i) ‚àô pc2(j))
  theory. Journal of Experimental Psychology, 114(2), 133‚Äì       weighted by the outcome probabilities. Thus, total expected
  158.                                                           payoffs are maximized when both players probability
                                                                 match. For outcome probabilities of .7 and .3, for example,
Rieskamp, J., & Otto, P. E. (2006). SSL: A Theory of How
                                                                 each player‚Äôs maximal total expected reward proportion
  People Learn to Select Strategies. Journal of
                                                                 equals .395 (compared with .35 for probability maximizing).
  Experimental Psychology: General, 135(2), 207‚Äì236.
                                                             3377

