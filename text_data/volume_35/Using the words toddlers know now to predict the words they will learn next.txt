UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Using the words toddlers know now to predict the words they will learn next
Permalink
https://escholarship.org/uc/item/8sk4t2c0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Beckage, Nicole
Colunga, Eliana
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

       Using the words toddlers know now to predict the words they will learn next
                                     Nicole M. Beckage (Nicole.Beckage@Colorado.edu)
                                       Department of Psychology and Neuroscience, 345 UCB
                                                          Boulder, CO 80309 USA
                                       Eliana Colunga (Eliana.Colunga@Colorado.edu)
                                       Department of Psychology and Neuroscience, 345 UCB
                                                           Boulder, CO 80309 USA
                             Abstract                                    is, we ask: can we use the words a child knows now to
                                                                         predict the words that a child will learn next?
  We set forth to show that lexical connectivity plays a role in
  understanding early word learning. By considering words that
  are learned in temporal proximity to one another to be related,        Measuring the developing lexicon
  we are able to better predict the words next learned by                One well-established way to characterize toddlers’ lexicons
  toddlers. We build conditional probability models based on             is to use vocabulary checklists, such as the MacArthur-Bates
  data from the growing vocabularies of 77 toddlers, followed
  longitudinally for a year. This type of conditional probability
                                                                         Communicative Development Inventory (CDI; Fenson et
  model outperforms the current norms based on baseline                  al., 1994). These parent-reported measures have been shown
  probabilities of learning given age alone. This is a first step to     to be effective in evaluating children’s communicative skills
  capturing the interaction between a child’s productive                 up to 30 months of age (e.g., Thal et al., 1999). The CDI:
  vocabulary and their learning environment in order to                  Words & Sentences Toddler form is a checklist of over 700
  understand what words a child might learn next. We also test           early words that at least 50% of children typically say at 30
  different types of variants of this conditional probability and        months of age. By pooling data over thousands of children,
  find that not only is there information in words that are
  learned in proximity to one another but that it matters how            the CDI provides norms of the percentage of children who
  models integrate this information. The application of this             say each of these words at a given age from 16 to 30 months
  work may provide better cognitive models of acquisition and            of age, month by month. Aside from being shown to be a
  perhaps allow us to detect children at risk for enduring               valid measure of communicative skills for this age group,
  language difficulties earlier and more accurately.                     the CDI has been recently shown to be an effective tool for
   Keywords: word learning, semantics, language acquisition,             sorting toddlers at the low and high end of the acquisition
   co-occurrence, development, longitudinal data, CDI                    distribution into late talkers and typically developing
                                                                         children (Heilmann, et. al, 2005). This might allow us to see
                         Introduction                                    universality in learning but it also masks some of how the
Do children learn words systematically? There is a lot of                process works—the aggregate cannot explain individual
evidence that words are not all learned equally. Perhaps not             differences but models of learning necessarily must.
surprisingly, for example, parents’ vocabulary is related to                The CDI norms can be used to build models of growth.
their children’s vocabulary (e.g., Weizman & Snow, 2001;                 For example, Hills and colleagues (2009) have used CDI
Veen, et al., 2009). That is, the child will learn the words in          norms to build growth models based on networks of words
his or her environment. In addition, some concepts, and                  connected by feature similarity or associative strength.
therefore the words that name them, may be easier to learn               Beckage, Smith and Hills (2011) used the connectivity of
than others. For example, concrete nouns are learned earlier             language within the vocabulary of young learners and
than verbs and adjectives (e.g., Sandhofer, Smith, & Luo,                showed that there are differences in the structure of the
2000; Gentner, 2006). Furthermore, the child may bring                   vocabularies of children at risk for language impairments
some preferences and constraints to the task of word                     and those of typically developing children.
learning. For example, children may become particularly                     Note that these approaches presuppose that there is
interested in dinosaurs or construction equipment or even                information in the relationships between words. If this is the
tea sets (DeLoache, Simcock, & Macari, 2007). That is, in                case, there should be predictive power in looking at the
characterizing the forces that guide word learning, there is             between-word dependencies over time. We do this by
evidence that at least three distinct but not necessarily                exploiting the statistical regularities present in the
mutually exclusive sources of information can come to bear:              developing vocabularies of 77 children, followed
a) the structure and composition of the linguistic                       longitudinally for a year, at monthly intervals.
environment, b) the structure of the concepts and categories
being named, and c) the characteristics of the learner itself.
                                                                         Rationale
In this paper we focus mostly on this third source of                    We propose a simple way of uncovering the interaction
variability by constructing conditional probability models               between the language environment and learning and thus
from longitudinal trajectories of word learning that make                uncovering more of the systematicity of word learning.
predictions at the word level, for individual children. That             Instead of just considering the frequency of production for a
                                                                     163

given word conditioned on the age of a child, as with the                                     Methods
CDI norms, we suggest that there might be additional
information in the structure of the language knowledge              Vocabularies and Co-occurrences
itself, in the set of words that are known. To pursue this
                                                                    We utilize CDI measured vocabularies collected at the
claim, we build the most naïve notion of relatedness and
                                                                    University of Colorado Boulder. Seventy-seven toddlers
leverage this information in order to predict what words a          between 15.7 and 18.6 months (mean starting age 17
child might learn next. We define relatedness to be the             months) were recruited as part of a year-long longitudinal
conditional probability of a word given the child knows             study. These participants completed monthly behavioral
another word. For the sake of this paper, we consider words         tasks as well as vocabulary assessments. The vocabulary
learned in temporal proximity to be related. We build up
                                                                    assessments were conducted through parent report using the
these values from the longitudinally collected CDIs by
                                                                    CDI Words & Sentences toddler forms. These CDIs were
considering a connection between words that are learned at          collected for 12 consecutive months with the majority of
the same time (within the same month). We then compute              parents completing the forms each month. On average we
the conditional probability as follows:                             have 9.8 months for each child.
                                                       (1)              In our study, we include a total of 650 words from the
                                                                    full form, marking duplicate words with parts of speech
 For example, we compute the probability of knowing “cat”
                                                                    (such as “orange” as a noun and “orange” as an adjective).
given “dog” by calculating the probability of a child
                                                                    We included words that were both on the full form and had
learning “cat” and “dog” in the same month, normalized by
                                                                    norms available online (http://www.sci.sdsu.edu/cdi/).
the probability of knowing “dog” in the population as a
                                                                    Words that were not part of our modeling included words
whole. We can then use a variety of methods to combine
                                                                    like above, after, on and off. All together we have 77
these conditional probabilities into a single probability of
                                                                    children and a total of 684 CDI forms. For the sake of this
learning word i given that they know a set of words J. That
                                                                    paper we consider each month to be independent of every
is, for each not-yet-known word, we can calculate the
                                                                    other month. That is, we build associative structure only
probability that the child will learn that word next, given the
                                                                    from words that are learned during the same month (or
set of words the child already knows.
                                                                    words that are known at the beginning of the study.) This
   In order to combine the conditional probabilities given the
                                                                    limits the co-occurrence measure to capture only short-term
set of known words, we need to integrate over the
                                                                    dependencies. In the future we plan to extend this work to
conditional probability given each of the words known.
                                                                    include cross-sectional vocabularies as well, which will
Here we test three different models of this: the Additive
                                                                    allow us to capture long-term dependencies.
model assumes that every conditional probability
                                                                        To derive the strength of connectivity, we simply take a
contributes equally. In the additive model we simply sum up
                                                                    count of the number of times two words appear in the same
the conditional probability of i given every j in the set of
                                                                    vocabulary (i.e. are learned in the same month) normalized
known words. This gives us a proportion of learning for
                                                                    by the population level knowledge as measured from our
every word not yet learned. The issue with this model is that
                                                                    sample for the words. This provides the basic counts that are
it requires a large amount of information and storage. One
                                                                    then used to compute an ‘activation level’ that will then give
rudimentary simplification would be to assume that only the
                                                                    rise to predictions of the next word learned. We then
maximum conditional probability was used. This model we
                                                                    calculate the probability of learning word i given that a child
call the Maximal model because we use only the strongest
                                                                    already knows word j. This is then compared to the models
conditional probability between i and some j from the set of
                                                                    based only on population level data as well as a model that
known words J. Finally in the Threshold model we
                                                                    assumes uniform learning.
compare a model that considers conditional probabilities in
an additive fashion but considers links only as present or
                                                                    Models
absent. The link is determined as present when the
conditional probability strength is above a certain threshold       We compute two population-based measures. The first
(in our case the median of all conditional probability values)      normed model is based on the CDI norms where we
and absent otherwise. We compare these conditional                  consider the likelihood of a child learning the specific set of
probability models to two population-based models, one              words we observe to be a function of the population level
based on the CDI norms (norm-baseline), another based on            age of acquisition (AoA) norms (Dale & Fenson 1996). The
the observed frequencies (observed-baseline), as well as            second measure is calculated analogously, but computing
the null model (the assumption that all words will be               the likelihood according to the AoA as observed in our own
learned with equal chance). We evaluate the conditional             sample. We also compare these and all other models to a
probability models by comparing their predictive power to           straw-man baseline measure (the null model) that gives
the population-based models, and use 5-fold cross-                  every unlearned word equal probability of being learned.
validation to evaluate the model’s performance in predicting           We compare these population level models to conditional
untrained trajectories.                                             probability models. For the additive model we calculate the
                                                                    probability that each word is learned as proportional to the
                                                                    sum of the conditional probability of all known words.
                                                                164

                                                          (2)        Evaluation
                                                                     We begin by looking at the percent of vocabularies better fit
In effect, the probability of learning word a is computed            than the null model and the percent likelihood improvement.
such that we sum across the conditional probability of each          This tells us some information about the general variability
known word b in the set of known words B. For example if             of the input and the ability of the model to account for this
a child knows words “cat” and “dog” then the probability             variability but little about which model is best. Thus we
that the child will learn word “pet” is proportional to the          consider the percent of vocabularies that are better than each
sum of the probability of learning “pet” given “cat” plus the        of the population based models. This tells us the proportion
probability of learning “pet” given “dog”. This assumes a            of vocabularies better fit by the model but not how much
level of independence that does not exist in language itself.        better (or worse) of a fit the models give us for our sample.
   Using similar methods, in the maximal model we                    Thus we include the total likelihood of the test data given
consider only the maximum conditional probability to be              each of the models. We then compare the likelihood fits
proportional to the probability of learning a given word. We         across models in order to understand how a model is
test this model because it requires only one point of                performing in comparison to other models. We also look at
information per word as opposed to considering all possible          the percent of vocabularies best fit by a given model.
combinations of known and unknown words.                                This gives us a good deal of information about the
                                                          (3)        performance of the models and the ability of the models to
                                                                     utilize and combine information in order to predict words
This simplification may still capture much of the variance if        learned next at the level of an individual child’s vocabulary.
the maximal connection dominates the additive model or if            To be sure that we are capturing actual signal we want to
strong connections in the learning environment really do             calculate the conditional probabilities based on a different
highlight words to be learned next.                                  set of vocabularies than those on which we test the models.
   We also consider thresholded conditional probability. The         Thus, we use cross validation and iteratively build up the
idea here is that the learner has access to most of the              necessary associations that the models require from 80% of
conditional probability space but only at a coarse level. The        the vocabularies and then test on the remaining 20%. We do
learner is considered to maintain only the strongest                 this at the child level since sequential vocabularies are not
conditional probabilities and that these are considered as           independent of the child (an issue we’ve ignored up to this
present or not. Mathematically, the threshold model is:              point). We randomly select the 20% test group and repeat
                                                                     this 5 times such that every observed set of vocabularies for
                                                          (4)
                                                                     a given child is in the test set once. This allows us to test
                                                                     how well the model can predict the vocabulary growth of a
Here I is the indicator function and is valued only when the
                                                                     child it has no direct information about. We compare the
conditional probability is greater than some constant c. For
                                                                     average performance on the five different test sets.
this analysis we let c be the median conditional probability
across all children. This adds an additional variable to our
model but it is set a priori and thus has little significance on
                                                                                                 Results
the complexity of the model. From an information                     We know what specific words a child learned in a given
processing point of view, this model may take less effort            month, and we use our model to calculate the probability of
since we consider only the presence and absence of a link            a given set of words. Some models give a zero probability to
and not the weight, reducing the complexity of the space.            learning certain words and thus we first want to look at what
   While we do not consider it here, even these very simple          percentage of our population cannot be fit by a specified
models can quickly be extended to other types of more                model. This will give us information about how constrained
sophisticated models. First, conditional probability is at best      the models are in their ability to fit the wide range of data
a first order approximation to the full complexity that              present in our sample. Column 1 of Table 1 shows the
language embodies. The model can only be as good as the              results. In general the models are able to capture the learned
measure used to inform it which in this case is simple co-           words fairly well. The worst model is the Threshold model
occurrences. Further, these co-occurrences are based on one          which is due to the fact that many words are assigned a zero
month time slots, we could consider data at other time               probability under this model since connections that are not
scales, or multiple timescales. Finally, we have chosen to           above the median strength are considered absent—this
integrate the conditional probability information here in the        results in about 4% of the observed vocabularies not being
simplest ways. These models can be seen as network models            explainable under the strictest definition of the model. The
which allows us to consider not only what words are                  model based on the CDI norms also has some difficulty
predicted to be learned next but how these mechanisms for            accounting for some (2%) of the vocabularies seen. In
learning transform the semantic structure present in the             practice, this means that some children learned words earlier
network. We mention this to highlight the implications of            than the normed CDI measures would have predicted—that
testing these basic assumptions on the larger word learning          is, in the vocabularies used to build up the norms, there were
models (e.g., Vitevitch, 2008; Hills et al, 2009).                   no children in the sample that learned some words that
                                                                     children in our study did. This is even more extreme for our
                                                                 165

observed CDI norms—which is probably an effect of                    threshold model outperforms all others, as shown in Table
sample size. In general a large majority of the 684                  2. This improvement is non-trivial as it accounts for the best
vocabularies could be fit under the models we constructed; a         model in over 50% of vocabularies. This is maintained
total of 633 overlapped across all models. We constrain our          when we look across children as well—most children are
model comparisons to this subset in all further evaluations.         best fit by the threshold model. The observed norm model
   The next major question is whether or not the constructed         does provide the best fit for 22% of the data suggesting that
models outperform the null model in which each word is               there is some predictive powers in the population level rate
given equal probability of being learned. The answer, in             and time of acquisition. When we look across the population
short, is that all models perform better than the null model         level models we see that over 70% of vocabularies are better
when we consider the total likelihood across all                     fit by a conditional probability model than by a population
vocabularies. Further, the minimum number of vocabularies            level normed model. Critically, this suggests that there is
better fit by our models than the null model was 82%. This           some added information in conditional probabilities.
suggests that there is systematicity to the order in which
children learn words. In fact we don’t only fit the observed          Table 2: Performance with cross-validation. Overall ability
data better we get a fairly substantial improvement in                  to account for the data as well as percent of vocabularies
overall likelihood when we utilize these models. We have at               best fit by a given model. For comparison the model
last 8% improvement and at most 19% improvement.                        performance is directly compared to population models.
  Table 1: Model performance compared to null model. We                      Model         % vocabs      % better      % better
  consider % of vocabularies not fit, improvement over null                                  best fit    normed        observed
      and % of vocabularies better fit by a given model.                     Normed        7.28                         30.82
                                                                             Observed      22.04           69.17
Model         vocabs not    improvement          vocabs. better              Additive      10.23           75.20        54.29
              fit (%)       over null (% llk)    than null (%)               Maximal       10.31           25.63        17.67
Normed        2.37          14.54                81.99                       Threshold     50.11           77.38        60.52
Observed      3.97          19.04                90.52
Additive      0             18.22                89.10                  To show the extent of improvement offered by
Maximal       0             8.39                 86.41               conditional probabilities, we consider the percent of
Threshold     4.05          18.66                82.94               vocabularies better fit by a given model and the CDI data. In
                                                                     Table 2, column 2 and 3, we see that most of the models
   However, showing that words are not acquired randomly             perform much better than the normed model with roughly
does not answer the question of how individual children              75% of children being better fit under a given model than
build up a vocabulary. Returning to the ideas from the               the published norms and further that many vocabularies are
introduction, this does not rule out the effect of the structure     better fit when compared to the norms based on our
of the environment. Children learning words proportional to          particular population of toddlers in boulder. This suggests
the frequency they encounter them in the environment could           that the norms may be predictive for some children but that
explain these results. This would maintain independence              in general accounting for the words that are learned
between the words a child knows and the words the child is           previously as well as the relationship of words that are
going to learn next. The two baseline models maintain this           learned together may help us predict what word a child will
independence as well: the model based on the normed CDIs             learn next. Further, the way we combine the type of joint
and the model fitting to the observed CDIs. In contrast, the         information about word learning may influence our ability
other models assume conditional probability plays a role in          to capture vocabulary growth.
prediction of vocabulary growth and uses this to link known
words to what words will be learned next. Thus, to get at                                      Discussion
our original question we want to compare these population            These results suggest that conditional probabilities do aid in
level models to the other models that require conditional            accounting for word learning trajectories. That is, the words
information. We already have a bit of information about the          that a child already knows can help predict the words that
overall model performance when we look at the total                  they are going to learn in the future. This implies that there
likelihood across all vocabularies. We see that we get the           is some sort of systematicity in word learning and that it is
largest improvement in likelihood when we utilize the                not explainable by structure of the environment alone or by
observed CDIs. And we also see that this model gives us the          conceptual complexity but rather by the interaction of the
most vocabularies that are fit better than random acquisition.       structure of concepts and meaning within the knowledge of
   The gains resulting from using conditional probability are        the individual child. The two models that are based on
clearer when we consider which vocabularies were best fit            normed data can be seen as independent of individual
rather than looking at the overall likelihood which could be         variation. That is, for these models to perform well at
easily inflated by isolated vocabularies that are particularly       predicting what words a child will learn next, children
difficult for a given model to fit. With cross-validation, the       across a variety of settings and in a variety of learning
                                                                 166

environments would be expected to learn words in similar            weights and summing up all of the conditional probabilities
proportion and at a similar rate. This could suggest that the       between the word candidate and all known words. This
input is structured in a systematic way or that the learning        resulted in a model that was able to fit much of the data and
strategy is the same across all children and not dependent on       often better than the population level models. But this was
the child’s productive vocabulary at any point in time. We          not the best fitting model suggesting that this model might
did see that these models in general can be fairly predictive       have required too much information, accumulating a ratio
of word learning and in fact the total likelihood of the data       that included significant noise in addition to the signal. A
was minimized under the model that built norms from CDIs            huge simplifying assumption that led to our next model was
collected in our lab. This suggests that these models capture       one that suggested that children would maintain only the
some important aspect of learning. However if we are                strongest relationship between a word candidate and known
interested in understanding the different styles of how             words. This model performed poorly—returning a total
children learn and capturing the variability across children,       likelihood significantly worse than the normed models and
these models, inherently, cannot help us with these types of        the closest to the null model. However, the children’s
questions as they average out variability.                          vocabularies that were better fit by this model than the CDI
   Looking more closely at the overall likelihood of the            norm models were vocabularies that were often best fit
models, we see a strong trend that the population models are        overall by this model. The best model is the model that
not as able to adapt to new data. When we fit the models on         forces a threshold on the conditional probability matrix.
the full data (that is we included the test set in the training     This suggests that strong connections may be the important
set) the observed CDI norms had a much better total                 ones and that the weight of the connection is not important
likelihood. However, this model took a big hit in the cross-        just that it is present.
validation method (results not shown in this paper)                    We do not only gain insight from looking at what models
suggesting that the observed CDI norms may have overfit             succeed but also what models failed and how. The CDI
the data. The fact that conditional probability models              norming data had difficulty capturing individual
performed better than the population level models in                vocabularies. It is important to note that in some way this
predicting unseen data suggests that the whole story is not in      model was handicapped from the beginning. None of the
the input alone, but that there is an interaction between a         observed data was used in building up the norms. On the
child’s productive vocabulary and what words the child will         other hand, the frequencies noted in the norms were accrued
learn next. Even with very simple models of conditional             over thousands of children, as opposed to our much smaller
probability we were able to increase our ability to predict         sample. Nonetheless, even when the other models were
and account for the ways vocabularies expand. Thus, if we           handicapped in the same fashion, the discrepancy in
were to refine our models to include other types of                 performance still exists. This highlights one of the major
relationships (or more meaningful semantic relationships)           weaknesses in utilizing normed data in order to help predict
between known words and words learned we might be able              future vocabulary progression. First, it fails to exploit the
to understand how children take in their language                   temporal dependencies available when using longitudinal
environment and combine this with their individual                  data. Second, it fails to utilize the dependencies between
vocabulary knowledge to learn new words. The work                   subsets of words. Of course the poor performance of the
presented here only begins to look at this by testing models        norm baseline could be due to a variety of other reasons
that combine the relationship of co-learned words in                which would plague any attempt to characterize universals
different ways, but refinement on these types of models             from individual data, and which pose problems to the
could provide a way for us to uncover not just how children         traditional norming studies. For example, geographic
learn new words but also how they integrate a variety of            changes between where the norms are collected and
information in order to develop representations of the world.       Boulder, CO, where our vocabularies were collected could
For example, here we considered the median and as our               produce variation thus restricting the generalizability of the
cutoff in the threshold models, but in theory this could be a       norms. Or there could be cohort differences due to the fact
free parameter fit at the level of individual children (or at       that the world in which our current children are growing up
the population level conditioned on age) and could hold             has a different underlying structure in small but significant
added information about how children interact with the              ways than the world of the children who contributed data to
learning environment. It is true that this threshold model has      the norms 20 years ago. This suggests a need for us to
an additional variable but by setting this before looking at        consider other tools and methods in order to build up a
the data we have dealt with any issues in comparing this            robust and predictive measure of infant word learning.
model to the other models. In the future we plan to do more
extensive parameter fits as well as extend the basic models                 Conclusions and Further Directions
in complexity. For example, we would like to allow the              Altogether, our findings demonstrate that the conditional
number of maximal values included in our maximal model              probabilities contain information that captures the
to be n instead of just 1, where n is a free parameter itself.      relationship between the words known by a child at a
   The first model (the additive model) tested combined             specific time point and the words that child will learn next.
conditional probabilities by maintaining connections and            Further, our results show that it matters how we integrate
                                                                167

these probabilities. For example, the maximal model is                                     References
utilizing only minimal information from the conditional
                                                                   Beckage, N., Smith, L., & Hills, T. (2011). Small worlds
probability (the strongest conditional probability between
                                                                     and semantic network growth in typical and late talkers.
known and candidate words only) and this model performs
                                                                     PloS One, 6(5), e19348.
very poorly. This suggests that, even though conditional
                                                                   Dale, P.S., & Fenson, L. (1996). Lexical development
probabilities do contain useful information, not every use of
                                                                     norms for young children. Behavior Research Methods,
it improves predictive power. The fact that the threshold
                                                                     Instruments, & Computers, 28, 125-127.
model does best, suggests that understanding how to
                                                                   DeLoache, J.S., Simcock, G., & Macari, S. (2007). Planes,
combine information can increase fit of the model and allow
                                                                     Trains, Automobiles —and Tea Sets: Extremely Intense
us to make more accurate future predictions. Interestingly,
                                                                     Interests in Very Young Children. Developmental
the model that integrated over the complete conditional
                                                                     Psychology, 43(6), 1576–1586.
probability matrix did not perform better than the model
                                                                   Fenson, L., Dale, P. S., Reznick, J. S., Bates, E., Thal, D. J.,
with less information. This result is not atypical for the
                                                                     Pethick, S. J., & Stiles, J. (1994). Variability in early
world of child language acquisition and suggests that
                                                                     communicative development.Monographs of the society
perhaps taking into account memory or other cognitive
                                                                     for research in child development.
constraints may be useful, if not necessary, in capturing
                                                                   Gentner, D. (2006). Why verbs are hard to learn. In K.
early learning (e.g. Phillips & Pearl, 2012).
                                                                     Hirsh-Pasek, & R. Golinkoff, (Eds.) Action meets word:
   This work offers evidence that word learning is affected
                                                                     How children learn verbs (pp. 544– 564). Oxford
by a combination of forces and understanding these forces
                                                                     University Press.
may allow us to predict words that a child would be likely to
                                                                   Heilmann, J., Ellis Weismer, S., Evans, J., & Hollar, C.
learn next. We would like to extend these results.
                                                                     (2005). Utility of the MacArthur–Bates Communicative
Specifically we would like to more closely examine what
                                                                     Inventory in identifying language abilities of late talking
types of relationships might exist and ways to measure
                                                                     and typically developing toddlers. American Journal of
them. If we understand the language environment where a
                                                                     Speech-Language Pathology, 14, 40–51.
child is learning as well as the way in which the child might
                                                                   Hills, T., Maouene, M., Maouene, J., Sheya, A., & Smith, L.
be integrating this information with their current vocabulary
                                                                     (2009). Longitudinal analysis of early semantic networks:
we should be able to predict which words a child may learn
                                                                     Preferential attachment or preferential acquisition?
next. This matters because this may allow us to capture
                                                                     Psychological Science, 20,
children who have learning strategies leading to language
                                                                   Phillips, L., & Pearl, L. (2012). “Less is More” in Bayesian
difficulty or impairment. These types of models could let us
                                                                     word segmentation: When cognitively plausible learners
diagnose such children earlier and may allow us to provide
                                                                     outperform the ideal. In Proceedings of the 34th Annual
effective and child specific interventions.
                                                                     Conference of the Cognitive Science Society.
   Another potential direction is the development of tools
                                                                   Sandhofer, C.M., Smith, L.B., & Luo, J. (2000). Counting
and techniques that allow us to understand temporal
                                                                     nouns and verbs in the input: differential frequencies,
dependencies at different time scales other than a month.
                                                                     different kinds of learning? J. Child Lang., 27, 561– 585.
Time series analysis combined with graph clustering on the
                                                                   Thal, D. J., O'Hanlon, L., Clemmons, M., & Fralin, L.
semantics may allow us to expand this work from joint
                                                                     (1999). Validity of a parent report measure of vocabulary
probability to a more complex probability space giving us
                                                                     and syntax for preschool children with language
better temporal resolution as well as more predictive
                                                                     impairment. Journal of Speech, Language and Hearing
models. Along those same lines, we may be able to fine-
                                                                     Research, 42(2), 482.
tune these models with cognitive theory (which are not
                                                                   van Veen, R., Evers-Vermeul, J., Sanders, T.J.M. & Bergh,
included at all in these models, see Hills et al, 2009 for a
                                                                     H. van den (2009). Parental input and connective
paper that does consider this) to test generative and process
                                                                     acquisition: a growth curve analysis. First Language,
motivated theories of word learning. This would allow us
                                                                     29(3), 267– 289.
not only to build new computational tools but to refine and
                                                                   Vitevitch, M.S. (2008). What can graph theory tell us about
expound upon theories of word learning.
                                                                     word learning and lexical retrieval? Journal of Speech,
   At the onset of this paper we asked whether it would be
                                                                     Language, and Hearing Research, 51, 408–422
possible to predict the words a child will learn next from the
                                                                   Weizman, Z. O. & Snow, C. E. (2001). Lexical output as
words she knows now. Our findings, even with this simple
                                                                     related to children’s vocabulary acquisition: Effects of
set of models, suggest that the answer to that question is
                                                                     sophisticated exposure and support for meaning.
yes. Significantly, this opens up doors that have far-reaching
                                                                     Developmental Psychology, 37, 265– 279.
implications. If we understand how children utilize their
environment, conceptual understanding and semantic
connectivity as they interact with the world and build up a
vocabulary, we can design individualized teaching
paradigms that may allow us to build upon, or compliment,
what the child already knows aiding in language acquisition.
                                                               168

