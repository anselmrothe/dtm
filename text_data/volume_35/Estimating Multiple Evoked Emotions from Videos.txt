UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Estimating Multiple Evoked Emotions from Videos
Permalink
https://escholarship.org/uc/item/40p7b8j3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Choe, Wonhee
Chun, Hyo-Sun
Noh, Junhyug
et al.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                            Estimating Multiple Evoked Emotions from Videos
                                           Wonhee Choe (wonheechoe@gmail.com)
                                       Cognitive Science Program, Seoul National University,
                                                   Seoul 151-744, Republic of Korea
                            Digital Media & Communication (DMC) R&D Center, Samsung Electronics
                                                  Suwon 443-742, Republic of Korea
                                             Hyo-Sun Chun (hschun@bi.snu.ac.kr)
                                              Junhyug Noh (jhroh86@gmail.com)
                              School of Computer Science and Engineering, Seoul National University,
                                                   Seoul 151-744, Republic of Korea
                                            Seong-Deok Lee (lsdlee@samsung.com)
                                   Future IT, Samsung Advanced Institute of Technology (SAIT),
                                                  Yongin 449-712, Republic of Korea
                                         Byoung-Tak Zhang (btzhang@bi.snu.ac.kr)
                       Computer Science and Engineering & Cognitive Science and Brain Science Programs,
                                    Seoul National University, Seoul, 151-744, Republic of Korea
                             Abstract                                   Until now, most of the research efforts have focused on a
                                                                     content- or genre-based analysis of videos despite many
   Video-sharing websites have begun to provide easy access to
   user-generated video content. How do we find what we want         users' needs for emotion-sensitive video retrieval.
   to view among the huge video database? When people search         Fortunately, some have studied the emotions evoked by
   for a video, they may want to know whether the video evokes       images (Wang & He, 2008; Yanulevskaya et al., 2008; Li,
   a certain emotional sensation. The evoked emotion is one of       Zhang & Tan, 2010). Wang and He (2008) focused on
   the important factors we consider when we select a video.         emotional semantic image retrieval (ESIR) instead of
   One of the key concepts of evoked emotions from videos: the       content-based image retrieval (CBIR) to reduce the
   evoked emotions are different for each scene and for each
   viewer. Considering these differences, we obtained human-
                                                                     “semantic gap.” Others studied emotional picture
   evoked emotions from 33 videos. We used these emotions to         categorization using the International Affective Picture
   estimate the multiple emotions evoked by each scene of the        System (IAPS) according to the 10-emotion model
   videos. Using a computational model of emotion estimation         (Yanulevskaya et al., 2008; Li, Zhang & Tan, 2010).
   based on mid-level visual features, we found that, in             However, the application of these approaches to a video
   individual videos, the same scene evoked multiple emotions.       sequence may not be appropriate due to the lack of
   Our results show that a video evoked different emotions from      consideration of temporal variations. Various moods change
   different people. A computational model might deliver
   probabilistic multiple-evoked emotions from video analyses.       sequentially within a given video. That is, within a given
                                                                     video, different emotions may be evoked across scenes.
   Keywords: evoked emotion; visual feature; video retrieval            Two different studies evaluated the emotions evoked by
                                                                     videos. Canini et al. (2009) evaluated the emotional identity
                          Introduction                               of videos. The research used light source color, motion
Video-sharing websites provide easy access to a wide                 dynamics, and audio track energy as the temporal features
variety of user-generated video content, including movie             of videos. It was a first attempt to evaluate the temporally
clips, television clips, music videos, and amateur content.          changing emotional identity of movies. They used a 3
We can search a huge database of videos for a video that we          dimensional (3D) emotional identity space (warm/cold,
want to see via the use of keywords or a search by genre.            dynamic/slow, and energetic/minimal) to show the
Unfortunately, these search strategies are not sufficient. If        trajectory of one video clip. Unfortunately, the emotional
we do not have any prior knowledge of a video, how do we             identity space was used to express movies’ content changes
find what we want to see? The mood of movie is one of the            rather than humans’ various emotional changes. Bailenson
most important factors we consider when we select a movie.           et al. (2008) focused on a classification algorithm of
When people search for a movie or TV series, they may                emotions evoked by videos. Facial feature tracking was used
want to know whether the video has a mood that is similar            and physiological responses were measured. The study tried
to that of a video they have viewed before. Sometimes, they          to predict 2 emotions (amusement and sadness) and the
may want to change their mood by watching a video.                   intensity of each emotion. Unfortunately, this approach is
                                                                     not applicable to the study of video retrieval.
                                                                2046

  Winter and Kuiper (1997) reported that individual                We designed a 12-emotion model that includes Gross and
difference factors play an important role in the experience of   Levenson’s (1995) 8-emotion model with 4 emotions
emotion. In fact, individuals may respond differently            (humor, romance, tension, and suspicion). Our emotional
depending on their current state of mind. However, most          space is balanced in that there are 4 positive emotions
research of the emotion of videos has assumed that an            (humor, romance, contentment, and joy/pleasure), 4 blended
emotion is unified at any given moment.                          emotions (suspicion, tension, surprise, and neutral), and 4
  In the present paper, we propose a new temporally              negative emotions (anger, disgust, sadness, and fear).
changed emotional analyzer that functions as a probabilistic
estimator of multiple emotions evoked by videos. The goal        Procedure
of this study is to generate sequentially changing emotional     20 people participated in the experiment. At the beginning
responses from a video clip. This paper is organized as          of the test, the 12 emotions were described (Fig. 1) to the
follows: First, a psychophysical experiment to investigate       participants. The video clips were shown to the participants
the evoked emotions by each scene is described. Second, the      on a TV monitor. The presenting order of videos was
proposed system is introduced with mid-level visual              randomized to minimize potential error. After watching
features, an estimation model, and a performance test.           each video clip, the participants were asked to choose 1 of
Finally, the conclusions are summarized and future tasks are     the 12 emotions that best represented the emotion evoked by
proposed.                                                        the video clip.
                         Method
Data Set
To investigate the evoked emotion of each video clip,
stimuli were taken from following movies and TV series:
Amélie of Montmartre (2001), Artificial Intelligence: AI
(2001), Curse of the Golden Flower (2006), The Amazing
Spider-Man (2012), Wuthering Heights (2012), Friends
Season8: The One Where Rachel Tells Ross (2001), and CSI:
Miami Season8 Episode4: In Plane Sight (2009). The
movies and TV series were selected non-intentionally. Each
was decomposed to a set of video scenes, the emotional
valence of which varied. Thirty-three video clips were
selected in order to not exceed one hour per experiment per
person. The average run time of the clips was 81 seconds.                      Figure 1 : The names and illustrations of the 12 emotions
Each video clip had one main event that occurred in one                                        used in this experiment.
location. We preferred that each video clip evoke one kind
of emotion.
                                                                                           12                                       Most frequently
                                                                                                                                    reported
                                                                    The number of movies
Evoked Emotions                                                                            10
                                                                                                                                    Second most
To examine the emotions evoked, we used emotional                                          8
                                                                                                                                    frequently reported
models instead of Canini’s 3D emotional identity space                                     6
(Canini et al., 2009). Ekman’s model is defined by 6 basic                                 4
facial emotional expressions: anger, surprise, disgust,                                    2
sadness, happiness, and fear (Ekman & Friesen, 1978).
                                                                                           0
However, this model may not be applicable for video-
evoked emotions because it contains more negative
emotions than positive ones: it has 4 negative emotions
(anger, disgust, sadness, and fear), one neutral emotion
(surprise) and one positive emotion (happiness). Thus, the                                      Figure 2 : Emotions evoked by the 33 video clips.
model needs to be balanced. Moreover, the model did not
include some key emotions that viewers experience while
watching videos. Gross and Levenson (1995) modified the
6-emotion model to an 8-emotion model. The 8-emotion
model comprises the following emotions: amusement, anger,
contentment, disgust, fear, neutral, sadness and surprise.
Unfortunately, this model is also not balanced.
                                                             2047

                            Results                                         Estimation Model of Evoked Emotions
Responses of Evoked Emotion
The participants’ responses are summarized by a 12-
emotion population for each video clip. The results of the
evoked emotional responses for the 33 video clips are
illustrated in Figure 2. The figure shows the frequency of
the most commonly reported and the second most
commonly reported emotion per video clip. Some emotions
were not reported often; this may generate some noise in our
model. Gross and Levenson (1995) reported that
contentment, anger, and fear are more difficult to be evoked              Figure 3 : A block diagram of our proposed estimation
by movies than are other emotions. Notably, anger was not                                          system.
evoked by the video clips.
                                                                      Do computers mimic human emotions such as the emotions
Statistical Analysis                                                  evoked by the video clips? To answer this question, we
To determine whether the response data varied according to            proposed a movie-evoked emotion estimator by introducing
the video clip, we analyzed the data statistically using SPSS         mid-level visual features. The mid-level visual features were
1.9. First, we tested independence of the participants’               composed by referring to color emotional theory with a 3-
responses for all of the test video clips, through a cross-           color combination and emotionally correlated general visual
tabulation analysis using chi-square tests. The relationship          features: contrast by histogram distribution, relative
between the evoked emotion and the video clip presented               brightness, and motion size. To create the evoked-emotion
was significant (p < 0.001).                                          estimator, we examined the relationship between humans’
   Second, the independence of participants’ evoked                   psychophysical responses and the mid-level visual features
emotions was evaluated in the several extracted video clips           extracted from the video clips. The emotion model was
from the same movie or TV series. The video clips were                trained by supervised learning with humans’ psychophysical
composed of 3 clips of AI, 5 clips of CSI, 4 clips of                 responses and their features. The proposed approach
Wuthering Heights, 4 clips of Amelie, 4 clips of Friends,             generated multiple emotional states of the video contents
and 12 clips of Spider-Man. The evoked emotion was                    sequentially and is illustrated in Figure 3.
significantly related to the video clip (p < 0.001). The
results provided in Tables 1 and 2 are examples of our                Visual Feature Extraction
statistical analysis applied to the data collected in response        Psychologists have investigated the emotion-eliciting
to the 3 clips of AI. While the AI clips evoked all of the 12         properties of industrial media (Gross & Levenson, 1995;
emotions, the other clips evoked as 3 or 4 emotions.                  Pos & Armytage, 2007; Kobayashi, 1981). In particular,
                                                                      Gross and Levenson (1995) focused on eliciting the emotion
 Table 1 : The results of the cross-tabulation analysis for the       of films. They determined that each film could evoke 8
                          AI video clips.                             different emotions from the viewers. The results show that
                                                                      video clips can be categorized by the different emotions that
                                Emotion
  Count                                                         Total they evoke from the viewers. However, they did not attempt
          Hum Rom Con Joy Sur Sus Ten Neu Ang Dis Sad Fea             to draw relationships between the visual cues and the
  AI_1     0    0 0 1 0 2 13 3 0 1 0 0 20 elicited emotions. The present study focused on some mid-
  AI_2     1    0 0 0 3 0 0 0 3 9 2 2 20 level visual cues and evoked emotions from some video
  AI_3     0    4 9 3 0 0 0 0 0 0 4 0 20 clips. The mid-level visual cues are reorganized by an
  Total    1    4 9 4 3 2 13 3 3 10 6 2 60 analysis of low-level features. Moreover, the mid-level cues
                                                                      were differentiated from low-level cues. The mid-level cues
    Table 2 : The results of the chi-square test for the data         were extracted by color, contrast, brightness, and motion.
           collected in response to the AI video clips.
                                                                      Color Color is known to correlate strongly with
                                                    Asymp. Sig.       psychological constructs. Many studies describe the
        Chi-Square Tests          Value      df                       relationships between these variables (Pos & Armytage,
                                                      (2-sided)
                                                                      2007; Kobayashi, 1981; Solli & Lenz, 2010), but to date,
      Pearson Chi-Square         102.100     22         .000
                                                                      correlations in movie settings have not been studied. Pos
        Likelihood Ratio         113.195     22         .000          and Armytage (2007) investigated the relationships between
 Linear-by-Linear Association     6.362       1         .012          emotions, facial expressions, and colors. Kobayashi (1981)
        N of Valid Cases           60                                 matched 1170 3-color combinations to 180 adjective words
                                                                      describing emotional appearance. Solli and Lenz (2010)
                                                                  2048

transformed and classified natural images on the basis of
Kobayashi’s list of emotional words. Kobayashi’s color
scale is useful for industrial design. Unfortunately, it is not
appropriate to apply the 180 emotional words (i.e, warm,
cold, luxury, etc.) to movies. Pos and Armytage (2007)
studied the relationships between 3-color combinations and
Ekman’s 6-emotional facial expressions. The 3-color
combination might connote simple feelings (e.g., warm,
cold) and emotions (e.g., happy, sad).
   We extracted the color distribution of a video clip to
estimate evoked emotions. We transformed the colors of the             Figure 5 : Contrast classification method (Kim, Choe, &
input video clips into Kobayashi’s 130-color scale Hue and                                      Lee, 2006).
Tone System. The transformation helped to reduce the
                                                                    Contrast Except for the color feature, we can extract many
complexity of the analysis (28ⅹ3  130) and to classify the         features from visual information. Above all, a high-contrast
input colors into emotionally meaningful representative             scene may evoke a different emotion than a low- or mild-
colors. An input pixel color (RGB) is converted to the Hue          contrast scene. We used a contrast-categorization-method
and Tone value (HT) as shown in Equation 1.                         based histogram analysis of image intensity (Kim, Choe, &
                                                                    Lee, 2006). The categorization performance of the method
 𝑅𝐺𝐵𝑡𝑜𝐻𝑇(𝑥) = arg min {𝑅𝐺𝐵_𝐷𝑖𝑠𝑡(𝑥, 𝐻𝑇[𝑖])}                 (1)      has already been proven by a previous human
                       𝑖
                                                                    psychophysical experiment. The method assigns the
                                                                    histogram distribution of every image to 1 of 5
   where x is 1 of the 28ⅹ3 colors used as an input color,          representative contrasts. We merged class D and class E
HT[i] is 1 of the 130 Hue and Tone colors (1≤ i ≤130), and          (see Figure 5) into 1 category since they yield similar
RGB_Dist(x,y) is a distance measuring method such as                contrast.
Euclidean distance on RGB space. The HT with the
minimum distance was selected as the HT value of the pixel.         Brightness Image brightness is 1 of the important cues
Then, we extracted 3 colors used most frequently in the             serving as a connotative feature of videos. Low brightness
frame. The 3 dominant colors were accumulated for all of            may not be used by directors to express a hopeful scene.
the frames of a video clip as a probability distribution of hue     However, it would be inappropriate to use absolute
and tone colors. The entire process is illustrated in Figure 4.     brightness as an estimating tool for eliciting emotion,
                                                                    because the overall brightness of a video depends on the
                                                                    brightness characteristics of the video camera or the
                                                                    director’s preference. Consequently, we extracted the
                                                                    relative brightness ( ̃ ) to the average brightness of the entire
                                                                    video as described in Equation 2. Yf (i) is an average
                                                                    brightness of all of the pixels in the ith frame, n is the total
                                                                    number of frames for a scene, and m is the total number of
                                                                    frames for an entire video.
                                                                                      ∑         (𝑖)   ∑      ( )
                                                                                  ̃=                                             (2)
                                                                    Motion Motion is an important factor in the evaluation of
                                                                    how dynamic scene is. We used a scale-invariant feature
                                                                    transform (SIFT, Lowe, 2004) to obtain motion information.
                                                                    SIFT features are extracted from the video frames and their
                                                                    trajectory is evaluated to estimate inter-frame motion (Lowe,
                                                                    2004). In this study considers the distance of their trajectory
 Figure 4 : Our color extracting method: (a) a video stream;        was considered as the motion size. Then, the motion feature
   (b) RGB images of each of the frames; (c) converted HT           defined the largest motion size of each inter-frame.
  images of each of the frames; (d) three dominant colors of
 each of the frames; and (e) a probability distribution of HT       Emotion Estimation
                     for all of the frames.                         To estimate emotions evoked in participants while watching
                                                                    videos, we used an evoked-emotion model. The
                                                                    psychophysical experiment was used to develop the emotion
                                                                    model and the multiple emotional responses were the
                                                                2049

training data. Then, the emotion model was learned by a            was 56%. In Table 3, 2nd emotion accuracy refers to the
supervised learning method using the psychophysical                percentage of times that either the most common or the
responses.                                                         second-most-common emotions was the predicted emotion.
Training Data As mentioned above, we extracted a motion               Table 3: Details of the accuracy of the evoked emotion
feature from inter-frame data and 3 other features from                                         model.
intra-frame data. We needed some representative values of
the features to train the emotion model. Thus, each feature                         1st emotion    2nd emotion     3rd emotion
was analyzed in every frame of a clip, and all feature data
were summarized during 1-second intervals. All video clips             Prediction
                                                                                          56%            63%             70%
were standardized to the same pixel resolution and frame                accuracy
rate. Every clip was composed of 24 frames per a second.
The color feature was encoded by using a 130-variable                Figure 6 shows a partial emotional profile of The Amazing
combination. The variable was calculated by normalization          Spider-Man using our model. The profile shows that the
(between 0 and 1) of the accumulated HT probability                movie does not have 1 evoked emotion; instead, it has 3 or
distribution for all of the frames. The contrast feature was       more evoked emotions at the same time. In addition, each
encoded by using a 5-variable combination and the variable         emotional state is shown as a probability value. The
was presented by the normalization of the accumulated              emotional state exhibits variations that are similar to the
probability distribution for contrast categories of all of the     original characteristics of the movie. For instance, the
frames. Brightness and motion features were calculated by          middle of the movie has many scenes with a heroine and a
averaging them over all of the frames.                             hero that induce a romantic mood, and the latter part has
   A training set of supervised learning consisted of an input     many episodes in which the hero challenges a powerful
feature vector and an output target vector. The target vector      villain. Further, the dominant emotion of each second from
was designed as 6 probability values. The probability values       Figure 6 is illustrated in Figure 7. The pie chart might help
were such that five emotions (contentment, surprise, anger,        one to search a movie for some of the most important
sadness, and fear) were filtered out to prevent mis-learning       information.
by null data (see Fig. 2). Moreover, “neutral” is a broad
emotional term with diverse meanings and, therefore, it was
also filtered out. Owing to the removal of the emotions, 6
clips in which one of the removed emotions predominated
had to be excluded.
Estimation Model The emotions evoked in the participants
were the target data used to train the model. Classification
and Regression Trees (CART, Breiman, et al., 1984), a kind
of decision tree learning, was selected as the learning
method to classify the evoked emotion from a video. One
advantage of CART is that it can consider misclassiﬁcation
costs in the tree induction, using handling numerical-valued
attributes. The model learns a probabilistic response for           Figure 6 : A partial emotional profile of The Amazing Spider-
each emotion and the extracted visual features. The                               Man using the proposed method.
participants’ responses are set as the independent variables
and mid-level visual features serve as the dependent
variables of the model.
Estimation Performance Finally, we conducted supervised
learning with CART. The model outputs probability values
(between 0 and 1) for every 6 emotions for an input video.
We evaluated the performance with a between-clip cross-
validation of 27 video clips.
   A summary of our estimation performance is shown in
Table 3. For the sake of convenience, the accuracy of
performance was calculated by using an emotion with the
estimated maximum probability for each clip. That is, each
of the 27 videos had a predicted emotion with a majority
probability. The predicted emotions were compared with the             Figure 7 : The emotional composition of The Amazing
most common emotion of the target data; the correction rate                                  Spider-Man.
                                                               2050

                         Discussion                                                     Acknowledgments
   Previous attempts to characterize the emotions of videos         This work was supported by the National Research
have used a single emotion, as a representative emotion for         Foundation (2012-0005643, Videome) grant funded by the
an entire video sequence (Gross & Levenson, 1995;                   Korea government (MEST) and was supported in part by the
Bailenson et al., 2008; Canini et al., 2009). However, a            Industrial Strategic Technology Development Program
person’s emotions vary every moment (Winter & Kuiper,               (10044009) funded by the Korea government (MKE).
1997).
   To take these individual differences into consideration,                                  References
the present paper proposes a probabilistic model that can           Bailenson, J.N. et al. (2008). Real-time classiﬁcation of
estimate multiple emotions evoked by a video over time.               evoked emotions using facial feature tracking and
The proposed method involves the automatic labeling of                physiological responses. International Journal of Human-
videos with the emotions that were evoked and the duration            Computer Studies, 66, 303-317.
of the evoked emotions. The present study attempted to              Breiman, L. et al. (1984). Classification and Regression
characterize the evoked emotions by using mid-level                   Trees, New York, Chapman & Hall (Wadsworth, Inc.).
features from the video frames, such as dominant colors,            Canini, L. et al. (2009). Emotional identity of movies. IEEE
contrast, brightness levels, and motion quantification. The           International Conference on Image Processing (ICIP).
characterization     was      derived    from a       previous        1821-1824.
psychophysical experiment using human participants. A               Ekman, P. & Friesen, W. V. (1978). Manual for facial
classical machine learning methodology was applied in                 action coding system. Palo Alto: Consulting Psychologists
order to build and test the model of the emotional categories         Press.
targeted.                                                           Gross, J.J., & Levenson, R.W. (1995). Emotion elicitation
   The present study provides 2 significant advances. First, it       using films. Cognition and Emotion, 9, 1, 87-108.
is the first to propose a new paradigm for video retrieval,         Kobayashi, S. (1981). Aim and method of the color image
using probabilistic multiple-evoked emotions. This                    scale. Color Research & Application, 6, 2, 93-107.
approach may be used to construct an emotion-based video            Kim, I., Choe, W., & Lee, S.D. (2006). Psychophysical
retrieval system, a video recommendation system, or an                measurement        for   perceptual    image    brightness
emotional treatment system using a video. It may also help a          enhancement based on image classification. Proceedings
machine to generate a new video profile that automatically            of the SPIE, 6057, 425-431.
describes sequentially changing emotions. Second, this              Li, S., Zhang, Y.J., & Tan, H.C. (2010). Discovering Latent
study provides a technically new approach. That is, we used           Semantic Factors for Emotional Picture Categorization.
emotionally meaningful mid-level visual features and we               IEEE International Conference on Image Processing
modeled them to estimate multiple-evoked emotional states             (ICIP). 1065-1068.
from videos.                                                        Lowe, D. (2004). Distinctive image features from scale-
   However, there are limitations to the present research.            invariant keypoints. International Journal of Computer
The experimental video clips were limited and participants            Vision, 60, 2, 91-110.
made non-various responses (Figure. 2). Having participants         Mikels, J.A. et al. (2005). Emotional category data on
choose only a single emotional response in the experiment             images from the International Affective Picture System,
might lead to inadequate data for generation of a                     Behav. Res. Methods 37, 626–630.
probabilistic emotional estimator. One of the interesting           Pos, O., & Armytage, P. G. (2007). Facial Expressions,
findings was that there were no anger responses. Mikels, et           Colours and Basic Emotions. Colour: Design &
al. (2005) reported the same result with still images. The            Creativity 1(1) 2, 1–20.
authors concluded that anger is very difficult to elicit with       Solli, M., & Lensz, R. (2010). Color semantics for image
the passive viewing conditions of static images. Our                  indexing. Proceedings of 5th European Conference on
experiment used some video clips that were a few minutes              Colour in Graphics, Imaging, and Vision, 353-358.
long. One explanation for the finding of no anger responses         Yanulevskaya, V. et al. (2008). Emotional valence
is that the video clips were not long enough to evoke the             categorization using holistic image features. IEEE
emotion.                                                              International Conference on Image Processing (ICIP).
   Thus, 3 tasks are left for future research to implement our        101-104.
approach in a real system. First, more video clips are              Wang, W., & He, Q. (2008). A survey on emotional
required to teach the various emotions to the evoked                  semantic image retrieval. IEEE International Conference
emotion model. Second, a larger sample is required to get a           on Image Processing (ICIP). 117-120.
robust trained model. Third, other machine learning                 Winter, K. A., & Kuiper, N. A. (1997). Individual
methods should be tried to find an optimal solution.                  differences in the experience of emotions. Clinical
                                                                      Psychology Review, 17(7), 791– 821.
                                                                2051

