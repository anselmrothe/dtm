UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Analogical Reinforcement Learning
Permalink
https://escholarship.org/uc/item/3xq80042
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Foster, James
Jones, Matt
Publication Date
2013-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                                         Analogical Reinforcement Learning
                                                   James M. Foster & Matt Jones
                                          james.m.foster@colorado.edu, mcj@colorado.edu
                                 University of Colorado, Department of Psychology & Neuroscience
                                                         Boulder, CO 80309 USA
                             Abstract                                   learning. Conversely, the prediction-error signals from RL
                                                                        can be used to guide induction of new higher-order relational
   Research in analogical reasoning suggests that higher-order
   cognitive functions such as abstract reasoning, far transfer,        concepts. Thus we propose there exists a computationally
   and creativity are founded on recognizing structural similari-       powerful synergy between analogy and RL. The simulation
   ties among relational systems. Here we integrate theories of         experiment reported here supports this claim. Because of the
   analogy with the computational framework of reinforcement
   learning (RL). We propose a computational synergy between            strong empirical evidence for each of these mechanisms taken
   analogy and RL, in which analogical comparison provides the          separately, we conjecture that the brain exploits this synergy
   RL learning algorithm with a measure of relational similar-          as well.
   ity, and RL provides feedback signals that can drive analogical
   learning. Initial simulation results support the power of this
   approach.                                                                                        Analogy
   Keywords: Analogy; Reinforcement Learning; Schema In-                Research in human conceptual knowledge representation has
   duction; Similarity; Generalization                                  shown that concepts are represented not just as distributions
                                                                        of features (cf. Nosofsky, 1986; Rosch & Mervis, 1975) but as
                         Introduction                                   relational structures. This relational knowledge includes both
The goal of the present work is to develop a computational              internal structure, such as the fact that a robin’s wings allow it
understanding of how people learn abstract concepts. Pre-               to fly (Sloman et al., 1998), as well as external structure, such
vious research in analogical reasoning suggests that higher-            as the fact that a dog likes to chase cats (Jones & Love, 2007).
order cognitive functions such as abstract reasoning, far trans-        Theories of analogical reasoning represent relational knowl-
fer, and creativity are founded on recognizing structural sim-          edge of this type in a predicate calculus that binds objects to
ilarities among relational systems (Doumas et al., 2008; Gen-           the roles of relations, for example CHASE ( DOG , CAT ). Ac-
tner, 1983; Hummel & Holyoak, 2003). However, we argue                  cording to these theories, an analogy between two complex
a critical element is missing from these theories, in that their        episodes (each a network of relations and objects) amounts
operation is essentially unsupervised, merely seeking patterns          to recognition that they share a common relational structure
that recur in the environment, rather than focusing on the ones         (Gentner, 1983; Hummel & Holyoak, 2003).
that are predictive of reward or other important outcomes.                  At a more mechanistic level, the dominant theory of anal-
   Here we integrate theories of analogy with the computa-              ogy is structural alignment (Gentner, 1983). This process
tional framework of reinforcement learning (RL). RL offers a            involves building a mapping between two episodes, mapping
family of learning algorithms that have been highly success-            objects to objects and relations to relations. The best map-
ful in machine learning applications (e.g., Bagnell & Schnei-           ping is one that maps objects to similar objects, maps rela-
der, 2001; Tesauro, 1995) and that have neurophysiological              tions to similar relations, and most importantly, satisfies par-
support in the brain (e.g., Schultz et al., 1997). A shortcom-          allel connectivity. Parallel connectivity means that, when-
ing of RL is that it only learns efficiently in complex tasks if it     ever two relations are mapped to each other, the objects fill-
starts with a representation (i.e., a means for encoding stimuli        ing their respective role-fillers are also mapped together. An
or states of the environment) that somehow captures the crit-           example is shown in Figure 1. Parallel connectivity is sat-
ical structure inherent in the task. We formalize this notion           isfied here because, for each mapped pair of ATTACK rela-
below in terms of similarity-based generalization (Shepard,             tions (red arrows), the objects filling the ATTACKER role are
1987) and kernel methods from statistical machine learning              mapped together (knight is mapped to queen), and the objects
(Shawe-Taylor & Cristianini, 2004). In other words, RL re-              filling the ATTACKED role are also mapped together (rook
quires a sophisticated sense of similarity to succeed in real-          to rook and king to king). Thus structural alignment con-
istically complex tasks. Psychologically, the question of how           stitutes a (potentially partial or imperfect) isomorphism be-
such a similarity function is learned can be cast as a question         tween two episodes, which respects the relational structure
of learning sophisticated, abstract representations.                    that they have in common. Importantly, if the search for a
   This paper proposes a computational model of analogical              mapping gives little emphasis to object-level similarity (as
RL, in which analogical comparison provides the RL learning             opposed to relation-level similarity and parallel connectivity),
algorithm with a measure of relational similarity, and RL pro-          then structural alignment can find abstract commonalities be-
vides feedback signals that can drive analogical learning. Re-          tween episodes having little or no surface similarity (i.e., in
lational similarity enables RL to generalize knowledge from             terms of perceptual features).
past to current situations more efficiently, leading to faster              We propose structural alignment is critical to learning of
                                                                    448

                                                                     example, one could list many relational patterns that arise in
                                                                     chess games but that are not especially useful for choosing a
                                                                     move or for predicting the course of the game. In previous
                                                                     work, we have found that implementing structural alignment
                                                                     and schema induction in a rich and structured artificial en-
                                                                     vironment results in discovery of many frequent but mostly
                                                                     useless schemas (Foster et al., 2012). An alternative, poten-
                                                                     tially more powerful model of analogical learning would in-
                                                                     volve feedback from the environment, so that the value of
                                                                     an analogy or schema is judged partially by how well it im-
                                                                     proves predictions of reward or other important environmen-
Figure 1: An example of structural alignment between two             tal variables. For example, the concept of a fork in chess
chess positions. Both positions contain instances of the ab-         is an important schema not (only) because it is a recurring
stract concept of a FORK: black’s piece is simultaneously at-        pattern in chess environments, but because it carries informa-
tacking both of white’s pieces. These attacking relations are        tion about significant outcomes (i.e., about sudden changes
represented by the red arrows. Cyan lines indicate the map-          in each player’s chances of winning). A natural framework
ping between the two episodes. The mapping satisfies paral-          for introducing this sort of reward sensitivity into theories of
lel connectivity because it respects the bindings between re-        analogy is that of RL, which we review next.
lations and their role-fillers.
                                                                                     Reinforcement Learning
abstract concepts for three reasons. First, perceived simi-          RL is a mathematical and computational theory of learning
larity of relational stimuli depends on structural alignability      from reward in dynamic environments. An RL task is char-
(Markman & Gentner, 1993). Second, structural alignment              acterized by an agent embedded in an environment that exists
is important for analogical transfer, which is the ability to        in some state at any given moment in time. At each time step,
apply knowledge from one situation to another, superficially         the agent senses the state of its environment, takes an action
different situation (Gick & Holyoak, 1980). For example, a           that affects what state occurs next, and receives a continuous-
winning move in one chess position can be used to discover a         valued reward that depends on the state and its action (Sutton
winning move in a different (but aligned) position, by trans-        & Barto, 1998). This framework is very general and can en-
lating that action through the analogical mapping. Third,            compass nearly any psychological task in which the subject
a successful analogy can lead to schema induction, which             has full knowledge of the state of the world at all times (i.e.,
involves extraction of the shared relational structure iden-         there are no relevant hidden variables).
tified by the analogy (Doumas et al., 2008; Gentner, 1983;              Most RL models work by learning values for different
Hummel & Holyoak, 2003). In the example of Figure 1, this            states or actions, which represent the total future reward that
schema would be a system of relational knowledge on ab-              can be expected from any given starting point (i.e., from any
stract (token) objects, including ATTACK ( PIECE 1, PIECE 2),        state or from any action within a state). These values can be
ATTACK ( PIECE 1, PIECE 3), and potentially other shared             learned incrementally, from temporal-difference (TD) error
information such as NOT ATTACKED ( PIECE 1) and                      signals calculated from the reward and state following each
KING ( PIECE 2).                                                     action (see Model section). There is strong evidence that the
    These three observations suggest that analogy plays an im-       brain computes something close to TD error, and thus that RL
portant role in learning and use of abstract relational con-         captures a core principle of biological learning (Schultz et al.,
cepts. The first two observations suggest that analogical            1997).
transfer can be cast as a form of similarity-based generaliza-          In principle, this type of simple algorithm could be used to
tion, as we elaborate in the next two sections. In brief, struc-     perfectly learn a complex task such as chess, by experiencing
tural alignment offers a sophisticated form of similarity that       enough games to learn the true state values (i.e., probability
can be used to generalize knowledge between situations that          of winning from every board position) and then playing ac-
are superficially very different. The third observation sug-         cording to those values. However, a serious shortcoming of
gests that analogy can discover new relational concepts (e.g.,       this naive approach is that it learns the value of each state in-
the concept of a chess fork, from Figure 1), which can in turn       dependently, which can be hopelessly inefficient for realistic
lead to perception of even more abstract similarities among          tasks that typically have very large state spaces. Instead, some
future experiences.                                                  form of generalization is needed, to allow value estimates for
    One potential shortcoming of the basic theory of analogy         one state to draw on experience in other, similar states.
reviewed here is that is it essentially unsupervised. In this           Many variants of RL have been proposed for implement-
framework, the quality of an analogy depends only on how             ing generalization among states (e.g., Albus, 1981; Sutton,
well the two systems can be structurally aligned, and not on         1988). Here we pursue a direct and psychologically moti-
how useful or predictive the shared structure might be. For          vated form of generalization, based on similarity (Jones &
                                                                 449

Cañas, 2010; Ormoneit & Sen, 2002). We assume the model             significantly improve reward prediction. Such analogies in-
has a stored collection of exemplar states, each associated          dicate that the structure common to the two analogue states
with a learned value. The value estimate for any state is            may have particular predictive value in the current task, and
obtained by a similarity weighted average over the exem-             hence that it might be worth extracting as a standalone con-
plars’ values; that is, knowledge from each exemplar is used         cept. For example, if the model found a winning fork move
in proportion to how similar it is to the current state. This        by analogical comparison to a previously seen state involving
approach is closely related to exemplar-generalization mod-          a fork, the large boost in reward could trigger induction of a
els in more traditional psychological tasks such as category         schema embodying the abstract concept of a fork.
learning (Nosofsky, 1986). It can also be viewed as a subset            The proposed model thus works as follows (see the next
of kernel methods from machine learning (Shawe-Taylor &              section for technical details). The model maintains a set of
Cristianini, 2004), under the identification of the kernel func-     exemplars E, each with a learned value, v(E). To estimate
tion with psychological similarity (Jäkel et al., 2008).            the value of any state s, it compares that state to all exemplars
   A critical consideration for all learning models (includ-         by structural alignment, which yields a measure of analogi-
ing RL models) is how well their pattern of generalization           cal similarity for each exemplar (Forbus & Gentner, 1989).
matches the inherent structure of the task. If generalization        The estimated value of the state, Ṽ (s), is then obtained as a
is strong only between stimuli or states that have similar val-      similarity-weighted average of v(E). After any action is taken
ues or outcomes, then learning will be efficient. On the other       and the immediate reward and next state are observed, a TD
hand, if the model generalizes significantly between stimuli         error is computed as in standard RL. The exemplar values are
or states with very different outcomes, its estimates or pre-        then updated in proportion to the TD error and in proportion
dictions will be biased and learning and performance will            to how much each contributed to the model’s prediction, that
be poor. The kernel or exemplar-similarity approach makes            is, in proportion to sim(s, E).
this connection explicit, because generalization between two            Additionally, whenever the structural alignment between a
states is directly determined by their similarity. As we pro-        state and an exemplar produces a sufficient reduction in pre-
pose next, analogy and schema induction offer a sophisticated        diction error (relative to what would be expected if that ex-
form of similarity that is potentially quite powerful for learn-     emplar were absent), a schema is induced from that analogy.
ing complex tasks with structured stimuli.                           The schema is an abstract representation, defined on token
                                                                     (placeholder) objects, and it contains only the shared infor-
                       Analogical RL                                 mation that was successfully mapped by the analogy. The
                                                                     schema is added to the pool of exemplars, where it can ac-
The previous two sections suggest a complementary relation-
                                                                     quire value associations directly (just like the exemplars do).
ship between analogy and RL, which hint at the potential for
                                                                     The advantage conferred by the new schema is that it allows
a computationally powerful, synergistic interaction between
                                                                     for even faster learning about all states it applies to (i.e., that
these two cognitive processes. We outline here a formal the-
                                                                     contain that substructure). For example, rather than learn-
ory of this interaction. The next two sections provide a math-
                                                                     ing by generalization among different instances of forks, the
ematical specification of a partial implementation of this the-
                                                                     model would learn a direct value for the fork concept, which
ory, and then present simulation results offering a proof-in-
                                                                     it could immediately apply to any future instances. A conse-
principle of the computational power of this approach.
                                                                     quence of the schema induction mechanism is that the pool
   The first proposed connection between analogy and RL is
                                                                     of concrete exemplars comes to contain more and more ab-
that structural alignment yields an abstract form of psycho-
                                                                     stract schemas. Thus the model’s representation transitions
logical similarity that can support sophisticated generaliza-
                                                                     from initially episodic to more abstract and conceptual.
tion (Gick & Holyoak, 1980; Markman & Gentner, 1993).
                                                                        Analogical RL thus integrates three principles from prior
Incorporating analogical similarity into the RL framework
                                                                     research: RL, exemplar generalization, and structural align-
could thus lead to rapid learning in complex, structured envi-
                                                                     ment of relational representations. Because each of these
ronments. For example, an RL model of chess equipped with
                                                                     principles has strong empirical support as a psychological
analogical similarity should recognize the similarity between
                                                                     mechanism, it is plausible that they all interact in a manner
the two positions in Figure 1 and hence generalize between
                                                                     similar to what we propose here. Thus it seems fruitful to
them. Consequently the model should learn to create forks
                                                                     explore computationally what these mechanisms can achieve
and to avoid forks by the opponent much more rapidly than if
                                                                     when combined.
it had to learn about each possible fork instance individually.
   The second proposed connection is that the TD error com-
                                                                                                   Model
puted by RL models, for updating value estimates, can poten-
tially drive analogical learning by guiding schema induction.        The simulation study presented below uses a variant of RL
Instead of forming schemas for whatever relational structures        known as afterstate learning, in which the agent learns val-
are frequently encountered (or are discovered by analogical          ues for the possible states it can move into (Sutton & Barto,
comparison of any two states), an analogical RL model can            1998). This is a reasonable and efficient method for the task
be more selective, only inducing schemas from analogies that         we use here—tic-tac-toe, or noughts & crosses—because the
                                                                 450

agent’s opponent can be treated as part of the environment              ! = r(t) + ! ! V! ( st+1 ) " V! ( st )
and is the only source of randomness. Our main proposal re-
                                                                                                                     V! (s) = " v(E)! a(E)
garding the interaction between RL and analogical learning is                                                                 E
not limited to this approach.
                                                                                 Δv(E) = α⋅δ⋅a(E)                                  v(E)
   The operation of the model is illustrated in Figure 2. On
each time step, the model identifies all possible actions and
                                                                                                              O"    O"           O"     X"
their associated afterstates. For each afterstate s, it computes            Exemplars (E)                 O"     X!                  O"       …  X – – – X
an analogical similarity, K, to each exemplar, E, by structural                                           X"        X"  O" X"     X"
alignment. Each possible mapping M : s ! E is evaluated
                                                                                                   K(s, E)
according to                                                                           a(E) =
                                                                                                 " K(s, E!)                     X  O       O   X
                                                                                                                                O            O
                                                                                                                                X          X
   F(M) = b · Â sim (o, M(o))                                                                                       X"  O"
                                                                                    Candidate state (s)             O"
                 o2s                                                                                                X"
                        "                                  #
                               nr
   + Â sim (r, M(r)) · 1 + Â I{M(childi (r))=childi (M(r))} .  (1)
      r2s                     i=1                                      Figure 2: Model operation. Each candidate afterstate is evalu-
                                                                       ated by analogical comparison to stored exemplars, followed
This expression takes into account object similarity, by com-          by similarity-weighted averaging among the learned exem-
paring each object o in s to its image in E; relational sim-           plar values. Learning is by TD error applied to the exemplar
ilarity, by comparing each relation r in s to its image in E;          values. On some trials, especially useful analogies produce
and parallel connectivity, by having similarity between mu-            new schemas that are added to the exemplar pool. In the
tually mapped relations “trickle down” to add to the similar-          example here, s and E both have guaranteed wins for X by
ity of any mutually mapped role-fillers (Forbus & Gentner,             threatening a win in two ways. The induced schema embodies
1989). The sim function is a primitive (object- and relation-          this abstract structure. Dots with red arrows indicate ternary
level) similarity function, b determines the relative contribu-        “same-rank” relations. r = reward; g = temporal discount pa-
tion of object similarity, nr is the number of roles in relation       rameter; a = learning rate; other variables are defined in the
r, childi (r) is the object filling the ith role of r, and I{P} is     text.
an indicator function equal to 1 when proposition P is true.
Analogical similarity is then defined as the value of the best
mapping (here the q parameter determines specificity of gen-           the square root of time, which seems to give good perfor-
eralization):                                                          mance.
                                                                          The more important form of representation learning in the
                                 ✓              ◆
                                                                       model is schema induction. Schema induction has not been
                 K(s, E) = exp q · max F(M) .                  (2)     implemented yet, but Figure 2 shows how it is expected to
                                      M
                                                                       work. Following learning after each trial, the model deter-
   The activation a(E) of each exemplar is determined by nor-          mines how much each exemplar contributed to reducing pre-
malizing the analogical similarities, and the estimated value          diction error, by comparing d to what it would have been
of s, Ṽ (s), is computed as a similarity-weighted average of          without that exemplar. If the reduction is above some thresh-
the exemplar values v(E) (Figure 2). Thus the estimate is              old, the analogical mapping found for that exemplar (lower
based on the learned values of the exemplars most similar to           right of figure) produces a schema that is added to the exem-
the candidate state.                                                   plar pool (far right). The schema is given a value of v initial-
   Once values Ṽ (s) have been estimated for all candidate af-        ized at Ṽ (st ). This schema value is updated on future trials
terstates, the model uses a softmax (Luce-choice or Gibbs-             just as are the exemplar values. Acquisition of new schemas
sampling rule) to select what state to move into (here t is an         in this way is predicted to improve the model’s pattern of gen-
exploration parameter):                                                eralization, tuning it to the most useful relational structures in
                                                                       a task.
                       Pr[st = s] µ eṼ (s)/t .                (3)
   Learning based on the chosen afterstate st follows the                                                        Simulation
SARSA rule (Rummery & Niranjan, 1994), after the model                 The model was tested on its ability to learn tic-tac-toe. Each
chooses its action on the next time step. This produces a TD           board position was represented by treating the nine squares
error, which is then used to update the exemplar values by             as objects of types 0 (blank), 1 (focal agent’s), and 2 (oppo-
gradient descent (see Equations for d and Dv(E) in Figure 2).          nent’s), and defining 8 ternary “same-rank” relations for the
   The model also grows its representation in two ways. First,         rows, columns, and diagonals. Thus a player wins by filling
it begins with no exemplars, and on each trial adds the state          all squares in any one of these relations. Object similarity
it moves to as a new exemplar with probability inversely pro-          was defined as 1 for matching object types and 0 otherwise.
portional to the number of exemplars already in the model.             Similarity between relations was always 1 because there was
This recruitment policy leads the exemplar pool to grow with           only one type of relation. Reward was given only at the end
                                                                   451

of a game, as +1 for the winner, -1 for the loser, or 0 for a
draw. After the game ended, it moved to a special terminal
state with fixed value of 0. For simplicity, all free parameters
of the model (b, q, a, g, t) were set to a default value of 1.
   Three variations of the model were implemented, differing
in their levels of analogical abstraction. The Featural model
was restricted to literal mappings between states (upper-left
square to upper-left square, etc.). This model still included
generalization, but its similarity was restricted to the concrete
similarity of standard feature-based models. The Relational
model considered all 8 mappings defined by rigid rotation             Figure 3: Learning curves. A: 50 copies of each model. B:
and reflection of the board. This scheme was used in place            Single copies of the two slower models over extended train-
of searching all 9! possible mappings for every comparison,           ing.
to reduce computation time. Finally, the Schema model ex-
tended the Relational model by starting with two hand-coded
                                                                      a powerful and general theory of representation learning, be-
schemas, 111 and 022. The first of these is a single same-rank
                                                                      cause it can be integrated with any form of representation that
relation bound to three instances of the player’s own token.
                                                                      yields a pairwise similarity function. Its TD error signal can
Thus moving into a state satisfying this schema produces an
                                                                      drive changes in representation via the objective of improving
immediate win. Likewise, moving into a state satisfying the
                                                                      generalization. In previous work, we have applied this idea to
second schema risks an immediate loss. The model was given
                                                                      learning of selective attention among continuous stimulus di-
no information about these schemas (i.e., v was initialized to
                                                                      mensions (Jones & Cañas, 2010). The current model offers a
0 for both), but it was capable of learning values for them.
                                                                      richer form of representation learning, in that it acquires new
The purpose of this model was to test the utility of having
                                                                      concepts rather than reweighting existing features.
schemas that capture task-relevant structures. Logically this
question is separate from that of how such schemas are ac-               The analogical RL model also builds on other models of re-
quired, although we have addressed that question elsewhere            lational learning. Tomlinson & Love (2006) propose a model
(Foster et al., 2012), and we plan to integrate a solution into       of analogical category learning, with essentially the same
the present model soon.                                               similarity and exemplar generalization mechanisms adopted
                                                                      in the present model. Our model adds to theirs in that it ap-
   Each model variant was trained in blocks of 10 games of
                                                                      plies to dynamic tasks and in that it grows its representation
self-play followed by a pair of testing games against an ideal
                                                                      through schema induction. Van Otterlo (2012) has developed
player (playing first in one game and second in the other).
                                                                      methods for applying RL to relational representations of the
Learning occurred only during training. In testing games, the
                                                                      same sort used here, although the approach to learning is quite
model was given one point for each non-losing move it made
                                                                      different. His models are not psychologically motivated and
(i.e., moves from which it could still guarantee a draw), for a
                                                                      hence learn in batches and form massive conjunctive rules,
maximum of 9 points per pair of testing games.
                                                                      with elaborate updating schemes to keep track of the possible
   Average learning curves are shown in Figure 3A for 50 in-
                                                                      combinations of predicates. In contrast, the present approach
dependent copies of each model over 5000 blocks (50,000
                                                                      learns iteratively, behaves probabilistically, and grows its rep-
training games). Figure 3B shows results for single copies of
                                                                      resentation more gradually and conservatively. This approach
the Relational and Featural models over 30,000 blocks. These
                                                                      is likely to provide a better account of human learning, but a
results show that the Featural model does eventually learn, but
                                                                      more interesting question may be whether it offers any perfor-
the Relational model learns an order of magnitude faster, and
                                                                      mance advantages from a pure machine-learning perspective.
the Schema model learns another order of magnitude faster
                                                                         In the present model, the activation of each exemplar
than the Relational model.
                                                                      elicited by a candidate state can be thought of as a feature
                                                                      of that state. The exemplar effectively has a “receptive field”
                          Discussion
                                                                      within the state space, defined by the similarity function.
The results presented here constitute a proof-of-principle that       This duality between exemplar- and feature-based represen-
analogy and schema induction can be productively integrated           tations is founded in the kernel framework (see Shawe-Taylor
with a learning framework founded on RL and similarity-               & Cristianini, 2004). The present model takes advantage of
based generalization. This integration leads to a model ex-           this duality, producing a smooth transition from an episodic,
hibiting sophisticated, abstract generalization derived from          similarity-based representation to a more semantic, feature-
analogical similarity, as well as discovery of new higher-order       based representation defined by learned schemas.
relational concepts driven by their ability to predict reward.           The model as currently implemented does have several
   The basic modeling framework used here applies not just to         limitations. Foremost, it does not yet include a mechanism
analogical similarity and schema induction, but to other forms        for inducing new schemas. We and others have shown how
of representational learning as well. Kernel-based RL offers          schema induction can be successfully deployed in an open-
                                                                  452

ended model in a complex environment (Doumas et al., 2008;              Jäkel, F., Schölkopf, B., & Wichmann, F. A. (2008). Gener-
Foster et al., 2012). We hope that building this type of mecha-            alization and similarity in exemplar models of categoriza-
nism into the analogical RL framework will produce a better-               tion: Insights from machine learning. Psychon B Rev, 15,
controlled, directed system capable of autonomously discov-                256–271.
ering genuinely new abstract concepts.                                  Jones, M., & Cañas, F. (2010). Integrating reinforcement
   A second limitation of the current model is its slowness                learning with models of representation learning. Proceed-
to learn, due to the nature of gradient descent operating in a             ings of the 32nd Annual Conference of the Cognitive Sci-
large weight space. In contrast, human learning often shows                ence Society, 1258–1263.
understanding of new concepts in as little as one trial (Maas &         Jones, M., & Love, B. C. (2007). Beyond common features:
Kemp, 2009). The theory of analogy via structure mapping                   The role of roles in determining similarity. Cognitive Psy-
seems like the best candidate for a process-level theory of                chol, 55, 196–231.
such rapid learning, and we predict that the full analogical            Maas, A. L., & Kemp, C. (2009). One-shot learning with
RL model with schema induction will show significant steps                 bayesian networks. Proceedings of the 31st Annual Con-
in that direction.                                                         ference of the Cognitive Science Society.
   The present work is complementary to hierarchical                    Markman, A. B., & Gentner, D. (1993). Structural align-
Bayesian models that discover relational structure through                 ment during similarity comparisons. Cognitive Psychol,
probabilistic inference (Tenenbaum et al., 2011). Whereas                  25, 431–431.
our model builds up schemas from simpler representations,
                                                                        Nosofsky, R. M. (1986). Attention, similarity, and the
the Bayesian approach takes a top-down approach, defining
                                                                           identification-categorization relationship. J Exp Psychol
the complete space of possibilities a priori and then selecting
                                                                           Gen, 115, 39–57.
among them. The top-down approach applies to any learn-
                                                                        Ormoneit, D., & Sen, S. (2002). Kernel-based reinforcement
ing model, because any well-defined algorithm can always be
                                                                           learning. Mach Learn, 49, 161–178.
circumscribed in terms of its set of reachable states. This is
a useful exercise for identifying inductive biases and abso-            Rosch, E., & Mervis, C. B. (1975). Family resemblances:
lute limits of learning, but it offers little insight into the con-        Studies in the internal structure of categories. Cognitive
structive processes that actually produce the learning. These              Psychol, 7, 573–605.
mechanistic questions are critical if the goal is to understand         Rummery, G. A., & Niranjan, M. (1994). On-line q-learning
how the human mind discovers new, abstract concepts.                       using connectionist systems (Tech. Rep. No. CUED/F-
                                                                           INFENG/TR 166). Cambridge University.
                    Acknowledgments                                     Schultz, W., Dayan, P., & Montague, P. R. (1997). A neural
Supported by AFOSR Grant FA-9550-10-1-0177 to MJ.                          substrate of prediction and reward. Science, 275, 1593–
                                                                           1599.
                         References                                     Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods
Albus, J. S. (1981). Brains, Behavior and Robotics. Byte                   for Pattern Analysis. Cambridge University Press.
   Books.                                                               Shepard, R. N. (1987). Toward a universal law of generaliza-
Bagnell, J. A., & Schneider, J. C. (2001). Autonomous he-                  tion for psychological science. Science, 237, 1317–1323.
   licopter control using reinforcement learning policy search          Sloman, S. A., Love, B. C., & Ahn, W. K. (1998). Fea-
   methods. IEEE Int Conf Robo, 1615-1620.                                 ture centrality and conceptual coherence. Cognitive Sci,
Doumas, L. A., Hummel, J. E., & Sandhofer, C. M. (2008).                   22, 189–228.
   A theory of the discovery and predication of relational con-         Sutton, R. S. (1988). Learning to predict by the methods of
   cepts. Psychol Rev, 115, 1–43.                                          temporal differences. Mach Learn, 3, 9–44.
Forbus, K. D., & Gentner, D. (1989). Structural evaluation of           Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learn-
   analogies: What counts. Proceedings of the 11th Annual                  ing: An Introduction. The MIT Press.
   Conference of the Cognitive Science Society, 341–348.                Tenenbaum, J. B., Kemp, C., Griffiths, T. L., & Goodman,
Foster, J. M., Cañas, F., & Jones, M. (2012). Learning                    N. D. (2011). How to grow a mind: Statistics, structure,
   conceptual hierarchies by iterated relational consolidation.            and abstraction. Science, 331, 1279–1285.
   Proceedings of the 34th Annual Conference of the Cogni-              Tesauro, G. (1995). Temporal difference learning and td-
   tive Science Society, 324–329.                                          gammon. Commun ACM, 38(3), 58–68.
Gentner, D. (1983). Structure-mapping: A theoretical frame-             Tomlinson, M. T., & Love, B. C. (2006). From pigeons to hu-
   work for analogy. Cognitive Sci, 7, 155–170.                            mans: Grounding relational learning in concrete examples.
Gick, M. L., & Holyoak, K. J. (1980). Analogical problem                   Proceedings of the 21st National Conference on Artificial
   solving. Cognitive Psychol, 12, 306–355.                                Intelligence (AAAI-06), 199–204.
Hummel, J. E., & Holyoak, K. J. (2003). A symbolic-                     Van Otterlo, M. (2012). Solving relational and first-order log-
   connectionist theory of relational inference and generaliza-            ical markov decision processes: A survey. Reinforcement
   tion. Psychol Rev, 110, 220–264.                                        Learning, 12, 253–292.
                                                                    453

