UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Constraining Bayesian Inference with Cognitive Architectures: An Updated Associative
Learning Mechanism in ACT-R
Permalink
https://escholarship.org/uc/item/6s60s3hv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Thomson, Robert
Lebiere, Christian
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                   Constraining Bayesian Inference with Cognitive Architectures:
                         An Updated Associative Learning Mechanism in ACT-R
                                         Robert Thomson (thomsonr@andrew.cmu.edu)
                                         Department of Psychology, Carnegie Mellon University
                                           5000 Forbes Avenue, Pittsburgh, PA, 15213, USA
                                                  Christian Lebiere (cl@cmu.edu)
                                         Department of Psychology, Carnegie Mellon University
                                           5000 Forbes Avenue, Pittsburgh, PA, 15213, USA
                               Abstract                                 their models with algorithmic and implementation (i.e.,
   Bayesian inference has been shown to be an efficient                 neural) level cognitive plausibility.
   mechanism for describing models of learning; however,                   Interestingly, ACT-R 6.0 (Anderson et al., 2004) is a
   concerns over a lack of constraint in Bayesian models (e.g.,         cognitive architecture which already uses Bayesian-inspired
   Jones & Love, 2011) has limited their influence as being a           inference to drive sub-symbolic learning (i.e., to generate
   description of the ‘real’ processes of human cognition. In this
                                                                        and update the activation strength of chunks in declarative
   paper, we review some of these concerns and argue that
   cognitive architectures can address these concerns by                memory). The architecture is both constrained by learning
   constraining the hypothesis space of Bayesian models and             rules (e.g., activation equations; base-level learning) and
   providing a biologically-plausible mechanism for setting             neuro-cognitively justified by many studies (Anderson &
   priors and performing inference. This is done in the context of      Lebiere, 1998; Anderson et al., 2004; Anderson, 2007).
   the ACT-R functional cognitive architecture (Anderson &              While there have been difficulties in adapting some aspects
   Lebiere, 1998), whose sub-symbolic information processing            of the Bayesian approach (e.g., in implementations of
   is essentially Bayesian. To that end, our focus in this paper is
   on an updated associative learning mechanism for ACT-R
                                                                        associative learning), ACT-R serves as an example whereby
   that implements the constraints of Hebbian-inspired learning         Bayesian inference can be constrained by a neurally-
   in a Bayesian-compatible framework.                                  localized and behaviorally-justified cognitive architecture.
   Keywords: cognitive architectures; Bayesian inference;               In this sense, ACT-R can act as a bridge between all three
   Hebbian learning; cognitive models; associative learning;            layers of Marr’s tri-level hypothesis.
                                                                           For the remainder of this paper, we present an overview
                          Introduction                                  of the debate over the applicability of Bayes inference to
Bayesian approaches to reasoning and learning have been                 cognition and argue that ACT-R represents the kind of
successful in such fields as decision-making (Tenenbaum,                constraint that addresses criticisms against Bayesian models.
Griffiths, & Kemp, 2006), language learning (Xu &                       We will further describe an updated associative learning
Tenenbaum, 2007), and perception (Yuille & Kersten,                     mechanism for ACT-R that links Bayesian-compatible
2006). Most specifically, Bayesian inference has been                   inference with a Hebbian-inspired learning rule.
exceptional in discovering some of the structure of language
and word learning with substantially less training than                                    Bayesian Inference
traditional connectionist networks.                                     The essential feature of Bayesian inference is that it reasons
   Despite their successes, Bayesian models have come                   over uncertain hypotheses (H) in probability space (i.e.,
under attack for being unconstrained, unfalsifiable, and                from 0 – 100% certainty). The Bayes rule is defined as:
overly reliant on optimality as an assumption for reasoning
(see Jones & Love, 2011; Bowers & Davis, 2012 for an
exhaustive review; and Griffiths et al., 2012 for a counter-            where the posterior probability of an outcome                is
argument). While these criticisms are not without merit (nor            derived from the likelihood                 of the hypothesis
are the Bayesians’ rebuttals fully convincing), the issue of            explaining the data, combined with the prior probability of
constraints remains a critical argument. It is also not a new           the hypothesis         , and normalized by the probability of
argument. Over 25 years ago the constraint argument was                 the data P(D). Thus, updating one’s belief is based on one’s
leveled against the field of connectionism (Fodor &                     prior belief influenced by the likelihood that some new
Pylyshyn, 1988). Then it was argued that, via several                   evidence supports this belief. At its core, Bayesian inference
learning rules and organizing principles, any behavior could            is an excellent derivation of the scientific method.
theoretically be captured by connectionist networks.                       A difference between Bayesian models and connectionist
   The degree that progress has slowed for the explanatory              implementations is that Bayes models of human cognition
power of connectionist networks is beyond the scope of this             tend to use richer, more structured, and symbolic knowledge
paper; however, constraints on neural network development               than connectionist models, which tend to use more
using a common learning rule in a stable cognitively-                   distributed representations operating over less structured
plausible architecture have been advanced (O’Reilly, 1998;              input. This level of inference places Bayesian models at the
O’Reilly, Hazy, & Herd, 2012). By corollary, to address                 computational level of Marr’s tri-level hypothesis, whereas
similar concerns, the Bayesian movement needs to develop                cognitive architectures and connectionist networks operate
constraints which balance the computational transparency of
                                                                    3539

more at the algorithmic level (Marr, 1982). By remaining at            instance, Kruschke (2008) reviewed two Bayesian models
a higher level of description, it is argued that Bayesian              of learning backward blocking in classical conditioning, the
descriptions of cognitive behaviors are better understood as           first using a Kalman filter (Dayan, Kakade, & Montague,
a framework for explaining cognition as opposed to an                  2000) and the other using a noisy-logic gate (Danks,
explanation of how cognitive operations and representations            Griffiths, & Tenenbaum, 2003). Both models gave
should behave in a given task (Tenenbaum et al., 2011).                substantively different predictions, with the Kalman filter
   This higher level of description leads to many of the               model unable to reproduce human behavior.
criticisms leveled against Bayesian models. We wish to                    Furthermore, there are several tasks whose results do not
address three related criticisms of Bayesian models: (1) they          readily fit within a naïve Bayesian explanatory framework.
are unconstrained; (2) they are unfalsifiable; and (3) there is        For instance, simple Bayesian models do not capture
little neuro-scientific evidence to support Bayesian theory. It        violations of the sure-thing principle. Given a random
is easy to see how (2) and (3) follow from (1) since without           variable x that has only two possible outcomes A or B, naïve
constraint, it is theoretically possible to redefine the priors        Bayesian inference requires p(x) to fall between p(x|A)
and hypothesis space of the model to curve fit to any data.            and p(x|B). A violation occurs when p(x) > p(x|A) and p(x)
Part of the issue with (3) is that Bayesian description tends          > p(x|B) or vice versa. Shafir and Tversky (1992) showed
to operate at the computational level, yet be described in             this violation of the sure-thing principle in a prisoner’s
stronger, more algorithmic terms (e.g., probabilistic                  dilemma task. Finding these unintuitive results that naïve
population codes; Ma et al. 2006).                                     Bayes models do not easily address, and finding constrained
   These criticisms have led to Bayesian theory being                  parameter learning rules (such as the noisy-logic gate)
criticized as a ‘just-so’ story (i.e., that the Bayesian               provides much needed constraints and falsifiability to the
framework commits the ad hoc fallacy; Bowers & Davis,                  Bayesian framework. Rather than being seen as anti-
2012). However, rebuttals by Griffiths et al. (2012), rather           Bayesian results, these models should be seen as shaping the
than addressing these criticisms in a constructive manner,             boundaries of Bayesian explanatory power.
countered with essentially a ‘you-too’ argument. Griffiths et             Finally, while there is contested neuro-scientific evidence
al. (2012) argued that curve-fitting models to data is not an          as to neural assemblies firing probabilistically, this does not
exclusive sin of Bayesian models, however, the                         necessarily imply a Bayesian implementation-level
transparency with which Bayesian models do so make them                explanation, but instead implies the softer claim of a
easy targets. In fact, as Griffiths et al. counter, criticisms (1)     Bayesian-compatible behavioral explanation of neural
and (2) may be leveled against any model or architecture               phenomena, especially when the Bayesian inferences are
with sufficient parametric degrees of freedom (which they              justified within a neurally-plausible cognitive architecture.
implicitly argue is a feature of most or all existing models).            In considering many of the criticisms of Bayesian theory,
This argument against architectures had previously been                it is important to note that more research needs to be done to
espoused by Roberts and Pashler (2000) over a decade ago.              find constraints. As we previously argued, connectionist
   In a recent Science article by Tenenbaum et al., (2011)             networks were not sufficiently constrained until sufficient
Bayesian inference is defined as being synonymous with                 model testing was performed and architectures developed
probabilistic inference. This leads to criticism (2). The              using a common learning rule and constrained set of
difficulty with making ‘Bayesian’ and ‘probabilistic’                  parameters. For the Bayesian framework, we argue that all
synonymous terms is that any algorithm that approximates               of criticisms (1) – (3) can be addressed by situating
probabilistic reasoning can be argued to be approximating              Bayesian inference within a cognitive architecture, and
Bayesian inference and thus be essentially Bayesian.                   furthermore that ACT-R 6 is already such an architecture.
Conversely, any Bayesian algorithm that does not
successfully reproduce human data can lead to the argument
                                                                                      The ACT-R Architecture
that the issue isn’t with the Bayesian algorithm per se, but in        ACT-R is a computational implementation of a unified
the transformation of data into a probability space (e.g., by          theory of cognition. It accounts for information processing
not having the correct priors or correct hypotheses) or in the         in the mind via task-invariant mechanisms constrained by
lack of human-like limitations of the algorithms to carry out          the biological limitations of the brain. ACT-R 6 includes
the computations. It is for this reason that some have argued          long-term declarative memory and perceptual-motor
that probabilities are "epistemologically inadequate"                  modules connected through limited-capacity buffers. Each
(McCarthy & Hayes, 1969).                                              module exposes a buffer, which contains a single chunk, to
   Instead of offering more criticisms, we wish to offer               the rest of the system. A chunk is a member of a specific
solutions. The issue with constraints is that, even if                 chunk type, and consists of a set of type-defined slots
Bayesian models do not have too many parameters, there is              containing specific values.
effectively unlimited freedom in setting priors and the                   The flow of information is controlled by a procedural
hypothesis space (which greatly influences the performance             module implemented using a production system, which
on the model). What is needed is a way to constrain the                operates on the contents of the buffers and uses a mix of
generation of the initial probability space and set of                 parallel and serial processing. Modules may process
algorithms to carry out inference for a set of models. For             information in parallel with one another. So, for instance,
                                                                       the visual and motor modules may both operate at the same
                                                                   3540

time. However, there are two serial bottlenecks in process.        production system can generate novel productions (through
First, only one production may execute during a cycle.             proceduralization) using production compilation. In
Second, each module is limited to placing a single chunk in        addition, the choice of which production to fire (conflict
a buffer.                                                          resolution) also constrains which chunks (i.e., hypotheses)
   Each production consists of if-then condition-action pairs.     will be recalled (limiting the hypothesis space), and are also
Conditions are typically criteria for buffer matches, while        subject to learning via production utilities.
the actions are typically changes to the contents of buffers          In production compilation, a new production is formed by
that might trigger operations in the associated modules. The       unifying and collapsing the conditions of the production,
production with the highest utility is selected to fire from       and possibly automatizing a given memory retrieval. This
among the eligible productions. In general, multiple               new production has a unique utility and can be considered
production rules can apply at any point. Production utilities,     an extension of the hypothesis space; perhaps with enough
learned using a reinforcement learning scheme, are used to         learning compiled productions are more analogous to
select the rule that fires.                                        overhypotheses (Kemp, Perfors, and Tenenbaum, 2007).
   When a retrieval request is made to declarative memory             In summary, the conflict resolution and production
(DM), the most active (highest Ai) matching chunk is               utilities algorithms both constrain the hypothesis space and
returned:                                                          provide an algorithm for learning how the space will evolve
                                                                   given experience, constrained within the bounds of a
where activation Ai is computed as the sum of base-level           neurally-consistent functional cognitive architecture. This
activation (Bi), spreading activation (Si), partial matching       bridges Bayesian inference from a computational-level
(Pi) and stochastic noise (εi). Spreading activation is a          framework within an algorithmic-level architecture.
mechanism that propagates activation from the contents of          However, this argument for constraint is not without
buffers to declarative memory proportionally to the strength       criticisms (some of which will be addressed in the
of association between buffer contents and memory chunks.          Discussion). As an example of increasing constraints and
Partial matching is a mechanism that allows for chunks in          grounding mechanisms, we will now present an updated
memory that do not perfectly match a retrieval request to be       associative learning mechanism in ACT-R.
recalled if their activation overcomes a similarity-based
mismatch penalty.
                                                                                     Associative Learning
                                                                   Associative learning - the phenomenon by which two or
ACT-R as a Constrained Bayesian Architecture                       more stimuli are associated together - is ubiquitous in
ACT-R’s sub-symbolic activation formula approximates               cognition, describable as both a micro (Hebbian learning
Bayesian inference by framing activation as log-likelihoods,       between neurons) and macro (classical and operant
with base-level activation (Bi) as the prior, the sum of           conditioning) feature of behavior. Associative learning is a
spreading activation and partial matching as the likelihood        flexible and stimulus-driven mechanism which instantiates
adjustment factor(s), and the final chunk activation (Ai) as       many major phenomena such as classical conditioning,
the posterior. The retrieved chunk has an activation that          context sensitivity, non-symbolic spread of knowledge, and
satisfies the maximum likelihood equation.                         pattern recognition (including sequence learning and
   ACT-R provides the much needed constraint to the                prediction error). At the neural level, associative learning is
Bayesian framework through the activation equation and             the process by which cells that fire together, wire together.
production system. The calculation of base-levels (i.e.,              In its simplest form, Hebbian learning can be described
priors) occurs within both a neurally- and behaviorally-           as:               , where Wij is the synaptic strength of the
consistent equation:                                               connection between neurons i and j, and xi and xj are the
                                                                   inputs to i and j (Hebb, 1949). When both i and j are active
where n is the number of presentations for chunk i, tj is the      together, Wij is strengthened. While the traditional Hebbian
time since the jth presentation, and d is a decay rate             rule was unstable due to a lack of mechanisms to control for
(community default value is .5). This formula provides for         weakening of connections (i.e., long-term depression; LTD)
behaviorally-relevant memory effects like recency and              or to set a maximum state of activation (i.e., to implement a
frequency, while providing a constrained mechanism for             softmax equation; Sutton & Barto, 1998), several variants
obtaining priors (i.e., driven by experience). Thus, we can        have addressed these issues to provide a stable learning rule.
address the constraint criticism (1) through this well                At a macro level, associative learning is a mechanism
justified mechanism (see Anderson et al., 2004).                   where, when a stimulus is paired with a behavior, future
   In addition, the limitations on matching in the production      presentation of the stimulus primes this behavior. Models of
system provide constraints to the hypothesis space and kinds       classical conditions are a common macro-level application
of inferences which can be made. For instance there are            of associative learning. At this level, associative learning
constraints on the kinds of matching that can be                   allows animals and humans to predict outcomes based on
accomplished (e.g., no disjunction, matching only to               prior experience with learning mediated by the degree of
specific chunk types within buffers) and, while user-              match between the predicted outcome and the actual result
specified productions can be task-constrained, the                 (Rescorla & Wagner, 1974; Pearce & Hall, 1980).
                                                               3541

   While macro-level models are normally processed at a               and/or production fires, and is magnified by the log-
more symbolic level, micro-level sub-symbolic processing              likelihood calculation which penalizes the inevitable low
can capture statistical regularities from the environment             context ratio in long-running models.
without recourse to explicitly coding context information.            Spreading Activation in ACT-R 6
There is evidence that humans do not explicitly encode                Due to the abovementioned issues with scalability,
positional information when sequentially recalling a list of          associative learning was deprecated in ACT-R and a simpler
items, yet ACT-R’s model of list memory required explicit             spreading activation function was implemented that does not
position information to drive recall (Anderson et al., 1998).         activation, but instead spreads a fixed amount of activation:
   Despite being a pervasive factor of human intelligence,
associative learning is no longer directly implemented in             where smax is a parameterized set spread of association
ACT-R. One reason for this absence is due to difficulties in          (replacing the m term from the previous equation), and fanji
scaling models in its Bayesian implementation of                      is the number of chunks associative with chunk j (the n
associative strengths, which treated both the activation              term). Fanji is traditionally the number of times chunk j is a
strength and associative strength of knowledge elements               slot value in all chunks in DM and represents interference.
(e.g., chunks) as likelihoods of successful recall.                      With a default smax usually between 1.5 and 2 (Lebiere,
Bayesian Associative Learning Rule                                    1999), this means that a chunk can appear as a value in 6-8
Associative learning was deprecated in ACT-R 5 due to a               chunks before becoming inhibitory. In the context of a
lack of scalability in spreading activation as the number of          modeling a single session psychology experiment this may
chunks in a model increased and as new productions fired              be reasonable, but if ACT-R models long-term knowledge
(i.e., new contexts generated). Instead, a simpler spreading          effects, then Sji will become inhibitory for most chunks.2
activation algorithm was used. The reason for this was that              As previously discussed, associative learning is a
the Bayesian formula used to calculate strength of                    ubiquitous mechanism in both human and animal cognition,
association (Sji) led to some unintended consequences which           which serves as a kind of statistical accumulator which is
would render larger and longer-running models unstable.               applicable at both the micro (neural) and macro (cognitive)
   In ACT-R 4/5, the strength of association (Sji) represented        behavioral level. It seems that to abstract this essential
the log likelihood ratio that chunk Ni was relevant given             learning mechanism, we are losing out on the exact kind of
context Cj:                                                           human-model comparisons that might provide evidence for
                                                                      these much-needed constraints. Perhaps, it is in part for this
                                                                      reason that ACT-R (and other cognitive architectures) have
                                                                      had their explanatory power limited due to a lack of newer,
When Cj is usually not in the context when Ni is needed,              more complex models being built from extant successful
            will be much smaller than             and the Sji will    models (ACT-R Workshop, 2012).
be very negative because the log-likelihood ratio will                   To both reconcile the difficulties in previous
approach 0. In a long-running model, these chunks may                 implementation of associative learning and show how we
have been recalled many times without being in context                can constrain Bayesian-compatible inference in a cognitive
together, leading to strongly inhibitory Sji.                         architecture, we will now present a Hebbian-inspired
   Once a connection was made, the initial prior Sji was set          associative learning rule influenced by spike-timing
by the following equation:                                            dependent plasticity (STDP; Caporale & Dan, 2008).
                                                                      Hebbian-Inspired Associative Learning Rule
where m is the total number of chunks in memory and n is              The major issues with the Bayesian associative learning rule
the number of chunks which contain the source chunk j.                were the reliance on ratio-driven log-likelihoods and the fact
This ratio is an estimation of the likelihood of retrieving           that context (Cj) was a global term which altered Sji
chunk i when j is a source of activation. As a convenience            whenever a new chunk was created and whenever a
unconnected chunks were set at 50% likelihood.1                       production fired. This is due to the fact that low log-
   As can be seen from the previous two equations, given              likelihoods become strongly inhibitory, and the generation
sufficient experience or sufficient numbers of chunks in the          of context-based ratios necessitates low-likelihoods in a
model, these context-ratio equations specify that Sji values          long-running model. In short, this Bayesian account based
will become increasingly and unboundedly negative as more             on the Naïve Bayes Assumption does not adequately capture
chunks are present in the model and more unique contexts              some of the features of associative learning such as locally-
experienced. This is a direct result of Sji reflecting the            driven strengthening of associations and bounded decay.
statistics of retrieval of chunk j given that source i is in the         An alternative framework is to eliminate the ratio function
context, and is a version of the Naïve Bayes Assumption.              and remove the global nature of context, while also moving
   The issue is with the ratio-driven global term (Cj) which          to a frequency-based algorithm instead of a probability-
alters Sji values for a chunk whenever a new chunk is added           based algorithm. The former removes the aforementioned
   1                                                                     2
     Before Cj appears in a slot of Ni, the total probability of           After presenting this at the 2012 ACT-R Workshop, a flag was
retrieving a chunk unconnected to Cj is 0 (which means Sji = -∞).     written in ACT-R to set a floor of 0 in the Sji computation.
                                                                  3542

issues with scalability, while the latter eliminates                   and necessary mechanism to account for refractory periods
                    ∞, where x is the likelihood. That said, a         in neural firings.
benefit of using log-likelihood in probability space is that              An advantage of this Hebbian-inspired implementation is
there is no need to squash activation strength (e.g., use a            that it avoids the inhibitory associations of low log-
softmax rule to keep Sji values from overwhelming Bi in the            likelihoods, but the learning rule requires a form of softmax
activation equation) because likelihoods cannot go above               equation (either driven by expectation or more simple
100% while frequency-based Hebbian activations can                     decay/inhibition) to keep Sji values from overwhelming
theoretically grow unbounded. Thus, the switch to                      base-level Bi (i.e., from the likelihood overwhelming the
frequencies is about reshaping the range of Sji values and             prior, in Bayesian terms). At the micro/neural level, softmax
making Sji independent of changing global context.                     approximates a maximum likelihood, while at a macro/
   Basing associative learning on frequencies also adds a              behavioral level, softmax simulates learning as expectation
more Hebbian flavor to the algorithm. Learning, rather than            violation. In Bayesian terms, the more active (c.f., likely)
being a global property of the system (as in the Bayesian              the existing association between chunks A → B, then the
mechanism) is instead a local property based on co-                    less marginal increase in Sji when chunk A is a source in the
occurrence and sequential presentation. As previously                  retrieval of chunk B.
discussed, our Hebbian-inspired mechanism is influenced by                There are several beneficial effects from this kind of
STDP. Unlike traditional Hebbian implementations which                 implementation. The first is that the mechanism is more
simply give a bump to association so long as the pre-                  balanced and geared towards specializing associative
synaptic and post-synaptic neurons both fire within a given            activations rather than just increasing all activations. Thus,
temporal window, in STPD if the pre-synaptic neuron fires              the mechanism is more stable as it grows (i.e., it will not
before the post-synaptic then the association is strengthened          tend towards all associations becoming either strongly
(long-term potentiation; LTP). Conversely, if the post-                excitatory or inhibitory; Sji doesn’t vary with number of
synaptic neuron fires before the pre-synaptic then the                 chunks in memory). Second, since the retrieved chunk self-
association is inhibited (long-term depression; LTD).                  inhibits, this reduces the chance that it will be the most
   This theory of neural plasticity was adapted to our                 active chunk in the following retrieval request (due to
modeling approach by assuming that the sources of                      recency effects), which can cause models to get into self-
activation from chunks in buffers act similarly to pre-                feedback loops. In short, this inhibition leads to a natural
synaptic firings, and the set of chunks in the buffers at the          refractory period for retrieving a chunk. Third, by self-
time the new chunk is retrieved is similar to post-synaptic            inhibiting and spreading activation to the next context, it
firings. The associative learning rule fires when a request is         provides a forward momentum for the serial recall of
made to retrieve a chunk from declarative memory. First, a             chunks. Combined with recency and frequency of base
positive phase occurs (LTD; or Hebbian) where the current              level, this provides a mechanism for automatic serial recall
contents of the buffers spread activation and a new chunk is           of lists without the need for coding of explicit positional
retrieved. The association between this new chunk and the              information (something required in prior models of list
sources of activation are strengthened according to standard           memory; Anderson et al., 1998) and marking of previously
Hebbian learning rules. However, once this new chunk is                retrieved chunks through finst-like mechanisms. The
placed in the retrieval buffer, a negative phase occurs (LTP;          uniqueness of the subsequent context drives order effects.
or anti-Hebbian) where the retrieved chunk will negatively                There are still, however, several design decisions and
associate with itself and with its context. In formal terms:           more empirical justification required in order to strengthen
                                                                       the constraint argument. Currently, the softmax learning
                                                                       term is based on ACT-R’s base-level learning equation.
                                                                       However, several candidate equations need to be compared
where      is a Hebbian learning term,                      is the     against human performance data to determine the best
context of source chunks             at the time of the retrieval      possible match. Furthermore, existing models of list
request for chunk Ni, and                       is the context of      memory and sequence learning need to be re-envisioned in
chunks         after chunk Ni has been retrieved. Note that            terms of the new associative learning mechanism.
only changes in context will have a net ΔSji due to the                   In summary, this balanced Hebbian/anti-Hebbian learning
balanced positive and negative learning phase. Furthermore,            mechanism avoids the issues of scalability (e.g., runaway
these associations are not symmetric (i.e., Sji ~= Sij).               activations) that have been associated with prior
   This balanced Hebbian/anti-Hebbian mechanism is geared
                                                                       implementations of associate learning in ACT-R. In
towards developing a local, scalable learning rule while
                                                                       addition, this mechanism is constrained by neural
maximizing neural plausibility by incorporating a negative
                                                                       plausibility constraints, can still be discussed in Bayesian-
inhibitory learning phase. We argue that this inhibitory
                                                                       compatible terms, and fits within the Bayesian description
phase, while seemingly unintuitive3, is actually a relevant
                                                                       of ACT-R’s sub-symbolic activation.
   3
                                                                                                Discussion
     Some have found the notion of a chunk being self-inhibitory
very unintuitive, because it conflicts with the idea that a chunk      This paper has described how a functional cognitive
should be maximally similar to itself and self-activating.             architecture can constrain Bayesian inference by tying
                                                                   3543

neurally-consistent mechanisms into Bayesian-compatible                    Anderson, J. R., and Lebiere, C. (1998). The atomic components of
sub-symbolic activations. This combination of grounded                       thought, Erlbaum, Mahwah, NJ.
                                                                           Bowers, J. S., & Davis, C. J. (2012). Bayesian Just-So Stories in
implementation- and algorithmic-level functions into                         Psychology and Neuroscience. Psychological Bulletin, 138 (3) 389-
cognitive-level Bayesian inference defuses many criticisms                   414.
of Bayesian inference, and provides a launch-point for                     Caporale, N., & Dan, Y. (2008). Spike Timing-Dependent Plasticity: A
future research into constraining the Bayesian framework                     Hebbian Learning Rule. Annual Review of Neuroscience, 31, 25-46.
across all three levels of Marr’s hypothesis. An example of                Chater, N., Oaksford, M., Hahn, U., & Heit, E. (2010). Bayesian
this research was provided by examining a novel                              models of cognition. WIREs Cognitive Science, 1, 811-823.
                                                                           Danks, D., Griffiths, T. L., & Tenenbaum, J. B. (2003). Dynamical
implementation for associative learning in ACT-R. In                         casual learning. In S. Becker, S. Thrun, & K. Obermayer (Eds.),
addition to the sub-symbolic layer being driven by Bayesian                  Advances in Neural information processing system. MIT Press:
mathematics, it is also compatible with neural localization                  Cambridge, MA.
and the flow of information within the brain.                              Dayan, P., Kakade, S., & Montague, P. R. (2000). Learning and
   It has been argued that ACT-R’s numerous parameters                       selective attention, Nature Neuroscience, 3, 1218-1223.
don’t really provide the kind of constraint necessary to                   Deneve, S. (2008). Bayesian spiking neurons II: Learning. Neural
                                                                             Computation, 20, 118-145.
avoid the criticisms discussed in this paper (Tenenbaum et                 Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and cognitive
al., 2011). However, the use of community and research-                      architecture: a critical analysis. Cognition, 28 (1), 3-71.
justified default values, the practice of removing parameters              Griffiths, T. L., Chater, N., Norris, D., & Pouget, A. (2012). How the
by developing more automatized mechanisms (such as the                       Bayesians got Their Beliefs (and What Those Beliefs Actually Are):
associative learning replacing spreading activation), and the                Comment on Bowers and DAvids (2012). Psychological Bulletin,
                                                                             138 (3), 415-422.
development of common modeling paradigms mitigates                         Kemp, C., Perfors, A., & Tenenbaum, J. B. (2007). Learning
these criticisms by limiting degrees of freedom in the                       overhypotheses with hierarchical Bayesian models. Developmental
architecture and thus constraining the kinds of models that                  Science, 10 (3), 307-321.
can be developed and encouraging their integration. In                     Krueger, L. E. (1984). Perceived numerosity: A comparison of
summary, the evolution of the architecture is not a process                  magnitude production, magnitude estimation, and discrimination
of invalidation, but instead moving towards more                             judgments. Perception and Psychophysics, 35(6), 536-542.
                                                                           Kruschke J. K. (2008). Bayesian approaches to associative learning:
constrained and more specific explanations.                                  From passive to active learning. Learning and Behaviour, 36 (3),
   As we have argued, the associative learning mechanism is                  210-226.
an attempt to increase constraint within the architecture and              Lebiere, C. (1999). The dynamics of cognition: An ACT-R model of
promote a broader explanatory power to numerous cognitive                    cognitive arithmetic. Kognitionswissenschaft., 8 (1), pp. 5-19.
phenomena. This mechanism is geared towards specializing                   Ma, W. J., Beck, J. M., Latham, P. E., & Pouget, A. (2006). Bayesian
associative strength to capture both symbolic and non-                       inference with probabilistic population codes. Nature Neuroscience,
                                                                             9, 1432-1438.
symbolic associative learning. A major contribution of this                O’Reilly, R. (1998). Six principles for biologically based
mechanism is its balance between Hebbian (LTP) and anti-                     computational models of cortical cognition. Trends in Cognitive
Hebbian (LTD) learning at each retrieval request, which                      Science, 2 (11), 455-462.
provides numerous benefits over traditional Hebbian and                    O’Reilly, R., Hazy, T. E., & Herd, S. A. (2012). The Leabra Cognitive
Bayesian implementations.                                                    Architecture: How to Play 20 Principles with Nature and Win! In S.
                                                                             Chipman (Ed) Oxford Handbook of Cognitive Science, Oxford:
                      Acknowledgments                                        Oxford University Press.
This work was conducted through collaboration in the                       Pearce J. M. and Hall G. (1980). A model for Pavlovian learning:
                                                                             Variations in the effectiveness of conditioned but not of
Robotics Consortium sponsored by the U.S Army Research                       unconditioned stimuli. Psychological Review, 87, 532-552.
Laboratory under the Collaborative Technology Alliance                     Rescorla, R. A. & Wagner, A. R. (1972). A theory of Pavlovian
Program, Cooperative Agreement W911NF-10-2-0016; and                         conditioning: Variations in the effectiveness of reinforcement and
by Intelligence Advanced Research Projects Activity via                      nonreinforcement. In A.H. Black & W.F. Prokasy (Eds.), Classical
DOI contract number D10PC20021. The views and                                Conditioning II, Appleton-Century-Crofts.
                                                                           Roberts, S., & Pashler, H. (2000). How persuasive is a good fit? A
conclusions contained herein are those of the authors and
                                                                             comment on theory testing. Psychological Review, 107, 358-367.
should not be interpreted as necessarily representing the                  Shafir, E. & Tversky, A. (1992) Thinking through uncertainty:
official policies or endorsements, either expressed or                       nonconsequential reasoning and choice. Cognitive Psychology 24:
implied, of IARPA, DOI, or the U.S. Government.                              449-474.
                                                                           Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An
                             References                                      Introduction. MIT Press, Cambridge, MA.
Anderson, J. R. (2007). How can the human mind occur in the physical       Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006). Theory-based
   universe? New York: Oxford University Press.                              Bayesian models of inductive learning and reasoning. Trends in
Anderson, J. R., & Betz, J. (2001). A hybrid model of categorization.        Cognitive Science, 10, 309–318.
   Psychonomic Bulletin & Review, 8, 629-647.                              Varma, S. (2011). Criteria for the Design and Evaluation of Cognitive
Anderson, J. R., Bothell, D., Byrne, M. D., Douglass, S., Lebiere, C.,       Architectures. Cognitive Science, 35 (7), 1329-1351.
   Qin, Y. (2004). An integrated theory of Mind. Psychological             Xu, F., & Tenenbaum, J. B. (2007). Word Learning as Bayesian
   Review, 111, 1036-1060.                                                   Inference, Psychological Review, 114 (2), 245-272.
Anderson, J. R., Bothell, D., Lebiere, C. & Matessa, M. (1998). An         Yuille, A., & Kersten, D. (2006). Vision as Bayesian inference:
   integrated theory of list memory. Journal of Memory and Language,         analysis by synthesis? Trends in Cognitive Science, 10, 301–308.
   38, 341-380.
                                                                       3544

