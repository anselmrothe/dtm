UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Role of Scene Gist and Spatial Dependency among Objects in the Semantic Guidance of
Attention
Permalink
https://escholarship.org/uc/item/0gb7c87k
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Wui, Chi-Chien
Wang, Hsueh-Cheng
Pomplun, Marc
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

    The Role of Scene Gist and Spatial Dependency among Objects in the Semantic
                                                       Guidance of Attention
                                               Chia-Chien Wu (chiachie@cs.umb.edu)
                                         Hsueh-Cheng Wang (hchengwang@gmail.com)
                                                 Marc Pomplun (marc@cs.umb.edu)
                               Department of Computer Science, University of Massachusetts at Boston
                                        100 Morrissey Boulevard, Boston, MA, 02125-3393, USA
                              Abstract                                    next. For example, if observers are aware that the viewed
                                                                          image is a kitchen, they may only attend the regions nearby
  A previous study (Hwang et al., 2011) found evidence for
  semantic guidance of visual attention during the inspection of          the counter or sink, where most of the kitchenware is likely
  real-world scenes, i.e., an influence of semantic relationships         located.
  among scene objects on overt shifts of attention. In particular,           The ways in which people acquire such global contextual
  the results revealed an observer bias toward gaze transitions           information is not well understood. Torralba, Oliva,
  between semantically similar objects. However, these results            Castelhano and Henderson (2006) found that observers
  are not necessarily indicative of semantic processing of                could extract some global scene properties - referred to as
  individual objects but may be confounded by knowledge of
  the scene gist, which does not require object recognition
                                                                          scene gist - without recognizing individual objects and use
  (Torralba et al., 2006), or by known spatial dependency                 this information to guide their attention and eye movements.
  among objects (Oliva & Torralba, 2007). To examine the                      Even when the global context, which usually comes from
  mechanisms underlying semantic guidance, in the present                 visual background information, is missing, it is still possible
  study, subjects were asked to view a series of displays with            to learn some context of the scene. Chun (2000) showed
  the scene gist removed and spatial dependency varied. Our               that some contextual information can be learned merely by
  results confirm the previous finding of semantic guidance and           the typical arrangement of elements and affect the
  show that it is not entirely due to either the effect of scene gist
  or the spatial dependency among objects. Even without scene             deployment of attention. Oliva and Torralba (2007) also
  gist or spatial dependency, subjects still retrieved semantic           found that spatial dependency among objects could provide
  information to guide their attention. This strategy may                 different contextual information about a scene. For example,
  facilitate scene understanding and object memorization.                 a chair may be expected to be located behind a table, or a
  Keywords: Attention, semantics, eye movements, visual                   fork may be expected to be next to a spoon.
  guidance, real-world scenes.                                               In summary, both the scene gist and the spatial
                                                                          dependency among scene objects may have caused a bias in
                          Introduction                                    observers’ gaze patterns that could explain the results of
                                                                          Hwang et al. (2011) without the need for semantic analysis
Real-world scenes contain rich information, which usually
                                                                          of extrafoveal scene objects. If gist, object dependency, or
is not thoroughly processed during natural viewing.
                                                                          both were entirely responsible for the effect observed in that
Therefore, the way in which the visual system deploys the
                                                                          study, the concept of semantic guidance would not be a new
limited attention resources is crucial for effective vision and
                                                                          phenomenon but rather a bias introduced by already known
has drawn huge interest over the last two decades. The
                                                                          factors. The aim of the present study was to discern the
guidance of attention based on the features of stimuli in the
                                                                          contributions of scene gist, object dependency, and semantic
visual environment has been well investigated in both its
                                                                          object analysis to semantic guidance in order to address this
bottom-up (Itti & Koch, 2001; Koch & Ullman, 1985) and
                                                                          problem.
top-down aspects (Hayhoe et al, 2003; Hwang, Higgins &
                                                                             To study the influence of spatial dependency among scene
Pomplun, 2009; Pomplun, 2006).
                                                                          objects, we employed the LabelMe object annotated image
  Visual attention is not only affected by factors based on
                                                                          data base (Russell, Torralba, Murphy, & Freeman, 2008) in
the overt visual appearance, but also by inherent factors,
                                                                          which scene images were manually segmented into
such as meaning and semantic relations among objects.
                                                                          annotated objects by volunteers. In addition, the locations
Hwang, Wang and Pomplun (2011) found that during
                                                                          of objects are provided as coordinates of polygon corner and
natural scene viewing, humans tend to bring their gaze to
                                                                          all objects are labeled with English words or phrases. It
the objects that are semantically similar either to the
                                                                          provides an excellent opportunity for not only segregating
currently fixated one or to the specified search target. This
                                                                          each object from its scene, but also shifting the object’s
result, however, may have been confounded by the
                                                                          coordinates to any desired location in the image.
observers’ knowledge of the global scene context. That is,
                                                                             One way to eliminate potential influence of scene gist on
instead of considering the semantic relation between the
                                                                          attentional guidance is to remove all background
currently fixated object and the objects located in the
                                                                          information and only keep segregated objects in the scene.
extrafoveal visual field, observers may simply use their
                                                                          We used the resulting images in an experimental condition
knowledge about the scene type to decide where to look
                                                                      1617

referred to as ‘fixed condition’. This procedure effectively
removes the relation between scene and objects as defined
by Torralba et al. (2006). For example, it is easier to predict
where a plate is located in a scene when the plate is shown
on a dining table than when it is shown by itself. The
spatial dependency among objects, however, is still retained
when the background information is excluded. For instance,
it is possible to predict the likely location of a glass merely
based on the location of a seen plate in a scene, even when
no context is provided. To remove the spatial dependency
among objects as well, we created another set of stimuli
(‘scrambled condition’) by generating scenes without
background as in the fixed condition and then randomly
shifting the objects within the scene.
   If the semantic guidance found in the previous study
(Hwang et al., 2011) were due to the spatial dependency
among objects, this effect should be eliminated once the
background information and spatial arrangement are
removed. On the other hand, if observers are able to use
conceptual semantic information between objects to guide
attention, their gaze transitions should still show an above-
chance semantic relevance.
                           Method
Subjects
Ten subjects, aged between 19-40 years old, were tested.
All had normal or corrected to normal vision and were naïve
                                                                    Figure 1: Original scene (top) and a sample trial (bottom).
as to the purpose of the study. Each subject received a $10
                                                                    The upper panel shows the original scene used to generate
honorarium.
                                                                    stimulus displays. The scene would be used to generate an
Apparatus                                                           image with objects at same coordinates (fixed condition)
                                                                    and an image with objects at randomly selected locations
Eye movements were tracked and recorded using an SR                 (scrambled condition). During each trial, the created image
Research EyeLink-2k system. Its sampling frequency was              was presented for 5 seconds. After the stimulus image
set to 1000 Hz. Stimuli were presented on a 22-inch                 disappeared, a word was presented and subjects had to
ViewSonic LCD monitor. Its refresh rate was set to 75 Hz            report whether the indicated object had been shown in the
and its resolution was set to 1024 x 768 pixels. Participant        previous display.
responses were entered using a keyboard.
Stimulus display                                                    Procedure
A total of 60 images (1024 x 768 pixels) were generated.            Subjects were instructed to inspect the scenes and memorize
Each image was composed of 13 to 15 objects selected from           them for the subsequent object recall test (see Figure 1,
a real-world scene from the LabelMe database                        bottom panel). Each image was presented for 5 seconds.
(http://labelme.csail.mit.edu). The selected scenes included        After the image had disappeared, an English word was
home interiors, landscapes and city scenes. Objects of              shown and subjects were asked whether the object indicated
extreme size (small or large) were not chosen as scene              by the word had been shown in the previous scene. Subjects
objects. To remove the scene gist or other global regularity        responded by pressing one of two possible keys on a
from the scene, all objects were segregated from the image          keyboard. If they believed the indicated object was shown
and were pasted on a grey canvas. Each object was placed            in the previous image, they would press the left arrow key.
at either the same coordinates as in the original scene, which      Otherwise, they would press the right arrow key. The next
was referred to as ‘fixed condition’, or at randomly selected       trial would begin once subjects made a response. Subjects
locations on the canvas, referred to as ‘scrambled condition’.      performed a total of 60 trials (30 trials each in the fixed and
In the scrambled condition, different objects were placed           scrambled conditions). Each scene was only presented once
manually to avoid overlap and clutter (see Figure 1 for an          to each subject, either in the fixed condition or in the
example).                                                           scrambled condition.
                                                                1618

                      Data Analysis                               higher-level conceptual semantic similarity, regardless of
                                                                  any geometrical relation, functional relation or visual
Assigning fixations to objects                                    relation.
Since all images excluded the global contextual information
                                                                                 Table 1: Sample LSA cosine values
by only leaving the selected objects on a grey canvas, some
fixations may land on the blank area rather than on any
object in the image. When this happened, we assumed this              Label 1               Label 2                   Cosine
fixation was aimed at the nearest object, i.e., the one whose         -                     -                         -
center had the shortest Euclidean distance to the current
                                                                      AIRPLANE              HELICOPTER                0.62
fixation location.
                                                                      AIRPLANE              TOY TRAIN                 0.28
Latent Semantic Analysis                                              AIRPLANE              PICTURE                   0.14
Similar to the original semantic guidance study (Hwang et             AIRPLANE              PILLOW                    0.03
al., 2011), we used Latent Semantic Analysis (referred to as
                                                                      -                     -                         -
LSA; Landauer & Dumais, 1997) to serve as a quantitative
measure of semantic similarity between objects. LSA is
                                                                     To compute semantic similarity for each pair of object
able to extract and represent the contextual usage-meaning
                                                                  labels in our experiment, a web-based LSA tool, LSA@CU
of words by statistical computations applied to a large
                                                                  (http://lsa.colorado.edu), developed at the University of
corpus of text. The basic premise in LSA is that the
                                                                  Colorado at Boulder, was used. This tool was set to create a
aggregate contexts in which a word does or does not appear
                                                                  semantic space from general readings up to 1st year college
provide a set of mutual constraints to deduce the word’s
                                                                  with 300 dimensions. Based on this space, we computed
meaning (Landauer, Foltz, & Laham, 1998). The greater the
                                                                  semantic similarity as the LSA cosine value, ranging
cosine value, the higher is the semantic similarity. Since
                                                                  between 0 and 1, for each object label compared to all other
annotated objects in LabelMe have descriptive text labels,
                                                                  objects’ labels for the same image.
their semantic similarity can be estimated by calculating
cosine values for the labels of object pairs.
   LSA similarity computation can be described as follows:
                                                                  Measuring semantic guidance
First, an occurrence matrix is constructed from a large           In this study, the semantic guidance effect was defined as
corpus of text, where each row typically stands for a unique      the extent to which the semantic relation/similarity between
word, and each column stands for a document, which is             the currently fixated object and the other objects in the
typically a collection of words. Each cell contains the           scene influences the choice of the next fixated object. In
frequency with which the word occurred in the document.           order to compute this effect quantitatively, the computation
Subsequently, each cell frequency is normalized by an             had to follow each subject’s eye movements. Since we were
information-theoretic measure.             However, it is         interested in the effect of semantic similarity on gaze
computationally inefficient to operate with this very high-       transitions, i.e., which object would be inspected next, only
dimensional matrix. Therefore, a form of factor analysis          eye movements that transitioned between distinct objects
called Singular Value Decomposition (SVD; see Berry,              were analyzed. For the starting point of each of these
Dumais, & Obrien, 1995) is applied to reduce the matrix to        transitions, a semantic landscape was generated based on the
a lower-dimensional vector space called ‘semantic space’.         LSA cosine value between the labels of the currently fixated
LSA can still estimate the semantic similarity of two words       object and each other object in the scene, as shown in Figure
even when they never co-occur in the same document (Jones         2. The semantic landscapes, excluding the area occupied by
& Mewhort, 2007; Landauer & Dumais, 1997).                        the currently fixated object, were normalized so that the sum
   Every term, every document, and every novel collection         of all activation was one. With the normalized semantic
of terms has a vector representation in the semantic space.       landscape, the Receiver Operating Characteristic (ROC)
Thus, the pair-wise semantic similarity between any of them       value was computed in a similar way as it was done in
can be calculated as the cosine value of the angle between        previous studies (Hwang et al., 2009; Tatler, Baddeley &
the two corresponding vectors, with greater cosine value          Gilchrist, 2005). Overall, each fixation would build its own
indicating greater similarity. Table 1 shows examples of          semantic landscape as a predictor of the target point of the
LSA cosine values for various object labels used in the           next transition. All ROC values computed along scan paths
LabelMe scene image ‘‘Child4’’ (see Figure 1) in terms of         were averaged across scenes to obtain the extent of semantic
the reference object label ‘‘AIRPLANE’’. This label has,          guidance during the inspection of a scene. If eye movements
for instance, a higher cosine value (greater semantic             were exclusively guided by semantic information, this
similarity) with ‘‘HELICOPTER’’ (0.62) than with                  average ROC value should be close to one. If there were no
‘‘PILLOW’’ (0.03). This difference indicates that in the text     semantic effect on eye movements at all, the average ROC
corpus, ‘‘AIRPLANE’’ and ‘‘HELICOPTER’’ occur in                  value should be close to 0.5, indicating prediction at chance
more similar contexts than ‘‘AIRPLANE’’ and ‘‘PILLOW’’.           level.
One of the nice features of LSA is that it can quantify
                                                              1619

                                                                   object data from scene n+1, and the eye movement data
                                                                   recorded from the last scene against the object data from the
                                                                   first scene. This mismatch conserved the spatial distribution
                                                                   of both the scene objects and the observers’ fixations and
                                                                   therefore the proximity effect (at least in the fixed condition
                                                                   in which the coordinates of selected objects were not
                                                                   changed). This method allowed us to examine whether any
                                                                   observed above chance level ROC value for the empirical
                                                                   data was simply caused by proximity, which would be
                                                                   indicated by ROC values in the dissociated case being
                                                                   similar to the actual ROC values).
                                                                                      Experimental Results
                                                                   Results showed that subjects recall performances were
                                                                   above chance level in both the fixed and scrambled
                                                                   conditions (Recall performance in the fixed condition, 79%,
                                                                   t(9) = 21.50, p < 0.05; recall performance in the scrambled
                                                                   condition: 70 %, t(9) = 4.36 , p < 0.05).
                                                                      As mentioned earlier, in order to examine semantic
Figure 2: Example of semantic landscapes. The currently            guidance, we computed ROC values for the two
fixated object is marked with an orange square. (a) The            experimental conditions (fixed vs. scrambled) for all three
original image that subjects inspected. (b) Semantic               data sets (empirical, random and dissociated). Figure 3
landscape during gaze fixation on the object labeled as            shows that the transitional semantic guidance values of
“AIRPLANE”. (c) Semantic landscape during gaze fixation            random fixations were close to 0.5 in both the fixed and
on the object labeled as “GLOBE”. (d) Semantic landscape           scrambled conditions. This result shows that the ROC
during gaze fixation on the object labeled as “STORAGE             computation was applied properly and the normalized
BOX”. As shown above, objects with conceptually higher             semantic landscapes used in our analysis were unbiased.
relevance – measured as greater sematic similarity to the             The ROC value in the fixed condition (ROC = 0.704 ±
currently fixated object - receive higher activation               0.14) was significantly higher than that in the scrambled
(brightness), for example, the helicopter in (a) shows a           condition (ROC = 0.65 ± 0.19), t(9) = 4.76, p < 0.05. ‘±’
higher activation due to the fixated object labeled as             here indicates a mean value and its standard error.
‘AIRPLANE’.                                                           This result suggests that the spatial dependency preserved
                                                                   in the fixed condition provided additional semantic
                                                                   information and facilitated semantic guidance. The ROC
Excluding potential confounds              by computing            value decreased when this spatial dependency was
control analyses                                                   eliminated by shuffling the locations of objects.
Following Hwang et al. (2011), to control for possible                Interestingly, in the scrambled condition, where the
confounds in the measurements of semantic guidance,                spatial dependency among objects was destroyed, the ROC
subjects’ ROC values computed from their empirical gaze            value of empirical transition between distinct objects was
transition data were compared with two control data sets: (1)      still substantially greater than both ROC values for the other
random fixations and (2) dissociated fixations. The random         two control cases. A one-way ANOVA showed that the
fixations were generated by replacing subjects’ fixation           effect was significant, F(2,27) = 51.61, p < 0.05. A post hoc
positions with randomly positioned coordinates in the scene.       Tukey test indicated that there was no difference between
This data set served as an unbiased test of ROC values.            the dissociated and random cases, p = 0.53. This finding
That is, since gaze transitions of the random data set were        shows that the proximity effect, at least in our experiment,
not affected by any other factor, we should always receive a       had no impact on semantic guidance.
chance level ROC value (ROC = 0.5).                                   Overall, the results indicate that, even without scene gist
   Furthermore, it is likely that any above-chance ROC             and the spatial dependency among objects, subjects were
value was simply caused by the proximity effect. This              still able to extract the semantic relevance between objects
effect is due to the previous finding (Hwang et al., 2011)         to guide their attention.
that semantically similar objects tend to be located closer to
each other and subjects’ saccades tend to be shorter than
gaze transitions in the random data set. To examine this
possible confound, subjects’ data were also compared with a
“dissociated” data set. The dissociated data were analyzed
using the eye movement data recorded in scene n against
                                                               1620

                                                                      Moreover, Hwang et al. (2011) also found an even greater
                                                                   effect of semantic guidance in a visual search task. That is,
                                                                   observers tend to fixate on the objects which are
                                                                   semantically similar to the specified target. Instead of using
                                                                   any verbal probe and search paradigm as they did, the
                                                                   present study used a natural viewing and memory task
                                                                   which was less constrained by cognitive goal and we still
                                                                   found a substantial effect of semantic guidance.
                                                                      Consequently, the question becomes how observers
                                                                   obtained this semantic information and how it influenced
                                                                   the guidance of attention. It is likely that extrafoveal visual
                                                                   processing may play a crucial role in enabling the semantic
                                                                   effect since observers had to recognize, at least partially, the
                                                                   objects in peripheral vision and processed the semantic
                                                                   relevance in the context of the currently fixated object.
                                                                   Kotowicz, Rutishauser and Koch (2010) found that, during
                                                                   visual search, observers already identified the extrafoveal
Figure 3: Transitional semantic guidance as measured by            target before fixating on it. We do not claim that in our task,
the ROC method in the fixed condition and the scrambled            observers were able to recognize the objects in the
condition. The red dashed line represents the chance level         extrafoveal field. At the very least, extrafoveal perception
(ROC = 0.5) and errors represent +/-1 standard error of the        may be used to increase the belief of what this object could
mean.                                                              be. Therefore, when contextual information was removed,
                                                                   people could still learn semantic information to help them
                                                                   determine where to fixate next by using the immediately
                        Conclusions                                acquired information from the current fixation and the
Hwang et al. (2011) found that, during scene inspection,           information accumulated from extrafoveal vision. Such a
observers tend to bring the line of sight to objects that are      strategy may facilitate scene understanding and
semantically relevant to the currently fixated object. Based       memorization. It is also possible that, instead of using
on these previous data alone, it cannot be ruled out that the      extrafoveal information, observers may construct their own
high semantic relevance of gaze transitions was contributed        scene representation merely based on the currently fixated
by scene gist information or by subjects’ prediction of local      object and update it during later fixations. This strategy
scene context based on the spatial layout of objects. In           may become useful when the extrafoveal information is not
other words, observers may not actually evaluate the               available or when the cost of processing it is too high.
semantics of peripheral objects for saccade target selection,         Overall, the current study showed that semantic guidance
and consequently, semantic guidance could not be                   of visual attention during the inspection of real-world scenes,
considered a new phenomenon but rather an effect caused            as reported by Hwang et al. (2011), is a novel phenomenon
by other known mechanisms.                                         that cannot be explained by effects of scene gist (e.g.,
   Our present results clarify the influence of these possible     Torralba et al., 2006) or spatial dependency among scene
confounds in the previous findings. In the fixed condition         objects (e.g., Oliva & Torralba, 2007) alone. It has been
in which the scene gist was removed, observers still showed        known that, in addition to the visual saliency from low-level
strong semantic guidance. This result demonstrates that            features, attention could be also driven by other particular
semantic guidance of visual attention in scene inspection is       classes of objects, such as faces (Judd, Ehinger, Durand, &
not entirely due to the scene gist. In fact, semantic guidance     Torralba, 2009) and texts (Wang & Pomplun, 2012). Our
in the present study was even higher than that measured in         result provides a new alternative class of features and
Hwang et al. (2011), suggesting that scene gist only plays a       suggests that the conceptual semantic effects may need to be
marginal, if any, role in semantic guidance.                       considered in the present model of attentional guidance.
   In the scrambled condition, in which both scene gist and        Further research on semantic guidance, its underlying
possible spatial dependency among objects were removed,            mechanisms, and its function is necessary before this
the effect of semantic guidance was slightly decreased but         concept can be integrated into existing models of visual
remained substantially higher than chance level. This              attention.
finding shows that the spatial arrangement of objects only
makes a small contribution to semantic guidance. Moreover,
these data reveal that even when the scene gist was excluded                           Acknowledgments
and the spatial dependency was removed, subjects could still       This research was supported by Grant Number R01
retrieve semantic information to guide their attention.            EY021802 from NIH to M.P.
                                                               1621

                          References                                 Torralba, A., & Oliva, A. (2003). Statistics of natural image
                                                                       categories. Network, 14(3), 391-412.
Berry, M., Dumais, S., & O'Brien, G. (1995). Using linear
                                                                     Torralba, A., Oliva, A., Castelhano, M. S., & Henderson, J.
   algebra for intelligent information retrieval. SIAM review,
                                                                       M. (2006). Contextual guidance of eye movements and
   37(4), 573-595.
                                                                       attention in real-world scenes: the role of global features
Chun, M. (2000). Contextual cueing of visual attention.
                                                                       in object search. Psychol Rev, 113(4), 766-786.
   Trends in Cognitive Sciences, 4(5), 170-178.
                                                                     Wang, H.C. & Pomplun, M. (2012). The attraction of visual
Hayhoe, M. M., Shrivastava, A., Mruczek, R., & Pelz, J. B.
                                                                       attention to texts in real-world scenes. Journal of Vision,
   (2003). Visual memory and motor planning in a natural
                                                                       12(6):26, 1–17,
   task. J Vis, 3(1), 49-63.
Hwang, A. D., Higgins, E. C., & Pomplun, M. (2009). A
   model of top-down attentional control during visual
   search in complex scenes. J Vis, 9(5), 25.1--2518.
Hwang, A. D., Wang, H.-C., & Pomplun, M. (2011).
   Semantic guidance of eye movements in real-world
   scenes. Vision Res, 51(10), 1192-1205.
Itti, L., & Koch, C. (2000). A saliency-based search
   mechanism for overt and covert shifts of visual attention.
   Vision Res, 40(10-12), 1489-1506.
Jones, M., & Mewhort, D. (2007). Representing word
   meaning and order information in a composite
   holographic        lexicon.     Psychological        Review;
   Psychological Review, 114(1), 1.
Judd, T., Ehinger, K., Durand, F., & Torralba, A. (2009).
   Learning to predict where humans look, IEEE
   International Conference on Computer Vision (ICCV),
   Kyoto, Japan, 2106 - 2113.
Koch, C., & Ullman, S. (1985). Shifts in selective visual
   attention: towards the underlying neural circuitry. Hum
   Neurobiol, 4(4), 219-227.
Kotowicz, A., Rutishauser, U., & Koch, C. (2010). Time
   course of target recognition in visual search. Frontiers in
   Human Neuroscience, 4, 12.
Landauer, T. K., Laham, D., & Derr, M. (2004). From
   paragraph to graph: latent semantic analysis for
   information visualization. Proc Natl Acad Sci U S A, 101
   Suppl 1, 5214-5219.
Landauer, T., & Dumais, S. (1997). A solution to Plato's
   problem: The latent semantic analysis theory of
   acquisition, induction, and representation of knowledge.
   Psychological Review; Psychological Review, 104(2), 211.
Landauer, T., Foltz, P., & Laham, D. (1998). An
   introduction to latent semantic analysis. Discourse
   processes, 25(2-3), 259-284.
Oliva, A., & Torralba, A. (2006). Building the gist of a
   scene: the role of global image features in recognition.
   Prog Brain Res, 155, 23-36.
Oliva, A., & Torralba, A. (2007). The role of context in
   object recognition. Trends Cogn Sci, 11(12), 520-527.
Pomplun, M. (2006). Saccadic selectivity in complex visual
   search displays. Vision Res, 46(12), 1886-1900.
Russell, B. C., Torralba, A., Murphy, K. P., & Freeman, W.
   T. (2008). LabelMe: A database and web-based tool for
   image annotation. International Journal of Computer
   Vision, 77, 157–173.
Tatler, B. W., Baddeley, R. J., & Gilchrist, I. D. (2005).
   Visual correlates of fixation selection: effects of scale and
   time. Vision Res, 45(5), 643-659.
                                                                 1622

