UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Watching Gestures during Learning about Movements with Dynamic Visualization Activates
the Human Mirror Neuron System: A fNIRS Study

Permalink
https://escholarship.org/uc/item/52j2v001

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Imhof, Birgit
Ehlis, Ann-Christine
Häußinger, Florian B.
et al.

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Watching Gestures during Learning about Movements with Dynamic Visualization
Activates the Human Mirror Neuron System: A fNIRS Study
Birgit Imhof (b.imhof@iwm-kmrc.de) a
Ann-Christine Ehlis (ann-christine.ehlis@med.uni-tuebingen.de) b
Florian B. Häußinger (florian.haeussinger@med.uni-tuebingen.de) b
Peter Gerjets (p.gerjets@iwm-kmrc.de) a
a

b

Knowledge Media Research Center, Schleichstrasse 6, 72076 Tuebingen, Germany
Department of Psychiatry & Psychotherapy, University Hospital Tuebingen, Calwerstr. 14, 72076 Tuebingen, Germany
Abstract

This study investigates whether viewing human gestures
facilitates learning about non-human biological movements
and whether correspondence between gesture and to-belearned movement is superior to non-correspondence.
Functional near-infrared-spectroscopy was used to address
whether gestures activate the human mirror-neuron-system
(hMNS) and whether this activation mediates the facilitation
of learning. During learning participants viewed triples of
visualizations (animation – gesture video – animation).
Results showed that for low-visuospatial-ability learners
corresponding gestures led to higher cortical activation in the
inferior-frontal cortex (part of the hMNS) and better learning
outcomes, whereas for high-visuospatial-ability learners the
type of gesture had no influence. Furthermore, results showed
that – if presented with non-corresponding gestures – only
low-visuospatial-ability learners who activated their inferiorparietal cortex (also part of the hMNS), improve their
learning. Thus, activating the hMNS facilitates learning about
movements and stimulating the hMNS via gestures seems to
be an adequate instructional strategy to enhance learning with
dynamic visualizations for low-visuospatial-ability learners.
Keywords:
Learning
about
movements;
visualizations; human mirror-neuron-system;
functional near-infrared-spectroscopy.

dynamic
gestures;

Learning about Movements
with Dynamic Visualizations
Many contents in the Natural Sciences as well as in other
domains, such as different sport disciplines or scene
perception, comprise the understanding of changes in space
over time. Dynamic visualizations can easily depict such
changes and they may be particularly suited for instructional
purposes if these changes do not occur in a discrete or linear
way, but rather involve more complex continuous aspects
(e.g., acceleration). However, they were not always superior
to static visualizations to convey dynamic information (e.g.,
Imhof et al., 2012). Thus, it is crucial to understand when
and for whom dynamic visualizations are beneficial to use
them effectively and to exploit their potential for learning.
Until now, research on the instructional use of dynamic
visualizations has yielded rather heterogeneous results: Not
only design factors and individual learner characteristics,

but also context factors, such as, the knowledge domain,
task requirements, or additional instructional support,
influence the effectiveness of dynamic visualizations (e.g.,
Höffler & Leutner, 2007; Lowe, Schnotz, & Rasch, 2011;
Tversky, Morrison, & Bétrancourt, 2002). These context
factors have become a focus of research on dynamic
visualizations.

Learning with Gestures
One idea on how to support learning about movements with
dynamic visualizations that is based on the embodied
cognition approach and proposed by De Koning and
Tabbers (2011) is the active and passive use of gesture.
Empirically, Hegarty et al. (2005) showed that gestures are
naturally used to express movements of depicted
components and thereby also the depicted processes in
mental animation problems. Moreover, it has already been
shown that the production of gestures during learning is
beneficial for acquiring knowledge about different scientific
topics and spatial problem solving (e.g., Chu & Kita, 2011;
Cook & Goldin-Meadow, 2006; Scheiter et al., 2012).
However, learners can either produce gestures on their own
or they can perceive gestures that are performed by others.
In line with the proposal of De Koning and Tabbers (2011),
it is also beneficial for learning to perceive gestures that
illustrate the depicted contents, for instance, performed by
teachers (e.g., Valenzeno, Alibali, & Klatzky, 2003).
Underlying this gesture watching effect might be the
activation of brain areas (i.e., the human mirror-neuronsystem [hMNS]; Fogassi & Ferrari, 2011; Rizzolatti &
Craighero, 2004) that are typically used to observe,
understand and imitate the actions of other persons. In a
related line of research, a current hypothesis that has
recently received considerable attention (e.g., Ayres et al.,
2009; Van Gog et al., 2009) is that the stimulation and
involvement of this hMNS might be beneficial for learning
about complex continuous aspects with dynamic
visualizations. The hMNS is typically activated by human
movements, but may be more generally used to also
represent other biological or even non-biological
movements, if the observer is able to anthropomorphize

2608

these movements (cf. De Koning & Tabbers, 2011; Engel et
al., 2008). Thus, in the domain of learning about biological
movements, one effective instructional strategy to activate
the hMNS might be to show learners not only the to-belearned movements via dynamic visualizations, but also
gestures displaying the to-be-learned dynamics in order to
trigger an anthropomorphized encoding. Hence, only
showing gestures that map onto the to-be-learned
movements should benefit learning about those movements.
This study addresses whether perceiving gestures in
addition to dynamic visualizations is also beneficial for
learning. Moreover, to investigate the role of the hMNS, the
underlying cognitive processes during learning were
investigated with neurophysiological methods in this study
(i.e., functional near-infrared-spectroscopy [fNIRS]). Until
now, to the best of our knowledge, there is no direct test of
the assumption that learners’ ability to recruit their hMNS
during processing dynamic visualizations may influence the
effectiveness of the visualizations. Moreover, it still has not
been investigated whether hMNS activation can be induced
by gesture-based interventions and then transferred to nonhuman movements because of mapping processes. This
approach might easily facilitate the understanding of
complex dynamic phenomena by implementing embodied
visualizations that activate specific brain areas into
instructional materials.

Learners’ Visuospatial Ability
Beyond context factors also individual learner
characteristics may play a role during learning about
biological movements. Because processing continuous
changes requires visuospatial ability (cf. Hegarty, 1992), it
is likely that learners’ visuospatial ability will determine
how much the learners profit from visualizations (cf.
Hegarty & Waller, 2005). Often the continuous processes do
not occur only in two-dimensional but rather in threedimensional space. Thus not only visual, but also spatial
aspects are important. Previous research on visuospatial
ability has revealed two important results, namely that (a)
learners with higher visuospatial ability outperform learners
with lower visuospatial ability during learning with
visualizations (see Höffler, 2010, for a meta-analysis) and
moreover, there is some evidence, that (b) visuospatial
ability may moderate the effectiveness of learning with
different visualization formats. Higher visuospatial ability
may compensate for “poor” instructions (i.e., in our case
unrelated non-corresponding gestures, cf. methods section),
whereas learners with lower visuospatial ability suffer from
such instructions (cf. ability-as-compensator hypothesis;
e.g., Hays, 1996; Hegarty & Kriz, 2008; Höffler, 2010).

Research Questions and Hypotheses
This study addressed by using neurophysiological methods
(i.e., functional near-infrared-spectroscopy [fNIRS], which
is a non-intrusive approach to gather data about cortical
activation of humans) the research question whether the
hMNS is activated during viewing gestures and whether the

viewing of these gestures is helpful for learning about
biological movements because learners map the human
movements to the non-human biological movements.
However, maybe solely the circumstance that learners see
a human during learning activates the hMNS and is thus
sufficient to facilitate learning about biological motions. In
other words, it might be also helpful for learners to see
gestures that have nothing to do with the to-be-learned
content. Thus, this study investigated whether viewing
gestures that correspond to the to-be-learned non-human
movements facilitate learning about these movements better
than unrelated non-corresponding gestures. Additionally, the
moderating role of learners’ visuospatial ability was
addressed. Furthermore, this study tested whether the
activation of the MNS mediates the facilitation of learning.
We hypothesize that viewing corresponding gestures
facilitates learning more than viewing unrelated noncorresponding gestures. This might be particularly true for
low-visuospatial-ability learners, whereas high-visuospatialability learners might not need this type of
anthropomorphization to learn about the depicted dynamic
processes (cf., ability-as-compensator hypothesis; e.g.,
Höffler, 2010). Moreover, we hypothesize that learners
differ with regard to recruiting the hMNS for processing and
that higher hMNS activation is associated with better
learning outcomes than lower hMNS activation. This might
again be particularly true for low-visuospatial-ability
learners, as they do not have available this ability to
compensate for such a hMNS actication.

Methods
Participants and Design
Forty-five university students (M = 24.98 years, SD = 4.57;
31 females) were asked to learn how to classify different
fish according to their movements based on visualizations
that illustrated four different movement patterns of fish. For
each movement pattern the participants saw three
visualizations: Firstly, they saw an animation of the specific
movement pattern. Secondly, they saw a video of a person
performing gestures with his hands and arms. These
gestures either did correspond or did not correspond (i.e.,
were unrelated) to the fish movement patterns. Therefore, at
this point the experimental manipulation with the betweensubjects factor type of gesture took place. Thirdly, the
learners saw the initial fish animation again.
An expert regarding fish movements performed the
gestures. For the corresponding gestures this expert was
instructed to display with his hands and arms
representations of the respective movements as clearly as
possible (see figure 1 left). For the non-corresponding
gestures the expert was instructed to perform gestures with
his hands and arms that were unrelated to the fish movement
patterns (i.e., waving, circulating the forearms around each
other, drumming, and pointing, see figure 1 right).
Each visualization was depicted for 30 s and was
followed by pauses of 30 s (black screen) between all

2609

visualizations. The learners were instructed to relax in these
pauses. In the pauses, the activations of the brain areas of
interest are supposed to decay to the baseline level before
the next visualization was displayed.

test comprised 21 dynamic multiple-choice items consisting
of underwater videos of real fish performing one of the four
to-be-learned movement patterns. To choose for each item
the kind of movement pattern that was depicted, learners
had to identify the body parts relevant for propulsion and
their way of moving. Each item was presented 7 s to the
participants and immediately afterwards they had 3 s time to
choose the correct answer by pressing a corresponding
button. The possible answers were indicated as static
screenshots from the learning animations of the four
movement patterns. Each item was awarded one point for
the correct answer (max. 21 points). The test items were
presented in blocks of 30 s so that 3 items were grouped
together. Pauses of 30 s (black screen) followed each block.
Learners’ Visuospatial Ability Learners’ visuospatial
ability was assessed with a short version of the paper
folding test (PFT, Ekstrom et al., 1976). This test measures
the ability to form representations of “object location,
movement, spatial relationships, and transformations”
(Blazhenkova & Kozhevnikov, 2009, p. 640) and thus is
well suited to cover the domain of fish movements. The
short version of the PFT consists of ten multiple-choice
items, where participants have to choose the correct answer
out of five options. The stimuli are depictions of stepwise
folded papers that were punched in the folded state, whereas
the answer options depict the punches of various unfolded
papers with the punches being either in the correct or
incorrect positions. A maximum of three minutes is
assigned to work on the items, and each correct answer is
worth one point (max. 10 points).

Figure 1: Learning visualizations in triples: corresponding
gestures (left) and non-corresponding gestures (right).

Materials
Participants had to learn to discriminate four different
patterns of fish movements. These movement patterns differ
in terms of the body parts that generate propulsion (i.e., the
body itself or several fins) and also in the manner of how
these body parts move in the three-dimensional space (i.e.
different wave-like or paddle-like movements). The four
different movement patterns were: 1. undulation of the
body; 2. undulation of the dorsal and anal fins; 3. oscillation
of the dorsal and anal fins (and undulation of the pectoral
fins); and 4. oscillation of the pectoral fins. One major
challenge in identifying these movement patterns is that fish
may deploy other movements in addition (e.g., to navigate),
that can easily be confused with movements used for
propulsion in another movement pattern.
Animations were rendered based on typical fish
performing the four movement patterns. These animations
were standardized in terms of the perspective, background,
position in the frame, and the swimming direction of the
fish. Moreover, in these deliberately designed visualizations,
we were able to only show the movements performed for
propulsion and omit other irrelevant movements. Beside
that, the depicted movements were highly realistic, thus
representing the movements of real fish adequately. The
movement cycles of the movement patterns were presented
in loops in the animations (30 s per movement pattern, 25
fps, size: 640 x 480 pixels) in the center of the screen.
For each movement pattern, videos of an expert regarding
to fish movements were recorded who performed either a
corresponding or a non-corresponding gesture. These
gestures were presented in the respective conditions in loops
in the videos (30 s per movement pattern, 25 frames per s,
size: 640 x 480 pixels) in the center of the screen. The
presentation of all visualizations was system-controlled.

Cortical Activation During viewing the gestures in the
learning phase, cortical activation was conducted via fNIRS
measurements with an ETG-4000 (Hitachi). As probe set we
used a 2x22 channel array, that was placed over the frontotemporo-parietal regions centered at the T3-T4 and C3-C4
positions (not exactly terminating on these positions
because of the fixed interoptode distances) according to the
standard locations of the 10-20 system. Changes of
absorbed near-infrared light were transformed into relative
concentration changes of oxygenated (O2Hb) and
deoxygenated haemoglobin (HHb). Local increases of O2Hb
as well as decreases of HHb are indicators of cortical
activity (Obrig & Villringer, 2003).

Procedure

Measures
Learning Outcomes To assess learning outcomes, a
movement pattern classification test was administered. This

Participants were tested individually. They first received a
printed overview in which they were informed about the
procedure on the different parts of the study. Subsequently,
they had to answer the PFT and a demographic
questionnaire. Subsequently, the fNIRS probe set was
placed on the scalp of the participants and adjusted with the
help of the experimenter. Then, the learning phase started
and the computer-based learning materials were presented.
For each of the four to-be-learned movement patterns
learners were presented with the triples of visualizations

2610

(fish animation – gesture video – fish animation). In the
learning phase the experimental manipulation took place.
Learners saw either the corresponding or the noncorresponding gestures. Following the learning phase (12
min) learners performed a filler task (8 min), in which they
listened to music. Subsequently, learners completed the
movement classification test (8 min). To answer the test
items participants were instructed to put both their
forefingers and both their middle fingers on predefined
keys. These keys were labeled with screenshots from the
corresponding fish animations on the screen. In total, a
single experimental session lasted approx. 50 minutes.

gestures were better for learning than non-corresponding
gestures (p = .04). Thus, the corresponding gestures are
beneficial for low-visuospatial-ability learners.

Cortical Activation

Results
Learning Outcomes
To analyze learning outcomes we conducted a multiple
regression analysis with the categorical predictor type of
gesture and the continuous predictor learners’ visuospatial
ability. We had to exclude four participants because of
technical reasons (data loss) resulting in a total number of
41 participants in this analysis. Further, we had to exclude
eight test items from the learning outcome measure, because
participants answered them with a response rate of more
than 95 %. The reliability analysis of the remaining 13 test
items achieved a good to excellent cronbach’s α of .85.
For learning outcomes the predictors in the regression
analysis explained a significant portion of variance (p =
.01). Results showed no effect of type of gesture on learning
outcomes (p = .41, ns), whereas there was an effect for
learners’ visuospatial ability on learning outcomes (p = .04).
This effect has to be interpreted in terms of the significant
interaction between type of gesture and learners’
visuospatial ability on learning outcomes (p = .04; figure 2).

To analyze the cortical activation we defined two regions of
interest (ROIs) on the left hemisphere for the hMNS among
the respective channels. The two ROIs were the left inferiorfrontal cortex (IFC) and the left inferior-parietal cortex
(IPC, cf. figure 3). To analyze cortical activation we
conducted two multiple regression analyses with the
predictors type of gesture and learners’ visuospatial ability.
We had to exclude additional eight participants from these
analyses because the data quality of these participants was
too poor resulting in a total number of 33 participants in
these analyses. For cortical activation on IPC the predictors
in the regression analysis did not explain a significant
portion of variance (p = .96, ns).

Figure 3. Spatial arrangement of the left probeset.
For cortical activation on IFC the predictors in the
regression analysis explained a significant portion of
variance (p < .001). Results showed an effect of type of
gesture on IFC activation (p < .001) and an effect for
learners’ visuospatial ability on IFC activation (p < .001).
These effects have to be interpreted in terms of the
significant interaction between type of gesture and learners’
visuospatial ability on IFC activation (p < .01; see figure 4).

Figure 4. Effects of type of gesture (G) and learners’
visuospatial activities (VSA) on cortical activation (left).

Figure 2. Interaction between learners’ visuospatial ability
and type of gesture on learning outcomes.
This interaction was resolved by a simple slopes analysis
(cf. Aiken & West, 1991). It revealed that for participants
with high visuospatial ability (defined as one standard
deviation above the sample mean) the type of gesture had no
influence on learning outcomes (p = .34, ns). As expected,
for participants with low visuospatial ability (defined as one
standard deviation below the sample mean) corresponding

Again a simple slopes analysis was conducted (cf. Aiken &
West, 1991). It revealed that for participants with high
visuospatial ability (defined as one standard deviation above
the sample mean) the type of gesture had no influence on
IFC activation (p = .14, ns). For participants with low
visuospatial ability (defined as one standard deviation below
the sample mean) corresponding gestures resulted in a
higher IFC activation than non-corresponding gestures (p <

2611

.001). Thus, the corresponding gestures helped lowvisuospatial-ability learners to activate the hMNS in terms
of IFC activation.

Effects of Cortical Activation
on Learning Outcomes
Finally, to address the question whether higher hMNS
activation is directly associated with better learning
outcomes, we conducted two multiple regression analyses
with the three predictors type of gesture, learners’
visuospatial ability and cortical activation in terms of IFC
activation or IPC activation respectively.
For learning outcomes the predictors in the regression
analysis with IFC activation did not explain a significant
portion of variance (p = .12, ns). Interestingly, the predictors
in the regression analysis with IPC activation did explain a
significant portion of variance for learning outcomes (p <
.01). There was a three-way interaction between the
predictors type of gesture, learners’ visuospatial ability, and
IPC activation on learning outcomes (p = .03; see figure 5).

Figure 5. Three-way interaction between type of gesture,
learners’ visuospatial ability, and IPC activation on learning
outcomes.
This triple interaction was resolved by simple slopes
analyses (cf. Aiken & West, 1991). Firstly, this approach
revealed that for learners who saw corresponding gestures
there was no two-way interaction between participants’
visuospatial ability and IPC activation (p = .59, ns). The
following simple slopes analyses revealed that IPC
activation did not predict learning outcomes for learners
who saw corresponding gestures: neither for highvisuospatial-ability learners (p = .47, ns), nor for lowvisuospatial-ability learners (p = .40, ns). However, for
learners who saw non-corresponding gestures there was an
interaction between participants’ visuospatial ability and
IPC activation (p < .01). We further resolved this two-way
interaction between participants’ visuospatial ability and
IPC activation for learners who saw non-corresponding
gestures. The simple slopes analyses revealed that in the
group of learners who saw non-corresponding gestures IPC
activation negatively predicted learning outcomes for highvisuospatial-ability learners (p = .04), whereas for lowvisuospatial-ability learners IPC activation positively

predicted learning outcomes (p = .001). Thus, for learners
who saw non-corresponding gestures, but have had high
visuospatial abilities at their disposal IPC activation is
detrimental for learning. However, for learners who did
neither have corresponding gestures nor high visuospatial
abilities at their disposal, activation of their hMNS in terms
of the IPC during processing the unrelated noncorresponding gesture improves their learning.

Discussion
This study tested whether viewing gestures performed by
others is helpful for learning about non-human movements
and whether these gestures stimulate anthropomorphization
via an activation of the hMNS. The anthropomorphization is
stimulated by an external video and is not accomplished by
the learners on their own. Our results showed that viewing
corresponding gestures activated the hMNS particularly for
low-visuospatial-ability learners. These learners achieved
the same learning outcomes as high-visuospatial-ability
learners. Low-visuospatial-ability learners seem to profit
from being demonstrated a connection between non-human
biological movements and movements of the human body
that correspond to these movements. Thus, learning about
biological movements can be facilitated by gesture-based
interventions activating parts of the hMNS: Gestures that
correspond to the to-be-learned movements and activate the
inferior-frontal cortex (IFC). This activation seems to
compensate missing viusospatial ability.
Furthermore, our results indicate another way of
improving learning about biological movements: When
looking at participants who neither have high visuospatial
ability, nor received the benefit of viewing corresponding
gestures, – namely, the group of low-visuospatial-ability
learners who processed non-corresponding gestures – the
result pattern was rather heterogeneous: Only participants
who activated another part of the hMNS (i.e., the inferiorparietal cortex [IPC]) were able to dramatically improve
their learning, whereas participants who did not activate this
area achieved only poor results. This indicates that the
activation of the inferior-parietal cortex helps participants to
learn about biological movements, particularly if they have
no access to other facilitating factors. In line with this
reasoning, learners who have available two facilitating
factors, namely high visuo-spatial abilities and an activation
of the IPC, performed worse when they saw noncorresponding gestures. In this case, the two facilitators
might compete and interfere with each other resulting in
inferior learning outcomes. Nevertheless, higher hMNS
activation is associated with better learning outcomes – at
least for low-visuospatial-ability learners: for IFC activation
it seems that there is a rather stepwise connection in that a
certain value has to be reached, whereas for IPC activation it
seems that it follows the more activation the better learning.
Stimulating the hMNS by means of gestures seems to be a
promising strategy to enhance learning with dynamic
visualizations for low-visuospatial-ability learners because
this intervention leads to higher activation in their IFC as

2612

part of the hMNS. However, further research needs to
replicate these findings with a larger sample size and
continue to disentangle the effects of this study. Particularly,
our findings have to be replicated with other examples of
gestures in different domains, as gestures about fish
movements might not be a typical example of gestures.
Furthermore, it is very important to investigate how the
activation of the IPC can also be fostered by instructions.
Furthermore, gesture-based instructions that support
anthropomorphization should be investigated in different
instructional domains and settings that involve learning
about continuous movements and processes to prove
whether they are in general a suitable method to enhance
learning about processes with dynamic visualizations.
Further research should also investigate whether effective
and less effective dynamic visualizations differ in their
ability to activate the MNS, thereby potentially explaining
inconsistent results on the effectiveness of dynamic
visualizations (e.g., Höffler & Leutner, 2007; Tversky et al.,
2002). The present study is one first step into this field of
research and our results suggest that it is important to not
only put further effort into designing better dynamic
visualizations, but also in providing learners with suitable
strategies to adequately process these visualizations.

References
Aiken, L.S., & West, S.G. (1991). Multiple regression:
Testing and interpreting interactions. Newbury Park:
Sage.
Ayres, P., Marcus, N., Chan, C. & Qian, N. (2009).
Learning hand manipulative tasks: When instructional
animations
are
superior
to
equivalent
static
representations. Computers in Human Behavior, 25, 348353.
Blazhenkova, O. & Kozhevnikov, M. (2009). The new
object-spatial-verbal cognitive style model: Theory and
measurement. Applied Cognitive Psychology, 23, 638663.
Chu, M., & Kita, S. (2011).The Nature of Gestures’
Beneficial Role in Spatial Problem Solving. Journal of
Experimental Psychology: General, 140, 102-116.
Cook, S.M., & Goldin-Meadow, S. (2006). The role of
gesture in learning: Do children use their hands to change
their minds? Journal of Cognition and Development, 7,
211 - 232.
De Koning, B.B., & Tabbers, H.K. (2011). Facilitating
understanding of movements in dynamic visualizations:
An embodied perspective. Educational Psychology
Review, 23, 501-521.
Engel, A., Burke, M., Fiehler, K., Bien, S., & Rösler, F.
(2008). What activates the human mirror neuron system
during observation of artificial movements: bottom-up
visual
features
or
top-down
intentions?
Neuropsychologia, 46, 2033-2042.
Ekstrom, R., French, J., Harman, H., & Dermen, D. (1976).
Manual for Kit of Factor-Referenced Cognitive Tests.
Princeton: Educational Testing Service.

Fogassi, L., & Ferrari, P.F. (2011). Mirror systems. Wiley
Interdisciplinary Reviews: Cognitive Science, 2, 22-38.
Hegarty, M. (1992). Mental animation: Inferring motion
from static diagrams of mechanical systems. Journal of
Experimental Psychology: Learning, Memory and
Cognition, 18, 1084-1102.
Hegarty, M., & Kriz, S. (2008). Effects of knowledge and
spatial ability on learning from animation. In R. Lowe &
W. Schnotz (Eds.), Learning with animation: Research
implications for design. Cambridge, England: Cambridge
University Press.
Hegarty, M., Mayer, S., Kriz, S., & Keehner, M. (2005).
The role of gestures in mental animation. Spatial
Cognition and Computation, 5, 333-356.
Hegarty, M., & Waller, D. (2005). Individual differences in
spatial ability. In P. Shah, & A. Miyake (Eds.), Handbook
of Visuospatial Thinking. Cambridge University Press.
Höffler, T.N. (2010). Spatial ability: Its influence on
learning with visualizations—a meta-analytic review.
Educational Psychological Review, 22, 245-269.
Höffler, T.N., & Leutner, D. (2007). Instructional animation
versus static pictures: A meta-analysis. Learning and
Instruction, 17, 722-738.
Imhof, B., Scheiter, K., Edelmann, J., & Gerjets, P. (2012).
How temporal and spatial aspects of presenting
visualizations affect learning about locomotion
patterns. Learning and Instruction, 22, 193-205.
Lowe, R.K., Schnotz, W., & Rasch, T. (2010). Aligning
affordances of graphics with learning task requirements.
Applied Cognitive Psychology, 25, 452–459.
Obrig, H., & Villringer, A. (2003). Beyond the visible –
Imaging the human brain with light. Journal of Cerebral
Blood Flow & Metabolism, 23, 1-18.
Rizzolatti, G., & Craighero, L. (2004). The mirror-neuron
system. Annual Review of Neuroscience, 27, 169–192.
Scheiter, K., Arndt, J., Imhof, B., & Ainsworth, S. (2012).
Move like a fish: Do gestures aid learning from
photographs and videos? In E. de Vries, & K. Scheiter
(Eds.), Proceedings EARLI Special Interest Group Text
and Graphics: Staging knowledge and experience: How
to take advantage of representational technologies in
education and training? Grenoble, France: Université
Pierre-Mendès-France.
Tversky, B., Morrison, J., & Bétrancourt, M. (2002).
Animation: Can it facilitate? International Journal of
Human-Computer Studies, 57, 247-262.
Valenzeno, L., Alibali, M.W., & Klatzky, R. (2003).
Teachers’ gestures facilitate students’ learning: A lesson
in symmetry. Contemporary Educational Psychology, 28,
187–204.
Van Gog, T., Paas, F., Marcus, N., Ayres, P., & Sweller, J.
(2009). The mirror-neuron system and observational
learning: Implications for the effectiveness of dynamic
visualizations. Educational Psychology Review, 21, 2130.

2613

