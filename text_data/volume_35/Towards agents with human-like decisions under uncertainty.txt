UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Towards agents with human-like decisions under uncertainty
Permalink
https://escholarship.org/uc/item/4hk048s0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Marques, Nuno
Melo, Francisco
Mascarenhas, Samuel
et al.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                   Towards agents with human-like decisions under uncertainty 1
                                                  Nuno Marques (nunocm@gmail.com)
                                                   Francisco Melo (fmelo@inesc-id.pt)
                                            Samuel Mascarenhas (samuel.fm@gmail.com)
                                                 João Dias (joao.dias@gaips.inesc-id.pt)
                                                Rui Prada (rui.prada@gaips.inesc-id.pt)
                                                    Ana Paiva (ana.paiva@inesc-id.pt)
                                                     INESC-ID, Instituto Superior Técnico
                                                       Av. Prof. Cavaco Silva, TagusPark
                                                         2780-990 Porto Salvo, Portugal
                               Abstract                                       Agents with the aforementioned characteristics can be spe-
   Creating autonomous virtual agents capable of exhibiting                cially useful for Multi-Agent Based Simulations (MABS)
   human-like behaviour under uncertainty is becoming increas-             (Davidsson, 2001). In these systems, human behaviour is
   ingly relevant, for instance in multi-agent based simulations           modelled at the individual (agent) level, and the resulting
   (MABS), used to validate social theories, and also as intelli-
   gent characters in virtual training environments (VTEs). The            structure is analysed after it emerges from the agent inter-
   agents in these systems should not act optimally; instead, they         actions. Typically, MABS have been used to validate so-
   should display intrinsic human limitations and make judge-              cial theories (eg. Davidsson, 2002). The inclusion of un-
   ment errors. We propose a Belief-Desire-Intention (BDI)
   based model which allows for the emergence of uncertainty re-           certainty is of special importance in market simulations, as
   lated biases during the agent’s deliberation process. To achieve        it strongly impacts the decisions of the agent (Arthur, 1991).
   it, a probability of success is calculated from the agent’s beliefs     From socio-cultural research, the Uncertainty Avoidance di-
   and attributed to each available intention. These probabilities
   are then combined with the intention’s utility using Prospect           mension of human cultures, identified by Hofstede (Hofstede,
   Theory, a widely validated descriptive model of human deci-             2001), is another example where these agents could be used
   sion. We also distinguish risk from ambiguity, and allow for            in the context of MABS. Our solution is also relevant for use
   individual variability in attitudes towards these two types of
   uncertainty through the specification of indices. In a travelling       in serious games, particularly virtual training environments.
   scenario, we demonstrate how distinct, more realistic agent be-         As these simulation often focus on social and communication
   haviours can be obtained by applying the proposed model.                aspects (eg. Johnson & Valente, 2009; Kim et al., 2009), it
   Keywords: Intelligent agents; Decision making; Cognitive bi-            is increasingly important to embed the virtual characters with
   ases                                                                    human-like behaviour.
                                                                              This paper is organized as follows. We start by giving
                           Introduction                                    a possible definition of uncertainty and describing Prospect
Uncertainty is a natural part of our world. No one can claim               Theory, and follow with work related to ours. Then we
to know everything, no one can predict the future. We deal                 present the model, and demonstrate it using an example sce-
with uncertainty on our everyday lives and our behaviour is                nario. Finally we discuss future improvements.
constantly influenced by it, even if we do not always realize
it. However, in the context of virtual agents, uncertainty has                                     Background
usually been seen as a problem that the agent must overcome                In tackling the effects of uncertainty, one should first have an
(eg. planning Peot & Smith, 1992), and thus most existing                  accurate definition of the term. However, this is not an easy
systems are aimed at achieving optimal agent behaviour un-                 task because different research fields or problem approaches
der these conditions.                                                      use it with different meanings.
   Our approach is different, in which we acknowledge the of-                 One important step is distinguishing uncertainty from the
ten sub-optimal, even “irrational” behaviour of humans when                closely related concept of risk. In a decision context, the later
confronted with uncertain situations. These decision biases                refers to choices involving known chances (eg. a spin of a
and judgement errors have been extensively studied and are                 roulette wheel). However, uncertainty arises in a decisions
supported by a wealth of empirical evidence (eg. Kahneman                  involving personal opinions (eg. betting on what football
& Tversky, 1979; Camerer & Ho, 1994). We propose an                        team will win a game). Moreover, uncertainty has distinct
agent model based on the classical Belief-Desire-Intention                 facets (Smithson, 2008): epistemic randomness or risk un-
(BDI) paradigm, which seeks to integrate in the agent’s de-                certainty is the subjective counterpart of risk, and is usually
liberation process these deviations from rational behaviour.               represented by subjective probabilities; ambiguity, which re-
    1 This work was partially supported by the Portuguese                  sults from overlapping beliefs (i.e, strong reasons to believe
Fundação para a Ciência e Tecnologia under project PEst-                and not believe) or uncertainty about probabilities (second
OE/EEI/LA0021/2011. (INESC-ID multiannual funding) through                 order uncertainty); and vagueness, reflected by fuzzy state-
the PIDDAC Program funds. Additionally, it was funded by the
National Project SEMIRA (ERA-Compl/0002/2009), and European                ments (eg. “John is tall” — what does “tall” mean?).
Project eCute (ICT-5-4.2 257666) projects.                                    The topic of how humans choose (or should choose) under
                                                                       2978

uncertainty has been extensively studied over the last cen-             The choice for the specific value and weighting functions
turies. Decision making theories which seek to predict the           is arbitrary, as long as they obey certain properties. The value
optimal choice, such as the classical Expected Utility theory        function should reflect the effects of diminishing sensitivity
(EU), are called normative. However, people do not generally         (variations in utility are less perceived the further they are
obey the axioms of normative theories (some examples of vi-          from the reference point), and thus be concave for gains and
olations are described in the following section). Given our          convex for losses (S-shape). Furthermore, it should be steeper
goal of achieving human-like behaviour, we focus on theories         for losses than for gains, reflecting the phenomena of loss
seeking to describe how humans actually act. Within these,           aversion.
decision behaviour has been observed to differ when the sub-            The weighting function transforms a probability, and
ject is offered a description of available choices (decisions        should also reflect the effects of diminishing sensitivity.
from description paradigm), versus when he can learn by di-          However, in this case there are two boundaries (p = 0 and
rect experimentation (decisions from experience paradigm),           p = 1), and thus the resulting function is inverse S-shaped.
as shown by Hertwig, Barron, Weber, & Erev, 2004. As we              The curvature of these functions reflect the individual propen-
will see, the solution proposed in this paper assumes that the       sity to decision biases, which is usually accounted for by as-
agent learns by asking and not by experimentation, and there-        suming parametrized functional forms.
fore we restrict ourselves to the former category.                      Our integration of PT in the BDI model, as shown later, is
                                                                     restricted to choices involving prospects with at most two out-
Prospect theory
                                                                     comes. Therefore, both Prospect Theory and its more recent
The most validated descriptive theory of human decision is           development, Cumulative Prospect Theory (CPT) (Tversky
called Prospect Theory (PT) (Kahneman & Tversky, 1979;               & Kahneman, 1992), coincide. We are presenting the orig-
Tversky & Kahneman, 1992). Some of the decision biases it            inal formulation of the theory. It is also important to note
addresses are:                                                       that, although PT is originally based on studies where proba-
  Framing Effects: there is evidence that the framing of op-         bilities were objectively stated, and thus related to decisions
   tions (in terms of gains or losses) significantly impacts the     under risk, its fundamental properties were also verified in
   choices people make (Tversky & Kahneman, 1986);                   decisions under risk uncertainty (Tversky & Fox, 1995).
  Nonlinear preferences: the idea that a risky prospect is lin-                              Related Work
   ear in outcome probabilities has been proved false, most
   prominently by the Allais paradox (Allais, 1979);                 In Pezzulo’s et. al. proposal, measures of ignorance (what
                                                                     the agent does not know), contradiction and uncertainty (dif-
  Source dependence: as demonstrated by the Ellsberg para-           ference between opposing beliefs) are computed by the agent,
   dox, people’s decisions depend not only on the degree of          and used in the decision process using custom rules (Pezzulo,
   uncertainty but also on its source; this phenomena has been       Lorini, & Calvi, 2004).
   explained both from an ambiguity aversion (people dislike            FAtiMA-PSI (Dias & Paiva, 2005; Mascarenhas, Dias,
   ambiguity, Ellsberg, 1961) and from a competence hypoth-          Prada, & Paiva, 2010) is an architecture geared towards the
   esis perspective (people prefer a bet on their area of compe-     creation of believable virtual characters. It has a strong fo-
   tence when compared to equivalent bet based on objective          cus on emotional aspects and human motivations. It already
   probabilities, Heath & Tversky, 1991);                            represents some forms of uncertainty, as stochastic action out-
                                                                     comes and estimations of goal success based on past obser-
  Fourfold pattern of risk: empirical studies indicate that peo-
                                                                     vations. This architecture also provides several parameters
   ple are generally risk averse for high probability gains and
                                                                     which allow an author to define agents with different person-
   low probability losses, and risk seeking for low probability
                                                                     alities. However, it does not model unreliable perceptions
   gains and high probability losses (Tversky & Kahneman,
                                                                     - the environment is considered completely observable, and
   1992).
                                                                     the decision process is based on EU theory.
   These biases are accounted by PT by assuming a fram-                 The graded BDI model and abstract architecture (g-BDI,
ing phase prior to the actual evaluation; a value function (v)       Casali, Godo, & Sierra, 2009) extends the classical BDI
which distort utilities; and a weighting function (π) which dis-     model by allowing uncertainty to be represented in the agent’s
torts probabilities. Our focus is on the biases created by these     mental attitudes. g-BDI permits not only uncertain (graded)
functions, and how to integrate them in the BDI model, as            beliefs, but also desires and intentions. Graded desires cor-
the modelling of framing effects has already been explored           respond to degrees of preference (or rejection) over states of
in a context similar to ours (Ito & Marsella, 2011). In PT,          the world, and graded intentions represent the preference over
the valuation attributed to a prospect (i.e, a gamble) f , which     specific ways (plans) to achieve desires. The formalization of
has n possible outcomes xi , i = 1...n, each with utility Ui and     the g-BDI allow different contexts to operate each in its own
probability pi , is given by:                                        logic. Thus, the belief context (BC), for example, can use
                                                                     probability measures to represent uncertainty.
                      V ( f ) = ∑ v(Ui )π(pi )
                                i
                                                                        The Contextually-Based Utility (CBU) model (Ito &
                                                                 2979

Marsella, 2011) combines principles of cognitive appraisal               In order to focus on the behavioural consequences of uncer-
theories with decision theoretic notions, with the main pur-          tainty, we made two simplifying assumptions: 1) all variables
pose of capturing framing effects with greater accuracy. For          are conditionally independent; and 2) the agent can only be
each possible goal outcome, a contextual utility value is cal-        uncertain about static propositions (propositions whose value
culated using two salient features: pleasantness, the out-            never change, because no actions exist with such effects). We
come’s intrinsic attractiveness or unattractiveness; and con-         expect to address these limitations in future work.
gruence, how much achieving the goal contributes to the
                                                                      Belief revision The agent can change its opinions by mak-
agent’s expectations. A decision weight is also computed us-
                                                                      ing either a direct observation or by asking questions to other
ing the outcomes’ probabilities. These three measures are
                                                                      agents. In the latter case, the degree at which the agent be-
transformed by an S-shaped function which models the ef-
                                                                      lieves what he is told depends on the evidence’s credibil-
fects of diminishing sensitivity in relation to a variable refer-
                                                                      ity. We represent the credibility of an evidence provided by
ence point, and are then linearly combined to obtain a goal’s
                                                                      source i, asserting a proposition A, as cr(εAi ) ∈ [0, 1[. The
final valuation.
                                                                      value cr is calculated based on the history of previously re-
   The work presented above share with ours the purpose of
                                                                      ceived answers from the same source. We follow the method
achieving human-like agent behaviours in decisions under un-
                                                                      proposed in (Pearl, 1988), which allows Bayes Rule to be ap-
certainty. However, almost none of them apply a validated de-
                                                                      plied to uncertain evidence. Assuming independent sources,
scriptive decision theory. The exception is CBU, which is not
                                                                      the agent’s beliefs are updated using the formula below:
an agent model in itself. Thus, our approach differs in what
we consider fundamental requisites of our solution: 1) captur-                                             A
                                                                                        (cr(εAi ) + 1−cr(εi ) ) · P(H)
                                                                                                          n        P(εAi )
                                                                                                                           ifH = A
ing widely validated findings on human decision behaviour;                  P(H|εAi ) = 1−cr(εA ) P(H)
and 2) being a generally applicable agent model.                                                i
                                                                                                   · P(εA )                otherwise
                                                                                              n
                                                                                                        i
                             Model                                       where P(H) is the prior probability of value H in a vari-
We chose the BDI model because its folk psychology roots              able. When new evidence comes which asserts A, the above
are consistent with our goal of modelling human-like be-              formula increases the belief in A while decreasing the belief
haviours, and also due to its flexibility and wide application.       in the other values of the same variable, such that they still
An overview of the proposed model is shown in figure 1. In            sum to one. Note that a direct observation corresponds to
the present section, each component is explained in detail.           cr(εAi ) = 1, and thus always leads to absolute certainty on a
                                                                      variable’s value.
                                                                      Solutions and doubts
                                                                      This section serves as a bridge between the preceding (uncer-
                                                                      tainty representation) and following (decision process) com-
                                                                      ponents, by demonstrating how the agent’s beliefs are inte-
                                                                      grated in the decision process. In the BDI model, the general
                                                                      behaviour of an agent is guided by the active intention - a
                                                                      commitment to achieve a goal and a plan to do it. We call
                                                                      each plan generated by the planning process a solution. We
                                                                      assume that each solution only contains indispensable actions
                                                                      (as usual in classical planning), and therefore if a single ac-
                                                                      tion fails, the solution also fails.
                                                                         The execution of an action, in turn, is dependent on its Ac-
                   Figure 1: Model overview                           tion Pre Conditions (APC) validity. APCs are world proposi-
                                                                      tions or their negation. Thus, success in achieving a goal is
Uncertainty representation                                            ultimately dependent on the validity of APCs and, if among
This component deals on how to represent uncertainty in the           the APCs some correspond to uncertain propositions, they
agent beliefs. We used bayesian probability, as it is the most        possibly invalidate the entire solution and make the goal im-
developed model of uncertainty representation. We assume a            possible to attain. Uncertain APCs are what we call doubts,
world defined by a set of crisp (true/false) propositions, repre-     and they are the basic components from which uncertainty
sented by upper-case letters, for example O=“Hotel is open”.          related biases will arise (see Figure 2). To distance ourselves
Exhaustive and mutually exclusive subsets of propositions are         from the problem of planning in uncertain environments, dur-
called variables. A probability distribution over the values of       ing planning doubts are considered valid and thus ignored.
each variable forms the agent’s belief state, i.e, his opinions
on the actual value of propositions. Thus, if Θ = {A1 , ..., An }     Decision process
is a variable, then ∑i P(Ai ) = 1, where P(Ai ) denotes the sub-      Within the deliberative process of the agent, our model deals
jective probability of Ai being true.                                 with the evaluation of competing solutions, which essentially
                                                                  2980

                                                                    where R ∈ [0, 1] is a risk aversion index available to the sce-
                                                                    nario author. A value of R close to 1 results in a risk seeking
                                                                    attitude, as the agent will generally overweight probability
                                                                    values; on the other hand, a value close to 0 corresponds to a
                                                                    risk averse attitude, as the probabilities are underweighted.
                                                                       The resulting functions are shown in figure 3.
  Figure 2: An example of a solution containing two doubts.
correspond to the intentions of the agent — possible paths to
commit to and pursue. Prospect Theory (PT), the psychology
based model described before, is used during this process.
This theory has been shown to accurately predict the choice
behaviour of humans in many real life situations, and as such
it nicely fits our goal — achieving actual, and not optimal
                                                                    Figure 3: Weighting function at different levels of risk aver-
behaviour. The prospects (i.e, the solutions) only have two
                                                                    sion (R).
possible outcomes: success and failure.
Value function We assume a utility of 0 for failing, and a          Ambiguity
utility of success given by the sum of utility of each individ-     Although PT captures risk biases, ambiguity related biases
ual action within in the solution. Using the status quo as the      are not considered. Before discussing them, we present how
reference point (v(0) = 0), the above definition implies that       is ambiguity quantified in the proposed model.
a solution is always a positive prospect. We applied Tversky           Research in psychology and neuroscience proposes infor-
and Kahneman two branch power function, but because we              mation entropy of possible meanings one attributes to a situ-
only have positive outcomes we can restrict to the positive         ation (or the world) as a mathematical quantification of am-
part (with α = 0.88 as estimated in Tversky & Kahneman,             biguity (Takahashi, Oono, Radford, & Others, 2007; Hirsh,
1992):                                                              Mar, & Peterson, 2012). Information entropy, also known
                         v = U α ,U ≥ 0                             as Shannon Entropy, measures the amount of “disorder”,
Weighting function The weighting function π(p) trans-               or uncertainty, associated with a random variable. If θ =
forms the beliefs the agent holds about states of the world         {A1 , ..., An } is a variable, it’s entropy is given by (we use e
into the decision weights he actually utilises when making          as the base of the logarithm):
a decision. The probability assigned to the successful out-                                          n
come corresponds to no invalid doubts at all (as a single doubt                         H(θ) = − ∑ p(Ai ) log p(Ai )
failure invalidates the whole solution). Thus, variables being                                       i
independent, p is obtained by multiplying the probabilities
of all solution doubts being valid. The probability p is then          In our model, the variables correspond the agent’s belief
transformed by the weighting function. We use the function          state. The ambiguity being experienced by the agent, how-
proposed by Wu & Gonzalez, 1999:                                    ever, depends only on the goal under pursuit, i.e, the solution
                                                                    under execution. Thus, it arises from solution doubt vari-
                                 δpγ                                ables:
                      π=
                           δpγ + (1 − p)γ                                            ∆(S) = ∑ H(θi ), θi ∈ {doubts(S)}
                                                                                              i
   In the above function, the two parameters reflect two dis-
tinct cognitive biases: γ (estimated as γ = 0.44) represents           Where ∆(S) represents the ambiguity of solution S. High
the impact of diminishing sensitivity to probability variations     levels of ambiguity are caused by ignorance about the ac-
through the function curvature; δ (estimated as δ = 0.77) rep-      tual value of doubts — when the probability distributions are
resents the attractiveness to gambles through the function el-      “flat”, and/or there are many doubts. Lower levels of ambi-
evation.                                                            guity, on the other hand, are characterized by strong beliefs
   Individual variability towards risk uncertainty is allowed       (probabilities close to either 1 or 0) and/or a fewer number
via the indirect specification of the δ parameter. Staying          of doubts. Note that the amount of ambiguity is independent
within the parameter values estimated by Wu & Gonzalez,             on what variable values are preferred, and thus distinct from
1999, we propose to substitute δ with δR , such that:               risk.
                                                                       Existing research seem to indicate that people are averse to
                       δR = δ + (0.5 − R)                           this type of ambiguity, i.e, they tend to choose lower entropy
                                                                2981

over higher entropy options (Takahashi et al., 2007). In order
                                                                                                       Table 3: Solutions
to represent this effect, we first reduce it to the [0, 1[ interval:       Solutions                                              Util     Prob         Amb
                             ∗         −k∆                                 S 1 : TravelTo(Agra) → Visit(Agra)                     6        0.9          0.3251
                           ∆ = 1−e                                         S 2 : TravelTo(Ladakh) → Visit(Ladakh)                 10       0.5          0.6931
   Where k = 0.2 is a normalizing factor. Then, we introduce
ambiguity in the δR parameter defined before, such that:
                                                                         risk averse and ambiguity tolerant agent (R = 1; A = 0), and
                          ∆     R          ∗                             Sara, who is risk seeking and also ambiguity tolerant (R = 0;
                        δ = δ (1 − A∆ )
                                                                         A = 0). We start with Peter’s evaluation:
Where A ∈ [0, 1] is an ambiguity aversion index. Therefore,                                                    
                                                                                                                        1.27∗0.90.44
                                                                                                                                           
                                                                                   V (S1 )=v(6)∗π(0.9)=60.88 ∗                                =22. 0 1
values of A close to 1 reinforce the aversion to solutions with                                                  1.27∗0.90.44 +(1−0.9)0.44
high ambiguity.
                                                                                                                                            
                                                                                  V (S2 )=v(10)∗π(0.5)=100.88 ∗           1.27∗0.50.44         =1.61
                                                                                                                   1.27∗0.50.44 +(1−0.5)0.44
                      Example scenario                                      And Sara’s evaluation is shown             below:              
In this section we demonstrate how the proposed model gives                        V (S1 )=v(6)∗π(0.9)=60.88 ∗          0.27∗0.90.44          =3.72
                                                                                                                 0.27∗0.90.44 +(1−0.9)0.44
origin to different behaviours, by focusing on preference re-                                                                               
                                                                                  V (S2 )=v(10)∗π(0.5)=100.88 ∗           0.27∗0.90.44
versals caused by distinct attitudes towards uncertainty. The                                                      0.27∗0.50.44 +(1−0.5)0.44
                                                                                                                                               =44. 2 4
scenario being presented is deliberately simple, in order to                As we can see, Peter avoids the higher utility solution due
facilitate the presentation.                                             to its greater risk. On the other hand, Sara is not concerned,
   Consider an author who wishes to create an uncertain travel           and visits Ladakh.
scenario. In addition to the agents introduced further below,               In the second test case, we demonstrate the effects of am-
it contains two other entities: the city of Agra (location of the        biguity. We use two other agents, both average risk averse.
TajMahal) and the region of Ladakh (home to several beauti-              However, John is ambiguity tolerant (R = 0.5; A = 0) while
ful Buddhist monasteries), which will be the available travel            Laura is ambiguity averse (R = 0.5; A = 1). John evaluates
destinations. When an agent learns about them (eg. at ini-               the solutions similarly as shown before, resulting in the val-
tialization time), the associated properties are stored in the           uations: V (S1 ) = 3.24 and V (S2 ) = 3 . 3 0 , and therefore goes
agent’s memory as variables. By default, properties are cer-             to Ladakh. In Laura’s decision, the parameter δA includes the
tain, but uncertainty about specific entities or specific proper-        effects of ambiguity as shown below:
ties may be specified through the agent’s configuration file.
                                                                                                    ∆∗1 =1−e−0.2∗0.3251 =0.0630
   In the presented test case, all agents will share the exact
same knowledge: they are completely ignorant about find-                                            ∆∗2 =1−e−0.2∗0.6931 =0.1294
ing accommodation in Ladakh, but are almost certain to find                                     δ∆1 =0.77∗(1−1.0∗0.0630)=0.7215
it in Agra. This is represented by two different probability
                                                                                                δ∆2 =0.77∗(1−1.0∗0.1294)=0.6703
distributions over the variables originated by the property ac-                                                                             
cAvailable of Agra and Ladakh, as shown in table 1. The                           V (S1 )=v(6)∗π(0.9)=60.88 ∗          0.7215∗0.90.44          =33. 1 7
                                                                                                                0.7215∗0.90.44 +(1−0.9)0.44
             Table 1: Agent’s uncertain knowledge
                                                                                                                                             
                                                                                 V (S2 )=v(10)∗π(0.5)=100.88 ∗           0.6703∗0.50.44         =3.04
                         Agra(accAvailable)                                                                       0.6703∗0.50.44 +(1−0.5)0.44
              Values             Belief      Entropy                        For Laura, the ambiguity associated with travelling to
              True               0.9         0.3251                      Ladakh is too great to overcome the potential reward, and she
              False              0.1
                                                                         prefers to go to Agra. As it can be seen, the effects of ambigu-
                       Ladakh(accAvailable)
                                                                         ity are independent of risk, given that the agent’s only differ in
              Values             Belief      Entropy
              True               0.5                                     their ambiguity attitude. It should be noted that, although we
                                             0.6931                      fixed the uncertainty levels and varied the agent parametriza-
              False              0.5
                                                                         tions, an equivalent preference reversal can be achieved by
atomic actions available to the agents in order to achieve their         varying the ambiguity levels while maintaining risk and the
Travel() goal are specified in table 2, which in this case dif-          indices.
fer only in their utility, as the doubts are equivalent (having a
place to sleep). The created solutions are shown in table 3.                                             Conclusion
                         Table 2: Actions                                In this paper, we proposed an agent model which combines
     Actions              Utility    Doubts                              the flexibility and generality of the BDI paradigm with a
     TravelTo(Agra)       -2                                             widely validated descriptive decision model, in order to cap-
     Visit(Agra)          8          Agra(accAvailable)=True
     TravelTo(Ladakh)     -3                                             ture decisions biases in the agent’s deliberative process. The
     Visit(Ladakh)        13         Ladakh(accAvailable)=True           two test cases presented in the previous section showed how
                                                                         different parametrizations in agent’s attitudes towards uncer-
   In the first case, we show the impact of distinct attitudes to-       tainty have distinct behavioural consequences. A rational
wards risk. The solutions are evaluated by Peter, an extremely           agent using EU, on the other hand, would always select the
                                                                     2982

same solution. Furthermore, by considering risk and ambigu-             nations. Sage Publications, Inc.
ity as separate constructs, we expect to better capture human        Ito, J., & Marsella, S. (2011). Contextually-based utility:
decisions under uncertainty.                                            An appraisal-based approach at modeling framing and de-
   Although the modelled risk biases are well grounded on               cisions. In Proceedings of the twenty-fifth {AAAI} confer-
empirical evidence, the effects of ambiguity require further            ence on artificial intelligence, august 7-11, 2011, san fran-
validation. In particular, the magnitude at which entropy               cisco, ca, united states (Vol. 2, pp. 60–65).
causes an aversion effect in a decision context should be fur-       Johnson, W. L., & Valente, A. (2009). Tactical Language
ther studied, so that a more accurate expression can be used            and Culture Training Systems: using AI to teach foreign
in the calculation of δA . We also expect to address the present        languages and cultures. AI Magazine, 30(2), 72.
limitations by possibly integrating a Bayesian Belief Network        Kahneman, D., & Tversky, A. (1979). Prospect theory: An
(Pearl, 1988), allowing for conditional dependences between             analysis of decision under risk. Econometrica: Journal of
variables, and using a planner capable of dealing with par-             the Econometric Society, 47(2), 263–292.
tially observable domains (eg. Peot & Smith, 1992), in order         Kim, J., Hill, R., Jr, Durlach, P., Lane, H., Forbell, E., Core,
to drop the static propositions assumption. Finally, we con-            M., . . . Hart, J. (2009). BiLAT: A game-based environ-
sider accounting for framing effects as an important step in            ment for practicing negotiation in a cultural context. In-
achieving more realistic decision behaviours, and the ideas             ternational Journal of Artificial Intelligence in Education,
behind the CBU model (Ito & Marsella, 2011) are certainty               19(3), 289–308.
worth exploring.                                                     Mascarenhas, S., Dias, J., Prada, R., & Paiva, A. (2010). A
                                                                        dimensional model for cultural behaviour in virtual agents.
                           References                                   Applied Artificial Intelligence, 24(6), 552–574.
                                                                     Pearl, J. (1988). Probabilistic reasoning in intelligent sys-
Allais, M. (1979). The so-called allais paradox and rational            tems: networks of plausible inference. Morgan Kaufmann.
   decisions under uncertainty. Springer.                            Peot, M., & Smith, D. (1992). Conditional nonlinear plan-
Arthur, W. B. (1991). Designing economic agents that act                ning. . . . conference on Artificial intelligence planning . . . ,
   like human agents: A behavioral approach to bounded ra-              189–197.
   tionality. The American Economic Review, 81(2), 353–              Pezzulo, G., Lorini, E., & Calvi, G. (2004). How do I Know
   359.                                                                 how much I don’t Know? A cognitive approach about Un-
Camerer, C. F., & Ho, T. H. (1994). Violations of the be-               certainty and Ignorance. Proceedings of COGSCI . . . .
   tweenness axiom and nonlinearity in probability. Journal          Smithson, M. (2008). Psychology ambivalent view of un-
   of risk and uncertainty, 8(2), 167–196.                              certainty. Uncertainty and risk: Multidisciplinary perspec-
Casali, A., Godo, L., & Sierra, C. (2009). g-bdi: A graded              tives, 205–218.
   intensional agent model for practical reasoning. Modeling         Takahashi, T., Oono, H., Radford, M. H. B., & Others. (2007,
   Decisions for Artificial Intelligence, 5–20.                         January). Comparison of probabilistic choice models in
Davidsson, P. (2001). Multi agent based simulation: beyond              humans. Behavioral and Brain Functions, 3(1), 20.
   social simulation. Multi-Agent-Based Simulation, 141–             Tversky, A., & Fox, C. R. (1995). Weighing risk and uncer-
   155.                                                                 tainty. Psychological review, 102(2), 269.
Davidsson, P. (2002). Agent based social simulation: A com-          Tversky, A., & Kahneman, D. (1986). Rational choice and
   puter science view. Journal of artificial societies and social       the framing of decisions. Journal of business, 251–278.
   simulation, 5(1).                                                 Tversky, A., & Kahneman, D. (1992, October). Advances in
Dias, J., & Paiva, A. (2005). Feeling and reasoning: A com-             prospect theory: Cumulative representation of uncertainty.
   putational model for emotional characters. Progress in Ar-           Journal of Risk and uncertainty, 5(4), 297–323.
   tificial Intelligence, 127–140.                                   Wu, G., & Gonzalez, R. (1999). Nonlinear decision weights
Ellsberg, D. (1961). Risk, ambiguity, and the Savage axioms.            in choice under uncertainty. Management Science, 74–85.
   The Quarterly Journal of Economics, 643–669.
Heath, C., & Tversky, A. (1991). Preference and belief: Am-
   biguity and competence in choice under uncertainty. Jour-
   nal of risk and uncertainty, 28(1), 5–28.
Hertwig, R., Barron, G., Weber, E. U., & Erev, I. (2004).
   Decisions from experience and the effect of rare events in
   risky choice. Psychological Science, 15(8), 534–539.
Hirsh, J. B., Mar, R. a., & Peterson, J. B. (2012, April).
   Psychological entropy: A framework for understanding
   uncertainty-related anxiety. Psychological review, 119(2),
   304–20.
Hofstede, G. (2001). Culture’s consequences: Comparing
   values, behaviors, institutions, and organizations across
                                                                 2983

