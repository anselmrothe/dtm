UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Bayesian vector analysis and the perception of hierarchical motion
Permalink
https://escholarship.org/uc/item/9fm7g6jp
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Gerhsman, Samuel
Jaekel, Frank
Tenenbaum, Joshua
Publication Date
2013-01-01
Peer reviewed
  eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                Bayesian Vector Analysis and the Perception of Hierarchical Motion
Samuel J. Gershman1 (sjgershm@mit.edu), Frank Jäkel2 (fjaekel@uos.de), Joshua B. Tenenbaum1 (jbt@mit.edu)
                                                1 Department  of Brain and Cognitive Sciences, MIT
                                            2 Institute of Cognitive Science, University of Osnabrück
                                  Abstract                                  objects. In the example of biological motion (see Johans-
                                                                            son, 1973), the global motion of the body is subtracted from
     Scenes filled with moving objects are often hierarchically or-         the image, revealing the relative motions of body parts; these
     ganized: the motion of a migrating goose is nested within the
     flight pattern of its flock, the motion of a car is nested within      parts are further decomposed by the same subtraction opera-
     the traffic pattern of other cars on the road, the motion of body      tion.
     parts are nested in the motion of the body. Humans perceive               While the vector analysis theory provides a compelling
     hierarchical structure even in stimuli with two or three moving
     dots. An influential theory of hierarchical motion perception          explanation of numerous motion phenomena (we describe
     holds that the visual system performs a “vector analysis” of           several below), it is incomplete from a computational point
     moving objects, decomposing them into common and relative              of view, since it relies on the theorist to provide the un-
     motions. However, this theory does not specify how to resolve
     ambiguity when a scene admits more than one vector analysis.           derlying motion components and their organization; it lacks
     We describe a Bayesian theory of vector analysis and show that         a mechanism for discovering a hierarchical decomposition
     it can account for classic results from dot motion experiments.        from sensory data. This is especially important in complex
     Our theory takes a step towards understanding how moving
     scenes are parsed into objects.                                        scenes where many different vector analyses are consistent
                                                                            with the scene. Various principles have been proposed for
     Keywords: motion perception; Bayesian inference; structure
     learning                                                               how the visual system resolves this ambiguity. For exam-
                                                                            ple, Restle (1979) proposed a “minimum principle,” accord-
                                                                            ing to which simpler motion interpretations (i.e., those with a
                              Introduction
                                                                            shorter description length) are preferred over more complex
  Motion is a powerful cue for understanding the organization               ones. Gogel (1974) argued for an “adjacency principle,” ac-
  of a visual scene. Infants use motion to individuate objects,             cording to which the motion interpretation is determined by
  even when it contradicts property/kind information (Kellman               relative motion cues between nearby points. However, there
  & Spelke, 1983; Xu et al., 1999). The primacy of motion in-               is still no unified computational theory that can encompass all
  formation is also evident in adult object perception (Mitroff             these ideas.
  & Alvarez, 2007). In addition to individuating and tracking                  In this paper, we recast Johansson’s vector analysis the-
  objects, motion is used by the visual system to decompose                 ory in terms of a Bayesian model of motion perception. The
  objects into parts. In biological motion, for example, the mo-            model discovers the hierarchical structure of a moving scene,
  tion of body parts are nested in the motion of the body. Object           resolving the ambiguity of multiple vector analyses using a
  motion may be hierarchically organized into multiple layers:              set of probabilistic constraints. We show that this model can
  an arm’s motion may be further decomposed into jointed seg-               account for several classic phenomena in the motion percep-
  ments, including the hand, which can itself be decomposed                 tion literature that are challenging for existing models.
  into fingers, and so on.
     The hierarchical organization of motion presents a                                       Bayesian vector analysis
  formidable challenge to current models of motion process-                 In this section, we describe our computational model for-
  ing. It is widely accepted that the visual system balances                mally. We start by describing a probabilistic generative model
  motion integration over space and time (necessary for solv-               of motion—a set of assumptions about the environment that
  ing the aperture problem) and motion segmentation in order                we impute to the observer. The generative model can be
  to perceive multiple objects simultaneously (Braddick, 1993).             thought of as stochastic “recipe” for generating moving im-
  However, it is unclear how simple segmentation mechanisms                 ages. We then describe how Bayesian inference can be used
  can be used to build a hierarchically structured representation           to invert this generative model and recover the underlying hi-
  of a moving scene. Segmentation lacks a notion of nesting:                erarchical structure from observations of moving images.
  when an object moves, its parts should move with it. To un-
  derstand nesting, it is crucial to represent the underlying de-           Generative model
  pendencies between objects and their parts.
                                                                            Our model describes the process by which a sequence of two-
     The experimental and theoretical foundations of hierarchi-
                                                                            dimensional visual element positions {sn (t)}Nn=1 is generated,
  cal motion perception were laid by the pioneering work of                                           y
                                                                            where sn (t) = [sxn (t), sn (t)] is the x and y position of element n
  Johansson (1950), who demonstrated that surprisingly com-                                 1
                                                                            at time step t. Elements can refer to objects, parts or features;
  plex percepts could arise from simple dot motions. Johansson
  proposed that the visual system performs a “vector analysis”                  1 This representation assumes that basic perceptual preprocessing
  of moving scenes into common and relative motions between                 has taken place (e.g., the correspondence problem has been solved).
                                                                        489

                                                                                              Figure 1: Illustration of how a moving scene
                                                                                              is decomposed into a motion tree. Each node
                                                                                              in the tree corresponds to a motion component.
                                                                                              Each object in the scene traces a path through
                                                                                              the tree, and the observed motion of the object
                                                                                              is modeled as the superposition of motion com-
                                                                                              ponents along its path.
in this paper we will simply refer to them as objects. The ob-            where j indexes motion components, M j is the number of pre-
ject positions are modeled as arising from a tree-structured              vious objects assigned to component j, and J is the number
configuration of motion components; we refer to this repre-               of components currently in use (i.e., those for which M j > 0).
sentation as the motion tree. Each motion component is a                  The assignment at depth d is restricted to a unique set of com-
transformation that maps the current object position to a new             ponents specific to the component assigned at depth d − 1.
position.                                                                 In this way, the components form a tree structure, and cn is
   An illustration of a motion tree is shown in Figure 1. Each            a path through the tree. The parameter γ ≥ 0 controls the
node in the tree corresponds to a motion component. The mo-               branching factor of the motion tree. As γ decreases, different
tion of the train relative to the background is represented by            objects will tend to share the same motion components. Thus,
the top-level node. The motions of Spiderman and Dr. Oc-                  the nCRP exhibits a preference for trees that use a small num-
topus relative to the train are represented at the second-level           ber of motion components.
nodes. Finally, the motions of each body part relative to the                  Note that so far we have generated a path through a poten-
body are represented at the third-level nodes. The observed               tially very deep tree for each object. Each path has the same
motion of Spiderman’s hand can then be modeled as the su-                 length D. Remember that each node in the tree will represent
perposition of the motions along the path that runs from the              a motion component. We want each object n to be associ-
top node to the hand-specific node. The aim for our model is              ated with a node in the tree and its overall motion to be the
to get as inputs the retinal motion of pre-segmented objects—             sum of all the motion components above it (including itself).
in this example, the motion of hands, feet, torsos, windows,              Hence, for each object we need to sample an additional pa-
etc.—and output a hierarchical grouping that reflects the com-            rameter dn ∈ {1, . . . , D} that determines to which level on the
position of the moving scene.                                             tree the object will be assigned. This depth specifies a trunca-
   The motion tree can capture the underlying motion struc-               tion of cn , thereby determining which components along the
ture of many real-world scenes, but inferring which motion                path contribute to the observations. The depth assignments
tree generated a particular scene is challenging because dif-             d = [d1 , . . . , dN ] are drawn from a Markov random field:
ferent trees may be consistent with the same scene. To ad-                                          (                                 )
                                                                                                          N   N                     N
dress this problem, we need to introduce a prior distribution
over motion trees that expresses our inductive biases about
                                                                                      P(d) ∝ exp α       ∑ ∑ I[dm = dn ] − ρ ∑ dn       , (2)
                                                                                                         m=1 n>m                  n=1
what kinds of trees are likely to occur in the world. This prior
should be flexible enough to accommodate many different                   where the indicator function I[·] = 1 if its argument is true and
structures while also preferring simpler structures (i.e., parsi-         0 otherwise. The parameter α controls the penalty for assign-
monious explanations of the sensory data). These desiderata               ing objects to different depths, and the parameter ρ controls a
are satisfied by a nonparametric distribution over trees known            penalty for deeper level assignments.
as the nested Chinese restaurant process (nCRP; Blei et al.,                   Each motion component, i.e. each node in the motion
2010). The nCRP generates a motion tree by drawing, for                   tree, is associated with a time-varying flow field, f j (s,t) =
each object n, a sequence of motion components, denoted by                                y
                                                                          [ f jx (s,t), f j (s,t)]. We place a prior on flow fields that en-
cn = [cn1 , . . . , cnD ], where D is the maximal tree depth.2 The        forces spatial smoothness but otherwise makes no assump-
component assignments are drawn according to:                             tions about functional form. In particular we assume that f jx
                                                                                    y
                                  (     Mj
                                                                          and f j are spatial functions drawn independently at each time
                                       n−1+γ   if j ≤ J                   discrete time step t from a zero-mean Gaussian process with
         P(cnd = j|c1:n−1 ) =            γ                       (1)
                                       n−1+γ   if j = J + 1               covariance function
                                                                                                                   ||s − s0 ||2
                                                                                                                               
    2 As described in Blei et al. (2010), trees drawn from the nCRP                                   0
                                                                                                k(s, s ) = τ exp −                ,       (3)
can be infinitely deep, but we impose a maximal depth for simplicity.                                                  2λ
                                                                      490

where τ is a global scaling parameter and λ > 0 is a length-
scale parameter controlling the smoothness of the flow field.
When λ is large, the flow field becomes rigid. Smoothness is
only enforced between objects covered by the same node in
the motion tree.
   To complete the generative model, we need to specify how
the motion tree gives rise to observations, which in our case
are the positions of the N objects over time. For each object,
the dot position at the next time step is set by sampling a
displacement from a Gaussian whose mean is the sum of the
flow fields along path cn truncated at dn :
                                    dn
           sn (t + 1) = sn (t) +   ∑ fcnd (sn (t),t) + εn (t),     (4)     Figure 2: Johansson (1950) two dot experiment. (A) Veridi-
                                   d=1                                     cal motion vectors. (B) Perceived motion. (C) Inferred mo-
                                                                           tion vectors. Each color corresponds to a different component
where εn (t) ∼ N (0, σ2 I).
                                                                           in the motion tree (D), but note that a component will predict
   This generative model contains a number of important spe-
                                                                           different vectors depending on spatial location.
cial cases under particular parameter settings. When γ = 0,
only one motion component will be generated; in this case,
the prior on flow-fields—favoring local velocities close to 0              where
that vary smoothly over the image—resembles the “slow and
smooth” model proposed by Weiss & Adelson (1998). When                                 Kmn (t) = k(sm (t), sn (t)) ∑ I[ j ∈ cm ∧ j ∈ cn ].   (7)
γ = 0 and λ → ∞, we obtain the “slow and rigid” model of                                                              j
Weiss et al. (2002). When D = 1, the model will generate
multiple motion components, but these will all exist at the                Intuitively, the covariance between two points counts the
same level of the hierarchy (i.e., the motion tree is flat, with           number of nodes shared between their paths, weighted by
no nesting), resulting in a form of transparent layered motion             their proximity in space.
(Wang & Adelson, 1993; Weiss, 1997).                                           The conditional distribution over dn is given by:
Inference                                                                                P(dn |c, s, d−n ) ∝ P(dn |d−n )P(s|c, d−n , dn ),   (8)
The goal of inference is to compute the posterior over the mo-
tion tree given a set of observations.3 Because we are mainly              where d−n denotes the level assignments excluding dn and
interested in the highest probability tree, we use annealed                                                  (                         )
Gibbs sampling to search for the posterior mode. The algo-                             P(dn |d−n ) ∝ exp α
rithm alternates between holding the depth assignments fixed
                                                                                                                 ∑ I[dm = dn ] − ρdn       . (9)
                                                                                                                 m6=n
while sampling the node assignments, and holding the node
assignments fixed while sampling the depth assignments. By                     To visualize the motion components that are given by a
raising the conditional probabilities to a power β > 1, the                grouping through dn and cn , we can calculate the posterior
posterior becomes peaked around the mode. We gradually                     predictive mean for object n at each component j (shown here
increase β, so that the algorithm eventually settles on a high             for the x dimension):
probability tree. We repeat this procedure 10 times (with 500
                                                                                                                    2 −1 x
sampling iterations on each run) and pick the tree with the                  E[ f jx (sn (t),t)] = k>                                  x
                                                                                                      n j (K(t) + σ I) (s (t + 1) − s (t)), (10)
highest posterior probability. Below, we derive the condi-
tional distributions used by the sampler.                                  where kn j is the N-dimensional vector of covariances be-
   The conditional distribution over cn is given by:                       tween sn (t) and the locations of all the objects whose paths
                                                                           pass through node j (if an object does not pass through node
                P(cn |c−n , s, d) ∝ P(cn |c−n )P(s|c, d),          (5)      j then its corresponding entry in kn j is 0).
where c−n denotes the set of all paths excluding cn . The first                                            Simulations
factor in Eq. 5 is the nCRP prior (Eq. 1). The second factor               In this section, we show how the Bayesian vector analysis
in Eq. 5 is the likelihood of the data, given by:                          model can account for several classic experimental phenom-
                                                                           ena. These experiments all involve stimuli consisting of mov-
    P(s|c, d) = ∏       ∏      N (sz (t + 1); sz (t), K(t) + σ2 I) (6)     ing dots, so for present purposes sn (t) corresponds to the posi-
                   t z∈{x,y}
                                                                           tion of dot n at time t. In these simulations we use the follow-
    3 The latent motion components can be marginalized analytically        ing parameters: D = 3, σ2 = 0.01, τ = 1, λ = 100, α = 1, ρ =
using properties of Gaussian processes.                                    0.1. The interpretation of σ2 and λ depend on the spatial scale
                                                                       491

                                                                                      A                              cycloid
                                                                                      B     rotation
                                                                                                                       translation
                                                                       Figure 4: Duncker wheel. (A) A light on the rim of a rolling
                                                                       wheel produces cycloidal motion. (B) Adding a light on the
                                                                       hub produces rolling motion (translation + rotation).
Figure 3: Johansson (1973) three dot experiment. (A)                                     Stimulus                          Model
Veridical motion vectors. (B) Perceived motion. (C) Inferred
motion vectors. (D) Inferred motion tree.
of the data; in general, we found that changing these param-
eters (within the appropriate order of magnitude) had little
influence on the posterior. We set λ to be large enough so that
objects assigned to the same layer moved near-rigidly.
   Johansson (1950) demonstrated that a hierarchical motion
percept can be achieved with as few as two dots. Figure 2A
shows the stimulus used by Johansson, consisting of two dots
translating orthogonally to meet at a single point. Observers,
however, do not perceive the orthogonal translation. Instead,          Figure 5: Simulations of the Duncker wheel. (Top) A sin-
they perceive the two dots translating along a diagonal axis           gle light on the rim produces one vector following a cy-
towards each other, which itself translates towards the meet-          cloidal path. (Middle) Adding a light on the hub produces
ing point (Figure 2B). Thus, observers perceive the stimulus           two vectors: translation + rotation, giving rise to the percept
as organized into common and relative motions. This percept            of rolling motion. (Bottom) Placing the light on the interior of
is reproduced by the Bayesian vector analysis model (Figure            the wheel produces weaker rolling motion: the translational
2C); the inferred motion tree (shown in Figure 2D) represents          component is no longer perfectly horizontal.
the common motion as the top level component and the rel-
ative motions as subordinate components. The subordinate
components are not perfectly orthogonal to the diagonal mo-            two-level hierarchy (translation + rotation) is provided by the
tion, consistent with the findings of Wallach et al. (1985); this      hub light.4 It has also been observed that placing a light in
arises in our model through a form of “explaining away”—               between the rim and the hub produces weaker rolling motion
i.e., posterior coupling between the motion layers implied by          (i.e., the translational component is no longer perfectly hori-
Eq. 10.                                                                zontal; Proffitt et al., 1979), a phenomenon that is reproduced
   Another example studied by Johansson (1973) is shown in             by Bayesian vector analysis (Figure 5, bottom).
Figure 3A. Here the bottom and top dot translate horizon-                 So far, we have been considering qualitative characteriza-
tally while the middle dot translates diagonally such that all         tions of various motion phenomena, but one advantage of a
three dots are always collinear. The middle dot is perceived           computational model is its ability to make quantitative predic-
as translating vertically as all three dots translate horizontally     tions. We illustrate the quantitative power of Bayesian vec-
(Figure 3B). Consistent with this percept, the Bayesian vector         tor analysis for the case of motion transparency. When two
analysis assigns all three dots to a common horizontal motion          groups of randomly moving dots are superimposed, observers
component, and additionally assigns the middle dot to a ver-           may see either transparent motion (two planes of motion slid-
tical motion component (Figure 3C-D).                                  ing past each other) or non-transparent motion (all dots mov-
                                                                       ing in the direction of the average motion of the two groups).
   Duncker (1929) showed that if a light is placed on the rim
                                                                       Which percept prevails depends on the relative direction of
of a rolling wheel in a dark room, cycloidal motion is per-
                                                                       the two groups (Braddick et al., 2002): as the direction differ-
ceived (Figure 4A), but if another light is placed on the hub
                                                                       ence increases, transparent motion becomes more percepti-
then rolling motion is perceived (Figure 4B). Simulations of
                                                                       ble. We computed the probability of transparent motion (i.e.,
these experiments are shown in Figure 5. When a light is
placed only on the rim, there is strong evidence for a single              4 Note that the model does not explicitly represent rotation but in-
cycloidal motion component, whereas stronger evidence for a            stead represents the tangential motion component in each time step.
                                                                   492

                               1                                                                            An interesting test case is biological motion perception: Jo-
                                                                                                            hansson (1973) showed that observers can recognize human
                              0.8
                                                                                                            motions like walking and running from lights attached to the
             P(transparent)
                              0.6                                                                           joints. Later work has revealed that a rich variety of infor-
                                                                                                            mation can be discriminated by observers from point light
                              0.4
                                                                                                            displays, including gender, weight and even individual iden-
                              0.2                                                                           tity (Blake & Shiffrar, 2007). We trained our model (with
                                                                                                            the same parameters) on point light displays derived from
                               0
                               0.5       1     1.5                                  2       2.5             the CMU human motion capture database.5 These displays
                                     Relative direction (rad)
                                                                                                            consisted of the 3-dimensional positions of 31 dots, includ-
                                                                                                            ing walking, jogging and sitting motions. The resulting mo-
Figure 6: Simulations of transparent motion. Transparency
                                                                                                            tion parse is illustrated in Figure 8: the first layer of motion
increases as a function of direction difference between two
                                                                                                            (not shown) captures the overall trajectory of the body, while
superimposed groups of dots.
                                                                                                            the second and third layers capture more fine-grained struc-
                                                                                                            ture, such as the division into limbs and smaller jointed body
        A                                                                       B
                                                                          0.8                               parts. Note that the model knows nothing about the underly-
                                                 Perceived motion speed
                                                                                                            ing skeletal structure; it infers body parts directly from the dot
                                                                          0.6
                                                                                                            positions. This demonstrates that Bayesian vector analysis
              A                      B
                                                                                                            can scale up to more complex and realistic motion patterns.
                                                                          0.4
                                                                          0.2
                                                                                                                                     Conclusion
                                                                                                            How does the visual system parse the hierarchical struc-
                                                                           0
                                                                                        A         B
                                                                                                            ture of moving scenes? In this paper, we have developed a
                                                                                                            Bayesian framework for modeling hierarchical motion per-
                                                                                                            ception, building upon the seminal work of Johansson (1950).
Figure 7: Motion contrast. (A) The velocity of the back-
                                                                                                            The key idea of our theory is that a moving scene can
ground (black) dots increases along the horizontal axis. Al-
                                                                                                            be interpreted in terms of an abstract graph—the motion
though A and B have the same velocity, A is perceived as
                                                                                                            tree—encoding the dependencies between moving objects.
moving faster than B. (B) Model simulation.
                                                                                                            Bayesian vector analysis is the process of inferring the mo-
                                                                                                            tion tree from a sequence of images. Our simulations demon-
                                                                                                            strated that this formalism is capable of capturing a number
two layers in our model) for a range of relative directions us-
                                                                                                            of classic phenomena in the literature on hierarchical motion
ing 20 dots. As the relative direction increases, the statistical
                                                                                                            perception.
evidence in favor of two separate layers increases, resulting
in a smoothly changing probability (Figure 6).                                                                 Two limitations of our theory need to be addressed. First,
                                                                                                            the generative model assumes that motion components com-
   Inferences about the motion hierarchy may interact with
                                                                                                            bine through summation, but this is not adequate in general.
the spatial structure of the scene. The phenomenon of mo-
                                                                                                            For example, a better treatment of the Duncker wheel would
tion contrast, originally described by Loomis & Nakayama
                                                                                                            entail modeling the composition of rotation and translation.
(1973), provides an illustration: The perceived motion of a
                                                                                                            In its current form, the model approximates rotation by infer-
dot depends on the motion of surrounding “background dots”
                                                                                                            ring motion components that are tangent to the curve traced
(the black dots in Figure 7A). If a set of dots moves on a
                                                                                                            by the rotation. We are currently investigating a version of
screen such that the dots on the left move more slowly than
                                                                                                            the generative model in which motion transformation com-
dots on the right, they form a velocity gradient. Two “target”
                                                                                                            pose with one another, which would allow for nonlinear in-
dots that move with the same velocity and keep a constant
                                                                                                            teractions. Second, although we described an algorithm for
distance (the red dots in Figure 7A) can still be perceived
                                                                                                            finding the optimal motion tree, Bayesian vector analysis is
as moving with radically different speeds, depending on the
                                                                                                            really specified at the computational level; our simulations
speed of the dots close by. In our model, most of the motion
                                                                                                            are not illuminating about the mechanisms by which the vec-
of the velocity gradient is captured by the Gaussian process
                                                                                                            tor analysis is carried out. Nor does it commit to any partic-
on the top-level motion component. However, this top-level
                                                                                                            ular neural implementation. More work is needed to connect
component does not capture all of the motion of each dot.
                                                                                                            all these levels of analysis. Grossberg et al. (2011) have de-
The target dots (in red), in particular, are each endowed with
                                                                                                            scribed a detailed theory of how vector analysis could be per-
their own motion component and move relative to the top-
                                                                                                            formed by the visual cortex, and their efforts offer a possible
level node. This relative motion differs depending on where
                                                                                                            starting point.
along the gradient the target dot is located, resulting in motion
                                                                                                               We view hierarchical motion as a model system for study-
contrast (Figure 7B).
   How does our model scale up to more complex displays?                                                       5 http://mocap.cs.cmu.edu/
                                                                                                      493

A                                            B
                                                                                 Figure 8: Analysis of human motion capture
                                                                                 data. Each color represents the assignment of
                                                                                 a node to a motion component. All nodes are
                                                                                 trivially assigned to the first layer (not shown).
                                                                                 In addition, all nodes were assigned to the sec-
                                                                                 ond layer (A). A subset of the nodes were also
                                                                                 assigned components in the third layer (B). Un-
                                                                                 filled nodes indicate that no motion component
                                                                                 was assigned at that layer. The skeleton is
                                                                                 shown here for display purposes; the model was
                                                                                 trained only on the dot positions.
ing more general questions about structured representations        Johansson, G. (1973). Visual perception of biological motion
in mind and brain. The simplicity of the stimuli makes them          and a model for its analysis. Perception, & Psychophysics,
amenable to rigorous psychophysical and neurophysiologi-             14(2), 201–211.
cal experimentation, offering hope that future work can iso-       Kellman, P., & Spelke, E. (1983). Perception of partly oc-
late the neural computations underlying structured represen-         cluded objects in infancy. Cognitive Psychology, 15(4),
tations like motion trees.                                           483–524.
                                                                   Loomis, J., & Nakayama, K. (1973). A velocity analogue of
                     Acknowledgments
                                                                     brightness contrast. Perception, 2(4), 425–427.
We thank Ed Vul and Peter Battaglia for helpful discussions.
                                                                   Mitroff, S., & Alvarez, G. (2007). Space and time, not surface
This work was supported by the Deutsche Forschungsge-
                                                                     features, guide object persistence. Psychonomic Bulletin &
meinschaft (DFG JA 1878/1-1), ONR MURI N00014-07-1-
                                                                     Review, 14, 1199–1204.
0937, IARPA ICARUS program, and the MIT Intelligence
Initiative.                                                        Proffitt, D., Cutting, J., & Stier, D. (1979). Perception of
                                                                     wheel-generated motions. Journal of Experimental Psy-
                          References                                 chology: Human Perception and Performance, 5, 289–
Blake, R., & Shiffrar, M. (2007). Perception of human mo-            302.
   tion. Annual Review of Psychology, 58, 47–73.                   Restle, F. (1979). Coding theory of the perception of motion
Blei, D., Griffiths, T., & Jordan, M. (2010). The nested chi-        configurations. Psychological Review, 86(1), 1–24.
   nese restaurant process and Bayesian nonparametric infer-       Wallach, H., Becklen, R., & Nitzberg, D. (1985). Vector anal-
   ence of topic hierarchies. Journal of the ACM, 57, 7.             ysis and process combination in motion perception. Jour-
Braddick, O. (1993). Segmentation versus integration in vi-          nal of Experimental Psychology: Human Perception and
   sual motion processing. Trends in Neurosciences, 16, 263–         Performance, 11, 93–102.
   268.                                                            Wang, J., & Adelson, E. (1993). Layered representation for
Braddick, O., Wishart, K., & Curran, W. (2002). Direc-               motion analysis. In Computer vision and pattern recogni-
   tional performance in motion transparency. Vision Re-             tion (pp. 361–366).
   search, 42(10), 1237–1248.                                      Weiss, Y. (1997). Smoothness in layers: Motion segmenta-
Duncker, K. (1929). Über induzierte Bewegung. (Ein Beitrag          tion using nonparametric mixture estimation. In Computer
   zur Theorie optisch wahrgenommener Bewegung). Psy-                vision and pattern recognition (pp. 520–526).
   chologische Forschung, 12, 180-259.                             Weiss, Y., & Adelson, E. (1998). Slow and smooth: A
Gogel, W. (1974). Relative motion and the adjacency prin-            bayesian theory for the combination of local motion sig-
   ciple. The Quarterly Journal of Experimental Psychology,          nals in human vision. AI Memo 1616, MIT.
   26, 425–437.                                                    Weiss, Y., Simoncelli, E., Adelson, E., et al. (2002). Motion
Grossberg, S., Léveillé, J., & Versace, M. (2011). How do          illusions as optimal percepts. Nature Neuroscience, 5(6),
   object reference frames and motion vector decomposition           598–604.
   emerge in laminar cortical circuits? Attention, Perception,     Xu, F., Carey, S., Welch, J., et al. (1999). Infants’ ability to
   & Psychophysics, 73(4), 1147–1170.                                use object kind information for object individuation. Cog-
Johansson, G. (1950). Configurations in event perception.            nition, 70, 137–166.
   Almqvist & Wiksell.
                                                               494

