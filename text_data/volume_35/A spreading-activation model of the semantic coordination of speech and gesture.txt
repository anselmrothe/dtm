UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A spreading-activation model of the semantic coordination of speech and gesture
Permalink
https://escholarship.org/uc/item/3vv1t6vv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Kopp, Stefan
Bergmann, Kirsten
Kahl, Sebastian
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                                A spreading-activation model
                             of the semantic coordination of speech and gesture
                                         Stefan Kopp (skopp@TechFak.Uni-Bielefeld.DE)
                                   Kirsten Bergmann (kirsten.bergmann@Uni-Bielefeld.DE)
                                        Sebastian Kahl (skahl@TechFak.Uni-Bielefeld.DE)
                     Faculty of Technology, Center of Excellence “Cognitive Interaction Technology” (CITEC),
                               Collaborative Research Center “Alignment in Communication” (SFB 673)
                                  Bielefeld University, P.O. Box 100 131, D-33501 Bielefeld, Germany
                               Abstract                                   empirical findings on semantic coordination of speech and
   In naturally occurring speech and gesture, meaning occurs or-          gesture, and we discuss mechanisms and models that have
   ganized and distributed across the modalities in different ways.       been put forward to explain it. We argue that building com-
   The underlying cognitive processes are largely unexplored.             putational models helps to elucidate the mechanisms and to
   We propose a model based on activation spreading within dy-
   namically shaped multimodal memories, in which coordina-               bridge the gap between descriptive models and observable
   tion arises from the interplay of visuo-spatial and linguistically     behavior. We propose the first model to present a detailed
   shaped representations under given communicative and cogni-            cognitive account of how meaning can be organized and coor-
   tive resources. An implementation of this model is presented
   and first simulation results are reported.                             dinated in speech and gesture. It is based on tenets of activa-
   Keywords: Speech, gesture, conceptualization, semantic co-             tion spreading in multimodal memory representations and it
   ordination, activation spreading                                       entails a number of, now explorable, assumptions about con-
                                                                          ceptualization of speech and gesture. We describe an imple-
                           Introduction                                   mentation of this model and present first results on how it can
Gestures are an integral part of human communication and                  simulate and explain different cases of semantic coordination
they are inseparably intertwined with speech (McNeill &                   reported in the literature.
Duncan, 2000). The detailed nature of this connection, how-
ever, is still a matter of considerable debate. The data that un-
                                                                                                  Background
derlie this debate have for the most part come from studies on            Semantic coordination of speech and gesture
the coordination of overt speech and gestures showing that the            A number of studies have shown that concomittant speech
two modalities are coordinated in their temporal arrangement              and gesture are coordinated in meaning. One line of evidence
and in meaning, but with considerable variations. When oc-                coming from cross-linguistic studies suggest that packaging
curring in temporal proximity, the two modalities express the             of content for co-speech gestures is influenced by the infor-
same underlying idea, however, not necessarily identical as-              mation packaging for the accompanying speech. For exam-
pects of it: Iconic gestures can be found to be redundant with            ple, Kita and Özyürek (2003) showed that speakers of En-
the information encoded verbally (e.g., ’round cake’ + gesture            glish who are able to combine manner and path of a move-
depicting a round shape), to supplement it (e.g., ‘cake’ + ges-           ment in a single clause (e.g. ‘he rolled down’ or ‘he swings’)
ture depicting a round shape), or even to complement it (e.g.,            accompanied this by a single gesture encoding both seman-
‘looks like this’ + gesture depicting a round shape). These               tic features. In contrast, Turkish and Japanese speakers en-
variations in meaning coordination–in combination with tem-               coded manner and path separately in two clauses (e.g. ‘he de-
poral synchrony–led to different hypotheses about how the                 scended as he rolled’) and are more likely to use two separate
two modalities encode aspects of meaning and what mutual                  gestures for these two features. Along the same line, when
influences between the two modalities could underlie this.                native speakers of Turkish (L1) speak English as their second
However, a concrete picture of this and in particular of the              language (L2) at different levels of proficiency, their gestures
underlying cognitive processes is still missing.                          were shown to follow the information packaging strategy they
   In previous work (Bergmann & Kopp, 2009) we explored                   adopt (Özyürek, 2002): Advanced L2 speakers typically en-
how the surface form of speech and gesture is determined and              coded manner and path information in one clause and their
how this formulation process can be simulated in a computa-               gestures followed, where as speakers at lower proficiency
tional model. In this paper we turn to the preceding stage,               levels typically used two-clause constructions in speech thus
namely, conceptualization by which meaning is structured,                 following the structure of Turkish, accompanied by separate
portioned and distributed across the two modalities, yielding             gestures for manner and path. A subsequent study (Kita et
different kinds of semantic coordination one can see in real-             al., 2007) showed that this effect also occurrs when native
life natural behavior. We thereby focus on speech along with              speakers of English are forced to produce one- or two-clause
shape-depicting (iconic) gestures. We start with reviewing the            descriptions of manner and path.
                                                                      823

   Other studies have investigated the cognitive factors that       resentation is transformed into a preverbal message, and an
influence frequency and nature of gesturing, including its co-      imagistic representation is transformed into a so-called sketch
ordination with speech. Bavelas, Kenwood, Johnson, and              and sent to a gesture planner. Kita and Özyürek (2003) agree
Philips (2002) found that speakers are more likely to pro-          that gesture and speech are two separate systems interacting
duce non-redundant gestures when their addressees could see         during the conceptualization stage. Based on cross-linguistic
them, as opposed to when their gestures are not visible and         evidence, their account holds that language shapes iconic ges-
hence less essential for their partners. Bergmann and Kopp          tures such that the content of a gesture is determined by three
(2006) report results from an analysis of natural gesturing         factors: (1) a communicative intention, (2) action schemata
in direction-giving, indicating that supplementary iconic ges-      selected on the basis of features of imagined or real space, (3)
tures are more likely in cases of problems of speech produc-        bidirectional interactions between speech and gesture produc-
tion (e.g. disfluencies) or when the information conveyed is        tion processes at the level of conceptualization, i.e. the orga-
introduced into the dialogue (and thus conceptualized for the       nization of meaning. An additional link between the speech
first time). In line with this, recent work has suggested that      formulator and the preverbal message generator allows for
speakers indeed produce more gestures at moments of rela-           feedback from grammatical or phonological encoding to the
tively high load on the conceptualization process for speaking      conceptualizer and thus to gesture.
(Kita & Davies, 2009), in particular on the linearization and          Hostetter and Alibali (2008) proposed the Gestures as Sim-
the focusing components of conceptualization (Melinger &            ulated Action framework that emphasizes how gestures may
Kita, 2007). Hostetter and Alibali (2007) report findings sug-      arise from an interplay of mental imagery, embodied simu-
gesting that speakers who have stronger visual-spatial skills       lations, and language production. According to this view,
than verbal skills produce higher rates of depictive gestures       language production evokes enactive mental representations
than other speakers. In a later study, Hostetter and Al-            which give rise to motor activation. Whether a gesture is pro-
ibali (2011) found that the speakers with high spatial skills       duced or not depends on the amount of motor activation, the
also produced a higher proportion of non-redundant gesture-         speaker’s variable gesture threshold, and the simultaneous en-
speech combinations than other speakers, whereas verbal-            gagement of the motor system for speaking.
dominant speakers tended to produce such gestures more in              Inspite of a consistent theoretical picture starting to
case of speech disfluencies. The authors hypothesize that           emerge, many questions about the detailed mechanisms re-
“non-redundant gesture-speech combinations occur because            main open. A promising approach to explicate and test hy-
mental images are more active in speakers minds at the mo-          potheses are cognitive models that allow for computational
ment of speaking than are verbal codes” [p.45]. Taken to-           simulation. However, such modeling attempts for the pro-
gether, this suggests that non-redundant gesture-speech com-        duction of speech and gestures are almost inexistent. Only
binations are the result of speakers having both strong spatial     Breslow, Harrison, and Trafton (2010) proposed an integrated
knowledge and weak verbal knowledge simultaneously, and             production model based on the cognitive architecture ACT-R
avoiding the effort of transforming the one into the other.         (Anderson, Bothell, Byrne, Lebiere, & Qin, 2004). This ac-
Models of speech and gesture production                             count draws on two major assumptions: (1) on Jackendoff’s
                                                                    claim that language representations include some irreducibly
Different models of speech and gesture production have been         spatial components; (2) on Goldberg’s approach according to
proposed. One distinguishing feature is the point where             which language processing is based on constructions which
cross-modal coordination can take place. The Growth Point           consist of both semantic and syntactic components. The au-
Theory (McNeill & Duncan, 2000) assumes that gestures               thors assume such constructions to prescribe spatial represen-
arise from idea units combining imagery and categorial con-         tations for what they call linguistic spatial gestures and which
tent. This combination is unstable and initiates dynamic            they assume to provide “little information not included in the
cognitive events through which speech and gesture unfold.           accompanying language” [p.14]. In this view, constructions
Speech and gesture, in this view, are inseparable and interact      are selected first and then words and gestures are determined
throughout the production process.                                  so as to realize the construction. Accordingly, semantic coor-
   Assuming that gestures are generated “pre-linguistically”,       dination is predetermined and does not result from a coordi-
Krauss, Chen, and Gottesman (2000) hold that gesture are            nation process based on problems with lexicalization or high
generated from a mental representation of a source concept          activation of particular visuo-spatial features. This model
comprising a set of semantic features (size, color, shape etc.)     hence has difficulties, e.g., to explain gestures that clearly
that are encoded in propositional and/or spatial format. While      complement or supplement verbally encoded meaning.
there is no influence of language production onto gesture in
this model, the readily planned and executed gesture facili-                     A spreading-activation model
tates lexical retrieval through cross-modal priming.
   De Ruiter (2000) proposed that speech-gesture coordina-          We investigate to what extent semantic coordination of
tion arises from a multimodal conceptualization process that        speech and gesture can be explained by cognitive principles
selects the information to be expressed in each modality and        of activation-based processing on multimodal memory. This
assigns a perspective for the expression. A propositional rep-      account is embedded in a larger production model (Kopp,
                                                                824

Bergmann, & Wachsmuth, 2008) that comprises three stages:                                              more or less coherent, multimodal message. These processes
conceptualization, where a message generator and an image                                              are constrained by principles of memory retrieval, which we
generator work together to select and organize information to                                          assume can be modeled by principles of activation spread-
be encoded in speech and gesture, respectively; formulation,                                           ing (Collins & Loftus, 1975). As in cognitive architectures
where a speech formulator and a gesture formulator deter-                                              like ACT-R (Anderson et al., 2004), activations float dynam-
mine appropriate verbal and gestural forms for this; motor                                             ically, spread across linked entities (in particular via SMCs),
control and articulation to finally execute the behaviors. Mo-                                         and decay over time. Activation of more complex SMCs are
tor control, articulation, and formulation have been subject of                                        assumed to decay more slowly than activation in lower VSR
earlier work (Bergmann & Kopp, 2009). What is missing is                                               or SPR.
a model for multimodal conceptualization that accounts for
the range of semantic coordination we see in real-life speech-                                                               Dynamic multimodal memory
gesture combinations.
                                                                                                                                      SMC
                                                                                                                                  (supramodal)
Basic assumptions
We posit that the semantic coordination of speech and ges-                                                                VSR                          SPR
                                                                                                               Image    (visual-                    (symbolic-    Message
ture emerges from (1) the communicative goal, (2) the need                                                   generator  spatial)                  propositional)  generator
to activate, retrieve and organize multimodal information to
achieve this goal, and (3) the expressive as well as cogni-
tive resources available to the speaker at the moment. To
model this process, we make a number of assumptions,                                                          Gesture                                              Speech
                                                                                                            formulator                                           formulator
partly in line with previous models. First, language pro-
duction requires a preverbal message to be formulated in
a symbolic-propositional representation that is linguistically
shaped (Slobin, 1996; Levelt, 1989) (SPR, henceforth). Dur-                                                    Motor
                                                                                                                                                                 Articulator
                                                                                                              control
ing conceptualization the SPR, e.g. a function-argument
structure denoting a spatial property of an object, often needs                                               Gesture                                              Speech
to be extracted from visuo-spatial representations (VSR), e.g.
the mental image of this object. We assume this process
to involve the invokation and instantiation of memorized                                                          Figure 2: Overall production architecture.
supramodal concepts (SMC, henceforth), e.g. the concept
‘round’ which links the corresponding visuo-spatial proper-                                               Production starts with the message generator and image
ties to a corresponding propositional denotation. Co-verbal                                            generator inducing local activations of modal entries, evoked
iconic gestures are then shaped by (1) the imagistic content in                                        by a communicative goal. VSRs that are sufficiently activated
VSR, (2) the invoked SMCs, and (3) the organization of SPR                                             invoke matching SMCs, leading to an instantiation of SPRs
for linguistic processing. We assume that units or entries of                                          representing the corresponding visuo-spatial knowledge in
these memory structures can be selectively activated and that                                          linguistically shaped ways. The generators independently se-
activation spreads along links between them. Fig. 1 illustrates                                        lect modal entries and pass them on to the formulators. As in
the overall relation between the three memory structures.                                              ACT-R, highly activated features or concepts are more likely
                                                                                                       to be retrieved and thus to be encoded. Note that, as acti-
                                                      Supramodal conceptual                            vation is dynamic, feature selection depends on the time of
                                                         left-of
                                                                                                       retrieval and thus available resources. The message gener-
              Visual-spatial
                                     similarity
                                                                            Symbolic-propositional     ator has to map activated concepts in SPR onto grammati-
                                                              top-of
               Schema
                                                                                                       cally determined categorical structures, anticipating what the
                                                                                   Left-of
                                         similarity                  round
                                                                                                       speech formulator is able to process (cf. (Levelt, 1989)). Im-
       Schema                                                                       top-of
                       Schema
                                           similarity
                                                                                round
                                                                                                       portantly, interaction between generators and formulators in
              Schema          Schema                                                                   each modality can run top-down and bottom-up. For exam-
                                                                                                       ple, a proposition being encoded by the speech formulator re-
                                                                                                       sults in reinforced activation of the concept in SPR, and thus
                                                                                                       increased activation of associated concepts in VSR.
Figure 1: Multimodal memory structures involed in speech-
gesture production (activations indicated by bold lines).                                                 In result, semantic coordination emerges from the local
                                                                                                       choices generators and formulators take, based on the ac-
                                                                                                       tivation dynamics in multimodally linked memory repre-
Overall production process                                                                             sentations. Redundant speech and gesture result from fo-
Fig. 2 shows an outline of the overall production architecture.                                        cused activation of supramodally linked mental representa-
Conceptualization consists of cognitive processes that oper-                                           tions, whereas non-redundant speech and gesture arise when
ate upon the abovementioned memory structures to create a,                                             activations scatter over entries not connected via SMCs.
                                                                                                   825

                Computational simulation                                (left) shows the activations of two linked entries. At point
We have implemented the activation-based model of seman-                t = 200 one entry gets temporarily activated and the activa-
tic coordination within our larger speech and gesture produc-           tion of the linked entry follows. The second important prop-
tion architecture (Bergmann & Kopp, 2009). Newly imple-                 erty of this rule is that activation of the more global SMC atsmc
mented parts are the VSR, SPR and SMC memory structures,                spreads to both linked entries, such that both are “pulled” to-
the activation dynamics upon these structures, and the gener-           wards this value. This can be seen in Fig. 3 (right) where
ator modules operating on them.                                         the SMC’s activation is increased by 2.0 at point t = 100.
                                                                        Note that activation of SMCs decays more slowly than ac-
Representations                                                         tivation of VSR and SPR entries. Activations of linked en-
To realize the VSR and part of the SMC, we employ a model               tries thus stabilizes at a higher level, such that stable multi-
of visuo-spatial imagery called Imagistic Description Trees             modal information packages emerge for a limited period of
(IDT) (Sowa & Kopp, 2003). The IDT model was designed,                  time. The duration of this time period depends on the decay
based on empirical data, to cover the meaningful visuo-spatial          rate of SMC activations. Finally, memory retrieval depends
features in shape-depicting iconic gestures. Important aspects          on the activation of the entries being retrieved. We adopt the
include (1) a tree structure for shape decomposition with ab-           ACT-R approach to map activation onto retrieval probability:
                                                                                       −(at −s)
stract object schemas as nodes, (2) extents in different di-            p = 1/(1 + e r ), where s is a threshold and r the noise in
mensions as an approximation of shape, and (3) the possi-               the activation levels.
bility of dimensional information to be underspecified. The
latter occurs, e.g., when the axes of an object schema cover
less than the three dimensions of space or when an exact di-
mensional extent is left open but only a coarse relation be-
tween axes like “dominates” is given. This allows to repre-
sent the visuo-spatial properties of SMCs such as ‘round’,
‘left-of’ or ‘longish’. Applying SMC to VSR is realized
through graph unification and similarity matching between
object schemas, yielding similarity values that assess how
well a certain SMC applies to a particular visuo-spatially rep-
resented entity (cf. Fig. 1). SPR are implemented straight              Figure 3: Activations of two memory entry linked via an
forward as predicate-argument sentences.                                SMC: temporary activation of one entry (left); activation of
                                                                        the linking SMC (right).
Activation dynamics
Each memory entry in VSR, SPR and SMC has a time-
dependent activation value at . Activation dynamics results
                                                                        Generators
from simple update and spreading rules applied to these val-            The message generator has to package activated SPR infor-
ues in each iteration of a stepwise cognitive simulation pro-           mation in a way that the speech formulator can produce an ap-
cess. At each step all of the following updates are performed:          propriate construction. We employ an LTAG-based (Lexical-
                                                                        ized Tree Adjoining Grammar) sentence planner for speech
• Activation update for memory entries: at+1 = at − d + r,              formulation (cf. (Bergmann & Kopp, 2009)). To make sure
   with decay d, random noise r (order of magnitude 10−1 )              that all facts necessary to generate a specific construction are
                                                at                      available, the message generator applies networks that reflect
• Activation spreading within VSR: at+1 = c·l      , where c is the
                                                                        the encoding options provided by the speech formulator’s
   number of outgoing links (fan-out effect) and l is the depth
                                                                        LTAG grammar (this conforms the view that the conceptu-
   in the hierarchical IDT structure (fade-out effect)
                                                                        alizer learns to anticipate the formulator’s abilities (Levelt,
• Activation spreading from SPR towards VSR via SMC:                    1989)). These message networks consist of type nodes for
                  spr
            vsr
     vsr = at +at + α · (asmc − avsr ) + r − d, where α controls        entities, properties of entities and relations between them.
   at+1         2          t       t
                                                                        These are connected via weighted links reflecting the com-
   the rate of convergence towards the SMC activation.
                                                                        bination of particular linguistic types in a language. For in-
• Activation spreading from VSR towards SPR via SMC:                    stance, relation nodes are strongly linked to two (or more) en-
                   spr
     spr   avsr +a                   spr                                tity nodes, while links between entity and property nodes are
   at+1 = t 2 t + α · (atsmc − at ) + r − d
                                                                        weaker.The message generator matches the activated proposi-
   The first formula models the decay and random noise of               tions in SPR against nodes of possible message networks and
each entry’s activation, the second realizes local spreading of         determines their initial activations. Activation, again, spreads
activation within VSR, the latter two at a global level between         via the weighted links and finally results in an overall acti-
VSR and SPR. Especially the global multimodal activation                vation pattern of a pre-verbal message. This has been im-
spreading is important as it ensures that linked visuo-spatial          plemented for a limited part of our domain of investigation
and propositional codes align and mutually stabilize. Fig. 3            (corresponding to NPs about buildings and their properties).
                                                                    826

   The image generator retrieves visuo-spatial information            information about shape and position of the entity. Accord-
from activated VSR and SMC entries in memory. It is in                ingly, the speech formulator is now enabled to plan a sentence
charge of unifying this information into an imagistic rep-            like “The church has a round window at the top” which–like
resentation, from which the gesture formulator can derive             the gesture(s) described previously–encodes both, shape in-
a gesture form specification (based on Bayesian decision              formation and the entity’s relative position.
networks learned from empirical data (Bergmann & Kopp,                   To quantify these observations, we ran a simulation experi-
2009)). For instance, information about shape is combined             ment in which we manipulated the available time (in terms of
with information about the object’s size or position. Depend-         memory update cycles) before the model had to come up with
ing on the knowledge encoded here, the gesture formulator is          a sentence and a gesture. We analyzed the resulting sentences
able to plan a shape-depicting gesture or rather a localizing         and gestures for semantic redundancy/non-redundancy. We
deictic or placing gesture.                                           defined two conditions: A time-constrained condition with
                                                                      a certain number N of cycles and a condition with twice as
                     Simulation results                               many cycles. We ran the model 100 times in each condi-
                                                                      tion. Fig. 4 shows that non-redundant (supplementary) ges-
The implemented model offers–and simulates–detailed ex-
                                                                      tures dominate in those runs with stricter temporal limita-
planations of how semantic coordination between speech and
                                                                      tions, while redundant ones become more likely when time
gesture arises (see next Section). In particular, it allows us
                                                                      available is increased. Notably, the information conveyed by
to manipulate the interaction between modality-specific pro-
                                                                      gesture was similar in both conditions. So, the higher redun-
duction processes. As a first exploration, we report results
                                                                      dancy in the less time-constrained condition is mostly due to
on how processing time as a cognitive resource affects the
                                                                      the fact that the verbal utterances were richer in content.
observable meaning coordination.
   The production process is initiated by setting the commu-                                   Redundant semantic features
nicative intention “introduce churchwindow-1”. Upon receiv-                                    Non-redundant semantic features
ing this goal, the image generator activates visuo-spatial im-                   120
agery of the church window in VSR, and the message gener-
                                                                                  90
ator activates symbolic representations of non-spatial seman-
tic concepts in SPR. These activations spread through mem-                        60
ory and lead to invokation of SMCs for, e.g., ‘round’ (bound
to churchwindow-1) and ‘at-top-of’ (the church-tower), as                         30
well as instantiation of the corresponding SPR entries. SMCs
along with their linked entries in VSR and SPR attain highest                      0
                                                                                            N cycles             2N cycles
and most slowly decaying activation values.
   After a preset number of processing cycles, both genera-
                                                                      Figure 4: Number of semantic gesture features encoded re-
tors retrieve modality-specific information from memory with
                                                                      dundantly vs. non-redundantly with speech in 100 simulation
a probability depending on current activation values, leading
                                                                      runs in more (left) or less (right) time-constrained conditions
to ‘round’ and ‘at-top-of’ concepts being encoded in speech
                                                                      (note that a gesture may carry more than one feature).
and gesture in a less coordinated way: the message genera-
tor may retrieve only information about the salient shape of
the window, but not about its position relative to other enti-
ties. Accordingly, a sentence like “The church has a round                           Discussion and conclusions
window” gets formulated. The image generator, on the other            We have presented the first model to explain semantic coordi-
hand, may receive information about the entity’s position as          nation between speech and gesture in terms of (1) how visuo-
well. This can result in shape depicting gestures, like drawing       spatial and symbolic-propositional memory entries are dy-
the shape of the window in the air, or a static posturing gesture     namically linked, (2) how activation spreads in these concept
where the hands becoming a model of the circular shape. As            structures, and (3) how this interacts with modality-specific
the position of the entity is also available, the gesture would       processes of conceptualization and formulation. We believe
be performed in that part of gesture space. So, the gesture           that this model offer mechanisms and thus possible explana-
would be non-redundant to speech, supplementing it with the           tions for many empirical findings and hypotheses put forth
position of the entity.                                               in literature: The hypothesis that gestures are more likely if
   If more time is available, however, the contents expressed         activation in visuo-spatial memory is higher, is directly ex-
either verbally or gesturally tend to converge. The message           plained by the activation-based retrieval probabilities when
generator will start to (re-)activate those entries being re-         the image generator accesses memory; the hypothesis that
trieved and selected by the speech formulator. This results           non-redundant gestures are more likely when spatial codes
in multimodal representations being better coordinated when           are not transformed into verbal codes is accounted for by
the modality-specific formulators start with their generation         entries in VSR and SPR not being linked via SMC, leading
work, as it is more likely that both generators receive the same      to less coordinated conceptual structures and activations. Fi-
                                                                  827

nally, the shaping of gesture by speech is accounted for, first,     Hostetter, A., & Alibali, M. (2007). Raise your hand if you’re
through SPR and SMC schematizing VSR in linguistically                      spatial—relations between verbal and spatial skills and
shaped ways and, second, through choices in linguistic for-                 gesture production. Gesture, 7, 73–95.
mulation reinforcing activations in SPR and thus VSR.                Hostetter, A., & Alibali, M. (2008). Visible embodiment:
   Our simulation study showed that the model also offers a                 Gestures as simulated action. Psychonomic Bulletin
natural account for the finding that non-redundant gesture are              and Review, 15/3, 495–514.
more likely when conceptualization load is high, based on            Hostetter, A., & Alibali, M. (2011). Cognitive skills and
the assumption that memory-based cross-modal coordination                   gesture-speech redundancy. Gesture, 11(1), 40–60.
consumes resources (memory, time) and is reduced or com-             Kita, S., & Davies, T. S. (2009). Competing conceptual repre-
promised when, e.g., time is limited. This examplifies how                  sentations trigger co-speech representational gestures.
a model like ours can help to make hypothesis testable by                   Language and Cognitive Processes, 24(5), 761-775.
giving rise to predictions that can be explored in computa-          Kita, S., & Özyürek, A. (2003). What does cross-linguistic
tional simulations as well as in appropriately set up empiri-               variation in semantic coordination of speech and ges-
cal experiments. While the model presented here mainly ac-                  ture reveal?: Evidence for an interface representation
counts for information distribution, work is underway to ex-                of spatial thinking and speaking. Journal of Memory
tend the model to account also for different ways to pack-                  and Language, 48, 16–32.
age information over multiple clauses, e.g., depending on            Kita, S., Özyürek, A., Allen, S., Brown, A., Furman, R.,
available linguistic or gestural resources. This will enable to             & Ishizuka, T. (2007). Relations between syntactic
simulate cross-linguistic differences in co-speech gesturing.               encoding and co-speech gestures: Implications for a
Another issue for future work will be to go beyond object-                  model of speech and gesture production. Language and
related gestures accompanying NP constructions, and to ad-                  Cognitive Processes, 22, 1212–1236.
dress descriptions of action events with a more complex in-          Kopp, S., Bergmann, K., & Wachsmuth, I. (2008). Multi-
ternal structure and thus a more demanding semantic coordi-                 modal communication from multimodal thinking - to-
nation to be achieved by the cognitive processes involved in                wards an integrated model of speech and gesture pro-
multimodal conceptualization.                                               duction. Semantic Computing, 2(1), 115-136.
                                                                     Krauss, R., Chen, Y., & Gottesman, R. (2000). Lexi-
                    Acknowledgements                                        cal gestures and lexical access: A process model. In
                                                                            D. McNeill (Ed.), Language and gesture (pp. 261–
This research is supported by the Deutsche Forschungsge-
                                                                            283). Cambridge, UK: Cambridge University Press.
meinschaft (DFG) in the Collaborative Research Center 673
                                                                     Levelt, W. J. M. (1989). Speaking: From intention to articu-
“Alignment in Communication” and the Center of Excellence
                                                                            lation. MIT Press.
277 “Cognitive Interaction Technology” (CITEC).
                                                                     McNeill, D., & Duncan, S. (2000). Growth points in
                          References                                        thinking-for-speaking. In D. McNeill (Ed.), Language
                                                                            and gesture (pp. 141–161). Cambridge, UK: Cam-
Anderson, J., Bothell, D., Byrne, M., Lebiere, C., & Qin, Y.                bridge University Press.
       (2004). An integrated theory of the mind. Psychologi-         Melinger, A., & Kita, S. (2007). Conceptualisation load trig-
       cal Review, 111(4), 1036–1060.                                       gers gesture production. Language and Cognitive Pro-
Bavelas, J., Kenwood, C., Johnson, T., & Philips, B. (2002).                cesses, 22(4), 473-500.
       An experimental study of when and how speakers use            Özyürek, A. (2002). Speech-gesture relationship across lan-
       gestures to communicate. Gesture, 2(1), 1–17.                        guages and in second language learners: Implications
Bergmann, K., & Kopp, S. (2006). Verbal or visual: How                      for spatial thinking and speaking. In Proceedings of the
       information is distributed across speech and gesture in              26th annual boston university conference on language
       spatial dialog. In D. Schlangen & R. Fernandez (Eds.),               development (pp. 500–509).
       Proceedings of the 10th workshop on the semantics and         Ruiter, J. de. (2000). The production of gesture and speech.
       pragmatics of dialogue (pp. 90–97).                                  In D. McNeill (Ed.), Language and gesture (pp. 284–
Bergmann, K., & Kopp, S. (2009). Increasing expressiveness                  311). Cambridge, UK: Cambridge University Press.
       for virtual agents - autonomous generation of speech          Slobin, D. I. (1996). From ”thought and language” to ”think-
       and gesture for spatial description tasks. In Proc. of               ing for speaking”. In J. J. Gumperz & S. C. Levison
       AAMAS 2009 (p. 361-368).                                             (Eds.), Rethinking linguistic relativity (p. 70-96). Cam-
Breslow, L., Harrison, A., & Trafton, J. (2010). Linguis-                   bridge Univ. Press.
       tic spatial gestures. In D. Salvucci & G. Gunzelmann          Sowa, T., & Kopp, S. (2003). A cognitive model for the rep-
       (Eds.), Proceedings of the 10th international confer-                resentation and processing of shape-related gestures. In
       ence on cognitive modeling (pp. 13–18).                              Proc. European Cognitive Science Conference.
Collins, A. M., & Loftus, E. F. (1975). A spreading-activation
       theory of semantic processing. Psychological Review,
       82(6), 407-428.
                                                                 828

