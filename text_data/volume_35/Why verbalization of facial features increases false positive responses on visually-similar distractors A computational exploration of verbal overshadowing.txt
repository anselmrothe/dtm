UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Why verbalization of facial features increases false positive responses on visually-similar
distractors: A computational exploration of verbal overshadowing
Permalink
https://escholarship.org/uc/item/6dp0r3ck
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Hatano, Aya
Ueno, Taiji
Kitagami, Shinji
et al.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                    Powered by the California Digital Library
                                                                      University of California

   Why verbalization of facial features increases false positive responses on visually-
          similar distractors: A computational exploration of verbal overshadowing
              Aya Hatano (hatano.aya@c.mbox.nagoya-u.ac.jp), Taiji Ueno (taijiueno7@gmail.com),
       Shinji Kitagami (kitagami@cc.nagoya-u.ac.jp), and Jun Kawaguchi (kawaguchijun@nagoya-u.jp)
                      Department of Psychology, Graduate School of Environmental Studies, Nagoya University,
                                       Furo-cho, Chikusa-ku, Nagoya City, Aichi 4648601, JAPAN
                               Abstract                                   subsequently identify them from a line-up. In such
                                                                          situations, it is crucial to prevent a false accusation and to
   Verbal overshadowing refers to a phenomenon whereby
   verbalization of a non-verbal stimulus (e.g., he had slant eyes)       examine the credibility of the eyewitness’s testimony.
   impairs subsequent non-verbal recognition accuracy. In order           Therefore, it is both theoretically and practically important
   to understand the mechanism by which this phenomenon                   to clarify the mechanism by which verbal overshadowing
   occurs, we constructed a computational model that was                  occurs. For this purpose, we constructed a parallel-
   trained to generate an individual-face-specific representation         distributed processing (PDP) model to simulate the effect of
   upon input of a noise-filtered retinotopic face (i.e., face            verbalization on subsequent visual recognition.
   recognition). When the model verbalized the facial features
   before receiving the retinotopic input, the model incorrectly
                                                                             A closer review of the literature allows us to gain further
   recognized a new face input as one of the different, yet               insight into this phenomenon and therefore to establish a
   visually-similar, trained items (that is, a false-alarm occurred).     more specific aim for our model. First, although not all of
   In contrast, this recognition error did not occur without prior        the past studies have split the recognition scores into
   verbalization. Close inspection of the model revealed that             positive and negative trials, false alarm is sometimes more
   verbalization changed the internal representation such that it         susceptible to verbalization before recognition than hit rates;
   lacked the fine-grained information necessary to discriminate          that is, participants often inaccurately identify distractors as
   visually-similar faces. This supports the view that
   verbalization causes unavailability/degradation of fine-               a target rather than miss a correct target (Meissner, Brigham,
   grained non-verbal representations, thus impairing                     & Kelley, 2001). Furthermore, recognition accuracy in this
   recognition accuracy.                                                  study was positively correlated with accuracy of the verbal
                                                                          description prior to recognition. Based on these observations,
   Keywords: verbal overshadowing;               face    recognition;
   computational modeling; verbalization                                  Meissner et al. proposed a recording interference account
                                                                          that assumed verbalization rendered the representations less
                          Introduction                                    accurate (compared to visual representations), thus
                                                                          impairing subsequent visual recognition.
Language is the principal medium for carrying out daily                      Second, Kitagami, Sato, & Yoshikawa (2002) revealed
communications. This is still true when communicating our                 that verbal overshadowing is also sensitive to the degree of
non-verbal experiences, such as recounting a crime scene                  similarity between targets and distractors (manipulated with
we have witnessed, or describing the physical appearance of               a morphing technique). Specifically, verbalization impaired
a criminal. Particularly, if we do not have a record of the               subsequent visual recognition only when distractors were
event such as a picture or video, then conveying an                       highly similar to the target (using a 9-alternative choice task
eyewitness memory relies on language. A crucial question                  with a “not present” response choice), but the impairment
in cognitive science, therefore, is the influence of                      disappeared when similarity was low. It is also worth noting
verbalization on non-verbal memory. Many studies have                     that this manipulation involved a change in the distractors,
revealed that language has extra-communicative functions,                 but not in the target picture itself. We revisited the original
in that it affects such cognitive functions as perception,                data and revealed that accuracy was impaired due to the
learning, and memory. For example, in a seminal study by                  more frequent choice of a distractor (a false alarm) rather
Schooler and Engstler-Schooler (1990), participants                       than an incorrect choice of “not present” (a miss). Schooler
watched a video of a bank robbery for 30 seconds and                      (2002) explained this result with the transfer inappropriate
following which half of the participants described the                    processing shift hypothesis. This hypothesis assumes that
appearance of the bank robber. Subsequently, all of the                   verbalization induces a processing shift from visual to
participants were shown a line-up that consisted of the bank              verbal, and that a shift to verbal processing makes fine-
robber’s photo and seven distractors. Results revealed that               grained non-verbal information about faces unavailable.
participants who had verbalized the bank robber’s                         This non-verbal information is crucial for discriminating the
appearance were worse at recognizing the target individual                target from others (see also, Maurer, LeGrand, & Mondroch,
than those who had not, a phenomenon known as verbal                      2002), especially in a high-similarity condition (Kitagami et
overshadowing. The procedure of these experiments can be                  al., 2002). Although Schooler’s hypothesis does not
experienced beyond an experimental setting. For example,                  necessarily assume a correlation between recognition
during criminal investigations, an eyewitness may provide a               accuracy and verbal description accuracy (see also Kitagami
statement describing the appearance of a criminal and
                                                                      2494

et al., 2002; Fallshore & Schooler, 1995), this hypothesis
and the recording interference hypothesis by Meissner et al.
(2001) share two ideas: First, both assume that fine-grained
non-verbal information is necessary for face recognition.
Second, both expect that a verbal representation which is
generated during verbalization lacks such fine-grained
information, thus impairing visual recognition.
   More recently, Clare and Lewandowsky (2004)
introduced an alternative hypothesis, arguing that
verbalization shifts the criterion threshold such that
participants say “The target is not present in the display”
more frequently when in fact the target is present. Although
this account can explain a range of existing data, two issues
deserve consideration. First, even when a “not present”               Figure 1: Architecture of the model (Hinton diagram).
response was disallowed (that is, responses were forced
choice), verbal overshadowing was observed in some                 demand in this large model, units between layers were
studies, especially when elaborative verbalization was             connected sparsely, such that a unit was not connected with
encouraged (Fallshore & Schooler, 1995). Second, the               others if the external input/target value of that unit was
shifting criterion hypothesis cannot explain the fact that         always zero (e.g., a unit in the top-left corner). The bottom
false alarm is more susceptible to verbalization than hit rate     layer was named the retinotopic layer, and its activation
(Kitagami et al., 2002; Meissner et al., 2001). Thus, as Clare     patterns represented a filtered (Gaussian noise) face
and Lewandowsky also speculated, there may be two                  stimulus. The left layer was named the verbal layer, and
mechanisms by which verbalization impairs subsequent               each unit in this layer represented a verbal label for facial
visual recognition: One is shifting-criterion (Clare &             features in a localist manner. The right layer was named the
Lewandowsky, 2004), and the other is degradation                   visual image layer, and its activation pattern represented a
(Meissner et al., 2001) or unavailability (Schooler, 2002) of      non-filtered (without Gaussian noise) face stimulus. With
fine-grained non-verbal representations crucial for face           this architecture and the representations in each layer, we
recognition, especially when a distractor is visually              trained the model for the three tasks described below.
confusing. This study focused on the latter possibility, and
investigated how the nature of representations changes upon        Tasks
verbalization, and how this affects subsequent visual              Visual encoding of a face from a retinotopic input (visual
recognition. Computational modeling is an effective                recognition). In this task, retinotopic face pattern was hard-
approach for this purpose. An explicitly implemented               clamped onto the retinotopic layer. Then, the network was
computational model allows a modeler to directly look at           trained to activate the non-filtered, unique visual face
the nature of computations/representations that are                information of the same person in the visual image layer
underpinning a simulated behavior. The PDP model here              (individuation or visual recognition – see later).
was trained for three facial processing tasks: One was to
represent the retinotopic input of a face in a non-verbal          Verbal encoding of facial features from a retinotopic
format (visual encoding/recognition); a second was to              input (verbalization). In this task, the input was the same
activate the correct units for verbal labels upon the same         as the previous visual recognition task, but the network was
retinotopic input of a face (verbal encoding); the third was       trained to activate the correct verbal units for each presented
to represent a face in a non-verbal format upon verbal inputs      face. For example, if the face had slanted eyes, then the
(the mental imagery of a face upon verbal cues). After being       model had to turn on the unit for “slant eyes”, and had to
trained for these tasks, the model was forced to activate          turn off the unit for “drooping eyes”.
some verbal units (i.e., verbalization), and we investigated
how this forced activation changed the nature of the               Mental imagery upon verbal cues. In this task, the verbal
computed representation in the model, and how it affected          labels of facial features were presented onto the verbal layer,
subsequent visual encoding of a retinotopic input.                 and the network was trained to activate the visual face
                                                                   information in the visual image layer. As we will explain
                                                                   later, the accuracy for this task never reached 100% because
                           Method                                  different faces sometimes shared the same verbal labels (i.e.,
                                                                   different targets from the same input pattern).
Model Architecture
   Figure 1 shows the architecture of the PDP model, built         Recognition. A standard experimental task on human
with LENS software (http://tedlab.mit.edu/~dr/Lens/). Three        recognition memory employs a N-alternative forced choice
peripheral layers were connected bi-directionally with a           task to probe recognition process, particularly when
single hidden layer. In order to reduce the computational          examining verbal overshadowing. The model, however, was
                                                                   not trained for making an explicit N-alternative forced
                                                               2495

“choice”. Therefore, we should adopt a proper measure to                The original bit patterns were transformed into the
probe the model’s recognition. It is one of the most                  retinotopic input pattern by smoothing with Gaussian
debatable issues in cognitive psychology regarding what               convolution (SD = 0.5) ( Plaut & Behrmann, 2011). The
process/mechanism is underpinning recognition. Following              original bit patterns were smoothed by Gaussian
previous studies (e.g., Plaut & Behrmann, 2011), we                   convolution (SD = 0.5). In summary, the model had to map
examined whether the model could represent item-specific              a noise-filtered retinotopic input (top row of Fig. 2) into a
information (i.e., unique face) as an approximation of                clearer visual representation (second row of Fig. 2), which is
recognition process. If the model computes item-specific              necessary for visual recognition.
information of an “old” face in the visual layer from a “new”           The pattern activations in the verbal layer represented the
retinotopic (noisy) input, then it can be considered the              verbal labels in a localist manner (third row of Fig. 2). For
model identifies this input as old face by mistake (especially        example, when presented with a retinotopic pattern of the
after a verbal label for the old face was activated). In this         drooping-eyes, long-nose, and thick-lip face, then the model
way, we can at least measure false alarm safely, which is the         had to activate the first, second, and third units of the verbal
target of the current study with this procedure.                      layer (the left two cases of Fig. 2 show these examples). In
                                                                      the mental imagery upon verbal cues task, the same units in
                                                                      the verbal layer were turned on, and the network was trained
                                                                      to activate the visual images in the internal image layer. The
                                                                      accuracy in this task can never be 100% because sometimes
                                                                      a different target should be generated from the same input
                                                                      pattern (i.e., the same verbal labels). For example, slant eyes
                                                                      (thin) and slant eyes (big) shared the same verbal label, slant
                                                                      eyes. Therefore, the same unit (slant eyes) was turned on for
                                                                      these two cases, but different output patterns (thin or big
                                                                      eyes) should be generated in the visual image layer. This is
                                                                      true to humans: We can imagine various kinds of faces but
                                                                      cannot specify a unique face by simply hearing “slant eyes,
                                                                      long nose, and thick lips”. A small amount of Gaussian
                                                                      noise (SD = 0.2) was added to the input for the hidden layer
Figure 2: Four examples of the training patterns. Note that           to encourage this layer to adopt more polarized outputs.
two examples within each half share the same verbal labels,
and thus the same pattern activations in the verbal layer.            Training
(However, they are different faces with different specific            Among the 64 face patterns, only 55 patterns were presented
features as shown in the parentheses).                                during training, and the remaining nine untrained patterns
                                                                      were used to evaluate the network’s generalization
Representations (face stimuli)                                        performance. Furthermore, this allowed us to investigate
Figure 2 shows examples of the face pictures that we                  how differently the model behaved with the trained faces
created            using           montage              software      (‘old’ items) and untrained faces (‘new’ items), as it was
(http://www1.mahoroba.ne.jp/~matumoto/nitaroS.html).                  crucial for us to investigate the effect of verbalization before
Sixty-four face pictures were created by combining four               the recognition phase (described later in more detail).
types of eyes, four types of nose, and four types of mouth               In each trial, units in the corresponding layer (retinotopic
(see the bottom row of Fig.2 for the possible features) in the        or verbal layer) were hard-clamped to their input values, and
following steps. First, we selected two verbal labels for each        the network was allowed to cycle 10 times. In each time
part of the face – slant eyes, drooping eyes, long nose,              step, the activation spread to the next layer, gradually being
button-shaped nose, downturned mouth, and thick lips. Next,           scaled by the values of the interconnecting weights, and
we selected two specific types for each verbal label (e.g.,           then the network would settle into the steady state (an
slant -eyes [thin] and slant eyes [big] for the label slant eyes,     attractor). After 10 cycles of updates, the discrepancy
as shown in the right two faces in Fig. 2). In this way, we           between the output activation pattern generated by the
created four types of eyes, nose, and mouth, resulting in 64          network and the correct target pattern was calculated, and
different faces by combining 4 by 4 by 4. In order to make            the connection strength was           adjusted to reduce the
the model trainable, we did not include other features such           discrepancy. The model was trained with a learning rate of
as hair. Finally, the size of each picture was 70*60 pixels,          0.05, and with a decay parameter set to 0.0000001. When
and the color information in each pixel was binarized (i.e.,          we evaluated the final performance, we used a strict
black pixel → 1; white pixel → 0). The resultant 4200-bit             criterion such that the output was scored correct if the
vector pattern was used as the target pattern of the visual           discrepancy was within 0.5 in every unit of the target layer
image layer in the visual recognition tasks (see second row           (i.e., the activation was less/more than 0.5 if the target was
of Fig. 2).                                                           zero/one for each unit respectively).
                                                                  2496

   Given that young infants recognize their parents easily, it
would be natural to assume that visual recognition skills are
acquired earlier than an ability to verbalize facial features,
or to imagine a face upon verbal cues.Thus, all 55 of the
face stimuli were first trained for the visual recognition task.
After learning to generate a steady state for more than 50%
of the training items in this task, the other two language-
related tasks were included in the training schedule.
                            Results
Training tasks
Five independent simulations were run with different
random seeds, and we confirmed consistent results across
five cases. The training finished after 2837 epochs of
training (in each epoch an item appeared once for each task
in a random order), at which point the network’s
performance reached 100% in both the visual recognition
and the verbalization tasks from a retinotopic input for both
trained and untrained items (i.e., generalization). The
accuracy in the visualization task from verbal labels was 0%
(see above for the reason).
Visual recognition with/without verbalization
In order to investigate the visual recognition process of a
retinotopic input after/without verbalization, we recorded
the activation patterns in the visual image layer (right
column of Fig. 3) when the network settled on 10 cycles
after the retinotopic input presentation (left column of Fig.
3). The upper two rows of Fig. 3 show the pattern
activations for a trained (‘old’) face (drooping eyes [thin],
long nose [high], and thick lip [bottom big]) and for a
visually-similar, yet untrained (‘new’) face (drooping eyes
[big], long nose [high], and thick lip [bottom big]),
respectively. Both retinotopic inputs were correctly mapped
onto every unit of the visual image layer. This means that
two visually-similar faces were successfully discriminated
(see the bigger eyes represented in the second row), unless
they were preceded by the verbalization process (100%
accuracy in computation of the individual-specific face                 Figure 3: Activation patterns in the visual image layer
information for all the nine untrained items). Next, the             (right) upon retinotopic inputs (left) for trained ‘old’ face
middle two rows of Fig. 3 show the activations for the same          and for untrained ‘new’ face in the visual recognition task.
two items as the upper rows but after verbalization.                 Upper two rows: without verbalization. Middle two rows:
Specifically, we simulated the following situation: Imagine          after verbalization of a similar ‘old’ face. Bottom row: after
that the network had encountered the ‘old’ face shown in             verbalization of a dissimilar ‘old’ face.
top row of Fig. 3 (drooping eyes [thin], long nose [high],
and thick lip [bottom big]), and the network had verbalized          steady pattern in the visual image layer (right column). A
the correct labels (drooping eyes, long nose, and thick lip).        visual inspection reveals the retinotopic input for the
To simulate this situation, the three verbal units for these         visually-similar ‘new’ face (drooping eyes [big]) was
labels were manually turned ‘on’ (generating the outputs of          mapped onto the pattern for the ‘old’ face (slant eyes [thin])
1.0) and the network was allowed to cycle 10 times, during           in the visual image layer (false alarm). Euclid distance of
which the activations spread into the other layers (it updated       the output pattern from the target “new” face was larger
its internal status 10 times). After 10 cycles, a retinotopic        than that from the lure “old” face (i.e., similar to the “old”
input for the trained face (‘target’) and that for the visually-     face pattern). This means that the model actually computed
similar, yet untrained face (‘new’) were presented                   the item-specific information of the “old” face (false alarm).
respectively, and the network was allowed to update its              The same analysis was conducted for all the nine “new”
status 10 times until each input pattern was mapped onto a           items (against its visually-similar “old” face, respectively),
                                                                 2497

                                                                     the internal activation patterns when the network
                                                                     successfully discriminated two visually similar ‘old’ and
                                                                     ‘new’ faces, respectively (shown in the upper panels of Fig.
                                                                     3). A visual inspection reveals these two representations are
                                                                     very similar. This concurs with the idea that fine-grained
                                                                     representations are crucial in face recognition (Maurer et al.,
                                                                     2002), without which one would be easily mapped to the
                                                                     other, incorrect, face pattern in the visual image layer (i.e.,
                                                                     incorrect recognition).
                                                                          Next, the left diagram of the middle row of Figure 4
                                                                     shows the activation pattern immediately after verbalization
                                                                     of ‘drooping eyes, high nose, and thick lips’. As a result,
                                                                     this internal representation immediately after verbalization
                                                                     was neither identical with that for visual recognition of the
                                                                     ‘old’ face (top left) nor that for visual recognition of the
                                                                     ‘new’ face (top right), concurring with the idea that
                                                                     verbalization generates the representation that lacks fine-
                                                                     grained information crucial for face recognition (Maurer et
                                                                     al., 2002). Though lacking such detailed information, it was
                                                                     nonetheless closer to the representation for the ‘old’ face
                                                                     (top left) than that for the ‘new’ face (top right). In other
Figure 4: Internal activation patterns in the hidden layer           words, the model’s internal status had already moved
(Hinton diagram: more white units denote higher activation           towards the pattern for the ‘old’ face. We will explain later
values than black units) at the various kinds of time point          why this representation increased the false alarm of the
(see main text).                                                     model when the distractor was similar to the target.
and averaged across the five individual simulations. The                                    Discussion
resultant recognition accuracy was 60% (40% false alarm),            The present computer simulation examined how internal
SD = 14.9%, which was significantly lower than 100% (t (4)           representations changed upon verbalization and how this
= 5.99, p = .003). This confirms that the example result in          affected     subsequent     visual     recognition.    Without
Figure 3 is generalized across other patterns. In contrast, the      verbalization, the model represented the correct and unique
retinotopic input for the ‘old’ item was mapped onto the             pattern activation for each old face and for a visually-similar
correct pattern (drooping eyes [thin]) in the visual image           new face, respectively, in the visual image layer. This
layer, though less weakly than when presented without                confirms that the model did not confuse two visually-similar
verbalization (top row). Taken together, these results               retinotopic inputs. On the other hand, the model failed to
confirm that false alarm was more susceptible to                     represent the correct pattern for a new face following the
verbalization than hit rate. Finally, the bottom row of Fig. 3       forced activation of verbal units for an ‘old’, visually-
shows the simulated result in the condition where the                similar face (i.e., verbalization). Instead, the represented
distractor was dissimilar to the target. Specifically, the           pattern in the visual layer assimilated to that for the
activation patterns were taken from the same untrained ‘new’         visually-similar ‘old’ face, suggesting that the model could
face (drooping eyes [big], long nose [high], and thick lip           not differentially recognize the ‘new’ face from the ‘old’
[bottom big]), but after activations of the irrelevant verbal        face (a false alarm). Importantly, this assimilation was
units (slant eyes, button-shaped nose, and downturned                weakened when the preceding verbalization included the
mouth). Thus, the model had encountered a dissimilar                 features of an ‘old’, yet dissimilar face. Therefore, these
person, and verbalized the dissimilar labels before visual           results mirrored Kitagami et al. (2002), who found that
recognition of a ‘new’ item. As a result, the network did not        participants’ false alarm increased upon verbalization when
settle into the pattern of the dissimilar target face (slant eyes    the distractors were similar to the target.
and downturned mouth), but the represented pattern was                  Explicit implementation of a computer model allowed us
more similar to the correct pattern (drooping eyes and thick         to directly look at the internal representations to understand
lip). In other words, the model did not confuse the presented        why the model behaved in this way. In a normal situation
retinotopic input (the ‘new’ face) with the previously               (without verbalization), the model computed very similar,
encountered ‘old’, yet dissimilar face, thus avoiding false          yet unique, internal representations for retinotopic inputs of
alarm in this low-similarity condition (Kitagami et al., 2002).      visually-similar faces. This fact is consistent with the idea
    Finally, in order to understand the mechanism of verbal          that fine-grained representation is necessary for visual
overshadowing, the pattern activation in the hidden layer            recognition of faces (Maurer et al., 2002), especially when
was measured at the various kinds of time point (Figure 4).          discriminating a target from similar distractors. When the
First, the Hinton diagrams at the top row of Figure 4 show           model verbalized the facial features, this internal
                                                                 2498

representation changed such that it was neither identical to        demonstrated that investigating the internal representation
that of an ‘old’ face, nor that of a ‘‘new’ face, supporting        of a model is a useful approach to advance the cognitive
the argument that verbalization either degrades the fine-           theory (e.g., Plaut & Behrmann 2011). Furthermore, the
grained representation (Meissner et al., 2001), or renders it       current model can be extended to other types of perceptual
unavailable (Schooler, 2002). Nonetheless, it was closer to         stimuli (not just face). Thus, we expect that the present
the representation for the verbalized ‘old’ face than to that       study would be an important step to clarify the relationship
of a ‘new’ face. In order to understand why this                    between language and perception in general. Finally, one
representation induced a false alarm for a visually-similar         issue deserves consideration: The current model simulated
face, it is useful to describe the general activity of PDP          the increase in false alarm upon verbalization (Kitagami et
models here. During training, a PDP network finds a unique          al., 2002; Meissner et al., 2001), rather than a missed
attractor state (a unique abstract pattern in the hidden layer)     response. Although some studies failed to detect a
associated with each input pattern. Therefore, generating a         significant difference between these two measures (Schooler
correct output is sometimes described as if the internal            & Engstler-Schooler, 1990, a recent experimental and
activity of the hidden layer falls into its unique attractor        computational study (Clare & Lewandowsky, 2004)
basin. Though they are unique, similar inputs are associated        suggested that it was actually the increase in “not present”
with similar/neighboring attractor basins (as shown in the          responses, a response type that was not implemented in the
top two panels of Fig. 4). Consequently, if the internal            current model. A future modeling target would be to
representation of the model is degraded for some reason, a          understand the mechanism by which verbalization increases
similar input can incorrectly drift and fall into the wrong         both “not present” responses and false alarms, of which the
attractor basin, generating an incorrect output. In the current     latter particularly occurs when the distractors and targets are
model, verbalization generated the internal representation          similar.
that lacked fine-grained information crucial for visual
recognition (middle-left row of Fig. 4). Though it lacked                                Acknowledgments
such information, it was nonetheless closer to the                  This study was supported by Grant-in-Aid for Scientific
representation of the ‘old’ face (the top and middle left rows      Research on Innovative Areas #23101006 (to S. Kitagami)
are similar in Fig. 4). In other words, the model’s internal        and Grants-in-Aid for Scientific Research (B) 21330168 (to
status had moved towards the attractor basin for the ‘old’          J. Kawaguchi).
face by verbalization. In such a situation, a similar
retinotopic input, which would have settled into a unique,
yet similar/neighboring attractor basin without verbalization
                                                                                             References
(top-right of Fig. 4), was easily captured by the attractor for
the ‘old’ face. The resultant (captured) internal activation        Clare, J., & Lewandowsky, S. (2004). Verbalizing facial
pattern is shown in the left bottom row of Fig. 4, which was           memory: Criterion effects in verbal overshadowing.
more similar to the top-left pattern than the top-right pattern        Journal of Experimental Psychology-Learning Memory
(i.e., incorrect recognition). This is the mechanism by which          and Cognition, 30(4), 739-755
verbalization impairs subsequent recognition, especially            Fallshore, M., & Schooler, J. W. (1995). Verbal
when the distractor is similar to the ‘old’ face. In contrast,         vulnerability of perceptual expertise. Journal of
when the dissimilar (inconsistent) labels were verbalized              Experimental Psychology: Learning Memory and
before visual recognition, the hidden layer activation pattern         Cognition, 21, 1608-1623.
was very different to that for a subsequent ‘new’ face (i.e.,       Kitagami, S., Sato, W., & Yoshikawa, S. (2002). The
the top and middle right rows are not similar). In this case,          influence of test-set similarity in verbal overshadowing.
the network is not captured by this dissimilar attractor basin,        Applied Cognitive Psychology, 16, 963-972.
as is shown in the bottom right diagram of Fig. 4.                  Maurer, D., LeGrand, R., & Mondloch, C. J. (2002). The
  In summary, as the present study has demonstrated, a                 many faces of configural processing. Trends in Cognitive
computer simulation is a useful tool for investigation of              Science. 6, 255-260
verbal overshadowing. It is difficult to examine verbal             Meissner, C. A., Brigham, J. C., & Kelley, C. M. (2001).
overshadowing empirically, given that the standard                     The influence of retrieval processes in verbal
paradigm involves a single-trial measurement. Therefore,               overshadowing. Memory & Cognition, 29, 176-186.
many participants are necessary for detecting a reliable            Plaut, D. C. & Behrmann, M. (2011). Complementary
effect, and it is difficult to systematically manipulate a             neural representations for faces and words: A
variable as a within-subject factor. In such a situation, it is        computational exploration. Cognitive Neuropsychology,
worthy to implement a computational model in order to                  28, 251-275.
understand the mechanism and to provide a theory-driven             Schooler, J.W. (2002), Verbalization produces a transfer
question that can be empirically testable in human                     inappropriate processing shift. Applied Cognitive
experiments. Of course, any computational modeling should              Psychology, 22, 989-997.
be concerned whether the model’s representation/process is          Schooler, J. W., & Engstler-Schooler, T. (1990). Verbal
the same as the human’s, but previous studies have                     overshadowing of visual memories: some things are better
                                                                       left unsaid. Cognitive Psychology, 22, 36-71.
                                                                2499

