UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Object-based Saliency as a Predictor of Attention in Visual Tasks
Permalink
https://escholarship.org/uc/item/1qf8p3tt
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Dziemianko, Michal
Clarke, Alasdair
Keller, Frank
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

               Object-based Saliency as a Predictor of Attention in Visual Tasks
                                       Michal Dziemianko (m.dziemianko@sms.ed.ac.uk)
                                                Alasdair Clarke (a.clarke@ed.ac.uk)
                                                  Frank Keller (keller@inf.ed.ac.uk)
                                           Institute for Language, Cognition and Computation
                                              School of Informatics, University of Edinburgh
                                               10 Crichton Street, Edinburgh EH8 9AB, UK
                              Abstract                                  (1998) show that participants benefit from learning spatial
                                                                        arrangement of the objects in consecutive searches. Theo-
   The top-down guidance of visual attention is an important fac-
   tor allowing humans to effectively process incoming visual in-       retically, such results can be accommodated by the Cogni-
   formation. Our understanding of the processes governing at-          tive Relevance Framework (Henderson et al., 2009), which
   tention is not complete, with growing evidence for attention         assumes that attention is allocated to locations that are cogni-
   selection based on cognitive relevance. In this paper, we inves-
   tigate whether models for salient object detection from com-         tively relevant for the task performed.
   puter vision can be used to predict attentional shifts in visual        Cognitive relevance predicts that objects should have a
   tasks. Our results show that the object-based interpretation         privileged status in visual processing, which is in line with
   of saliency provided by these models is a substantially bet-
   ter predictor of fixation locations than traditional pixel-based     experimental evidence suggesting that the allocation of at-
   saliency.                                                            tention is object-based rather than pixel-based. For example,
   Keywords: eye-tacking; saliency; visual attention.                   Henderson et al. (2007) argue that saliency does not account
                                                                        for fixated areas in visual search, while Nuthmann & Hender-
                           Introduction                                 son (2010) show that the preferred fixation point or landing
Virtually every human activity occurs within a visual con-              position is the center of an object: fixations are distributed
text and many tasks require visual attention in order of be             normally around an object’s center of mass, where the spread
successfully accomplished (Land & Hayhoe, 2001). When                   might be explained by oculomotor errors. Consistent with
processing a visual scene, humans have to localize objects,             this, Einhauser et al. (2008) show that the position of objects
identify them, and establish their spatial relations. The eye-          is a better predictor of fixations than early saliency in tasks
movements involved in this process provide important infor-             such as artistic evaluation, analysis of content, and search.
mation about the cognitive processes that unfold during scene              An alternative view on saliency comes from the computer
comprehension.                                                          vision literature, which deals with task of salient object de-
   A number of models have been proposed to predict eye-                tection: the objects that are perceived by humans as visually
movements during scene processing and they can be broadly               most interesting have to be separated from the background.
divided into two categories. The first category consists of             Typically this involves image segmentation and the calcula-
bottom-up models that exploit low-level visual features to              tion of visual features in order to select pixels belonging to
predict areas likely to be fixated. A number of studies have            salient objects. In this context, saliency is a feature of an ob-
shown that certain features and their statistical unexpected-           ject, rather than an early pixel-based attractor of attention.
ness attract human attention (e.g., Bruce & Tsotsos, 2006).                In this paper, we investigate the extent to which methods
Moreover, low-level features are believed to contribute to the          proposed for salient object detection can be applied to the pre-
selection of fixated areas, especially when the visual input            diction of fixations. We are not concerned with the prediction
does not provide useful high-level information (Peters et al.,          of salient image patches, but rather with the selection of ob-
2005). These experimental results are captured by models                jects that are likely to be fixated. This approach allows us to
that detect salient areas in the visual input and use them to           develop computational models of attentional selection based
predict attention. The best-known example is the model of Itti          on cognitive relevance defined over objects (Henderson et al.,
et al. (1998), which builds a pixel-based saliency map using            2007, 2009). We compare the performance of this approach
color, orientation, and scale filters inspired by neurobiologi-         to traditional models which predict fixation locations using
cal results.                                                            pixel-based saliency maps.
   The second group of models assumes that top-down su-
pervision of attention contributes to the selection of fixation                                  Background
targets (e.g., Torralba et al., 2006). Various types of such su-        As discussed above, there is experimental evidence for the
pervision have been observed experimentally. Humans show                object-based allocation of attention. Additionally, some ob-
the ability to learn general statistics of the appearance, posi-        jects seem to inherently attract more attention than oth-
tion, size, spatial arrangement of objects, and their relation-         ers, a fact that has been conceptualized using proto-objects:
ships (e.g., Zelinsky, 2008). They also exploit visual mem-             pre-recognition entities that draw attention (Rensink, 2000).
ory during scene comprehension tasks (e.g., Shore & Klein,              Proto-objects have been incorporated into saliency-based
2000). Moreover, studies such as those of Chun & Jiang                  models (Walther & Koch, 2006) and have also been applied
                                                                    2237

                                                                    currently processed image. The local features are computed
                                                                    as a set of Steerable pyramid responses computed over three
                                                                    color channels for six orientations and four scales, totaling 72
                                                                    values at each position.
                                                                       Based Torralba et al.’s model, we can define a group of
                                                                    models which convert pixel-based saliency values to object-
Figure 1: Example of proto-objects extracted from an image          based salience scores. Such a conversion can be performed by
using the model of Walther & Koch (2006). From left to right:       computing functions such as the maximum, mean, median, or
original image, saliency map computed according to Itti et al.      mode of the pixels that make up an object. Examples for the
(1998), proto-object mask. The salient patches, and hence           use of this method exist in the literature (e.g., Spain & Per-
the proto-objects, do not necessarily correspond to the real        ona, 2011), with maximum and mean being common. These
objects in the scene.                                               models will be referred to as converted in this paper.
in robotics to create attentional systems for virtual and phys-     B. Liu et al. Features
ical agents (see e.g., Yu et al., 2010). These models perform
                                                                    Liu et al. (2011) describes a system for salient object detec-
image segmentation to identify proto-objects: the image is
                                                                    tion based on conditional random fields, which simultane-
divided into a collection of regions that correspond to areas
                                                                    ously segments pixels into areas corresponding to objects and
enclosed by constant, high saliency values. Figure 1 shows
                                                                    computes the pixel’s salience. The model is based on three
an example of such proto-objects extracted from an image
                                                                    feature channels – contrast, center-surround histograms and
using the model of Walther & Koch (2006).
                                                                    spatial color – which are described below. The salience of a
   While Walther and Koch’s model is conceptually interest-
                                                                    pixel is defined to be the a weighted sum of these three feature
ing, its cognitive status is questionable, as there is evidence
                                                                    maps, while the salience of an object is defined as the sum
that it does not predict fixation locations well (Nuthmann &
                                                                    over all pixels within the object’s boundary. The full speci-
Henderson, 2010). Alternative models of attention selection
                                                                    fication of our implementation of Liu’s model can be found
based on objects rather than proto-objects have been pro-
                                                                    in Dziemianko (2013). Examples of the feature channels are
posed in computer vision. For example, the work of Liu
                                                                    given in Figure 3.
et al. (2011) focuses on detecting objects annotated by people
as salient. These models use machine learning techniques            Multiscale Contrast Contrast is one of the most commonly
to compute which arrangements of visual features such as            used features in saliency models and is implemented over a
center-surround histograms, orientation, scale are perceived        multiscale Gaussian pyramid. In each layer of the pyramid,
as salient. However, in a computer vision context, attentional      the contrast at pixel (x, y) is defined to be the mean squared
selection is regarded merely as an engineering task: the aim is     difference of the intensity of pixel at (x, y) and its adjacent
to identify areas matching pre-annotated training data, rather      neighbors. The multiscale contrast for I(x, y) is then taken
than to gain a greater understanding of human behavior.             to be the sum over the layers of the corresponding pyramid.
                                                                    This has the effect of approximating human receptive field by
                             Models                                 highlighting high-contrast boundaries while omitting homo-
We implemented and evaluated three models for salient ob-           geneous regions within objects.
ject detection. Throughout our work we assume that the im-          Center–Surround Histograms One of the weaknesses of
ages are fully annotated with object boundaries, therefore the      previous measures of visual salience is that, due to their re-
problem of segmentation and separation of objects from the          liance on high-contrast center-surround features, they tend
background does not need to be solved within the models.            to emphasis the boundaries of objects while giving very low
This assumption makes it possible to evaluate object-based          scores to pixels within an object’s boundary (see Figure 2). To
saliency models separately from image segmentation algo-            tackle this issue, Liu et al. (2011) propose to use region-based
rithms, which can vary widely in their performance.                 features in addition to the center-surrounds described above.
                                                                    These are computed by considering the histogram of colors
A. Conversion of Standard Saliency
                                                                    within an object’s bounding box, and comparing it with a sur-
Standard, pixel-based saliency is the baseline against which        rounding region of equal area (see Figure 2). The χ2 metric is
we evaluate object-based models. The baseline model we use          used to measure the distance between histograms and the full
is Torralba et al.’s (2006), which approximates saliency as the     details on how these regions are constructed can be found in
probability of the local images feature L in a given location       Liu et al. (2011).
based on the global distribution of these features:
                                                                    Color Spatial Distribution The last feature used by Liu
                              − 12 [(L−µ)T Σ−1 (L−µ)]               et al. (2011) is the spatial color distribution, motivated by the
                 p(L) ∝      e                              (1)
                                                                    observation that salient objects are less likely to contain col-
Here, µ is the mean vector and Σ the covariance matrix of           ors that are distributed widely throughout the image. A sim-
the Gaussian distribution of local features estimated over the      ple method for quantifying this is to compute the spatial vari-
                                                                2238

Figure 2: An example of high saliency values being assigned          Figure 4: Examples of scenes used in the visual counting ex-
to object boundaries due to its reliance on high-contrast fea-       periment. Targets on the images on the left and in center are
tures. From left to right: original image, traditional saliency,     man, while for the image on the right it is goggle.
an object (red) and its surrounding area (green).
                                                                     the most effective). However, it performs a comparison of
                                                                     histograms of color cluster assignments within the object and
                                                                     its surrounding area.
                                                                        In the first phase, the means µ and covariances Σ of these
                                                                     Gaussians are extracted by fitting a Gaussian mixture model
                                                                     (GMM) with W components over all pixels in the image.
                                                                     Similar to Eslami & Williams (2011), we use W = 15 Gaus-
                                                                     sians. At this stage object boundaries and locations are ig-
Figure 3: Examples of features from Liu et al. (2011). From          nored. In the subsequent step, pixels are clustered into W
left to right: original image, multiscale contrast, center-          clusters according to the associated GMM components by se-
surround histogram, color spatial distribution (image from           lecting a component, ŵ, that maximizes the probability of a
Liu et al. (2011) with modifications).                               pixel being drawn from the Gaussian distribution. The final
                                                                     step of the first phase consists of computing global histograms
                                                                     H of the pixel assignments ŵ representing the proportion of
ance of color. This involves representing the distribution of        pixels belonging to each cluster.
colors contained in the image by a Gaussian mixture model.              The saliency scores are computed in the second phase. At
We then carry out soft assignment: for each of these Gaus-           this stage, the model assumes that the image is fully anno-
sians, c, we then calculate p(c|I(x, y)), the probability of as-     tated (i.e., boundaries for each object within the scene are
signing pixel I(x, y) to the Gaussian N (µc , Σc ). Using these      provided). For each object in the scene, we calculate the his-
we can calculate the weighted mean and variance for each             togram of pixel assignments over the pixels within the ob-
color component along the horizontal axis:                           ject’s boundary. We then define an interestingness value for
                                                                     each object as the Kullback-Leibler (KL) divergence between
                          1                                          the local (object) pixel distribution and the global distribu-
        Mh (c) =                 p(c|I(x, y)) · x             (2)
                        |X|c ∑ x                                     tion H. Intuitively, I represents how different the object is
                          1                                          from its surroundings and thus interesting.
         Vh (c) =                p(c|I(x, y)) · |x − Mh (c)|2 (3)
                        |X|c ∑ x
                                                                                               Evaluation
where |X|c = ∑x p(c|Ix ). The vertical spatial variance, Vv , is     Method
computed in the same way, and V (c), the spatial variance of
                                                                     We evaluate the performance of the models discussed on eye-
each color component, is then simply defined as:
                                                                     tracking data collected in a visual counting and an object
                     V (c) = Vh (c) +Vv (c)                   (4)    naming task. In the visual counting task, 25 participants
                                                                     were asked to count the number of occurrences of a cued
Finally, the feature function fs (x, y) is defined as:               target object, which was either animate (e.g., man) or inan-
                                                                     imate (e.g., goggle). The data set consisted of 72 fully object-
            fs (x, y) ∝     ∑ p(c|I(x, y)) · (1 −V (c))       (5)    annotated photo-realistic scenes (both indoor and outdoor),
                             c
                                                                     with total of 1809 polygons with mean of 25.12±11 and a
                                                                     median of 25 polygons per image, containing zero to three
C. Color-component Histograms                                        instances of the target object. The data was collected using
In addition to the models described above, we have imple-            an Eyelink II head-mounted eye-tracker with a sampling rate
mented our own model based on a simplified factored shapes           of 500 Hz. The images were displayed with a resolution of
and appearances representation (Eslami & Williams, 2011).            1024 × 768 pixels, subtending a visual field of approximately
This model shares some characteristics with the spatial color        34 × 30 degrees. The data set consists of 54,029 fixations.
distribution described above, as it assumes that the pixels cor-     Figure 4 presents examples of scenes used in the experiment.
responding to each object have been generated by a number               The object naming dataset (Clarke et al., under revision)
of Gaussians in a feature space (we found Lab-space to be            contains data collected during an object naming experiment.
                                                                 2239

                                                                             Model                        Obj. counting     Obj. naming
                                                                             Saliency                          61.66            55.87
                                                                             Object overlay                    63.60            59.78
                                                                             Center bias                       68.02            69.17
                                                                             Converted (max)                   55.27            64.66
                                                                             Converted (mean)                  70.44            68.65
Figure 5: Examples of stimuli used in the object naming ex-                  Liu et al. 2011 features          66.67            67.42
periment. Typical responses are: cars, crossing, person for                  Color-component hist.             66.73            67.40
the left, bench, man for the center, and barbecue, charcoal,
chimney for the right image.                                              Table 1: Estimated percentage areas under the ROC curves
                                                                          presented in Figure 6.
The stimuli consists 132 fully object-annotated images with
a total of 2,858 polygons with mean of 14.2 ± 5 and a me-                 the saliency values to select the desired proportion of pix-
dian of 26 polygons per image. The images were presented to               els. The ROC plots for object-based models can not be con-
24 participants after the task was explained using written in-            structed this method as it would not ensure that entire objects
structions. Before each trial, participants were asked to fixate          are selected. Instead, an increasing number of objects with
a central cross. The image was then displayed for 5000 ms,                the highest saliency values is iteratively selected, and their
followed by a beep, after which the participants named ob-                total area is plotted in the ROC curve. The ROC curves con-
jects present in the scene. The image was displayed until the             structed this way are incomplete, representing only selection
participant finished the trial. Image presentation and appara-            of up to about 50% of the image area. Constructing ROC plot
tus were the same as in the visual counting data set. A to-               for larger selections would result in significant discontinuities
tal of 2,904 usable trials were collected, resulting in 88,371            due to the fact of all small objects being already selected and
fixations. Examples of images used as stimuli are shown in                essentially only large objects corresponding to surfaces such
Figure 5.                                                                 as floor, sky, or wall being left.
Analysis                                                                  Results and Discussion
                                                                          The results are presented in Figure 6. The ROC curves show
As well as the models described above, we test two baselines
                                                                          that selection based on object overlay is better than saliency
that do not use saliency in any form. The first one weights
                                                                          for thresholds smaller than 40%. Object-based saliency mod-
objects by their Euclidean distance from the center of the im-
                                                                          els in turn outperform object overlay. Center bias turns out
age, normalized by object area. This approach is inspired by
                                                                          to be a very competitive baseline, which is only matched by
experimental evidence of center bias in scene viewing (e.g.
                                                                          converted (mean).
Tatler, 2007), and will be referred to as center bias.
                                                                             An analysis of the areas under the ROCs, summarized in
   Secondly, based on the findings of Nuthmann & Hender-
                                                                          Table 1, confirm these observations. The ANOVAs reveal that
son (2010), we also include a baseline that predicts fixations
                                                                          for both datasets, object position overlay is significantly bet-
by selecting object centers. In this case, a map is built as
                                                                          ter than saliency with F(1, 24) = 9.27, p < 0.005 for object
a sum of Gaussians centered on the bounding boxes of the
                                                                          counting, and F(1, 23) = 9, 84, p < 0.005 for object naming.
object in the image. The covariances of the Gaussians are
                                                                             The calculation of area under ROC curve for object-based
dependent on object’s size, with a factor fitted using 10-fold
                                                                          models is not trivial due to the discontinuity of the plot.
cross-validation to avoid overfitting the datasets. This base-
                                                                          We estimated the AUC by interpolating the missing val-
line is referred to as object overlay.
                                                                          ues.2 The analysis of the interpolated curves shows that for
   In the Results and Discussion section below, we show
                                                                          both datasets, object-based selection is superior to traditional
how the different models perform by using receiver operat-
                                                                          saliency, and to object overlay. These differences are sta-
ing characteristic (ROC) plots, which indicate the sensitivity
                                                                          tistically significant, for example converted (mean) is better
(i.e., true positive rate vs. false positive rate) of a classifier as
                                                                          than saliency with F(1, 24) = 165.60, p < 0.001 for count-
its discrimination threshold varies. Moreover, in order to sta-
                                                                          ing and F(1, 23) = 279.30, p < 0.001 for naming; for color
tistically compare model performance, we calculate the area
                                                                          histogram the values are F(1, 24) = 34.67, p < 0.001 and
under the ROC curve (AUC) of each participant. The AUC
                                                                          F(1, 24) = 227.40, p < 0.001 respectively.
measures the probability that a classifier will rank a randomly
                                                                             The pattern for Converted (max) is more complicated.
chosen positive instance higher than a randomly chosen nega-
                                                                          On the naming data, it is significantly better than saliency
tive one.1 We submit the AUC means to an ANOVA analysis
                                                                          (F(1, 24) = 132.10, p < 0.001), but not as good as any of
to compare the performance of the different models pairwise,
                                                                          the other methods. On the counting data, it is signifi-
e.g., saliency against converted (mean). For standard pixel-
                                                                          cantly weaker than standard saliency (F(1, 23) = 245.70, p <
based saliency, the ROC curve is constructed by thresholding
                                                                              2 The discontinuities were interpolated by plotting linear seg-
    1 The AUC is equivalent to a Wilcoxon test of ranks.                  ments between end points of the ROC curve.
                                                                      2240

Figure 6: Performance of object-based selection of fixation locations on the Visual Count (top) and Object Naming (bottom)
datasets. Note that traditional saliency and object-based models cannot be compared directly due to differences in the selection
method, see text for details.
0.001), operating around chance level. This can be explained       fall within the object, but rather belong to its neighbors.
by the fact that saliency is sensitive to high contrast edges,        A surprising results is that object-based selection does not
usually corresponding to object boundaries. As such, the           outperform selection based on center bias. However, closer
highest saliency values corresponding to the object might not      investigation of the object rankings based on center bias and
                                                               2241

Converted (mean) reveals that the average correlation coef-             Chun, M., & Jiang, Y. (1998). Contextual cueing: Implicit learning
ficient between the respective rankings is only 0.50 for the                and memory of visual context guides spatial attention. Cognitive
                                                                            Psychology, 36, 28–71.
naming and 0.43 for the counting data. This indicates that              Clarke, A., Coco, M., & Keller, F. (under revision). The impact
different sets of objects are selected by the two model for a               of attentional, linguistic and visual features during object nam-
given threshold, accounting for different subsets of fixations.             ing. In G. Zielinsky, T. Berg, & M. Pomplun (eds.), Frontiers
A combined model would be a promising next step.                            in Perception Science: Research Topic on Scene Understanding:
                                                                            Behavioural and computational perspectives.
                                                                        Dziemianko, M. (2013). Modelling Eye Movements and Visual At-
                           Conclusion                                       tention in Synchronous Visual and Linguistic Processing. Ph.D.
                                                                            thesis, University of Edinburgh.
In this paper, we discussed the issue of objectness and its re-         Einhauser, W., Spain, M., & Perona, P. (2008). Objects predict fixa-
lation to the allocation of visual attention. We demonstrated               tions better than early saliency. Journal of Vision, 8, 1–26.
that it is possible to develop object-based version of saliency.        Eslami, S., & Williams, C. (2011). Factored shapes and appear-
Object-based saliency is not calculated as a value for each                 ances for parts-based object understanding. In Proceedings of
                                                                            the British Machine Vision Conference, (pp. 18.1–18.12). BMVA
of the image pixels (or coordinates), but rather over an area               Press.
within the boundaries of an object. In this approach, saliency          Henderson, J., Brockmole, J., & Castelhano, M. (2007). Vi-
is treated as a feature of an object, similar to other features             sual saliency does not account for eye-movements during visual
                                                                            search in real-world scenes. Eye movements research: insights
such as position. This approach is compatible with theories                 into mind and brain.
assuming an object-based allocation of attention, such as the           Henderson, J., Malcolm, G., & Schandl, C. (2009). Searching in the
Cognitive Relevance Framework (Henderson et al., 2009).                     dark: cognitive relevance drives attention in real-world scenes.
   The evaluation we presented used an object counting and                  Psychonomic Bulletin & Review, 16, 850–856.
                                                                        Itti, L., Koch, C., & Niebur, E. (1998). A model of saliency-based
an object naming data set. In spite of both of these tasks be-              visual attention for rapid scene analysis. IEEE Transactions on
ing object-centric by definition, we believe that our results               Pattern Analysis and Machine Intelligence, 20, 1254–1259.
generalize to other experimental tasks. Such tasks are often            Land, M., & Hayhoe, M. (2001). In what ways do eye movements
either object-centric as well (e.g., visual search), or evidence            contribute to everyday activities? Vision Research, 41, 3559–
                                                                            3565.
exists that attentional access is object-based even if the task         Land, M., Mennie, N., & Rusted, J. (1999). The roles of vision and
defined in terms of objects (e.g., in aesthetic judgment or in-             eye movements in control of activities of daily living. Perception,
terestingness judgment, see Nuthmann & Henderson 2010;                      28.
Einhauser et al. 2008). Indeed it was shown that visual at-             Liu, T., Yuan, Z., Sun, J., Wang, J., Zheng, N., Tang, X., & Shum,
                                                                            H. (2011). Learning to detect salient objects. IEEE Transactions
tention is object-based during everyday interaction with the                on Pattern Analysis and Machine Intelligence, 33, 353–367.
surrounding world (Land et al., 1999). Finally, it has been             Nuthmann, A., & Henderson, J. M. (2010). Object-based attentional
suggested that free viewing does not mean that viewers look                 selection in scene viewing. Journal of Vision, 20, 1–19.
at images without any task constraints, but rather with con-            Peters, R., Iyer, A., Itti, L., & Koch, C. (2005). Components of
straints to which experimenters do not have access (see Tatler              bottom-up gaze allocation in natural images. Vision Research,
                                                                            45, 2397–2416.
et al., 2011, for further discussion).                                  Rensink, R. (2000). Seeing, sensing, and scrutinizing. Vision Re-
   Even though the intuition that salience is a property of ob-             search, 10-12, 1469–1487.
jects has been utilized before, we are not aware of any exten-          Shore, D., & Klein, R. (2000). On the manifestations of memory in
                                                                            visual search. Spatial Vision, 14, 59–75.
sive experimental study aiming to investigate whether object-
                                                                        Spain, M., & Perona, P. (2011). Measuring and predicting object
based saliency and techniques used to detect salient objects                importance. International Journal of Computer Vision, 91, 59–
in computer vision can reliably predict human fixations. We                 76.
showed that the prediction of fixations based on objects and            Tatler, B. W. (2007). The central fixation bias in scene viewing:
their visual features is not only possible, but superior to stan-           Selecting an optimal viewing position independently of motor bi-
                                                                            ases and image feature distributions. Journal of Vision, 7, 4.
dard saliency. However, using the maximum value of saliency             Tatler, B. W., Hayhoe, M., Land, M., & Ballard, D. (2011). Eye
within an object was not confirmed as a reliable predictor of               guidance in natural vision: Reinterpreting salience. Journal of
whether object is going to be fixated, which is a important                 Vision, 11.
result considering the popularity of this feature in previous           Torralba, A., Oliva, A., Castelhano, M., & Henderson, J. (2006).
                                                                            Contextual guidance of eye movements and attention in real-
modeling studies.                                                           world scenes: The role of global features on object search. Psy-
                                                                            chological Review, 113, 766–786.
                     Acknowledgments                                    Walther, D., & Koch, C. (2006). Modelling attention to salient proto-
                                                                            objects. Neural Networks, 19, 1395–1407.
The support of the European Research Council under award                Yu, Y., Mann, G., & Gosine, R. (2010). An object-based visual
number 203427 “Synchronous Linguistic and Visual Process-                   attention model for robotic applications. IEEE Transactions on
                                                                            Systems, Man, and Cybernetics: Cybernetics, 5, 1398–1412.
ing” is gratefully acknowledged.
                                                                        Zelinsky, G. (2008). A theory of eye movements during target ac-
                                                                            quisition. Psychological Review, 115, 419–433.
                           References
Bruce, N., & Tsotsos, J. (2006). Saliency based on information max-
   imization. In Advances in Neural Information Processing Systems
   18, (pp. 155–162). Cambridge, MA: MIT Press.
                                                                    2242

