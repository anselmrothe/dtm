UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Generative Inferences Based on a Discriminative Bayesian Model of Relation Learning
Permalink
https://escholarship.org/uc/item/1t3991np
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Chen, Dawn
Lu, Hongjing
Holyoak, Keith
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                               Generative Inferences Based on a Discriminative
                                         Bayesian Model of Relation Learning
                                                     Dawn Chen1 (sdchen@ucla.edu)
                                                  Hongjing Lu1, 2 (hongjing@ucla.edu)
                                           Keith J. Holyoak1 (holyoak@lifesci.ucla.edu)
                                                 Departments of Psychology1 and Statistics2
                                                     University of California, Los Angeles
                                                           Los Angeles, CA 90095 USA
                              Abstract                                    formal grammar or a set of logical rules that generates
                                                                          alternative relational “theories”, which are in turn used to
   Bayesian Analogy with Relational Transformations (BART)
   is a discriminative model that can learn comparative relations         predict the observed data. That is, the set of possible
   from non-relational inputs (Lu, Chen & Holyoak, 2012). Here            relational structures is provided to the system by specifying
   we show that BART can be extended to solve inference                   a grammar that generates them.
   problems that require generation (rather than classification) of          Despite their impressive successes, there are some reasons
   relation instances. BART can use its generative capacity to            to doubt whether the generative approach provides an
   perform hypothetical reasoning, enabling it to make quasi-             adequate basis for all psychological models of relation
   deductive transitive inferences (e.g., “If A is larger than B, and
   B is larger than C, is A larger than C?”). The extended model
                                                                          learning. Since the postulated grammar of relations is not
   can also generate human-like instantiations of a learned               itself learned, the generative approach implicitly makes
   relation (e.g., answering the question, “What is an animal that        rather strong nativist assumptions. Moreover, generative
   is smaller than a dog?”). These modeling results suggest that          models of relation learning do not fit the intuitive causal
   discriminative models, which take a primarily bottom-up                direction. For example, it seems odd to claim that a binary
   approach to relation learning, are potentially capable of using        relation such as larger than somehow acts to causally
   their learned representations to make generative inferences.           generate an ordered pair (e.g., <dog, cat>) that constitutes
   Keywords:        Bayesian     models;      generative     models;      an instantiation of the relation. It seems more natural to
   discriminative models; relation learning; transitive inference;        consider how observable features of the objects in the
   deduction; induction; hypothetical reasoning                           ordered pair give rise to the truth of the relation, i.e., to
                                                                          apply a discriminative approach.
                          Introduction
                                                                          Discriminative Models of Relation Learning
Generative and Discriminative Models
                                                                          Recently, discriminative models have also been applied to
Bayesian models of inductive learning can be designed to                  relation learning. Silva, Heller, and Ghahramani (2007)
focus on learning either the probabilities of observable                  developed a discriminative model for relational tasks such
features given concepts (generative models) or the                        as identifying classes of hyperlinks between webpages and
probabilities of concepts given features (discriminative                  classifying relations based on protein interactions. Although
models; Friston et al., 2008; Mackay, 2003). Generative                   their model was developed to address applications in
models are especially powerful as they are capable of not                 machine learning, the general principles can potentially be
only classifying novel instances of concepts (using Bayes’                incorporated into models of human relational learning. One
rule to invert conditional probabilities), but also generating            key idea is that an n-ary relation can be represented as a
representations of possible instances. In contrast,                       function that takes ordered sets of n objects as its input and
discriminative models focus directly on classification tasks,             outputs the probability that these objects instantiate the
but do not provide any obvious mechanism for making                       relation. The model learns a representation of the relation
generative inferences.                                                    from labeled examples, and then applies the learned
   In recent years, generative Bayesian models have been                  representation to classify novel examples. A second key
developed to learn complex concepts based on relational                   idea is that relation learning can be facilitated by
structures (e.g., Goodman, Ullman & Tenenbaum, 2011;                      incorporating empirical priors, which are derived using
Kemp & Jern, 2009; Kemp, Perfors & Tenenbaum, 2007;                       some simpler learning task that can serve as a precursor to
Tenenbaum, Kemp, Griffiths & Goodman, 2011).                              the relation learning task.
Representations of alternative relational structures are used                These ideas were incorporated into Bayesian Analogy
to predict incoming data, and the data in turn are used to                with Relational Transformations (BART), a discriminative
revise probability distributions over alternative structures.             model that can learn comparative relations from non-
The highest level of the structure typically consists of a                relational inputs (Lu, Chen & Holyoak, 2012). Given
                                                                      2028

independently-generated feature vectors representing pairs       generated features characterizing 129 different animals (De
of animals that exemplify a relation, the model acquires         Deyne et al., 2008; see Shafto, Kemp, Mansinghka, &
representations of first-order comparative relations (e.g.,      Tenenbaum, 2011). Each animal in the norms is associated
larger, faster) as weight distributions over the features.       with a set of frequencies across more than 750 features. We
Learning is guided by empirical priors for the weight            created vectors of length 50 based on the 50 features most
distributions derived from initial learning of one-place         highly associated with the subset of 44 animals that are also
predicates (e.g., large, fast). BART’s learned relations         in the ratings dataset (Lu et al., 2012). Figure 1 provides a
support generalization to new animal pairs, allowing the         visualization (for 30 example animals and the first 15 of the
model to discriminate between novel pairs that instantiate a     50 features) of these high-dimensional and distributed
relation and those that do not. Moreover, BART’s learned         representations, which might be similar to the
weight distributions can be systematically transformed to        representations underlying people’s everyday knowledge of
solve analogies based on higher-order relations (e.g.,           various animals.
opposite).                                                          alligator
                                                                    bat
                                                                    beaver
   BART has thus demonstrated promise as a discriminative
                                                                    canary
                                                                    cat
                                                                    chicken
                                                                    cow
                                                                    deer
                                                                    dinosaur
                                                                    dog
                                                                    dolphin
                                                                    donkey
                                                                    dove
                                                                    duck
                                                                    eagle
model of relation learning, which does not presuppose an
                                                                    elephant
                                                                    fly
                                                                    fox
                                                                    frog
                                                                    giraffe
                                                                    goldfish
                                                                    hippopotamus
                                                                    horse
                                                                    kangaroo
                                                                    lion
                                                                    mouse
                                                                    ostrich
innate grammar of relations. However, the challenge
                                                                    owl
                                                                    penguin
                                                                    pig
                                                                                                             appears in fairy tales and stories
                                                                                                             can be bred
remains to extend the model to tasks requiring generative                                                    can fly
                                                                                                             can have different colors
inferences. For example, people are able to construct actual                                                 can swim
                                                                                                             can't fly
instantiations of relations, answering questions such as,                                                    cartoon figure
                                                                                                             eats fish
“What is an animal that is smaller than a dog?” (Although                                                    eats grass
                                                                                                             eats insects
one might suppose that such questions could be answered                                                      eats plants
                                                                                                             exists in different sizes and kinds
by undirected trial-and-error, we shall see that people’s                                                    has a bill
                                                                                                             has a tail
                                                                                                             has feathers
answers are often systematically guided by their
representations of the relation and of the animal provided as
a cue.) Another challenging task is purely hypothetical          Figure 1: Illustration of Leuven vectors (reduced to 15
reasoning, which requires making inferences about arbitrary      features to conserve space) for some example animals. The
instances of the relation. Comparative relations such as         cell intensities represent feature values (light indicates high
larger exhibit the logical properties of transitivity and        values and dark indicates low values).
asymmetry, supporting deductions such as “If A is larger
than B, and B is larger than C, then A is larger than C.”
                                                                 Relations as Weight Distributions
Children as young as five or six years can make such             BART represents a relation using a joint distribution of
transitive inferences reliably (Halford, 1992; Goswami,          weights, w, over object features. A relation is learned by
1995; Kotovsky & Gentner, 1996). In the present paper we         estimating the probability distribution P( w | XS , RS ), where
describe an extension of the BART model that addresses            X S represents the feature vectors for object pairs in the
these challenges of making generative inferences.
                                                                 training set, the subscript S indicates the set of training
        BART Model of Relation Learning                          examples, and R S is a set of binary indicators, each of
                                                                 which (denoted by R) indicates whether a particular object
Domain and Inputs                                                (or pair of objects) instantiates the relation or not. The
                                                                 vector w constitutes the learned relational representation,
We focus on the same domain and inputs used in the initial
                                                                 which can be interpreted as weights reflecting the influence
BART project (Lu et al., 2012): the domain of comparative
                                                                 of the corresponding feature dimensions in X on judging
relations between animal concepts (e.g., a cow is larger than
                                                                 whether the relation applies. The weight distribution can be
a sheep). To establish the “ground truth” of whether various
                                                                 updated based on examples of ordered pairs that instantiate
pairs of animals instantiate different comparative relations,
                                                                 the relation. Formally, the posterior distribution of weights
Lu et al. used a set of human ratings of animals on four
                                                                 can be computed by applying Bayes’ rule using the
different continua (size, speed, fierceness, and intelligence;
                                                                 likelihood of the training data and the prior distribution for
Holyoak & Mah, 1981). These ratings made it possible to
                                                                 w:
test the model on learning eight different comparative
relations: larger, smaller, faster, slower, fiercer, meeker,                                     P  R S | w, X S  P  w 
                                                                            P  w | XS , RS                               . (1)
smarter, and dumber.
  Each animal concept is represented by a real-valued                                         w  P  RS | w, XS  P  w 
feature vector. In order to avoid the perils of hand-coded         The likelihood is defined as a logistic function for
inputs (i.e., the possibility that the model’s successes may     computing the probability that a pair of objects instantiates
be partly attributable to the foresight and charity of the       the relation, given the weights and feature vector:
modelers), we use what we call “Leuven vectors.” These                                                    1
                                                                                    P( R  1| w, x)          T .          (2)
representations are derived from norms of the frequencies                                             1  e w x
with which participants at the University of Leuven
                                                             2029

   The prior, P(w), is a Gaussian distribution and is                                                    P(xA|xB)                            P(xA|xB,R=1)
                                                                                               10                                    10
constructed using a bottom-up approach in which initial                                         9                                     9
learning of simple concepts provides empirical priors that                                      8                horse                8                horse
                                                                                                7                                     7
guide subsequent learning of more complex concepts.                                             6 fly
                                                                                                            dog
                                                                                                                                      6 fly
                                                                                                                                                  dog
                                                                                      Speed                                  Speed
Specifically, BART extracts empirical priors from weight                                        5     mouse sheep elephant
                                                                                                                 cow
                                                                                                                                      5     mouse sheep elephant
                                                                                                                                                       cow
distributions for one-place predicates such as large to guide                                   4                                     4
                                                                                                3                                     3
the acquisition of two-place relations such as larger. Lu et                                    2                                     2
al. (2012) trained BART on the eight one-place predicates                                       1 worm                                1 worm
                                                                                                0                                     0
(e.g., large, small, fierce, meek) that can be formed using                                      0 1 2 3 4 5 6 7 8 9 10
                                                                                                           Size
                                                                                                                                       0 1 2 3 4 5 6 7 8 9 10
                                                                                                                                                 Size
the extreme animals at each end of the four relevant
                                                                                               10                                    10
continua (size, speed, ferocity, and intelligence).                                            9                                     9
   After learning the joint weight distribution that represents                                8                horse                8                horse
                                                                                               7                                     7
a relation, BART discriminates between pairs that                                              6 fly
                                                                                                           dog
                                                                                                                                     6 fly
                                                                                                                                                 dog
                                                                                       Speed                                 Speed
instantiate the relation and those that do not by calculating                                  5     mouse sheep elephant            5     mouse sheep elephant
                                                                                                                cow                                   cow
                                                                                               4                                     4
the probability that a target pair x T instantiates the relation                               3                                     3
                                                                                               2                                     2
R:                                                                                             1 worm                                1 worm
               P( RT  1| xT , X S , R S )                                                    0
                                                                                                0 1 2 3 4 5 6 7 8 9 10
                                                                                                                                     0
                                                                                                                                      0 1 2 3 4 5 6 7 8 9 10
                                                              (3)                                          Size                                  Size
                P( RT  1| xT , w) P( w | XS , RS ).
                  w
  Although the general framework of the relation learning                       Figure 2: Illustration of the generative model for inferring
model is straightforward, the calculations of the                               an animal that is larger than a sheep. Colors annotate
normalization term in Eq. (1) and the integral in Eq. (3) are                   probability densities (red indicates high values and blue
intractable, lacking analytic solutions. As in Silva, Heller,                   indicates low values). The top panel shows the prior and
and Gharamani (2007), we employed the variational method
                                                                                posterior distributions with  2  7 (favoring similarity-
developed by Jaakkola and Jordan (2000) for Bayesian
logistic regression to obtain closed-form approximations to                     based completions such as cow), and the bottom panel
the posterior weight distribution P( w | XS , RS ) and the                      shows the prior and posterior with  2  25 (favoring
                                                                                “landmark” completions such as elephant). Various animals
predictive probability P( RT  1| xT , XS , RS ).                               are represented in the two-dimensional space based on their
                                                                                size and speed ratings. The posterior was generated using
Extension to Generative Inference                                               the relational weights that BART learned from the full
The goal of the present paper is to endow BART with                             ratings input (i.e., all four dimensions).
generative abilities, allowing it (for example) to complete a
partially-instantiated relation, answering questions such as,                   with the first relational role ( w1 ) and weights associated
“What is an animal that is smaller than a dog?” We use the                      with the second relational role ( w 2 ). Similarly, the feature
weight representation for a relation learned by BART to
                                                                                vector x for a pair of objects is separated into the feature
construct a new generative model for the completion task.
When presented with a cue relation (e.g., smaller) and a cue                    vector for object A ( x A ) and the feature vector for object B
object (e.g., dog), the model produces possible responses for                   ( x B ).
                                                                                   The prior for the features of object A, P  x A | x B  , is the
the remaining object (e.g., cat) so that the ordered object
pair satisfies the relation. More specifically, given the
features of an object B, x B , and the knowledge that relation                  conditional distribution given the features of object B. It is
                                                                                defined as the following:
R holds for the object pair (A, B), the model generates a
probability distribution for the features of object A, x A , by                                    P  x A | x B   N  x B , 2I  .    (6)
making the following inference:                                                 We assume that object B (the referent) serves a starting
        P  x A | x B , R  1  P  R  1| x A , x B  P  x A | x B  . (4)   point for generating object A, so the means of P  x A | x B 
  The likelihood term, P  R  1| x A , x B  , is the probability              are taken to be the feature values of object B, reflecting a
                                                                                certain degree of semantic dependency between the two
that relation R holds for a particular hypothesized object A,                   objects (i.e., people’s tendency to think of A objects that are
 x A , and the known object B, x B . It is defined using a                      similar to B). The prior also encodes the assumptions that
logistic function, just as in Eq. (2):                                          the features of A are uncorrelated and have the same
               P  R  1| x A , x B  
                                                1
                                                               . (5)            variance  2 , the value of which is a free parameter
                                        1  e w1 x A  w2 x B
                                                T        T
                                                                                reflecting the strength of the model’s preference for
Relative to Eq. (2), we have only introduced small                              generating A objects that are similar to B.
differences in the notation. The learned relational weights,                      Our generative model infers a feature distribution for
w, are written as two separate halves: weights associated                       object A that reflects a compromise between (1) maximizing
                                                                            2030

the semantic similarity of A and B, which is reflected in the      the animal category. This is a Gaussian distribution with a
prior term; and (2) maximizing the probability that the            mean vector and covariance matrix that were directly
relation holds, which is reflected in the likelihood term. We      estimated from the feature vectors of the 44 animals in the
adapted the variational method to estimate the posterior           Leuven dataset that are included in the ratings dataset.
distribution, using the following update rules for the mean μ         Given the sampled animal B, the generative model
and covariance matrix V of the feature distribution, as well       constructs a distribution for animal A (e.g., to satisfy the
as the variational parameter ξ:                                    premise that “A is larger than B”) by letting B fill the second
                             I                                     role of the relevant relation. Similarly, the model constructs
                   V 1  2  2   w1w1T ,
                                                                  a distribution for animal C (e.g., to satisfy the premise that
                                                                   “B is larger than C”) by letting B fill the first role of the
                      I           w                 
              μ  V  2 x B  1  2 k     w1  ,        (7)    same relation. Next, the model creates feature
                                  2                             representations for specific animals A and C by setting their
                     2  w1T  V  μμT  w1 ,                     feature vectors, x A and x C , to be the means of the inferred
                                                                   feature distributions for A and C, respectively. Note that
                tanh  12   k                                 these “imagined” animals are hypothetical: although their
where                            and k  wT2 x B .
                    4   k                                      features are sampled from the distribution of animal
   Figure 2 illustrates the operation of the model in              features, the results will seldom correspond to actual
generating an animal (A) that is larger than a sheep (B). The      animals. To ensure that the premises have actually been
feature distribution for A is updated from a prior favoring        satisfied, the model accepts the imagined animal A only if
some degree of similarity between the two animals (left             P( R  1| x A , x B )  0.5 and P( R  1| x B , x A )  0.5, and the
panel; top: high similarity, bottom: low similarity) to a          imagined animal C only if P( R  1| x B , xC )  0.5 and
posterior distribution after taking into consideration the          P( R  1| xC , x B )  0.5.
relation (i.e., larger) instantiated by the animals (right
panel). These distributions are shown in a simplified two-            Finally, if x A and x C have been accepted as satisfying
dimensional feature space (the size and speed ratings for          the premises, the model calculates both P( R  1| x A , xC ) ,
animals; Holyoak & Mah, 1981).                                     denoting the probability that A is larger than C, and
                                                                    P( R  1| xC , x A ), denoting the probability that C is larger
            Modeling Transitive Inference
                                                                   than A. The model concludes that the relation holds for the
Comparative relations such as larger exhibit the logical
                                                                   pair <A, C> (and not for <C, A>) if P( R  1| x A , xC )  0.5
properties of transitivity and asymmetry, supporting
deductions such as, “If A is larger than B and B is larger         and P( R  1| xC , x A )  0.5, implying that a counterexample
than C, then A is larger than C.” Such hypothetical                has not yet been found to refute the transitive inference.
reasoning seems to depend on the ability to generate                  We conducted tests of transitive inference using the
arbitrary instantiations of the relation without any guidance      relational representations that BART learned based on 100
from object features (as the object representations are            randomly-chosen training pairs. For comparison, we also
semantically empty). Our first test evaluated whether the          tested a baseline model that substituted an uninformative
generative extension of BART enables transitive inferences         prior for the empirical prior that guides BART’s relation
on comparative relations using arbitrary hypothetical              learning (see Lu et al., 2012). For each of the eight
instances.                                                         comparative relations, the relation learning model was run
                                                                   ten times, each time with a different set of training pairs and
Operation of the Model                                             resulting in a different learned weight distribution. For each
The basic approach to transitive inference is                      of these learned weight distributions, we let the model
straightforward: The model “imagines” objects A, B, and C          generate 100 A-B-C triads satisfying the premises, testing
that instantiate the two given premises, as in the example         the relevant relationship between A and C for each triad. To
above, and then tests the unstated relationship for the pair       assess the influence of the free parameter in model
<A, C>. If the larger relation that BART has learned is            predictions, the tests were conducted multiple times with
indeed transitive, then any such instantiation will satisfy the    different values of  2 ranging from 1 to 1000. The
conclusion, “A is larger than C.” This test is done                strongest tests are those in which  2 is set at low values,
repeatedly, in essence searching for a counterexample. If no       creating a strong prior preference that A, B, and C are
counterexample is ever found, the transitive inference is          similar to one another. When the similarity constraint is
accepted.                                                          strong, the model is forced to generate animals that are
   Specifically, for each of the eight comparative relations       similar on the relevant dimension, and hence more likely to
that BART learned, we first let the model “imagine” an             yield a counterexample. When the value of  2 was reduced
animal B (because the statement “A is larger than B” implies
                                                                   below 1, the models produced many instantiations that did
that B is the referent against which A is being compared) by
                                                                   not satisfy the required premises (i.e., A > B, B > C, and not
sampling a feature vector from a distribution representing
                                                                   vice versa). We therefore treated the value of 1 as the
                                                               2031

minimal value of  2 that yields triplets of animals with                                     Smaller than a Dog
discriminable values on the relevant dimension.                                               r = .76              Observed
                                                                           Proportion
                                                                                        0.6
                                                                                                                   Predicted
                                                                                        0.4
Results and Discussion
                                                                                        0.2
Figure 3 shows the mean proportion correct (i.e., the mean                               0
proportion of triads that satisfy the conclusion based on
transitive inference) for BART and the baseline model as a
function of  2 . These results are averaged over the eight
comparative relations. The critical result is that the BART’s                                  Slower than a Dog
accuracy remains constant at 100% as  2 is reduced to the                                    r = .72              Observed
                                                                                        0.8
                                                                           Proportion
effective minimal value of 1. Thus, BART demonstrates                                   0.6
                                                                                                                   Predicted
what may be considered an inductive approximation to                                    0.4
deduction: despite exhaustive search for a counterexample                               0.2
to the transitive inference, no counterexample is ever found.                            0
In contrast, the baseline model often fails to infer that A > C
(and not vice versa) even when the value of  2 is as large
as 100.
                                                                  Figure 4: Observed human response proportions and
                      1                                           BART’s predictions for the queries, “Name an animal that is
                     0.9                                          smaller than a dog” (top), and “Name an animal that is
          Accuracy
                     0.8                                          slower than a dog” (bottom).
                     0.7
                               BART
                     0.6       Baseline                           probability distribution over the animals included in the
                     0.5                                          human responses. The model’s predicted probabilities were
                           1   10         100   1000              averaged across the ten runs.
                                     σ2                              The human results were complex, and here we report only
                                                                  a partial and preliminary attempt to make a comparison with
Figure 3: Mean proportion correct on the transitive               model predictions. Qualitatively, human responses were
inference task for BART and baseline model, as a function         dominated by two trends: (1) reporting an animal similar to
of the variance parameter. These results are averaged across      the cue animal and fitting the cue relation (e.g., cat for
the eight comparative relations.                                  “smaller than a dog”), or (2) reporting a “landmark” animal
                                                                  at an extreme of the continuum (e.g., turtle for “slower than
                     Animal Generation Task                       a dog”). The landmark animal coupled with the cue animal
A second evaluation of the model involves predicting the          provides an ideal example of the cue relation. This tradeoff
distribution of human responses in an animal generation           between reporting animals that are similar to the cue animal
study conducted using Amazon Mechanical Turk. In this             and reporting animals that are landmarks for the cue relation
free-generation study, participants typed responses to            (and usually more dissimilar to the cue animal) is captured
queries of the form, “Name an animal that is larger than a        by the single free parameter in the generative module,  2 .
dog.” They were instructed to enter the first animal that         As explained earlier (see Figure 2), a low  2 results in a
came to mind. Four comparative relations (larger, smaller,        response distribution that favors animals similar to the cue
faster, and slower) and nine cue animals (shark, ostrich,         animal, whereas a high  2 leads to a preference for
sheep, dog, fox, turkey, duck, dove, and sparrow) were
                                                                  response animals that are more likely to satisfy the cue
used. At least 50 responses were collected for each of the 36
                                                                  relation with respect to the cue animal (i.e., landmark
relation-animal pairs. To minimize learning across trials, we
                                                                  animals for the cue relation).
asked each participant to answer only two questions about a
                                                                     To reflect the unique pattern of human responses to each
single animal: either larger and then slower, slower and
                                                                  question, the variance parameter in the generative model
then larger, faster and then smaller, or smaller and then
                                                                  was chosen separately for each question (from the values, 1,
faster.
                                                                  5, 10, 50, and 100) to maximize the average of Pearson’s r
   The same relation-animal pairs were presented to the
                                                                  and Spearman’s ρ (rank-order) correlations between the
model after it had been trained on the relevant relations. For
                                                                  model’s predicted probabilities and the observed response
each question, the model produces a continuous posterior
                                                                  proportions for that question. Here we present results for
distribution for the feature vector of the missing animal
                                                                  two illustrative questions. The top panel of Figure 4 shows
using Eq. (4). This distribution was used to calculate the
                                                                  the model’s predicted response distribution and the human
probability densities for the feature vectors of all animals
                                                                  response distribution for the request, “Name an animal that
among the human responses that had Leuven vectors. These
                                                                  is smaller than a dog.” The human response pattern reveals a
probability densities were normalized to produce a discrete
                                                                  strong influence of semantic similarity between the cue
                                                               2032

animal and generated animal. The most common human                                          References
response was cat, followed by mouse (the landmark animal
                                                                    De Deyne, S., Verheyen, S., Ameel, E., Vanpaemel,W.,
for the smaller relation). With  2 = 10, the correlation             Dry, M., Voorspoels, W., & Storms G. (2008). Exemplar
between the model predictions and the human response                  by feature applicability matrices and other Dutch
pattern was r = .76.                                                  normative data for semantic concepts. Behavior Research
   The bottom panel of Figure 4 depicts the model                     Methods, 40, 1030-1048.
predictions and human response pattern for the request,             Friston, K., Chu, C., Mourão-Miranda, J., Hulme, O., Rees,
“Name an animal that is slower than a dog.” For this                  H., Penny, W., & Ashburner, J. (2008). Bayesian
question, the most common response was the landmark                   decoding of brain images. NeuroImage, 39, 181-205.
animal turtle. With  2 = 50, the correlation between the           Goodman, N. D., Ullman, T. D., & Tenenbaum, J. B.
model predictions and the human response pattern was r =              (2011). Learning a theory of causality. Psychological
.72. The higher variance assumed for this question (relative          Review, 118, 110-119.
to that for the smaller question) reflects the dominance of         Goswami, U. (1995). Transitive relational mappings in 3-
the landmark response for the slower question.                        and 4-year-olds: The analogy of Goldilocks and the Three
   Note that even though the two questions use the same cue           Bears. Child Development, 66, 877-892.
animal (dog), different sets of animals were generated              Halford, G. S. (1992). Analogical reasoning and conceptual
depending on the cue relation, revealing that humans do               complexity      in    cognitive   development.       Human
take relations into consideration in this free generation task.       Development, 35, 193-217.
The model showed a similar pattern of results.                      Holyoak, K. J., & Mah, W. A. (1981). Semantic congruity
                                                                      in symbolic comparisons: Evidence against an expectancy
                        Conclusions                                   hypothesis. Memory & Cognition, 9, 197-204.
These results provide initial evidence that a discriminative        Jaakkola, T. S., & Jordan, M. I. (2000). Bayesian logistic
model of relation learning, BART (Lu et al., 2012), can be            regression: A variational approach. Statistics and
extended to yield generative inferences. These inferences             Computing, 10, 25-37.
can involve relations between either hypothetical (in the           Johnson-Laird, P.N. (2008) Mental models and deductive
case of transitive inference) or actual (in the case of the           reasoning. In L. Rips & J. Adler. (Eds.), Reasoning:
animal generation task) objects. In the latter free generation        Studies in human inference and its foundations (pp. 206-
task, preliminary analyses indicate that BART achieves                222). Cambridge, UK: Cambridge University Press.
some success in modeling human response patterns.                   Kemp, C., & Jern, A. (2009). Abstraction and relational
   The model’s ability to make transitive inferences based on         learning. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K.
relations it has learned in a bottom-up fashion from                  I. Williams & A. Culotta (Eds.), Advances in Neural
examples illustrates the potential power of the                       Information Processing Systems, 22, 943-951.
discriminative approach to relation learning. Importantly,          Kemp, C., Perfors, A., & Tenenbaum, J. B. (2007).
BART is not endowed with any notion of what a “transitive             Learning overhypotheses with hierarchical Bayesian
and asymmetric” relation is (though like a 6-year-old child,          models. Developmental Science, 10, 307–321.
it is endowed with sufficient working memory to integrate           Kemp, C., & Tenenbaum, J. B. (2008). The discovery of
two relations as premises). Rather, it simply uses its learned        structural form. Proceedings of the National Academy of
comparative relations to imagine possible object triads, and          Sciences, USA, 105, 10687-10692.
without exception concludes that the inference warranted by         Kotovsky L, & Gentner D. (1996). Comparison and
transitivity holds in each such triad. The model thus                 categorization in the development of relational similarity.
approximates “logical” reasoning by a systematic search for           Child Development, 67, 2797-2822.
counterexamples (and failing to find any), akin to a basic          Lu, H., Chen, D., & Holyoak, K. J. (2012). Bayesian
mechanism postulated by the theory of mental models                   analogy with relational transformations. Psychological
(Johnson-Laird, 2008). The fact that BART achieves error-             Review, 119, 617-648.
free performance in the tests of transitive inference is            Mackay, D. (2003). Information theory, inference and
especially impressive given that its inductively-acquired             learning algorithms. Cambridge, UK: Cambridge
relational representations are most certainly fallible (e.g.,         University Press.
the model makes errors in judging which of two animals              Shafto, P., Kemp, C., Mansinghka, V., & Tenenbaum, J. B.
close in size is the larger; see Lu et al., 2012). It turns out       (2011). A probabilistic model of cross-categorization.
that imperfect representations of comparative relations,              Cognition, 120, 1-25.
acquired by bottom-up induction, can be sufficiently robust         Silva, R., Heller, K., & Ghahramani, Z. (2007). Analogical
as to yield reliable quasi-deductive transitive inferences.           reasoning with relational Bayesian sets. In M. Mella & X.
                                                                      Shen (Eds.), Proceedings of the Eleventh International
                                                                      Conference on Artificial Intelligence and Statistics.
                    Acknowledgments                                 Tenenbaum, J. B., Kemp, C., Griffiths, T. L., & Goodman,
Preparation of this paper was supported by grant                      N. D. (2011). How to grow a mind: Statistics, structure,
N000140810186 from the Office of Naval Research.                      and abstraction. Science, 331, 1279-1285.
                                                                2033

