UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Syntactic category disambiguation within an architecture of human language processing
Permalink
https://escholarship.org/uc/item/2918782b
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Author
Bauman, Peter
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

    Syntactic category disambiguation within an architecture of human
                                                language processing
                               Peter Baumann (baumann@u.northwestern.edu)
                                                    Northwestern University
                                       Department of Linguistics, 2016 Sheridan Road
                                                   Evanston, IL 60208, USA
                           Abstract                               for constraint-based models can also be accounted for un-
                                                                  der a modular architecture with a module for bottom-up
   Syntactic category ambiguities are very frequent in nat-
   ural languages, and all architectures of language process-     syntactic category assignment. In this paper, we follow
   ing need a mechanism for disambiguating syntactic cat-         Corley and Crocker’s proposal and provide further evi-
   egory ambiguities. Corley and Crocker (2000) suggested         dence for the existence of a syntactic category module
   that syntactic category disambiguation can be assigned
   its own module within a modular architecture. We will          by showing that Corley and Crocker’s model of syntac-
   show that the model defined by Corley and Crocker can          tic category disambiguation is a significant predictor of
   account for a considerable amount of variance in read-         reading times in naturally occurring texts. In addition,
   ing times of naturally occurring texts. In addition, we
   provide evidence that syntactic category disambiguation        we provide evidence that syntactic category disambigua-
   may be independent of syntactic top-down expectations,         tion may be independent of syntactic top-down expec-
   emphasizing the important role of bottom-up processes          tations, emphasizing the critical role of bottom-up pro-
   within an architecture of human language processing.
                                                                  cesses within a modular architecture of human language
   Keywords: sentence processing; reading; eye-tracking;          processing.
   ambiguity; lexical access.
                                                                  Syntactic Category Ambiguity
                      Introduction
                                                                  Many words in English (and presumably all other lan-
Successful language processing requires the integration
                                                                  guages) are ambiguous, they can have different senses
of bottom-up information extracted from the current
                                                                  and/or belong to different syntactic categories or part-
input and top-down expectations generated from what
                                                                  of-speech (i.e. noun, verb, adjective, etc.). The following
has been processed so far. When and how bottom-up
                                                                  example (from Boland, 1997) illustrates these ambigui-
and top-down processes interact has been a distinguish-
                                                                  ties:
ing feature of different processing architectures. On the
one hand, there are constraint-based models (e.g. Mac-            (1)       I saw her duck . . .
Donald, Pearlmutter, & Seidenberg, 1994; Trueswell &                   a.   . . . under the porch to eat some potato chips.
Tanenhaus, 1994; Tabor, Juliano, & Tanenhaus, 1997),                   b.   . . . under the porch eat some potato chips
which assume one single processing unit, in which all
available information is considered simultaneously. Mod-          In (1), the word duck is ambiguous between its verb and
ular architectures, on the other hand, consist of several         noun readings, and only the following context can disam-
distinct processing modules (e.g. Frazier, 1987; Frazier          biguate between the two syntactic categories and senses.
& Clifton, 1996; Corley & Crocker, 2000). These mod-              Syntactic category ambiguity and lexical ambiguity (in
ules are restricted to each having its own internal rep-          terms of different senses) need not come together like in
resentation, and they are independently predictive and            (1). Lexical ambiguity often occurs within the same syn-
informationally encapsulated (Crocker & Corley, 2002).            tactic category as in the word cabinet, which as a noun
Assuming this definition of modules in terms of informa-          can denote either a group of advisors or a closet. Syn-
tion flow, bottom-up processes are more likely to be mod-         tactic category ambiguity, on the other hand, does not
ular than top-down processes (Appelbaum, 1998; Fodor,             require lexical ambiguity, as evidenced by the English
1983).                                                            verbal system, where for all regular verbs there is only
   One particular process, for which constraint-based             one form for the past-tense and the past-participle. This
and modular models make contradicting predictions,                ambiguity is crucial to many garden-path sentences.
is syntactic category assignment or disambiguation:
constraint-based models assume that rich contextual in-           (2)       The horse raced past the barn fell.
formation is utilized to determine the syntactic category         (3)       The horse ridden past the barn fell.
(i.e part of speech) of a word, while modular architec-
tures only allow context-independent information. Al-             While example (2) is a classical garden-path sentence,
though previous research may seem to have provided ev-            which upon first encounter may be nearly impossible to
idence for both positions, Corley and Crocker (2000) (see         understand, example (3) is unambiguous and relatively
also Gibson, 2006) have shown that most of the evidence           easy to process. The fact that example (2) is derived
                                                              1833

from (3) only by replacing the ambiguous word raced                was proposed by Corley and Crocker (2000), will be in-
with the unambiguous ridden demonstrates the impor-                troduced in the next section and forms the basis of this
tant role of syntactic category disambiguation in lan-             paper.
guage processing (cf. Chomsky & Lasnik, 1977).
                                                                   The Statistical Lexical Category Module
Previous Research                                                  One curious fact about syntactic category disambigua-
One particular type of syntactic category ambiguity,               tion is that computers seem to be nearly as good at it as
which has received considerable attention in research is           humans are: unlike many other tasks in natural language
the noun-verb ambiguity. Based on three experiments,               processing, part-of-speech tagging has been an area in
Frazier (1987) suggested that the processor delays re-             which rather simple models can achieve near-ceiling ac-
solving the ambiguity until disambiguating information             curacy (Charniak, 1993). Inspired by this observation,
is encountered, as readers spent less time on ambiguous            Corley and Crocker (2000) assumed that syntactic cat-
words and more time on disambiguating context than on              egory disambiguation is distinct from syntactic parsing.
unambiguous words and their respective contexts. These             Reasons for this assumption are that syntactic category
results were put into question by MacDonald (1993) (see            disambiguation happens extremely locally, that the rel-
also MacDonald, 1993), who in turn argued that differ-             evant statistics are different from syntactic parsing, and
ent statistical measures and biases such as semantic bi-           that syntactic category disambiguation does not involve
ases, syntactic context and word co-occurrences could              structure building. This means that syntactic category
influence syntactic category disambiguation. Similarly,            disambiguation can have its own internal representation,
Tabor et al. (1997) showed that readers are sensitive              be informationally encapsulated and independently pre-
to syntactic context when resolving syntactic category             dictive, thus constituting the requirements for a separate
ambiguities between the determiner and complementizer              module, the Statistical Lexical Category Module 1 (Corley
readings of that: a reading time delay occurred when that          & Crocker, 2000).
following a verb was disambiguated as a determiner or                  Corley and Crocker’s model for the Statistical Lexical
sentence-initial that was disambiguated as a complemen-            Category Module (SLCM) is based on a simple bigram
tizer.                                                             statistical part-of-speech tagger defined by Equation 1,
   While the results cited so far suggest that syntactic           which expresses the assumption that the joint proba-
category disambiguation is – at least to some degree               bility P (t0 , . . . , tn , w0 . . . wn ) of all part-of-speech tags
– dependent on syntactic or discourse context, Boland              t0 , . . . , tn and all words w0 . . . wn read so far can be rea-
(1997) and Boland and Blodgett (2001) demonstrated                 sonably approximated by the product of the lexical bias
in a series of experiments that when reading a syntactic           (i.e. the probability of word wi given tag ti ) and the
category ambiguous word like duck, readers are sensi-              category bigram transitional probability.
tive to its lexical bias, i.e. the relative frequencies of the                                                n
lexical entries for this word, independent of the syntac-
                                                                                                             Y
                                                                        P (t1 , . . . , tn , w0 . . . wn ) ≈     P (wi |ti )P (ti |ti−1 ) (1)
tic or discourse context it appears in. In a similar vein,                                                   i=1
Stolterfoht, Gese, and Maienborn (2010) showed that for
German adjectival passives (e.g. closed), whose forms                  Since lexical bias P (wi |ti ) is a property of the word,
are ambiguous between passive participle and adjective,            the category bigram transitional probability P (ti |ti−1 )
there is an increase in reading times when preceded by             is the only means to capture context-dependence in this
an adjective-copula auxiliary as compared to the passive           model of syntactic category disambiguation, implying
auxiliary, and as compared to unambiguous adjectives.              that syntactic context-dependence is in fact only a de-
This suggests that syntactic category disambiguation has           pendence on the syntactic category of the preceding
a strong bottom-up component, which cannot be over-                word.
ridden by any top-down information. It is thus rather                  One may object that limiting context-dependence to
uncontroversial that lexical bias plays an important role          the category of only the preceding word is a too restric-
in syntactic category disambiguation (cf. e.g. Gibson,             tive assumption, but Corley and Crocker (2000) (see also
2006).                                                             Crocker & Corley, 2002) showed that it is enough to
   However, it remains open to what extent addi-                   model the results reported by MacDonald (1993) and
tional contextual information is used in this process:             Tabor et al. (1997).
Gibson (2006) proposed that in addition to the context-                However, the aim of this paper is not to try to explain
independent lexical bias syntactic category disambigua-            all psycholinguistic evidence involving syntactic category
tion is also affected by context-dependent syntactic ex-           disambiguities. Instead, we will evaluate Corley and
pectations, which he broadly formalizes as the probabil-           Crocker’s SLCM model on a larger scale as a predictor of
ity of a syntactic category in a given ‘syntactic environ-         reading times in naturally occurring text. While Corley
ment’. A more restrictive notion of sufficient contextual               1
                                                                          Corley and Crocker (2000) refer to syntactic category
information in syntactic category disambiguation, which            ambiguity as ‘lexical category ambiguity’.
                                                               1834

and Crocker assume a direct link between the proba-             in the first pass, i.e. before leaving the word either to
bilities derived from Equation 1 and human processing           the right or to the left. Data points were removed if a
difficulties, we follow common practice (e.g. Demberg &         word was not fixated, appeared as the first or last word
Keller, 2008; Pynte, New, & Kennedy, 2008) and take             in a line, or contained any non-letter symbol.
the logarithm as the linking function between probabil-
                                                                Control Variables All regression models included the
ities and reading times.
                                                                following control variables, which are known to have
   We thus obtain the following measure log PSLCM for a
                                                                an influence on reading times (c.f. Demberg & Keller,
word wi given its tag ti and the tag ti−1 of the previous
                                                                2008): number of characters per word, position of word
word:
                                                                in a sentence, an indicator variable whether the previ-
         log PSLCM = log P (wi |ti ) + log P (ti |ti−1 ) (2)    ous word was not fixated, and indicator variable whether
                                                                the following word was not fixated, the frequency of
   This measure is evaluated in Experiment 1, where we          the word, the frequency of the previous word, the for-
show that it is a significant predictor of reading times        ward transitional probability, i.e. bigram probability
in naturally occurring texts. In Experiment 2, we evalu-        P (wi |wi−1 ), and the backward transitional probability
ate both terms in Equation 2 separately and show that           P (wi |wi+1 ). All frequencies and transitional probabili-
lexical bias and category bigram transitional probabil-         ties were obtained by fitting a unigram or bigram model
ities make independent contributions to the model fit           with modified Kneser-Ney smoothing (Chen & Good-
observed in Experiment 1. In the final experiment, we           man, 1998) to the British National Corpus (100 million
provide evidence that syntactic category disambiguation         words) using the SRILM toolkit (Stolcke, 2002). All con-
may be independent of syntactic top-down expectations           tinuous variables were centered and scaled to two stan-
as measured by surprisal (Hale, 2001) based on a prob-          dard deviations to minimize collinearity. In addition,
abilistic context-free grammar.                                 all frequencies and transitional probabilities were log-
                                                                transformed before scaling.
                      Experiments
                                                                Estimating Probabilities in the SLCM Model
In recent years, it has become standard to evaluate
                                                                The probabilities in Equation 2 were estimated from a
computational models of language processing on ‘eye-
                                                                corpus obtained by concatenating the CLAWS-tagged
tracking corpora’, i.e. on eye-tracking data of people
                                                                versions of the British National Corpus and the Dundee
reading naturally occurring texts (Pynte et al., 2008;
                                                                Corpus. The lexical bias P (wi |ti ) was estimated as is, i.e.
Demberg & Keller, 2008). The basic idea is to fit two
                                                                without any smoothing. For estimating the the category
regression models to a measure of readings times. One
                                                                bigram transitional probability P (ti |ti−1 ) we again used
regression model (baseline model) includes as predictors
                                                                a bigram model with modified Kneser-Ney smoothing.
control variables, which are known to have an influence
on reading times. The second regression model includes          Regression Models For the regression models we
all those predictors as well, but in addition it also in-       used linear ‘mixed-effects’ models (Pinheiro & Bates,
cludes our computational model of language processing           2000; Gelman & Hill, 2007) of first-pass reading times
as a predictor. To test whether our computational model         with participant, word and text number as random ef-
of language processing is a significant predictor we com-       fects, as a generalization of the common by-subject and
pare the fit of the two regression models to the data by        by-item analyses, thus taking into account that the dif-
means of a log-likelihood test.                                 ferent words and texts read by the participants are ran-
                                                                dom samples in the same sense as the participants are
Methods                                                         (cf. Clark, 1973). All models were fit in R (R Develop-
In this section we describe the methodological detail           ment Core Team, 2011) using the lme4 package (Bates,
common across all three experiments.                            2005).
Data and Dependent Variable All three experi-                   Baseline Model Results
ments use the Dundee Corpus (Kennedy & Pynte, 2004),
                                                                The coefficients and standard errors of the baseline
a collection of eye-movement data from 10 participants
                                                                model are shown in Table 1. The coefficients are as
reading 51,501 words each of the British newspaper
                                                                expected based on prior research: e.g. reading times
The Independent. We approximated lexical categories
                                                                decreases with increasing position in the sentence and
by part-of-speech (PoS) tags, which were obtained by
                                                                increasing word frequency, and increase with an increas-
tagging the Dundee Corpus with the CLAWS tagger
                                                                ing number of characters in a word.
(Garside, 1987). Since syntactic category disambigua-
tion is assumed to happen ‘early’ in processing, we chose
first-pass reading times as our dependent variable. First-
pass reading times are calculated for a given word and
participant as the sum of all eye fixations on that word
                                                            1835

                                          Table 1: Baseline model coefficients                       Table 2: Model coefficient of full SLCM model
 Predictor                                                 Coeff.    Std.Error            t              Predictor      Coeff.   Std.Error        t
 (Intercept)                                               206.34         7.31        28.22              log PSLCM       -6.85        1.05    -6.54
 Position in Sentence                                       -6.02         0.51       -11.76
 Number of Characters                                       51.68         1.16        44.45
 Frequency of Word                                         -23.89         1.58       -15.15      partial effect of log PSLCM with all other predictors held
 Freq. of Prev. Word                                       -12.84         0.61       -20.90      constant at their respective means. It can be seen that
 Forward Trans. Prob                                       -10.24         0.94       -10.94      reading times increase as log PSLCM or PSLCM decrease.
 Backward Trans. Prob.                                      -1.95         0.70        -2.77
 No Fixation Next                                           10.14         0.49        20.61
                                                                                                 Discussion The above result shows that the simple
 No Fixation Previous                                       27.84         0.52        53.70
                                                                                                 model of syntactic category disambiguation in Equa-
                                                                                                 tion 2 cannot only account for many empirical results
                                                                                                 in psycholinguistic experiments, but is also a significant
                                                                                                 predictor of reading times in naturally occurring text.
Experiment 1                                                                                     The direction of the effect is in line with experimental
The objective of Experiment 1 is to evaluate Corley and                                          evidence modeled by Corley and Crocker (2000) in the
Crocker’s model of the SLCM as a predictor of reading                                            sense that a lower probability in Equation 2 leads to
times. The predictor to be evaluated is the full model as                                        higher reading times.
stated in Equation 2.
                                                                                                 Experiment 2
                                                                                                 In Experiment 2 we investigate whether lexical bias
Figure 1: Partial effect of full SLCM model with all other                                       and category bigram transitional probability are also in-
predictors held constant                                                                         dependently significant as predictors of reading times.
                                                                                                 To test this hypothesis, we fitted three models, one
                                                                                                 with only lexical bias (log-transformed P (wi |ti )), one
                                                                                                 with only category bigram transitional probability (log-
                                    260
  First-Pass Reading Times [msec]
                                                                                                 transformed P (ti |ti−1 )), and a third one with both terms
                                                                                                 as additional predictors to the baseline model.
                                    250
                                                                                                 Results The coefficients and standard errors for lex-
                                    240                                                          ical bias and category bigram transitional probability
                                                                                                 are shown in Table 3. The negative coefficients indicate
                                    230                                                          that increasing the lexical bias (i.e. making the ‘cor-
                                                                                                 rect’ category more likely) and increasing the category
                                    220                                                          bigram transitional probability both lead to shorter read-
                                                                                                 ing times. A log-likelihood test confirmed that a model
                                    210                                                          with either lexical bias (χ2 = 7.37, p < .001) or category
                                                                                                 bigram transitional probability (χ2 = 22.97, p < .0001)
                                                                                                 yields a significantly better fit to the data than the base-
                                            -3      -2       -1       0          1
                                                                                                 line model, and that a model with both predictors sig-
                                                         log PSLCM                               nificantly improves over a model with only one.
                                                                                                 Discussion Our results show that both lexical bias
                                                                                                 and category bigram transitional probability are signif-
Results The coefficient and standard error of the full                                           icant predictors of reading times. For lexical bias this
tagger-based model of syntactic category disambiguation                                          is in line with the results of Boland (1997) and Boland
(Equation 2) are shown in Table 22 . A log-likelihood
test between the regression model with the predictor
log PSLCM and the baseline model confirmed that Equa-
                                                                                                 Table 3: Model coefficient for lexical bias and category
tion 2 is a significant predictor of reading times (χ2 =
                                                                                                 bigram probabilities
29.955, p < .0001). The relation between log PSLCM and
reading times is plotted in Figure 1, which shows the                                                 Predictor            Coeff.    Std.Error        t
                   2
    Coefficients for the control variables are not listed as they                                     Lexical Bias          -4.86         1.57    -3.09
are qualitatively similar to the ones reported for the baseline                                       Category Bigram       -4.28         0.69    -6.18
model.
                                                                                              1836

and Blodgett (2001), who also found a significant effect         pothesis, and suggests instead that syntactic top-down
of lexical bias on reading times. The effect of category         expectations and bottom-up syntactic category disam-
bigram transitional probabilities shows that the imme-           biguation may be independent processes, as suggested
diately preceding category contains information beyond           by Gibson (2006) and Corley and Crocker (2000).
what is contained in the corresponding preceding word,
as including category bigram transitional probabilities                           General Discussion
improves over a baseline model, which already contained          In our experiments, we have shown that the model of
word bigram transitional probabilities.                          a Statistical Lexical Category Module as formulated by
                                                                 Corley and Crocker (2000) is a significant predictor of
Experiment 3                                                     reading times in naturally occurring texts. While our
In Experiment 3 we test whether the effects of syntac-           results do not necessarily imply that syntactic cate-
tic category disambiguation accounted for by the SLCM            gory disambiguation is a separate module, they provide
model can be ascribed to syntactic top-down expecta-             further evidence for modular models relying on simple
tions. If this were the case, it would provide strong ev-        context-independent statistics for lexical category dis-
idence against any modular approach to syntactic cat-            ambiguation. The observation that SLCM model is a
egory disambiguation. Syntactic top-down expectations            significant predictor of reading times in addition to syn-
are often measured by surprisal (Hale, 2001), which can          tactic expectations as measured by surprisal indicates
be calculated from a probabilistic context-free grammar.         that Corley and Crocker’s model may indeed account for
   We calculated unlexicalized surprisal values for all          bottom-up processes in reading, while surprisal accounts
words in the Dundee Corpus using the top-down parser             for top-down processes.
described in (Roark, 2001) and (Roark, Bachrach, Car-               Since any architecture of language processing needs to
denas, & Pallier, 2009) and included it as an additional         integrate bottom-up and top-down processes, one may
predictor in our baseline model. We than compared this           conclude that the combination of a restricted (or modu-
enriched baseline model to a regression model, which             lar) model of bottom-up syntactic category disambigua-
contained both surprisal and the log-probabilities of the        tion with a model of syntactic top-down expectations
tagger-based model of syntactic category disambiguation          may ultimately lead to better models of the architec-
(Equation 2).                                                    ture of human language processing and, more specifi-
                                                                 cally, to a better understanding of syntactic category
Results The coefficients and standard errors for sur-            disambiguation as a phenomenon at interface of lexical
prisal and the tagger-based model of syntactic category          access and syntactic processing, as recent experiments
disambiguation are shown in Table 4. As in Experiment            have shown that syntactic category ambiguity also plays
1, the coefficient of the tagger-based model is negative         a crucial rule in lexical-semantic access and disambigua-
coefficients indicating that increasing the probability in       tion (Jones, Folk, & Brusnighan, 2012).
Equation 1 leads to shorter reading times. The coeffi-              Finally, our results may also contribute to the ongo-
cient of surprisal is positive. This is expected as higher       ing debate on lexicalized vs. unlexicalized measures of
surprisal is associated with longer reading times (Hale,         syntactic expectations and their reflections in reading
2001; Demberg & Keller, 2008). A log-likelihood test             times (for a review, see Roark et al., 2009): since bigram
confirmed that a model with the tagger-based model and           probabilities are the simplest form of syntactic expecta-
surprisal improves significantly over a baseline model           tions, our observation that category bigram probabilities
with only surprisal (χ2 = 13.18, p < .001).                      are a significant predictor of reading times, even if con-
Discussion The above results show that SLCM model                trolled for word bigram probabilities, suggests that lex-
is a significant predictor of reading times even if surprisal    icalized and unlexicalized measures of syntactic expec-
is included in the baseline regression model. Although           tations may have independent contributions to reading
this does not rule out the hypothesis that the effects of        times.
syntactic category disambiguation accounted for by the
SLCM model may be reduced to syntactic top-down ex-
                                                                                  Acknowledgments
pectations, it provides strong evidence against such a hy-       The author would like to thank the four anonymous re-
                                                                 viewers for their suggestions and Masaya Yoshida and
                                                                 Daniel Müller-Feldmeth for many fruitful discussions.
Table 4: Model coefficient for surprisal and the full                                   References
SLCM model                                                       Appelbaum, I. (1998). Fodor, modularity, and speech
          Predictor     Coeff.    Std.Error        t               perception. Philosophical Psychology, 11 (3), 317–330.
          Surprisal       2.17          0.69   3.13              Bates, D. M. (2005). Fitting linear mixed models in R:
          log PSLCM      -5.67          1.11  -5.10                Using the lme4 package. R News: The Newsletter of
                                                                   the R Project, 5 (1), 27–30.
                                                             1837

Boland, J. E. (1997). Resolving syntactic category am-         Hale, J. (2001). A probabilistic Earley parser as a psy-
  biguities in discourse context: Probabilistic and dis-         cholinguistic model. In Proceedings of the 2nd Con-
  course constraints. Journal of Memory and Language,            ference of the North American Chapter of the Asso-
  36 , 588–615.                                                  ciation for Computational Linguistics (pp. 159–166).
Boland, J. E., & Blodgett, A. (2001). Understanding              Pittsburgh, PA.
  the constraints on syntactic generation: Lexical bias        Jones, A., Folk, J., & Brusnighan, S. (2012). Resolv-
  and discourse congruency effects on eye movements.             ing syntactic category ambiguity: An eye- movement
  Journal of Memory and Language, 45 , 391–411.                  analysis. Journal of Cognitive Psychology, 24 (6), 672–
Charniak, E. (1993). Statistical Language Learning.              688.
  Cambridge: MIT Press.                                        Kennedy, A., & Pynte, J. (2004). Parafoveal-on-foveal
Chen, S. F., & Goodman, J. (1998). An empirical study            effects in normal reading. Vision Reserach, 45 , 153–
  of smoothing techniques for language modeling (Tech.           168.
  Rep.). Center for Research in Computing Technology,          MacDonald, M. (1993). The interaction of lexical and
  Harvard University.                                            syntactic ambiguity. Journal of Memory and Lan-
Chomsky, N., & Lasnik, H. (1977). Filters and control.           guage, 32 , 692–715.
  Linguistic Inquiry, 8 (3), 425–504.                          MacDonald, M., Pearlmutter, N., & Seidenberg, M.
Clark, H. H. (1973). The language-as-fixed-effect fal-           (1994). The lexical nature of syntactic ambiguity res-
  lacy: A critique of language statistics in psychological       olution. Psychological Review , 101 (4), 676-703.
  research. Journal of Verbal Learning and Verbal Be-          Pinheiro, J. C., & Bates, D. M. (2000). Mixed Effects
  havior , 12 (4), 335–359.                                      Models in S and S-Plus. New York: Springer.
Corley, S., & Crocker, M. W. (2000). The Modular Sta-          Pynte, J., New, B., & Kennedy, A. (2008). A multiple
  tistical Hypothesis: Exploring Lexical Category Am-            regression analysis of syntactic and semantic influences
  biguity. In M. W. Crocker, M. Pickering, & C. Clifton          in reading normal text. Journal of Eye Movement Re-
  (Eds.), Architectures and Mechanisms for Language              search, 2 (4), 1–11.
  Processing (pp. 135–160). Cambridge: Cambridge               R Development Core Team. (2011). R: A Language
  University Press.                                              and Environment for Statistical Computing [Com-
                                                                 puter software manual]. Vienna.
Crocker, M. W., & Corley, S. (2002). Modular Archi-
                                                               Roark, B. (2001). Probabilistic top-down parsing and
  tectures and Statistical Mechanisms: The Case from
                                                                 language modeling. Computational Linguistics, 27 (2),
  Lexical Category Disambiguation. In P. Merlo &
                                                                 249–276.
  S. Stevenson (Eds.), The Lexical Basis of Sentence
                                                               Roark, B., Bachrach, A., Cardenas, C., & Pallier, C.
  Processing: Formal, computational and experimental
                                                                 (2009). Deriving lexical and syntactic expectation-
  issues (pp. 157–180). Amsterdam: John Benjamins
                                                                 based measures for psycholinguistic modeling via in-
  Publishing.
                                                                 cremental top-down parsing. In Proceedings of the
Demberg, V., & Keller, F. (2008). Data from eye-
                                                                 Conference on Empirical Methods in Natural Language
  tracking corpora as evidence for theories of syntactic
                                                                 Processing (EMNLP). Singapore.
  processing complexity. Cognition, 109 (2), 193–210.
                                                               Stolcke, A. (2002). SRILM - an extensible language
Fodor, J. A. (1983). The Modularity of Mind. Cam-
                                                                 modeling toolkit. In Seventh international conference
  bridge: MIT Press.
                                                                 on spoken language processing.
Frazier, L. (1987). Theories of Sentence Processing.           Stolterfoht, B., Gese, H., & Maienborn, C. (2010). Word
  In J. Garfield (Ed.), Modularity in Knowledge Repre-           category conversion causes processing costs: Evidence
  sentation and Natural Language Processing (pp. 291–            from adjectival passives. Psychonomic Bulletin & Re-
  308). Cambridge: MIT Press.                                    view , 17 (5), 651–656.
Frazier, L., & Clifton, C. (1996). Construal. Cambridge:       Tabor, W., Juliano, C., & Tanenhaus, M. K. (1997).
  MIT Press.                                                     Parsing in a dynamical system: An attractor-based
Garside, R. (1987). The CLAWS Word-tagging System.               account of the interaction of lexical and structural con-
  In R. Garside, G. Leech, & G. Sampson (Eds.), The              straints in sentence processing. Language and Cogni-
  Computational Analysis of English: A Corpus-based              tive Processes, 12 (211–272).
  Approach. London: Longman.                                   Trueswell, J., & Tanenhaus, M. (1994). Toward a lexi-
Gelman, A., & Hill, J. (2007). Data Analysis Using               cal framework of constraint-based syntactic ambiguity
  Regression and Multilevel/Hierarchical Models. Cam-            resolution. In C. Clifton & L. Frazier (Eds.), Perspec-
  bridge: Cambridge University Press.                            tives on Sentence Processing (pp. 155–179). Mahwah:
Gibson, E. (2006). The interaction of top–down and               Erlbaum.
  bottom–up statistics in the resolution of syntactic cat-
  egory ambiguity. Journal of Memory and Language,
  54 , 363–388.
                                                           1838

