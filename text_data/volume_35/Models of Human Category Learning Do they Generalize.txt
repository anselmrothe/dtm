UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Models of Human Category Learning: Do they Generalize?
Permalink
https://escholarship.org/uc/item/8cv657kb
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Conaway, Nolan
Kurtz, Kenneth
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                     Models of Human Category Learning: Do They Generalize?
                                         Nolan Conaway (nconawa1@binghamton.edu)
                                           Kenneth J. Kurtz (kkurtz@binghamton.edu)
                                           Department of Psychology, Binghamton University
                                                        Binghamton, NY 13905 USA
                             Abstract                                   (e.g., Homa, 1984; Nosofsky, 1992; see also Medin &
                                                                        Schaffer, 1978 and the ensuing literature on behavioral
  Generalization to new examples is an essential aspect of
  categorization. However, recent category learning research has        experimentation and model-fitting with the 5-4
  not focused on how people generalize their category                   classification problem).
  knowledge. Taking generalization to be a critical basis for                In a somewhat different approach to studying the
  evaluating formal models of category learning, we employed a          generalization of category knowledge, researchers have
  ‘minimal case’ approach to begin a systematic investigation of        investigated whether exemplar models can account for rule-
  generalization. Human participants received supervised                like generalization after category learning (Erikson &
  training on a two-way artificial classification task based on two     Kruschke, 1998, 2002; Nosofsky & Johansen, 2000). In
  dimensions that were each perfect predictors. Learners were           these studies, participants were asked to classify novel
  then asked to classify new examples sampled from the
                                                                        instances after learning an artificial two-way classification
  stimulus space. Most participants based their judgments on one
  or the other dimension. Varying the relative levels of                based on a unidimensional rule with exceptions. The critical
  dimension salience influenced generalization outcomes, but            test items were highly similar to the exceptions, but clearly
  varying category size (2, 4, or 8 items) did not. We fit two          classifiable using the rule. The outcomes of these studies
  theoretically distinct similarity-based models (ALCOVE and            were somewhat mixed and appear to depend on stimulus
  DIVA) to aggregate learning data and tested on the                    attributes and also on the structure of the categories that are
  generalization set. Both models could explain important               learned.
  aspects of human performance, but DIVA produced a superior                 The goal of the present research is two-fold: 1) to
  overall account.                                                      explore a different approach to investigating the psychology
  Keywords: generalization; categorization; formal models of            of category generalization; and 2) to use generalization
  category learning; similarity; cognitive modeling.                    performance as a basis to compare and differentiate models
                                                                        that are highly successful in fitting human learning data.
                          Introduction                                  Toward the first goal, our experimental approach is broadly
    Categorization is an essential cognitive function –                 comparable to the psychological studies of generalization
categories serve to organize knowledge and, critically, as a            discussed above: after a learning phase, participants are
basis for extending knowledge to make sense of new                      asked to classify novel examples. However, our work differs
experience. A full understanding of human categorization                in that we use minimal category learning conditions (small
depends on developing models and theories that account for              numbers of examples that are readily assigned to two fully
systematic patterns of human learning and generalization                coherent classes). Our primary aim is to identify basic,
performance (for an overview of generalization, see                     systematic properties of generalization performance.
Levering & Kurtz, 2010).                                                     Regarding the second goal, the field presently offers a
    In classic research, Roger Shepard (1957, 1987) put                 small group of formal models of category learning that are
forth the idea of a universal law in which stimulus                     general purpose (applicable to any classification problem),
generalization follows an exponential function of distance in           that provide explanation at the level of process/mechanism,
psychological space. This work has had broad implications               and that yield good fits to established benchmarks for
for theoretical models of categorization. Highly influential            human category learning. Within the realm of fitting human
reference point models (such as the exemplar view)                      classification learning performance, there is some sense of
compute classification in a manner that closely follows                 having hit the ceiling in terms of differentiating among
Shepard’s proposal. Specifically, the class membership of a             these models despite their having distinct explanatory
known item is likely to be generalized to a new item if the             elements. Our rationale is that models that do quite well in
two items are highly similar. The key additional design                 fitting learning data may diverge in their ability to account
feature needed to account for human classification                      for patterns of generalization performance. In particular we
performance is the inclusion of a selective attention                   are compelled by the prospect of fitting model parameters to
mechanism such that particular dimensions can matter more               the learning data and then holding the models to these
or less in the computation of similarity. Generalization                values in evaluating ensuing generalization (as discussed
performance (classification of previously unseen items) has             below). Toward this end, we evaluate two successful
been one of the most important important testing grounds in             models: a canonical representative of the reference point
the debate between exemplar- and prototype-based accounts               approach, ALCOVE (Kruschke, 1992) and an updated
                                                                    2088

version of a competing theoretical alternative, DIVA (Kurtz,              DIVA’s focusing parameter (β) allows it to selectively
2007).                                                                attend to stimulus dimensions based on the disparity in the
                                                                      output activations for that dimension across category
    ALCOVE. ALCOVE is an exemplar based adaptive                      channels. DIVA’s focusing mechanism differs significantly
network model. According to the model, categories are                 from selective attention in ALCOVE in that it does not
represented by individual exemplars stored in memory.                 change the encoding of the stimulus or manipulate the
ALCOVE learns to classify by adjusting association weights            representation learned by the model. DIVA’s form of
between exemplar nodes and category nodes, as well as by              focusing is decisional, rather than perceptual or
adjusting a set of attention weights that determine the               representational in nature, as it operates at the level of the
importance of each stimulus dimension.                                classification response.
    DIVA. DIVA offers a more generative than                                                Experiment 1
discriminative approach to classification learning and deals              This experiment was designed to explore generalization
in distributed rather that localist internal represenations.          under two conditions: when all stimulus dimensions are
Learning to classify examples is accomplished by                      diagnostic and equally salient; and when all dimensions are
minimizing reconstructive error along the channels of a               diagnostic, but unequally salient. Figure 1 depicts the two
divergent autoencoder that is comprised of recoding (input            category structures.
 hidden) weights shared for all categories and separate                  Stimulus scaling is an important aspect of our salience
sets of decoding (hidden  output) weights dedicated to               manipulation. In order to determine the relation between the
each category. Classification judgments are based on which            stimulus dimensions, we scale the examples in a pairwise
category channel yields the lowest error, i.e., which channel         similarity study. The similarity study generates a full set of
has been tuned to expect (and successfully reconstruct) a set         scaled examples, which allows us to manipulate the distance
of features like those of the current item.                           between examples on any dimension. The salience of a
    DIVA is similarity-based in the sense that the model              dimension can be specified by the distance between the
learns, for each category, how to effectively predict feature         categories on that dimension.
values for particular regions in recoding space – when an                 In a pilot study, we explored an extreme case of
input item projects into a region that is well handled by a           classification learning in which both stimulus dimensions
category, the reconstructive error in predicting the features         were diagnostic, but one dimension was much less salient.
will be low. DIVA does not apply Shepard-like stimulus                Participants were generally insensitive to variation in the
generalization to categorization – an item is likely to belong        less salient dimension. In light of these findings, we
to a category because its feature values conform to what a            expected that generalization gradients would show
category channel has been optimized to successfully recode            sensitivity given a relatively moderate difference in
and decode, not because it is highly similar to a known               dimension salience.
member of the category.
                                                                      Participants and Materials. 108 undergraduates from
Our approach to model comparison. We compare models                   Binghamton University participated in partial fulfillment of
based on their ability to account for human generalization            a course requirement. Stimuli were rectangles varying in
after category learning. An important advantage of focusing           shading and the distance between two lines within the
on generalization performance is that we avoid the                    rectangle. Examples were generated at 8 positions on each
traditional reliance on post-hoc fits. In all cases, we first fit     dimension (8 shading * 8 line spacing = 64 examples). The
DIVA and ALCOVE to averaged learning data from each                   category structures are depicted in Figure 1 along with
condition in order to find best-fitting parameters across the         sample stimuli.
full set of conditions. This procedure allows us to separate
out the parameter fitting process, so that the generalization         Procedure. Participants were randomly assigned to either
performance is genuinely a prediction based on a selected             the equal salience group or the unequal salience group. In
model.                                                                the equal salience condition, the category prototypes were
    We elected to fit ALCOVE using a grid search over its             separated by distances of 0.64 and 0.54 on the first and
response mapping (φ), specificity constant (c), association           second dimensions (shading and line spacing), respectively.
weight learning rate, and attention learning rate parameters.         In the unequal salience condition, the category prototypes
We also fit DIVA using a grid search over the parameters:             were separated by a distance of 0.65 and 0.34 on the first
learning rate, weight range, number of hidden nodes, and a            and second dimensions. In each condition, there were 4
new focusing parameter (β) that gives DIVA the ability to             training examples in each of the two categories.
account for sensitivity to differences in dimension
diagnosticity (Kurtz, 2008).
                                                                  2089

                                                                  from the subsequent analyses for failing to correctly classify
                                                                  7 out of 8 training items presented during the test phase. The
                                                                  remaining participants were more than 96% accurate during
                                                                  the last training block in both conditions.
                                                                       There were significant individual differences in the
                                                                  generalization data. A k-means analysis revealed three
                                                                  profiles based on the sensitivity estimates described above:
                                                                  these were unidimensional generalization based on either
                                                                  one or the other stimulus dimension (shading or spacing)
                                                                  and multidimensional generalization based on both
                                                                  dimensions. We compared the k-means findings across
                                                                  salience conditions (results are shown in Figure 2).
                                                                       While a very few participants were sensitive to both
                                                                  dimensions at test, the majority of participants generalized
                                                                  undimensionally. A Fisher’s Exact test revealed that the rate
                                                                  of each unidimensional profile differed between salience
                                                                  conditions (p < .001). Participants in the unequal salience
     Figure 1: Top: Four examples of stimuli (taken from          condition were more likely to be sensitive to the salient
     the corners of the stimulus space). Bottom left &            dimension (shading) than participants in the equal salience
     center: Category structures with equally and                 condition.
     unequally salient dimensions. Bottom right: Test set              We observed a bias towards the line spacing dimension
     used for Experiments 1 and 2. Note that all training         in the equal salience group that is not consistent with the
     items are included in the test set. Positions of             scaling. Interestingly, this may reflect a task difference
     examples reflect prior scaling.                              between pairwise similarity and classification learning that
                                                                  renders participants differentially sensitive to our stimulus
     Each participant completed 32 learning trials. On each       dimensions.
trial, a training item was presented on the computer screen            The main conclusions we can draw from this study of a
and participants were prompted to make a classification           ‘minimal case’ category structure are that: 1) participants
decision by clicking one of two buttons (labeled ‘Alpha’ and      tended to generalize according to a single dimension despite
‘Beta’). After responding, participants were given corrective     an optimal diagonal bound; and 2) dimension salience
feedback on their response. In the test phase, participants       increased the likelihood of the dimension serving as the
classified the 64 examples sampled across the stimulus            basis for generalization.
space (test set depicted in Figure 1). The 8 training items
were also presented during the test phase.                        Modeling Analyses. We tested DIVA and ALCOVE for
                                                                  their ability to account for these generalization findings.
     Gradient Analysis. In the test phase, participants           Specifically, we sought to determine whether the models
provide data that yield a generalization gradient of their        could account for: (1) the tendency of learners to generalize
classification responses. For each participant, we calculated     based on a single dimension; (2) the substantial degree of
the standard deviation of classification responses at 8           selection of each of the two dimension as the focal one by
positions on each dimension of the gradient. We then              different sets of learners; and (3) the effect of salience on
estimated sensitivity to each dimension by calculating the        dimensional sensitivity.
mean of these 8 values. Insensitivity to a dimension is                Before generating predictions for generalization, we
indicated by uniformity of classification responses across        obtained optimal parameter sets by fitting the models to the
that dimension.                                                   aggregate learning data (minimizing the sum of squared
                                                                  deviations, SSD, across learning blocks). We then generated
Results and Discussion. 24 participants were excluded             predictions for generalization across a range of optimal
  Table 1: Parameter values for ALCOVE and DIVA that best fit all conditions of learning performance in Experiments
  1 and 2.
                          ALCOVE                                                              DIVA
                              Experiment 1    Experiment 2                                       Experiment 1    Experiment 2
                               SSD < .003       SSD < .06                                         SSD < .004       SSD < .03
   c (specificity)                      3.4            10.5        number of hidden nodes                     1               1
   φ (response mapping)                 2.8            1.45        β (focusing)                             20               80
  attention learning                    0.0              0.0       learning rate                          0.14            0.18
  association learning                  0.1              0.3       initial weight range                 +/-0.5           +/-1.5
                                                              2090

parameter sets to gain a full understanding of how the two        either dimension. With larger weight ranges, DIVA
models performed.                                                 produced varied distributions of generalization profiles.
                                                                      Our analysis of DIVA’s generalization also revealed
                                                                  that, with a high focusing parameter, the model is more
                                                                  likely to generalize based on a salient dimension than a less
                                                                  salient dimension. This trend resembles the effect of
                                                                  salience that was observed previously. When the dimensions
                                                                  are equally salient, DIVA tends to produce
                                                                  multidimensional profiles at a greater rate than would be
       Equal Salience                     Unequal                 predicted given our behavioral findings.
             N=39                           N=41
    Figure 2: Results of k-means clustering results for
    Experiment 1. Number of participants shown below
    each chart.
    Both ALCOVE and DIVA provided good fits to the
learning data under a range of parameters. The best fitting              Figure 3: Category structures for Experiment 2.
parameter sets are shown in Table 1. When we tested the
models on generalization using these parameters, we found             These modeling results confirm that generalization
that both models were sensitive to the salience of each           provides a promising basis for model evaluation. We found
dimension, but neither predicted the unexpected bias toward       that DIVA and ALCOVE produce generalization gradients
the line spacing dimension that was observed behaviorally.        that are consistent with the salience of each dimension, and
    ALCOVE’s attention learning parameter largely                 that attentional mechanisms allow similarity-based models
governed the model’s ability to generalize to a single            to generate unidimensional gradients. Furthermore, a
dimension. ALCOVE produced unidimensional gradients               random component can partially explain variability in
with high levels of attention learning and multidimensional       dimensional selection.
gradients with low levels of attention learning. Given a high
attention learning parameter, ALCOVE generalized based                                  Experiment 2
on whichever dimension was most salient. We note that                 This study was designed to replicate and extend
ALCOVE lacks any random element such as initial weight            Experiment 1. As in the first study, we manipulated the
values, so the output is deterministic; for this reason, the      salience of dimensions by modifying the distance between
model does not account for the heavy use of both possible         the two categories. We extend the design by incorporating
unidimensional rules in the generalization data. Future           category size as a between-participants facto (Figure 3
research will explore generalization using a stochastic           depicts the category structures that were employed).
version of ALCOVE.                                                Category size is a potentially interesting factor in our studies
    Similar to ALCOVE’s use of attention, DIVA’s focusing         because increasing the number of examples in each category
parameter allowed the model to generate either                    also increases variation in representational demands for
unidimensional or multidimensional gradients. But unlike          exemplar models like ALCOVE without altering the
ALCOVE, DIVA is initialized with random weights on                solution that the model is required to find. Furthermore,
every run. An analysis of results on individual runs revealed     increases in category size should decrease the
that when DIVA’s focusing parameter was large and the             memorizability of each example (see Homa, 1984 for
dimensions were equally salient, the random initial weights       background on category size effects).
sometimes lead to unidimensional generalization based on              Our primary predictions were that: (1) generalization
                                                                  after learning would reflect sensitivity to a salient
                                                              2091

dimension; and (2) shifts in category size would impact the      Results and Discussion. 56 participants were excluded
prevalence of integrated, multidimensional generalization.       from subsequent analyses because they made more than one
                                                                 error on training items presented during the test phase. The
    Table 2: Distance between            opposite-category       remaining participants were more than 94% accurate during
    prototypes on each dimension.                                the last training block.
                                                                     The analysis of the generalization data was conducted as
               Equal Salience          Unequal Salience          in the first study. Results are displayed in Figure 4. The data
             Shading Spacing         Shading      Spacing        do not reveal an effect of category size on generalization.
                                                                 Consequently, our discussion focuses on the salience
    2 eg    0.70        0.55        0.72        0.34
                                                                 manipulation across category size conditions.
    4 eg    0.67        0.55        0.69        0.34                 As in Experiment 1, the majority of participants
    8 eg    0.64        0.54        0.65        0.34             generalized to a single dimension. A Fishers Exact test
                                                                 (conducted across size groups) reveals a significant effect of
Participants and Materials. 228 undergraduates from              salience (p < .01). Participants in the unequal salience group
Binghamton University participated in this experiment            tended to generalize using the salient dimension over the
toward partially fulfillment of a course requirement. The        less salient dimension.
materials were like those used in Experiment 1.                      We observed the same bias towards the line spacing
                                                                 dimension in the equal salience conditions: our participants
Procedure. Participants were randomly assigned to one of         were highly sensitive to the line spacing dimension, even
six conditions (2 levels of salience x 3 levels of category      when the scaling revealed that the dimensions were equally
size). The category structures are depicted in Figure 3.         salient.
Participants learned a classification based on two, four, or
eight unique examples per category.                              Modeling Analyses. We again tested DIVA and ALCOVE
                                                                 on their ability to match human generalization performance.
                                                                 In general, the modeling results for Experiment 2 parallel
                                                                 the results of Experiment 1. Both models found good fits to
                                                                 the aggregate learning data, but neither model predicted the
                                                                 unexpected bias towards the line spacing dimension during
                                                                 generalization. Neither model was affected by our category
                                                                 size manipulation. Parameter information can be found in
                   2egs            4egs             8egs         Table 1.
                                                                     As in Experiment 1, ALCOVE’s attention learning
  Equal                                                          parameter allowed it to account for unidimensional
                                                                 generalization. Given a high attention learning parameter,
Salience                                                         ALCOVE generalized based on whichever dimension is
                                                                 most salient. But due to the lack of a random component,
                 N=35             N=33             N=30          ALCOVE could not account for the use of either single
                                                                 dimension.
                                                                     As was the case for attention learning in ALCOVE,
Unequal                                                          DIVA’s focusing parameter allowed it to account for
Salience                                                         unidimensional generalization. Replicating our findings
                                                                 from Experiment 1, we found that when DIVA’s focusing
                                                                 parameter was large and the dimensions were equally
                 N=26                                            salient, the random initial weights lead to a distribution of
                                  N=30             N=22
                                                                 generalization profiles based on either or both dimensions.
    Figure 4: Experiment 2 k-means clustering results.           With larger initial weight ranges, DIVA produced more
    Number of participants shown below each chart.               varied patterns of generalization.
                                                                     The distributions produced by DIVA reflected the
    The salience manipulation was similar to that used in        salience of the stimulus dimensions. Specifically, DIVA
Experiment 1 with one departure – we partially re-arranged       was more likely to generalize using a salient dimension than
the members of the second category so that the category          a less salient dimension. This trend is similar to the effect of
prototypes would more evenly spaced apart in the equal           salience that we observed behaviorally. Lastly, as in
salient condition. The distances between prototypes for each     Experiment 1, DIVA tended to produce more
condition are shown in Table 2. All other aspects of the         multidimensional profiles when the dimensions were
procedure are identical to Experiment 1.                         equally salient.
                                                             2092

                  General Discussion                               Erickson, M. A. & Kruschke, J. K. (2002). Rule-based
                                                                      extrapolation in perceptual categorization. Psychonomic
Our behavioral results revealed that: (1) category knowledge          Bulletin & Review, 9(1), 160-168.
tends to be generalized based on a single dimension; and (2)       Homa, D. (1984). On the nature of categories. In G. H.
the salience of a dimension affects the probability that it is        Bower (Ed.), Psychology of learning and motivation
selected. We compared DIVA and ALCOVE on their ability                (Vol. 18, pp. 49-94). New York: Academic Press.
to account for these findings. We learned that these               Kruschke, J. K. (1992). ALCOVE: An exemplar-based
similarity-based models are sensitive to salience differences         connectionist model of category learning. Psychological
between dimensions and can use attention to generate                  Review, 99, 22-44.
unidimensional gradients. We also found that a random              Kurtz, K. J. (2007). The divergent autoencoder (DIVA)
component can help predict arbitrary dimension selection:             model of category learning. Psychonomic Bulletin &
DIVA’s initial weights randomly offset the models salience            Review, 14, 560 –576.
appraisal and allowed it to generalize to a single dimension.      Kurtz, K. J. (2008). Advances in modeling human category
    These results help to establish generalization as an              learning with DIVA. Presented at the 2008 Annual
important basis for formal model evaluation. By requiring             Meeting of the Psychonomic Society, Chicago, IL.
that models account for generalization and learning based          Levering, K., & Kurtz, K. J. (2010). Generalization in
on the same parameter fits, we systematically widen the               higher-order cognition: Categorization and analogy as
scope of what models are held accountable for explaining.             bridges to stored knowledge. In M. T. Banich & D.
In our work, generalization proved not only to be area where          Caccamise (Eds.), Generalization of knowledge:
DIVA and ALCOVE made different predictions, but it also               Multidisciplinary perspectives(pp. 175–196). New York,
provided an opportunity to reduce our reliance on post-hoc            NY: Psychology Press
fits by searching for parameters using aggregate learning          Medin, D. L., & Schaffer, M. M. (1978). Context theory of
data. In future work, we plan to conduct simulations using a          classification learning. Psychological Review, 85, 207-
stochastic modification of ALCOVE in order to determine               238.
how well the model matches our distributions of human              Nosofsky, R. M. (1992). Exemplars, prototypes, and
generalization performance. We also plan to conduct new               similarity rules. In A. F. Healy, & S. M. Kosslyn
simulations based on fitting the models to individual                 (Eds.), Essays in honor of William K. Estes (pp. 149-
learning curves rather than aggregate data.                           167). Hillsdale, NJ, England: Lawrence Erlbaum
                                                                      Associates, Inc.
                   Acknowledgments                                  Nosofsky, R. M. & Johansen, M. K. (2000). Exemplar-
                                                                      based accounts of “multiple-system” phenomena in
We would like to thank the members of the Learning and                perceptual categorization. Psychonomic Bulletin &
Representation in Cognition (LaRC) Laboratory at                      Review, 7, 375-402.
Binghamton University.                                             Shepard, R. N. (1957). Stimulus & response generalization:
                                                                      A stochastic model relating generalization to distance in
                                                                      psychological space. Psychometrika, 22, 325-345.
                        References                                 Shepard, R. N. (1987). Toward a universal law of
Erickson, M. A. & Kruschke, J. K. (1998). Rules and                   generalization for psychological science. Science, 237,
    exemplars in category learning. Journal of Experimental           1317-1323.
    Psychology: General, 127, 107-140.
                                                               2093

