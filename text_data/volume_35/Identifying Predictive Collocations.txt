UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Identifying Predictive Collocations
Permalink
https://escholarship.org/uc/item/6c67d4zm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Weinbach, Silas
Demberg, Vera
Publication Date
2013-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                                           Identifying Predictive Collocations
            Silas Weinbach (silasw@coli.uni-saarland.de) Vera Demberg (vera@coli.uni-saarland.de)
                  Department of Computational Linguistics                   Cluster of Excellence, Saarland University,
                Campus C7.2, 66123 Saarbrücken, Germany                   Campus C7.4, 66123 Saarbrücken, Germany
                               Abstract                                 1993). They are idiosyncratic because there is no rule which
   Idioms and common multi-word expressions are often argued            can tell us why a some specific lexemes (e.g., “strong tea” in-
   to be stored as chunks of words or fixed configurations in the       stead of “powerful tea”) are combined to express a particular
   mind, and to therefore be accessed faster and interpreted more       concept (McKeown & Radev, 2000).
   easily than fully compositional word combinations. Experi-
   mental research has furthermore shown that a specific “recog-
   nition point” can be identified in such expressions, at which        Representation of Collocations in Humans
   enough information is present to access the meaning of the           Idioms are a special type of collocations whose semantic
   whole expression and predict the remaining words of the col-
   location.                                                            meaning is not compositional of the meaning of the words it
   In this paper, we suggest measures for automatically identify-       contains, but are more idiosyncratic such as “give a whirl”
   ing those multi-word expressions where the first part is partic-     (meaning to try) or “spill the beans”. The status of these
   ularly predictive of the rest, and evaluate our measures against     expressions in the lexicon is still under debate. It has often
   human association data collected in a cloze test.
                                                                        been argued (Swinney & Cutler, 1979) that these idiomatic
   Keywords: Predictivity; Multi-Word Expressions; Collo-
   cations; Entropy                                                     expressions should be part of the lexicon. Some have even ar-
                                                                        gued that non-idiomatic collocations may likewise be stored
                          Introduction                                  as chunks in longterm memory (Ellis, 2001; Ellis, Frey, &
“When her boyfriend proposed to her, she was in seventh                 Jalkanen, 2009).
heaven.” “After jogging, he quenched his thirst with some                  An alternative model was proposed by Cacciari and Ta-
nice orange juice.” The above sentences contain colloca-                bossi (1988) and holds that both decomposable and idiomatic
tions where the first part of the collocation (e.g., “in seventh”,      expressions are represented in the lexicon “as configurations”
“quench”) is very predictive of the second part (“heaven” and           and that these configurations can get activated during process-
“thirst”, respectively). Such predictive collocations can be id-        ing as soon as enough information has been perceived to ren-
iomatic (as in the first example), or literal, fully compositional      der the collocation recognizable. Tabossi, Fanari, and Wolf
configurations. Previous studies observing human processing             (2009) present evidence that both idiomatic and literal collo-
of idioms have argued that there exists a “recognition point”,          cations may be stored in memory as such configurations.
at which comprehenders have identified the idiom and can                   On the other hand, Vespignani, Canal, Molinaro, Fonda,
predict the rest. Some also argue that not only idioms, but             and Cacciari (2010) find in an ERP experiment which com-
also frequent collocations, may be stored in the lexicon.               pares the processing of idiomatic expressions with literal
   However, by far, not all collocations are predictive, con-           phrases that language comprehenders have categorial tem-
sider for example light verb constructions where a very un-             plates for idioms in their lexicon, and that these can be ac-
predictive verb is combined with a sense-carrying noun. Be-             tivated at a specific recognition point after which a prediction
ing able to pick out predictive collocations among the set of           process is initiated. Their results suggest that this prediction
all collocations, and automatically identifying the recogni-            process can be distinguished from non-idiomatic predictive
tion point in idioms could be very useful for psycholinguis-            mechanisms. If such effects are to be modelled in a computa-
tic models of language processing: Processes of predicting              tional model, it is necessary for the model to have access to a
specific upcoming words, and accessing idiomatic meaning                set of idiomatic expressions and their recognition points.
could then potentially be captured in a broad-coverage model.              The goal of this paper is not to answer the question con-
   This paper takes a first step in this direction by propos-           cerning which types of collocations may be stored in memory
ing a number of alternative statistical methods for identifying         and which ones may be processed compositionally. Instead,
predictive collocations and evaluating them with respect to a           we evaluate statistical measures for automatically identifying
cloze task where people were asked to complete verbs with               predictive collocations. The methods and measures are gen-
the argument they associated most strongly. This evaluation             erally applicable and may later be used in combination with
captures the predictive strength of a verb in the absence of            a filter for identifying idiomatic expressions.
further predictive context, and is supposed to compare which               An important point for our study however is the relevance
of the measures works best at identifying good candidates for           of a recognition point and the notion of predictability of a
predictive collocations.                                                multi-word expression. Tabossi, Fanari, and Wolf (2005)
                                                                        showed that only the meanings of predictable idioms, but not
             Background and Related Work                                of all idioms, become available early on in idiom process-
Collocations are commonly used phrasal expressions which                ing. Such a prediction process may be beneficial to language
have become characteristic for a language or jargon (Smadja,            understanding because, as Tabossi et al. (2005) finds, recog-
                                                                    1581

nizing an initial fragment of a predictable idiom inhibits the          An alternative is the z-score (variant suggested by Evert
recognition of the literal meaning of the rest of the expression     (2008)). The formula below estimates the mean      p of the distri-
and hence facilitates comprehension by reducing ambiguity.           bution as E1,1 and its standard deviation as E(1, 1)
Automatically Identifying Collocations                                                                       O1,1 − E1,1
                                                                                       z − score : z =         p
The basic idea in automatically identifying collocations (for                                                      E1,1
a good overview, see (Manning & Schütze, 1999)) is to count            Pearson’s χ2 test (for a more detailed description, see
how often a set of words occur together within a specific dis-       (Manning & Schütze, 1999)) is very similar to the z-score,
tance of one another (e.g., always adjacent) or within a syn-        except it uses the square of the z-values and takes into ac-
tactic relationship (e.g., verb-argument). Many word-pairs or        count not only the probability of the words occurring together
multi-word expressions with frequent co-occurrence however           (O1,1 ), but compares also the estimated and observed frequen-
aren’t interesting collocations (like “in the”) because the rea-     cies of a w1 not occurring with w2 , w2 not occurring with w1
son for their high co-occurrence frequency is the high fre-          and the co-occurrence of words different from both w1 and
quency of each of the words and the syntactic constraints with       w2 .
which they occur.
   Two strategies are commonly used to ignore such cases:                                             (Oi, j − Ei, j )2
                                                                                          χ2 = ∑
the first one is to use statistical tests that indicate whether                                 i, j        Ei, j
two words were observed together more often than would be
expected otherwise. Collocation candidates are then ranked              Finally, the log likelihood ratio λ, similarly to χ2 , uses
with respect to significance scores. Note though that only           weighted on the similarity of the words w1 and w2 occurring
the ranking, but not the exact significance level, is usually        together or with different words.
considered interesting, as most co-occurrences are significant                                                    Oi, j
                                                                                           λ = 2 ∑ Oi, j log
simply due to the fact that language has some regular patterns                                    i, j            Ei, j
due to syntactic rules (Manning & Schütze, 1999). Another
common approach is to calculate the pointwise mutual infor-             Pointwise Mutual Information (PMI; Church and Hanks
mation (PMI) between two words.                                      (1989)) is and information-theoretic concept and measures
   The second strategy is to specify what types of colloca-          how much information is shared between words w1 and w2
tions should be found by specifying POS tag patterns or              – it is a symmetric measure. If there are two words with only
dependency relations between words (e.g., only considering           occur in the context of each other, then one of the words con-
adjective-noun pairs or only considering modifiers of nouns).        veys all the information that the two of them convey and their
   Finally, automatic methods developed for detecting id-            mutual information is maximal.
iomatic collocations often also use semantics to identify these                                               O1,1
                                                                                             PMI = log2
expressions: in non-compositional expressions, the meaning                                                    E1,1
of the words in the idiom are less likely to be semantically
related to the rest of the context (Katz & Giesbrecht, 2006).        Filters Previous work on collocation extraction has shown
   The following paragraphs are going to explain the most            (Seretan & Wehrli, 2009; Fazly, Cook, & Stevenson, 2009;
commonly used measures for detecting collocations, as well           Lin, 1998) that result quality depends also on choosing good
as the word patterns used in this work.                              patterns in which to observe collocation candidates. These
                                                                     have been defined via windows of observation, via fixed POS
Association Measures for Identifying Collocations To as-             tag sequences or via syntactic dependencies, as for example
sess whether a pair of words w1 w2 is a collocation, we can          from a dependency parser. The present study focusses on
count how often these words can be observed together O1,1 ,          verb-argument pairs as extracted from a large text resource
and calculate how often we would expect to see them together         using a dependency parser.
given their unigram frequencies and the size N of our data
                    1)          2)
set: E1,1 = f req(w
                 N     × f req(w
                             N     × N. If we observe them to-       Asymmetric Association Measures While there is a large
gether much more often than would be expected given their            body of literature on the topic of automatic recognition
unigram frequencies, we conclude that they are strongly as-          of multi-word expressions and idioms, there is almost no
sociated and represent a collocation.                                work on asymmetric association measures. An exception is
   The most commonly used association measures (AMs)                 Michelbacher, Evert, and Schütze (2007, 2011), who use con-
are the following: In a t-test (see for example (Manning &           ditional probability (see below), as well as a number of rank
Schütze, 1999)), the higher the t-value, the more likely that       measure which are based on the traditional association mea-
the observed co-occurrence of the words w1 and w2 would              sures explained above. As we found out after first submitting
not have happened by chance.                                         this paper, a related proposal for developing directional asso-
                                     O1,1 −E1,1
                                                                     ciation measures has been made by Gries (to appear). A com-
                                                                     parison between our measures and the associative-learning
                     t − Test : t = p N
                                         O1,1                        based approach should be addressed in future work.
                                                                 1582

              Proposed Predictive Measures
                                                                    Table 1: Correlation (Spearman’s rho)     between different as-
One way of capturing how predictive one word is of another          sociation measures for top 500 ranks.
word is to calculate the conditional probability (CP; also sug-               ρ FRQ T             Z    χ2     λ     PMI      CP
gested by Michelbacher et al., 2007; 2011) of the second                 FREQ 1             .62 .28 .29       .46   .06      .2
word given the first word. High CP indicates that the first                   T .62         1     .86 .83     .88   .72      .28
word is highly predictive of the second word.                                 Z .28         .86 1      .97    .91   .96      .38
                                                                             χ2 .29         .83 .97 1         .97   .93      .4
                     CP(w1 , w2 ) = P(w2 |w1 )                                λ .46         .88 .91 .97       1     .82      .4
                                                                           PMI .06          .72 .96 .93       .82   1        .33
   A straightforward approach to predictive collocations is                  CP .2          .28 .38 .4        .4    .33      1
to use conditional probability as an association measure, or
to combine existing measures for association between two
words with the conditional probability of the second word           the Gigaword’s 1.7 billion tokens, we extracted all depen-
given the first word. Different ways of combining the mea-          dency triples of the type “VB*:dobj:NN*” (i.e., verbs and
sures are possible, such as for example weighted additive           their direct arguments), for which the verb occurred to the
combination (a × CP + b × AM), or multiplicative combina-           left of the argument in the text. Verb-argument pairs which
tion (CP × AM).                                                     occurred less than 16 times in the corpus were excluded from
   In our preliminary experiments, it turned out that the addi-     the analysis, as some of the association measures are not ap-
tive models (which essentially represent a form of averaging        plicable when counts are too low. Furthermore, we removed
between the measures) do not perform well. While they boost         all verb-argument pairs containing words which were not in
the score of collocation candidates which are both strongly         WordNet under the correct POS tag. This later step filtered
associated and predictive, they do usually not boost it enough      out POS-tagging errors like “unsalted butter” or “quantum
to achieve rankings higher than those of candidate colloca-         mechanic” where “unsalted” and “quantum” were tagged as
tions which are extremely good on just one of the measures,         verbs, or “smile slyly” where “slyly” was tagged as a noun,
such that the resulting highest ranked candidates still contain     as well as foreign language material.
a lot of highly associated but non-predictive word pairs.
   Multiplicative combination, on the other hand, can be                                      Cloze Task
thought of as a filter that ranks down any collocation can-         The goal of our experiment is to evaluate whether combin-
didates which are highly associated but not predictive, and         ing one of the established measures for collocation extraction
boost highly predictive candidates, resulting in a cleaner list     with conditional probabilities will lead to a good measure for
of predictive collocations. Based on this observation, we pro-      identifying predictive collocations, and which of the proposed
pose the following new measures: CP, CP×χ2 , CP×PMI and             measures works best. For the evaluation, we use a simple
CP×λ, which we will evaluate in the remainder of this paper.        task which is independent of any sentential context: we ask
                                                                    human participants to complete a list of verbs with a noun
         Comparison of Association Measures                         they associate first, and then compare which of our measures
It is instructive to inspect how similar the alternative as-        predicts best the cloze probabilities of each verb. A reason
sociation measures are to one another. To this end, we              for evaluating with a completion experiment instead of sim-
sorted 3.6 million adjective-noun pairs from the ukwac corpus       ply comparing to a verb’s entropy or conditional probability
(Ferraresi, Zanchetta, Baroni, & Bernardini, 2008) according        on the corpus itself is that many of the highly ranked collo-
to each of our association measures and calculated the corre-       cations in our measures are in fact not necessarily generally
lations between these sorted lists. Table 1 shows that four of      valid predictive colloctions – some are very domain-specific,
our measures, χ2 , Z, λ and PMI actually result in very similar     such as rise percent (from “rise 20 percent”) and tell reporter.
rankings, with correlations ρ > .9. Only rankings by t-value
look a bit more dissimilar, and relatively more similar than        Experimental Materials
other measures to the overall frequency of word pairs (indi-        For evaluating predictive collocations, we were looking for
cated as FRQ in Table 1). It is also important to observe that      a set of verbs which contains a good portion of potentially
conditional probability (CP) leads to a very different ranking      predictive verbs. We therefore selected verbs for our com-
and is only correlated at 0.28 < ρ < 0.4 with the other mea-        pletion experiment by first calculating ranked lists of some
sures.                                                              of our target measures that we want to compare: CP×χ2 , χ2
                                                                    and CP×λ, and randomly chose 50 verbs out of the 200 best-
       Identification of Predictive Collocations                    ranked verb-object pairs of each measure. This procedure left
We dependency-parsed the Gigaword Corpus1 using the Stan-           us with a set of 118 verbs for our completion experiment.
ford parser (Marneffe, MacCartney, & Manning, 2006). From           The rationale behind choosing verbs this way instead of just
                                                                    selecting a random set of verbs is that we wanted to avoid
    1 http://www.ldc.upenn.edu                                      ending up with only a very small number of predictive verbs.
                                                                1583

                                                                                                Evaluation
Table 2: Arguments filled in for the verb “heal” during our
completion experiment. We also collected completion times             We evaluate our measures of predictive collocations in two
for each response.                                                    ways. A good measure should rank highly those collocations
        Answer.w2 Seconds Answer.w2 Seconds                           where the first part is highly predictive of the second part.
        the sick               7 wounds                55
        a wound               19 bodies                  8            Identifying Predictive Collocations We select a group of
        the sick               5 a wound                 8            highly predictive verbs (determined by their entropy in the ex-
        the wound             37 yourself              15             periment) and generate verb-noun pairs by selecting the most
        a wound               13 a sore                  9            common completion for those verbs in the experiment. This
        a wound               18 the wound             10             results in a list of verb-noun collocations where the verb is
        sores                  4 wounds                  4            highly predictive of the noun. Next, we calculate the average
        a wound                7 the wound               7            rank of these verb-noun pairs for each of our measures, see
                                                                      table in Figure 1.
                                                                         An important note to keep in mind when interpreting the
Procedure                                                             average ranks in the table in Figure 1 is that the set of verbs
We ran our experiment via Amazon Mechanical Turk                      was originally randomly chosen from among the top-ranked
(Paolacci, Chandler, & Ipeirotis, 2010). In order to explain          200 verb noun pairs of the measures CP×CHI, CHI and
the task to our subjects, we gave them three examples of com-         CP×λ; note also that Z is almost identical to CHI in the rank-
pleted verb-argument pairs, using verbs which were not part           ing it generates – these measures are therefore marked in bold
of the 118 verbs that we wanted to collect completion data            in the table.
for: “to quench thirst”, “to rob a bank” and “to feed the dog”.          The newly proposed measure CP×CHI clearly outper-
We restricted our subjects to people living in the U.S. and           forms the other measues. It has the lowest average rank,
instructed them to only take part in the experiment if they           meaning that the verb-noun pairs which we have identified as
qualified as native speakers of English. Furthermore, we also         being particularly predictive are ranked highest in this mea-
restricted our pool of workers to ones that had in the past got-      sure. Note that 44 verb-noun pairs satisfied the criterion of
ten > 95% of their HITs2 approved and had successfully com-           the verb entropy in the experiment being under the threshold
pleted at least 1000 HITs. We collected a total of 1888 verb-         of 1.5. This gives us an average rank of 22.5 as the best possi-
argument associations (i.e., 16 associations for each verb).          ble ranking which could possibly be achieved. Of course, not
Each worker was allowed to complete as many verbs as they             all possible verbs were tested in our experiment, hence di-
wanted (but, of course, each verb only once). The 1888 asso-          rect comparison to this value is not meaningful. More impor-
ciations were completed by 40 separate workers.                       tant is the comparison to the average ranks of other measures.
                                                                      Clearly, the combined measure CP×CHI is much better than
Collected Data
                                                                      either of its parts, and also clearly outperforms CP×λ.
For each verb, we collected 16 argument-associations. For                It is also fair to compare the measures which were not part
example, see completions for the verb “heal” in Table 2.              of constructing the evaluation verb set (not in bold) to one an-
We lemmatized all answers, and dealt with typos (e.g., ha-            other. Clearly, combining CP with the association measures
vok instead of havoc), orthographic variants (e.g., judgment          improves identification of predictive collocations, in particu-
vs. judgement) using minimum edit distance.                           lar there is an interesting boost in the performance of the t-test
   To assess the predictive strength of a verb, we calculated         measure when combined with conditional probabilities. We
the entropy of each verb given the types of responses (after          also conclude that λ is not a useful measure for identifying
clustering them by lemma and dealing with typos etc, as de-           predictive collocations.
scribed above). For example, the entropy of “heal” would                 Additional insight comes from plotting average ranks for
be 1.53. As we collected at most 16 associations per verb,            all verb-noun pairs with identical cloze probability, see Fig. 1.
entropy ranges between 0 and 4 for our data set. We can               For a measure which is good at identifying predictive collo-
then use the entropy to classify our verbs into highly selec-         cations, we expect there to be a linear relationship between
tive verbs (such as “grit”, “honk”, “flex”, “sing”, “twiddle”),       cloze probabilities and log rank (log rank makes sense be-
less selective verbs (e.g., “pay”, “fire”, “attend”) and non-         cause there are by definition more different noun pairs when
selective ones (e.g., “quote”, “shout”, “request”). In a linear       cloze probability is lower). Monotonicity in the trend of the
mixed effects regression analysis with random intercept and           log rank indicates that the measure correctly distinguishes be-
random slope for verb entropy under subject, we found that            tween different levels of cloze predictability. Furthermore,
verb entropy is a significant positive predictor of completion        average log rank for verb noun pairs with cloze probability
times (p < 0.01), i.e., when an argument of a verb is less pre-       1 should be close to 0. The plots show that CP×CHI comes
dictable, people take longer to fill in the slot.                     closest to the described ideal correlation. The r squared mea-
    2 HIT stands for “Human Intelligence Task” and is used as the     sure given in the title of each plot in Figure 1 quantifies the fit
official term for tasks in Amazon Mechanical Turk.                    between the plotted data points are from the regression line.
                                                                  1584

 verb entropy threshold 1.5
                                               log(CPxCHI RANK); r.squared= 0.770                                                  log(CPxLogLik RANK); r.squared= 0.762                                                     log(CHI RANK); r.squared= 0.736
 measure              rank
 CPxCHI             87.275                      ●                                                                                      ●                                                                                     ●
                                          10                                                                                      10                                                                                    10
 CHI               152.864                          ●
                                                                                                                                           ●                                                                                     ●
 Z                 152.948                              ●
                                                              ●   ●                   ●
                                                                                                                                               ●                                                                                     ●
                                                                                                                                                                                                                                               ●                   ●
                                          8                                                                                       8                  ●   ●                   ●                                          8                  ●
 CPxPMI            237.692
                                                                                                                                                                     ●                                                                             ●   ●
 CPxT              258.124
                               log rank                                                                                log rank                                                                              log rank
                                                                      ●                                                                                                                        ●
                                                                                                                                                             ●           ●                                                                                                 ●         ●
                                                                          ●       ●                                                                                                                                                                            ●
                                          6                                                                                       6                              ●                                                      6
                                                                                                        ●                                                                                                                                                              ●
 CP                291.342                                                    ●
                                                                                          ●
                                                                                              ●
                                                                                                                                                                                 ●
                                                                                                                                                                                     ●    ●                                                                ●                    ●
                                                                                                                                                                                                   ●
 CPxλ              404.471                4
                                                                                                   ●
                                                                                                                                  4                                                                    ●
                                                                                                                                                                                                                        4
 CPxFRQ            709.715                                                                                                                                                                                                                                                               ●
 PMI              1037.151                2
                                                                                                            ●
                                                                                                                                  2                                                                                     2
                                                                                                                                                                                                                                                                                             ●
 λ                2403.245                                                                                      ●
 Z                6863.703                0                                                                                       0                                                                                     0
 T                9378.727                              0.2           0.4         0.6             0.8           1.0                            0.2           0.4         0.6             0.8           1.0                           0.2           0.4         0.6             0.8           1.0
 ceiling              22.5                                  cloze probabilities                                                                    cloze probabilities                                                                   cloze probabilities
Figure 1: Table at left: Average rank in lists ranked according to association measures; set of predictive verb-noun pairs defined
based on different thresholds for verb entropy in experiment.
Plots: Average rank for CPxCHI, CPxλ and CHI grouped by cloze probabilities as obtained from MTurk experiment.
While measure CP×λ also follows a clear linear relationship,                                                                      tive collocations, suggesting possible measures and evaluat-
it does not locate the items with high predictability in its low-                                                                 ing these measures on a cloze task of verb-argument associa-
est ranks, indicating that it might be a good measure for quan-                                                                   tions, the next important step is to evaluate these methods on a
tifying collocations in general but not for predictiveness given                                                                  more specific task such as automatically identifying recogni-
the first part.                                                                                                                   tion points of idioms. Furthermore, this paper has only dealt
                                                                                                                                  with one type of collocation (verb-argument pairs) and has
                                                                                                                                  focussed on collocations consisting of only two words.
Correlation with human associations Our second evalua-
                                                                                                                                     In future work, we furthermore plan to evaluate the useful-
tion compares the association values from all measures to the
                                                                                                                                  ness of predictive collocations by including them in a model
cloze probabilities obtained in the experiment. We again eval-
                                                                                                                                  of language processing in the form of lexical configurations.
uate on average association values for each set of verb-noun
pairs with a given cloze probability, see Figures 2 and 3. A                                                                                                                     Acknowledgments
good measure should increase monotonically with increasing
                                                                                                                                  We would like to thank Stefan Thater for providing the
cloze probabilities.
                                                                                                                                  Stanford-parsed version of Gigaword.
   Among previously existing measures, PMI values can ex-
plain the largest amount of the variance in terms of average                                                                                                                                       References
PMI values compared to cloze probabilities from our exper-                                                                        Cacciari, C., & Tabossi, P. (1988). The comprehension of
iments. It is clear from Figure 2 that the common frequency                                                                         idioms. Journal of memory and language, 27(6), 668–683.
of the two words, as well as the log likelihood measure are                                                                       Church, K. W., & Hanks, P. (1989). Word association norms,
very poor predictors of predictive collocations.                                                                                    mutual information and lexicography. In ACL (Vol. 27,
   Among traditional measures combined with conditional                                                                             p. 76-83).
probabilities, CP×FRQ, CP×λ and CP×T perform very                                                                                 Ellis, N. (2001). Memory for language. Cognition and second
poorly. The problem for these measures is that they reflect                                                                         language instruction, 33–68.
strongly the overall frequency of a word pair. On the other                                                                       Ellis, N., Frey, E., & Jalkanen, I. (2009). The psycholinguis-
hand, average log CP×CHI values have the strongest linear                                                                           tic reality of collocation and semantic prosody (1). Studies
relationship with cloze probabilities, with few atypical points,                                                                    in Corpus Linguistics (SCL), 89.
as also reflected in the high R2 . This result is thus consistent                                                                 Evert, S. (2008). Corpora and collocations. Corpus Linguis-
with the rank analysis in the first evaluation.                                                                                     tics. An International Handbook, 2.
                                                                                                                                  Fazly, A., Cook, P., & Stevenson, S. (2009). Unsuper-
               Conclusions and Outlook                                                                                              vised type and token identification of idiomatic expres-
Our experiments indicate that the combination of conditional                                                                        sions. Computational Linguistics, 35(1), 61–103.
probability and the χ2 measure might work best for identi-                                                                        Ferraresi, A., Zanchetta, E., Baroni, M., & Bernardini, S.
fying collocations where the first word is highly predictive                                                                        (2008). Introducing and evaluating ukwac, a very large
of the second one. While this paper went a first step in de-                                                                        web-derived corpus of english. In Proceedings of the WAC4
voting some attention to the problem of identifying predic-                                                                         Workshop at LREC08.
                                                                                                                      1585

                                             FRQ; r.squared= 0.000                                                                log CHI; r.squared= 0.775                                                                      CPxFRQ; r.squared= 0.059                                                                      log CPxCHI; r.squared= 0.783
                                                                                                                                                                                                                                                                                                                8
                                                                                                                                                                                                                500
                                                                               ●                                                                                                          ●                                                                            ●                                                                                                                   ●
          8                                                                                                                                                                           ●                                                                                                                                                                                                ●
                                                                                                                     2                                                                                                                                                                                          6
                                                                                                                                                                                                                                                                                               AVG log CPxCHI
                                                                                                                                                                                                                400
                                                                                                                                                                             ●                                                                                                                                                                                                ●
                                                                                                                                                                                                   AVG CPxFRQ
                                                                                                                                                                                                                                                                                                                                                                     ●
                                                                                                       AVG log CHI
                                                                                                                                                                    ●
                                                                                                                                                                                                                                                                                                                4
AVG FRQ
          6                                                                                                          0                                      ●                     ●                                                                                                                                                                  ●                   ●         ●
                                                                                                                                                                                                                300
                                                                                                                                                ●   ●   ●                                                                                                                                                       2                                            ●
                                                                                                                                                                        ●                                                                                                                                                                        ●       ●
                                                   ●
                                                                                                                     −2                 ●                                                                                                                                                                       0
                                                                                                                                                                                                                200
          4
                                                                                                                                            ●                   ●                                                                                                                                                                        ●
                                                                                                                                                                                                                                                                                                                                             ●
                                                                                                                                                                                                                                                                                                                −6 −4 −2
                                                                                                                                  ●                                                                                                                                                                                                ●                             ●
                                                                   ●
                                               ●
                                                                                                                     −4
          2
                                                       ●
                                                               ●                                                              ●                                                                                 100                        ●                                ●                                                  ●
                                     ●   ●                 ●           ●            ●                                                                                                                                                              ●                                 ●   ●
                                                                                             ●                                                                                                                                         ●       ●           ●       ●
                                                                                         ●                                                                                                                                       ●                     ●       ●
                                 ●                                         ●                                                                                                                                                 ●                                                   ●
                                                                                                 ●
                                                                                                                     −6   ●                                                                                     0        ●                                                                                                 ●
                                         0.2           0.4         0.6             0.8           1.0                              0.2           0.4         0.6             0.8           1.0                                    0.2           0.4         0.6             0.8           1.0                                       0.2           0.4         0.6             0.8           1.0
                                                   cloze probability                                                                        cloze probability                                                                              cloze probability                                                                                 cloze probability
                                             PMI; r.squared= 0.803                                                                LogLik; r.squared= 0.047                                                                       CPxPMI; r.squared= 0.719                                                                      CPxLogLik; r.squared= 0.119
                                                                                                 ●                                                                      ●                                                                                                                ●                      700                                                      ●
          40 45 50 55 60 65 70
                                                                                             ●                       10
                                                                                                                                                                                                                5000
                                                                                                                                                                                                                                                                                               AVG CPxLogLik
                                                                                                                                                                                                                                                                                                                500
                                                                                                       AVG LogLik                                                                                  AVG CPxPMI
                                                                                    ●                                8
AVG PMI
                                                                           ●                                                                                                                                                                                                         ●
                                                                                         ●
                                                               ●                                                                                                                                                                                                   ●
                                                                                                                                                                                                                3000
                                                           ●                                                         6
                                                                                                                                                                                                                                                                                                                300
                                                                   ●
                                                                                                                                                                                                                                                                            ●
                                                       ●                       ●                                                                                                                                                                                                 ●
                                                                       ●                                             4                                                                                                                                                 ●
                                                                                                                                                                                                                                                   ●   ●
                                               ●                                                                                            ●
                                         ●                                                                                                                  ●                                                                                                                                                                                                                              ●
                                                                                                                                                                                                                0 1000
                                                   ●                                                                                    ●                                    ●        ●                                                        ●           ●
                                                                                                                                                                                                                                                                                                                0 100
                                                                                                                                                                                                                                                                                                                                                                              ●        ●
                                                                                                                     2                          ●       ●       ●
                                     ●                                                                                                              ●                                     ●                                                                    ●                                                                                 ●   ●       ●       ●
                                                                                                                                  ●                                                                                              ●     ●                                                                                                 ●   ●           ●       ●
                                                                                                                              ●                                     ●             ●                                          ●             ●                                                                                       ●
                                 ●                                                                                        ●                                                                                              ●                                                                                                 ●   ●                                                   ●
                                         0.2           0.4         0.6             0.8           1.0                              0.2           0.4         0.6             0.8           1.0                                    0.2           0.4         0.6             0.8           1.0                                       0.2           0.4         0.6             0.8           1.0
                                                   cloze probability                                                                        cloze probability                                                                              cloze probability                                                                                 cloze probability
                                             CP; r.squared= 0.735                                                                       Z; r.squared= 0.612                                                                       CPxT; r.squared= 0.119
                                                                                                 ●                   50                                                                   ●                     700                                                    ●
          80
                                                                                                                     40                                                               ●
                                                                                                                                                                                                                500
                                                                                                                                                                                                   AVG CPxT
          60                                                               ●
AVG CP
                                                                                             ●
                                                                                                       AVG Z
                                                                                                                     30
                                                                                    ●
                                                                                                                                                                                                                300
                                                                               ●
          40                                                                             ●
                                                           ●
                                                               ●                                                     20                                 ●               ●    ●
                                                                                                                                                            ●
                                                       ●                                                                                                            ●
          20                                                       ●                                                                            ●                                 ●                                                                                                      ●
                                                                                                                                                                                                                0 100
                                                                                                                                                    ●                                                                                                                       ●        ●
                                         ●     ●                       ●                                             10           ●     ●   ●
                                                                                                                                                                ●
                                                                                                                                                                                                                                               ●   ●
                                                                                                                                                                                                                                                       ●   ●       ●
                                     ●                                                                                                                                                                                           ●     ●   ●                   ●
                                                   ●
                                                                                                                              ●                                                                                              ●                                                   ●
                                 ●                                                                                        ●                                                                                              ●
          0
                                         0.2           0.4         0.6             0.8           1.0                              0.2           0.4         0.6             0.8           1.0                                    0.2           0.4         0.6             0.8           1.0
                                                   cloze probability                                                                        cloze probability                                                                              cloze probability
Figure 2: Average association values for each measure                                                                                                                                              Figure 3: Average association values for each combined mea-
grouped by cloze probabilities from MTurk experiment.                                                                                                                                              sure grouped by cloze probabilities from MTurk experiment.
Gries, S. T. (to appear). 50-something years of work on col-                                                                                                                                       Paolacci, G., Chandler, J., & Ipeirotis, P. (2010). Running
  locations: what is or should be next. International Journal                                                                                                                                        experiments on amazon mechanical turk. Judgment and
  of Corpus Linguistics, 18(1).                                                                                                                                                                      Decision Making, 5(5), 411–419.
Katz, G., & Giesbrecht, E. (2006). Automatic identification                                                                                                                                        Seretan, V., & Wehrli, E. (2009, March). Multilingual col-
  of non-compositional multi-word expressions using latent                                                                                                                                           location extraction with a syntactic parser. Language Re-
  semantic analysis. In Proceedings of the workshop on mul-                                                                                                                                          sources and Evaluation, 43(1), 71–85.
  tiword expressions (pp. 12–19).                                                                                                                                                                  Smadja, F. (1993). Retrieving collocations from text: Xtract.
Lin, D. (1998). Extracting collocations from text corpora.                                                                                                                                           Association for Computational Linguistics.
Manning, C. D., & Schütze, H. (1999). Foundations of statis-                                                                                                                                      Swinney, D., & Cutler, A. (1979). The access and processing
  tical natural language processing. In (second ed., chap. 5).                                                                                                                                       of idiomatic expressions. Journal of verbal learning and
  The MIT Press.                                                                                                                                                                                     verbal behavior, 18(5), 523–534.
Marneffe, M.-C. de, MacCartney, B., & Manning, C. D.                                                                                                                                               Tabossi, P., Fanari, R., & Wolf, K. (2005). Spoken idiom
  (2006). Generating typed dependency parses from phrase                                                                                                                                             recognition: Meaning retrieval and word expectancy. Jour-
  structure parses. LREC.                                                                                                                                                                            nal of psycholinguistic research, 34(5), 465–495.
McKeown, K. R., & Radev, D. R. (2000). Collocations. In A                                                                                                                                          Tabossi, P., Fanari, R., & Wolf, K. (2009). Why are idioms
  handbook of natural language processing (pp. 507–523).                                                                                                                                             recognized fast? Memory & cognition, 37(4), 529–540.
Michelbacher, L., Evert, S., & Schütze, H. (2007). Asym-                                                                                                                                          Vespignani, F., Canal, P., Molinaro, N., Fonda, S., & Cacciari,
  metric association measures. Proceedings of the Recent                                                                                                                                             C. (2010). Predictive mechanisms in idiom comprehension.
  Advances in Natural Language Processing (RANLP 2007).                                                                                                                                              Journal of cognitive neuroscience, 22(8), 1682–1700.
Michelbacher, L., Evert, S., & Schütze, H. (2011). Asymme-
  try in corpus-derived and human word associations. Corpus
  Linguistics and Linguistic Theory, 7(2), 245–276.
                                                                                                                                                                                                1586

