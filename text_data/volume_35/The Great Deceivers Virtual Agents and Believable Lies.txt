UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Great Deceivers: Virtual Agents and Believable Lies
Permalink
https://escholarship.org/uc/item/0g31m8s7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Dias, João
Aylett, Ruth
Paiva, Ana
et al.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                        The Great Deceivers: Virtual Agents and Believable Lies
                                                João Dias (joao.dias@gaips.inesc-id.pt)
                                       INESC-ID and IST, Universidade Técnica de Lisboa, Portugal
                                                   Ruth Aylett (r.s.aylett@hw.ac.uk)
                                                Heriot-Watt University, Edinburgh, Scotland
                    Henrique Reis and Ana Paiva (henrique.reis@ist.utl.pt, ana.paiva@inesc-id.pt)
                                       INESC-ID and IST, Universidade Técnica de Lisboa, Portugal
                                Abstract                                 tion can be seen as a human-like characteristic that would
                                                                         enhance the believability of synthetic characters portrayed in
   This paper proposes a model giving Theory of Mind (ToM)
   capabilities to artificial agents to allow them to carry out de-      real world social situations.
   ceptive behaviours. It describes a model supporting an N-level           A Theory of Mind (ToM) process allows an agent to at-
   Theory of Mind and reports a study to assess whether equip-           tribute an artificial mental state to another agent and reason
   ping agents with a two-level ToM results in them being per-
   ceived as more socially intelligent than agents with a single-        about it. In a single-level ToM, agent A can represent only its
   level ToM. A deception game being developed for intercultural         belief about what an agent B is thinking; an agent C that can
   training of children, used for this study, is described. Finally,     not only model what B is thinking but can also model what
   we report results from this study consistent with the hypothe-
   sis that a two-level Theory of Mind better supports agents in         agent B thinks about C has a two-level ToM. In this paper
   deceptive behaviour.                                                  we investigate the hypothesis that an agent with a single-level
   Keywords: Virtual Agents; Theory of Mind; Deception                   ToM will be less successful in believable deception then an
                                                                         agent with a two-level ToM. Deception cannot be investigated
                           Introduction                                  in abstract but requires a concrete scenario. Our work uses
The work reported in this paper arises from the use of syn-              an interactive game played by and with autonomous graphi-
thetic graphical characters interacting in rich virtual worlds.          cal characters. This is based on the popular game Mafia, or
These may be required for interactive drama applications                 Werewolf, described below, in which deception is fundamen-
(Mateas & Stern, 2003), or for story-based education and                 tal to successful play. The characters are implemented with
training applications (Paiva et al., 2004) (Swartout et al.,             a cognitive appraisal-based architecture (Dias & Paiva, 2005)
2006). A key criterion for success is that such agents be be-            that includes a deliberative mechanism and has been extended
lievable, that is lead a user, or viewer, to feel that they have         to support an N-level ToM mechanism.
an inner life of their own, with goals, motivations and emo-
tions, and are in some sense ’alive’ (Bates, 1994). Thus inter-
                                                                                     Background and Related Work
action between such characters must display features related             We define a “lie” as a direct communicative act that an agent
to human-human interaction; whether the actions they carry               performs to deceive another agent. We consider deception
out, their emotional expressions, ability to exhibit empathy, or         through verbal mechanisms - speech acts - though deception
non-verbal as well as verbal communications. Such features               may also be achieved through non-verbal mechanisms. De-
must be contextually appropriate, and in order to achieve this,          ception has been widely studied in AI, though usually with
characters may be driven by an architecture uniting cognitive            disembodied software agents.
and affective models, for example using a cognitive appraisal               GOLEM (Castelfranchi & deRosis, 1998) is based on the
approach (Dias & Paiva, 2005) (Marsella & Gratch, 2009).                 blocks world of AI planning research. Goals conflict, since
   Computationally implemented cognitive appraisal models                agents aim to build different structures from the same avail-
are often naive, assuming entirely open behaviour, sometimes             able blocks. Agents can achieve goals through their own ac-
referred to as meeting the sincerity condition (Searle, 1976).           tions or by asking for “help” from others. Agents have task
However, this is unusual in everyday human-human commu-                  delegation and adoption preferences and different capabili-
nication where deception often occurs. This may be as sim-               ties, used to plan their actions based on their knowledge of
ple as masking anger in front of a social superior or fear in            other agents. Deception is instrumental, resulting only from
front of a child on a dark night (Rosis, Pelachaud, Poggi,               goal conflicts, though it extends to deception about capabil-
Carofiglio, & Carolis, 2003), (Prendinger & Ishizuka, 2001),             ities, goals or personality. However, agents in GOLEM can
or as complex as deliberately misleading or lying to another             only produce lies within this limited scope. They cannot for
person in order to gain an advantage. Deceptive behaviour                example lie about the requests they have made or plan to
includes not only the generation of false beliefs in others              make. This would require second order reasoning about the
but also the claiming of desired identities, the exchange of             reasoning of other agents, which is not present here.
non-existent emotions, and the communication of false pref-                 De Rosis and Carofiglio (deRosis, F; Carofiglio, V; Gras-
erences or opinions (Wyer & Epstein, 1996). Thus decep-                  sano & Castelfranchi, 2003) focus on the communicative per-
                                                                     2189

spective of a deceptive action. In their scenario, a Sender            world and their subjective interpretations of world dynamics.
agent tries to convince a Receiver agent that some fact X              In particular, messages are implicit ways through which one
is not true, where the Sender can lie or use other deceptive           agent may influence the beliefs of another.
strategies. Their system, “Mouth of Truth” implements rea-                Wagner and Arkin developed algorithms to give an an in-
soning models as belief networks (Neapolitan, 1990; Pearl,             telligent robot the ability to deceive (Wagner & Arkin, 2010).
1997), where nodes represent belief and probabilities across           The Deceiver seeks to induce a false belief in another agent,
links to other node represent uncertainty. This allows the             the Target, who is modeled as an action model and utility
Sender to lie not about the belief they want to manipulate, but        functions with associated outcomes matrix for a specific sit-
about one connected to it. Thus uncertainty can be increased           uation. This involves performing some action in the environ-
for the belief “it rained” if the Sender claims “the floor outside     ment transmitting a false communication to the Target, so that
is dry”. However, the Sender needs a model of the Receiver’s           it will behave in a way benefiting the Deceiver. This modi-
beliefs to be able to do this and so acts as if its own set of         fies the outcome matrix for the Target, the induced outcome
beliefs and reasoning rules is replicated in the Receiver. This        matrix. Wagner and Arkin showed that knowledge of the Tar-
can then be used to influence the decision making process of           get affected the success of a deceit attempt. However this
the Sender.                                                            work did not explore the implications of different levels of
   The work so far discussed did not ground deception in an            ToM. Although there are systems that implemented a Theory
explicit model of other agents. Theory of Mind is a term               of Mind in agents, and interesting projects on deception, we
coined by (Premack & Woodruff, 1978) who define it as the              believe this is the first generic model that combines the two
ability to infer the full range of epistemic mental states of          in a way that is flexible enough to be featured in a game. Fur-
others, i.e. beliefs, desires, intentions and knowledge. This          ther, we also show a study that compares different levels of
is a mechanism that helps to make sense of the behaviour of            abstraction in the way agents are perceived in terms of lying.
others in specific contexts and to predict their next action.
                                                                                     A Mindreading Agent Model
   Recent work (Harbers & Meyer, 2009) focuses on a com-
putational implementation of ToM, giving agents the capac-             Our agent ToM is based on the Mindreading model of (Baron-
ity to interact in a believable way with trainees, and to ex-          Cohen, 1995), and follows the ST approach of Meyer et al.-
plain their actions and decisions after the training is over.          see Figure 1. A central Knowledge Base (KB) stores the
The agents model a trainee’s mind and give feedback either             agent’s beliefs and world knowledge and is the foundation
through simple action decisions, or by an explanation at the           for the agent’s behaviour given that its actions are based on
end. Meyer et al. here combined two prominent but con-                 its knowledge.
ceptually different approaches to the human theory of mind:
the Theory-Theory approach (TT) and the Simulation-Theory
approach (ST).
   In TT, the mental state we attribute to others is not ob-
servable, but is knowable through intuition and insight. Im-
plementationally, this is achieved by using inference rules to
reason about the beliefs of others. On the other hand, ST
claims that each person simulates being another while trying
to reason about their epistemic state, using the same struc-
tures and processes as those updating their own beliefs and                  Figure 1: Proposed model for a Mindreading Agent
knowledge (Aylett & Louchart, 2008). Meyer et al. showed
that the main difference lay in ease of implementation rather             The ToM has three components, following Baron-Cohen1 :
than in outcome, as ST models are better in terms of code              the EDD (Eye Direction Detector), SAM (Shared Attention
re-usability and modularity. Moreover, the TT approach can             Model), and ToMM (Theory of Mind Mechanism). EDD de-
only deal with BDI (Beliefs Desires Intentions) models (Rao            termines who sees what, while SAM constructs higher level
& Georgeff, 1995) due to a rigid representation of the men-            relations between entities (John sees that Luke sees the book).
tal state of other agents in terms of beliefs, limiting it to a        The ToMM represents and stores the mental states of other
specific symbolic representation.                                      agents and is used to influence or deceive another agent.
   PsychSim (Pynadath & Marsella, 2005) is a multi-agent               However, a deceiving agent must also be able to plan and
based simulation tool for modeling interactions using a                reason about the consequences of its own actions. Thus our
decision-theoretic approach. Unlike most such frameworks,              model includes a Deliberation component giving planning ca-
where agents select actions maximizing rewards using their             pabilities using knowledge from the KB and the ToMM to
own beliefs, PsychSim agents also take into account their be-          select the best actions for the agent to perform to meet its
liefs about other agent’s beliefs. These recursively- ”nested          current goals.
beliefs” may include subjective views of the agent itself.                 1 There is an additional component, the Intentionality Detector
Agents update their beliefs according to the changes in the            but to simplify our model it was not included
                                                                   2190

Representing Models of Others                                       target, parameters) and associates it with a list of effects. Two
                                                                    main types of effects are used in these rules:
Each Model of Other in the ToMM represents the beliefs of
a specific Other the agent knows. A single-level theory of          • Global effect - effect of an action assumed to be per-
mind allows us to represent an agents’s beliefs about another          ceived and shared by everyone (who is close enough).
                                                                       E.g. ∗:Werewol f (Rob) represents that everyone can perceive
agent’s beliefs. However, human adults are able to model               Werewol f (Rob).
more than one level (e.g. beliefs about another’s beliefs about
another’s beliefs). Children start to develop a second level        • Local effect - an effect perceived only by a particular agent.
                                                                       E.g. John:Werewol f (Rob) represents that only John will per-
of ToM at around the age of six. Thus agents intended to               ceive Werewol f (Rob).
function believably at the level of older children - as in the
Werewolf game used as a study - require a model with more              When EDD receives percept P, it determines two lists, per-
than one level of ToM.                                              ceptionVisibilities and agentVisibilities. The perceptionVis-
   A specific Model of Other contains its own ToMM also             ibilities list contains all pairs Ag:P, such that agent Ag per-
containing Models of Others, creating a recursive hierarchical      ceives proposition P, while the agentsVisibilities list contains
tree-like structure - see Figure 2.                                 all pairs of form Ag:Ag, stating which agents see which other
                                                                    agents. SAM uses this to update Models of Others. It tra-
                                                                    verses the tree hierarchy, establishing whether a Model M
                                                                    should perceive P applying the following test:
                                                                   1. Test if Model M is contained in perceptionVisibilities.
                                                                   2. Test if the pair Predecessor(M):M is contained in the
                                                                       agentVisibilities list. Predecessor(M) returns the prede-
       Figure 2: ToMM Hierarchy: 3 agents and 2 levels                 cessor of model M in the tree hierarchy.
   Thus three agents, A, B, and C, each with a two-level ToM       3. If both tests are verified, then model M can perceive P,
modeling ability, need six models each. If agents include a            otherwise the algorithm stops following the remaining sub-
three-level ToM, this rises to fourteen models, and with four          tree and continues the recursive process.
levels, to thirty models. The more complex the tree structure
                                                                       For example, suppose three agents, A, B and C. When A
for the model hierarchy, the more effort is required for each
                                                                    receives a percept P, it will update its own KB with P, but
update cycle. We will focus on a two-level ToM, bearing in
                                                                    will also process P in its ToM to update models for B and C.
mind that in the human case, applying more than two levels
                                                                    Further, suppose that A knows that both B and C perceived P,
also causes a substantial overhead. More levels could be used
                                                                    and also knows that B does not see C (so it will not see that C
in exchange for a slower reasoning cycle.
                                                                    perceives P). In this situation A’s Model of B will be updated
   The ST approach represents others by simulating ones own         with P but A’s model of B’s Model of C (second level) will
processes in that same situation. Hence a ToMM Model of             not be updated.
Other corresponds to a simplified version of the Agent Model
depicted in Fig. 1, including both data structures and pro-         Using the ToMM Information
cesses. A Model of Other can therefore be updated with a            Agents have two reasoning mechanisms, one forwards (from
given percept through the same process used to update the           data to conclusions) using inference rules, and one backwards
agent’s own model.                                                  (from goals to actions needed to achieve them) used to create
                                                                    plans that achieve the agent’s goals. An inference rule is a
Updating Models of Others
                                                                    tuple < R, P, E > where R is the name of the rule, P (Precon-
When a given percept is received (e.g. a property has               ditions) is a list of propositions that need to be verified for
changed, or an action was performed), the agent updates its         the rule to be applied, and E (Effects) a list of propositions
KB and its Models of Others. This is done through the EDD           that will be added to or removed from the KB when the rule
and SAM components.                                                 is applied. Whenever new knowledge is added to the KB,
   The EDD determines what entities, objects, and events            the deliberation component will test the preconditions of the
are perceived by other agents. It first checks whether a tar-       existing Inference rules. If any rule is fired (i.e. its precondi-
get agent is within a certain radius or in the same location        tions are verified) the deliberation component will automati-
as the agent, and if so, asserts that it also receives the per-     cally update the KB with the effects in its effects list. If this
cept.However this does not deal with more complex percepts          process adds a new proposition to the KB, the inference pro-
such as a whisper into an ear, where only the specific receiv-      cess will be repeated until no more changes are verified.
ing agent will know what was said. Hence the EDD may                   The second mechanism involves goals, plans and actions.
also include domain-specific rules about actions with particu-      A goal is a tuple < G, P, S > where G is the Goal’s name, P
lar restrictions on the perceptual mechanism. A rule specifies      a list of propositions that correspond to the goal’s precondi-
information about the action (such as subject, action name,         tions, and S a list of propositions that correspond to the goal’s
                                                                2191

success conditions (i.e. the desired goal state). The delib-         both A:B:Suspects(C) and A:D:Suspects(C).
eration component is constantly checking to see if any goal             When an inference rule has an effect specified with an
becomes active by testing its preconditions. Once a goal be-         agents list (e.g Ag:P), instead of updating its own KB, the
comes active, the planner tries to build a plan of actions to        deliberation component will traverse the tree hierarchy in or-
achieve the goal’s success conditions. The actions used by the       der to update the corresponding Models of Others. More-
planner are defined using a STRIPS-like (Fikes & Nilsson,            over, the ST approach means that the Model of Other cor-
1971) formalism and correspond to a tuple < Ag, A, P, E >            responds to a version of an Agent Model with its own in-
where Ag is the agent who performs the action, A is the ac-          ference mechanism. When creating a Model of Other, the
tion’s name, while P and E correspond to a list of precondi-         agent assumes that others will use the same inference rules as
tions and effects. Given the similar representations, Inference      its own. Therefore, every update cycle, the inference mech-
Rules can also be used by the planner to build plans of ac-          anism will also be executed recursively for each Model of
tions; the difference is that when an Inference Rule is selected     Other. In other words, the agent will simulate other’s infer-
for execution (when the agent is executing the plan) it is not       ence processes, and update the corresponding models. This
returned as an action to be performed in the environment. For        process is applied even if the effects of the inference rule
more details about these mechanisms, please refer to (Aylett,        specify an agent’s list. For instance, if the Model of Other
Dias, & Paiva, 2006).                                                of John at level 1, applies an inference rule that results in the
                                                                     effect Rob:Suspects(John), it will update John’s Model about
   The first step in making the ToM information available to
                                                                     Rob’s Model at level 2.
the deliberation component is to allow the specification of
preconditions that are not tested against the agent’s own KB            Due to its greater complexity, we did not include goal se-
but using a particular Model of Other. This is done by spec-         lection/planning, and thus simplified the version of the Agent
ifying explicitly the Model of Other to be tested by repre-          used as a Model of Other. The agent is therefore not capable
senting preconditions as a list of colon separated agents fol-       of simulating the planning process of others.
lowed by a proposition Ag1 :...:Agn :P. When the deliberative
                                                                                                 Case Study
component finds such a precondition it starts by traversing
the tree hierarchy of Models of Others using the list of colon       The model above was used to build NPCs that deceive in a
separated agents, and selecting the corresponding Model Of           system for intercultural training, MIXER (Hall et al., 2011).
Other. Then the proposition P is tested using the selected           This is aimed at children aged 9-11 and conflict between
Model of Other’s KB. As example, A:B:Suspects(A) is true             groups (an in- and out-group scenario) is presented through
if Suspects(A) is true in the Model of B that is stored in the       a social game. Rules act as cultural expectation and if they
agent’s Model of A (intuitively representing ”‘I think that A        are varied, conflict will occur. Older children usually define
thinks that B suspects him to be the Werewolf”’). If a propo-        rules before starting to play, but late primary children gen-
sition P does not specify a Model of Other it will be tested         erally only discover the difference in rules when the conflict
against the agent’s own model, in other words, its own KB.           occurs, often with game abandonment and shouts of ”‘it’s not
                                                                     fair”’ and ”‘I don’t want to play any more”’. The user acts
   Using preconditions this way allows us to specify goals           as an invisible (out-of-game) friend to a character thrust into
and inference rules triggered according to beliefs of others.        this situation with the pedagogic aim of showing that the ex-
It would be even more useful to model higher-level goals and         istence of different rules is not the same thing as ’cheating’.
inference rules, i.e. explicit goals and rules to change the         MIXER uses variations of the game Werewolf, or Mafia2 .
mental states of others. To do so, we use the same mech-                A simplified version of the game involves five players, the
anism used to specify local and global effects as described          Villagers, who are divided into two groups, one Werewolf
previously. An effect is specified as Ag1 :...:Agn :P, where         and four potential Victims. Victims have limited information,
Agi is an an agent’s name, or the symbol ”‘*”,’ and rep-             since they do not know who the Werewolf is (they are ’killed’
resents that only the Models of Others obtained by the list          at night). Characters can be human players or NPCs (Non
Ag1 :...:Agn will have the proposition P added to its KB. The        Playable Characters) running the architecture supporting de-
symbol ”‘*”’ represents that all Models of Others at that par-       ception. The goal is to discover who is the Werewolf: the
ticular level will be selected. The planner was extended to          character who is lying.. The Werewolf must lie purposefully:
be able to handle matching and detection of conflicts be-            its objective is to remain hidden until no longer outnumbered
tween preconditions and local/global effects. In planning            by Victims. Thus it tries to eliminate Victims while conceal-
terms, a precondition is matched or threatened by a local ef-        ing its true identity.
fect only if their agents lists are compatible and if they re-          The game has been implemented in turn-based rounds. In
fer to the same proposition P. In its simplest version, two          each round every character performs the Accuse action in or-
agents lists are compatible if they have the same size and           der, naming another character as the Werewolf (see Figure3 ).
the agents are unifiable (the symbol ”‘*”’ unifies with every-       The Werewolf deceptively accuses one of the victims, know-
thing). As examples, the effect A:B:Suspects(C) matches the          ing they are not in fact the Werewolf. At the end of each turn,
precondition A:B:Suspects(C), but does not match the pre-
condition B:A:Suspects(C), whilst A:∗:Suspects(C) matches                2 http://en.wikipedia.org/wiki/Mafia (party game)
                                                                 2192

the agreed werewolf is excluded from the game and informs                SELF represents the Werewolf agent itself. Thus the two-
the other agents about its true identity. This is used to infer          level ToM Werewolf will accuse villagers that are already be-
new information about past accusations. The real Werewolf                ing accused by other victims.
wins if it reaches the last turn alive, when there is only one
victim left. At this stage the Werewolf announces its identity.                             Tests and Evaluation
Victims win if they manage to discover who the Werewolf is               Two tests were run comparing these two versions. A first
before the last turn.                                                    simulation test assessed how well the two types of ToMs per-
                                                                         formed in the game. In order to test the hypothesis that an
                                                                         agent with a single-level ToM is less successful in believable
                                                                         deception then an agent with a two-level ToM, a second eval-
                                                                         uation was conducted with users, assessing their perception
                                                                         of the single-level and two-level ToM Werewolves.
                                                                            As an autonomous agent architecture is being used, scenar-
                                                                         ios are unscripted and do not run identically. To avoid differ-
                                                                         ent outcomes biasing user responses, a video of a particular
                                                                         run was used for the second test. The simulation test allowed
                                                                         us to select this video.
        Figure 3: An agent performing the Accuse action                     In the first test, two versions of the system were gener-
                                                                         ated. The first was parameterized so that the Werewolf used
    The following inference rules allow the victims to reason            the single-level ToM (ToM1 condition), and in the second it
about past actions, trying to determine possible werewolf sus-           used the two-level ToM (ToM2 condition). The victims used
pects:                                                                   a single level ToM in both conditions. With five players, the
                                                                         maximum number of possible rounds is 4. Both versions ran
• I suspect those that were accused by someone I don’t suspect           ten times, from the beginning until the Werewolf was caught
• I stop suspecting someone who accuses a target I suspect               or won the game. The number of turns the Werewolf man-
                                                                         aged in each run was recorded. The video of the best scoring
• I suspect those who accused a victim that was eliminated the pre-      run for each version was used for the second test.
    vious round
                                                                            In the TOM1 condition, the Werewolf never lasted four
• Someone who accuses a target suspects that they are a werewolf         rounds, and so did not win a single game. The ToM2 Were-
                                                                         wolf won in two out of ten runs and on average lasted 0.6
• Someone that is accused will suspect the accuser
                                                                         more turns than the ToM1 version.
Modeling the Werewolf                                                    User Perception of the Lying Agents
Two versions of the Werewolf agent were implemented. One                 The second test evaluated user-perceptions of the two Were-
has a single-level ToM, able to represent what victims believe,          wolf versions’ believability. An online questionnaire was
but not what victims think it or the other victims believe. The          used with the two videos selected from the first test. Sixty
second has a two-level ToM, able to represent what victims               participants (34 M, 26 F), of which 55 were aged 19-25, were
think about what it knows and in general, what victims think             recruited online, and randomly assigned to one of the two ver-
about the suspicions of others. Both versions also have the              sions. They were asked to pay special attention to agents’ ac-
inference rules above, used by victims to determine suspects.            tions and to try to work out who was lying. They watched the
    The single-level Werewolf has two main strategies compat-            game and then rated affirmations using a Likert scale (ranged
ible with its single-level ToM: eliminate victims that suspect           from -2 meaning totally disagree, to 2 meaning totally agree)
it, and make a victim suspect another victim who has not been            in four sections: (1) affirmations about the game itself; (2)
accused yet. The second goal corresponds to changing the                 affirmations about all players; (3) the same affirmations as
victim’s beliefs, and can be modeled by the success condition            (2) but only for the the liar; (4) affirmations focused on de-
[v1 ]:Suspects([v2 ]), where [v1 ] is a variable representing a vic-     ceptive behaviour. Data was analyzed using a non-parametric
tim and [v2 ] is a variable representing another victim. These           Mann-Whitney statistical test to compare conditions ToM1
variables will be instantiated by the goal activation process,           and ToM2.
and the agent will then try to make [v1 ] suspect [v2 ].                    Participants perceived the ToM2 condition as more inter-
    The two-level Werewolf agent has a strategy commonly                 esting according to A1: ”‘The game is interesting”’ (p <
used by human players in this game. The agent will ”‘Lay                 0.05, r = −0.263) and would play this version of the game
low”’, by avoiding suspicious actions, trying to make vic-               more ”‘A2: I would play a game like this”’(p < 0.05, r =
tims believe that it thinks the same way they do. This is                −0.292). ToM2 scores were significantly lower (p < 0.05)
modeled with the following second level success condition                for A3: ”‘It is easy to win while playing as a Victim”’ and
[v]:SELF:Suspects([target]), where [v] is a victim, [target]             significantly higher (p < 0.5) for A4: ”‘It is easy to win
is another villager that [v] suspects to be the Werewolf, and            while playing as a Werewolf”’. We conclude that participants
                                                                     2193

thought the liar did a more competent job in the ToM2 ver-                under project PEst-OE/EEI/LA0021/2011. The authors are solely
sion.                                                                     responsible for the content of this publication. It does not represent
   Answers to A8: ”‘Players behaved in a predictable way”’                the opinion of the EC or the FCT, which are not responsible for any
were also significantly different in the ToM2 condition (p <              use that might be made of data appearing therein.
0.001, |r| = 0.5): player characters were seen as less pre-
dictable in ToM1 than ToM2. This is seen as a surrogate for                                            References
believability given the answers to A10: ”‘Players are easily              Aylett, R., Dias, J., & Paiva, A. (2006). An affectively-driven plan-
                                                                             ner for synthetic characters. In Proc. icaps.
deceived”’ gave significantly lower values in ToM1 than in                Aylett, R., & Louchart, S. (2008). If I were you: double appraisal in
ToM2 (p < 0.001, r = −0.478), reflecting the more believ-                    affective agents. In Aamas-volume 3 (pp. 1233–1236).
able performance of the Werewolf in ToM2.                                 Baron-Cohen, S. (1995). Mindblindness: An Essay on Autism and
                                                                             Theory of Mind. MIT Press. Paperback.
   Finally, two additional measures: “how well did the liar               Bates, J. (1994). The role of emotion in believable agents. Commu-
play” and its “intelligence” also lead to statistically signifi-             nications of the ACM, 37, 122–125.
cant differences between the two conditions (p < 0.001) sup-              Castelfranchi, R., C; Falcone, & deRosis, F. (1998). Deceiv-
                                                                             ing in Golem: How to strategically pilfer help. In Autonomous
ported by a large effect size(|r| = 0.5). We conclude that the               agents: Workshop on deception, fraud and trust in agent soci-
liar in ToM2 is perceived as more intelligent than in ToM1.                  eties. Kluwer.
Also statistically significant (p < 0.001, r = −0.467) were               deRosis, F; Carofiglio, V; Grassano, G., & Castelfranchi, C. (2003).
                                                                             Can computers deliberately deceive? a simulation tool and its ap-
answers to A15: The liar is affected by others’ actions in-                  plication to Turing’s imitation game. Computational Intelligence,
dicating that the Werewolf was seen as more responsive to                    19(3), 235-263.
the play of others in ToM2. Finally, the higher results in                Dias, J. a., & Paiva, A. (2005). Feeling and reasoning: A computa-
                                                                             tional model for emotional characters. In C. Bento, A. Cardoso,
ToM2 for A21: The liar managed to deceive the other players                  & G. Dias (Eds.), Progress in artificial intelligence (Vol. 3808,
(p < 0.001, r = −0.524) confirm those for A10 above.                         p. 127-140). Springer.
                                                                          Fikes, R. E., & Nilsson, N. J. (1971). Strips: A new approach to
                        Conclusions                                          the application of theorem proving to problem solving. Artificial
                                                                             Intelligence, 2(3-4), 189-208.
This paper advances a model for virtual agents that are able              Hall, L., Lutfi, S., Nazir, A., Hodgson, J., Hall, M., Ritter, C., et al.
to deceive, embedding a ToM mechanism inspired by work                       (2011). Games based learning for exploring cultural conflict. In
                                                                             Aisb 2011 symposium: Ai & games, york (pp. 6–7).
on the human ToM. The model can produce N-level ToM be-                   Harbers, K. v. d., Maaike; Bosch, & Meyer, J.-J. (2009). Modeling
haviour using a simulation approach, where the agent runs its                agents with a theory of mind. In Proc. 2009 ieee/wic/acm int. joint
own mechanisms, reasoning about the beliefs and actions of                   conf. web intelligence and intelligent agent technology - v. 02 (pp.
                                                                             217–224). Washington, DC, USA: IEEE.
others as if it was in their shoes. Parametrization allows the            Marsella, S., & Gratch, J. (2009). Ema: A process model of ap-
number of levels of ToM to be easily varied.                                 praisal dynamics. Cognitive Systems Research, 10(1), 70–90.
   Evaluation was carried out using a social game, MIXER,                 Mateas, M., & Stern, A. (2003). Façade: An experiment in building
                                                                             a fully-realized interactive drama. In Game developers confer-
for intercultural training of children aged 9-11. This game                  ence, game design track (Vol. 2, p. 82).
includes one character, the Werewolf, that must lie in order              Neapolitan, R. E. (1990). Probabilistic reasoning in expert systems:
to play successfully. The first test showed that when a Were-                theory and algorithms. New York, NY, USA: John Wiley & Sons,
                                                                             Inc.
wolf was given a two-level rather than single-level ToM and               Paiva, A., Dias, J., Sobral, D., Aylett, R., Woods, S., Zoll, C., et
played against Villagers with a single-level ToM, the Were-                  al. (2004, July). Caring for agents and agents that care: Building
wolf’s game performance improved. The user testing with 60                   empathic relations with synthetic agents. In Aamas’2004. ACM
                                                                             Press.
subjects showed that participants clearly perceived the ToM2              Pearl, J. (1997). Probabilistic reasoning in intelligent systems :
version Werewolf as better at deceiving the other agents, and,               networks of plausible inference. Morgan Kaufmann. Paperback.
furthermore, saw this as more intelligent behaviour. These re-            Premack, D., & Woodruff, G. (1978). Does the chimpanzee have a
                                                                             theory of mind? Behavioral and Brain Sciences, 1(04), 515–526.
sults support the hypothesis that an agent with a single-level            Prendinger, H., & Ishizuka, M. (2001). Social role awareness in
ToM will be less successful in believable deception then an                  animated agents. In Autonomous agents (pp. 270–277).
agent with a two-level ToM.                                               Pynadath, D., & Marsella, S. (2005). Psychsim: Modelling theory
                                                                             of mind with decision-theoretic agents. In Ijcai (p. 1181-1186).
   As future work, it would be interesting to compare differ-             Rao, A. S., & Georgeff, M. P. (1995). Bdi agents: From theory to
ent combinations of the scenarios (e.g. one-level werewolf                   practice. In Proc 1st int. conf. multiagent systems. San Francisco.
against two-level victims), and to include the simulation of              Rosis, F., Pelachaud, C., Poggi, I., Carofiglio, V., & Carolis, B.
                                                                             (2003). From Greta’s mind to her face: modelling the dynam-
other’s planning processes in order to make it posible to rea-               ics of affective states in a conversational embodied agent. Int. J.
son about other agent’s goals and plans. Another interesting                 Human-Computer Studies, 59(1), 81–118.
extension to this work would be to apply the model to a dif-              Searle, J. (1976). A classification of illocutionary acts. Language in
                                                                             Society, 5, 1–23.
ferent type of deception than verbal lies, for example to de-             Swartout, W., Gratch, J., Hill Jr, R., Hovy, E., Marsella, S., Rickel,
ceptive display of affective states (Rosis et al., 2003).                    J., et al. (2006). Toward virtual humans. AI Magazine, 27(2), 96.
                                                                          Wagner, A., & Arkin, R. (2010). Acting Deceptively: Providing
                     Acknowledgments                                         Robots with the Capacity forDeception. Int. J. Social Robotics,
                                                                             1-22–22.
This work was partially supported by the European Community               Wyer, B. D. D. K. S. K. M., & Epstein, J. (1996). Lying in everyday
(EC), through the ECUTE project (ICT-5-4.2 257666), and by na-               life. Journal of Personality and Social Psychology, 70, 979-995.
tional funds through Fundação para a Ciência e a Tecnologia (FCT),
                                                                      2194

