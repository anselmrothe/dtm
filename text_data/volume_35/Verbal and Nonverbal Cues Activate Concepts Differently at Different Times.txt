UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Verbal and Nonverbal Cues Activate Concepts Differently, at Different Times

Permalink
https://escholarship.org/uc/item/7n46p97c

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Edminston, Pierce
Lupyan, Gary

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Verbal and Nonverbal Cues Activate Concepts Differently, at Different Times
Pierce Edmiston (pedmiston@wisc.edu)
Department of Psychology, 1202 W. Johnson Street
Madison, WI 53706 USA

Gary Lupyan (lupyan@wisc.edu)
Department of Psychology, 1202 W. Johnson Street
Madison, WI 53706 USA
Abstract
Although the word “dog” and an unambiguous barking sound
may point to the same concept DOG, verbal labels and
nonverbal cues appear to activate conceptual information in
systematically different ways (Lupyan & Thompson-Schill,
2012). Here we investigate these differences in more detail.
We replicate the finding that labels activate a more
prototypical representation than do sounds, and find that
sounds activate exemplars consistent with the source of the
sound, such that after hearing a barking sound, people are
faster to recognize a dog with an open-mouth than a closed
mouth, but critically, only when the sound and picture are
presented simultaneously. The results are consistent with
perceptual cues indexing their source while labels activating a
more decontextualized representation of the target category.
Keywords: categorization, concepts, sounds, recognition,
cross-modal effects, language

Introduction
Most concepts are multimodal and can be activated in a
variety of ways (Hoffman & Ralph, 2013). For example, the
concept DOG can be activated by seeing a wagging tail,
hearing a bark, or petting its furry coat. However, the
concept DOG can also be activated by hearing the word
‘dog’—without seeing, hearing, or touching an actual dog.
This raises the question of how concepts activated by
nonverbal sensory cues compare to those activated by verbal
category labels.
In the experiments reported here we compare how verbal
and nonverbal cues activate representations of purportedly
the same concepts. In particular, we focus on visual aspects
of familiar animals and artifacts as cued by natural sounds:
auditory events with a distinct source (e.g., cat meowing,
chainsaw revving), and how these same concepts are
activated by verbal labels (words like “cat” and
“chainsaw”).
The mechanisms underlying recognition of nonverbal
sounds and of speech appear to be quite similar.
Recognition of both words and natural sounds varies as a
function of familiarity, frequency, and context (Ballas,
1993; Stuart & Jones, 1995). Perception of both natural
sounds and speech is influenced by signal ambiguity and
noise in similar ways (Aramaki, Marie, Kronland-Martinet,
Ystad, & Besson, 2010; Gygi, Kidd, & Watson, 2004). Both
labels and natural sounds elicit similar N400 event-related
potentials—a coarse index of semantic processing
(Cummings et al., 2006; Van Petten & Rheinfelder, 1995)—

even when the identification of the natural sound is
incidental to task demands (Orgs, Lange, Dombrowski, &
Heil, 2008). Functional imaging during similar sequential
processing tasks reveals largely overlapping cortical areas
recruited in processing labels and natural sounds (Dick et
al., 2007). Lastly, patterns of naming deficits in patients
with aphasia suggest the labeling of everyday objects and
the visual recognition of natural sound sources rely on
similar cognitive resources (Goll et al., 2010; Saygin, Dick,
Wilson, Dronkers, & Bates, 2003).
The perception of meaningful nonverbal sounds and of
words is thus dependent on many of the same properties and
activate largely the same semantic networks. Although it
may seem that verbal and nonverbal cues are in important
respects equivalent, there are critical differences. One such
difference is that natural sounds, unlike labels, have a causal
relationship with a specific physical source (Ballas, 1993).
Recognizing these relationships requires learning, but the
relationship between a referent and its natural sound is not
arbitrary. We call these relationships “motivated”: that is,
they are determined by physics (e.g., thunder) or driven by
biology (e.g., large dogs—and agitated dogs—have deeper
barks). Auditory perceivers are able to exploit such
“motivated” relationships and surmise features of a hidden
physical source, such as the size of a barking dog (Taylor,
Reby, & McComb, 2008), the shape of resonating plates
(Kunkler-Peck & Turvey, 2000), or the hardness of
percussion mallets (Freed, 1990). The perception of these
auditory sources is surprisingly accurate, reflecting the
lawful relationships between signals and sources in the
environment (Fowler, 1990). Importantly, sounds covary
lawfully within as well as between categories. For example,
a barking sound informs us not only that its source is a dog,
but can inform us of the approximate size of the dog.
In contrast, the relationship between labels and their
referents is “unmotivated.” By this term we do not simply
mean that words are arbitrary, i.e., that “dog” refers to dogs
by convention (cf. Hockett, 1966), but that there exists a
word “dog” that denotes the entire category of dogs rather
than a particular type or instance (dachshund, German
shepherd, dog-on the left, dog-far away, etc.). In short, barks
index specific occurrences of dogs. Even though we can
interpret natural sounds at a more categorical level, the
surface properties of a specific bark still indexes a
particular dog. Verbal labels, on the other hand, abstract
over these specifics. When we say “dog” we can leave all

2243

that information unspecified. On this view, labels may
activate concepts in a more categorical way. This prediction
has been supported by a variety of findings (Lupyan, 2012).
For example, Lupyan & Thompson-Schill (2012) found that
label cues resulted in faster visual processing over equally
predictive nonverbal cues. This advantage persisted across a
number of cue-to-image delay periods and extended to
artificially created objects with novel labels and “natural”
sounds, suggesting that labels do not activate conceptual
representations faster but differently than nonverbal cues. In
our view, labels activate representations that emphasize the
differences between categories, and thus play a facilitative
role in category learning (Lupyan, Rakison, & McClelland,
2007). These categorical representations enable faster
recognition of category-typical objects (Lupyan &
Swingley, 2012), but blur within-category differences
reflected in biased exemplar memory (Lupyan, 2008).
However, what is not clear from these previous results is
how “unmotivated” and “motivated” cues differ in
activating different instances of purportedly the same
concept. If “unmotivated” verbal cues activate more
categorical representations, then what do “motivated”
nonverbal cues activate? Given the inherent causal link
between a natural sound and its particular physical source,
we predicted that natural sound cues would lead to faster
processing of images depicting the production of the
auditory cue. The results ended up being more interesting.

Category Typical

Sound Match

Figure 1: Sample stimuli from Experiment 1. Does
hearing the sound of a revving chainsaw activate a
representation of a chainsaw in action?

Experiment 1
Hearing a sound characteristic of an animal or artifact
may automatically activate particular instances of that
category. Consider the kind of chainsaw one might expect
upon hearing a chainsaw sound (Fig. 1). Here, we asked
whether verbal and nonverbal cues lead to different
expectations about subsequent visual information. In
Experiment 1 we investigated if label and natural sound
cues influence visual processing differently based on the
action depicted in target images. In line with previous
research, we predicted that when presented a label cue,
participants would respond faster to category-typical
images. Conversely, we predicted that when presented a
natural sound cue, participants would respond faster to
sound-matched images.

Methods
Participants 14 University of Wisconsin—Madison
undergraduates participated for course credit.
Materials Auditory cues were spoken labels and natural
sounds for 12 target categories of familiar animals and
artifacts used in Lupyan & Thompson-Schill (2012).1 Visual
images were 4 color photographs for each category: 2
category-typical images and 2 sound-producing images. The
images were normed, ensuring unambiguous identification.
In addition, participants in a separate image rating study
evaluated each picture on one of two dimensions (category
typicality and sound match) using a 5-point Likert scale. For
category typicality, participants viewed e.g., a dog, and were
asked: “How typical is this dog of dogs in general?” For
sound match ratings, participants listened to e.g., a bird
chirping, saw a picture of a bird, and were asked: “How well
does that sound go with this picture?” Each participant
performed either category-typicality or sound-matching
judgments. As expected, the canonical images were rated
higher on category typicality (M=4.57) than on sound match
(M=3.49), while sound-producing images were rated higher
on sound match (M=4.37) than on category typicality
(M=4.05). These ratings were standardized (z-score) and
used as predictors in subsequent analyses.
Procedure Participants completed a category verification
task in which an auditory cue—either a spoken category
label (e.g., ‘cat’) or a natural sound (e.g., <meow>)—
preceded a visual image. Participants determined if each
cue-image pair matched on a category level by pressing
‘Yes’ or ‘No’ using a labeled gaming controller. For
example, if they heard a chainsaw revving or the spoken
word “chainsaw” and then saw a picture of a chainsaw, they
would press the ‘Yes’ button. The picture disappeared after
each response, and performance feedback was given. Cue
type (label, natural sound) and picture exemplar (4 per
category) varied randomly within-subjects. There were a
total of 576 trials per subject (50% cue-image category
match). Each trial began with a 250 msec fixation cross
followed by the auditory cue. The target image appeared 1
sec after auditory cue offset. This long delay ensured that
participants had ample time to process sounds and labels
(see Lupyan & Thompson-Schill, 2012). The experiment
took 30 minutes to complete.

Results and Discussion2
Overall accuracy was high (96%). Only correct response
times (RTs) on matching trials were included. RTs less than
250 msec or greater than 1500 msec were excluded (<4% of
correct trials). We fit the data with linear mixed regression
(Bates, Maechler, & Bolker, 2012) to predict response times
(RTs) from the interaction between cue type (label, natural
sound) and image rating (category-typicality or sound1
Target categories for Experiment 1: bird, bee, toilet, scissors,
dog, chainsaw, bowling ball, cat, car, keyboard, river, baby.
2
Portions of Experiment 1 were presented at the Vision
Sciences Society Meeting, May 2011.

2244

Experiment 2
Our second experiment extends the first in two important
ways. First, we compiled a more extensive set of stimuli by
sampling from the 2-dimensional space of category typical
and sound-matched category exemplars (Fig. 3). Second, we
varied the cue-to-image delay. We did this because natural
sounds, unlike labels, index the animals and objects that
produce them. While labels often occur in the absence of the
referent (we talk about things not presently in view), sounds
are temporally contingent on the presence of the referent. If
we hear a bark, chances are a dog is in the vicinity.
In Experiment 2, we investigated if label and natural
sound cues influence recognition speed based on the fit
between an auditory cue and an image, and on the delay
between the cue and the image. In line with the results of
Experiment 1, we predicted a label cue would improve
processing of category-typical images. We also predicted
that a natural sound would improve processing of a fuller set
of sound-matched images—that is, where the image
depicted an animal or object that was the likely source of the
natural sound—and that this effect would be greater when
the cue and image were temporally coupled—that is,
presented simultaneously.

Figure 2: Significant interaction between cue type and
category typicality, but not between cue type and sound
match when the target picture lagged auditory cue
offset by 1000 msec. Confidence bands denote ±1
standard error of linear mixed regression point
estimates (Mazerolle, 2012). Error bars denote ±1
standard error of main effect of cue type.

Methods

match) with random subject and item effects (target
category). As expected (Lupyan & Thompson-Schill, 2012),
responses to label cues (M=609 msec) were reliably faster
than responses to natural sound cues (M=639 msec),
F(1,13)=22.03, p<0.001.3 The effect of cue type was
moderated by category-typicality, F(1,13)=10.45, p=0.002
(Fig. 2, left), but not by sound-match, F(1,13)=0.001,
p=0.98 (Fig. 2, right).
To summarize, labels, but not natural sounds, resulted in
faster processing of category-typical images, but neither cue
resulted in faster processing of sound-matched images.
These results replicate previous findings that labels facilitate
visual processing more effectively than nonverbal cues
(Lupyan & Thompson-Schill, 2012) and that labels improve
recognition of category-typical exemplars (Lupyan &
Swingley, 2012). The results clearly show that labels and
natural sounds activate familiar concepts differently and that
labels appear to activate a representation that is more
categorical/typical. Unexpectedly, natural sounds did not
selectively facilitate recognition of pictures that were better
matches to the sound-cues. This finding is investigated
further in Experiment 2.

Participants 56 University of Wisconsin—Madison
undergraduates participated for course credit.
Materials Auditory cues comprised spoken labels and
natural sounds for 10 of the 12 target categories used in
Experiment 1 (categories river and toilet were excluded; all
sounds edited to 600 msec). Image ratings (categorytypicality and sound-match) for an augmented set of images
were collected via Amazon’s Mechanical Turk (mTurk).
mTurk workers (N=42) heard either 10 spoken labels or 10
natural sounds to be used in Experiment 2, and were

Figure 3: Sample stimuli from Experiment 2. Categorytypicality was measured independently of sound-match.

3

All p-values were generated using Markov chain Monte
Carlo sampling (10,000 simulations).

2245

presented 8 to 10 pictures for each category with the
following instructions: “Please listen to the following audio
clip and report how well each image fits with the audio
file.” Ratings were given on a 5-point Likert scale. From
these data, we selected 4 images for each category
corresponding to the quadrants depicted in Fig. 3. There was
a positive correlation between category-typicality and
sound-match (Pearson’s r=0.27). These ratings were
standardized (z-score) and used as predictors in subsequent
analyses.
Procedure The procedure was the same as in Experiment 1.
Cue type (Label, Natural Sound), picture exemplar (4 per
category), and image delay (Simultaneous or Delayed 400
msec) varied randomly within-subject for a total of 427
trials per subject (75% cue-image category match4). Each
trial began with a 250 msec fixation cross. On a random half
of the trials, the auditory cue and picture were presented
simultaneously; on the remaining trials the picture was
presented 400 msec after the offset of the auditory cue. The
experiment took 30 minutes to complete.

predictor of RTs, F(1,41)=10.30, p=0.001. Importantly, this
effect remained constant across both cue types and both
image delays. That is, the RT advantage for more categorytypical images over less category-typical images was
equivalent for label and natural sound cues, on both
simultaneous and delayed trials (Fig. 4, left column).
Responses following natural sound cues were predicted by
category-typicality of the image during simultaneous and
400 msec delayed trials, an effect not found at the longer
delay in Experiment 1.
Sound Match We now report how image ratings of soundmatch influenced response times differently by cue type and

Results and Discussion
Overall accuracy was high (M=97%), except trials in
which pictures of scissors were cued by a sound of scissors
cutting paper (M=91%, SD=1.8). Participants also reported
difficulties with these trials during debriefing (24 out of 56
participants; next most frequent was 5 for bee), and these
trials were removed from subsequent analyses (<5%).5 We
excluded trials using the same exclusion criteria as in
Experiment 1 (<2% of correct trials removed). Again, we fit
the data with linear mixed regression to predict response
times from cue type (label, natural sound), delay
(simultaneous, delayed), and image rating (category
typicality or sound typicality) allowing random subject and
item effects (picture category).
Delay and Cue Type We first report how the effect of cue
type varied by image delay. As in Experiment 1, responses
to label cues were reliably faster than responses to natural
sound cues, F(1,41)=30.14, p<0.0001. The effect of cue
type was moderated by delay, F(1,41)=6.86, p=0.009. The
RT advantage of labels over natural sounds was greater on
simultaneous trials than it was on delayed trials (Fig. 4).
Category Typicality We next report how image ratings of
category typicality influenced RTs differently by cue type
and by image delay. Category typicality was a reliable
4

This increase in response validity compared to Exp. 1
allowed us to fully counterbalance all trial variables on matching
trials while keeping the length of the experiment manageable.
5
It is possible some of the natural sound cues were simply
harder for participants to identify. To ensure unambiguous
recognition of the remaining natural sounds used in Experiment
2, we enlisted 29 additional participants (mTurk) to report the
source of each auditory cue in a free response task. Participants
correctly identified the source of the natural sound 78% of the
time. There was no relationship between cue identification
(percentage correct by cue category) and response latencies on
the sound cued trials (Pearson r=-0.025).

2246

Figure 4: Label and natural sound auditory cues affect
response latencies differently by cue-image delay (rows)
and by image rating (columns). Confidence bands denote
±1 standard error of linear mixed regression point
estimates (Mazerolle, 2012). Error bars denote ±1
standard error of main effect of cue type.

by image delay. There was a reliable three-way interaction
between sound-match, cue type, and image delay, F(1,46) =
4.67, p=0.03. On simultaneous presentation trials, RTs
following natural sound cues decreased as the sound-match
of the image increased, while RTs following label cues did
not vary by sound-match, t(46)=-3.47, p<0.001, (Fig. 4,
upper right). However, there was no such cue type × soundmatch interaction at the 400 msec delay, t(46)=-0.44, p=0.66
(Fig. 4, lower right). That is, sound-match predicted RTs
following natural sounds and not labels when the delay was
simultaneous, but not with a 400 msec delay.
To summarize: the image ratings for category-typicality
and sound-match correlated with response times based on
the cue and the cue-image delay. First, when presented with
a spoken label, RTs were predicted by category-typicality of
the image, and this effect held across both cue-to-image
delay periods. Second, when presented with a natural sound,
the sound-match of the image correlated with the response
time to that image, but only when the cue-image pair was
presented simultaneously. That is, hearing a natural sound
improved processing of a particular kind of visual image: a
picture depicting an object that could have made the sound
at the moment the sound was detected. These results show
that the ways in which an auditory cue influences
recognition of visual images depends on both the fit of the
image to the auditory cue and the time course of the
presentation.

General Discussion
In two experiments we demonstrated that verbal and
nonverbal cues systematically differ in how they activate
conceptual information, as tested by the speed of visual
recognition of category exemplars. Experiment 1 revealed
more category-typical exemplars were recognized faster
following a spoken label cue but not a natural sound. In
addition, Experiment 1 revealed that exemplars that were
more sound producing were not recognized faster following
either auditory cue. Importantly, responses following natural
sound cues did not vary as a function of category-typicality
while those following labels did, suggesting that verbal and
nonverbal cues are indeed operating on different gradients.
Experiment 2 added to these results with a fuller stimulus
set and varying image delays. In Experiment 2, but not in
Experiment 1, responses following natural sounds did vary
with category-typicality. We believe this result to be due to
the shorter delays used in Experiment 2 (see Lupyan &
Thompson-Schill, 2012 for differences between labels and
natural sounds at longer delays). In Experiment 2, but not in
Experiment 1, responses to natural sounds varied as a
function of the match between the sound and image, but the
relationship was time sensitive. In particular, high soundmatched exemplars were recognized faster following a
natural sound only during simultaneous presentation, and
sound-match did not predict RTs following verbal cues.
Together, the two experiments reported here highlight the
role of multisensory integration as a feature of what we have
called “motivated” cues. We associate barking with dogs,

but the bark informs us about the particular dog that made
it—a deeper bark is likely to come from a larger dog, and
hearing a bark usually temporally coincides with seeing the
actual animal. Such contingencies result in audiovisual
integration of simultaneous auditory and visual cues that
improves detection (Laurienti, Kraft, Maldjian, Burdette, &
Wallace, 2004). For example, Chen & Spence (2011)
reported increased visual detection of masked pictures when
presented with a congruent natural sound cue, and that the
effectiveness of an auditory cue varied by cue-image delay.
The present results support the time sensitivity in cross
modal priming of natural sounds and pictures, and measure
the strength of this relationship through a “motivated”
sound-to-image match.
In contrast, word-to-referent mappings are “unmotivated”
(cf. Hockett, 1966). Saying “dog” in a deeper voice does not
systematically imply a larger or angrier dog.6 So, even
though both “dog” and a dog-bark may be unambiguously
associated with dogs, the dog-bark indexes a specific dog
with a specific size, location, and temperament. The word
“dog”, while varying systematically with aspects of the
speaker (e.g., the lower the pitch, the more likely the
speaker is to be male), does not systematically vary with the
referent. We can talk about particular dogs, of course, but
the word “dog” can and often does remain categorical,
abstract.
In addition, these findings establish a heretofore
underappreciated relationship between an auditory cue and a
sound-matched image in similar cognitive processing tasks.
Future attempts to compare semantic and conceptual
processing of labels to that of natural sounds may benefit
from operationalizing what we have termed the soundmatch between a natural sound and its purported referent
(e.g., Saygin, Dick, & Bates, 2005).
Conclusion We found verbal and nonverbal cues activate
different conceptual representations evident in patterns of
response latencies to recognize and verify different category
exemplars. In a replication of previous findings, verbal cues
facilitated recognition of category-typical images. We
extended these findings to discern the specifics of
conceptual representations activated via natural sound cues:
Natural sounds facilitated visual processing of images that
fit with the presented sound, but only if the sound and image
were presented simultaneously. Critically, these effects were
mediated by time, with natural sound cues improving
responses to sound-matched images only during
simultaneous presentation.

2247

6

There is intriguing evidence that sometimes, speakers do
modulate pronunciations of words in a graded fashion and that
listeners are sensitive to these modulations (Nuckolls, 1999;
Parise & Pavani, 2011), e.g., speaking faster or slower to
describe a faster or slower moving object (Shintel, Nusbaum, &
Okrent, 2006). Language can be easily stripped of these features
however (e.g., in written form) while still being perfectly
understandable.

References
Aramaki, M., Marie, C., Kronland-Martinet, R., Ystad, S.,
& Besson, M. (2010). Sound categorization and
conceptual priming for nonlinguistic and linguistic
sounds. Journal of Cognitive Neuroscience, 22(11),
2555–2569.
Ballas, J. A. (1993). Common factors in the identification of
an assortment of brief everyday sounds. Journal of
Experimental Psychology: Human Perception and
Performance, 19(2), 250.
Bates, D., Maechler, M., & Bolker, B. (2012). lme4: Linear
mixed-effects models using S4 classes. R package version
0.999999-0. http://CRAN.R-project.org/package=lme4.
Chen, Y. C., & Spence, C. (2010). When hearing the bark
helps to identify the dog: Semantically-congruent sounds
modulate the identification of masked pictures. Cognition,
114(3), 389–404.
Chen, Y. C., & Spence, C. (2011). Crossmodal semantic
priming by naturalistic sounds and spoken words
enhances visual sensitivity. JEP: Human Perception and
Performance, 37(5), 1554–1568.
Cummings, A., Čeponienė, R., Koyama, A., Saygin, A. P.,
Townsend, J., & Dick, F. (2006). Auditory semantic
networks for words and natural sounds. Brain Research,
1115(1), 92–107.
Dick, F., Saygin, A. P., Galati, G., Pitzalis, S., Bentrovato,
S., D'Amico, S., et al. (2007). What is involved and what
is necessary for complex linguistic and nonlinguistic
auditory processing: evidence from functional magnetic
resonance imaging and lesion data. Journal of Cognitive
Neuroscience, 19(5), 799–816.
Fowler, C. A. (1990). Sound‐producing sources as objects
of perception: Rate normalization and nonspeech
perception. The Journal of the Acoustical Society of
America, 88, 1236.
Freed, D. J. (1990). Auditory correlates of perceived mallet
hardness for a set of recorded percussive sound events.
The Journal of the Acoustical Society of America, 87, 311.
Goll, J. C., Crutch, S. J., Loo, J. H. Y., Rohrer, J. D., Frost,
C., Bamiou, D. E., & Warren, J. D. (2010). Non-verbal
sound processing in the primary progressive aphasias.
Brain, 133(1), 272–285.
Gygi, B., Kidd, G. R., & Watson, C. S. (2004). Spectraltemporal factors in the identification of environmental
sounds. The Journal of the Acoustical Society of America,
115(3), 1252.
Hockett, C. F. (1966). The problem of universals in
language. In J. H. Greenberg (Ed.), Universals of
Language (2nd. Ed.) (Vol. 2, pp. 1–29). Cambride, MA:
The MIT Press.
Hoffman, P., & Ralph, M. A. L. (2013). Shapes, scents and
sounds Quantifying the full multi-sensory basis of
conceptual knowledge. Neuropsychologia, 51(1), 14–25.
Kunkler-Peck, A. J., & Turvey, M. T. (2000). Hearing
shape. JEP: Human Perception and Perform, 26(1), 279.
Laurienti, P., Kraft, R., Maldjian, J., Burdette, J., &
Wallace, M. (2004). Semantic congruence is a critical

factor in multisensory behavioral performance.
Experimental Brain Research, 158(4).
Lupyan, G. (2008). From chair to “chair”: A
representational shift account of object labeling effects on
memory. Journal of Experimental Psychology: General,
137(2), 348–369. doi:10.1037/0096-3445.137.2.348
Lupyan, G. (2012). Linguistically modulated perception and
cognition: the label-feedback hypothesis, 1–13.
Lupyan, G., & Swingley, D. (2012). Self-directed speech
affects visual search performance. The Quarterly Journal
of Experimental Psychology, 65(6), 1068–1085.
Lupyan, G., & Thompson-Schill, S. L. (2012). The
evocative power of words: Activation of concepts by
verbal and nonverbal means. Journal of Experimental
Psychology: General, 141(1), 170–186.
Lupyan, G., Rakison, D. H., & McClelland, J. L. (2007).
Language is not just for talking. Psychological Science,
18(12), 1077–1083.
Mazerolle, M. J. (2012) AICcmodavg: Model selection and
multimodel inference based on (Q)AIC(c). R
package
version
1.26.
http://CRAN.Rproject.org/package=AICcmodavg.
Morey, R. D. (2008). Confidence intervals from normalized
data: A correction to Cousineau (2005). Tutorial in
Quantitative Methods for Psychology, 4(2), 61–64.
Orgs, G., Lange, K., Dombrowski, J.-H., & Heil, M. (2008).
N400-effects to task-irrelevant environmental sounds:
Further evidence for obligatory conceptual processing.
Neuroscience Letters, 436(2), 133–137.
Nuckolls, J. (1999). The Case for Sound Symbolism.
Annual Review of Anthropology, 28, 252, 225.
Parise, C., & Pavani, F. (2011). Evidence of sound
symbolism in simple vocalizations. Experimental Brain
Research, 214(3), 373–380.
Saygin, A. P., Dick, F., & Bates, E. (2005). An on-line task
for contrasting auditory processing in the verbal and
nonverbal domains and norms for younger and older
adults. Behavior Research Methods, 37(1), 99–110.
Saygin, A. P., Dick, F., Wilson, S. W., Dronkers, N. F., &
Bates, E. (2003). Neural resources for processing
language and environmental sounds Evidence from
aphasia. Brain, 126(4), 928–945.
Shintel, H., & Nusbaum, H. C. (2007). The sound of motion
in spoken language: Visual information conveyed by
acoustic properties of speech. Cognition, 105(3), 681–
690.
Stuart, G. P., & Jones, D. M. (1995). Priming the
identification of environmental sounds. The Quarterly
Journal of Experimental Psychology, 48(3), 741–761.
Taylor, A. M., Reby, D., & McComb, K. (2008). Human
listeners attend to size information in domestic dog
growls. The Journal of the Acoustical Society of America,
123(5), 2903.
Van Petten, C., & Rheinfelder, H. (1995). Conceptual
relationships between spoken words and environmental
sounds: Event-related brain potential measures.
Neuropsychologia, 33(4), 485–508.

2248

