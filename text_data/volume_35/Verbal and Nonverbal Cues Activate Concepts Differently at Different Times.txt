UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Verbal and Nonverbal Cues Activate Concepts Differently, at Different Times
Permalink
https://escholarship.org/uc/item/7n46p97c
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Edminston, Pierce
Lupyan, Gary
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

      Verbal and Nonverbal Cues Activate Concepts Differently, at Different Times
                                           Pierce Edmiston (pedmiston@wisc.edu)
                                        Department of Psychology, 1202 W. Johnson Street
                                                       Madison, WI 53706 USA
                                                Gary Lupyan (lupyan@wisc.edu)
                                        Department of Psychology, 1202 W. Johnson Street
                                                       Madison, WI 53706 USA
                            Abstract                                  even when the identification of the natural sound is
  Although the word “dog” and an unambiguous barking sound
                                                                      incidental to task demands (Orgs, Lange, Dombrowski, &
  may point to the same concept DOG, verbal labels and                Heil, 2008). Functional imaging during similar sequential
  nonverbal cues appear to activate conceptual information in         processing tasks reveals largely overlapping cortical areas
  systematically different ways (Lupyan & Thompson-Schill,            recruited in processing labels and natural sounds (Dick et
  2012). Here we investigate these differences in more detail.        al., 2007). Lastly, patterns of naming deficits in patients
  We replicate the finding that labels activate a more                with aphasia suggest the labeling of everyday objects and
  prototypical representation than do sounds, and find that           the visual recognition of natural sound sources rely on
  sounds activate exemplars consistent with the source of the
  sound, such that after hearing a barking sound, people are          similar cognitive resources (Goll et al., 2010; Saygin, Dick,
  faster to recognize a dog with an open-mouth than a closed          Wilson, Dronkers, & Bates, 2003).
  mouth, but critically, only when the sound and picture are             The perception of meaningful nonverbal sounds and of
  presented simultaneously. The results are consistent with           words is thus dependent on many of the same properties and
  perceptual cues indexing their source while labels activating a     activate largely the same semantic networks. Although it
  more decontextualized representation of the target category.        may seem that verbal and nonverbal cues are in important
  Keywords: categorization, concepts, sounds, recognition,            respects equivalent, there are critical differences. One such
  cross-modal effects, language                                       difference is that natural sounds, unlike labels, have a causal
                                                                      relationship with a specific physical source (Ballas, 1993).
                          Introduction                                Recognizing these relationships requires learning, but the
  Most concepts are multimodal and can be activated in a              relationship between a referent and its natural sound is not
variety of ways (Hoffman & Ralph, 2013). For example, the             arbitrary. We call these relationships “motivated”: that is,
concept DOG can be activated by seeing a wagging tail,                they are determined by physics (e.g., thunder) or driven by
hearing a bark, or petting its furry coat. However, the               biology (e.g., large dogs—and agitated dogs—have deeper
concept DOG can also be activated by hearing the word                 barks). Auditory perceivers are able to exploit such
‘dog’—without seeing, hearing, or touching an actual dog.             “motivated” relationships and surmise features of a hidden
This raises the question of how concepts activated by                 physical source, such as the size of a barking dog (Taylor,
nonverbal sensory cues compare to those activated by verbal           Reby, & McComb, 2008), the shape of resonating plates
category labels.                                                      (Kunkler-Peck & Turvey, 2000), or the hardness of
  In the experiments reported here we compare how verbal              percussion mallets (Freed, 1990). The perception of these
and nonverbal cues activate representations of purportedly            auditory sources is surprisingly accurate, reflecting the
the same concepts. In particular, we focus on visual aspects          lawful relationships between signals and sources in the
of familiar animals and artifacts as cued by natural sounds:          environment (Fowler, 1990). Importantly, sounds covary
auditory events with a distinct source (e.g., cat meowing,            lawfully within as well as between categories. For example,
chainsaw revving), and how these same concepts are                    a barking sound informs us not only that its source is a dog,
activated by verbal labels (words like “cat” and                      but can inform us of the approximate size of the dog.
“chainsaw”).                                                             In contrast, the relationship between labels and their
  The mechanisms underlying recognition of nonverbal                  referents is “unmotivated.” By this term we do not simply
sounds and of speech appear to be quite similar.                      mean that words are arbitrary, i.e., that “dog” refers to dogs
Recognition of both words and natural sounds varies as a              by convention (cf. Hockett, 1966), but that there exists a
function of familiarity, frequency, and context (Ballas,              word “dog” that denotes the entire category of dogs rather
1993; Stuart & Jones, 1995). Perception of both natural               than a particular type or instance (dachshund, German
sounds and speech is influenced by signal ambiguity and               shepherd, dog-on the left, dog-far away, etc.). In short, barks
noise in similar ways (Aramaki, Marie, Kronland-Martinet,             index specific occurrences of dogs. Even though we can
Ystad, & Besson, 2010; Gygi, Kidd, & Watson, 2004). Both              interpret natural sounds at a more categorical level, the
labels and natural sounds elicit similar N400 event-related           surface properties of a specific bark still indexes a
potentials—a coarse index of semantic processing                      particular dog. Verbal labels, on the other hand, abstract
(Cummings et al., 2006; Van Petten & Rheinfelder, 1995)—              over these specifics. When we say “dog” we can leave all
                                                                  2243

that information unspecified. On this view, labels may            Methods
activate concepts in a more categorical way. This prediction      Participants 14 University of Wisconsin—Madison
has been supported by a variety of findings (Lupyan, 2012).       undergraduates participated for course credit.
For example, Lupyan & Thompson-Schill (2012) found that           Materials Auditory cues were spoken labels and natural
label cues resulted in faster visual processing over equally      sounds for 12 target categories of familiar animals and
predictive nonverbal cues. This advantage persisted across a      artifacts used in Lupyan & Thompson-Schill (2012).1 Visual
number of cue-to-image delay periods and extended to              images were 4 color photographs for each category: 2
artificially created objects with novel labels and “natural”      category-typical images and 2 sound-producing images. The
sounds, suggesting that labels do not activate conceptual         images were normed, ensuring unambiguous identification.
representations faster but differently than nonverbal cues. In    In addition, participants in a separate image rating study
our view, labels activate representations that emphasize the      evaluated each picture on one of two dimensions (category
differences between categories, and thus play a facilitative      typicality and sound match) using a 5-point Likert scale. For
role in category learning (Lupyan, Rakison, & McClelland,         category typicality, participants viewed e.g., a dog, and were
2007). These categorical representations enable faster            asked: “How typical is this dog of dogs in general?” For
recognition of category-typical objects (Lupyan &                 sound match ratings, participants listened to e.g., a bird
Swingley, 2012), but blur within-category differences             chirping, saw a picture of a bird, and were asked: “How well
reflected in biased exemplar memory (Lupyan, 2008).               does that sound go with this picture?” Each participant
   However, what is not clear from these previous results is      performed either category-typicality or sound-matching
how “unmotivated” and “motivated” cues differ in                  judgments. As expected, the canonical images were rated
activating different instances of purportedly the same            higher on category typicality (M=4.57) than on sound match
concept. If “unmotivated” verbal cues activate more               (M=3.49), while sound-producing images were rated higher
categorical representations, then what do “motivated”             on sound match (M=4.37) than on category typicality
nonverbal cues activate? Given the inherent causal link           (M=4.05). These ratings were standardized (z-score) and
between a natural sound and its particular physical source,       used as predictors in subsequent analyses.
we predicted that natural sound cues would lead to faster         Procedure Participants completed a category verification
processing of images depicting the production of the              task in which an auditory cue—either a spoken category
auditory cue. The results ended up being more interesting.        label (e.g., ‘cat’) or a natural sound (e.g., <meow>)—
                                                                  preceded a visual image. Participants determined if each
                                                                  cue-image pair matched on a category level by pressing
                                                                  ‘Yes’ or ‘No’ using a labeled gaming controller. For
                                                                  example, if they heard a chainsaw revving or the spoken
                                                                  word “chainsaw” and then saw a picture of a chainsaw, they
                                                                  would press the ‘Yes’ button. The picture disappeared after
                                                                  each response, and performance feedback was given. Cue
        Category Typical                     Sound Match          type (label, natural sound) and picture exemplar (4 per
                                                                  category) varied randomly within-subjects. There were a
 Figure 1: Sample stimuli from Experiment 1. Does                 total of 576 trials per subject (50% cue-image category
 hearing the sound of a revving chainsaw activate a               match). Each trial began with a 250 msec fixation cross
 representation of a chainsaw in action?                          followed by the auditory cue. The target image appeared 1
                                                                  sec after auditory cue offset. This long delay ensured that
                                                                  participants had ample time to process sounds and labels
                        Experiment 1                              (see Lupyan & Thompson-Schill, 2012). The experiment
                                                                  took 30 minutes to complete.
   Hearing a sound characteristic of an animal or artifact
may automatically activate particular instances of that           Results and Discussion2
category. Consider the kind of chainsaw one might expect
upon hearing a chainsaw sound (Fig. 1). Here, we asked              Overall accuracy was high (96%). Only correct response
whether verbal and nonverbal cues lead to different               times (RTs) on matching trials were included. RTs less than
expectations about subsequent visual information. In              250 msec or greater than 1500 msec were excluded (<4% of
Experiment 1 we investigated if label and natural sound           correct trials). We fit the data with linear mixed regression
cues influence visual processing differently based on the         (Bates, Maechler, & Bolker, 2012) to predict response times
action depicted in target images. In line with previous           (RTs) from the interaction between cue type (label, natural
research, we predicted that when presented a label cue,           sound) and image rating (category-typicality or sound-
participants would respond faster to category-typical
images. Conversely, we predicted that when presented a              1
                                                                       Target categories for Experiment 1: bird, bee, toilet, scissors,
natural sound cue, participants would respond faster to           dog, chainsaw, bowling ball, cat, car, keyboard, river, baby.
sound-matched images.                                               2
                                                                        Portions of Experiment 1 were presented at the Vision
                                                                  Sciences Society Meeting, May 2011.
                                                              2244

                                                                                           Experiment 2
                                                                       Our second experiment extends the first in two important
                                                                    ways. First, we compiled a more extensive set of stimuli by
                                                                    sampling from the 2-dimensional space of category typical
                                                                    and sound-matched category exemplars (Fig. 3). Second, we
                                                                    varied the cue-to-image delay. We did this because natural
                                                                    sounds, unlike labels, index the animals and objects that
                                                                    produce them. While labels often occur in the absence of the
                                                                    referent (we talk about things not presently in view), sounds
                                                                    are temporally contingent on the presence of the referent. If
                                                                    we hear a bark, chances are a dog is in the vicinity.
                                                                       In Experiment 2, we investigated if label and natural
                                                                    sound cues influence recognition speed based on the fit
                                                                    between an auditory cue and an image, and on the delay
                                                                    between the cue and the image. In line with the results of
                                                                    Experiment 1, we predicted a label cue would improve
                                                                    processing of category-typical images. We also predicted
                                                                    that a natural sound would improve processing of a fuller set
                                                                    of sound-matched images—that is, where the image
                                                                    depicted an animal or object that was the likely source of the
                                                                    natural sound—and that this effect would be greater when
   Figure 2: Significant interaction between cue type and           the cue and image were temporally coupled—that is,
   category typicality, but not between cue type and sound          presented simultaneously.
   match when the target picture lagged auditory cue
   offset by 1000 msec. Confidence bands denote ±1                  Methods
   standard error of linear mixed regression point                  Participants 56 University of Wisconsin—Madison
   estimates (Mazerolle, 2012). Error bars denote ±1                undergraduates participated for course credit.
   standard error of main effect of cue type.                       Materials Auditory cues comprised spoken labels and
                                                                    natural sounds for 10 of the 12 target categories used in
                                                                    Experiment 1 (categories river and toilet were excluded; all
match) with random subject and item effects (target                 sounds edited to 600 msec). Image ratings (category-
category). As expected (Lupyan & Thompson-Schill, 2012),            typicality and sound-match) for an augmented set of images
responses to label cues (M=609 msec) were reliably faster           were collected via Amazon’s Mechanical Turk (mTurk).
than responses to natural sound cues (M=639 msec),                  mTurk workers (N=42) heard either 10 spoken labels or 10
F(1,13)=22.03, p<0.001.3 The effect of cue type was                 natural sounds to be used in Experiment 2, and were
moderated by category-typicality, F(1,13)=10.45, p=0.002
(Fig. 2, left), but not by sound-match, F(1,13)=0.001,
p=0.98 (Fig. 2, right).
  To summarize, labels, but not natural sounds, resulted in
faster processing of category-typical images, but neither cue
resulted in faster processing of sound-matched images.
These results replicate previous findings that labels facilitate
visual processing more effectively than nonverbal cues
(Lupyan & Thompson-Schill, 2012) and that labels improve
recognition of category-typical exemplars (Lupyan &
Swingley, 2012). The results clearly show that labels and
natural sounds activate familiar concepts differently and that
labels appear to activate a representation that is more
categorical/typical. Unexpectedly, natural sounds did not
selectively facilitate recognition of pictures that were better
matches to the sound-cues. This finding is investigated
further in Experiment 2.
     3
                                                                      Figure 3: Sample stimuli from Experiment 2. Category-
       All p-values were generated using Markov chain Monte           typicality was measured independently of sound-match.
  Carlo sampling (10,000 simulations).
                                                                2245

presented 8 to 10 pictures for each category with the                    predictor of RTs, F(1,41)=10.30, p=0.001. Importantly, this
following instructions: “Please listen to the following audio            effect remained constant across both cue types and both
clip and report how well each image fits with the audio                  image delays. That is, the RT advantage for more category-
file.” Ratings were given on a 5-point Likert scale. From                typical images over less category-typical images was
these data, we selected 4 images for each category                       equivalent for label and natural sound cues, on both
corresponding to the quadrants depicted in Fig. 3. There was             simultaneous and delayed trials (Fig. 4, left column).
a positive correlation between category-typicality and                   Responses following natural sound cues were predicted by
sound-match (Pearson’s r=0.27). These ratings were                       category-typicality of the image during simultaneous and
standardized (z-score) and used as predictors in subsequent              400 msec delayed trials, an effect not found at the longer
analyses.                                                                delay in Experiment 1.
Procedure The procedure was the same as in Experiment 1.                 Sound Match We now report how image ratings of sound-
Cue type (Label, Natural Sound), picture exemplar (4 per                 match influenced response times differently by cue type and
category), and image delay (Simultaneous or Delayed 400
msec) varied randomly within-subject for a total of 427
trials per subject (75% cue-image category match4). Each
trial began with a 250 msec fixation cross. On a random half
of the trials, the auditory cue and picture were presented
simultaneously; on the remaining trials the picture was
presented 400 msec after the offset of the auditory cue. The
experiment took 30 minutes to complete.
Results and Discussion
   Overall accuracy was high (M=97%), except trials in
which pictures of scissors were cued by a sound of scissors
cutting paper (M=91%, SD=1.8). Participants also reported
difficulties with these trials during debriefing (24 out of 56
participants; next most frequent was 5 for bee), and these
trials were removed from subsequent analyses (<5%).5 We
excluded trials using the same exclusion criteria as in
Experiment 1 (<2% of correct trials removed). Again, we fit
the data with linear mixed regression to predict response
times from cue type (label, natural sound), delay
(simultaneous, delayed), and image rating (category
typicality or sound typicality) allowing random subject and
item effects (picture category).
Delay and Cue Type We first report how the effect of cue
type varied by image delay. As in Experiment 1, responses
to label cues were reliably faster than responses to natural
sound cues, F(1,41)=30.14, p<0.0001. The effect of cue
type was moderated by delay, F(1,41)=6.86, p=0.009. The
RT advantage of labels over natural sounds was greater on
simultaneous trials than it was on delayed trials (Fig. 4).
Category Typicality We next report how image ratings of
category typicality influenced RTs differently by cue type
and by image delay. Category typicality was a reliable
      4
         This increase in response validity compared to Exp. 1
   allowed us to fully counterbalance all trial variables on matching
   trials while keeping the length of the experiment manageable.
      5
        It is possible some of the natural sound cues were simply
   harder for participants to identify. To ensure unambiguous             Figure 4: Label and natural sound auditory cues affect
   recognition of the remaining natural sounds used in Experiment         response latencies differently by cue-image delay (rows)
   2, we enlisted 29 additional participants (mTurk) to report the        and by image rating (columns). Confidence bands denote
   source of each auditory cue in a free response task. Participants      ±1 standard error of linear mixed regression point
   correctly identified the source of the natural sound 78% of the        estimates (Mazerolle, 2012). Error bars denote ±1
   time. There was no relationship between cue identification
   (percentage correct by cue category) and response latencies on
                                                                          standard error of main effect of cue type.
   the sound cued trials (Pearson r=-0.025).
                                                                     2246

by image delay. There was a reliable three-way interaction        but the bark informs us about the particular dog that made
between sound-match, cue type, and image delay, F(1,46) =         it—a deeper bark is likely to come from a larger dog, and
4.67, p=0.03. On simultaneous presentation trials, RTs            hearing a bark usually temporally coincides with seeing the
following natural sound cues decreased as the sound-match         actual animal. Such contingencies result in audiovisual
of the image increased, while RTs following label cues did        integration of simultaneous auditory and visual cues that
not vary by sound-match, t(46)=-3.47, p<0.001, (Fig. 4,           improves detection (Laurienti, Kraft, Maldjian, Burdette, &
upper right). However, there was no such cue type × sound-        Wallace, 2004). For example, Chen & Spence (2011)
match interaction at the 400 msec delay, t(46)=-0.44, p=0.66      reported increased visual detection of masked pictures when
(Fig. 4, lower right). That is, sound-match predicted RTs         presented with a congruent natural sound cue, and that the
following natural sounds and not labels when the delay was        effectiveness of an auditory cue varied by cue-image delay.
simultaneous, but not with a 400 msec delay.                      The present results support the time sensitivity in cross
   To summarize: the image ratings for category-typicality        modal priming of natural sounds and pictures, and measure
and sound-match correlated with response times based on           the strength of this relationship through a “motivated”
the cue and the cue-image delay. First, when presented with       sound-to-image match.
a spoken label, RTs were predicted by category-typicality of         In contrast, word-to-referent mappings are “unmotivated”
the image, and this effect held across both cue-to-image          (cf. Hockett, 1966). Saying “dog” in a deeper voice does not
delay periods. Second, when presented with a natural sound,       systematically imply a larger or angrier dog.6 So, even
the sound-match of the image correlated with the response         though both “dog” and a dog-bark may be unambiguously
time to that image, but only when the cue-image pair was          associated with dogs, the dog-bark indexes a specific dog
presented simultaneously. That is, hearing a natural sound        with a specific size, location, and temperament. The word
improved processing of a particular kind of visual image: a       “dog”, while varying systematically with aspects of the
picture depicting an object that could have made the sound        speaker (e.g., the lower the pitch, the more likely the
at the moment the sound was detected. These results show          speaker is to be male), does not systematically vary with the
that the ways in which an auditory cue influences                 referent. We can talk about particular dogs, of course, but
recognition of visual images depends on both the fit of the       the word “dog” can and often does remain categorical,
image to the auditory cue and the time course of the              abstract.
presentation.                                                        In addition, these findings establish a heretofore
                                                                  underappreciated relationship between an auditory cue and a
                    General Discussion                            sound-matched image in similar cognitive processing tasks.
   In two experiments we demonstrated that verbal and             Future attempts to compare semantic and conceptual
nonverbal cues systematically differ in how they activate         processing of labels to that of natural sounds may benefit
conceptual information, as tested by the speed of visual          from operationalizing what we have termed the sound-
recognition of category exemplars. Experiment 1 revealed          match between a natural sound and its purported referent
more category-typical exemplars were recognized faster            (e.g., Saygin, Dick, & Bates, 2005).
following a spoken label cue but not a natural sound. In          Conclusion We found verbal and nonverbal cues activate
addition, Experiment 1 revealed that exemplars that were          different conceptual representations evident in patterns of
more sound producing were not recognized faster following         response latencies to recognize and verify different category
either auditory cue. Importantly, responses following natural     exemplars. In a replication of previous findings, verbal cues
sound cues did not vary as a function of category-typicality      facilitated recognition of category-typical images. We
while those following labels did, suggesting that verbal and      extended these findings to discern the specifics of
nonverbal cues are indeed operating on different gradients.       conceptual representations activated via natural sound cues:
Experiment 2 added to these results with a fuller stimulus        Natural sounds facilitated visual processing of images that
set and varying image delays. In Experiment 2, but not in         fit with the presented sound, but only if the sound and image
Experiment 1, responses following natural sounds did vary         were presented simultaneously. Critically, these effects were
with category-typicality. We believe this result to be due to     mediated by time, with natural sound cues improving
the shorter delays used in Experiment 2 (see Lupyan &             responses to sound-matched images only during
Thompson-Schill, 2012 for differences between labels and          simultaneous presentation.
natural sounds at longer delays). In Experiment 2, but not in
Experiment 1, responses to natural sounds varied as a
function of the match between the sound and image, but the
relationship was time sensitive. In particular, high sound-             6
                                                                          There is intriguing evidence that sometimes, speakers do
matched exemplars were recognized faster following a                 modulate pronunciations of words in a graded fashion and that
natural sound only during simultaneous presentation, and             listeners are sensitive to these modulations (Nuckolls, 1999;
sound-match did not predict RTs following verbal cues.               Parise & Pavani, 2011), e.g., speaking faster or slower to
   Together, the two experiments reported here highlight the         describe a faster or slower moving object (Shintel, Nusbaum, &
role of multisensory integration as a feature of what we have        Okrent, 2006). Language can be easily stripped of these features
called “motivated” cues. We associate barking with dogs,             however (e.g., in written form) while still being perfectly
                                                                     understandable.
                                                              2247

                          References                                   factor in multisensory behavioral performance.
                                                                       Experimental Brain Research, 158(4).
Aramaki, M., Marie, C., Kronland-Martinet, R., Ystad, S.,
                                                                     Lupyan, G. (2008). From chair to “chair”: A
  & Besson, M. (2010). Sound categorization and
                                                                       representational shift account of object labeling effects on
  conceptual priming for nonlinguistic and linguistic
                                                                       memory. Journal of Experimental Psychology: General,
  sounds. Journal of Cognitive Neuroscience, 22(11),
                                                                       137(2), 348–369. doi:10.1037/0096-3445.137.2.348
  2555–2569.
                                                                     Lupyan, G. (2012). Linguistically modulated perception and
Ballas, J. A. (1993). Common factors in the identification of
                                                                       cognition: the label-feedback hypothesis, 1–13.
  an assortment of brief everyday sounds. Journal of
                                                                     Lupyan, G., & Swingley, D. (2012). Self-directed speech
  Experimental Psychology: Human Perception and
                                                                       affects visual search performance. The Quarterly Journal
  Performance, 19(2), 250.
                                                                       of Experimental Psychology, 65(6), 1068–1085.
Bates, D., Maechler, M., & Bolker, B. (2012). lme4: Linear
                                                                     Lupyan, G., & Thompson-Schill, S. L. (2012). The
  mixed-effects models using S4 classes. R package version
                                                                       evocative power of words: Activation of concepts by
  0.999999-0. http://CRAN.R-project.org/package=lme4.
                                                                       verbal and nonverbal means. Journal of Experimental
Chen, Y. C., & Spence, C. (2010). When hearing the bark
                                                                       Psychology: General, 141(1), 170–186.
  helps to identify the dog: Semantically-congruent sounds
                                                                     Lupyan, G., Rakison, D. H., & McClelland, J. L. (2007).
  modulate the identification of masked pictures. Cognition,
                                                                       Language is not just for talking. Psychological Science,
  114(3), 389–404.
                                                                       18(12), 1077–1083.
Chen, Y. C., & Spence, C. (2011). Crossmodal semantic
                                                                     Mazerolle, M. J. (2012) AICcmodavg: Model selection and
  priming by naturalistic sounds and spoken words
                                                                       multimodel inference based on (Q)AIC(c). R
  enhances visual sensitivity. JEP: Human Perception and
                                                                       package         version        1.26.        http://CRAN.R-
  Performance, 37(5), 1554–1568.
                                                                       project.org/package=AICcmodavg.
Cummings, A., Čeponienė, R., Koyama, A., Saygin, A. P.,
                                                                     Morey, R. D. (2008). Confidence intervals from normalized
  Townsend, J., & Dick, F. (2006). Auditory semantic
                                                                       data: A correction to Cousineau (2005). Tutorial in
  networks for words and natural sounds. Brain Research,
                                                                       Quantitative Methods for Psychology, 4(2), 61–64.
  1115(1), 92–107.
                                                                     Orgs, G., Lange, K., Dombrowski, J.-H., & Heil, M. (2008).
Dick, F., Saygin, A. P., Galati, G., Pitzalis, S., Bentrovato,
                                                                       N400-effects to task-irrelevant environmental sounds:
  S., D'Amico, S., et al. (2007). What is involved and what
                                                                       Further evidence for obligatory conceptual processing.
  is necessary for complex linguistic and nonlinguistic
                                                                       Neuroscience Letters, 436(2), 133–137.
  auditory processing: evidence from functional magnetic
                                                                     Nuckolls, J. (1999). The Case for Sound Symbolism.
  resonance imaging and lesion data. Journal of Cognitive
                                                                       Annual Review of Anthropology, 28, 252, 225.
  Neuroscience, 19(5), 799–816.
                                                                     Parise, C., & Pavani, F. (2011). Evidence of sound
Fowler, C. A. (1990). Sound‐producing sources as objects
                                                                       symbolism in simple vocalizations. Experimental Brain
  of perception: Rate normalization and nonspeech
                                                                       Research, 214(3), 373–380.
  perception. The Journal of the Acoustical Society of
                                                                     Saygin, A. P., Dick, F., & Bates, E. (2005). An on-line task
  America, 88, 1236.
                                                                       for contrasting auditory processing in the verbal and
Freed, D. J. (1990). Auditory correlates of perceived mallet
                                                                       nonverbal domains and norms for younger and older
  hardness for a set of recorded percussive sound events.
                                                                       adults. Behavior Research Methods, 37(1), 99–110.
  The Journal of the Acoustical Society of America, 87, 311.
                                                                     Saygin, A. P., Dick, F., Wilson, S. W., Dronkers, N. F., &
Goll, J. C., Crutch, S. J., Loo, J. H. Y., Rohrer, J. D., Frost,
                                                                       Bates, E. (2003). Neural resources for processing
  C., Bamiou, D. E., & Warren, J. D. (2010). Non-verbal
                                                                       language and environmental sounds Evidence from
  sound processing in the primary progressive aphasias.
                                                                       aphasia. Brain, 126(4), 928–945.
  Brain, 133(1), 272–285.
                                                                     Shintel, H., & Nusbaum, H. C. (2007). The sound of motion
Gygi, B., Kidd, G. R., & Watson, C. S. (2004). Spectral-
                                                                       in spoken language: Visual information conveyed by
  temporal factors in the identification of environmental
                                                                       acoustic properties of speech. Cognition, 105(3), 681–
  sounds. The Journal of the Acoustical Society of America,
                                                                       690.
  115(3), 1252.
                                                                     Stuart, G. P., & Jones, D. M. (1995). Priming the
Hockett, C. F. (1966). The problem of universals in
                                                                       identification of environmental sounds. The Quarterly
  language. In J. H. Greenberg (Ed.), Universals of
                                                                       Journal of Experimental Psychology, 48(3), 741–761.
  Language (2nd. Ed.) (Vol. 2, pp. 1–29). Cambride, MA:
                                                                     Taylor, A. M., Reby, D., & McComb, K. (2008). Human
  The MIT Press.
                                                                       listeners attend to size information in domestic dog
Hoffman, P., & Ralph, M. A. L. (2013). Shapes, scents and
                                                                       growls. The Journal of the Acoustical Society of America,
  sounds Quantifying the full multi-sensory basis of
                                                                       123(5), 2903.
  conceptual knowledge. Neuropsychologia, 51(1), 14–25.
                                                                     Van Petten, C., & Rheinfelder, H. (1995). Conceptual
Kunkler-Peck, A. J., & Turvey, M. T. (2000). Hearing
                                                                       relationships between spoken words and environmental
  shape. JEP: Human Perception and Perform, 26(1), 279.
                                                                       sounds: Event-related brain potential measures.
Laurienti, P., Kraft, R., Maldjian, J., Burdette, J., &
                                                                       Neuropsychologia, 33(4), 485–508.
  Wallace, M. (2004). Semantic congruence is a critical
                                                                 2248

