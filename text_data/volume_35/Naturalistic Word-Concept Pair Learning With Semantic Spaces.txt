UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Naturalistic Word-Concept Pair Learning With Semantic Spaces
Permalink
https://escholarship.org/uc/item/0v91c41n
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Kievit-Kylar, Brent
Kachergis, George
Jones, Michael
Publication Date
2013-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                Naturalistic Word-Concept Pair Learning With Semantic Spaces
                             Brent Kievit-Kylar1, George Kachergis3, and Michael N. Jones1,2
                                          {bkievitk, gkacherg, jonesmn}@indiana.edu
                                                       1
                                                         Cognitive Science Program
                                            2
                                              Department of Psychological & Brain Sciences
                                                      Bloomington, IN 47405 USA
                                               3
                                                Psychology Department, Leiden University
                                                       Leiden 2311 EZ Netherlands
                              Abstract                                  space approach can take advantage of the fact that similar
                                                                        words carry mutually reinforcing information about each
   We describe a model designed to learn word-concept pairings
   using a combination of semantic space models. We compare             other’s object referents. In addition, the similarity between a
   various semantic space models to each other as well as to            noun and semantically related verb or adjective contains
   extant word-learning models in the literature and find that not      information about the noun’s referent. Bounce may often be
   only do semantic space models require fewer underlying               used when a ball is present in the environment, even in
   assumptions, they perform at least on par with existing              absence of the noun ball. The semantic similarity between
   associative models. We also demonstrate that semantic space          the words bounce and ball may be used as an indirect cue to
   models correctly predict different word-concept pairings from
   existing models and can be combined with existing models to
                                                                        the mapping between the noun and object.
   perform better than either model can individually.                      We next provide background on the problem, data, and
                                                                        existing models of word-concept learning. Then we turn to a
   Keywords: Childes; natural language processing; semantic             summary of a variety of semantic space models used, and a
   space models; associative learning.
                                                                        general purpose technique for creating word-concept
                                                                        learners from semantic models adapted from Kintsch’s
                          Introduction                                  (2001) algorithm. Finally, we test these models on a labeled
While the task of word-concept matching may seem trivial                fragment of the CHILDES corpus and explore the benefits
to an adult, imagine the task from the perspective of a young           of combining different semantic models into hybrids.
child. A child hears a series of vocalizations, which are then
parsed into word units, and must perceive instances of                                   Child Learning Models
objects in their environment through visual inspection. From
                                                                        While there are a number of existing word-concept mapping
that information the child must determine a set of objects or
                                                                        models from the child learning literature, we will focus on
concepts that are present. These two tasks are challenging
                                                                        two recent models that have both been applied to the object-
enough, but the child must then find a way to correlate the
                                                                        tagged corpus data that we use (described below).
words that he/she has heard with the objects in the
                                                                          The data used for training in these simulations are from an
immediate environment. Extracting the correct mappings
                                                                        annotated version of the Rollins section of the CHILDES
from the myriad possible ones is complicated by things such
                                                                        corpus (MacWhinney, 2000) used by Frank et al (2007).
as the potential absence of matches between object and
                                                                        The entire corpus takes place over approximately ten
word (e.g., an object is mentioned that is not present), and
                                                                        minutes of talk taken from a caregiver to a child. Each
the fact that not all words refer to objects (e.g., verbs and
                                                                        sentence is annotated with the objects that are visible to the
function words).
                                                                        child when that sentence was being spoken. Thus the corpus
   Several techniques have been proposed to help simplify
                                                                        consists of entries in the form {W0, W1, …, Wn, C0, C1, …
the word-concept acquisition problem, the majority of
                                                                        Cm} where Wi is a word token and Ci is a concept token
which require the child to have prebuilt assumptions (e.g., a
                                                                        (each represented by a string). A unique identifier was used
novel word must map to a novel object), or to perform
                                                                        to differentiate concepts from words, as both are represented
complex Bayesian logic calculations (Frank et al. 2007). In
                                                                        in the dataset by similar strings of characters. In this paper,
this paper, we explore a new class of model that is based on
                                                                        we will use the angle brackets as delimiters, such that “dog”
the rapidly growing field of semantic space models. In
                                                                        represents the word dog, and “<dog>” represents the
particular, we generalize Kintsch’s (2001) predication
                                                                        concept or object dog.
algorithm to the problem of word-concept learning in
semantic spaces.
                                                                        Frank et al.’s (2007) Bayesian Framework
   Kintsch’s (2001) algorithm simulates the process of
matching based on shared neighbors in a semantic space.                 Frank et al. (2007) propose a Bayesian model to jointly
The result of our adaptation is a model that learns to map              learn word-concept mappings, as well as which objects a
words and objects to semantic clusters, greatly simplifying             speaker intends to speak about in a situation. Using a model
the problem of word-object mapping. Rather than casting                 similar to Latent Dirichlet Allocation (LDA) used by Topic
the problem as one of learning associations between                     models, they assume that words are generated from the
independent words and independent objects, a semantic                   lexicon according to what objects are present and are likely
                                                                   2716

                                                                        M w,o = M w,o +
                                                                                               w∈ S      o∈ S H (w) · H (o) · M w,o
                                               Supercapacity:
                                                                                                   H (w) · H (o) · M w,o · χ
                                                                            M w,o = M w,o +
                                                                                                    w∈ S      o∈ S H (w) · H (o)
to be talked about (i.e., the intention of the speaker). This       each word and object is calculated from the normalized row
model is a computational-level model that Best       model
                                               does not     (scaled(column)
                                                        specify       ent ropy):vector of associations for that word (object),
learning mechanisms, but rather specifies how to calculate          p(Mw,·), as follows:
the likelihood of a particular lexicon, given all of the                                          eλ ·(H (w)+ H (o)) · M w,o · χ
                                                                       M w,o = M w,o +                         λ ·(H (w)+ H (o)) · M
situations that one has observed. The inferred lexicon is                                     w∈S       o∈ S e                       w,o
simply a collection of word-object pairings, and tends to be
small because the prior favors smaller lexicons. The model             Theropy
                                                                            update
handles nonreferential words: if a given word  Bestappears
                                                     modelwith
                                                            (scaled ent          witrule for adjusting and allocating strengths for
                                                                                    h decay):
                                                                    the stimuli presented on a trial is:
many objects only a few times, these mappings will likely
not be added to the lexicon. The inferred lexicon will mostly                                      χ · eλ ·(H (w)+ H (o)) · M w,o
                                                                      M w,o = αM w,o +                          λ ·(H (w)+ H (o)) · M
be comprised of the highest co-occurring word-object pairs;                                    w∈ S      o∈ S e                       w,o
there is no explicit penalization for linking words to
multiple objects, nor a word to multiple objects. There is no       In this equation,  is a parameter governing forgetting,  is
                                               Ent ropy:            the attention weight being distributed, and  is a scaling
learning of associations among words, nor among objects.
Frank et al. demonstrate impressive performance from this           parameter governing differential weighting of uncertainty
model on subsequent testing of word-object pairings.                and prior knowledge (familiarity).
                                                                                                n                 As  increases, the
  The semantic space approach we propose differs                    weight   of uncertainty
                                                                             H (M w,·) = −   (i.e., the
                                                                                                    p(M w,i ) · log(p(Mentropy
                                                                                                         exponentiated       w,i )) term,
theoretically from the Frank et al. (2007) model in at least        which    includes  both   the
                                                                                               i= 1 word     and    object’s  association
two ways. Firstly, while the Frank et al. approach attempts         entropies) increases relative to familiarity. The denominator
to calculate an underlying generator that maps from                 normalizes the numerator so2 that exactly  associative
concepts to words through the lexicon, the semantic space           weight is distributed among the potential associations on the
approach is more passive, projecting words and concepts             trial. For stimuli not on a trial, only forgetting operates. This
onto points in psychological space. Secondly, the semantic          model aims to capture the process of learning simple word-
space approach attempts to learn the relations between              concept associations using basic cues a learner may have.
words, including the relations between concepts. This added
structure allows a semantic space model to bootstrap                                 Semantic Space Models
additional partial information from indirect relationships.         Semantic space models have seen a great amount of both
                                                                    attention and success in the literature over the past decade.
Kachergis, Yu, & Shiffrin (2012) Associative Model                  There are a variety of semantic space models currently in
Kachergis et al. (2012) introduced an incremental model             the literature, but all are fundamentally based on the
that learns word-object associations. Competing attentional         assumption that the contexts in which a word occurs may be
biases for familiarity (i.e., already-strong associations) and      used to infer its meaning, commonly projected into a high-
for stimuli with uncertain associates (i.e., high entropy)          dimensional psychological space. Words that frequently co-
allow this model to exhibit mutual exclusivity and other            occur in contexts together, or that frequently occur in
word-learning principles, as well as associative learning           similar contexts, become more proximal in semantic space.
effects such as blocking and highlighting (Kachergis, 2012).        We explore a variety of semantic space model
   The model stores knowledge in M, a word-object                   representations of the CHILDES data here, all using the
association matrix that grows during training. Cell Mw,o is         same mapping mechanism adapted from Kintsch’s (2001)
the strength of association between word w and object o.            algorithm.       We next very briefly describe each
Before the first trial, M has no information: each cell is set      representation model used in our comparison.
to 1/m. Association strengths decay, and on each new trial a
fixed amount of associative weight, , is distributed among         BEAGLE
the associations between words and objects, and added to            The BEAGLE model (Jones & Mewhort, 2007) uses
the strengths. The rule for distributing  (i.e., attention)        holographic vector manipulation to represent word
balances a preference for attending to unknown stimuli with         similarities. In BEAGLE, each new word encountered is
a preference for strengthening already-strong associations.         assigned an environmental vector with elements generated
When a word and referent are repeated, extra attention (i.e.,       independently from a Gaussian distribution, and a lexical
) is given to this pair—a prior knowledge bias. Pairs of           vector of the same length but initialized to zeros. When
stimuli with no or weak associates also attract attention,          encountering a sentence, the environmental vector of each
whereas pairings between uncertain objects and known                word is added to the lexical vector of each word it co-occurs
words, or vice versa, do not attract much attention. Stimulus       with. Similarity is measured using cosine similarities
uncertainty is captured using entropy (H), a measure that is        between words’ lexical vectors.
0 when the outcome of a variable is certain (e.g., a word
appears with only one object), and maximal (log2n) when all         ESA
of the n possible object (or word) associations are equally         Explicit semantic analysis (Gabrilovich & Markovitch,
likely (e.g., for a novel stimulus, or one that appears with all    2007) was designed for use with the Wikipedia corpus. It
stimuli equally). In the model, on each trial the entropy of        uses a centroid-based classifier that correlates given input
                                                                2717

text to a weighted list of concepts associated with each          semantic data without resorting to complex inference
target word.                                                      mechanisms.
FDTRI                                                                               Word-Concept Models
Fixed Duration Temporal Random Indexing, introduced by            We developed a generalized technique to transform any
Jurgens and Stevens (2009), attempts to bypass the                semantic space model into a word-concept learning model.
computational difficulty inherent in singular value               Word-concept models are divided into a learning phase and
decomposition through the use of random projections onto          a prediction phase. In the learning phase, the model is
lower dimensional space. Similar to BEAGLE, each word             applied to sentences composed of words. The generalized
has an environmental vector, although FDTRI vectors are           modification for a word-concept model is to simply
generated to be sparse. Rather than producing a word-by-          concatenate the word tokens with the concept tokens into a
meaning matrix, FDTRI incorporates time in a word-by-             single concept/label sensory episode. In the prediction
meaning-by-time tensor. The additional temporal                   phase, we attempt to assign an object token to each word
information could be useful in word-concept pairing, if the       token.
sequential information given by the caregiver is relevant to        All semantic space models have the ability to determine
object detection. For example, a caregiver may be more            similarity between any word pair, thus prediction can be as
likely to start with the label and then continue with a           simple as finding the object with the maximum similarity to
description of the object.                                        the given target word:
HAL
The Hyperspace Analogue to Language (HAL; Lund &
                                                                                              (
                                                                                     maxargi sim ( wtarg, oi )   )            (1)
Burgess, 1996) uses a fixed size window that is slid along          While this performs reasonably, it is possible to improve
the corpus. A matrix is built which is an accumulation of         on this technique by adding a second step. Building from
pairs of words that co-occur within any given window of           Kintsch’s (2001) predication algorithm, we first activate the
text. Order information is partially preserved through the        neighbor set of N most similar words to our target word:
use of “occurring before,” and “occurring after” co-
                                                                                                   (                   )
occurrence matrices.
                                                                          NSet targ = maxargi sim ( wtarg, wi )               (2)
LSA
                                                                     Then for every object, we calculate its activation, Acti, as
Latent Semantic Analysis (LSA; Landauer & Dumais, 1997)
                                                                  the similarity between that object and every one of the top N
operates by applying singular value decomposition to a
                                                                  word matches, weighted by the similarity of that word to the
word-by-context frequency matrix, reducing the matrix from
                                                                  target word:
high dimensionality (documents) to lower dimensional
space (latent semantic components). The premise is that the
reduction removes irrelevant features of the word usage,
                                                                      Acti =   å      sim(wtarg , w j ) p * sim(o j , w j )   (3)
                                                                             j e NSet
yielding a semantic abstraction in the resulting space.
ISA                                                                  The mechanism provides a match not only to the target
                                                                  word but also to the target’s region of semantic space. This
Baroni, Lenci and Onnis (2007) developed incremental              is particularly important because there are always more
semantic analysis (ISA) to analyze children’s speech data.        words than concepts. Using Kintsch’s (2001) predication
ISA is based on random indexing models with a few                 allows non-nouns to influence the outcome of the similarity
variations. First, updating occurrence information includes       measurement through their similarity to the nouns. Thus, if
both the signature (or environmental vector) as well as           the word “red” is strongly associated with the word “apple”
information on the learned history of the other word. This        in the discussion and “red” is also associated with the
allows ISA to capture higher order relations. Second, word        concept <apple>, then “red” can be used to discover the
frequency discounts are updated online as the model learns        underlying link between “apple” and <apple>. This mapping
for information about the distribution of words in the world.     can be done implicitly, without knowledge of the part of
                                                                  speech as long as the target words that are to be matched to
PMI                                                               objects are known.
Pointwise Mutual Information (PMI; Church & Hanks,
1989) is a basic information theoretic metric that looks at                               Experiment
the probability of two words occurring together relative to
                                                                  Each of the above models was trained on the CHILDES
the probability of each word occurring individually. This
                                                                  corpus and the results were compared to the gold standard
provides a first order word co-occurrence metric, and has
                                                                  model in Frank et al. (2007), as well as to a baseline model
demonstrated remarkable effectiveness at explaining human
                                                                  that simply counts which words and objects co-occur. There
                                                                  are many different ways to evaluate model performance, and
                                                              2718

there does not seem to be agreement in the field about the
correct measure to use. To remain comparable to Frank et al
(2007), we examined the best F-score (the harmonic mean
of precision and recall) achieved by each model. We also
looked at the overall proportion of pairings matching the
golden standard if all 37 words are assigned a concept
meaning. We do this in order to determine what mappings
the system makes if forced, although we note that this
random slice of parent-child interaction may not be enough
to disambiguate all of the mappings.
   We also explore hybrid models, asking which two models
contribute the most complementary (non-redundant)
information. This exercise may hint at what combinations of
mechanisms are most important for learning in the natural
language environment.
                           Results
One popular way of visualizing the results of word-concept
learning is through a confusion matrix as shown in Figure 1.
The confusion matrix shows the similarity between word-
concept pairings as gradients from black to white (with
lighter being a higher association) filling each grid cells.
According to the gold standard, each word is associated
with exactly one object (except for “bird” which can refer to
<duck> or <bird>). The cells outlined in red indicate the             Figure 2: Word space. For each model by word, a black
correct word-object pairings according to the gold standard.          square indicates if model correctly identified that word.
   In Figure 1a and 1b, a winner-takes-all filter has been
applied for each word. Thus, the object that has the highest         It is important to understand which word-object pairs each
association has been assigned the similarity of 1, and all         individual model gets correct or incorrect. It is also
others have been assigned a similarity of 0. The values            important to see which word/object pairs are overall more or
returned by the semantic space models cannot be directly           less likely to be found by the model. Figure 2 shows for
interpreted as probabilities for pairing selections. Hence,        each model, and each word, the probability of accurately
only relative similarity measures are used here. In Figure 1c,     identifying the target word. Black squares indicate word-by-
we display the gradient similarity ratings after having been       model pairs in which model correctly identifies the object
scaled to a power of five (thus exaggerating the differences       associated with that word. There is also a calculated average
between predictions).                                              correctness for each word (rightmost column) and for each
                                                                   model (bottom row). Some of the semantic space models are
                                                                   non-deterministic (BEAGLE, FDTRI and ISA). For these
                                                                   models, 100 runs were computed and a correct identification
                                                                   granted when more than half of the runs correctly identified
                                                                   that pairing. No partial credit was given in any form for
                                                                   coming close to correctly matching each word. Parameter
                                                                   values were selected to be those optimized (relative to
                                                                   expected overall matches) for the individual model.
                                                                     The F-scores are important indicators to help understand
                                                                   how well each model has correctly inferred the word-object
                                                                   pairings. To calculate the F-scores for each model, we first
                                                                   computed a similarity matrix for every word-object pair. For
                                                                   non-deterministic models, the similarities were averaged
                                                                   across 100 similarity runs for the given model. Next,
                                                                   maximum values were determined within each word to
                                                                   select an object. Pairings that were correct were labeled true,
                                                                   and pairings labeled that were incorrect, were labeled false.
                                                                   These similarity measures were then ordered based on
                                                                   strength (both correct and incorrect measures). For each N,
                                                                   precision and recall figures were then calculated for each of
                                                                   the top N word pairings.
              Figure 1: Confusion matrix results.
                                                               2719

  This results in the receiver operator curves shown in            Because different models seem to make different
Figure 3, and Table 1 shows the maximum F score values           mistakes, we also explored how hybrid models might be
taken from the ROC curve chart for the top 5 models.             able to exploit these differences. Each model M is able to
                                                                 assign some similarity measure to and word-object pair
                                                                 Msim(Wx,Oy). We considered hybrid models of the following
                                                                 form: MA,Bsim(Wx,Oy) = MAsim(Wx,Oy) * c + MBsim(Wx,Oy).
                                                                 Each pair of models was optimized, relative to the average
                                                                 number of matches with the golden standard, for the
                                                                 constant c.
                                                                   Figure 5 shows the correct number of matches for each
                                                                 optimum pairing of models. A heat map of colors has been
                                                                 added to indicate highest (red) to lowest (purple) values.
                                                                 The optimum model is a co-occurrence by BEAGLE hybrid
                                                                 with the later having a weight three times greater than the
                                                                 former. This hybrid model results in 30 correct mappings on
                                                                 the gold standard.
              Figure 3: ROC curve for all models.
                    Table 1. Top F Scores
Model     Hybrid    BEAGLE        Frank   ESA     COOC
Best F       .83        .55         .54    .54     .53
  Using the Word 2 Word language visualization tool
(Kievit-Kylar & Jones, 2012) we can visualize all of the
word/object pairings as a graph. In Figure 4, words are            Figure 6: Word network visualization of optimum hybrid.
nodes and each similarity measure is an edge. In the
visualization below, we see all concepts lined up across the
top with each word referring to them shown below. The
green connections indicate the strongest similarities
observed by the system. Ideally, all lines would link to the
word directly above.
                                                                    Figure 7: Thresholded confusion matrix for best hybrid.
  Figure 4: Word network visualization of BEAGLE model
                           solution.
                                                             2720

                                                 Figure 5: Hybrid Pairings.
                                                                 Church, K.W., Hanks, P. (1989). Word Association Norms,
                      Conclusions                                  Mutual Information and Lexicography. In: Proceedings of
The semantic space approach to word-concept learning is a          the 27th Annual Conference of the Association of
fruitful endeavor with potential to better understand how          Computational Linguistics, 76-83.
humans make use of mechanisms and mutually reinforcing           Frank, M. C., Goodman, N. D., & Tenenbaum, J. B. (2007).
information sources across learning. The best pure semantic        A Bayesian framework for cross-situational wordlearning.
space models was able to predict the gold standard to a            Advances in neural information processing systems, 20,
higher degree of accuracy than existing models while still         1212–1222.
conforming to known semantic and processing constraints.         Gabrilovich, E. and S. Markovitch. (2007). Computing
The adaptation of Kintsch’s (2001) mechanism for                   semantic relatedness using Wikipedia-based explicit
predication allows semantic models to consider not only the        semantic analysis. Proceedings of IJCAI, 1606-1611.
semantic similarity between a word and object, but to also       Jones, M. N., & Mewhort, D. J. K. (2007). Representing
consider mutual information from the semantic                      word meaning and order information in a composite
neighborhoods. This procedure provided a benefit to each of        holographic lexicon. Psychological Review, 114, 1-37.
the semantic space models tested.                                Jurgens D., Stevens K. (2009). Event detection in blogs
   Hybrid models also provide interesting insight into the         using temporal random indexing. In Proceedings of the
word/concept-learning problem. The optimum hybrid model            Workshop on Events in Emerging Text Types, 9-16.
merged the co-occurrence model with BEAGLE. This                 Kachergis, G. (2012). Learning Nouns with Domain-
optimal fusion makes intuitive sense, as the co-occurrence         General Associative Learning Mechanisms. Proceedings
model provides first-order co-occurrence information that          of the 34th Annual Conference of the Cognitive Science
can be best supplemented by the higher-order co-occurrence         Society 533-538.
information inherent in the semantic space models. The           Kachergis, G., Yu, C., & Shiffrin, R. M. (2012). An
performance of the hybrid model suggests that infants may          Associative Model of Adaptive Inference for Learning
be capitalizing on both raw co-occurrence information and          Word-Referent Mappings. Psychonomic Bulletin &
an emerging ability for higher-order semantic abstraction.         Review, 19(2), 317-324.
Knowledge of which words are similar to each other from          Kievit-Kylar, B., & Jones, M. N. (2012). Visualizing
linguistic experience may be used to bootstrap word-object         multiple word similarity measures. Behavior Research
mappings across learning.                                          Methods, 44, 656-674.
                                                                 Kintsch, W. (2001). Predication. Cognitive Science, 25,
                   Acknowledgments                                 173-202.
This research was supported by grants from Google                Landauer, T.K., Dumais, S.T. (1997). A solution to Plato's
Research and NSF BCS-1056744 to MNJ.                               problem: The latent semantic analysis theory of
                                                                   acquisition, induction, and representation of knowledge.
                       References                                  Psychological Review, 104(2), 211-240.
                                                                 Lund, K., & Burgess, C. (1996). Producing high-
Baroni, M., Lenci, A., and Onnis, L. (2007). Isa meets lara:
                                                                   dimensional semantic spaces from lexical co-occurrence.
   A fully incremental word space model for cognitively
                                                                   Behavior Research Methods, Instruments, and
   plausible simulations of semantic learning. In
                                                                   Computers, 28, 203-208.
   Proceedings of the 45th Meeting of the Association for
                                                                 MacWhinney, B. (2000). The CHILDES Project: Tools for
   Computational Linguistics. 49–56.
                                                                   Analyzing Talk. Lawrence Erlbaum.
                                                             2721

