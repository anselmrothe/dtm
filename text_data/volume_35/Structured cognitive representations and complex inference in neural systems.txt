UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Structured cognitive representations and complex inference in neural systems
Permalink
https://escholarship.org/uc/item/7zg3346h
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Gerhsman, Samuel
Tenenbaum, Joshua
Pouget, Alexander
et al.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

  Structured Cognitive Representations and Complex Inference in Neural Systems
                       Samuel J. Gershman, Joshua B. Tenenbaum ({sjgershm,jbt}@mit.edu)
                                        Department of Brain and Cognitive Sciences, MIT
                                                    Cambridge, MA 02139 USA
                                    Alexandre Pouget (Alexandre.Pouget@unige.ch)
                                       Department of Neuroscience, University of Geneva
                                                   CH-1211 Geneva 4 Switzerland
                                    Matthew Botvinick (matthewb@princeton.edu)
                                         Department of Psychology, Princeton University
                                                      Princeton, NJ 08540 USA
                                          Peter Dayan (dayan@gatsby.ucl.ac.uk)
                             Gatsby Computational Neuroscience Unit, University College London
                                               London WC1N 3AR United Kingdom
   Keywords: Bayesian models, rational analysis, perception, ol-         This symposium addresses the question: how do neural cir-
   faction, memory                                                   cuits acquire and compute with structured representations?
                                                                     This question is examined from a number of angles. Ger-
                          Summary                                    shman introduces the basic issues and discusses attempts
The dream of cognitive neuroscience has always been a seam-          to articulate a neurally plausible theory of structured cogni-
less integration of cognitive representations with neural ma-        tion. Pouget describes recent work on implementing com-
chinery, but—despite decades of work—fundamental gaps                plex probabilistic computations in neural circuits. Botvinick
remain. Part of the problem is that many contemporary the-           shows how neural circuits can be used to discover hierarchical
ories of cognition are formulated in terms of representations        task structure in the environment. Finally, Dayan discusses
and computations that are quite different from those used in         work on wedding richly structured models of semantics with
computational neuroscience. Bridging this gap requires more          representations of individual episodes. Each talk will be 20
than simply a translation between theoretical concepts in the        minutes long, followed by a 20 minute panel discussion with
two fields; what is needed is a more radical updating of neu-        speakers moderated by Tenenbaum.
roscience’s theoretical vocabulary.
   What should this vocabulary look like? Some important                    Gershman: from knowledge to neurons
features of representations and computations used in contem-         How can neurons express the structured knowledge represen-
porary cognitive theories are:                                       tations central to intelligence? This problem has been at-
                                                                     tacked many times from various angles. I discuss the history
• Compositional, recursive and relational representations
                                                                     of these attempts and situate our current understanding of the
   (Fodor, 1975; Smolensky, 1990; Hummel & Holyoak,
                                                                     problem. I then outline a new approach based on the idea
   2003; Stewart et al., 2011).
                                                                     of compressing structured knowledge using neurons in a way
• Flexible use of different structural forms (e.g., taxonomic        that supports probabilistic inference. I illustrate this approach
   vs. causal knowledge; Kemp & Tenenbaum, 2009).                    using examples from motion perception and value-based de-
                                                                     cision making.
• Multiple levels of abstraction (Tenenbaum et al., 2011).
                                                                       Pouget: modeling the neural basis of complex
• Knowledge partitioning / clustering (Lewandowsky &
   Kirsner, 2000).                                                                     intractable inference
                                                                     It is becoming increasingly clear that neural computation can
• Complex intuitive theories (e.g., naive physics, theory of         be formalized as a form of probabilistic inference. Several
   mind; Carey, 2009).                                               hypotheses have emerged regarding the neural basis of these
• Algorithms that operate on these representations (e.g., dy-        inferences, including one based on a type of code known
   namic programming, Monte Carlo methods; Griffiths et al.,         as probabilistic population codes or PPCs (Ma et al., 2006).
   2012).                                                            PPCs have been used to model simple forms for multisensory
                                                                     integration, attentional search, perceptual decision making or
These representations and computations are “structured” in           causal inference, for which human subjects have been shown
the sense that they incorporate rich domain knowledge and            to be nearly optimal. However, most inferences performed by
strong constraints (Tenenbaum et al., 2011).                         the brain are too complex be solved optimally in a reasonable
                                                                  83

amount of time and must therefore involve approximate so-            Carey, S. (2009). The origin of concepts. Oxford University
lutions. We have started to explore how neural circuits could          Press, USA.
implement a particular form of approximation, called vari-           Fodor, J. (1975). The language of thought. Harvard Univer-
ational Bayes, with PPCs (Beck et al., 2012). Remarkably,              sity Press.
this approximation requires a nonlinearity known as divisive         Griffiths, T., Vul, E., & Sanborn, A. (2012). Bridging levels
normalization which has already been found in most neural              of analysis for probabilistic models of cognition. Current
circuits. This approach can be applied to a wide range of              Directions in Psychological Science, 21(4), 263–268.
complex inferences, such as the ones involved in olfactory
                                                                     Hummel, J., & Holyoak, K.            (2003).     A symbolic-
processing, image processing in the primary visual cortex and
                                                                       connectionist theory of relational inference and generaliza-
other related problems.
                                                                       tion. Psychological Review, 110(2), 220–264.
      Botvinick: discovering hierarchical task                       Kemp, C., & Tenenbaum, J. (2009). Structured statisti-
                           structure                                   cal models of inductive reasoning. Psychological Review,
                                                                       116(1), 20–58.
Naturalistic action displays a hierarchical structure: Simple
                                                                     Lewandowsky, S., & Kirsner, K. (2000). Knowledge parti-
actions cohere into subtask sequences or component skills,
                                                                       tioning: Context-dependent use of expertise. Memory &
which in turn combine to realize overall goals. Computa-
                                                                       Cognition, 28(2), 295–305.
tional models from cognitive psychology, artificial intelli-
gence, and most recently neuroscience, have sought to char-          Ma, W., Beck, J., Latham, P., & Pouget, A. (2006). Bayesian
acterize the representations and mechanisms underlying hi-             inference with probabilistic population codes. Nature Neu-
erarchical action control (Botvinick, 2008). However, such             roscience, 9(11), 1432–1438.
models tend to neglect a fundamental question: How do hi-            Smolensky, P. (1990). Tensor product variable binding and
erarchical representations of action or task structure initially       the representation of symbolic structures in connectionist
arise? We approach this as a learning problem, asking how              systems. Artificial intelligence, 46(1), 159–216.
useful component skills can be inferred from experience. Be-         Stewart, T., Bekolay, T., & Eliasmith, C. (2011). Neural rep-
havioral evidence suggests that such learning arises from a            resentations of compositional structures: representing and
structural analysis of encountered problems, one that max-             manipulating vector spaces with spiking neurons. Connec-
imizes representational efficiency and, as a direct result, de-        tion Science, 23(2), 145–153.
composes task into subtasks by ‘carving’ them at their natural       Tenenbaum, J., Kemp, C., Griffiths, T., & Goodman, N.
‘joints.’ A key question is how this analysis and optimization         (2011). How to grow a mind: Statistics, structure, and
process might be implemented neurally. Recent data suggests            abstraction. Science, 331(6022), 1279–1285.
an intriguing answer: Detection of hierarchical task structure
might arise as a natural consequence of predictive represen-
tation. I’ll present computational work fleshing out this pos-
sibility, along with behavioral and fMRI data that lend it con-
siderable initial support.
       Dayan: unsupervised learning and the
          representation of episodic structure
The representation of hierarchically structured knowledge in
systems using distributed patterns of activity is an abiding
concern for the connectionist solution of cognitively rich
problems. One particularly important unresolved issue con-
cerns episodic versus semantic structure—how rich genera-
tive models of the semantics of domains can be used in the
representation of particular, structured, entities. I will unpack
this problem and suggest some routes to solutions.
                         References
Beck, J., Heller, K., & Pouget, A. (2012). Complex inference
   in neural circuits with probabilistic population codes and
   topic models. In Advances in neural information process-
   ing systems 25 (pp. 3068–3076).
Botvinick, M. (2008). Hierarchical models of behavior and
   prefrontal function. Trends in Cognitive Sciences, 12(5),
   201–208.
                                                                  84

