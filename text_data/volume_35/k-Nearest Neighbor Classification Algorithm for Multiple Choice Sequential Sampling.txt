UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
k-Nearest Neighbor Classification Algorithm for Multiple Choice Sequential Sampling
Permalink
https://escholarship.org/uc/item/9kx3f7b5
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Noh, Yung-Kyun
Park, Frank Chongwoo
Lee, Daniel D.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                               k-Nearest Neighbor Classification Algorithm for
                                        Multiple Choice Sequential Sampling
                                              Yung-Kyun Noh (nohyung@snu.ac.kr)
                                             Frank Chongwoo Park (fcp@snu.ac.kr)
                             School of Mechanical and Aerospace Engineering, Seoul National University
                                                           Seoul 151-744, Korea
                                              Daniel D. Lee (ddlee@seas.upenn.edu)
                            Department of Electrical and Systems Engineering, University of Pennsylvania
                                                       Philadelphia, PA 19104, USA
                               Abstract                                 speed-accuracy tradeoff. In these methods, one or more vari-
                                                                        ables are commonly introduced for accumulating sampled in-
   Decision making from sequential sampling, especially when            formation, and a criterion is used to determine whether to
   more than two alternative choices are possible, requires appro-
   priate stopping criteria to maximize accuracy under time con-        continue collecting more information or to make a decision
   straints. Optimal conditions for stopping have previously been       with given information. Here, we propose a common mathe-
   investigated for modeling human decision making processes.           matical framework combining these methods and providing a
   In this work, we show how the k-nearest neighbor classification
   algorithm in machine learning can be utilized as a mathemati-        systematic explanation for understanding different methods.
   cal framework to derive a variety of novel sequential sampling          Our framework combining sequential sampling methods is
   models. We interpret these nearest neighbor models in the con-
   text of diffusion decision making (DDM) methods. We com-             the k-nearest neighbor (NN) classification in machine learn-
   pare these nearest neighbor methods to exemplar-based models         ing. The sequential sampling situation with multiple choices
   and accumulator models, such as Race and LCA. Computa-               is explained as the multiway k-NN classification from the the-
   tional experiments show that the new models demonstrate sig-
   nificantly higher accuracy given equivalent time constraints.        oretical analysis on k-NNs in the asymptotic situation. Due
                                                                        to this connection, we can interpret all different types of se-
   Keywords: sequential sampling; decision making; diffusion
   decision making model; k-nearest neighbor classification; evi-       quential sampling methods as different methods of choosing
   dence; sequential probability ratio test                             k adaptively in k-NN classification. By further analyzing the
                                                                        strategy of choosing k in k-NN classification using the Se-
                          Introduction                                  quential Probability Ratio Test (SPRT) (Wald & Wolfowitz,
                                                                        1948) and Bayesian inference, we can obtain five different ac-
Whenever a faster decision is required to save time and re-             cumulating variable and stopping criteria for optimal tradeoff.
sources, the decision making process should focus on choos-             Interestingly, all these five optimal methods are interpreted as
ing whether to proceed with a decision in light of the given in-        different kinds of DDM strategies.
formation or to postpone the decision in order to collect more
information for a higher confidence level. In many previous                Our work is directly applied to a recently reported neuro-
and recent psychology works, various computational models               scientific decision making mechanism. The proposed mech-
have been introduced seeking to explain the speed-accuracy              anism considers an output neuron which sends out a decision
tradeoff and to understand the decision making process in hu-           result. By collecting Poisson spike trains from different neu-
mans. However, apart from the understanding of individual               rons, the output neuron makes a decision about which neuron
models, there has been little systematic way of understand-             gives Poisson spikes at the highest rate (Shadlen & Newsome,
ing these models in one mathematically unified framework.               1998; Ma, Beck, Latham, & Pouget, 2006; Beck et al., 2008;
Moreover, multiple-choice problems were not discussed in-               Zhang & Bogacz, 2010). The output neuron can achieve op-
tensively in any of the methods.                                        timality by using our proposed strategies.
   The optimality in decision making with sequential sam-                  The proposed method can be compared with traditional ex-
pling is discussed with the optimality in speed-accuracy                emplar models which explain memory retrieval using similar-
tradeoff. In other words, the objective of the present work             ity weighted voting based on stored exemplars. Our work is
is to seek the fastest decision with the same average accuracy          different from this line of research by using majority voting
or the maximum accuracy if the same average decision time               of adaptively chosen k number of NNs. We discuss the ad-
is used. Sequential sampling methods such as Race (Smith                vantages and disadvantages of our method when it is applied
& Vickers, 1988; Vickers, 1970), diffusion decision making              to the memory retrieval problem.
(DDM) (Ratcliff, 1978; Ratcliff & Rouder, 2000; Shadlen,                   The rest of the paper is organized as follows. We introduce
Hanks, Churchland, Kiani, & Yang, 2006; Ratcliff & Mck-                 the sequential sampling problem in Section 2 especially from
oon, 2008), and leaky competing accumulator (LCA) (Usher                the point of view of multiple-choice. In Section 3, we in-
& McClelland, 2001; Bogacz, Usher, Zhang, & McClelland,                 troduce problems to which sequential sampling methods can
2007) are all interested in explaining this optimality in the           be applied, and we show how k-NN classification can be natu-
                                                                   3151

rally introduced as a common framework for explaining these          but it considers the interaction between the variables. LCA
problems. In Section 4, we derive the examples of two- and           considers the dynamics of activation with the decay of the
multiple-choice evidence for DDM in light of k-NN classifi-          activation as well as the inhibitory interaction between acti-
cation. After we explain the relationship between our method         vation variables. This LCA dynamics is very flexible in that
and other exemplar methods in Section 5, we present simu-            the strategy can be either similar to Race or DDM as its spe-
lation experiments in Section 6. Finally, we conclude with           cial case, but the maximum performance is known to be that
discussion in Section 7.                                             of DDM (Bogacz et al., 2007).
               Computational Methods in                              Multiple-Choice Extension
            Sequential Sampling Problems                             Among the aforementioned sequential sampling models, the
                                                                     multiple-choice extension of the Race and LCA models is
Sequential sampling methods consider decision making using
                                                                     straightforward, by just increasing the number of accumulat-
incoming information over time. With unlimited time, the
                                                                     ing variables. However, the extension of DDM is more com-
decision can be made late enough to increase the expected
                                                                     plex. Fortunately, the Multiple SPRT (MSPRT) method was
accuracy. However, if the decision should be made as soon
                                                                     previously developed by extending the SPRT method using
as possible, there is a trade-off between the speed and accu-
                                                                     the number of signals (Dragalin et al., 1999; Zhang & Bo-
racy of the decision. In order to address this tension, decision
                                                                     gacz, 2010). In addition to this MSPRT result, we also pro-
making strategies introduce criteria to determine whether or
                                                                     vide different criteria for multiple-choice DDM using deriva-
not to make a decision at a certain time.
                                                                     tions from other approaches of MSPRT and Bayesian infer-
Accumulator Model: One simple method of determining                  ence. Our result provides an evidence diffusing in a C − 1
whether the accumulated information has reached a certain            dimensional space for a C alternative choice problem.
level of confidence is the accumulator model. This model
considers one variable for each choice and accumulates in-                          Sequential Sampling Problems
formation separately in favor of each choice. Once one of            Decision making problem using sequential sampling can be
the accumulating variable reaches a predefined threshold, the        found in many examples. Here we introduce two exemplary
decision is made immediately thereafter.                             problems. One example can be found in neuronal decision
   This simple model with no interaction between different           making as in the left figure of Fig. 1. When an output neuron
choices is known as suboptimal. This method can be com-              tries to make a decision as to whether one incoming signal
pared with the DDM strategy in the next section, where the           has a higher Poisson rate than the other has, the output neuron
accuracy of the accumulator model is always less than the ac-        can collect signals until the accumulated information reaches
curacy of the DDM model (Zhang & Bogacz, 2010). This                 a certain level.
model of doing race between accumulators is also called the             Another example can be found in a Bayes classification
Race model.                                                          problem where we only have data generated from unknown
                                                                     underlying density functions. Bayes classification selects the
Diffusion Decision Making (DDM) Model: In this model
                                                                     class having the highest underlying density, but the classifier
with two choices, one variable is introduced to collect infor-
                                                                     in this case cannot directly access the underlying density in-
mation and diffuse toward one of the choice. This variable,
                                                                     formation. A surrogate method of determining the class of
also known as the evidence, represents the bias in the pref-
                                                                     highest density is through k-NN classification. By collecting
erence of accumulated information toward a choice. Finally,
                                                                     more nearest neighbors, the confidence of choosing a class of
once the evidence reaches a pre-defined level of any choice,
                                                                     the highest density is expected to increase to a targeted level.
it stops diffusing and selects the choice.
                                                                        Here, we show that the two problems are in fact exactly
   A canonical method of determining the evidence vari-
                                                                     the same by explaining several theoretical results on k-NN
able and stopping criterion uses the sequential probability
                                                                     classification in the asymptotic situation:
ratio test (SPRT) (Wald & Wolfowitz, 1948; Dragalin, Ter-
takovsky, & Veeravalli, 1999; Zhang & Bogacz, 2010). Pre-            Majority Voting Rule in k-Nearest Neighbor Classifica-
vious work using this test has considered two incoming Pois-         tion: When there are N number of training data with labels,
son signals aiming to determine the signal with the higher           D = {xi , yi }Ni=1 , where each datum xi ∈ RD is represented as
Poisson rate from the accumulation of signals. In this case,         a D-dimensional vector, and the label has one of C labels,
the diffusing evidence is just the difference in the number of       yi ∈ {1, . . . ,C}, k-NN classification assigns class y to a class-
signals within a certain time, and the decision is made once         unknown datum x according to the majority voting with k
this difference exceeds a threshold. This method is known to         labels of nearest data in D :
be optimal among sequential sampling methods such as Race                                           k
and LCA (Bogacz, Brown, Moehlis, Holmes, & Cohen, 2006;                                y = arg max ∑ 1I(yn(i) = c)                   (1)
                                                                                                c
Bogacz et al., 2007).                                                                              i=1
Leaky Competing Accumulator (LCA): LCA uses one                      with nearest neighbor index n(i), i = 1, . . . , k. The theoretical
variable for each choice similar to the accumulator model,           study of this majority voting strategy originates from Cover
                                                                 3152

Figure 1: Diffusion decision making (DDM) in neurons determines the input neuron with the higher firing rate, and its analogous
k-NN classification determines the larger class-conditional density function
and Hart (Cover & Hart, 1967; Cover, 1967), and in their              with continuous time and discrete accumulation of informa-
work, once there are enough data, the expected error mono-            tion, because the accumulation variable is the function of the
tonically and asymptotically decreases with the number of k:          number of NNs. In contrast, the second design uses the dis-
                                                                      crete time and continuous accumulation of information.
         E(k = 1) ≥ E(k = 2) ≥ . . . ≥ E(k = ∞)               (2)
                                                                      Distribution of the distances: Now, we show that k-NN
for k  N. This continuous decrease of error will encourage           classification is in fact equivalent to sequential sampling for
the use of more nearest neighbors, which explains the tradeoff        determining the signal with the highest Poisson rate.
between the number of nearest neighbors k and the classifica-            A recent study discussed the distribution of the distance to
tion accuracy.                                                        the NNs when there are enough data (Leonenko, Pronzato, &
                                                                      Savani, 2008). Instead of directly dealing with the distribu-
Distance Comparison Rule of k0 -th NNs of Each Class: If              tion of distance, they changed the random variable to u = NV ,
we consider a strategy of comparing two k0 -th NN in class 1          with volume V of D dimensional hypersphere having the dis-
and class 2, we can easily prove that this strategy is equivalent     tance to the k-th NN as a diameter multiplied by the number
to the majority voting rule with k = 2k0 − 1 NNs.                     of data N. Then the distribution of samples approaches the
   Proof: Consider a comparison between k0 -th NN of class            Erlang density function:
1 and another k0 -th NN of class 2. If k0 -th NN in class 1 is
closer than k0 -th NN in class 2, then we can say that the k0 -th                                λk
                                                                                    ρ(u|λ) =         exp(−λu)uk−1                 (3)
NN in class 2 can never be included in the closest (2k0 − 1)                                   Γ(k)
NNs, because at least k0 number of NNs in class 1 and (k0 −1)
number of NNs in class 2 have less distance than the k0 -th           with a parameter λ, which is the probability density p(x) at
NN in class 2. Therefore, comparing strategies of k0 -th NN           x ∈ RD . Moreover, this special Erlang function implies the
in each class is the same as majority voting with (2k0 − 1)           Poisson distribution of the number of NNs k within a speci-
nearest neighbors.                                                    fied volume of the hypersphere (Wasserman, 2003):
   Therefore, the monotonic increase of accuracy is also sat-
isfied with the increase of k0 .                                                                    λk
                                                                                     ρ(k|λ) =            exp(−λ).                 (4)
                                                                                                Γ(k + 1)
Two Sequential Sampling Methods in k-NN Classification:
From the monotonic increase of the accuracy with the in-              This equation shows that the number of NNs within a growing
crease of k (or k0 ), we can make two different sequential sam-       hypersphere at a constant rate in volume is a Poisson process.
pling methods showing the speed-accuracy tradeoff.                       Comparing this Poisson process interpretation with the
   First, we can consider the majority voting strategy using          aforementioned neuronal decision making, we can draw sev-
number of NNs within a certain distance from the testing              eral corresponding analogies. The firing rate of the Poisson
point. If we do not have enough accuracy with the current             signal corresponds to the underlying density function in k-NN
distance, we can increase it to use more resources. Another           classification, the number of spikes corresponds to the num-
example can be designed by considering the distance to the            ber of NNs, the time within which spikes are counted cor-
same k0 -th NN in each class and making a decision by com-            responds to the volume of the hypersphere within which we
paring the distances.                                                 count NNs, and as a consequence, determining a choice with
   The first design corresponds to the sequential sampling            the highest firing rate corresponds to the problem of deter-
                                                                  3153

  mining the class with the highest underlying density function,                 we can decide the choice. For two-choice problem (C = 2),
  which is also known as the Bayes classification.                               comparing the MSPRT criteria with a certain value reduces
      The correspondence shows that these two very well-known                    to a simple comparison whether g∗ (k1 − k2 ) and h∗ (u2 − u1 )
  methods from different disciplines can share optimal strate-                   is greater than a certain confidence threshold, for Eq. (6) and
  gies as well as theoretical knowledge. However, the study                      Eq. (7), respectively.
  of a method in one field is rarely investigated in another; the                    For DV, an additional conservative method can be consid-
  strength of the correspondence suggests that whenever a good                   ered. The decision can be made more carefully for the class of
  strategy is found for DDM, a corresponding strategy should                     interest (here, class 1), by using the maximum possible vol-
  be examined for machine learning. Conversely, when a new                       ume containing k-th NN, in other words, the volume of the
  strategy is provided for the k-NN method, its relevance to psy-                hypersphere of (k + 1)-th NN of class 1 instead of the volume
  chology should also be investigated.                                           of k-th NN. We call this strategy “conservative DV” (CDV),
                                                                                 and in CDV, an additional NN is always used to calculate the
               Derivation of Stopping Criteria                                   accumulated information.
  In this section, we now derive stopping criteria from k-NN
  classification using MSPRT and Bayesian inference for a
                                                                                 Bayesian Inference
  multiple-choice problem.                                                       Another method of utilizing the Bayesian method is to use the
                                                                                 prior density function for λ with parameters a and b:
  Multiple Sequential Probability Ratio Test
                                                                                                                     ba a−1
  One simple statistical test for determining whether one of C                                         p(λ) =              λ exp(−λb).                                  (8)
  different choices has the highest probability density is the                                                     Γ(a)
  MSPRT. MSPRT uses fixed parameters of densities λ+ and                         With conjugacy relationship, we can calculate the posterior
  λ− where λ+ > λ− , it calculates the likelihood that the first                 probability that the underlying density of choice 1, λ1 , is
  data came from the density λ+ and others from λ− , and then                    greater than the underlying densities of the other choices
  compares those likelihoods.                                                    λ2 , . . . , λC with given condition D on nearest neighbor in-
      Without loss of generality, we consider the likelihood that                formation. The calculation of P(λ1 > λ2 , . . . , λC |D) is per-
  the highest density λ+ is occupied by λ1 . In other words,                     formed using the probability primitives such as P(λ1 < λ2 |D),
  λ1 = λ+ , and λc = λ− for c = 2, . . . ,C. Because of the inde-                P(λ1 < λ3 |D), . . ., and P(λ1 < λ2 , . . . , λC |D):
  pendence between classes,
                                                                                                                      Z ∞                           Z ∞                   
                                                                                 P(λ1 > λ2 , . . . , λC |D) = dλ1 p(λ1 |D) 1 − dλ2 p(λ2 |D)
                                                                           
log P k1 , . . . , kC , u1 , . . . , uC λ1 = λ+ , λ2 = λ− , . . . , λC = λ−                                            0                              λ
                                                                                                                             Z ∞                    1
                                        C
      = log ρ(k1 , u1 |λ+ ) + ∑ log ρ(kc , uc |λ− ).                      (5)                                  · · · 1 − dλC p(λC |D)                                     (9)
                                                                                                                                λ1
                                       c=2
                                                                                                             = 1 − P(λ1 < λ2 |D) . . . − P(λ1 < λC |D) +
  The posterior P1 that λ1 occupies λ+ is proportional to this                                                 . . . + (−1)C−1 P(λ1 < λ2 , . . . , λC |D). (10)
  likelihood Eq. (5). From the Poisson distribution in Eq. (4)
  with kc , the number of NNs of class c within the same volume,                 When the condition is on the number of nearest neighbors
  we can obtain the log of posterior:                                            k1 , . . . , kC within a certain volume, the general form of primi-
                                                         !                       tives is presented with multinomial coefficients:
                                             C
              log P1 = g∗ k1 − log          ∑ exp(g∗ kc )                 (6)   P(λ1 < λ j2 , . . . , λ jL |k1 , . . . , kC ) =                                         (11)
                                            c=1
                                                                                       k j2    k jL
                                                                                                                                           k1 + ∑Lc=2 (k jc − i jc )
                                                                                                                                                                          
                                                                                                                   1
  with a predetermined ratio g∗ = log(λ+ /λ− ). If we consider                         ∑· · · ∑       L (k1 +1+∑Lc=2 (k jc −i jc ))     k j2 − i j2 , · · · , k jL − i jL
  the volume distribution for the same k-th NNs, the equation                       i j2 =0 i jL =0
  for the posterior also becomes
                                                                                 where L and j1 , . . . , jL are determined according to the prim-
                                             C
                                                            !                    itives in Eq. (10). In addition, when volume information
           log P1 = −h∗ u1 − log            ∑ exp(−h∗ uc )                (7)    u1 , . . . , uC is given for k1 , · · · , kC -th NN in each class, respec-
                                            c=1                                  tively, the primitives are
  with h∗ = λ+ − λ− . We call Eq. (6) “DN”, which considers                       P(λ1 < λ j2 , . . . , λ jL |u1 , . . . , uC ) =                                    (12)
  the difference in the number of NNs within a specific vol-                              k j2 k jL                                         k1 +1    L       i jc
                                                                                                           k + ∑L i jc                      u1 ∏c=2 u jc
                                                                                                                               
  ume of the hypersphere and Eq. (7) “DV”, which considers
  the difference in the volumes of the same k-th NNs. In order
                                                                                          ∑· · ·∑ i1j2 , · ·c=2      · , i jL                                   L
                                                                                                                                    (u1 + ∑Lc=2 u jc )k1 +∑c=2 i jc +1
                                                                                       i j2 =0 i jL =0
  to make a decision with confidence, we can first increase the
  volume of hypersphere or increase the number of NNs un-                        for L and j1 , . . . , jL determined from the primitive. Now,
  til the criterion exceeds a pre-defined confidence level, then                 Eq. (10) with primitives in Eq. (11) can be considered as a
                                                                             3154

           PV                        PN                        DV                         CDV                      DN
Figure 2: Examples of diffusion of evidence for three-choice decision making. The diffusion of posteriors, P1 and P2 , are
plotted on the horizontal and vertical axes. The threshold is set to .8.
criterion “PN” and Eq. (10) with primitives in Eq. (12) can
be considered as a criterion “PV.” Here, we used a = 1 and
positive small value b.
   For a two-choice problem, with kc -th NNs in c class within
the same hypersphere, the probability result becomes a very
simple equation
                          k            m 2k+1−m
                               2k + 1      u1 u2
   P(λ1 > λ2 |u1 , u2 ) = ∑                         2k+1
                                                          (13)
                         m=0
                                 m        (u1 + u2 )
from Eq. (11). Similarly, with u1 and u2 of k-th NN in each
class, Eq. (12) becomes
                                  k1                 
                            1            k1 + k2 + 1
  P(λ1 > λ2 |k1 , k2 ) = k +k +1 ∑                      . (14)      Figure 3: Performance of adaptive k-NN classification using
                         2 1 2 m=0            m
                                                                    PN, DN, Race, and a machine learning criteria, “Cons.” Ac-
Both Eq. (13) and Eq. (14) are the sums of binomial distri-         curacy is plotted with an average number of NNs used for var-
butions which can be interpreted analogous to coin tossing          ious thresholds of confidence. Cons makes a decision when
problem with a biased and an unbiased coin. Eq. (13) corre-         the number of recent consecutive NNs of the same class ex-
sponds to the probability of having heads less than or equal        ceeds a threshold (Ougiaroglou et al., 2007).
to k among 2k + 1 tosses of a biased coin, and Eq. (14) cor-
responds to the probability of having heads less than or equal
to k1 among k1 + k2 + 1 tosses of an unbiased coin.                 utilizing exemplars, where the theoretical explanation shows
   We can note that all derived stopping criteria have a pos-       optimality in certain situations (Bailey & Jain, 1978). Our
terior representation where the sum over classes equals one.        model is also different from the random walk model using
Therefore, we can consider a C − 1 dimensional simplex and          conventional exemplar models (Nosofsky & Palmeri, 1997).
the diffusion of the posterior within this simplex. Therefore,      The random walk is performed according to the random re-
a vector with posterior elements for all candidate classes ex-      trieval from already generated data, while our model directly
tending Eq. (10) can be considered as a diffusing evidence in       considers the underlying density function and uses the gener-
a DDM model, and all criteria derived in this work can sub-         ated data without any additional randomness. A severe prob-
sequently be considered as DDM models.                              lem in the memory retrieval of Nosofsky and Palmeri is that
                                                                    a repetitive retrieval of one very similar exemplar will affect
   Relationship with other Exemplar Methods                         the decision predominantly where a noise on this particular
One typical method of learning with exemplars is utilizing the      exemplar can severely affect the decision accuracy.
similarity measures with exemplars (Nosofsky, 1986; Shep-
ard, 1987). Recently, this model was connected to kernel                      Experiments with Simulation Data
learning methods in machine learning (Jäkel, Schölkopf, &         The examples of diffusion of the evidence for each criteria
Wichmann, 2008), which connected the similarity notion to           are shown in Fig. 2. In this experiment, the proposed five
an associated reproducing kernel Hilbert space as well as           examples of evidence PV, PN, DV, CDV, and DN, diffuse with
to Bayesian inference (Shi, Griffiths, Feldman, & Sanborn,          the same NN information. In the figure, all five examples
2010). These similarity-based methods utilizing exemplars           diffuse differently, but they reach the same threshold. The
are computationally well-integrated with various machine            parameters used are λ1 = .25, λ2 = .35, and λ3 = .4, and
learning methods.                                                   the decision threshold is .8. Though they diffuse differently,
   However, majority voting with equal weights, which is pro-       CDV shows a smoother diffusion than the others, and PN and
posed in this work, is a completetly different approach of          DN show more sampling-wise configuration.
                                                               3155

                                                                                              Acknowledgments
                                                                   This research was supported in part by the AIM Center for
                                                                   Advanced Intelligent Manipulation and the ROSAEC-ERC.
                                                                                                   References
                                                                   Bailey, T., & Jain, A. K. (1978). A Note on Distance-Weighted k-Nearest
                                                                       Neighbor Rules. IEEE Transactions on Systems, Man, and Cybernetics,
                                                                       SMC-8(4), 311–313.
                                                                   Beck, J. M., Ma, W. J., Kiani, R., Hanks, T., Churchland, A. K., Roitman,
                                                                       J., et al. (2008, 26). Probabilistic population codes for Bayesian decision
                                                                       making. Neuron, 60(6), 1142–1152.
                                                                   Bogacz, R., Brown, E., Moehlis, J., Holmes, P., & Cohen, J. D. (2006). The
                                                                       physics of optimal decision making: A formal analysis of models of per-
                                                                       formance in two-alternative forced-choice tasks. Psychological Review,
                                                                       113(4), 700–765.
                                                                   Bogacz, R., Usher, M., Zhang, J., & McClelland, J. L. (2007). Extending
                                                                       a biologically inspired model of choice: multi-alternatives, nonlinearity
Figure 4: Performance using volume evidence. The CDV                   and value-based multidimensional choice. Philosophical Transactions of
                                                                       the Royal Society, Series B.
with smooth diffusion is slightly better than DV, while PV         Cover, T. (1967). Estimation by the nearest neighbor rule. IEEE Transac-
outperforms both CDV and DV with large margins.                        tions on Information Theory, 14(1), 50–55.
                                                                   Cover, T., & Hart, P. (1967). Nearest neighbor pattern classification. IEEE
                                                                       Transactions on Information Theory, 13(1), 21–27.
                                                                   Dragalin, V. P., Tertakovsky, A. G., & Veeravalli, V. V. (1999). Multihy-
                                                                       pothesis sequential probability ratio tests . part i: asymptotic optimality.
   The performance evaluation of the methods is shown us-              IEEETransactions on Information Technology, 45, 2448–61.
                                                                   Jäkel, F., Schölkopf, B., & Wichmann, F. A. (2008, 4). Generalization and
ing k-NN classification. We first generated data randomly              similarity in exemplar models of categorization: Insights from machine
from three uniform probability densities, λ1 = .2, λ2 = .7,            learning. Psychonomic Bulletin and Review, 15(2), 256-271.
                                                                   Leonenko, N., Pronzato, L., & Savani, V. (2008). A class of Rényi informa-
and λ3 = .1, and compared the adaptive k-NN classification             tion estimators for multidimensional densities. Annals of Statistics, 36,
method between the proposed criteria and other criteria from           2153–2182.
                                                                   Ma, W. J., Beck, J. M., Latham, P. E., & Pouget, A. (2006, 22). Bayesian in-
psychology and machine learning models. In Fig. 3, as ex-              ference with probabilistic population codes. Nature Neuroscience, 9(11),
pected in our analysis, the Race accumulator model without             1432–1438.
                                                                   Nosofsky, R. M. (1986). Attention, similarity, and the identification-
interaction does not outperform the criteria from statistical          categorization relationship. Journal of Experimental Psychology: Gen-
tests, PN and DN, although using a Race criterion does give            eral, 115(1), 39–57.
                                                                   Nosofsky, R. M., & Palmeri, T. J. (1997). An exemplar-based random walk
a better performance than a simple majority voting method              model of speeded classification. Psychological Review, 104(2), 266–
with fixed k. We also compared our results with a conven-              300.
                                                                   Ougiaroglou, S., Nanopoulos, A., Papadopoulos, A. N., Manolopoulos, Y.,
tional machine learning method, which considers the number             & Welzer-Druzovec, T. (2007). Adaptive k-nearest-neighbor classifica-
of recently appeared NNs belonging to the same class.                  tion using a dynamic number of nearest neighbors. In Proceedings of the
                                                                       11th east european conference on advances in databases and information
   In Fig. 4, three criteria using volume information, PV, DV,         systems (pp. 66–82).
                                                                   Ratcliff, R. (1978). A theory of memory retrieval. Psychological Review,
and CDV, are compared. According to a few realizations in              85, 59–108.
Fig. 1, the diffusion of CDV is in general much smoother           Ratcliff, R., & Mckoon, G. (2008). The diffusion decision model: theory
                                                                       and data for two-choice decision tasks. Neural Computation, 20(4), 873–
than that of DV, and the CDV criterion shows a little better           922.
accuracy than DV. PV shows better performance than either          Ratcliff, R., & Rouder, J. N. (2000). A diffusion model account of masking
DV or CDV.                                                             in two-choice letter identification. Journal of Experimental Psychology
                                                                       Human Perception and Performance, 26(1), 127–140.
                                                                   Shadlen, M. N., Hanks, A. K., Churchland, A. K., Kiani, R., & Yang, T.
                                                                       (2006). The speed and accuracy of a simple perceptual decision: a math-
                          Conclusion                                   ematical primer. Bayesian brain: Probabilistic approaches to neural
                                                                       coding.
                                                                   Shadlen, M. N., & Newsome, W. T. (1998). The variable discharge of corti-
                                                                       cal neurons: Implications for connectivity, computation, and information
In this work, a general framework integrating decision mak-            coding. Journal of Neuroscience, 18, 3870–3896.
ing with sequential sampling is proposed based on its rela-        Shepard, R. N. (1987). Toward a universal law of generalization for psy-
                                                                       chological science. Science, 237, 1317–1323.
tionship with the exemplar-type machine learning algorithm,        Shi, L., Griffiths, T. L., Feldman, N. H., & Sanborn, A. N. (2010). Exemplar
k-NN classification. In contrast to previous research on sub-          models as a mechanism for performing Bayesian inference. Psychonomic
                                                                       bulletin & review, 17(4), 443–464.
optimal weighted voting, we have shown how k-NN majority           Smith, P. L., & Vickers, D. (1988). The accumulator model of two-choice
voting can be used to better understand the sequential sam-            discrimination. Journal of Mathematical Psychology, 32, 135–168.
                                                                   Usher, M., & McClelland, J. L. (2001, July). The time course of perceptual
pling decision making process. Using an adaptive k-NN clas-            choice: the leaky, competing accumulator model. Psychological review,
sification framework, we also showed how the proposed five             108(3), 550–592.
                                                                   Vickers, D. (1970). Evidence for an accumulator model of psychophysical
examples of optimal criteria are derived for multiple-choice           discrimination. Ergonomics, 13, 37–58.
decision making, minimizing the error for any given average        Wald, A., & Wolfowitz, J. (1948). Optimum character of the sequential
                                                                       probability ratio test. Annals of Mathematical Statistics, 19, 326–339.
resource that can be used. Our future work includes extend-        Wasserman, L. (2003). All of Statistics: A Concise Course in Statistical
ing this relationship among decision making methods to form            Inference (Springer Texts in Statistics). Springer. Hardcover.
                                                                   Zhang, J., & Bogacz, R. (2010). Optimal decision making on the basis of
a scaffold of understanding within the mathematical frame-             evidence represented in spike trains. Neural Computation, 22(5), 1113–
work of k-NN methods.                                                  1148.
                                                               3156

