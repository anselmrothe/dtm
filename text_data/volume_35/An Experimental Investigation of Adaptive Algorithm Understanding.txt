UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Experimental Investigation of Adaptive Algorithm Understanding
Permalink
https://escholarship.org/uc/item/4q0637k8
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Terada, Kazunori
Yamada, Seiji
Ito, Akira
Publication Date
2013-01-01
Peer reviewed
  eScholarship.org                             Powered by the California Digital Library
                                                                University of California

            An Experimental Investigation of Adaptive Algorithm Understanding
                                               Kazunori Terada (terada@gifu-u.ac.jp)
                                           Department of Information Science, Gifu University
                                                     1-1 Yanagido, Gifu, Japan 501-1193
                                                      Seiji Yamada (seiji@nii.ac.jp)
                            National Institute of Informatics / SOKENDAI / Tokyo Institute of Technology
                                                   2-1-2 Chiyoda, Tokyo, Japan 101-8430
                                                        Akira Ito (ai@gifu-u.ac.jp)
                                           Department of Information Science, Gifu University
                                                     1-1 Yanagido, Gifu, Japan 501-1193
                               Abstract                                  goal (intention) is not required in human-computer collabo-
                                                                         ration because a goal is given and explicitly shared with both
   There have been few studies on a cognitive model for algo-            a human and a computer. Instead, algorithm level abstraction
   rithm understanding in a human-computer cooperative situa-
   tion. In the present study, we conducted an experiment with           is needed. In a human-computer collaboration task, under-
   participants to investigate the cognitive process of higher level     standing a computer’s algorithms in order to accomplish the
   abstraction (algorithm understanding) performed in a human-           given goal is quite important because a human relies on the
   computer collaboration task. The most recently used (MRU)
   algorithm, known to be one of the simplest adaptive algo-             computer’s underlying mechanisms in order to predict its be-
   rithms, and probabilistic MRU algorithm were used to test the         haviors and to adapt to it.
   human capability to understand an algorithm. The experimen-
   tal results showed that inductive reasoning in which partici-            One way to predict the future behavior of a target is to use
   pants observed the history of computer action, and they up-           input-output association acquired on the basis of sequence
   dated a statistical model while restricting their focus on a cer-     learning (Clegg, DiGirolamo, & Keele, 1998; Sun & Giles,
   tain history with deteministic bias and Markov bias played key
   role to correctly understand the MRU algorithm. The results           2001a; Winkler, Denham, & Nelken, 2009). In a typical se-
   also showed that deductive reasoning was used to understand           quence learning problem (Nissen & Bullemer, 1987), humans
   algorithms when participants rely on prior knowledge, and that        learn a recurring loop of action sequences from given exam-
   there was a case in which the algorithm, even known to be the
   simplest one, was never understood.                                   ples, and as a result, their reaction time for the given ex-
                                                                         amples decreases. This learning is done both explicitly and
   Keywords: algorithm understanding; inductive reasoning; de-
   ductive reasoning; adaptive user interface;                           implicitly (sensory-motor learning), and currently, implicit
                                                                         sequence learning is actively studied (Sun & Giles, 2001b).
                                                                         The situation in which humans observe only the action se-
                           Introduction                                  quences given to them is the same in both sequence learning
The number of situations in which humans collaborate with                and algorithm understanding. However, the learning target
computers has been increasing with the advance of informa-               of algorithm understanding is procedures with variables that
tion technology. Although user-adaptive systems that adapt               describe the internal states of computers, and this target is
to a user, including adaptive user interfaces, have been a main          quite different from that of sequence learning (i.e., sequence
topic in the human-computer interaction community and arti-              patterns of values). Obviously, the number of hypotheses in
ficial intelligence machine learning community (Findlater &              algorithm understanding is far more than that in sequence
McGrenere, 2004; Oviatt, Swindells, & Arthur, 2008; Bigde-               learning, and this makes algorithm understanding very hard.
lou, Schwarz, & Navab, 2012), an adequate design policy for              Hence, algorithm understanding requires quite strong biases
implementing useful user-adaptive systems still remains un-              to find adequate algorithms. Another difference between un-
clear (Shneiderman & Maes, 1997; Lavie & Meyer, 2010;                    derstanding cooperative algorithms and sequence learning is
Gajos, Everitt, Tan, Czerwinski, & Weld, 2008). Further-                 the type of interactivity in the tasks. In algorithm understand-
more, there have been few studies on a cognitive model for               ing in a cooperative situation, a computer’s behaviors change
algorithm understanding in the context of human-computer                 depending on the behaviors of humans because it adapts to
collaboration tasks.                                                     them. In sequence learning, sequences are given to humans
   In a human-human collaboration task, mutual intention un-             as physical stimuli.
derstanding plays the key role in accomplishing successful                  The research objective of this study is to build a cogni-
work (Byrne & Whiten, 1988; Call & Tomasello, 2008).                     tive model to describe the human capability to understand
However, in a collaboration task with a computer, the abstrac-           computer algorithms in the context of a human-computer col-
tion level of behavior necessary to understand a collaborator’s          laboration task. We introduce one of the simplest human-
behavior is lower than that used in a human-human collabo-               computer collaboration tasks, in which a computer adapts to
ration task (Dennett, 1987). Behavior abstraction in terms of            humans who are asked to try and understand the computer
                                                                     1438

algorithms. Concretely, we investigated how humans under-          Cooperative Mark-Matching Game
stand the most recently used (MRU) algorithm (Lee et al.,
                                                                   The cooperative mark-matching game is a repeated game
1999; Findlater & McGrenere, 2004; Gajos et al., 2008). The
                                                                   with two players. Each player has the same marks (e.g., ♠,
MRU algorithm is well known to be one of the simplest adap-
                                                                   ♢, ♡) and must secretly choose one of the marks. The play-
tive algorithms in which a computer’s current statement sim-
                                                                   ers then reveal their own choices simultaneously. If the marks
ply corresponds to the user’s last one. Examples of the imple-
                                                                   match each other, both players obtain a certain score, and if
mentation of the MRU algorithm are the most recently used
                                                                   not, nobody obtains a score. In our experiments, the two play-
files (Amer & Oommen, 2006), which lists the user’s most re-
                                                                   ers were a human and a user-adaptive system.
cently accessed files in an application, and the most recently
                                                                      In a situation of the human-computer adaptation, a system
used menu (called adaptive menu (Arcuri, Coon, Johnson,
                                                                   predicts the user’s next action (e.g., a menu item that will be
Manning, & Tilburg, 2000)), which lists the user’s most re-
                                                                   chosen next by a user in an adaptive menu (Findlater & Mc-
cently used menu.
                                                                   Grenere, 2004)) and adapts to him/her by modifying the user
   The MRU algorithm has succeeded in contributing to mak-         interface (e.g., changing the menu item positions (Findlater &
ing useful interactive software that includes adaptive user in-    McGrenere, 2004)). If the prediction is correct (i.e., the two
terfaces (Findlater & McGrenere, 2004). One reason is that it      marks of the human and user-adaptive system matched in the
can be easily understood by users. If users can not find any       game), the user and system obtain efficiency together. The
meaning (regularity or rules for computer’s behaviors) from        number of the mark corresponds to the number of menu items
a list in which the order of the items is frequently changed,      in the adaptive menu. The key difference between a cooper-
the list causes the user stress. The reason the MRU algorithm      ative mark-matching game and human-computer adaptation
is easily understood is that there are explicit descriptions of    with AUIs is that a user can freely choose his/her next action
the algorithm, i.e., there may be a description such as “most      by him/herself in the game in contrast to the user’s action se-
recently used file.” In this work, we investigate the human        quence being determined to achieve a task with AUIs.
ability to understand an algorithm in a situation without such        While the simplest strategy for a cooperative game is for
explicit knowledge.                                                participants in each trial to simply choose the action that
   One preferable explanation of algorithm understanding is        in the recent past gave the most rewards (known as rein-
induction because rule finding is considered to be an inductive    forcement learning), a more sophisticated strategy is to try
process (Haverty, Koedinger, Klahr, & Alibali, 2000; Simon         to predict the system’s next actions by taking into account
& Kotovsky, 1963; Verguts, Maris, & Boeck, 2002; Schmid            a statistical model constructed on the basis of the history of
& Kitzelmann, 2011). In general, induction needs to be done        prior actions. Studies on game theory (Fudenberg & Levine,
only with a small number of examples. It is hard to induce ad-     1998)(Berger, 2005) and sequence learning (Sun & Giles,
equate rules with finite examples that can cover infinite facts    2001a) with an opponent player (a user-adaptive system) in a
because there is a huge number of hypotheses of rules that         game situation suggest that opponent strategy is identified on
can be induced from the examples. Thus, we need heuristics         the basis of a mixed strategy, which is defined as a probabil-
(called inductive biases) to sufficiently restrict the hypothe-    ity distribution over the alternative actions available to each
sis of rules for practical induction. In algorithm understand-     player.
ing, since humans have to induce computer algorithms only
with tens of examples, we consider they have a strong bias         Statistical model
for algorithm understanding. In this paper, to investigate hu-     We hypothesize that, as mentioned earlier, a higher level ab-
man algorithm understanding, we hypothesize biases on al-          straction, i.e., algorithm identification, for a computer’s strat-
gorithm understanding and verify them in experiments with          egy is carried out on the basis of biases. We set the starting
participants.                                                      point of our discussion to statistics in which a human updates
                                                                   the conditional probability distribution of the system’s next
       Cognitive Model of Adaptive Algorithm                       choice over time.
                       Understanding
                                                                                     p(ats |at−1
                                                                                             s
                                                                                                 , · · · , asj , at−1
                                                                                                                  h
                                                                                                                      , · · · , ahk )     (1)
Adaptive algorithm understanding is a subclass of algorithm
understanding. An adaptation in human-computer interaction            , where ah , as ∈ A, and A are available choices for both the
refers to a feature of algorithms in which strategies of a com-    system and human and at−1       s , · · · , as and ah , · · · , ah are the
                                                                                                                   k            t−1   j
puter dynamically change according to user’s input in order to     past choices of the system and human, respectively. Indices j
pursue given goals. The goals refer not only corporation but       and k denote the length of the history, which the human takes
also competition (Hampton, Bossaerts, & O’Doherty, 2008).          into account, and vary depending on focus. However, detect-
In the present study, we focus on a cooperative situation. We      ing the computer’s algorithm on the basis of only observed
introduce a cooperative mark-matching game as a simplified         behaviors is an ill-posed inverse problem because humans do
and generalized task of human-computer adaptation in which         not know how to restrict their focus to a certain history, and
a user adapts to a user-adaptive system.                           in addition, different strategies sometimes produce the same
                                                               1439

Table 1: Conditional probability distributions correspond to
most recently used and probabilistic most recently used algo-
rithm                                                                                                 4
                    h                                    h
                   at−1                                at−1                                                               3
             ♡      ♠     ♢                      ♡       ♠    ♢
        ♡     1      0     0               ♡     .9     .05  .05              1
  ats   ♠     0      1     0        ats    ♠    .05      .9  .05
        ♢     0      0     1               ♢    .05     .05   .9
               (a) MRU                  (b) Probabilistic MRU
                                                                                                                     2
history. Thus, we consider that a human does sufficiently re-
stricted exploration with inductive biases.
                                                                        Figure 1: Interface of on-line experimental system: 1) history
   The MRU algorithm is formalized as the following distri-             of both players’ choices, 2) choice marks (marks are click-
bution.                                                                 able), 3) round number and remaining time, 4) place for un-
                            p(ats |at−1
                                    h
                                        )                       (2)     veiling players’ choice and scores for both players
   The actual distribution produced by the MRU algorithm in
the cooperative mark-matching game is shown in Table 1(a).              actual distribution produced by the probabilistic MRU algo-
The system’s choice (ats ) depends only on the human’s most             rithm is shown in Table 1(b).
recent choice (at−1h ) and is independent from any other his-
                                                                           The P condition was prepared to contrast the effect of noise
tory of choices. If the human’s most recent choice is heart,            on the inductive reasoning performed to understand the MRU
for example, the system’s next choice will be heart, repre-             algorithm. In particular, we expected that the deterministic
sented as p(ats = ♡|at−1h = ♡) = 1. Infinite numbers of trials
                                                                        bias was strongly affected by the noise and performance dete-
are, theoretically, required to convince a human that the prob-         riorated in the P condition. It was also expected that the score
ability is 1. Hence, one reasonable strategy for this problem           of those who participated in the P condition was at most 10%
is to use inductive biases to adequately control the inference          worse than that of the D condition if the participants merely
process. As such inductive biases, we consider deterministic            estimated the probability distribution and did not use any bi-
bias and Markov bias. If a human has a deterministic bias that          ases to identify an algorithm.
assumes computer’s behaviors are deterministic, not proba-
bilistic, only one piece of evidence is necessary to estimate           Experimental setup and measurement
the probability distribution. Markov bias, in which the condi-          The game was implemented with JavaScript and HTML and
tional probability distribution of the next choice depends only         played in a Web browser (Firefox). Figure 1 shows the game
upon the present choice, not on the sequence of events, is also         interface. The computer’s choices were automatically con-
necessary to ignore any unnecessary history of choice.                  trolled by a JavaScript program. Participants were instructed
                                                                        to click the mark corresponding to his/her choice within 10
                         Experiments                                    seconds for every round. Scores for both players were shown
We conducted an experiment with participants to investigate             in the interface. The choices of the past five rounds for both
the cognitive process of higher level abstraction (algorithm            players remained displayed so that the participant was able to
identification) performed in the context of a human-computer            recognize the computer’s strategy.
cooperation task. The MRU algorithm and probabilistic MRU                  A single-factor two-level between-subject experimental
algorithm was used to test the human capability of algorithm            design was used. Fifty people (9 female) aged 19 to 47 (mean
understanding. Participants were asked to play a cooperative            = 28) recruited via direct e-mail participated in the experi-
game with a computer, and after that they were asked to an-             ment. All participants had moderate to high experience using
swer the computer’s algorithm.                                          computers. Participants were randomly assigned to either a
   A 50-round repeated cooperative mark-matching game                   deterministic or probabilistic condition. Participants were in-
with different statistical profiles of the MRU algorithm was            formed of an ostensible goal of the experiment - that the point
used. We used the following two conditions.                             of the experiment was to assess the usability of an on-line
                                                                        game system. They were also informed that “the computer
Deterministic (D) condition Computer’s choice is com-
                                                                        was cooperative.” Participants were told that they would win
pletely the same as the human’s most recent choice (deter-
                                                                        a PC gadget as a prize according to the score (under 20 points:
ministic MRU algorithm, see Table 1(a)).
                                                                        around $5, 21 to 44 points: around $15, 45 to 50: around
Probabilistic (P) condition Although 90% of the com-                    $30).
puter’s choices are the same as the human’s most recent                    In the P condition, a 50-round sequence with 10% random
choices, 10% differs (probabilistic MRU algorithm). The                 noise, which corresponds to 5 rounds in which MRU rules
                                                                    1440

                     100                                                                       100
                                                                                                                                          percentage in the P condition indicates that the 10% noise in
                                                                                                     Correct Solution Percentage (%)
                                                                                                                                          the MRU algorithm caused the computer’s algorithm to be
                          80                                                                   80
 Winning Percentage (%)
                                                                                                                                          difficult to identify and made the participants require longer
                          60                                                                   60
                                                                                                                                          rounds to identify it.
                                                                                                                                             Figure 2 shows the percentages of the participants who
                          40                                                                   40                                         won the round plotted against the round numbers (solid line).
                                                                                                                                          The dotted line in Figure 2 represents the percentage of par-
                          20                                              Deterministic        20                                         ticipants who took a “fixed choice strategy,” indicating the
                                                                          Probabilistic                                                   percentage of participants who became aware of the cor-
                          0                                                                    0                                          rect solution to the game. Note that the correct solution is
                               1   5        10   15   20 25 30       35     40     45     50
                                                      Round Number
                                                                                                                                          found not only by identifying the MRU algorithm, but also
                                                                                                                                          by merely choosing the same mark without thought.
Figure 2: Percentage of participants who won each round                                                                                      Figure 3 illustrates the computer’s algorithm identified by
(solid line) and percentage of participants who started to                                                                                participants. While 72% of participants in the D condition
take a “fixed choice strategy” (correct solution to the game)                                                                             correctly identified the MRU algorithm after 50 rounds, only
throughout the remaining rounds (dotted line)                                                                                             52% in the P condition succeeded in identifying it. However,
                                                                                                                                          a chi-square test revealed that there was no statistically signif-
                                                                                  MRU                                                     icant difference in the distribution of the identified algorithm
 Deterministic                                                                    Regular pattern                                         between the two conditions (χ2 (4) = 3.41, p = 0.49).
                                                                                  No strategy
          Probabilistic                                                           Most frequent                                                                    Discussions
                                                                                  Fixed choice                                            In the present study, we investigated the human capability to
                                       0%               50%               100%                                                            understand the MRU algorithm. In particular, we expected
                                                                                                                                          that inductive biases such as deterministic and Markov bias
      Figure 3: Computer’s algorithm identified by participants                                                                           are used to understand the algorithm. In the succeeding sub-
                                                                                                                                          sections, we will discuss whether these biases were applied
                                                                                                                                          to accomplish the game.
are violated, is generated, and sequences that do not fit the
following criteria are omitted: 1) errors do not appear in the                                                                            Inductive algorithm understanding
first and last 5 rounds and 2) five errors appear within the re-                                                                          The red dotted line in Figure 2 reveals that 60% of partici-
maining 40 rounds. The computer’s choice for the first round                                                                              pants (15 participants) in the D condition found the correct
was selected not to match the participant’s choice in both con-                                                                           solution to the game. The result of the questionnaire revealed
ditions.                                                                                                                                  that while 13 of the 15 participants inferred the computer’s
   The outcomes of all 50 rounds were recorded. The round                                                                                 algorithm as the MRU, one inferred no strategy, and one in-
in which participants became aware of the correct solution to                                                                             ferred a fixed choice. A typical behavioral pattern for these
the game was identified by detecting the round in which par-                                                                              kinds of participants is shown in Figure 4(a). They observed
ticipants started to continue to select the same mark through-                                                                            the history of the choices and might have inferred the MRU
out the remaining rounds. After the game, participants were                                                                               algorithm on the basis of the obtained statistical model. How-
asked to answer 7-point Likert scale questions, such as Q. Did                                                                            ever, while detecting a statistical model of the computer’s
the computer make its choices strategically?, and one open-                                                                               strategy essentially requires an infinite number of trials, they
ended question if participants gave a rating of 5 to 7 (positive)                                                                         rapidly identified certain algorithms. One explanation for this
to this question - Describe the computer’s strategy.                                                                                      rapid identification is the deterministic bias and Markov bias.
                                                                                                                                          If the algorithm was assumed to be deterministic, the partici-
Results                                                                                                                                   pants did not need to take into account the six cases filled out
The average scores were 43.7 (SD = 7.0) in the D condition                                                                                as zero in Table 1(a) and required at least three trials to deter-
and 31.4 (SD = 7.5) in the P condition. ANOVA revealed                                                                                    mine the computer’s strategy. Without Markov bias, partici-
that there was statistically significant difference (F(1, 48) =                                                                           pants could not focus only on the one round past choice and
33.99, p < 0.01) between the two conditions. The difference                                                                               required longer rounds.
of the average scores between the two conditions was 12.3.                                                                                   The deterministic bias also accounts for the worse perfor-
A difference of more than 5 (10%) indicates that participants                                                                             mance of those who participated in the P condition. If the
used deterministic bias to accomplish the game. This gap                                                                                  participants merely estimated the probability distribution, as
is explained by the difference in the increasing rate of the                                                                              expected, an optimal strategy against a mixed strategy would
winning percentage. While the winning percentage of the                                                                                   have been taken, and performance would have been at most
D condition rapidly reached a high value (e.g., 80% at the                                                                                10% worse than in the D condition.
sixth round), that of the P condition slowly increased (e.g.,                                                                                The lowest score for all 50 participants was 19, which was
80% at the 35th round). The slower increase of the winning                                                                                higher than the theoretically calculated score (16.67) when
                                                                                                                                       1441

         C
         H
        (a) Understanding algorithm on the basis of inductive reasoning (correct identification). After eight trials of active learning
        phase, the participant realized the algorithm was the MRU one.
         C
         H
        (b) Understanding algorithm on the basis of inductive reasoning (wrong identification). The detected algorithm was “the
        computer increased the number of times by repeating the same choice.”
         C
         H
        (c) Understanding algorithm on the basis of deductive reasoning. The participants used a heuristic from the beginning:
        “Adaptive system ⇒ MRU algorithm.”
         C
         H
                                                (d) The participant did not detect any algorithm.
                   Figure 4: Examples of typical behavioral pattern in the D condition. C: computer, H: human.
participants did not take any strategy, i.e., a random strat-              efficient algorithm for human-computer cooperation was the
egy. This implies that almost all of the participants arbitrar-            MRU algorithm. In fact, their MRU algorithm hypothesis
ily attributed some kind of strategy to the computer’s choice.             was confirmed by the computer’s succeeding choice. The
In fact, the rules of the game allowed the participants to at-             confirmation bias (Klayman & Ha, 1987) was used to con-
tribute strategies other than the MRU algorithm, such as “the              vince them that the computer used the MRU algorithm. They
computer simply selected the same mark” (fixed choice strat-               marked the highest score 49 (all participants were sure to lose
egy) and “the computer changed its choice alternatively” or                the first round because of the game setting). There was no in-
“the computer increased the number of times by repeating the               centive to explore another strategy and gather evidence to test
same choice such as ♢♠♠♡♡♡” (increasing number strat-                      another hypothesis unless their hypothesis was violated be-
egy), see Figure 4(b)). Three participants in the D condi-                 cause their goal was to get as many points as possible and
tion answered that the computer’s algorithm was “increasing                not to detect the algorithm exactly. Indeed, while three par-
number strategy.” Interestingly, they did not aware that the               ticipants in the P condition started to fix their choice in the
timing to change the mark was determined by themselves.                    first round, two of the three changed their choice after the
They completely unaware of the rule in which the computer                  noise pattern in the computer’s choice appeared, indicating
changed its output according to their input.                               that their confirmation bias was destroyed by the noise (falsi-
                                                                           fication).
Deductive algorithm understanding
The results also indicated that some participants understood
                                                                           Algorithm detection fail
the algorithm on the basis of deductive reasoning. Sixteen                 Surprisingly, even though the MRU has been supposed to be
percent of participants (four participants) in the D condition             one of the most predictable adaptation algorithms, the result
and four percent (one participant) in the P condition fixed                showed that two participants in the D condition and three in
their choice in the first round and never changed during the               the P condition failed to identify any strategy in the 50 rounds
game (see Figure 4(c)). Surprisingly, all of them described                (see Figure 4(d)). The visual cue shown in the history area in
their identified computer algorithm as the MRU. The prior                  the game s interface might have been a strong cue indicat-
knowledge given to the participants in the instruction phase               ing that the computer’s choice was the same as participant’s
lead them to deduct the following logic:                                   choice one round before. However, they could not detect the
                                                                           algorithm. Further investigation will be required to account
              Adaptive system ⇒ MRU algorithm                     (3)      for this failure.
   In the instruction phase, participants were explicitly in-                                        Summary
formed that the goal of the task was to get as much points                 To the best of our knowledge, this is the first study to in-
as possible in cooperation with the partner computer. This                 vestigate the human capability to understand adaptive algo-
top down adaptive bias might have enabled them to identify                 rithm in a human-computer collaboration task. In the theoret-
the algorithm immediately without exploring the computer’s                 ical model of a human cognitive process for algorithm under-
strategies. They might have logically inferred that the coop-              standing, a user identifies a computer’s algorithm by estimat-
erative system acted adaptively to humans and that the most                ing the conditional probability distribution associated with a
                                                                      1442

particular strategy and restricting his/her focus on certain his-    Gajos, K. Z., Everitt, K., Tan, D. S., Czerwinski, M., & Weld,
tory data by using inductive biases. The most recently used            D. S. (2008). Predictability and accuracy in adaptive user
(MRU) algorithm, known to be one of the simplest adaptive              interfaces. In Proceeding of the 26th annual sigchi con-
algorithms, was used to test the human capability to under-            ference on human factors in computing systems (pp. 1271–
stand an algorithm. The probabilistic MRU algorithm was                1274).
also used to contrast the effect of noise on the inductive rea-      Hampton, A. N., Bossaerts, P., & O’Doherty, J. P. (2008).
soning performed to understand the MRU algorithm. The ex-              Neural correlates of mentalizing-related computations dur-
perimental results indicated that most participants correctly          ing strategic interactions in humans. Proceedings of the
identified the MRU algorithm and used deteministic bias and            National Academy of Sciences, 105(18), 6741–6746. (fic-
Markov bias in their inductive reasoning for algorithm iden-           titious play, Reinforcement Learning)
tification. The results also indicated that some participants        Haverty, L. A., Koedinger, K. R., Klahr, D., & Alibali, M. W.
understood the algorithm on the basis of deductive reasoning.          (2000). Solving inductive reasoning problems in mathe-
Surprisingly, few participants failed to identify any algorithm        matics: Not-so-trivial pursuit. Cognitive Science, 24(2),
within 50 rounds.                                                      249–298.
    The present findings implies that designed behavior of           Klayman, J., & Ha, Y. won. (1987). Confirmation, disconfir-
computers is not necessarily understood correctly, suggesting          mation, and information in hypothesis testing. Psychologi-
that both an understandable algorithm and transparency of the          cal Review, 94(2), 211–228.
internal state of a computer might be important for designing        Lavie, T., & Meyer, J. (2010). Benefits and costs of adaptive
effective adaptive systems.                                            user interfaces. International Journal of Human-Computer
                                                                       Studies, 68, 508–524.
                      Acknowledgments                                Lee, D., Choi, J., Kim, J.-H., Noh, S. H., Min, S. L., Cho,
                                                                       Y., et al. (1999). On the existence of a spectrum of poli-
This work was supported by MEXT KAKENHI Grant Num-
ber 24118703.                                                          cies that subsumes the least recently used (lru) and least
                                                                       frequently used (lfu) policies. In Proceedings of the 1999
                          References                                   acm sigmetrics international conference on measurement
                                                                       and modeling of computer systems (pp. 134–143). New
Amer, A., & Oommen, B. J. (2006). Lists on lists: A frame-             York, NY, USA: ACM.
   work for self-organizing lists in environments with locality      Nissen, M., & Bullemer, P. (1987). Attentional requirements
   of reference. In Proceedings of the 5th International Work-         of learning: Evidence from performance measures. Cogni-
   shop on Experimental Algorithms (pp. 109–120).                      tive psychology, 19(1), 1–32.
Arcuri, M., Coon, T., Johnson, J., Manning, A., & Tilburg,           Oviatt, S., Swindells, C., & Arthur, A. (2008). Implicit user-
   M. van. (2000). Adaptive menus. (US Patent: 6,121,968).             adaptive system engagement in speech and pen interfaces.
Berger, U. (2005). Fictitious play in 2×N games. Journal of            In Proceedings of the 26th annual sigchi conference on hu-
   Economic Theory, 120, 139–154.                                      man factors in computing systems (pp. 969–978).
Bigdelou, A., Schwarz, L., & Navab, N. (2012). An adaptive           Schmid, U., & Kitzelmann, E. (2011). Inductive rule learning
   solution for intra-operative gesture-based human-machine            on the knowledge level. Cognitive Systems Research, 12(3–
   interaction. In Proceedings of the 17th International Con-          4), 237–248.
   ference on Intelligent User Interfaces (pp. 75–84).               Shneiderman, B., & Maes, P. (1997). Direct manipulation vs.
Byrne, R. W., & Whiten, A. (1988). Machiavellian intel-                interface agents. Interactions, 4(6), 42–61.
   ligence: Social expertise and the evolution of intellect in       Simon, H. A., & Kotovsky, K. (1963). Human acquisition
   monkeys, apes, and humans (R. W. Byrne & A. Whiten,                 of concepts for sequential patterns. Psychological Review,
   Eds.). Oxford Science Publications.                                 70(6), 534–546.
Call, J., & Tomasello, M. (2008, May). Does the chimpanzee           Sun, R., & Giles, C. (Eds.). (2001b). Sequence learn-
   have a theory of mind? 30 years later. Trends in Cognitive          ing: Paradigms, algorithms, and applications (Vol. 1828).
   Sciences, 12(5), 187–192.                                           Springer.
Clegg, B. A., DiGirolamo, G. J., & Keele, S. W. (1998). Se-          Sun, R., & Giles, C. L. (2001a). Sequence learning: From
   quence learning. Trends in Cognitive Sciences, 2(8), 275–           recognition and prediction to sequential decision making.
   281.                                                                IEEE Intelligent Systems, 16(4), 67–70.
Dennett, D. C. (1987). The intentional stance. Cambridge,            Verguts, T., Maris, E., & Boeck, P. D. (2002). A dynamic
   Mass, Bradford Books/MIT Press.                                     model for rule induction tasks. Journal of Mathematical
Findlater, L., & McGrenere, J. (2004). A comparison of                 Psychology, 46(4), 455–485.
   static, adaptive, and adaptable menus. In Proceedings of the      Winkler, I., Denham, S. L., & Nelken, I. (2009). Modeling the
   11th international conference on intelligent user interfaces        auditory scene: predictive regularity representations and
   (pp. 89–96).                                                        perceptual objects. Trends in Cognitive Sciences, 13(12),
Fudenberg, D., & Levine, D. K. (1998). The theory of learn-            532–540.
   ing in games. MIT Press, Cambridge, MA.
                                                                 1443

