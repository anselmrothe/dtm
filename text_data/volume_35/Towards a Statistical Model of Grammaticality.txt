UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Towards a Statistical Model of Grammaticality
Permalink
https://escholarship.org/uc/item/0s47f6vj
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Clark, Alexander
Giorgolo, Gianluca
Lappin, Shalom
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                Towards a Statistical Model of Grammaticality
                                   Alexander Clark, Gianluca Giorgolo, and Shalom Lappin
                                                          firstname.lastname@kcl.ac.uk
                                            Department of Philosophy, King’s College London
                               Abstract                                    inal BNC sentences and their permuted variants in which a
   The question of whether it is possible to characterise grammat-
                                                                           word in each sentence is randomly exchanged with another
   ical knowledge in probabilistic terms is central to determin-           word three positions away from it. These distortions consti-
   ing the relationship of linguistic representation to other cog-         tute syntactic infelicities. The first case involves gross struc-
   nitive domains. We present a statistical model of grammati-             tural ill-formedness similar to the word salad example, while
   cality which maps the probabilities of a statistical model for
   sentences in parts of the British National Corpus (BNC) into            the second introduces subtler, more local mistakes. We score
   grammaticality scores, using various functions of the parame-           the test corpora using three alternative conditions. Our binary
   ters of the model. We test this approach with a classifier on test      classifiers predict that a string is well-formed (original) or dis-
   sets containing different levels of syntactic infelicity. With ap-
   propriate tuning, the classifiers achieve encouraging levels of         torted (either reversed or permuted) on the basis of a score de-
   accuracy. These experiments suggest that it may be possible to          rived through normalising its log probability (logprob) value
   characterise grammaticality judgements in probabilistic terms           in various different ways. We also test different standard de-
   using an enriched language model.
                                                                           viations from the distributional norm in setting cut off points
   Keywords: enriched language models, probability distribu-
   tion, grammaticality judgements, probabilistic syntax                   for our binary classifiers In our best cases we obtain an accu-
                                                                           racy rate of 98.9% for the original-reversal test set, and 79.1%
Introduction                                                               for the original-permutted test set.
The past two decades have seen a lively debate over whether                   These results suggest that by looking at the internal com-
linguistic knowledge is probabilistic or categorically rule-               ponents of a probability distribution and the stages through
based in nature (see the papers in Bod, Hay, and Jannedy                   which it is computed we can identify additional information
(2003) for some of this discussion). Given the success of                  that may be used to specify significant correlations between
probabilistic accounts of learning, representation, and infer-             probability and grammaticality. This opens up an interesting
ence across a wide range of cognitive domains, this debate                 set of research questions on the relationship between speak-
has considerable importance for the way in which knowledge                 ers’ knowledge of the probability distribution for a language
of language is integrated into our general view of human cog-              and their grammaticality judgements.
nition.
   On the classical view of syntax developed within linguistic             Probability and Grammaticality
theory over the past sixty years, the grammaticality and the               As has often been noted, it is not possible to reduce gram-
probability of a sentence are entirely distinct properties with            maticality directly to probability. First, short ungrammati-
no direct relationship. Chomsky (1957) presents the origi-                 cal sentences generally receive higher probability values than
nal argument for the irrelevance of probability in determining             long, complex grammatical sentences containing words with
grammaticality.1 This argument depends on the inability of                 low frequencies. Second, if one specifies a probability value
a simple word n-gram model to predict a distinction in prob-               (or even a range of such values) as the minimal threshold for
ability between a syntactically well-formed but unlikely (se-              grammaticality, then one is committed to the existence of a fi-
mantically anomalous) sentence like Colorless green ideas                  nite number of grammatical sentences. The sum of the prob-
sleep furiously, and a word salad like Furiously sleep ideas               abilities of the possible strings of words in a language sum to
green colorless. Pereira (2000) shows that a smoothed class-               1, and so at most 1/ε sentences can have a probability of at
based n-gram model trained on a newspaper corpus predicts                  least ε.
a significant distinction in probability between the two sen-                 On the other hand, probabilistic inference does appear to
tences.                                                                    be pervasive throughout all domains of cognition (Chater,
   While it is certainly the case that grammaticality cannot be            Tenenbaum, and Yuille (2006)). Moreover, language mod-
directly reduced to probability, the question of whether there             els do seem to play a crucial role in speech recognition and
is a significant correlation between the two remains open and              sentence processing. Without them we would not be able to
interesting. Our general approach is as follows. We train                  identify speech sounds, and meaningful syntactic and seman-
a smoothed class-based trigram model on a filtered subclass                tic structures in noisy environments. Finally, grammatical-
of the BNC. We test this model on two corpora. One is di-                  ity appears to track speakers’ acceptability judgements, and
vided into original sentences of part of the BNC and their re-             these are, in many cases, graded. Probability provides a nat-
versed counterparts. The second consists of a subset of orig-              ural basis for generating such a gradient (Crocker and Keller
    1 See Fong, Malioutov, Yankama, and Berwick (2013) for a re-           (2006)).
cent discussion of some of the issues involved in identifying gram-           Our starting point is a language model: a statistical model
maticality with probability of occurrence.                                 that defines a probability distribution over sentences.
                                                                       2064

   We construct a log-linear model, parameterised by some                   preceding words: θwi |wi−1 wi−2 . To compute the probability of
vector of parameters Θ = hθ1 , . . . , θk i. This framework cov-            a sentence we sum the relevant parameters to obtain the log
ers a wide range of different models from n-gram models to                  probability. For a sentence hw1 , . . . , wn i the log probability is
PCFG s.2
                                                                                                                         n
   The probabilities defined by this model cannot be used to                         log PTRIGRAM (hw1 , . . . , wn i) = ∑ θwi |wi−1 wi−2
define a notion of grammaticality for several reasons. First,                                                           i=1
as the sentences increase in length, the probability of the sen-
tence will always decrease exponentially, for sufficiently long                We take the sequence of relevant parameters
sentences, while we assume that long sentences can be as                    hθw1 |w0 w−1 , . . . , θwn |wn−1 wn−2 i, and, rather than summing
grammatical as short sentences. Second, one can often substi-               them, we perform other computations. We consider the aver-
tute a rarer semantically related word for an open class word               age or the minimum of the set of parameters as alternatives
of the same POS without affecting grammaticality, but the                   for defining values that correspond to grammaticality.
substitution will reduce probability. Figure 1 shows that the                  Our most basic score is the mean of this value, the logprob
log probabilities for sentences that have been reversed or per-             divided by the word length of the sentence:
muted, and are thus generally ungrammatical, overlap com-                                            1
pletely with the log probabilities of normal sentences (see the                     Meanlogprob =      log PTRIGRAM (hw1 , . . . , wn i)
                                                                                                     n
next section for details of the experimental protocols). We
                                                                               This eliminates the dependence of the logprob on the
need to augment our model with an additional component to
                                                                            length. Our next score divides the logprob of the original tri-
convert probability into a score that correlates with grammat-
                                                                            gram model by the logprob with respect to a unigram model.
icality in an interesting way.
                                                                                                       log PTRIGRAM (hw1 , . . . , wn i)
                                                                                      Normalised =
             0.015                                                                                     log PUNIGRAM (hw1 , . . . , wn i)
                                                                               This removes the variation in logprob caused by rare lex-
                                                                            ical items. Note that if the unigram model is uniform (if
                                                                            we had equal numbers of each word in the training corpus),
                                                                            then the log of the unigram model would be a multiple of the
             0.010                                                          length, and so it would reduce to the previous value.
                                                                               Our third score uses the observation that a sentence with
                                                           Condition
                                                                            one grammatical error in it is ungrammatical. In order to mea-
   density
                                                              original
                                                              permuted
                                                                            sure grammaticality we look at the minimum of some score
                                                              reversed
                                                                            over the parts of the sentence. We take the minimum of the
             0.005                                                          ratio of the log trigram probability to log unigram probability.
                                                                                                              log θwi |wi−1 wi−2
                                                                                                                                
                                                                                           Minimum = min
                                                                                                         i        log θwi
                                                                               None of these measures will produce a score which is in
             0.000                                                          the range [0, 1], though it would be possible to map them into
                 −500   −400   −300      −200   −100   0
                                                                            this range. This value will also vary even for grammatical
                                  Log prob
                                                                            sentences. The scores will be numbers that are distributed in
                                                                            some way. Figure 2 shows the distribution of these scores for
Figure 1: Histograms for the distributions of log probabilities             the test data. As this score specifies a continuum of values, we
under the three conditions.                                                 are able to accommodate a gradient notion of grammaticality.
                                                                               Given these three measures we use various standard tech-
   We use statistical properties of the parameters of the model.            niques to see whether new sentences are anomalous or not.
In order to compute the probability of a sentence with respect              For a collection of naturally occurring grammatical sentences
to a model we do calculations on the parameters. For a log                  we train our models, and then we consider the distribution of
linear model, this gives a linear function of certain indicator             these scores. We estimate the mean and standard deviation
variables; a weighted sum. To compute a score that correlates               of the score. We can then judge new sentences as ungram-
with grammaticality, we consider other functions, such as a                 matical if they are unusually low in score- more than a few
weighted mean, or a minimum over certain scores.                            standard deviations away from the mean.
   In a trigram model each parameter will then correspond to                   Pauls and Klein (2012) apply a related approach to another
the log conditional probability of one word, given the two                  problem. They use scores based on the logprob values of a
   2 We use smoothing techniques that, in general, can take the             language model to discriminate between grammatical and un-
model outside the class of log-linear models, but we pass over this         grammatical sentences in order to improve the performance
technical detail here.                                                      of natural language processing systems.
                                                                         2065

Experiments and Results                                                   The probability P(wi |Ci ) is given by the count of occurrences
For our experiments, we use the standard n-gram language                  of wi divided by the count of occurrences of Ci , while the
model, which is an instance of a Markov model for sequences.              other factor of the product can be estimated with a smoothed
To estimate the probability of a sequence of words w1 . . . wk            model like Interpolated Kneser-Ney. Brown et al. (1992) de-
we use the chain rule of probability, as in (1).                          scribes a technique for generating the optimal clustering in a
                                                                          corpus, given a parametrically specified number of classes.
    P(w1 . . . wk ) = P(w1 )P(w2 |w1 ) . . . P(wk |w1 . . . wk−1 ) (1)       We implemented our own procedures for the training and
                                                                          the assessment of n-gram language models, using Interpo-
The problem with this approach is that we have to estimate                lated Kneser-Ney as the smoothing technique. For cluster-
the conditional probability of an extremely large number of               ing we applied the improved version of Brown et al. (1992)’s
possible subsequences. Therefore a common method is to                    algorithm described in Liang (2005).
reduce the conditional dependencies to a smaller predefined                  Both the training of the language models and the measure-
sequence of a given length n, the so called order of a model.             ment of their performance in the given tasks are performed
Using this assumption we approximate the components in (1)                on portions of the BNC. The BNC is a heterogenous collec-
using (2).                                                                tion of linguistic data. To obtain a more consistent sample
                                                                          of English we first restricted the available texts by exclud-
           P(wi |w1 . . . wi−1 ) ≈ P(wi |wi−n+1 . . . wi−1 )       (2)    ing transcriptions of spoken language, poetic texts and tech-
                                                                          nical/scientific material. The corpus used for training and the
The probability assigned to a sequence of words is given by
                                                                          one used for testing were generated from this subset of the
the product in (3)
                                                                          BNC by randomly selecting 600k sentences for training, and
                                k                                         60k for testing. This gave us a training corpus of slightly less
             P(w1 . . . wk ) ≈ ∏ P(wi |wi−n+1 . . . wi−1 )         (3)    than 13 million words, and a testing corpus of approximately
                               i=1                                        1.3 million words.
                                                                             To avoid the problem of unknown words in testing, we re-
A common choice for n, that we adopt for our experiments,
                                                                          constructed both the training and the testing corpus. We sub-
is three (trigrams).
                                                                          stituted, in both the training and the test corpus, the POS tag
   The standard strategy to estimate the probability of each n-
                                                                          for each word which appears less than five times in the train-
gram is maximum likelihood estimation (MLE), which counts
                                                                          ing corpus. This insures that the test corpus vocabulary is a
the number of times the n-gram appears in a training corpus,
                                                                          subset of the training corpus vocabulary.
and normalizes the count by the sum of the counts of all n-
                                                                             Three different types of test corpus (conditions) were gen-
grams that share the same initial subsequence:
                                                                          erated. The original condition is left intact, and we assume
                                         C(wi−n+1 . . . wi )              that it contains only grammatical sentences. The permuted
         P(wi |wi−n+1 . . . ww−1 ) =                               (4)    condition is generated from the original by randomly swap-
                                      ∑w C(wi−n+1 . . . w)
                                                                          ping two words, separated by two intervening words, in each
To avoid assigning 0 probability to unseen n-grams (a com-                sentence. The sentences in this corpus are taken to be less
mon case, given the huge number of possible n-grams) we                   grammatical than those in the original condition. Finally, the
use smoothing or discounting, which transfers a small por-                third test corpus was produced by reversing the order of the
tion of probability mass from seen n-grams to unseen ones.                words in the original sentences. This reversed condition is
A large number of smoothing techniques have been pro-                     considered to be the most syntactically distorted of the three.
posed in the literature (see Chen and Goodman (1999) for                     We used a simple binary classifier to measure the perfor-
a thorough overview). In our experiments we use a form                    mance of our language model in predicting the grammatical-
of interpolated smoothing known as Interpolated Kneser-Ney                ity of a sentence. After calculating the three scores (Mean
(Goodman (2001)), which has been shown to give consis-                    log prob, Normalised, and Minimum) in all three conditions
tently good results with different types of metrics.                      we designed two different binary classifiers that assign a la-
   To reduce the search space of our language model we also               bel to every sentence in each condition. The first classifier
employ clustering, which groups together words that occur                 is a simple threshold set to different values for the mean and
in similar contexts. In this way we can better estimate the               the standard deviation of the distributions of the alternative
probability of a word following a certain sequence, given the             normalised scores for the original condition. For each binary
observations we have made of similar words in the same con-               comparison the classifier assigns a label to the sentence z us-
text. Brown, deSouza, Mercer, Pietra, and Lai (1992) intro-               ing the following rule:
duced the standard technique for using clustering in language                                 (
models. The general form of a cluster-based language model                                     original if score(z) ≥ m − S · s
                                                                                     c1 (z) =                                          (6)
is given in equation (5), where Ci is the cluster to which word                                other      otherwise
wi is assigned to.
                                                                          where m is the mean for the score in the original condition, s
   P(wi |wi−n+1 . . . wi−1 ) = P(wi |Ci )P(Ci |Ci−n+1 . . .Ci−1 ) (5)     is the standard deviation and S is a factor by which we move
                                                                      2066

the threshold away from the mean. The principle of this clas-
                                                                                      Table 1: Linear classifier accuracy
sifier is that the normalised logprob scores for ungrammati-
cal sentences will be lower than those for grammatical ones,               Accuracy                           permuted    reversed
making it possible to distinguish between the two conditions.              Mean log prob + Normalised              71.2       97.9
We adapted this procedure to distinguish between local un-                 Mean log prob + Minimum                 77.1       97.2
grammaticality (permutation), and more global ungrammati-                  Normalised + Minimum                    79.1       98.1
cality (reversed cases).                                                   Threshold classifier baseline           77.3       98.9
    The second classifier is a simple linear classifier con-
structed on the basis of the first one. It combines the informa-
tion from two different scores. This second classifier uses the       to perform better in general for this comparison, obtaining a
following general rule:                                               maximum accuracy of 77.1%.
            (                                                             Not surprisingly, all three scores perform very well when
              original if score2 (z) ≥ −score1 (z) + t1 + t2          distinguishing between the original and the reversed version
  c2 (z) =                                                     (7)
              other      otherwise                                    of the sentence, with accuracies above 95%. The sharp drop
                                                                      in accuracy in the case of the Minimum score that we observe
where score1 (z) is the first of the scores assigned to the sen-      when setting the S parameter to 2.75 is due to the spike we
tence, score2 (z) is the second one, t1 is the best performing        have in the case of the reversed condition (see rightmost graph
threshold for this specific comparison as found in the case           in figure 2).
of the first type of classifier for the first score, and t2 is the        Table 1 reports the accuracy for the linear classifier that
same kind of threshold for the second score. We simply check          combines the results of two threshold classifiers (with the best
whether the two scores are above or below the bisector of the         single classifier scores listed in the bottom row as a baseline
second and the fourth quadrant in the space formed by the             comparison). Despite the simplicity of this linear classifier,
two scores, and translated by the best thresholds for the same        we observe an improvement in the original/permuted com-
two scores. The intuition here is, again, that grammatical sen-       parison.
tences will have consistently better scores than ungrammati-
cal ones.                                                             Error analysis
    We performed experiments using both the standard and              It is interesting to analyse the cases where our classifiers fail.
the cluster-based language models. For the standard case              We looked at the cases that form the tails of the distributions
we trained models using words and part-of-speech tags as              for the Normalised threshold, as it is this score that gives the
tokens. In what follows we report only the results for the            best general level of classifier accuracy.
cluster-based experiments, as these achieved better accuracy.             The following ten sentences receive the lowest Normalised
We used 250 clusters. The language model was trained on the           score according to our language model for the original con-
training corpus, and the three scores are computed for the sen-       dition: interview · Swims · / · contracts · then · TELEPHONE
tences in each condition (original, permuted and reversed).           · 75% · Hotel deal · mimic each item across · Ian ! 90%
Figure 2 summarises the distributions of the three scores for         These cases are very marginal English sentences. Their pres-
each condition of the cluster-based language model. It is clear       ence in the corpus may well be due to transcription error in
that all scores are reasonably good at distinguishing between         the BNC, or to the idiosyncratic nature of the text from which
the original and the reversed conditions, given the small over-       they are extracted. However, other cases of false ungram-
lap between the distributions. As expected, the overlap be-           matical sentences include perfectly acceptable sentences like
tween the original and permuted conditions is much higher.            the following: Amnesty has been given Greetings Magazine’s
It is also interesting to note that the while in the case of Mean     “Best Charity Card of the Year” award .
log prob and Normalised score the distributions for all the               For permuted sentences, when we analyse the tail of the
conditions are roughly normal (with some degree of skew-              distribution, we encounter many cases where the permutation
ing), the Minimum score gives a more irregular distribution,          produces the same sentence as the original, because the per-
at least for the ungrammatical cases.                                 muted words are identical. In other cases the permutation
    On the basis of these distributions we created the first type     generates semantically odd, but otherwise well-formed sen-
of classifier. The results for the two comparisons we per-            tences, as in It should be a match of a humdinger. These
formed (original/permuted and original/reversed) are sum-             are the ten permuted sentences that receive the highest Nor-
marised in figure 3. The graphs show the accuracy for each            malised scores (and they are therefore mislabelled as origi-
score obtained by varying the S parameter as described in (6).        nal): He glanced round the bar from the door. · He said that
In our experiments we let S vary in the interval [0, 2.75], using     he had not been informed of the dissolution of the National
a step interval of 0.25.                                              Assembly on Jan. 4. · There ’s something I hear you to want.
    In the case of the original/permuted comparison we ob-            · Sometimes , of course , it does not work. · Don’t know, I
tained the best accuracy (77.3%) by using the Normalised              worry why. · I assure you I’m not. · It should be a match of a
score and setting the threshold at 0.75 standard deviations           humdinger. · She put her hand to her brow. · “Yes , I under-
to the left of the mean. However the Minimum score seems              stand ,” said Drew quietly. · But there was nothing there.
                                                                  2067

   Finally in the case of reversed sentences, we observe that         butional parameters and stochastic classifiers, and test data
sentences that are assigned extremely high Normalised scores          that includes realistic syntactic infelicities. We are evaluat-
tend to be proper names. Due to their low frequency in the            ing these models against native speakers’ acceptability judge-
training corpora, proper names are most likely to be replaced         ments.
by their POS tag in the training and testing phase. There-
fore, the language model cannot distinguish the original and          Acknowledgments
the reversed versions of the sequence, given that they appear         The research described in this paper was done in the framework
                                                                      of the Statistical Models of Grammaticality (SMOG) project at
identical. Again we report here the ten reversed sentences            King’s College London, funded by grant ES/J022969/1 from the
that receive the highest Normalised score. Terrazze Alle ·            Economic and Social Research Council of the UK. We are grate-
seven - eight - nine · 2 TN. WOKINGHAM , 3 TN. FARN-                  ful to Rens Bod, Stephen Clark, Aarne Ranta, Khalil Sima’an, and
                                                                      Jelle Zuidema. for helpful comments on an earlier draft of this paper.
HAM · Debts : MAIDSTONE · Gloucester / BROCKWORTH                     We also thank our PhD students, Jekaterina Denissova and Monika
· FLAUBERT MME · VALLI FRANKIE · PATEL GARGY ·                        Podsiadlo, for useful discussion of the experimental design of this
BATTERSEA HORSMAN UDO · REUNITE / JAFFE LUCKY                         work, and for logistical support.
Discussion and Conclusions                                                                       References
Clark and Lappin (2011) propose an outline for a stochastic           Bod, R., Hay, J., & Jannedy, S. (2003). Probabilistic linguis-
model of indirect negative evidence. In this outline a function          tics. MIT Press.
maps the probability value of a string, and a set of properties       Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra, V. J. D.,
of the string and of the probability distribution over strings           & Lai, J. C. (1992). Class-based n-gram models of natural
of the language, to a threshold value that gives the minimum             language. Computational Linguistics, 18, 467–479.
frequency with which the string must occur in the primary lin-        Chater, N., Tenenbaum, J., & Yuille, A. (2006). Probabilistic
guistic data in order to be well-formed. The threshold spec-             models of cognition: Conceptual foundations. Trends in
ifies the normalised minimal expectancy of occurrence for                Cognitive Sciences, 10(7), 287–291.
a sentence of a certain type (length, lexical class sequence,         Chen, S., & Goodman, J. (1999). An empirical study of
etc.). This model provides a language learner with a proce-              smoothing techniques for language modeling. Computer
dure for querying the data to which he/she is exposed in order           Speech & Language, 13(4), 359–393.
to determine the extent to which the absence of a string in the       Chomsky, N. (1957). Syntactic structures. The Hague: Mou-
data indicates its ungrammaticality.                                     ton.
   Here we effectively invert this strategy. We identify a set of     Clark, A., & Lappin, S. (2011). Linguistic nativism and the
structural properties of a string together with parameters for           poverty of the stimulus. Malden, MA: Wiley-Blackwell.
the distribution of logprob-derived scores, in order to define        Crocker, M., & Keller, F. (2006). Probabilistic gram-
a grammaticality threshold, which we use to classify strings             mars as models of gradience in language processing. In
as grammatical or ill-formed. This model offers a stochas-               G. Fanselow, C. Féry, M. Schlesewsky, & R. Vogel (Eds.),
tic characterisation of grammaticality without reducing gram-            Gradience in grammar: Generative perspectives (pp. 227–
maticality to probability. It represents a core element of what          245). Oxford University Press.
speakers know about the syntax of their language through              Fong, S., Malioutov, I., Yankama, B., & Berwick, R. (2013).
a set of parameters in a model whose values correspond to                Treebank parsing and knowledge of language. In A. Villav-
properties of the modified probability distributions that the            icencio, T. Poibeau, A. Korhonen, & A. Alishahi (Eds.),
model generates.                                                         Cognitive aspects of computational language acquisition
   We are not, of course, suggesting that enriched n-gram                (p. 133-172). Springer Berlin Heidelberg.
models are adequate to express the full content of speakers’          Goodman, J. (2001). A bit of progress in language modeling.
syntactic knowledge. However, the fact that simple models                Computer Speech & Language, 15(4), 403–434.
of the sort that we have used are able to achieve a relatively        Liang, P. (2005). Semi-supervised learning for natural
high degree of accuracy on wide coverage, domain general                 language processing. Unpublished master’s thesis, De-
grammaticality classification tasks suggests that there is an            partment of Electrical Engineering and Computer Science,
interesting correlation between properties of the probability            MIT.
distribution over the sentences of a language and a speaker’s         Pauls, A., & Klein, D. (2012). Large-scale syntactic lan-
grammaticality judgements.                                               guage modeling with treelets. In Proceedings of the 50th
   Should the correlation prove robust it suggests that gram-            annual meeting of the association for computational lin-
matical knowledge is, to a significant extent, determined by             guistics (pp. 959–968). Jeju, Korea.
the stochastic patterns of the primary linguistic data to which       Pereira, F. (2000). Formal grammar and information theory:
speakers are exposed. This result will have significant conse-           together again? Philosophical Transactions of the Royal
quences for both the representation of syntactic competence              Society of London. Series A: Mathematical, Physical and
and the nature of the language acquisition process.                      Engineering Sciences, 358(1769), 1239–1253.
   In current work we are exploring this correlation further
with more sophisticated language models, different distri-
                                                                  2068

                                                                                                               6
                                                                                                                                                                                                                6
          1.0                                                                                                  4
                                                                                                                                                                                                                4
                                                                                  Condition                                                                                     Condition                                                                                    Condition
density                                                                                              density                                                                                          density
                                                                                     original                                                                                       original                                                                                     original
                                                                                     permuted                                                                                       permuted                                                                                     permuted
                                                                                     reversed                                                                                       reversed                                                                                     reversed
          0.5                                                                                                  2
                                                                                                                                                                                                                2
          0.0                                                                                                  0                                                                                                0
                       −4          −3                 −2          −1                                                      −1.5                  −1.0              −0.5                                                   −3                 −2                 −1
                                   Mean log prob                                                                                            Normalised                                                                                      Minimum
Figure 2: Histograms for the distributions of sentence scores. Each graph shows the distribution of a single score for the three
conditions. The x-axis represents the value of the score and the y-axis gives a measure of the frequency with which the score is
represented in the data. On the left are the scores given by taking the mean (equivalently normalising by length). In the middle
are the scores given by normalising with the unigram probability. On the right are the scores using the minimum condition.
These scores still overlap significantly, but much less so than the raw logprobs as shown in Figure 1.
                                                                    original                                                                                                                     original
                                                                   permuted                                                                                                                     reversed
              100
                                                                                                                                                                                                                               ●      ●           ●      ●
                                                                                                                                                                                                                     ●
                                                                                                                                                                                                                ●
                                                                                                                                                                                                ●                    ●         ●      ●           ●
                                                                                                                                                                                                                ●    ●         ●
                                                                                                                                                                                                ●               ●
                                                                                                                                                                                  ●                                                   ●
                                                                                                                                                                                                ●
                                                                                                                                                                                  ●
                                                                                                                                                                                  ●                                                               ●
                90                                                                                                                                                        ●
                                                                                                                                                                          ●
                                                                                                                                                                          ●                                                                              ●
                                                                                                                                                                   ●
                                                                                                                                                                   ●
                                                                                                                                                                   ●
                                                                                                                                                            ●
                80
                                                                                                                                                            ●                                                                                                       Score
   Accuracy
                                                ●                                                                                                                                                                                                        ●
                                         ●                  ●                                                                                               ●
                                         ●
                                                            ●
                                                                       ●                                                                               ●                                                                                                               Mean log prob
                             ●                                              ●
                             ●
                                                                                          ●                                                                                                                                                                            Minimum
                                                                       ●
                      ●                                                                                                                                ●
                                                                                                 ●                                                     ●                                                                                                               Normalised
                70    ●                                                     ●                                       ●
                                         ●
                             ●                                                                                                   ●
                                                ●
                      ●
                                                                                          ●
                                                            ●                                                                         ●
                                                                                                 ●
                                                                       ●                                            ●
                60                                                                                                               ●
                                                                                                                                      ●
                                                                            ●
                                                                                          ●
                                                                                                 ●
                                                                                                                    ●
                                                                                                                                 ●
                                                                                                                                      ●
                50
                     0.00   0.25        0.50   0.75        1.00    1.25    1.50          1.75   2.00               2.25      2.50    2.75          0.00    0.25   0.50   0.75    1.00          1.25        1.50     1.75      2.00   2.25        2.50   2.75
                                                                                                                                            S
Figure 3: Results for the threshold classifier. The two graphs show two comparisons: original/permuted and original/reversed.
The x-axis represents the different values that control the distance from the mean of the threshold, while the y-axis shows the
accuracy expressed in percentages.
                                                                                                                                                       2069

