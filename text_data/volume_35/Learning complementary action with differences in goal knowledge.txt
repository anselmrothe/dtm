UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning complementary action with differences in goal knowledge
Permalink
https://escholarship.org/uc/item/8bg2k4fh
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Karnowski, Jeremy
Hutchins, Edwin
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

              Learning complementary action with differences in goal knowledge
                                         Jeremy Karnowski (jkarnows@cogsci.ucsd.edu)
                                           Department of Cognitive Science, 9500 Gilman Drive
                                                        La Jolla, CA 92093-0515 USA
                                           Edwin Hutchins (ehutchins@cogsci.ucsd.edu)
                                           Department of Cognitive Science, 9500 Gilman Drive
                                                        La Jolla, CA 92093-0515 USA
                               Abstract                                  tive learning. Not all learning and reorganization in a multi-
                                                                         agent system is imitative, however, and another focus of mod-
   Humans, as a cooperative species, need to coordinate in order         eling should be on complementary action learning (Hutchins
   to achieve goals that are beyond the ability of one individual.
   Modeling the emergence of coordination can provide ways to            & Johnson, 2009). It has been shown elsewhere that agents
   understand how successful joint action is established. In this        can learn to coordinate in complementary ways without shar-
   paper, we investigate the problem of two agents coordinating          ing information about each other (Sen, Sekaran, Hale, et al.,
   to move an object to one agent’s target location through com-
   plementary action. We formalize the problem using a decision-         1994), but this presumes an environment where there is only
   theoretic framework called Decentralized Partially Observable         one destination and both agents know its identity. By com-
   Markov Decision Processes (Dec-POMDPs). We utilize multi-             bining aspects from these two studies, we can investigate sce-
   agent Q-learning as a heuristic to obtain reasonable solutions to
   our problem and investigate how different agent architectures,        narios in which agents must collaboratively, through comple-
   which represent hypotheses about agent abilities and internal         mentary action, arrive at a goal location known to only one
   representations, affect the convergence of the learning process.      agent.
   Our results show, in this problem, that agents using external
   signals or internal representations will not only eventually per-        While it is typically intractable to find the optimal solution
   form better than those that are coordinating in physical space        to many multi-agent coordination problems, these problems
   alone but also outperform agents that have independent knowl-
   edge of the goal. We then employ information theoretic mea-           are particularly important because their inherent challenges
   sures to quantify the restructuring of information flow over the      highlight several important features of social interaction and
   learning process. We find that the external environment state         group dynamics that need to be studied:
   varies in its informativeness about agents’ actions depending
   on the agents’ architecture. Finally, we discuss how these re-
   sults, and the modeling technique in general, can address ques-      1. Non-stationary World: Agents are constantly adapting to
   tions regarding the origins of communication.                            the statistics of their environment, including other agents.
   Keywords: Dec-POMDPs; multi-agent Q-learning; Behav-                     Since other agents do not have a fixed method of interact-
   ioral Info-Dynamics; mutual information                                  ing with the world a priori, the world is inherently non-
                                                                            stationary (Buşoniu, Babuška, & Schutter, 2008).
                           Introduction
                                                                        2. Non-independent Sampling: An agent’s own actions af-
The moment we move from a study of individual cognition                     fect its incoming sensory information and this in turn
to a detailed analysis of the social realm, we have commit-                 affects the regularities it can extract from the world
ted ourselves to the investigation of a different type of sys-              (Lungarella & Sporns, 2005). Motor activity and sensory
tem. There is no centralized controller; this system is inher-              information obtained from the environment are interdepen-
ently decentralized. The questions we ask, however, may be                  dent; the way we move in the world shapes our understand-
similar. Just as we wish to study how a individual decision                 ing of it and these patterns of data have structure.
maker adapts its behavior in a task environment, we can in-
vestigate the ways in which multiple, possibly non-identical,           3. Distribution of Knowledge: Not all agents in the world
decision makers reorganize their internal world and their ex-               have access to the same information or capabilities. The
ternal interactions to form a new functional system that solves             social realm is comprised of more than just a set of identi-
a problem which cannot be addressed by one individual alone                 cal individual problem solvers (Hutchins, 1995).
(Hutchins, 1995).
   One important problem that cooperative agents face is how                Another prominent research direction in studying multi-
to coordinate their movements to arrive at a goal known                  agent systems is determining “(h)ow to develop... problem
only to one of the agents. This problem was addressed                    solving protocols (information flow) that enable agents to
in Hazlehurst and Hutchins (1998), where the authors con-                share results and knowledge in a timely, effective manner”
structed an algorithm that allowed for a set of agents to con-           (Sen, 1997). It is important to understand how a group of in-
verge on similar form-meaning mappings which also related                dividual agents reorganizes in functional ways that alter the
to their movements within a given environment. This setup,               flow of information; we need to understand “what informa-
like many modeling studies that focus on issues of hidden                tion goes where and in what form” (Hutchins, 1995) and how
goals of other agents, has a strong predilection towards imita-          these pathways change. This situation is complicated by the
                                                                     2668

fact that researchers in Cognitive Science hold different as-         the exploration of regions between models with different as-
sumptions about the internal organization and external be-            sumptions.
havior of agents, which specifies the model elements, and                Formally, a Dec-POMDP can be defined by a tuple
this constrains the possible ways to reconfigure information          h{Ag}, S, {A}, P, {Ω}, O, Ri, where {Ag} = {1, 2, . . . , n} is the
flow. This situation can be rectified, however, by utilizing a        set of agents, S is the possible states of the world, {A} =
common formalism for comparing and contrasting the conse-             {A1 } × {A2 } × . . . × {An } is the set of joint actions (with
quences of different sets of assumptions.                             a = (a1 , a2 , . . . , an ) being a joint action and action ai is the ac-
   In this paper, we utilize a formal framework, Decentral-           tion of agent i), P is the transition function (with P(s0 |s, a) be-
ized Partially Observable Markov Decision Processes (Dec-             ing the transition to state s’ given current state s and joint ac-
POMDPs), to place our problem of interest into a larger set           tion a), {Ω} is the set of possible observations, O is the matrix
of multi-agent coordination problems in order to investigate          that defines the probability of seeing observation o given state
coordination problems when agents have access to different            s, and R = R(s, a, s0 ) is the reward for taking the joint action
amounts of information (Karnowski, accepted). We then dis-            a in state s and transitioning to state s’. The goal of solving
cuss how several assumptions about agent architecture map             a Dec-POMDP is to find a joint policy π = {π1 , π2 , . . . , πn }
into specific changes in the problem structure, demonstrat-           (where each πi is a local policy of one agent that maps an ob-
ing how we can vary our hypotheses by altering the compo-             servation of a state to an action, i.e. πi : S → Ai ) such that the
nents of the Dec-POMDP. Through the use of multi-agent Q-             group minimizes some cost function over time (similarly, it
learning, we can demonstrate the speed with which agents              can maximize a reward function).
reorganize themselves into stable patterns of behavior that al-
low them to coordinate their actions and achieve a joint goal.        Multi-agent Q-learning
This reorganization brings differences in performance, how-           Dec-POMDPs are a useful abstraction which allows for a
ever, based on the assumptions made about agent capabilities.         common language when speaking about coordination prob-
We utilize mutual information to measure the changes in sta-          lems. These problems, are typically difficult to solve
tistical dependencies among streams of information and to             (D. Bernstein et al., 2000), but solution algorithms are a cur-
show how agents’ behaviors respond to environmental regu-             rent research trend (Spaan & Oliehoek, 2008). Another way
larities. We conclude by discussing how one problem formu-            to address these problems is to use on-line adaptive heuristic
lation may provide insights into the study of the evolution of        algorithms that provide good approximate solutions, such as
communication and future directions in this area.                     Q-learning (CJC, 1989), as they stochastically approximate
                                                                      off-line learning of optimal policies. In this paper, we use the
                          Methods                                     Q-learning algorithm in a multi-agent context (Buşoniu et al.,
Decentralized Partially Observable Markov                             2008). Within each agent, state-action pairs are strengthened
                                                                      depending on the outcome of the chosen action. For instance,
Decision Processes (Dec-POMDPs)
                                                                      if an agent transitions to state s0 after performing action a
Dec-POMDPs (D. Bernstein, Zilberstein, & Immerman,                    while in state s, an agent will receive a reinforcement R and
2000) are a way to formalize multi-agent coordination prob-           update the value of that state-action pair (s, a):
lems. They provide a common structure that aids in the dis-
cussion of related problems and the development of solution               Q(s, a) ← (1 − α)Q(s, a) + α(R + γmaxa0 ∈A Q(s0 , a0 ))           (1)
techniques. While there exist other frameworks that tackle
problems of agent coordination and problem solving (Dec-              Other parameters relate to the learning algorithm itself. The
POMDP-COM, MTDP, and COM-MTDP with perfect re-                        learning rate, α, determines the degree to which the current
call), many of them have been shown to be formally equiva-            state is updated given new experience, and the discount factor,
lent (Seuken & Zilberstein, 2008). The reason for the variety         γ, specifies how influential future states and actions are to the
is that the frameworks emphasize different features. For in-          current state. In this experiment, actions were chosen in a
stance, while Dec-POMDPs and Dec-POMDP-COMs (Dec-                     greedy manner.
POMDPs with communication) (Goldman, Allen, & Zilber-
stein, 2007) are formally equivalent, the former tends to fo-         Behavioral Info-Dynamics
cus on bodily coordination in physical space and the latter           Consider an isolated animal collective X consisting of n freely
with problems that also involve symbolic coordination. In ad-         moving animals. Temporal data is collected on each animal’s
dition to communication, frameworks often contain assump-             behavior generating a unique time series. Given a collec-
tions about the representational capacities of their agents, pro-     tion of sensorimotor time series data from a set of animals,
viding agents with, for example, the ability to model the goals       we can measure statistical dependencies during different be-
or actions of other agents (Claus & Boutilier, 1998). Provid-         havioral patterns. Tononi, Sporns, and Edelman (1994) (and
ing a language for researchers in Cognitive Science to sys-           later Tononi, Edelman, and Sporns (1998)) introduced a set
tematize problems in cooperative multi-agent interactions and         of appropriately defined information-theoretic measures to
make explicit their assumptions about individual architecture         capture the statistical properties of a system with n compo-
will allow for a thorough comparison of current models and            nents. While their methods were originally designed to study
                                                                  2669

neural systems, more recent work has adapted these mea-                by the two agents. The new coordinates are then assigned
sures to study sensorimotor coordination in embodied agents            to the correct discrete bin. The location of the block is used
by collecting sensor and motor time series data (Lungarella,           as feedback for the agents, depending on which scenario is
Pegors, Bulwinkle, & Sporns, 2005). We utilize a Python                being considered.
implementation of these measures (available at https://                   In our problem, {Ag} is a set of two agents, S is the x-
github.com/OpenCV-at-DCog-HCI/BID) to further extend                   coordinate in a 20x20 grid world, the actions are a vector-
these measures to study the behavior of a system of agents. In         addition of individual agent actions that combine force and
this paper, we focus only on the mutual information between            angle (0.2 ≤ |~Fi | ≤ 2.0) in 0.2 increments and 15 ≤ θi ≤ 165
pairs of time series. Depending on their interaction with the          in 15 degree increments), P is deterministic (the probabili-
world, solitary agents and collections of agents exploit dif-          ties of moving to the next state given a joint action is 1 and
ferent statistical dependencies among streams of information.          the rest are zero), the set of observations is always the cur-
We can show these changes by measuring mutual information              rent x-coordinate in the grid world but more information is
(Sporns, Karnowski, & Lungarella, 2006; Di Prodi, Porr, &              added depending on the scenario (for the agent with the goal,
Wörgötter, 2010).                                                    the current goal is also added to the observation), Ω is deter-
    Entropy defines the uncertainty inherent in a time series, or      ministic (the probabilities of an agent perceiving a particular
the average amount of information present. For instance, if            observation given a state is 1 and the rest are zero), and the
knowing the state of the system at a given point in time will          feedback depends on the scenario.
give you a lot of information about the time series as a whole,           The first goal of our study was to establish a baseline. We
then this will contribute to a lower entropy. This could happen        implemented the scenario as found in (Sen et al., 1994):
if that state is highly unlikely, and thus is more informative. If
every state, however, is equally likely, then knowing the state       0. Agent 2 also knows goal (Full Information): Both agents
at one point in time gives no information about the time series           receive an observation of their x-coordinate and the goal.
as a whole and entropy is maximal.                                        Their feedback is a function of their distance from the goal
                                                                          path P.
                                n
                   H(X) = − ∑ p(x j )log(p(x j ))              (2)        Even though there are two possible paths, there is only one
                               j=1                                     goal for each trial, and therefore our agents acted in simi-
    Mutual information measures the dependence between two             lar manner and replicated the results obtained by Sen et al.
distributions (and in our case, time series). It is defined as the     (1994). We then set out to construct a situation where there
Kullback-Leibler distance (DKL ) between the joint distribu-           is a disparity in the amount of information accessible to each
tion p(X1 , X2 ) and the independent distribution p(X1 )p(X2 ).        agent. In our ‘base case’, we consider the impact of remov-
Mutual information is also defined as the sum of the entropies         ing information about the goal from Agent 2 and only allow-
of the individual parts with the joint entropy subtracted out.         ing Agent 1 to have this knowledge. From here, our mod-
                                                                       els were motivated by research agendas within Cognitive Sci-
    MI(X1 , X2 ) = DKL [p(X1 , X2 )||p(X1 )p(X2 )] =                   ence. Given different assumptions of agent architecture, we
                                                                       alter the Dec-POMDP in specific ways:
                                H(X1 ) + H(X2 ) − H(X1 , X2 ) (3)
                                                                      1. Agent 1 knows the goal but Agent 2 does not (‘Base
Any dependence between the two time series will increase the              Case’): Agent 1 remains identical to previous results, but
mutual information between them. For instance, if the state               the observation Agent 2 receives does not contain informa-
of one agent provides a lot of information about the state of             tion about the goal. The feedback for Agent 2 is a function
another agent, this will result in higher mutual information.             of the distance from the closest path (i.e. when there is no
If the agents are completely independent, then this predictive            information about the goal, the closest path is the best)
power is lost, and mutual information will be zero.
                                                                      2. Agent 2 tracks probability of goal (‘Theory of Mind’):
             Problem and Experimental setup                               Giving an agent the ability to represent the goal of another
To explore how two agents could coordinate via complemen-                 agent and make inferences about that goal given data is
tary actions to arrive at a hidden goal, we created an exten-             one way to conceptualize Theory of Mind. In this situa-
sion of the ‘block pushing problem’ (Matarić, 1996; Sen et               tion, Agent 2 begins a trial with the prior belief that either
al., 1994) where two agents are tasked to move from a start               goal is the possible target. At each time step, the state of
location to the goal, which is one of two possible locations,             the world is a sample with which Agent 2 updates its belief
and follow as closely as possible a path P between the two. At            of the current goal via Bayes rule. The probability of this
every timestep, Agent i uses a force ~Fi , where 0 ≤ |~Fi | ≤ Fmax        sample is the probability that the x-coordinate is sampled
on the block at an angle θi , where 0 < θi < π, which results             from a Gaussian distribution with the x-coordinate of the
in the block being offset by |~Fi | cos(θ) in the x direction and         goal being the mean and a standard deviation of 2.5 (Alter-
|~Fi | sin(θ) in the y direction. The new position of the block           ing this distribution is future work). The probability space
is calculated by vector addition of the displacement created              was discretized into 10 bins. The feedback for Agent 2 is
                                                                   2670

    an weighted average (given current belief) of the feedback          to perform better than those with complete information. Hav-
    for both paths.                                                     ing the ability to represent and make inferences about the
                                                                        goals of another agent provides even more improvement in
3. Agent 1 can make sounds (’Communication’): Agent 1                   joint coordination.
    produces either a 0 or 1 which becomes part of the state
    which Agent 2 will experience on the next time step. The
    feedback for Agent 2 is a function of the closest path.
4. Agent 1 can make sounds and Agent 2 tracks probabil-
    ity of goal (‘Theory of Mind’ and ‘Communication’):
    This is a combination of the previous two alterations. The
    feedback for Agent 2 is the weighted average of the feed-
    back for both paths.
    The feedback in each of these cases is determined by
 a function of the distance from the desired path, f (δx) =
 K ∗ a−δx , similar to the original setup in Sen et al. (1994).
 This provides a high value for being on the path and an ex-
 ponentially decreasing value further away from the desired
 path. Starting out the learning process with high values for
 state-action pairs and providing feedback after every trial was
 another feature in Sen et al. (1994) that allowed the agents to        Figure 1: The average distance of the actual path from the
 explore the available actions (alternatively, one could set the        goal path given different agent assumptions (α = 0.01,γ =
 values in the beginning to be zero, but receiving feedback af-         0.9). Each experiment had 5000 trials and the data has been
 ter just one trial would bias the agent to take the same path          averaged over 100 experiments. Other learning rates (α ∈
 every trial). Also, any updates to state-action pairs could not        {0.1, 0.2, 0.3}) resulted in the similar patterns of performance
 be larger than the original high value (in our case, this was set      with different rates of convergence.
 to 100).
    At the beginning of every trial, the two agents start at               We can determine how the two agents functionally reor-
 (x, y) = (10, 0) and the goal is randomly chosen from two              ganized themselves based on the levels of statistical depen-
 options: (3, 20) or (17, 20). They make individual actions             dence between different data streams. Mutual information
 which combine into a joint action as outlined above. If the            provides a way to measure how predictable one data stream
 agents move the object outside of the 20x20 grid world, then           is from another. As we can see in Figure 2, both the sce-
 the trial ends. Similarly, if the agents arrive at the goal state,     nario in which Agent 1 and Agent 2 have full knowledge of
 the trials ceases. In the rare chance that agents would take           the goal and the ‘base case’, where Agent 2 does not know
 more than 100 timesteps, the trial would also stop (forcing            the goal, there is an increase in the mutual information be-
 the angles to not allow agents to travel parallel to the x-axis        tween the x-coordinate and the angle of Agent 2 but this mu-
 helps alleviate this problem). An additional feature incorpo-          tual informativeness plateaus. In the scenarios where there is
 rated into the world dynamics was an automatic movement                Theory of Mind, Agent 2 is receiving a wealth of information
 forward if the agents did not move forward enough on a trial.          about the goal through its current location but not necessarily
 This was added to ensure agents did not remain still and al-           needing to rely on any connection between its angle action
 lowed for better convergence.                                          choices and its location, which would have forced it to be
                                                                        more precise in its actions. In the scenarios with sound, there
                             Results                                    is a lot of extra structure in the shared environment that be-
 In our experiments, agents always began with equally valu-             comes highly predictive of the x-coordinate and therefore in
 able state-action pairs and this caused their actions to be se-        the actions of Agent 2, including the angle. Another situa-
 lected randomly. Over many trials, as agents adjust the values         tion was created in which Agent 1 produced a sound but the
 of different actions within each state, their behaviors begin to       state also included another random noise (to take away the
 become patterned. Practices reduce the entropy of the shared           special nature of the sound but not its ability to be manipu-
 environment, which leads to better policies and to a decrease          lated). While the graph does not show the full increase of MI,
 in the average distance from the goal path. One would sus-             other simulations showed this had the same trend as the case
 pect, however, that performance would be best when there               with communication, just over a longer period of time. This
 is complete information for both agents and that scenarios             makes sense if agents were learning to utilize structure, but
 in which one agent has partial and incomplete information,             randomness was slowing this process down.
 the resulting joint actions would lead to poorer performance.             We did not find that the forces with which agents pushed
 This is not what we find, as shown in Figure 1. Having the             the box had any predictive power for other data streams.
 ability to produce and utilize sounds allows agents, over time,        When there was an increase in mutual information, it ap-
                                                                    2671

                                                                    x-coordinate, is about another data stream, the angle chosen
                                                                    by Agent 2 and charted the changes in this informativeness
                                                                    over time.
                                                                       The results for this particular problem formulation provide
                                                                    a partial ranking of models based on performance. There
                                                                    are, however, a couple of caveats. First, while our simu-
                                                                    lated agents chose their actions in a greedy manner, differ-
                                                                    ent results might be obtained through other action selection
                                                                    methods, such as using a Boltzmann action selection mecha-
                                                                    nism. Second, Dec-POMDPs are typically used when there is
                                                                    some uncertainty in state transitions (due to modeling motor
                                                                    noise) or observations (due to sensory noise or partial view
                                                                    of the world). While this problem does not utilize this fea-
                                                                    ture, future work manipulating these parameters may change
                                                                    the success of models with different assumptions about agent
                                                                    architecture.
                                                                       This work highlights several of the open problems in the
                                                                    study of the emergence of communication, as it simulta-
                                                                    neously investigates the origin of signaling channels, the
                                                                    sources of representation in signals, and the roles of social in-
                                                                    teraction in learned communication systems (Lyon, Nehaniv,
                                                                    & Cangelosi, 2006).
                                                                       Future work related to this particular example will strive to
                                                                    explore how agents could learn to discover that one informa-
                                                                    tion stream is informative about another, a hallmark of com-
                                                                    munication. As a starting point, for instance, we are particu-
                                                                    larly interested in the case where the agents have an ability to
                                                                    put structure into the shared environment through sounds. In
Figure 2: The mutual information between the x-coordinate           this case, it could be that the agent with the goal is able to cre-
and the angle of Agent 2.                                           ate noises, which allows the second agent to adjust its policy
                                                                    given this external structure. This in turn forces more regular
                                                                    behavior to which the speaking agent can then adjust. Orig-
peared to be due to the high predictability of angle and x-         inally, the noise was not functionally related to the current
coordinate. As the world dynamics forced agents ahead one           state; in the beginning, sounds just happened. As engagement
step if they did not apply enough force, it may have been the       proceeds, that noise ends up carrying information, and at that
case that this affected the importance of force as a predictive     moment, the sounds would become a signaling channel.
element. This is probably not the case, however, as the agents         This process, however, hasn’t held any commitments to
in our model (and those in Sen et al. (1994)) only observe          the content of that signaling channel. It may turn out that
the x-coordinates, which would in turn dampen some of the           the speaking agent, through features of the algorithm, con-
informativeness of force in agent action choices.                   verges on highly rewarding action-sound pairings and the sec-
                                                                    ond agent only need adjust its behavior accordingly. In either
                          Discussion                                case, we suspect that putting structure out into the world may
In this paper, we have discussed the benefits of utilizing a        create stable regularities with which agents could take advan-
common theoretical framework for addressing cooperative             tage and eventually internalize (Vygotsky, 1978). Agent in-
multi-agent problems in Cognitive Science and demonstrated          teractions themselves would be the determining factor behind
how changes to framework elements can encapsulate various           the sources of representations in the signals they employ. In
hypotheses about agent actions and internal representational        problems similar to ours, it is often the case that multi-agent
capacities. We have designed a new multi-agent problem,             Q-learning fails, precisely because neither agent experiences
focusing on understanding the acquisition of complementary          a stationary environment (Claus & Boutilier, 1998). Placing
actions in a goal-directed task where there is an information       stationary-creating behavior at the center of new algorithms
disparity. We used Q-learning, an algorithm commonly used           is also possible future work.
in modeling single agent decision making, in a multi-agent             Here we have shown that we can operationalize several
setting to investigate how agent hypotheses affect the conver-      assumptions in Cognitive Science and discover what struc-
gence of the learning process. And finally, we used mutual          ture and organization emerge from these hypotheses. In the
information to quantify how informative one data stream, the        present examples, however, agents are endowed with cer-
                                                                2672

tain abilities a priori. We would really like to explore the              structure of sensory and motor data. Neuroinformat-
conditions under which language-like abilities and Theory                 ics, 3(3), 243–262.
of Mind-like processes could emerge from ongoing interac-           Lungarella, M., & Sporns, O. (2005). Information self-
tions between autonomous agents. Additional future work                   structuring: Key principle for learning and develop-
will look at the space between these hypotheses and how var-              ment. Proceedings The 4nd International Conference
ious learning algorithms could take agents from a lack of abil-           on Development and Learning 2005, 25–30.
ities to a state where additional mental abilities have emerged     Lyon, C., Nehaniv, C., & Cangelosi, A. (2006). Emergence
through agent interactions.                                               of communication and language. Springer.
                                                                    Matarić, M. (1996). Learning in multi-robot systems. Adap-
                     Acknowledgments                                      tion and Learning in Multi-Agent Systems, 152–163.
The authors would like to thank Chris Johnson, Ben Bergen,          Sen, S. (1997). Multiagent systems: Milestones and new
and Ben Cipollini for helpful discussions. Jeremy Karnowski               horizons. Trends in Cognitive Sciences, 1(9), 334–340.
is a Jacobs Fellow and is the recipient of a CARTA Graduate         Sen, S., Sekaran, M., Hale, J., et al. (1994). Learning to co-
Fellowship in Anthropogeny.                                               ordinate without sharing information. In Proceedings
                                                                          of the national conference on artificial intelligence (pp.
                           References                                     426–426).
                                                                    Seuken, S., & Zilberstein, S. (2008). Formal models and al-
Bernstein, D., Zilberstein, S., & Immerman, N. (2000). The                gorithms for decentralized decision making under un-
        complexity of decentralized control of markov decision            certainty. Autonomous Agents and Multi-Agent Sys-
        processes. In Proceedings of the sixteenth conference             tems, 17(2), 190–250.
        on uncertainty in artificial intelligence (pp. 32–37).      Spaan, M., & Oliehoek, F. (2008). The multiagent decision
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein,                 process toolbox: Software for decision-theoretic plan-
        S. (2002). The complexity of decentralized control of             ning in multiagent-systems.
        markov decision processes. Mathematics of Operations        Sporns, O., Karnowski, J., & Lungarella, M. (2006). Map-
        Research, 27(4), 819–840.                                         ping causal relations in sensorimotor networks. In
Buşoniu, L., Babuška, R., & Schutter, B. De. (2008, March).             Proc. of the 5th int. conf. on development and learn-
        A comprehensive survey of multi-agent reinforcement               ing.
        learning. IEEE Transactions on Systems, Man, and            Tononi, G., Edelman, G., & Sporns, O. (1998). Complex-
        Cybernetics, Part C: Applications and Reviews, 38(2),             ity and coherency: integrating information in the brain.
        156–172.                                                          Trends in cognitive sciences, 2(12), 474–484.
CJC, H. (1989). Learning from delayed rewards. Cambridge            Tononi, G., Sporns, O., & Edelman, G. (1994). A measure for
        University, Cambridge, England, Doctoral thesis.                  brain complexity: relating functional segregation and
Claus, C., & Boutilier, C. (1998). The dynamics of rein-                  integration in the nervous system. Proceedings of the
        forcement learning in cooperative multiagent systems.             National Academy of Sciences of the United States of
        In In proceedings of national conference on artificial            America, 91(11), 5033–5037.
        intelligence (aaai-98 (pp. 746–752).                        Vygotsky, L. (1978). Mind in society. Harvard University
Di Prodi, P., Porr, B., & Wörgötter, F. (2010). A novel in-             Press.
        formation measure for predictive learning in a social
        system setting. From Animals to Animats 11, 511–522.
Goldman, C., Allen, M., & Zilberstein, S. (2007). Learning
        to communicate in a decentralized environment. Au-
        tonomous Agents and Multi-Agent Systems, 15(1), 47–
        90.
Hazlehurst, B., & Hutchins, E. (1998). The emergence of
        propositions from the co-ordination of talk and action
        in a shared world. Language and Cognitive Processes,
        13(2-3), 373–424.
Hutchins, E. (1995). Cognition in the wild. MIT Press.
Hutchins, E., & Johnson, C. (2009). Modeling the emer-
        gence of language as an embodied collective cognitive
        activity. Topics in Cognitive Science, 1(3), 523–546.
Karnowski, J. (accepted). Modeling collaborative coordina-
        tion requires anthropological insights. Topics in Cog-
        nitive Science.
Lungarella, M., Pegors, T., Bulwinkle, D., & Sporns, O.
        (2005). Methods for quantifying the informational
                                                                2673

