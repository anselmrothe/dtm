UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An integrated model of concept learning and word-concept mapping
Permalink
https://escholarship.org/uc/item/0z91x65b
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Lewis, Molly
Frank, Michael
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

             An integrated model of concept learning and word-concept mapping
                                   Molly Lewis                                             Michael C. Frank
                               mll@stanford.edu                                         mcfrank@stanford.edu
                            Department of Psychology                                    Department of Psychology
                                Stanford University                                        Stanford University
                               Abstract
   To learn the meaning of a new word, children must solve two
   distinct problems: identify the referent under ambiguity and
   determine how to generalize that word’s meaning to new ob-
   jects. Traditionally, these two problems have been addressed
   separately in the literature, despite their tight relationship with
   one another. We present a hierarchical Bayesian model that
   jointly infers both the referent of a word in ambiguous con-
   texts and the concept associated with a word. As a first step
   in testing this model, we provide evidence that our model fits
   human data in a simple cross-situational concept learning task.
   Keywords: cross-situational word learning; Bayesian models
                           Introduction                                    Figure 1: Schema of the two problems associated with learn-
Learning a new word requires drawing a link in your mental                 ing the meaning of a word. Learning a new word requires that
lexicon between a word and a concept. But, children do not                 the child both identify which object the word refers to in the
observe associations between words and abstract concepts;                  referential context (the Mapping Problem) and how to gener-
they observe associations between words and exemplars of                   alize that word to objects of the same kind (the Generalization
those concepts. Furthermore, the associations between words                Problem).
and objects are ambiguous: a single word uttered in any par-
ticular context is consistent with an infinite number of possi-
ble interpretations (Quine, 1960). There are thus two prob-                objects across situations. When considered in an isolated sit-
lems a child must solve in order to learn the meaning of a                 uation, the referent of a word may be ambiguous, but when
new word: Determine which object is referred to by a word                  situations are aggregated across, the learner is able to con-
in context (the Mapping Problem) and determine the relevant                strain the hypothesis space of likely meanings. There is evi-
concept of the object (the Generalization Problem; see Figure              dence that children as young as 12-months-old can learn word
1).                                                                        meanings in this way (Smith & Yu, 2008).
   To understand these two problems more clearly, suppose                     A second class of constraints on the Mapping Problem are
you lived in an (impoverished) world with two words, “apple”               accounts of the disambiguation effect. The disambiguation
and “cherry,” and three objects, a green apple, a red apple, and           effect refers to children’s tendency to select a novel, as op-
a cherry. You hear the word “apple” in the context of a single             posed to familiar, object as a referent for a novel word. One
red apple on the table. You somehow infer that “apple” refers              account of this phenomenon is the principle of mutual ex-
to the red object on the table, and thus correctly solve the               clusivity (Markman & Wachtel, 1988; Markman, Wasow, &
Mapping Problem. But you have not yet succeeded in solving                 Hansen, 2003). Under this proposal, there is a constraint
the Generalization Problem. To correctly solve the General-                on the types of lexicons considered when learning the mean-
ization Problem, you must decide whether “apple” also refers               ing of a new word. With this constraint, children are biased
to the green apple, which is similar in shape to your observed             to consider only those lexicons that have a one-to-one map-
apple exemplar, or whether it also refers to the cherry, which             ping between words and objects. Thus, when faced with an
is similar in color to your observed apple exemplar. Or, al-               ambiguous referential context, the child solves the mapping
ternatively, whether “apple” refers to neither of these other              problem by assuming that the novel word refers to the object
objects (i.e. a proper name). Thus, to learn the word “apple”              for which she does not yet have a word in her lexicon. This
in this world, you must infer both that “apple” refers to the red          is the inferred mapping because it is the only referent that al-
object on the table, and that “apple” should be generalized to             lows the learner to maintain a one-to-one structure between
other apple-shaped objects.                                                words and concepts in the lexicon. Others have proposed that
   Separate learning mechanisms and constraints have been                  general pragmatic assumptions can also account for this ef-
proposed to account for each of these problems. In the case                fect (Clark, 1987; Diesendruck & Markson, 2001).
of the Mapping Problem, one proposed constraint is cross-                     There are also a range of proposals about how children
situational statistics (Pinker, 1984; Smith & Yu, 2008; Yu &               might solve the Generalization Problem. One proposal is that
Smith, 2007). Under this account, learners are hypothesized                children have a bias to generalize by shape (Smith, Jones,
to aggregate the statistics of associations between words and              Landau, Gershkoff-Stowe, & Samuelson, 2002). With this
                                                                       882

bias, a child who has learned that “apple” maps to apple, for
example, can generalize “apple” to all apple-shaped things.
This bias allows learners to rule out alternative, less probable
generalizations strategies, such as generalization along the di-
mension of color. A second proposal is that children have
a bias to generalize to another object of similar kind, rather
than to one that is thematically related (“Taxonomic Assump-
tion”; Markman, 1990). For example, upon hearing the word
“cherry,” a child with this bias would be more likely to gen-
eralize the word to another fruit, as opposed to ice cream,
despite the fact the ice cream and cherries often go together
(see Xu & Tenenbaum, 2007, for a probabilistic view).
   Though theoretically distinct, and investigated separately,
these the two problems are intimately related. If a child has
solved the Generalization Problem for a particular category,
the Mapping Problem becomes much easier. For example,
suppose a child is faced with a never-before-seen apple and
a novel object, and hears the word “dax”. If the child has
solved the Generalization Problem, the child can identify the
apple as an exemplar of the APPLE1 concept, and determine
the correct referent by mutual exclusivity. Conversely, if a              Figure 2: The generative process for our model. Shading in-
learner can easily solve the Mapping Problem, the learner will            dicates observed variables
accumulate more correct exemplars of a category, and thus be
more likely to infer the correct concept. Thus, existing pro-             children generalize novel words (e.g. Sloutsky, Lo, & Fisher,
posals about how each of these problems is solved takes the               2001).
other problem for granted. But, importantly, a child acquiring
                                                                             We present a hierarchical Bayesian model that solves the
language begins with neither of these problems solved; both
                                                                          Mapping and Generalization Problems in parallel. In mod-
must be solved in parallel. That is, a learner must determine
                                                                          eling the Generalization Problem, we draw on the Boolean
both what object a word refers to, and how to generalize that
                                                                          concept learning framework in which objects are defined by
meaning beyond the particular context. And, critically, she
                                                                          a set of features with a range of values (Shepard, Hovland, &
must do both at the same time.
                                                                          Jenkins, 1961). The goal for the word learner is conceptual-
   There is limited work exploring how children might solve               ized as the task of mapping a word to a set of features that de-
these two problems in parallel. A study by Akhtar and                     fine the relevant concept. In modeling the Mapping Problem,
Montague (1999) begins to address this question by asking                 we focus on the role of cross-situational statistics. In partic-
whether children might use cross-situational statistics to learn          ular, we build on the model developed by Frank, Goodman,
the relevant features for generalization. In their task, 2-4 year         and Tenenbaum (2009) that takes into account the intentions
old children were presented with three novel objects that all             of the speaker in order to identify the referent in ambiguous
shared a common feature (e.g. color), but varied along two                contexts.
other features (e.g. texture and shape). Children were able
                                                                             The plan for the paper is as follows. We first describe the
to correctly infer that the novel word referred to the shared
                                                                          design of this extended model, and then describe the results
feature. This result provides important evidence that children
                                                                          of an experiment that explores adult performance in a cross-
can infer word concepts cross-situationally. However, it is
                                                                          situational Boolean concept learning task.
unclear whether this type of learning generalizes to the real
world because the actual learning environment is not struc-                                  Design of the Model
tured in a way that perfectly disambiguates word meanings
cross-situationally.                                                      The goal of our model is to understand how children arrive
   Apart from word learning, the Generalization Problem has               at an understanding about the meanings of words, on the ba-
been well-studied in adults (Laurence & Margolis, 1999;                   sis of limited evidence about the associations between words
Rosch & Mervis, 1975; Rosch, Mervis, Gray, Johnson, &                     and objects. That is, the goal is to infer a lexicon — a set
Boyes-Braem, 1976; Medin & Ortony, 1989). However, lim-                   of word-concept mappings — on the basis of basis of obser-
ited research has attempted to extend this body of literature to          vations of words and objects. To model this, we consider a
work with children. One exception is work by Sloutsky and                 set of variables relevant to this learning problem, and assume
colleagues which adopts models of similarity to explain how               that they are related probabilistically. We assume an identical
                                                                          dependency structure as the model developed by Frank et al.
    1 Small capital letters are used to distinguish concepts from ob-     (2009), with the addition of a concept layer to the generative
jects.                                                                    process (see Figure 2). This model is the same underlying
                                                                      883

model as presented in Lewis and Frank (2013) but with the
addition of a theory of Boolean concepts. For completeness,
we present the full model here, but details are identical except
where noted.
   We model a word learner as performing Bayesian inference
to infer a lexicon l, which we represent as a (sparse) bipartite
graph connecting words W = w1 ...wn to concepts C = c1 ...cm .
Concepts are written as a vector of features with values 1, 2
or *. The ⇤ notation denotes a feature that is irrelevant to the
definition of a concept. For example, [1 ⇤ ⇤] represents a con-
cept that is defined only by the value of the first feature. This
hierarchical formulation of concepts is substantially similar
to the concept learning model proposed by Goodman, Tenen-
baum, Feldman, and Griffiths (2010). The full possible set of
lexicons is denoted as L.                                                       Figure 3: Experimental stimuli. Each object is defined by a
   The learner infers a distribution over lexicons, given a cor-                binary value for each of three features: shape, appendage, and
pus S of situations (each consisting of sets of words w̄s and                   color.
objects ōs ). From Bayes’ rule, the posterior probability of a
lexicon is given by                                                                We assume that there is some level of noise in both the
                                           P(S|l)P(l)                           choice of word given intention P(ws |is , l) and the choice of
                   P(l|S) =                                      .      (1)     intention given object P(cs |is ), such that the speaker could
                                    Â  l 0 2L P(S|l 0 )P(l 0 )
                                                                                in principle have been mistaken about their referent or mis-
The prior P(L) is assumed to be uniform over lexicons that                      spoken. We implement this decision by assuming a constant
map a concept to at most one word (one word to many con-                        probability of random noise for each of these, which we no-
cepts). We now define the likelihood term P(S|L).                               tate a; for simplicity, a is assumed to be the same for both de-
   Using the generative process in Figure 2, we can write the                   cisions. The particular choice of a values only serves to scale
likelihood of a particular situation in terms of the relationship               the predictions, and does not influence the relative predictions
between the objects that were observed in the situation s, the                  of the test item types. However, as in nearly all probabilistic
speaker’s referential intention is (a choice to speak about one                 models, some level of uncertainty about the individual obser-
of the objects), the concept cs selected by the speaker to repre-               vations is necessary to be able to make graded predictions.
sent the intention, and the referring word ws . As in our prior                    In the simulations reported here, we did inference by exact
work, we assume that referential intentions are unobserved                      inference via full combinatoric enumeration of the space of
and sum across all possible intentions uniformly:                               possible lexicons.
                  P(s|l) =        Â       p(ws , cs , is , os , |l)     (2)                              Experiment
                                 is 2o¯s
                                                                                Our model jointly solves the two problems associated with
By the conditional independence of words and objects, we                        learning the meaning of a new word, the Mapping and Gen-
use the chain rule to expand to:                                                eralization Problems. As a first step in evaluating the model,
                                                                                we compared human and model performance in a cross-
             P(s|l) =     Â P(ws |cs , l)P(cs |is )P(is |os )           (3)     situational Boolean concept learning task. Participants were
                         is 2o¯s
                                                                                given a situation in which a word is seen in the context of two
Finally, we aggregate across situations by taking the product                   objects, but in a way that is ambiguous as to which of these
of each independent situation:                                                  objects (either or both) the label refers to. The learner is then
                                                                                presented with a second such situation. While each of these
           P(S|l) = ’ Â P(ws |cs , l)P(cs |is )P(is |os )               (4)     situations is individually ambiguous, the learner could aggre-
                       s2S i2ōs                                                gate information across situations to infer the concept associ-
   To find the key term in our concept model, p(cs | is ), we                   ated with the word. As predicted by the model, we found that
use a noisy Naive Bayes classifier:                                             participants generalized the meaning of the label in a graded
                         ⇢                                                      manner: the more features the training objects shared with
                                                       j       j    j           the test object, the more likely participants were to generalize
                              1 a              if (cs = is ) _ (is = ⇤)
  P(cs | is ) = ’                                                       (5)
                              a                otherwise                        the label to the test object.
                j=1... f
This formulation quantifies the probability of a concept given                  Method and Materials
an intended object in terms of the match between the three                      Participants Two hundred and and sixty-six adults were re-
features.                                                                       cruited from Amazon’s Mechanical Turk. Twenty-two were
                                                                            884

                                                                                           Baseline          Baseline
                                                                                                               Baseline                                                                             Confounded
                                                                                                                                                                                                       Confounded
                                                                                                                                                                                         Confounded across        across  situations:
                                                                                                                                                                                                                     across
                                                                                                                                                                                                           situations:                1 feature
                                                                                                                                                                                                                             situations:
                                                                                                                                                                                                                       1 feature          1 feature                                                                                 Confounded
                                                                                                                                                                                                                                                                                                                                       Confounded
                                                                                                                                                                                                                                                                                                                         Confounded across        across
                                                                                                                                                                                                                                                                                                                                           situations:    situations:
                                                                                                                                                                                                                                                                                                                                                     across  situations:
                                                                                                                                                                                                                                                                                                                                                       2 features     2 features
                                                                                                                                                                                                                                                                                                                                                                          2 features
                                                                                    100                                    s1                                                                                        100                    s1                                                                                                         100                s1
                                                        Mean bet on test item                                                                                                            Mean bet on test item                                                                                                             Mean bet on test item
                        100                                                                                                                              100                                                                                                                               100
                                                                                                                           s2
Mean bet on test item         Mean bet on test item                                                                              Mean bet on test item         Mean bet on test item                                                                               Mean bet on test item         Mean bet on test item
                        80                                                          80                                                                   80                                                          80                     s2                                             80                                                          80                 s2
                                                      20   40     60      80  100                                                                                                      20   40     60      80  100                                                                                                       20   40     60      80  100
                        60                                                          60                                                                   60                                                          60                                                                    60                                                          60
                        40                                                          40                                                                   40                                                          40                                                                    40                                                          40
                        20                                                          20                                                                   20                                                          20                                                                    20                                                          20
                        0                                      0                    0                                                                    0                                      0                    0                                                                     0                                      0                    0
                                                                                          Test item type        Test item type
                                                                                                            Test item type                                                                                                 Test item type    TestTest  typetype
                                                                                                                                                                                                                                                  itemitem                                                                                                         Test item type             typetype
                                                                                                                                                                                                                                                                                                                                                                                         itemitem
                                                                                                                                                                                                                                                                                                                                                                                    TestTest
                                                                                                           Test    item type                                                                                                            Test item type                                                                                                         Test item type
                                                                          Confounded
                                                                       Confounded
                                                            Confounded within           within
                                                                                     within
                                                                              situations:       situations:
                                                                                             situations:
                                                                                          1 feature          1 feature
                                                                                                         1 feature                                                                                   Confounded
                                                                                                                                                                                                  Confounded
                                                                                                                                                                                       Confounded within           within
                                                                                                                                                                                                                within
                                                                                                                                                                                                         situations:       situations:
                                                                                                                                                                                                                        situations:
                                                                                                                                                                                                                     2 features         2 features
                                                                                                                                                                                                                                    2 features                                                                                                                              Confounded:
                                                                                                                                                                                                                                                                                                                                                             Confounded: Confounded:
                                                                                                                                                                                                                                                                                                                                                                         3 features      3 features
                                                                                                                                                                                                                                                                                                                                                                                     3 features
                                                                                    100                                                                                                                              100                                                                                                                               100
                                                      Mean bet on test item                                                                                                            Mean bet on test item                                                                                                             Mean bet on test item
                        100                                                                                                                              100                                                                                                                               100
                                                                                                                           s1                                                                                                               s1                                                                                                                            s1
Mean bet on test item         Mean bet on test item                                                                              Mean bet on test item         Mean bet on test item                                                                               Mean bet on test item         Mean bet on test item
                                                                                    80                                                                                                                               80                                                                                                                                80
                        80
                                                                                                                           s2                            80
                                                                                                                                                                                                                                            s2                                             80
                                                                                                                                                                                                                                                                                                                                                                          s2
                                                         40    60      80   100                                                                                                           40    60      80   100                                                                                                            40    60      80   100
                        60                                                          60                                                                   60                                                          60                                                                    60                                                          60
                        40                                                          40                                                                   40                                                          40                                                                    40                                                          40
                        20                                     20                   20                                                                   20                                     20                   20                                                                    20                                     20                   20
                        0                                      0                    0                                                                    0                                      0                    0                                                                     0                                      0                    0
                                                                                          Test item type    TestTest
                                                                                                                 itemitem
                                                                                                                      typetype                                                                                             Test item typeTestTest
                                                                                                                                                                                                                                               item     typetype
                                                                                                                                                                                                                                                   itemitem
                                                                                                                                                                                                                                                  Test
                                                                                                                                                                                                                                                        type                                                                                                       Testitem
                                                                                                                                                                                                                                                                                                                                                                 Test          typeTest item type
                                                                                                                                                                                                                                                                                                                                                                        item type       Test item type
                                                                                                           Test item type
                                          Figure 4: Bets on the probability of “dax bren nes” generalizing to each of the relevant test item types in each condition.
                                          Error bars represent 95% confidence intervals as computed via non-parametric bootstrap. Example items seen in the training
                                          situations are given in the top-right box of each plot. Given the particular training items shown here, an item in each of the
                                          relevant test item types (defined by the number of features shared with the training items) is shown along the x-axis. Actual
                                          training items (and thus test items) were counter-balanced across participants.
                                          excluded for either not completing the task appropriately (e.g.                                                                                                                                Across participants, we manipulated the number of fea-
                                          by responding with values greater than 100) or failing to pro-                                                                                                                              tures shared within and across situations.2 We tested an un-
                                          vide responses for all 8 test objects. All reported that they                                                                                                                               ambiguous baseline condition in which the same object was
                                          were native speakers of English.                                                                                                                                                            paired with a different object in each situation and 5 ambigu-
                                                                                                                                                                                                                                      ous conditions in which the features of the objects were con-
                                          Stimuli Each object in our stimulus set was constructed                                                                                                                                     founded either within or across situations. For the ambigu-
                                          to have three binary features. The features of interest were                                                                                                                                ous conditions, we tested cases in which 1 or 2 features were
                                          shape, appendage and color. When fully permuted, this de-                                                                                                                                   shared within situations (“confounded within” conditions), 1
                                          fines a space of eight possible objects (see Figure 3).                                                                                                                                     or 2 features shared across situations (“confounded across”
                                                                                                                                                                                                                                      conditions), and a case in which 3 features were shared both
                                          Procedure Participants viewed a webpage that showed two                                                                                                                                     within and across situations (Figure 4).
                                          situations with two objects each. In the first situation, they
                                          were instructed: “Suppose you saw these two objects and
                                          heard ‘dax bren nes.’” A multi-word novel label was used to
                                          avoid biases towards meanings consistent with the grammat-                                                                                                                                      2 This manipulation was motivated by the observation that dif-
                                          ical class of the word. In other words, we wanted to avoid                                                                                                                                  ferent types of ambiguity license different inferences. To illustrate
                                          participants inferring that because the word was an adjective,                                                                                                                              this, imagine a learner in a confounded across context. The learner
                                          it was more likely to refer to a property (e.g. color) than a                                                                                                                               observes a situation with two apples and a situation with two or-
                                                                                                                                                                                                                                      anges. In each situation, she hears “dax bren nes”. The referent
                                          particular object (i.e. a proper noun), for example. Two more                                                                                                                               is clear in each individual situation — apple and orange, respec-
                                          objects were presented below and participants were asked to                                                                                                                                 tively — and the learner might infer that this phrase corresponds to
                                          “Now suppose you saw these two new objects and heard ‘dax                                                                                                                                   a superordinate category, such as FRUIT. In the confounded within
                                                                                                                                                                                                                                      context, the learner observes two situations, both containing an ap-
                                          bren nes’ again.” They were then asked to “bet whether or not                                                                                                                               ple and an orange, and again hears “dax bren nes” in each. Unlike
                                          you think each of the objects below could also be called ‘dax                                                                                                                               in the across case, a learner in this context would have no informa-
                                          bren nes.”’ Images of all eight objects (including the train-                                                                                                                               tion about how to correctly map the meaning of this phrase, since
                                                                                                                                                                                                                                      the context is consistent with both a subordinate and superordinate
                                          ing items) were then presented, and participants were asked                                                                                                                                 interpretation. Different generalization patterns are thus predicted in
                                          to provide a bet 0–100 indicating their judgement.                                                                                                                                          the confounded across and within conditions.
                                                                                                                                                                                                                              885

                                              Cross-Situational Concept Model                                                  Feature Distance Model
                                    100                                                                        100
                                    80                                                                         80
                   mean human bet                                                             mean human bet
                                    60                                                                         60
                                    40                                                                         40
                                    20                           r = 0.95                                      20                                      r = 0.89
                                          8   10     12          14           16   18   20                           0    20        40          60          80    100
                                                          model predictions                                                        model predictions
Figure 5: Mean bets in each experiment condition, plotted by predictions for each of the models (model predictions are scaled
on the horizontal axis). Error bars represent 95% confidence intervals, as computed via non-parametric bootstrap. The line of
best fit is plotted in red.
Results and Model Fits                                                                                         minimal version of this experiment, the learner could observe
Participants showed a consistent gradient of generalization                                                    w1 with [11] and [12] and w2 with [22] and [12]. A learner
such that greater number of distinct features resulted in lower                                                who assumes that the speaker refers to both objects within
bets (weaker generalizations), consistent with previous exper-                                                 each situation, might infer a mapping between w1 with [1⇤]
iments (Figure 4).                                                                                             and a mapping between w2 with [⇤2], given this referential
   Model fits are shown in Figure 5. Our model fits the data                                                   context. Using situations such as these, this paradigm can be
with a correlation of r = .95. We compared this fit against                                                    extended to directly explore joint inference of both the Map-
a null model in which we calculated the target’s total fea-                                                    ping and Generalization problems.
ture distance from objects in the situations. This was calcu-                                                     An important underlying assumption of this model is that
lated by counting the number of features for which the target                                                  features are given a priori. This seems like an extreme po-
differed from each situation object (e.g. the feature distance                                                 sition given that it is implausible that children acquiring lan-
[111] and [122] is 2) and summing across all four objects in                                                   guage have an innate “appendage” feature, for example. It is,
the situation. This standard exemplar style model fits the data                                                in a sense, the very goal of this model to explain how children
relatively well (r = .89). Nevertheless, our model provides                                                    acquire such abstract concepts as APPENDAGE. That is, fea-
a substantial gain in fit. Using non-parametric bootstrap, the                                                 tures are themselves concepts that can be considered as prim-
cross-situational concept-learning model fits the data signifi-                                                itives in the construction of more complex concepts. This
cantly better than the feature distance model (p < .05).                                                       problem, however, is not specific to the word learning prob-
                                                                                                               lem, but rather is a challenge more generally to the Boolean
                  General Discussion                                                                           concept learning framework. Nonetheless, a complete theory
In this cross-situational Boolean concept learning task, our                                                   of how children acquire word concepts will need to provide
model performed competitively with a simple feature dis-                                                       an account for the origin of features.
tance model. Critically, however, our model has the ma-                                                           Given this theoretical point, our model should be under-
chinery to solve not only this simple concept-mapping prob-                                                    stood as a computational level description of the problem of
lem under minimal ambiguity, but can also deal with more                                                       acquiring word-concepts, given some set of concepts (i.e. fea-
complex worlds in which multiple words are present. Given                                                      tures). Our model remains agnostic about the origins and na-
that no existing model is able to jointly account for both the                                                 ture of these initial concepts but, given some primitive set
Mapping and Generalization Problems, this model provides a                                                     of concepts, our model describes how a learner might boot-
fruitful theoretical tool for future work to explore how chil-                                                 strap from these primitives to infer more and more complex
dren might solve these problems.                                                                               concepts. While it seems unlikely that children have an in-
   For example, this experiment could be straight-forwardly                                                    nate APPENDAGE feature, there is evidence that children may
extended to introduce a more complex Mapping Problem                                                           have certain perceptual categories, such as color, very early in
component to the task. This could be done by adding ad-                                                        development (Bornstein, Kessen, & Weiskopf, 1976). Primi-
ditional words to the cross-situational learning context. In a                                                 tive perceptual features like color categories may provide the
                                                                                             886

initial building blocks for the construction of more complex         Markman, E., Wasow, J., & Hansen, M. (2003). Use of
concepts, given experience with the environment.                       the mutual exclusivity assumption by young word learners.
   In sum, our model provides a rich framework for studying            Cognitive Psychology, 47(3), 241–275.
the word learning problem at the computational level. Previ-         Medin, D., & Ortony, A. (1989). Psychological essentialism.
ous research has explored how children might solve the two             Similarity and Analogical Reasoning, 179–195.
subproblems associated with word learning — the General-             Pinker, S. (1984). Language learnability and language de-
ization and Mapping Problems — separately. Our model con-              velopment, with new commentary by the author (Vol. 7).
tributes to this area by providing a unifed account for both of        Harvard University Press.
these problems. The experiment reported here suggests that           Quine, W. (1960). Word and object (Vol. 4). The MIT Press.
our model is able to account for participants’ behavior in solv-     Rosch, E., & Mervis, C. (1975). Family resemblances: Stud-
ing one of these problems — the Generalization Problem —               ies in the internal structure of categories. Cognitive Psy-
in a simple cross-situational task. Importantly, our model’s           chology, 7(4), 573–605.
contribution to theories of the Generalization Problem is to         Rosch, E., Mervis, C., Gray, W., Johnson, D., & Boyes-
provide an account of the generalization inferences, given an          Braem, P. (1976). Basic objects in natural categories. Cog-
initial set of primitive concepts. This account, coupled with          nitive Psychology, 8(3), 382–439.
the ability to explore the Mapping Problem, lays the ground-         Shepard, R., Hovland, C., & Jenkins, H. (1961). Learning
work for a more cohesive understanding of how children learn           and memorization of classifications. Psychological Mono-
the meanings of words.                                                 graphs: General and Applied, 75(13), 1–42.
                                                                     Sloutsky, Lo, Y.-F., & Fisher, A. V. (2001). How much does
                    Acknowledgements                                   a shared name make things similar? Linguistic labels, sim-
                                                                       ilarity, and the development of inductive inference. Child
We thank Mia Kirkendoll for her assistance in data collection.
                                                                       Development, 72(6), 1695-1709.
                          References                                 Smith, L. B., Jones, S. S., Landau, B., Gershkoff-Stowe, L.,
                                                                       & Samuelson, L. (2002). Object name learning provides
Akhtar, N., & Montague, L. (1999). Early lexical acquisi-              on-the-job training for attention. Psychological Science,
   tion: The role of cross-situational learning. First Language,       13(1), 13-19.
   19(57), 347–358.                                                  Smith, L. B., & Yu, C. (2008). Infants rapidly learn word-
Bornstein, M., Kessen, W., & Weiskopf, S. (1976). Color vi-            referent mappings via cross-situational statistics. Cogni-
   sion and hue categorization in young human infants. Jour-           tion, 106(3), 1558–1568.
   nal of Experimental Psychology: Human Perception and              Xu, F., & Tenenbaum, J. (2007). Word learning as Bayesian
   Performance, 2(1), 115.                                             inference. Psychological Review, 114(2), 245.
Clark, E. (1987). The principle of contrast: A constraint on         Yu, C., & Smith, L. (2007). Rapid word learning under un-
   language acquisition. Mechanisms of language acquisition.           certainty via cross-situational statistics. Psychological Sci-
   Hillsdale, NJ: Erlbaum.                                             ence, 18(5), 414–420.
Diesendruck, G., & Markson, L. (2001). Children’s avoid-
   ance of lexical overlap: A pragmatic account. Develop-
   mental Psychology, 37(5), 630.
Frank, M. C., Goodman, N., & Tenenbaum, J. (2009). Us-
   ing speakers’ referential intentions to model early cross-
   situational word learning. Psychological Science, 20(5),
   578.
Goodman, N., Tenenbaum, J., Feldman, J., & Griffiths, T.
   (2010). A rational analysis of rule-based concept learning.
   Cognitive Science, 32(1), 108–154.
Laurence, S., & Margolis, E. (1999). Concepts: Core read-
   ings. In E. Margolis & S. Laurence (Eds.), (chap. 1). MIT
   Press.
Lewis, M., & Frank, M. C. (2013). Modeling disambiguation
   in word learning via multiple probabilistic constraints. In
   Proceedings of the 35th Annual Meeting of the Cognitive
   Science Society.
Markman, E. (1990). Constraints children place on word
   meanings. Cognitive Science, 14(1), 57–77.
Markman, E., & Wachtel, G. (1988). Children’s use of mutual
   exclusivity to constrain the meanings of words. Cognitive
   Psychology, 20(2), 121–157.
                                                                 887

