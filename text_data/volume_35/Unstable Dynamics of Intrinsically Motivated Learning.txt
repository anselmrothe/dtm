UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Unstable Dynamics of Intrinsically Motivated Learning
Permalink
https://escholarship.org/uc/item/1fj9s25r
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Zgonnikov, Arkady
Lubashevsky, Ihor
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                        Unstable Dynamics of Intrinsically Motivated Learning
                                       Arkady Zgonnikov (arkady.zgonnikov@gmail.com)
                       University of Aizu, Tsuruga, Ikki-machi, Aizu-wakamatsu, 965-8580 Fukushima, Japan
                                             Ihor Lubashevsky (i-lubash@u-aizu.ac.jp)
                       University of Aizu, Tsuruga, Ikki-machi, Aizu-wakamatsu, 965-8580 Fukushima, Japan
                               Abstract                                  ponent’s strategies (Galla, 2009, 2011) and scale-free mem-
                                                                         ory (Lubashevsky & Kanemoto, 2010). Virtually all men-
   Employing the dynamical systems framework, we study the
   effects of intrinsic motivation on the dynamics of the learn-         tioned studies emphasize that the learning process dynam-
   ing processes. The intrinsic motivation here is the one’s desire      ics in game theoretic setting is naturally rich and non-trivial.
   to learn not because it may cause some benefits in future, but        Even the simplest systems of two agents learning to play
   due to the inherent joy obtained by the very process of learn-
   ing. We study a simple example of a single agent adapting             rock-paper-scissors game may produce quasiperiodic tori,
   to unknown environment; the agent is biased by the desire to          limit cycles and deterministic chaos (Sato et al., 2002; Sato &
   select the actions she has little information about. We show          Crutchfield, 2003); the latter is often reported to be a common
   that intrinsic motivation may cause the instability of the learn-
   ing process that is stable in the case of rational agent. There-      behavior of dynamical systems describing learning processes
   fore, we suggest that the effects of human intrinsic motivation       (Sato et al., 2005; Lubashevsky & Kanemoto, 2010; Galla &
   in particular and the irrationality in general may be of excep-       Farmer, 2013).
   tional importance in complex sociopsychological systems and
   deserve much attention in the formal models of such systems.             Indeed, the perfect rationality axiom appears unsuitable in
   Keywords: Mathematical modeling; decision making; learn-              a whole class of problems. As one may see, this fact moti-
   ing; dynamical systems.                                               vated much current research on the development of the learn-
                                                                         ing approach to game theory and corresponding mathematical
                           Introduction                                  models of learning. The canonical game theory implies that
Mathematical models of learning play great role in a diverse             a player has full information about both the game played and
range of fields, with eminent applications found in cogni-               the opponents faced. In contrast, the learning paradigm hy-
tive science (Daw, O’Doherty, Dayan, Seymour, & Dolan,                   pothesizes that most of this information is concealed from the
2006; Burke, Tobler, Baddeley, & Schultz, 2010; Ahn et al.,              players, who only possess the complete knowledge about the
2012), artificial intelligence (Sutton & Barto, 1998) and game           set of available actions and gradually learn the consequences
theory (Fudenberg & Levine, 1998). The latter traditionally              of these actions. Even so, in the vast majority of situations
concentrates on the analysis of the Nash equilibria in games             studied within the learning framework so far the agents are
played by perfectly rational agents, thereby imposing “heroic            practically assumed to be strictly rational. In other words,
assumptions about the knowledge and calculating abilities of             even learning agents still act selfishly and optimally; their ra-
the players”(Macy & Flache, 2002). It is the learning ap-                tionality is bounded only in the sense of having less a priori
proach to game theory that addresses this issue by focusing on           information. Put within the constraints imposed by the learn-
the adaptive behavior of the players. First, it assumes that the         ing paradigm, agents now have to learn the appropriate behav-
agents initially know little about the game context and should           ior strategy, but their final goal remains ultimately rational —
gradually explore the game while it is repeated indefinitely             to maximize the total payoff throughout the whole process.
many times. Second, players base their actions solely on the             It means that in the course of learning the agent behavior is
previous observations; they learn by trial and error while their         driven only by external factors — the actions of other play-
ultimate goal is to maximize the cumulative payoff through-              ers and the corresponding payoffs observed previously. In
out the game.                                                            the modern dynamical models of learning the agents basically
   In the game learning setting it turns out that the players            lack any kind of personality, they posess no emotions, desires
often fail to eventually come up with a certain efficient strat-         or personal preferences. Up to now it is completely unknown
egy (either pure or mixed), so their behavior can not be char-           how the dynamics of the learning would change if the agents
acterized in terms of Nash equilibria. Therefore, the inher-             are endowed with any kind of individuality. In the present
ent dynamics of learning becomes vital. A growing number                 study we face this problem.
of studies develop the theory behind the applications of dy-                One of the most important aspects of learning processes
namical systems to learning. Coupled replicator equations                is the intrinsic motivation, which is commonly defined as an
were proposed as a framework for describing the adaptive                 inspiration to do something “because it is inherently interest-
behavior of multiple learning agents interacting via a sim-              ing or enjoyable” (Ryan & Deci, 2000). In contrast, extrin-
ple game (Sato, Akiyama, & Farmer, 2002; Sato & Crutch-                  sic motivation refers to doing something “because it leads to
field, 2003; Sato, Akiyama, & Crutchfield, 2005). Based                  a separable outcome” (Ryan & Deci, 2000). In relation to
on this formalism a whole range of agent behavior proper-                learning, an intrinsically motivated person learns something
ties have been modelled, including noisy perception of op-               not (or not only) because it will lead her to a tangible reward
                                                                     3853

or payoff, but for the sake of joy obtained by the learning            ation where the agent is not provided with any information
itself. Such person innately likes the very process of gain-           about the foregone payoffs (also known as choice reinforce-
ing new knowledge. The concept of intrinsic motivation is              ment (Ho, Camerer, & Chong, 2007)), in contrast to the con-
widely studied in psychology (Deci & Ryan, 1985; Ryan &                ventional weighted fictitious play scheme. The agent accu-
Deci, 2000) and has vital applications in education, as well           mulates the memories of the obtained rewards, and in such
as in organizational psychology and psychotherapy. Besides,            manner builds up an inner myopic model of the outer world.
intrinsically motivated reinforcement learning (Oudeyer, Ka-           Each time the agent makes a choice she relies on the currently
plan, & Hafner, 2007; Oudeyer et al., 2007; Singh, Lewis,              collected information about the quality of both actions, and,
Barto, & Sorg, 2010) is a hot topic in computer science: ma-           at the same time, is affected by her intrinsic motivation to
chine learning algorithms inspired by human cognitive pro-             learn, or to obtain new information. We interpret the latter in
cesses demonstrate improved performance in a wide class of             a sense that the agent inherently likes to select the options that
tasks. Still, despite the solid theoretical basis of intrinsically     add much new information to her inner model of the world.
motivated learning, the dynamics of such learning processes            Therefore, at each instant t there are three values associated
remains a murky subject. What is the impact of the intrin-             with each option xi :
sic motivation on the outcome of a learning process? Can we
expect that intrinsic (and in a certain sense irrational) desire      1. pi — the probability of choosing xi at time t
to learn will change the agent behavior substantially? Do the         2. qi — the agent memories about the rewards obtained in the
intrinsic motives deserve as close attention as the extrinsic             past for selecting xi (objective quality of xi )
ones?
   Employing the dynamical systems framework, we propose              3. ni — the novelty of the option xi (subjective quality of xi )
a toy model capturing the effects of intrinsic motivation to
learn. We study the example of a single agent facing an un-            In order to complete the model, we, first, define how the
known environment, who is forced to make a repeated choice             choice probability pi depends on qi and ni . Second, we write
between two rewarded alternatives. The purpose of the agent            the equations describing time evolution of the agent memo-
is to maximize the total sum of the rewards gained through-            ries about xi and corresponding values of novelty.
out the process; the agent therefore should learn which of the            The Boltzmann distribution (sometimes referred to as
alternatives is better rewarded. The key point of the present          “softmax” model) fits much experimental data and is com-
study is that the agent is biased: along with collecting the           monly used as a model for randomized human choice. We
rewards, she also satisfies the internal need to acquire new           adopt it as a probability of choosing action xi at time t
knowledge. Therefore, the agent behavior is governed by two
factors: objective (to gain as much reward as possible) and                                               eβ[qi +ni ]
                                                                                              pi (t) =                 ,              (1)
subjective (to satisfy the internal desire to learn). Our global                                       ∑ eβ[q j +n j ]
                                                                                                        j
aim in the present paper is to demonstrate on this simple ex-
ample how such subjective factors may greatly impact on the            where qi + ni represents the total quality of option xi . Here
dynamics of systems describing human behavior.                         without loss of generality we assume that objective and sub-
                                                                       jective factors are equally important for the agent. The con-
                             Model                                     stant parameter β defines to what extent the agent choice is
We construct the continuous-time reinforcement learning                randomized (β = 0 corresponds to the completely random
model of a single agent adaptation, or learning, under the ef-         choice, while β = ∞ makes the agent always select the op-
fect of intrinsic motivation. The discrete-time learning mod-          tion with the highest total quality).
els is the more conventional way to describing the learning               We describe the evolution of the objective values qi , i =
processes. However, for purposes of analysis of system dy-             1, 2 over time by the following differential equations:
namics the continuous models are more appropriate. We re-                                                             qi
frain from discussing the connection between the discrete-                                 q̇i = W (qi , q)ri pi −       ,            (2)
                                                                                                                      Tq
time and continuous-time reinforcement learning formula-
tion, which is covered in detail in the literature (Sato et al.,       where pi is defined by expression (1), ri is the reward asso-
2005; Lubashevsky & Kanemoto, 2010). We only note that                 ciated with action xi . Term ri pi can be regarded as a basic
the continuous-time process is actually the limit case of the          reinforcement, which is subjected to saturation effect. The
discrete-time learning, when the learning agent repeatedly             term Tqqi stands for the effect of the bounded capacity of the
makes a choice infinitely many times.                                  agent’s memory. The events in the past separated from the
   In our model the agent interacts with the unknown envi-             present by the time considerably exceeding Tq practically do
ronment by repeatedly choosing one of the two available ac-            not affect the agent’s behavior.
tions xi , i ∈ 1, 2 and receiving corresponding reward ri . Af-           The saturation factor W (qi , q) is a weighting function de-
ter each decision, only the action that was actually chosen is         pending on q = (q1 , q2 ). We chose W (qi , q) in such way that
being reinforced. In game theory it corresponds to the situ-           it bounds the infinite growth of the objective value function.
                                                                   3854

In other words, it implements the saturation effect: we tend
                                                                                               25
to underestimate frequent events and overestimate rare ones.
We define W (qi , q) as logistic function                                                      20
                                                                                   value
                                                                                               15
                                     1
                    W (qi , q) =         qi −q   ,           (3)                               10
                                                                                                                            objective quality, q1
                                   1+e      γ
                                                                                                5                           novelty, n1
where γ is the saturation coefficient and q = q1 +q    2
                                                   2 . If the                                   0
                                                    qi −q                                        0   100   200             300            400       500
current objective value of xi is relatively large ( γ  1),                                                      time, t
the probability pi is very high and xi is selected frequently,                                  1
so the agent underestimates the reward gained: W (qi , q) ≈ 0.
                                                                                               0.8
                                                                          probability, p1(t)
On the opposite, for the rarely selected actions pi is low (so
qi −q
   γ  1), and when such actions are chosen the agent pays
                                                                                               0.6
full attention to their reward: W (qi , q) ≈ 1.                                                0.4
    In order to take into account the effect of intrinsic motiva-
                                                                                               0.2
tion to select the options that brings much information to the
agent environment model, we augment system (1–3) with the                                       0
                                                                                                 0   100   200             300            400       500
equations describing time evolution of the novelty values for                                                    time, t
each option
                                          ni                          Figure 1: Stable dynamics of the analyzed system. Top frame
                     ṅi = φ(1 − pi ) −      .               (4)      illustrates the time evolution of the objective quality q1 and
                                          Tn
                                                                      novelty value n1 ; the bottom frame represents the choice
Here φ is the parameter indicating agent’s novelty rate that          probability p1 evolution. The time series were obtained for
is the same for all of the alternative choices. In analogy to         the time span of 500 units and following values of system pa-
the equation (2) we define the memory capacity coefficient            rameters: r1 = r2 = 1, β = 5, φ = 1, γ = 1, Tq = 70, Tn = 50;
Tn accounting for the characteristic duration of the novelty          the initial conditions were chosen randomly.
effect.
   Equations (1–4) form the basic model of the agent adap-
tation under the assumptions stated above. In the rest of the         stable dynamics was found only for relatively narrow sets of
paper we present the preliminary analysis of the results of           parameters.
the numerical experiments aimed at elucidation of the basic              The typical example of the stable dynamics is illustrated
properties of the developed model.                                    in Fig. 1. The agent eventually learns the mixed strategy
                                                                      p1 = p2 = 0.5, which is the stable equilibrium of the sys-
                 Numerical simulation                                 tem. However, it is instability that often characterizes the hu-
Prior to discussion of the numerical results, we have to under-       man behavior, so we focus our attention on the second case.
line that the similar system describing the behavior of rational      Fig. 2 represents the periodic motion of the system at hand.
agent have been analyzed previously (Sato et al., 2005). It has       As can be seen from the top two frames, the system trajec-
been elucidated that the system dynamics in case of rational          tory forms a limit cycle. Starting from the randomly selected
agent is very simple. Namely, the agent tends to one of the           intial values, system variables undergo periodic oscillations
equilibria depending on the system parameters, and the se-            after a short transition process. The observed dynamical pat-
lected equilibria is stable with respect to the perturbations of      terns correspond to the case when the decision maker changes
intial conditions. Therefore, there are very few studies in-          her preferences from time to time, or, in other words, period-
vestigating the single agent adaptation problems, due to the          ically “switches” from one alternative to another. The im-
absence of any complications of system behavior. We show              plicit dependence between the objective quality and the nov-
that introducing intrinsic motivation makes the situation com-        elty of the option can bee seen in the bottom left frame of
pletely different.                                                    Fig. 2. When the quality of the alternative (as represented in
   Under the assumption of equal rewards (r1 = r2 = 1) we             the agent memories) attains local maximum, the correspond-
numerically simulated the dynamics of system (1–4). We dis-           ing choice probability also peaks. So this alternative is cho-
covered that depending on the values of the system parame-            sen frequently during some period of time and, thus, its nov-
ters the structure of the system phase space trajectory may           elty takes the lowest possible value. On the other hand, when
take one of two general forms: either the stable equilibrium          the probability of xi being chosen is low, the agent has rela-
exists or the system is unstable and has the limit cycle. We          tively little information about the consequences of this action
have not aimed at analyctically deriving the explicit condi-          (because the memories about it eventually vanish if not re-
tions of the system instability, but the empirical observations       inforced regularly). Therefore, the agent intrinsic desire to
indicate that the stable behavior is rather common, while un-         learn motivates her to choose this option due to the relatively
                                                                   3855

                                   10
                                                                                                                             14
       novelty difference, n1−n2
                                    5                                                                                        12
                                    0                                                                     novelty, n1        10
                                                                                                                              8
                                   −5
                                                                                                                              6
                                   −10                                                                                        4
                                    −10     −5              0              5                      10                          10   12       14         16        18      20
                                           objective quality difference, q1−q2                                                           objective quality, q1
                                   20                                                                                         1
                                                                                                                             0.8
                                                                                                        probability, p1(t)
                                   15
                                                                                                                             0.6
                         value     10
                                                                                                                             0.4
                                    5                                     objective quality, q1
                                                                          novelty, n1
                                                                                                                             0.2
                                    0                                                                                         0
                                     0    100       200             300           400             500                          0   100     200             300   400     500
                                                          time, t                                                                                time, t
Figure 2: Unstable dynamics of the analyzed system. Top left frame illustrates the system trajectory on the plane (q1 − q2 , n1 −
n2 ), while top right depicts the projection of the system phase space trajectory onto the (q1 , n1 ) plane. Two bottom frames
demonstrate the time evolution of the objective quality q1 , novelty value n1 (both on the bottom left frame) and the choice
probability p1 (bottom right) for option x1 . Represented results were obtained for the time span of 500 units and following
values of system parameters: r1 = r2 = 1, β = 2, φ = 0.4, γ = 4, Tq = 70, Tn = 50; the initial conditions were chosen randomly.
large amount of information that the agent might acquire.                                                          and the parameters characterizing the capacity of the agent
   Finally, the evolution of the choice probability p1 (t) (see                                                    memory.
bottom right frame in Fig. 2) demonstrates that during con-
siderable periods of time the probability of choosing x1 re-                                                                                 Conclusion
mains close to zero; these intervals slightly precede the peri-                                                    We have proposed a dynamical model of intrinsically mo-
ods when q1 is low and n1 is high. Then, after staying within                                                      tivated learning. In the various learning models developed
the vicinity of zero, the probability rapidly reaches the max-                                                     previously in game theory and cognitive science the learning
imum value around unity and in turn remains near this value                                                        subject is assumed to act rationally in achieving the ultimate
for the next half-cycle.                                                                                           goal — to maximize the cumulative reward gained during the
   The conducted numerical analysis confirms that the the                                                          learning. We challenge this approach by assigning a piece of
system (1–4) actually exhibits the properties one may intu-                                                        non-rationality to the learning agent. The curiosity is what
itively anticipate from the intrinsically motivated agent. The                                                     biases the selfish agent in our model.
agent learns one of the optimal options, but being biased                                                             We confine our scope to the case of single agent adapta-
she eventually tends to discard the established strategy that                                                      tion and follow the reinforcement learning setting. The agent
proved its efficiency in favor of the novel one. Moreover, the                                                     behavior in our model is governed by two stimuli. The ob-
preliminary analysis of the non-symmetric case revealed that                                                       jective stimulus is traditional — to maximize the total pay-
the similar behavior can be observed even when the rewards                                                         off collected throughout the process. The subjective one is
are not equal. This fact requires a thorough investigation and                                                     irrational — to engage in active learning as much as possi-
will be reported elsewhere.                                                                                        ble, because the very learning process is enjoyable. We show
   The results presented in the present work already enable                                                        that the agent biased in such way at least under some condi-
us to conclude that even the simplest systems with boundedly                                                       tions does not stick to the optimal strategy of behavior, in con-
rational agents may exhibit non-trivial dynamics. However,                                                         trast to the rational learning agent. Rather, in such cases the
more detailed analysis of the proposed model is required. Par-                                                     agent preference continuously varies in an oscillatory way.
ticularly, the system stability conditions are still to be deter-                                                  Performing the simple numerical analysis of the model, we
mined. Also under the scope of future work is the question                                                         demonstrate that the intrinsic motivation leads to the instabil-
of how the system dynamics patterns depend on the sysem                                                            ity of the learning dynamics.
parameters, namely, the novelty rate, perception thresholds                                                           Our results give evidence to the fact that the intrinsic mo-
                                                                                                  3856

tivation in particular and the bounded rationality in general          Economic Theory, 133(1), 177–198.
may cause the significant changes in the behavior of single-         Lubashevsky, I., & Kanemoto, S. (2010). Scale-free memory
and multi-agent systems. We argue that the intrinsic motives           model for multiagent reinforcement learning. Mean field
should be paid no less attention than the extrinsic ones, if one       approximation and rock-paper-scissors dynamics. The Eu-
considers the systems where human decisions are of the pri-            ropean Physical Journal B-Condensed Matter and Com-
mary importance.                                                       plex Systems, 76(1), 69–85.
                                                                     Macy, M., & Flache, A. (2002). Learning dynamics in so-
                         References                                    cial dilemmas. Proceedings of the National Academy of
Ahn, W.-Y., Rass, O., Shin, Y.-W., Busemeyer, J., Brown,               Sciences, 99(Suppl 3), 7229–7236.
   J., & O’Donnell, B. (2012). Emotion-based reinforcement           Oudeyer, P., Kaplan, F., & Hafner, V. (2007). Intrinsic moti-
   learning. In Proceedings of the 34th Annual Conference of           vation systems for autonomous mental development. Evo-
   the Cognitive Science Society (pp. 124–129).                        lutionary Computation, IEEE Transactions on, 11(2), 265–
Burke, C., Tobler, P., Baddeley, M., & Schultz, W. (2010).             286.
   Neural mechanisms of observational learning. Proceed-             Ryan, R., & Deci, E. (2000). Intrinsic and extrinsic motiva-
   ings of the National Academy of Sciences, 107(32), 14431–           tions: Classic definitions and new directions. Contempo-
   14436.                                                              rary educational psychology, 25(1), 54–67.
Daw, N., O’Doherty, J., Dayan, P., Seymour, B., & Dolan,             Sato, Y., Akiyama, E., & Crutchfield, J. (2005). Stability and
   R. (2006). Cortical substrates for exploratory decisions in         diversity in collective adaptation. Physica D: Nonlinear
   humans. Nature, 441(7095), 876–879.                                 Phenomena, 210(1), 21–57.
Deci, E., & Ryan, R. (1985). Intrinsic motivation and self-
                                                                     Sato, Y., Akiyama, E., & Farmer, J. (2002). Chaos in learning
   determination in human behavior. Springer.
                                                                       a simple two-person game. Proceedings of the National
Fudenberg, D., & Levine, D. (1998). The theory of learning
                                                                       Academy of Sciences, 99(7), 4748–4751.
   in games. MIT press.
                                                                     Sato, Y., & Crutchfield, J. (2003). Coupled replicator equa-
Galla, T. (2009). Intrinsic noise in game dynamical learning.
                                                                       tions for the dynamics of learning in multiagent systems.
   Physical review letters, 103(19), 198702.
                                                                       Physical Review E, 67(1), 015206.
Galla, T. (2011). Cycles of cooperation and defection in im-
                                                                     Singh, S., Lewis, R., Barto, A., & Sorg, J. (2010). Intrin-
   perfect learning. Journal of Statistical Mechanics: Theory
                                                                       sically motivated reinforcement learning: An evolution-
   and Experiment, 2011(08), P08007.
                                                                       ary perspective. Autonomous Mental Development, IEEE
Galla, T., & Farmer, J. (2013). Complex dynamics in learning
                                                                       Transactions on, 2(2), 70–82.
   complicated games. Proceedings of the National Academy
                                                                     Sutton, R., & Barto, A. (1998). Reinforcement learning: An
   of Sciences, 110(4), 1232–1236.
                                                                       introduction (Vol. 1) (No. 1). Cambridge Univ Press.
Ho, T., Camerer, C., & Chong, J. (2007). Self-tuning ex-
   perience weighted attraction learning in games. Journal of
                                                                 3857

