UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Neural-network Modelling of Bayesian Learning and Inference
Permalink
https://escholarship.org/uc/item/0818h70q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Kharratzadeh, Milad
Shultz, Thomas
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                  Neural-network Modelling of Bayesian Learning and Inference
                                 Milad Kharratzadeh (milad.kharratzadeh@mail.mcgill.ca)
                  Department of Electrical and Computer Engineering, McGill University, 3480 University Street
                                                     Montreal, QC H3A 2A7 Canada
                                         Thomas R. Shultz (thomas.shultz@mcgill.ca)
              Department of Psychology and School of Computer Science, McGill University, 1205 Penfield Avenue
                                                     Montreal, QC H3A 1B1 Canada
                              Abstract                                 ing and inference in a general form. We do this by using three
   We propose a modular neural-network structure for imple-            main modules, two responsible for computing priors and like-
   menting the Bayesian framework for learning and inference.          lihoods based on observations, and one responsible for apply-
   Our design has three main components, two for computing the         ing Bayes rule and computing the posteriors. We show that
   priors and likelihoods based on observations and one for apply-
   ing Bayes’ rule. Through comprehensive simulations we show          our model is able to successfully implement Bayesian learn-
   that our proposed model succeeds in implementing Bayesian           ing and inference and replicate analytical results with high
   learning and inference. We also provide a novel explanation of      precision in a brain-like fashion which could later be used to
   base-rate neglect, the most well-documented deviation from
   Bayes’ rule, by modelling it as a weight decay mechanism            gain intuition into how brains implement Bayesian reasoning.
   which increases entropy.                                            Our work also provides a framework to study the deviations
   Keywords: Neural-network; Bayes’ rule; Bayesian learning            from optimal Bayesian reasoning which result from base-rate
   and inference; base-rate neglect; weight decay; entropy
                                                                       neglect (Kahneman & Tversky, 1996; Eddy, 1982).
                          Introduction                                    Our work is novel in its precise and complete implementa-
Bayesian models are becoming prominent across a wide                   tion of a Bayesian framework in a modular, brain-like fash-
range of problems in cognitive science including inductive             ion. The proposed network takes observations as inputs
learning (Tenenbaum, Kemp, & Shafto, 2006), language ac-               and computes the posterior probabilities (i.e., updated be-
quisition (Chater & Manning, 2006), and vision (Yuille &               liefs). Moreover, using a fast, constructive learning algorithm
Kersten, 2006). While these Bayesian ideas provide compu-              (sibling-descendent cascade-correlation) for the network pro-
tation level models, it is beneficial, and sometimes necessary,        vides the advantage of a self-organizing learning and infer-
to appeal to some implementation-level (biological) models             ence method which is similar to humans’ developmental, au-
to explain human behaviour. Connectionist approaches pro-              tonomous inference and learning (Shultz & Fahlman, 2010).
vide a neural-based model of cognitive processes.                      Another novelty of this work is the modelling of base-rate
   There is growing evidence in neuroscience supporting the            neglect as a weight decay mechanism.
relevance of Bayesian models on a neural level (Doya, Ishii,              The idea of neural implementation of Bayesian phenom-
Pouget, & Rao, 2007). Many perceptual and sensorimotor                 ena was suggested before. Shi and Griffiths (2009) intro-
tasks that are learned and performed by the central nervous            duced a scheme for implementing importance sampling with
system can be described in a Bayesian framework (Konrad &              radial basis function (RBF) networks. They assume that the
Wolpert, 2004). Neural computations, as simple as summing              unit activation functions are of radial basis type. Hence, in
up the firing rates, can be seen as analogous to a Bayesian            their model, a single RBF neuron measures likelihood, result-
inference process, with population activity patterns encoding          ing in a straightforward implementation of importance sam-
posterior distributions (Pouget, Dayan, & Zemel, 2003).                pling. Also, Griffiths et al. (2012) used a linear network to
   In theoretical terms, connectionist models show promis-             approximate the generalization performance of a probabilis-
ing prospects in implementing computational-level Bayesian             tic model. Their linear networks produce different solutions
ideas. Under certain assumptions, and inspired by the na-              from Bayes on structures other than those based on Wishart
ture of neural activation functions, neural units can compute          priors. Our work differs in two major ways. First, we as-
posterior probability values (McClelland, 1998). So-called             sume that the characteristics of the likelihood/priors and the
Boltzmann machines, a type of stochastic recurrent neural              network’s structure are unknown a priori and learn them for
network, were also suggested to implement Bayesian learn-              a wide range of likelihood and prior distributions through a
ing (Ackley, Hinton, & Sejnowski, 1985). Later work intro-             constructive training phase. Also, these functions are learned
duced generative networks called restrictive Boltzmann ma-             precisely (i.e., no approximation) using a population of neu-
chines that can make bidirectional inferences based on what            rons with simple though realistic activation functions (sig-
they learned (Hinton & Osindero, 2006).                                moid) rather than linear or complex ones (RBF).
   All these connections between Bayesian and neural-
network models motivate further exploration of the relation             The Basics of Bayesian Learning and Inference
between the two. In this paper, we propose a complete, mod-            The Bayesian framework addresses the problem of updating
ular neural-network structure implementing Bayesian learn-             beliefs and making inferences based on observed data. As-
                                                                   2686

sume that we have a set of mutually exclusive and exhaus-
tive hypotheses, H = {h1 , . . . , hN }, and want to infer which
of these hypotheses best explains observed data. In this set-
ting, we denote the degree of belief in different hypotheses
by probabilities. Bayesian inference is based on a simple for-
mula known as Bayes’ rule. This rule specifies how posterior
probabilities (of a hypothesis being true given the observed
data) can be computed using the product of data likelihood
and prior probabilities:
                     P(d|hi )P(hi )       P(d|hi )P(hi )
         P(hi |d) =                 = N                    .  (1)
                         P(d)          ∑i=1 P(d|hi )P(hi )           Figure 1: Module 1 computes the posterior based on the like-
   Priors, P(hi ), represent how much we believe in a hypoth-        lihoods and prior and according to Bayes’ rule.
esis before observing data. Likelihoods, P(d|hi ), denote the
probability with which we would expect to observe these                 Module 1 assumes that the values of prior and likelihood
data if a hypothesis were true. The denominator, known as            probabilities are given and then applies Bayes’ rule using
marginal probability of data, is a normalizing sum which en-         them as input. Module 2 (shown in Fig. 2) is responsible for
sures that the posteriors for all hypotheses sum to one.             computing the likelihoods. It takes observation(s) as input(s).
   The Bayesian framework is generative. This means that             Due to the generative nature of the Bayesian framework, we
observed data are generated by an underlying mechanism or            describe likelihoods as probability distributions. The role
hypothesis. Then, the role of inference is to evaluate different     of module 2 is to learn these distributions as the underlying
hypotheses and choose the one which is the most likely mech-         mechanisms generating data. However, this should be done
anism responsible for generating the data (i.e., the one with        without any implicit or explicit knowledge about the specifi-
the highest posterior probability). These generative processes       cations of the distribution and solely based on the observed
can be specified by probabilistic models. Here, we describe          training data.
likelihoods and priors respectively with probability distribu-
tions and mass functions as their generative mechanisms.
            Proposed Connectionist Model
                                                                     Figure 2: Module 2 computes the likelihoods based on the
We construct a modular neural network implementing
                                                                     observed data.
Bayesian learning and inference. The algorithm we use to
build our artificial neural modules is a variant of the cascade-        We denote the generative process (probability distribution)
correlation (CC) method called sibling-descendant cascade-           as a function of the observed data and hypothesis, f (d, h).
correlation (SDCC) which is a constructive method for learn-         For example, if the likelihood distribution for hypothesis h is
ing multi-layer artificial neural networks (Baluja & Fahlman,        a Gaussian with average h and standard deviation 1, then:
1994). CC offers two major advantages over standard back-
                                                                                                           1 (d−h)2
propagation (BP) methods. First, it constructs the network in                                  f (d, h) =    e 2 .                      (2)
an autonomous fashion (i.e., a user does not have to design the                                           2π
topology of the network). Second, its greedy learning mech-             Finally, module 3 (shown in Fig. 3) computes the hy-
anism can be orders of magnitude faster than the standard BP         potheses’ priors by learning their generative discrete distri-
algorithm. In addition to these, SDCC has another important          bution function. Note that the input to this module can be
benefit; due to its design, it can often reduce the depth of the     chosen from a finite number of possible hypotheses, H =
network drastically (Baluja & Fahlman, 1994).                        {h1 , . . . , hN }, and hence the generative distribution is discrete.
   We build our model in a modular fashion. Module 1                 It takes the hypotheses as input and gives their prior proba-
(shown in Fig. 1) implements Bayes’ rule. For this module,           bility as output. We represent the generative mechanism of
we assume that there are two hypotheses to compare; extend-          priors as a probability mass function denoted by g(h). For
ing it for any finite number of hypotheses is straightforward.       instance, this function can be of the following form:
There are three inputs (the prior and the two likelihoods) and                                                h2
one output (the posterior), and the module learns and imple-                                      g(h) = α · e 2 ,                      (3)
ment (1). We run this module once for each hypothesis.
                                                                     where α is chosen so that the sum of priors equals 1.
   When data are observed in consecutive rounds, posteriors
at one round are taken as priors for the next round; this is how
the beliefs are updated in light of new observed data (rational
inductive inference). Therefore, by connecting the output of
module 1, P(h1 |d), to the input corresponding to the prior,
P(h1 ), we can model this aspect of human cognition.                      Figure 3: Module 3 computes the prior probabilities.
                                                                 2687

   If we have N hypotheses, to run a complete Bayesian learn-
                                                                                               1
ing and inference, we learn modules 1 and 3 one time and
use them N times (for respectively computing the posterior
                                                                                              0.8
and prior of each hypothesis). However, since the likelihood
distributions might be different for different hypotheses, we
                                                                                True values
                                                                                              0.6
learn N different units of module 2 and use each of them once.
                                                                                                                                             y = 0.998x + 0.00358
                   Simulation Results                                                         0.4                                            R2 = 0.9896
Setup
                                                                                              0.2
We test each module individually. Using SDCC, we first
train each module by presenting input(s)-output pairs as train-                                0
                                                                                                0          0.2        0.4            0.6             0.8              1
ing patterns. Then, we test the generalization ability of the                                                       Outputs of the network
learned module by utilizing a set of input(s)-output testing
patterns. The accuracy of the module’s outputs is examined                          Figure 4: Outputs of module 1 plotted against true values.
by comparing them with the correct outputs presented in the
testing set. In all our modules, the hidden units have sigmoid
                                                                           We start with the Gaussian distribution. In this case, the
activation functions and the output units have linear activation
                                                                        likelihood is given as in equation (2). The observed data, d,
functions.
                                                                        is the input and its likelihood is the output. Given the hy-
Module 1                                                                pothesis h, we have a Gaussian likelihood with mean h, and
                                                                        standard deviation 1. For example, h could be 0 or 1 in the
Module 1 takes the two likelihoods, P(d|h1 ) and P(d|h2 ), and
                                                                        case where we have two hypotheses. Each of these distribu-
the prior, P(h1 ) as inputs and gives the posterior, P(h1 |d), as
                                                                        tions can be learned using SDCC. We define our training set
the output. The training set is all the triplets starting from
                                                                        to be the collection of equidistant points from −5 to 5 with
(0.1, 0.1, 0.1) and going up to (0.9, 0.9, 0.9) with steps of size
                                                                        steps of size 0.01 and the testing set to be equidistant points
0.1 paired with appropriate output derived from (1). The test-
                                                                        from −4.975 to 4.975, with the same step size, all paired with
ing set is all the triplets starting from (0.05, 0.05, 0.05) and
                                                                        appropriate outputs derived from (2). Across 50 learned net-
going up to (0.95, 0.95, 0.95) with steps of size 0.1 paired
                                                                        works, on average, module 2 had 1.8 hidden layers and 8.2
with correct outputs. Thus, we have 1000 training and 1000
                                                                        hidden units.
testing points and these two sets have no overlap.
                                                                           To check the accuracy of the module’s outputs, we plot
   Because of the random nature of SDCC, we get a differ-               them against the actual Gaussian values in a scatter plot
ent network with different structure every time we run the              shown in Fig. 5. The high correlation and the equation of
algorithm on the training set. Across 50 learned networks for           the fitted line show that this module succeeds in learning the
module 1, on average, each network had 13.2 hidden units                Gaussian distribution. To further assess the performance of
and 3.4 hidden layers, and the training took 970 epochs.                module 2, we plot the probability distribution function gener-
   We compare the outputs of module 1 with the actual re-               ated by it alongside the actual Gaussian distribution in Fig. 6a.
sults of Bayes’ rule by plotting them against each other in a           We observe that the two curves are very close.
scatter plot. In order to check the generalization accuracy of
the built network we use the testing set data (which are not
used in training the network) in our analysis. The results in                       0.45
Fig. 4 show that there is a high correlation between the out-                            0.4
puts of the network and the true values. Also the slope and                         0.35
y-intercept of the fitted line are respectively near 1 and 0. In                         0.3
                                                                      True values
sum, the learned module 1 produces highly precise outputs
                                                                                    0.25
and we can conclude that it implements the Bayes’ rule suc-
                                                                                         0.2                                                 y = 0.975x + 0.0013
cessfully.                                                                                                                                   R2 = 0.9889
                                                                                    0.15
Module 2                                                                                 0.1
Module 2 computes the likelihood given the observed data.                           0.05
This module learns the distribution generating data solely                                     0
                                                                                                0   0.05   0.1   0.15    0.2     0.25    0.3      0.35      0.4     0.45
based on the training set presented to it during the training                                                       Outputs of the network
phase. It has no prior information about the form or charac-
teristics of this distribution. Different hypotheses can have           Figure 5: Outputs of module 2 plotted against true values of
different likelihood distributions, hence we run one module 2           a Normal distribution. For the Normal, there are two x values
for each hypothesis. Through several experiments, we show               for every y value; hence, there are two lines of dots.
that module 2 can learn a variety of likelihood distributions.
                                                                   2688

                                         Normal                                                     Beta(5,1)                                                 Gamma(2,2)
              0.4                                                          5                                                                     0.2
                     Network’s outputs                                          Network’s outputs                                                                            Network’s outputs
                     Actual values                                              Actual values                                                                                Actual values
              0.3                                                          4
                                                                                                                                                0.15
                                                                           3
 Likelihood                                                  Likelihood                                                           Likelihood
              0.2                                                                                                                                0.1
                                                                           2
              0.1                                                                                                                               0.05
                                                                           1
                0                                                                                                                                  0
                                                                           0
              −0.1                                                        −1                                                                   −0.05
                −5                         0           5                    0        0.2       0.4         0.6    0.8     1                         0   2      4         6         8         10
                                  data (observation)                                         data (observation)                                             data (observation)
 (a) Normal distribution; Gaussian distribu-                 (b) Beta distribution with parameters                                (c) Gamma distribution with parameters
 tion with mean 0 and standard deviation 1.                  α = 5 and β = 1.                                                     k = 2 and θ = 2
                                 Figure 6: Outputs of module 2 compared with the actual values for three sample distributions.
   The replication of the original likelihood distribution by                                           of this work, we show that this neglect can be modelled as
our model is of special importance, because it is done without                                          weight decay in our proposed neural network. This expla-
any explicit or implicit information about the actual distribu-                                         nation is particularly of interest because it is neurologically
tion. The only information available to the learner is the prob-                                        plausible and in accordance with theories explaining memory
ability values of the points in the training set. Based on that                                         loss and decline in some other cognitive functions as a result
and by generalization, module 2 learns the actual distribution                                          of synaptic decay over time (Hardt, Nader, & Nadel, in press).
which generates the data. This capacity is not limited only                                                Base-rate neglect is an error in computing the posterior
to the Gaussian distribution. In our simulations, we observe                                            probability of a hypothesis without taking full account of the
that a wide range of distribution functions can be learned by                                           priors. In the Bayesian framework, in the extreme case of
this module. In Fig. 6, we show that for a couple of sample                                             entirely ignoring the priors, Bayes’ rule in (1) becomes:
probability distributions module 2 produces results very close                                                                           P(d|hi )
                                                                                                                            P(hi |d) = N             .               (4)
to the actual distribution functions.                                                                                                  ∑i=1 P(d|hi )
                                                                                                        Looking at this equation from a different perspective, we can
Module 3
                                                                                                        assume that in the original Bayes’ rule, all the hypotheses had
Module 3 computes hypotheses’ priors. Although their inputs                                             equal priors and these priors were cancelled out to give equa-
and outputs are of different nature, modules 2 and 3 are func-                                          tion (4). Therefore, in the Bayesian framework, base-rate ne-
tionally the same. They both compute the output based on a                                              glect is translated into assuming equal priors (i.e., equiproba-
probability distribution over the input. Therefore, like module                                         ble hypotheses). This means that the more the original priors
2, module 3 is capable of learning the underlying structure of                                          (base rates) are averaged out and approach the uniform dis-
its inputs — the possible hypotheses. The only difference be-                                           tribution, the more they are neglected in Bayesian inference.
tween modules 2 and 3 is that the likelihood distributions (in                                          We can explain this more abstractly by using the notion of
module 2) are continuous while prior distributions (in mod-                                             entropy defined in information theory as a measure of uncer-
ule 3) are discrete. We analyse module 3 in more detail while                                           tainty. Given a discrete random variable X = {h1 , . . . , hN }
discussing base-rate neglect in the next section.                                                       with probability mass function P(·), its entropy is defined as:
                                                                                                                                                  N
                     Base-rate Neglect as Weight Decay                                                                   H(X) = − ∑ P(hi ) log2 P(hi ).                                  (5)
In contemporary cognitive science, rationality in learning and                                                                                   i=1
inference is frequently defined and measured in terms of con-                                              Entropy quantifies the expected value of information con-
formity to Bayes’ rule. However, this appears to conflict                                               tained in a distribution. It is easy to show that a uniform dis-
with the Nobel-prize-winning work showing that people are                                               tribution has the maximum entropy (equal to log2 N) among
somewhat poor Bayesians due to biases such as base-rate                                                 all discrete distributions over the set {h1 , . . . , hN }. In sum, we
neglect, representativeness heuristic, and confusing the di-                                            can conclude that in the Bayesian framework, base-rate ne-
rection of conditional probabilities (Kahneman & Tversky,                                               glect is equivalent to ignoring the priors in the form of aver-
1996). Even experienced medical professionals deviate from                                              aging them out to get a uniform distribution, or equivalently,
optimal Bayesian inference and make major errors in their                                               increasing their entropy.
probabilistic reasoning (Eddy, 1982). More recently, Prime                                                 We show that weight decay in our proposed neural network
and Shultz showed that base rates (i.e., priors) are not entirely                                       produces the same results as just described, namely approach-
ignored but just de-emphasized (Prime & Shultz, 2011).                                                  ing a uniform distribution and increasing entropy. We then
   We first show how base-rate neglect can be interpreted in                                            conclude that we can model base-rate neglect in the Bayesian
the Bayesian framework. Then, as an important contribution                                              framework by a weight decay mechanism in our brain-like
                                                                                               2689

                    0.45                                                                        10
                     0.4                                                                         5
                    0.35                                                                         0
                     0.3
Prior Probability
                                                                                                −5
                    0.25                                       Time Step=0                                                                 decay rate = 0.2
                                                                                     Entropy
                                                               Time Step=2                     −10                                         decay rate = 0.3
                     0.2                                       Time Step=6                                                                 decay rate = 0.4
                                                                                               −15
                                                               Time Step=15                                                                decay rate = 0.5
                    0.15
                                                                                               −20
                     0.1
                                                                                               −25
                    0.05
                                                                                               −30
                      0
                                                                                               −35
                      −5                    0                                 5                   0      5             10             15                  20
                                   Input (hypothesis)                                                               Time Step
(a) The priors’ initially Gaussian distribution approaches uniform as                (b) The entropy of prior distributions increases as time passes and
time passes and weights decay more (decay rate = 0.2).                               weights decay more.
                                                        Figure 7: Effects of connection weight decay.
        network implementing the posterior inference. We take priors                    There is a literature on using weight decay to improve
        as the states of a learning and inference system. As weights                 the ability of neural networks to generalize (Krogh & Hertz,
        decay, the system moves towards more stable states and thus                  1992). Krogh and Hertz showed that a weight decay can
        the entropy increases. In special cases, some priors are up-                 improve generalization by suppressing any irrelevant compo-
        dated so often that the effects of decay are overcome and                    nents of the weight vector. This effect is evident in Fig. 7a, as
        these priors stay strong. On the other hand, likelihoods are                 the bumps (overfitting) get smoothed out as time passes.
        new evidence and thus, they are not subject to much decay.                      In Fig. 7b, entropies are plotted as a function of time for
           In module 3, an SDCC network learns the hypotheses’ pri-                  four values of decay rate. In all four cases, entropy increases
        ors. Assume that we have N hypotheses, {h1 , . . . , hN }, with              with time until it converges to its maximum which corre-
        probability mass function given by equation (3). We present                  sponds to uniform prior distribution. In our case, the max-
        the results for this specific mass function, but the results are             imum entropy is log2 N = log2 399 = 8.64.
        similar for other discrete distributions. Our training set is                   In conclusion, we show that the proposed neural network
        the collection of 401 equidistant points from −10 to 10 (with                model contributes to the resolution of the discrepancy be-
        steps of size 0.05), and our testing set is the collection of 399            tween demonstrated Bayesian successes and failures by mod-
        equidistant points from −9.975 to 9.975, all paired with the                 elling base-rate neglect as weight decay in a connectionist
        correct outputs derived from (3). We choose these sets such                  network implementing Bayesian inference. This is done by
        that there is no overlap between the testing and training sets               showing that as weights decay, the priors’ probability mass
        in order to measure the generalization abilities of the module.              function approaches a uniform distribution and its entropy in-
        Note that we do not specify the form of the probability mass                 creases. Consequently, the prior terms eventually cancel out
        function in any way and the network learns it by generaliza-                 from Bayes’ rule, resulting in the neglect of base rates.
        tion from the training input. Also note that in the case we
        consider here, N = 399 as we define our hypotheses to be the                                           Discussion
        collection of points in the testing set.                                     We propose a modular neural-network structure to implement
           After learning, the network’s weights decay exponentially                 Bayesian learning and inference. Through simulations, we
        over time steps as follows:                                                  show that the proposed three modules, responsible for com-
                 Wi (t + 1) = (1 − r) ·Wi (t) = (1 − r)t ·Wi (1),      (6)           puting Bayes’ rule, likelihoods, and priors, succeed in learn-
        where Wi (t) is the weight of connection i at time t and r is the            ing their assigned task. Employing a weight-decay mecha-
        decay rate. Clearly, as the connection weights of the learned                nism, we provide a novel explanation of base-rate neglect,
        network decay, the output will change. Fig. 7 demonstrates                   the most well-documented deviation from Bayes’ rule. We
        the effects of weight decay on the output of module 3. In                    show that weight decay increases the entropy of priors in a
        Fig. 7a, for r = 0.2, we observe that as time passes, and hence              Bayesian system. In nature, this is very similar to the second
        as the weights continue to decay, the distribution of the hy-                law of thermodynamics which states that the entropy of iso-
        potheses approaches a uniform distribution. We consider a                    lated systems never decreases and that they evolve towards
        discrete case where we have a finite number of hypotheses,                   equilibrium — the state of maximum entropy. Our model of
        and therefore Fig. 7 represents the probability mass function                base-rate neglect predicts that older, less continuously sup-
        where the sum of all probabilities must be equal to 1. Note                  ported (i.e., more isolated) priors would be subject to more
        that in the 15th time step, the distribution is almost uniform;              decay, and consequently more neglect.
        thus, the value of the probability is 1/N = 1/399 (this small                   Our model is a first step towards implementing the
        value should not be mistaken with zero in Fig. 7(a)).                        Bayesian framework with neural networks. As such, it still
                                                                                  2690

has several limitations. For instance, our current model             Baluja, S., & Fahlman, S. E. (1994). Reducing net-
is only capable of handling a finite number of hypotheses.             work depth in the cascade-correlation learning architec-
When the number of hypotheses gets uncountably infinite,               ture (Tech. Rep.). Carnegie Mellon University, School of
the current structure will be infeasible due to large numbers          Computer Science.
of outputs that must be remembered.                                  Chater, N., & Manning, C. D. (2006). Probabilistic models of
   Our networks learn to approximate Bayesian functions                language processing and acquisition. Trends in Cognitive
with the output being a function value. However, this as-              Sciences, 10(7), 335 - 344.
sumption might be unrealistic and does not explain how the           Doya, K., Ishii, S., Pouget, A., & Rao, R. (2007). Bayesian
brain innately represents probabilities. It is more realistic to       brain : probabilistic approaches to neural coding. Cam-
approximate such functions from input instances occurring at           bridge, MA : MIT Press.
various frequencies. Although we do not address this here,           Eddy, D. M. (1982). Probabilistic reasoning in clinical
our further experiments show that SDCC networks can do                 medicine: problems and opportunities. In D. Kahneman,
this, which is effectively probability matching. With that, our        P. Slovic, & A. Tversky (Eds.), Judgment under uncer-
modular neural network, similar to human brain, makes sense            tainty: Heuristics and biases. Cambridge Univ. Press.
of data by representing probability distributions and applying       Griffiths, T. L., Austerweil, J. L., & Berthiaume, V. G.
Bayes’ rule to find the best explanation for any given data.           (2012). Comparing the inductive biases of simple neural
   In this paper, we apply our model to base-rate neglect.             networks and bayesian models. In Proc. the 34th Annual
However, there are many other subtle and potentially difficult         Conf. of the Cog. Sci. Society.
Bayesian phenomena, such as hierarchical Bayesian struc-             Hardt, O., Nader, K., & Nadel, L. (in press). Decay happens:
tures and causal networks, to consider. Also, our work does            the role of active forgetting in memory. Trends in Cog. Sci..
not address the origin of the Bayesian competencies; all we          Hinton, G. E., & Osindero, S. (2006). A fast learning algo-
show is that neural networks can implement Bayesian infer-             rithm for deep belief nets. Neural Computation, 18, 2006.
ence and learning. The origin of theses capacities could be in       Kahneman, D., & Tversky, A. (1996). On the reality of cog-
evolution, learning, development or some combination. Fi-              nitive illusions. Psychological Review, 103, 582 - 591.
nally, we only use SDCC as the learning method and do not            Konrad, K., & Wolpert, D. (2004). Bayesian integration in
try other neural-network approaches.                                   sensorimotor learning. Nature, 427, 244 - 247.
   There is plenty of scope for future research to address the       Krogh, A., & Hertz, J. A. (1992). A simple weight decay can
issues just discussed. For instance, we can illuminate the is-         improve generalization. Advances in neural information
sue regarding the origin of the Bayesian competencies of our           processing systems, 4, 950-957.
model by agent-based simulations of evolution of Bayesian            McClelland, J. L. (1998). Connectionist models and Bayesian
inference and learning. Preliminary results in the context of          inference. In M. Oaksford & N. Chater (Eds.), Rational
social learning strategies have shown that evolution favours           models of cognition. Oxford, UK: Oxford Univ. Press.
Bayesian learning, based on passing posteriors, over imita-          Montrey, M., & Shultz, R. (2010). Evolution of social learn-
tion and environment sampling (Montrey & Shultz, 2010).                ing strategies. , 95-100.
   With no doubt, Bayesian models provide powerful analyti-          Pouget, A., Dayan, P., & Zemel, R. (2003). Inference and
cal tools to rigorously study deep questions of human cogni-           computation with population codes. Annual Review in Neu-
tion that have not been previously subject to formal analysis.         roscience, 26, 381-410.
These Bayesian ideas, providing computation-level models,            Prime, H., & Shultz, T. R. (2011). Explicit Bayesian reason-
are becoming prominent across a wide range of problems in              ing with frequencies, probabilities, and surprisals. Proc. of
cognitive science. On the other hand, connectionist models             33rd Annual Conf. Cog. Sci. Society.
offer an implementation-level framework for modelling men-           Shi, L., & Griffiths, T. (2009). Neural implementation of hi-
tal phenomena in a more biologically plausible fashion. We             erarchical Bayesian inference by importance sampling. In
present this work in the spirit of theoretical unification and         Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams,
enhancement of these two approaches. We are not advocat-               & A. Culotta (Eds.), Advances in neural information pro-
ing replacement of one approach in favour of the other.                cessing systems 22 (pp. 1669–1677).
                                                                     Shultz, T. R., & Fahlman, S. E. (2010). Cascade correla-
                   Acknowledgements                                    tion. In C. Sammut & G. I. Webb (Eds.), Encyclopedia of
This research was supported in part by McGill Engineer-                machine learning (p. 139 - 147). Heidelberg, Germany:
ing Doctoral Award to MK and a Discovery grant to TRS                  Springer-Verlag.
from the Natural Sciences and Engineering Research Coun-             Tenenbaum, J. B., Kemp, C., & Shafto, P. (2006). Theory-
cil of Canada. Mark Coates and Deniz Üstebay contributed              based Bayesian models of inductive learning and reason-
thoughtful comments on an earlier draft.                               ing. Trends in Cognitive Sciences, 10(7), 309 - 318.
                                                                     Yuille, A., & Kersten, D. (2006). Vision as Bayesian infer-
                         References                                    ence: analysis by synthesis? Trends in Cog. Sci., 10(7),
Ackley, H., Hinton, E., & Sejnowski, J. (1985). A learning             301 - 308.
   algorithm for Boltzmann machines. Cog. Sci., 147–169.
                                                                 2691

