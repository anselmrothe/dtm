UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Assumption of Class-conditional Independence in Category Learning
Permalink
https://escholarship.org/uc/item/7tr842nb
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Jarecki, Jana
Meder, Bjorn
Nelson, Jonathan D.
Publication Date
2013-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

         The Assumption of Class-Conditional Independence in Category Learning
                                              Jana Jarecki (jarecki@mpib-berlin.mpg.de)∗
                                              Björn Meder (meder@mpib-berlin.mpg.de)∗
                                         Jonathan D. Nelson (nelson@mpib-berlin.mpg.de)∗
                                             ∗ Center for Adaptive Cognition and Behavior (ABC),
                                        Max Planck Institute for Human Development, Lentzeallee 94
                                                               14195 Berlin, Germany
                                  Abstract                                   in Reichenbach’s (1956) common-cause principle in the phi-
    This paper investigates the role of the assumption of class-             losophy of science and in causal modeling (Spirtes, Glymour,
    conditional independence of object features in human classi-             & Scheines, 1993; Pearl, 2000).
    fication learning. This assumption holds that object feature
    values are statistically independent of each other, given knowl-            Both the philosophical and psychological literature make
    edge of the object’s true category. Treating features as class-          claims about the normative bases of the assumption of class-
    conditionally independent can in many situations substantially           conditional-independence of features. Our focus here is not
    facilitate learning and categorization even if the assumption is
    not perfectly true. Using optimal experimental design princi-            on the general normativity or nonnormativity of that assump-
    ples, we designed a task to test whether people have this de-            tion, but on whether the assumption of class-conditional inde-
    fault assumption when learning to categorize. Results provide            pendence may (perhaps tacitly) underlie people’s inferences
    some supporting evidence, although the data are mixed. What
    is clear is that classification behavior adapts to the structure of      in learning and multiple-cue categorization tasks. We think
    the environment: a category structure that is unlearnable under          of this assumption as one of many possible default (heuris-
    the assumption of class-conditional independence is learned by           tic or meta-heuristic) assumptions that, if close enough to an
    all participants.
                                                                             environment’s actual structure, may facilitate learning and in-
    Keywords: Multiple-cue classification learning; class-
    conditional independence; naı̈ve Bayes; causal Markov con-               ferences.
    dition
                                                                               The Psychology of Conditional Independence
                              Introduction
                                                                             Some psychological models of categorization incorporate as-
Categorization is fundamental for cognition. Grouping to-                    sumptions of class-conditional independence, such as the cat-
gether objects or events helps us to efficiently encode envi-                egory density model (Fried & Holyoak, 1984) or Anderson’s
ronmental patterns, make inferences about unobserved prop-                   (1991) rational model of categorization. Both models treat
erties of novel instances, and make decisions. Without cate-                 features of instances as class-conditionally independent to
gorization we could not see the woods for the trees.                         make inferences about category membership or unobserved
    Despite the ease with which we form categories and use                   item properties.
them to make inferences or judgments, from a computational                      Other research has focused more directly on the role of
perspective categorization is a challenging problem. For in-                 conditional independence assumptions in human reasoning.
stance, different diseases can cause similar symptoms, en-                   For instance, a key assumption in many formal causal mod-
tailing that diagnostic inferences are often only probabilistic.             eling approaches (e.g., Pearl, 2000; Spirtes et al., 1993) is
Patients may have new symptom combinations and still re-                     the so-called causal Markov condition, which assumes that a
quire a diagnosis. Depending on the specific assumptions the                 variable in a causal network is independent of all other vari-
physician makes about the relationship between the diseases                  ables (except for its causal descendants), conditional on its di-
and symptoms, a physician could justifiably make very dif-                   rect causes. As this assumption facilitates probabilistic infer-
ferent inferences about the diseases.                                        ences across complex causal networks it was suggested that
    In the present paper, we investigate the role of the possi-              people’s causal inferences could also comply with this condi-
ble assumption of class-conditional independence of features                 tional independence assumption.
in category learning. Class-conditional independence holds if
                                                                                Von Sydow, Meder, and Hagmayer (2009) investigated
the features of the category members are statistically indepen-
                                                                             reasoning about causal chains and found that subjects’ infer-
dent given the true class. This assumption can facilitate clas-
                                                                             ences indicated a use of conditional independence assump-
sification and learning of category structures. The concept
                                                                             tions, even if the learning data suggested otherwise.1 Other
of class-conditional independence underlies the naı̈ve Bayes
                                                                             research, however, found violations of the causal Markov
classifier in machine learning (Domingos & Pazzani, 1997),
                                                                             condition (Rehder & Burnett, 2005). Asked to infer the prob-
and is also a key assumption in some psychological classifica-
tion models (e.g., Fried & Holyoak, 1984; Anderson, 1991).                       1 For instance, applying the causal Markov condition to a causal
It is related to ideas of channel separability in sensory percep-            chain X → Y → Z entails that Z is independent of X given Y (e.g.,
tion (Movellan & McClelland, 2001). Similar ideas are found                  P(z|y, x) = P(z|y, ¬x).
                                                                        2650

ability for one effect when knowing the common cause of                   mensionality). One way to sidestep the problem is to assume
several effects, people’s judgments were influenced by the                that features are class-conditionally independent.
status of the other effects rather than treating all effects as
independent of each other given the cause. One explanation                Class-Conditional Independence
for this “nonindependence effect” (Rehder & Burnett, 2005)                If class-conditional independence holds the individual fea-
is that it might be due to subjective explanations that disable           tures within a class are statistically independent (e.g., Domin-
all causal links between the cause and effects at once (Walsh             gos & Pazzani, 1997). This means that the probability of
& Sloman, 2007). Other researchers have argued that these                 a feature configuration given a class can be factorized such
Markov violations do not indicate flawed human reasoning,                 that:
but reflect the use of abstract causal knowledge that is sen-                                                 J
sitive to contextual information (Mayrhofer, Hagmayer, &                            P(F = f | C = c) = ∏ P(Fj = f j | C = c)             (2)
Waldmann, 2010).                                                                                            j=1
                                                                          where P(F = f | C = c) denotes the likelihood of the fea-
                      Research Questions                                  ture configuration given the class, P(Fj = f j | C = c) is the
Should the assumption of class-conditional feature indepen-               marginal likelihood of the jth feature value given the class,
dence be used in classification learning? Do people use that              and j = 1, . . . , J indexes the different features. Thus, accord-
assumption to guide learning about the structure of a novel               ing to the assumption of class-conditional independence, the
environment? We extend previous research fourfold: (1) We                 likelihood of each feature value combination can be estimated
use optimal experimental design principles (Myung & Pitt,                 from the likelihoods of the individual feature values.
2009; Nelson, 2005) to explicitly address the assumption in
classification, (2) we are interested in categorization learn-            Advantages The key advantage of assuming that features
ing as opposed to causal reasoning, (3) we investigate how                are class-conditionally independent is that it reduces the curse
people’s experience with a new environment shapes their                   of dimensionality. For example, for 10 binary features there
classification behavior, whereas many previous studies have               are 210 possible feature configurations. That means, we
measured explicit numerical probability judgments. (4) We                 have to estimate 1024 likelihoods of feature configurations
use an experience-based research paradigm, whereas previ-                 for each class. Assuming class-conditional independence re-
ous studies used numerical (Rehder & Burnett, 2005) or ver-               duces the number of required likelihoods from 1024 to 8.
bal (Mayrhofer et al., 2010) formats. Personal experience                    Another benefit is that class-conditional independence al-
of events has been shown to result in different behavior and              lows inferences about new feature configurations. Even if
learning than word- or number-based presentation of prob-                 a particular combination of feature values has not been ob-
abilities (Hertwig, Barron, Weber, & Erev, 2004; Nelson,                  served yet, assuming class-conditional independence allows
McKenzie, Cottrell, & Sejnowski, 2010). Before describing                 inference of the likelihood of the feature configuration from
the task we designed, let us turn to the normative question of            the marginal likelihoods of the individual feature values,
class-conditional independence in classification.                         thereby enabling computing the posterior class probabilities.
                                                                          Robustness While class-conditional independence may
Class-Conditional Independence in Classification
                                                                          rarely exactly hold in real-world environments, violations
Categorization entails assigning an object to a class. Let F              of this assumption do not necessarily impair performance.
denote an object consisting of a vector of feature values f ,             For instance, a widely used classifier in machine learning
and let C denote a random variable whose values are the pos-              is the naı̈ve Bayes model, which treats features as class-
sible classes c1 , . . . , cn . The posterior probability of the class    conditionally independent and computes the posterior class
given the observed feature values, P(class | features), can be            probabilities accordingly. Both simulation studies and ana-
inferred using Bayes’ rule:                                               lytic results demonstrate the robustness of this model under
                                                                          a variety of conditions (Domingos & Pazzani, 1997). For
                                   P(F = f | C = c)P(C = c)               instance, if the optimality criterion is classification accuracy
        P(C = c | F = f ) =                                        (1)
                                           P(F = f )                      (error minimization, i.e., a zero-one loss function), then even
                                                                          if the derived posterior probabilities do not exactly corre-
where P(F = f | C = c) denotes the likelihood of feature
                                                                          spond to the true posterior, as long as the correct category
value vector f given class c, P(C = c) is the prior probability
                                                                          receives the highest posterior probability, classification error
of the class, and P(F = f ) is the occurrence probability of the
                                                                          will be minimized.
feature configuration. An important question is how we esti-
mate the relevant probabilities to infer the posterior probabil-          Summary Treating features as class-conditionally indepen-
ity. Estimating the classes’ prior probabilities, P(C = c), from          dent in a classification task can be helpful, as it simplifies
the data is relatively straightforward. However, estimating the           the problem of parameter estimation and violations of class-
likelihood of the features given the class, P(F = f | C = c),             conditional independence do not necessarily entail a loss in
is more complicated, as the number of probabilities grows                 classification accuracy. On the other hand, assuming class-
exponentially with the number of features (the curse of di-               conditional independence also puts constraints on the types
                                                                      2651

of classification problems that can be solved. For instance,           Environment Using optimal experimental design (OED)
treating features as class-conditionally independent can make          principles (Myung & Pitt, 2009; Nelson, 2005) we conducted
it impossible to solve certain classification problems, such as        simulations to find environmental probabilities that best dif-
nonlinearly-separable category structures (Domingos & Paz-             ferentiate between a learner that assumes class-conditional in-
zani, 1997).                                                           dependence and a learner that makes predictions based only
    From a psychological perspective, however, presuming               on previous instances of the same feature configuration. The
class-conditional independence might be a plausible default            possible environmental probabilities for our task consisted
assumption in category learning. If features are (approxi-             of the following parameters: (i) the base rate of Species A
mately) class-conditionally independent, this facilitates learn-       (determining the Species B base rate), (ii) the likelihoods of
ing and inference substantially. We designed an experiment to          each of the eight possible feature value combinations given
investigate whether people initially presume class-conditional         Species A and (iii) the corresponding values for Species B.
independence, and if people change their beliefs and classi-           The parameter values were obtained via optimization, us-
fication behavior when class-conditional independence does             ing genetic algorithms to search for desirable environments
not hold in the environment.                                           which had frequent configurations of features with large ab-
                                                                       solute discrepancies between the actual posterior probability
                         Experiment                                    of Species A, and the posterior probability presumed based
                                                                       on the class-conditional independence assumption. Formally,
Our goal was to examine whether people use class-                      the genetic algorithm optimized the following fitness func-
conditional independence as a default assumption in cate-              tion:
gory learning when the true environmental probabilities are
not known yet, that is, early in learning. In order to test this         I
question, we designed a learning environment in which clas-             ∑ [Ptrue (C = c | F = fi ) − Pcci (C = c | F = fi )]2 ×P(F = fi )2
                                                                       i=1
sification decisions would be strongly different if the learner                                                                     (3)
presumes class-conditional feature independence, rather than           where i indexes all possible feature value combinations and
basing classification decisions solely on the previous in-             the subscripts true vs. cci indicate the posteriors calculated
stances with the exact same configuration of feature values.           according to the true vs. class-conditionally independent pa-
                                                                       rameters.
Method                                                                     The obtained environment is summarized in Figure 1. The
Participants Thirty subjects (Mage = 23, SD = 3.3 years,               environment contains five out of eight possible feature com-
70 % females) participated in a computer-based experiment              binations (henceforth denoted as 111, 000, 100, 010, 001);
in exchange for 12 Euro.                                               the remaining three combinations (011, 101, 110) do not oc-
                                                                       cur. The figure illustrates the category base rates, the likeli-
Task Participants’ task was to learn classify objects with             hoods of the feature configurations given the two classes, as
three binary features into one out of two categories. As stim-         well as the marginal likelihoods of the features, which pro-
uli we used simulated biological “plankton” specimens differ-          vide the basis for inferring posterior probabilities according
ing in three binary features (“eye”, “tail”, and “claw”, shown         to the class-conditional independence assumption. Note that
in the left image in Figure 1). The classes were labelled as           although nothing in the optimization prescribed finding a de-
“Species A” vs. “Species B”. The assignment of the actual              terministic environment, in fact the posterior probabilities of
physical features and their values to the underlying proba-            Category A are one or zero, for each of the feature configura-
bilities, as well as the class labels, were randomized across          tions that occurs.
participants.                                                              In this environment, assuming class-conditional indepen-
Procedure We used a trial-by-trial supervised multiple-                dence leads to classification decisions that systematically de-
cue probabilistic category learning paradigm (e.g., Knowlton,          viate from decisions based on the true environmental proba-
Squire, & Gluck, 1994; Meder & Nelson, 2012; Nelson et al.,            bilities. Table 1 summarizes the feature configurations, their
2010; Rehder & Hoffman, 2005). After introducing the task              probability of occurrence, the posterior probabilities accord-
and familiarizing subjects with the three features, on each            ing to the true environmental probabilities, and the poste-
trial a plankton exemplar with a specific feature value com-           rior probabilities derived assuming class-conditional inde-
bination was randomly drawn according to the true environ-             pendence. For four out of the five feature configurations, the
mental probabilities (see below) and displayed on the screen.          classification decision derived assuming class-conditional in-
After participants made a classification decision, feedback on         dependence conflicts with the actual class membership (indi-
the true class was given and the next trial started. Learning          cated by = in Table 1).
continued until criterion performance was achieved. Crite-                 Consider feature configuration 111. This item always be-
rion performance was defined as both (1) an overall classifi-          longs to Species A in the true environment. If features
cation accuracy of 98 % over the last 100 trials, and (2) accu-        are treated as class-conditionally independent, it belongs to
rate classification of the last five instances of every individual     Species A with probability 0.91. The small difference be-
configuration of features.                                             tween the actual probability of 1.00 and 0.91 should not
                                                                   2652

                                                                        with probability 0.78. This holds for attending solely to any
                                                                        of the three features.
                                                                        Hypotheses
                                                                        If participants make no (not even tacit) assumptions of class-
                                                                        conditional feature independence, and learn each item sep-
                                                                        arately, then items could be learned in order of their fre-
                                                                        quency of occurrence (a frequency-of-configuration hypoth-
                                                                        esis). If participants approach the task by assuming features
                                                                        to be class-conditionally independent, classification decisions
                                                                        should systematically deviate from ones derived from the true
                                                                        environmental probabilities, especially early in learning (a
                                                                        posterior-discrepancy hypothesis).
                                                                           Both hypotheses predict the fewest errors for item 111,
                                                                        the most frequent feature configuration and the one for
Figure 1: Task environment. a) Stimuli and base rates of                which the class-conditional independence posterior is clos-
classes. b) Joint likelihoods of true environment. c) Marginal          est to accurate. For the four critical items, the differ-
  likelihoods used assuming class-conditional independence              ence in posterior probability is the largest for item 000.
                                                                        The posterior-discrepancy hypothesis predicts the most er-
                                                                        rors for item 000, and thus that the ordering of errors should
change the learner’s classification decision for this stimulus.         be 111<100≈010≈001<000. However, the frequency-of-
This, however, is not true for the other items. For instance, ac-       configuration hypothesis predicts that the ordering of classi-
cording to the true environment, item 000 belongs to Species            fication errors should be 111<000<100≈010≈001.
A with probability 1, but assuming class-conditional indepen-              Key empirical questions are therefore whether there are
dence entails that it belongs to Species B with probability             any systematic differences in learning rate for the individual
0.67. Thus, a learner assuming class-conditional indepen-               items, whether the early learning data suggest a presumption
dence would believe that on average about 67 % of the 000               of class-conditional independence, and if so, whether the oc-
items belong to Species B, despite experiencing that it al-             currence frequency of an item or the degree to which class-
ways belongs to Species A. The same divergence holds for                conditional independence fails on it determine learning.
the other three configurations (100, 010, 001): whereas all of
those items actually belong to category B, treating features            Results and Discussion
as class-conditionally independent entails that the probability         All participants reached criterion performance, i.e. learned
for category A is higher (0.58).                                        the category structure (in a mean number of 391 trials,
                                                                        SD=155, Md=348, range 210 to 808 trials). To reach criterion
      Table 1: True environment vs. assuming class-                     performance, participants needed to classify each individual
                conditional independence (cci).                         feature configuration correctly five times in a row. To investi-
                                                                        gate whether there was a difference in learning speed for the
        Features    P(features)      P(class | features)                different feature configurations, we calculated the number of
                     true env     true env               with cci       times each item needed to be observed before reaching this
        1  1  1        0.39           A   1    =          A 0.91        criterion (Table 2). We will first consider learning time and
        1  0  0        0.11           B   1     =        A 0.58        then error rates.
        0  1  0        0.11           B   1       =      A 0.58
        0  0  1        0.11           B   1         =    A 0.58               Table 2: Number of trials an item needed to be
        0  0  0        0.28           A   1           =  B 0.67                seen to correctly classify it five times in a row.
    The strongest discrepancy is for the 000 configuration,                     Features                      Trials
which is the second-most-frequent configuration, occurring                                      mean           SD (SE) median
with probability .28. Note that a hypothetical learner (even                    1   1  1        10.4         10.7 (1.9)        7.0
with perfect memory) who assumes class-conditional inde-                        1   0  0        11.4           8.0 (2.1)       7.5
pendence of features, and is unable to give up this assump-                     0   1  0        11.5           7.7 (2.1)       9.0
tion, will never learn the true statistical structure of this envi-             0   0  1        11.5           7.0 (2.1)       9.0
ronment, even after completing a quadrillion learning trials.                   0   0  0        15.8         11.5 (2.9)       13.5
    Achieving criterion performance would also be impossible
if learners looked at one feature only (at 1xx, or x1x, or xx1             In our data most subjects learned item 111 before item
and ignoring the x). Considering single features, participants          000 (22 out of 30, binomial p < .02), which is con-
should think any feature configuration belongs to Species A             sistent with both hypotheses. Did learning time follow
                                                                    2653

the frequency-of-configuration hypothesis, or the posterior-                             General Discussion
discrepancy hypothesis? The posterior-discrepancy hy-
                                                                     The present paper examined the role of the assumption of
pothesis predicts an ordering of 111<100≈010≈001<000,
                                                                     class-conditional independence of features in category learn-
whereas the item-frequency hypothesis’s ordering prediction
                                                                     ing. While different types of conditional independence as-
is 111<000<100≈010≈001. The critical difference in pre-
                                                                     sumptions play an important role in various scientific debates
dictions is between the learning time for items 100, 010, and
                                                                     and computational models of cognition, little is known about
001 and item 000. The frequency hypothesis predicts that
                                                                     their descriptive validity in the context of classification learn-
item 000 will be learned faster, whereas the posterior discrep-
                                                                     ing with multiple cues. Our goal was to empirically investi-
ancy hypothesis predicts that items 100, 010, and 001 will be
                                                                     gate whether people initially (early in learning) treat features
learned first. Here, our results strongly support the posterior
                                                                     as class-conditionally independent. The present results par-
discrepancy hypothesis, and contradict the item frequency hy-
                                                                     tially support the idea that people initially treat features as
pothesis. Items 100, 010 and 001 were learned more quickly
                                                                     class-conditionally independent and make classification deci-
by more people than item 000, despite item 000’s greater fre-
                                                                     sions accordingly. We think of the results as tentative because
quency (item 001 faster: 21 out of 30, binomial p < .05; item
                                                                     some aspects of the data are not perfectly clear.
010 faster: 20 out of 30, binomial p < .1; item 100 faster:
21 out of 30, binomial p < .05). Moreover, there was a non-              Our focus in the present study was on participants’ be-
significant trend for items 100, 010, and 001 to take longer         havior early in learning, when evidence about the category
than item 111; consistent with the posterior discrepancy hy-         structure and environmental probabilities is limited. This ap-
pothesis but not the configuration frequency hypothesis.             proach is similar to the studies of Smith and Minda (1998),
                                                                     who investigated possible transitions in categorization strate-
                                                                     gies and stimulus encoding over the course of learning.Their
                                                                     finding was that late in learning exemplar models (e.g., Medin
                                                                     & Schaffer, 1978) accounted best for subjects’ behavior, but
                                                                     that this was not the case early in learning (in which a pro-
                                                                     totype model seemed to better account for human perfor-
                                                                     mance, see below). This is also a possible explanation for
                                                                     the finding that despite strongly violating class-conditional
                                                                     independence, the environment in our experiment was clearly
                                                                     learnable. Participants could have initially treated features as
                                                                     class-conditionally independent and computed posteriors ac-
                                                                     cordingly and later shifted to an exemplar-based strategy to
                                                                     minimize classification error.
                                                                         A key methodological aspect of our study was to use opti-
                                                                     mal experimental design principles to find environments that
                                                                     would allows us to directly test whether people use class-
                                                                     conditional independence as a default assumption in catego-
                                                                     rization. Interestingly, the optimizations told us that the best
                                                                     environment to differentiate between a learner that assumes
                                                                     class-conditional independence and a learner that makes pre-
                                                                     dictions based only on previous instances of the same feature
                                                                     configuration was deterministic. The crucial aspect of this
Figure 2: Percentage incorrect classifications for the first 50
                                                                     environment, however, is not that it is deterministic, but that
                trials each item was encountered.
                                                                     it entails a nonlinearly separable category structure. Since the
                                                                     class-conditional independence model induces a linear deci-
   The error rates throughout early learning are summarized          sion bound (Domingos & Pazzani, 1997), it could not achieve
in Figure 2. This figure corroborates the analysis of the num-       criterion performance in this particular task environment.
ber of learning trials required for each stimulus configuration:         This, in turn, relates our study to earlier research in psy-
item 000 was clearly the most difficult to learn. As this fea-       chology, which investigated whether linearly separable cat-
ture configuration is the one for which the difference in pos-       egories are easier to learn than nonlinearly separable ones
terior probability is largest when assuming class-conditional        (e.g., Medin & Schaffer, 1978; Medin & Schwanenflugel,
independence versus using the full true environmental prob-          1981). This research focused on two types of categorization
abilities, this finding is consistent with the idea that people      models, exemplar- and prototype-models, both of which as-
treat features as being class-conditionally independent early        sume that categorization decisions are derived from similar-
in learning. However, items 100, 010 and 001 were much               ity comparisons (either to specific exemplars stored in mem-
closer to (or even indistinguishable from) item 111, consis-         ory or to prototypes of categories). By contrast, we investi-
tent with the above analysis in Table 2.                             gated category learning and human subjects’ initial assump-
                                                                 2654

tions from the perspective of probabilistic inference (see also      Knowlton, B., Squire, L., & Gluck, M. (1994). Probabilistic
Anderson, 1991; Fried & Holyoak, 1984), a conceptually dif-            classification learning in amnesia. Learning & Memory, 1,
ferent view. Nevertheless, there are some interesting connec-          106–120.
tions between our work and these earlier (similarity-based)          Mayrhofer, R., Hagmayer, Y., & Waldmann, M. (2010).
models. For instance, assuming class-conditional indepen-              Agents and causes: A Bayesian error attribution model of
dence entails that not all information (about feature configu-         causal reasoning. In Proceedings of the 32nd Annual Con-
rations and corresponding class probability) is encoded dur-           ference of the Cognitive Science Society.
ing learning, but only marginalized conditional likelihoods          Meder, B. & Nelson, J. D. (2012). Information search with
and category base rates. In this respect the class-conditional         situation-specific reward functions. Judgment and Deci-
independence model is similar to prototype models, which               sion Making, 7, 119–148.
encode parametric information of central tendencies (e.g.,           Medin, D. L. & Schaffer, M. M. (1978). Context theory of
mean or mode of feature values) that form the prototype (e.g.,         classification learning. Psychological Review, 3, 207–238.
Smith & Minda, 1998).                                                Medin, D. L. & Schwanenflugel, P. (1981). Linear separabil-
   Importantly, these accounts assume that information is              ity in classification learning. Journal of Experimental Psy-
stored separately for each feature and the to-be-classified item       chology: Human Learning and Memory, 355–368.
is compared to the prototypes separately on each feature di-         Movellan, J. R. & McClelland, J. L. (2001). The Morton-
mension individually. Conversely, a learner who makes no as-           Massaro law of information integration: Implications for
sumptions about the structure of the relations between classes         models of perception. Psychological Review, 108, 113–
and features and directly tracks the true environmental prob-          148.
abilities is conceptually more similar to exemplar models of         Myung, J. & Pitt, M. (2009). Optimal experimental design
category learning. The difference is that prototype models,            for model discrimination. Psychological Review, 116, 832–
like our independence model, do not need to store each indi-           840.
vidual instance that is experienced.                                 Nelson, J. D. (2005). Finding useful questions: On Bayesian
   In sum, the current paper adds to the debate about the role         diagnosticity, probability, impact, and information gain.
of conditional independence assumptions for computational              Psychological Review, 112, 979–999.
models of cognition. The task environment identified based           Nelson, J. D., McKenzie, C. R. M., Cottrell, G. W., & Se-
on optimal experimental design principles allowed us to di-            jnowski, T. J. (2010). Experience matters. Psychological
rectly examine the descriptive validity of this assumption in          Science, 21, 960–969.
category learning. Here, we do find evidence consistent with         Pearl, J. (2000). Causality. Models, Reasoning and Inference.
its use.                                                               New York: Cambridge University Press.
                                                                     Rehder, B. & Burnett, R. (2005). Feature inference and the
                    Acknowledgments                                    causal structure of categories. Cognitive Psychology, 50,
This research was supported by grants NE 1713/1 to JDN and             264–314.
ME 3717/2 to BM, from the Deutsche Forschungsgemein-                 Rehder, B. & Hoffman, A. (2005). Eyetracking and selective
schaft (DFG) as part of the priority program “New Frame-               attention in category learning. Cognitive Psychology, 51,
works of Rationality” (SPP 1516). We would like to thank               1–41.
Gregor Caregnato for data collection and Laura Martignon,            Reichenbach, H. (1956). The Direction of Time. Berkeley:
Michael Waldmann, Ralf Mayrhofer, and the reviewers for                University of California Press.
their helpful comments. We also thank Jorge Rey and Sheila           Smith, J. D. & Minda, J. P. (1998). Prototypes in the mist: The
O’Connell (University of Florida, FMELe) for allowing us               early epochs of category learning. Journal of Experimental
to base our artificial plankton stimuli on their copepod pho-          Psychology: Learning, Memory, and Cognition, 24, 1411–
tographs.                                                              1436.
                                                                     Spirtes, P., Glymour, C., & Scheines, R. (1993). Causation,
                         References                                    Prediction, and Search. New York: Springer.
Anderson, J. R. (1991). The adaptive nature of human cate-           Von Sydow, M., Meder, B., & Hagmayer, Y. (2009). A tran-
   gorization. Psychological Review, 3, 409–429.                       sitivity heuristic of probabilistic causal reasoning. In Pro-
Domingos, P. & Pazzani, M. (1997). On the optimality of the            ceedings of the 31st Annual Conference of the Cognitive
   simple Bayesian classifier under zero-one loss. Machine             Science Society.
   Learning, 29, 103–130.                                            Walsh, C. R. & Sloman, S. A. (2007). Updating beliefs with
Fried, L. S. & Holyoak, K. J. (1984). Induction of cate-               causal models: Violations of screening off. In J. R. A.
   gory distributions: A framework for classification learning.        M. A. Gluck & S. M. Kosslyn (Eds.), Memory and Mind:
   Journal of Experimental Psychology: Learning, Memory,               A Festschrift for Gordon H. Bower.
   and Cognition, 2, 234–257.
Hertwig, R., Barron, G., Weber, E., & Erev, I. (2004). Deci-
   sions from experience and the effect of rare events in risky
   choice. Psychological Science, 15, 534–539.
                                                                 2655

