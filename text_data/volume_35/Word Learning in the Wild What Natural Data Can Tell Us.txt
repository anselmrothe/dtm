UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Word Learning in the Wild: What Natural Data Can Tell Us
Permalink
https://escholarship.org/uc/item/7qs8h3g6
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Beekhuizen, Barend
Fazly, Afsaneh
Nematzadeh, Aida
et al.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                    Word Learning in the Wild: What Natural Data Can Tell Us
                      Barend Beekhuizen                       Afsaneh Fazly, Aida Nematzadeh, Suzanne Stevenson
            Leiden University Centre for Linguistics                       Department of Computer Science
                         Leiden University                                        University of Toronto
                b.f.beekhuizen@hum.leidenuniv.nl                       {afsaneh,aida,suzanne}@cs.toronto.edu
                              Abstract                                     Interest has grown in the use of multimodal material for
                                                                        computational studies of word-meaning acquisition, since it
   When a child first begins to acquire a lexicon, the sources of
   word-meanings must be available from the situational context.        contains language embedded in a video of the situation of
   However, it has been argued that the situational availability of     its use. There have been experiments with virtual environ-
   the meanings of relational terms, such as verbs, is lower than       ments (Fleischman & Roy, 2005), natural environments in
   that of whole-object labels, such as nouns. In this paper, we
   present a corpus of child-directed language, paired with situ-       which participants were asked to label objects and actions
   ational descriptions, that enables us to explore the situational     (Yu & Ballard, 2003; Roy & Pentland, 2002), and natural
   availability of word-meanings using a computational learner.         caregiver–child interaction (Roy et al., 2006; Frank, Good-
   Keywords: word learning; relational meaning; corpus devel-           man, & Tenenbaum, 2009). Despite the greater potential for
   opment; computational modeling                                       naturalistic data, these corpora also suffer from limitations.
                                                                        First, some only code whole-object labels, thus restricting
                          Introduction                                  themselves to the meaning of one sort of words, viz. nouns
However the lexical acquisition process in infants develops             (Roy et al., 2006; Frank et al., 2009). In others, the language
beyond the earliest stages, the seeds of the first word mean-           is not child-directed (Fleischman & Roy, 2005; Yu & Bal-
ings must be found in the immediate situational context of              lard, 2003), or the language and situation are unrealistically
early linguistic interaction (Gleitman, 1990). Bootstrapping            temporally aligned (Yu & Ballard, 2003). In this paper, we
these early meanings across a variety of situations, so-called          also overcome the above limitations by developing a corpus
cross-situational learning (Akhtar & Montague, 1999), is one            of child-directed language paired with a precise description
of the early cognitive tasks that children need to perform.             of the situational context. Unlike other corpora, the restric-
For cross-situational learning to work, the situational contexts        tion of our corpus to a particular structured activity allows us
have to contain information that can be extracted and used to           to precisely describe situational aspects that are relevant to
determine what the caregiver is likely referring to. However,           the meanings of various sorts of content words, although the
relatively little is known about the information actually con-          resulting corpus is necessarily small.
tained in situational contexts.                                            One topic we explore in detail is the extent to which words
   In this paper, we present a corpus of child-directed lan-            with observable relational meanings (i.e., physical actions
guage in which the situational context, as found in the accom-          and spatial relations) can be bootstrapped from cross-situa-
panying video material, is described in a precise, formalized           tional learning. As Gentner (1978) argues, mapping words
manner. Not only have the basic-level categories of objects             to relations is more difficult than to objects because relations
been coded, but also some of their properties and the observ-           can typically be construed in more ways. Gleitman (1990)
able relations among agents and objects. This annotated cor-            shows how even observable relations are often not present
pus enables us to explore the situational availability of these         at the time of uttering a word referring to them. This paper
various sources of meaning using computational modeling                 shows, using a different methodology, that relational terms
techniques. As such, we demonstrate the use of computa-                 are indeed harder to glean from the situational context.
tional models as a methodological tool to gain insight the in-
formation that children have available in their natural learn-          A Situated Corpus of Child-Directed Language
ing environment, and that can contribute to cross-situational           Our goal is to construct a corpus that contains situational in-
learning of word meaning.                                               formation that is available to a learner and that can be used in
   The process of cross-situational learning has been studied           learning the meaning of a variety of content words. For devel-
using a multitude of methodologies, each with its limitations.          oping such a corpus, there are two requirements. At a min-
Experimental set-ups must trade off control of the stimuli              imum, in very early word learning at least, we assume that
with the naturalism of the interaction, and thus typically un-          the information that contributes to a word’s meaning must be
derestimate the complexity of the situations caregiver–child            situationally available—that is, the information must be re-
interactions normally take place in (as Medina, Snedeker,               flected in the situation that is perceivable at or near the time
Trueswell, and Gleitman (2011) recently noted again). Some              that the word is uttered. But it must also be the case that
computational studies use child-directed language from tran-            the learner can process this information and understands its
scribed child language corpora, which require the researchers           relevance to the interaction with an interlocutor—i.e., the in-
to automatically enrich the corpora with artificial meaning             formation must be cognitively available as well.
representations (Fazly, Alishahi, & Stevenson, 2010).                      In recent work on coding the available whole objects in
                                                                    1857

video data paired with child-directed language (Roy et al.,
                                                                      Table 1: Coded relations. Parentheses denote optionality. Ag
2006; Frank et al., 2009), generally only situational availabil-
                                                                      = Agent, Pa = Patient, In = Instrument, Re = Recipient, So =
ity need be considered, because the cognitive availability of
                                                                      Source, Go = Goals, Fi = Figure, Gr = Ground
the objects is implicitly assumed. Turning to relational terms,
as we do here, we must explicitly argue that the appropri-               type       name                           roles
ate meanings are cognitively available, because of the evi-              action     grab,letgo,hit                 Ag, Pa, (In)
dence that gleaning the appropriate relational meanings from             action     point,show                     Ag, Pa, Re, (In)
a situation is more difficult (cf., Gentner, 1978). Here we as-          action     move,force                     Ag, Pa, So, Go, (In)
sume that, although child–caregiver interactions take place in           action     position                       Ag, Pa, Gr, (In)
the complex world of everyday life, cognitive availability of            spatial    in,on,off,out,at,near          Fi, Gr
meanings for the child is eased (again, early on) because of             spatial    match,mismatch                 Fi, Gr
the highly-structured nature of such situations, along with the
joint attention caregivers and children share for their objects,
relations, goals and consequences, which function to nar-
row down the set of meanings communicated (cf., Tomasello,            triangular,star} for blocks and holes. The relations and
2003). Thus we focus the annotation on those meanings we              their roles are in Table 1.
argue to be cognitively available to the child, which are not all        For every three-second interval of video, all coder-
the objects and relations in the situation, but only the subset       observed relations, their associated objects and their proper-
that pertains to the current activity.                                ties were coded.2 The actions (first four rows of Table 1) de-
   The result is a corpus that provides information on both           note simple manual behavior, which we assume children can
the situationally and cognitively available objects, properties       recognize (Baillargeon & Wang, 2002). The spatial relations
of objects, and relations between objects. These annotations          reflect basic categories of containment and support (in,on)
rely on relatively lean assumptions about the cognitive avail-        and their negation (out,off), as well as two relations denot-
ability of this information. To the best of our knowledge, this       ing non-containment and non-support contact (at) and near-
is the first corpus that pairs observed objects, properties and       ness (near). Understanding basic spatial relations precedes
relations with spontaneously produced language. As such               the onset of meaning acquisition and can thus be assumed to
annotation is costly, the corpus is necessarily small. It can,        be in place (Needham & Baillargeon, 1993; Hespos & Bail-
however, give us insight into the availability of the sources of      largeon, 2001), although many specifics may be language-
lexical meaning in the situational context, and the problems          specific (Choi, 2006).3 The match or mismatch with a hole
a lack of availability may bring about. In that respect this          was furthermore inferred from these relations. Spatial rela-
small but naturalistic corpus complements earlier annotated           tions were deemed salient if a change in the relation occurred
corpora in enabling us to explore what is and is not available        (e.g., if a block was the Figure of an in-relation in the current
at the time some word is uttered.                                     interval, when it was not in the previous interval).
   The source of our material is a collection of 131 videotaped          The coding procedure was evaluated for inter- and intra-
dyadic interactions (recorded for other purposes) between             coder agreement (Carletta, 1996). All relations were coded
Dutch-speaking mothers and their 16-month-old daughters,              reliably both within and between coders (Cohen’s κ > 0.8),
containing activities such as playing games and eating. In the        except position (intercoder: κ = 0.51, intracoder: κ =
videos, each dyad played a game of putting variously-shaped           0.47). When the coders disagreed, the first author decided the
blocks in a bucket with holes of matching shapes in the lid . A       annotation. A sample of the resulting data is given in Table 2.
set of 32 block games (152 minutes of video) was selected for
our annotation. The first author (a native speaker of Dutch)
                                                                                       The Computational Model
transcribed all speech according to CHAT-guidelines1 , and            We use the probabilistic alignment-based word learning
two assistants coded the video data for the objects, properties       model of Fazly et al. (2010), which has been shown to per-
and relations in the situations. The transcriptions contained         form well using naturalistic data. Using a computational
7842 word tokens (480 types) in 2492 utterances. The lan-             model, we can manipulate input, and doing so, explore the
guage mostly refers to aspects of the game.                           situational and cognitive availability of information, as well
   The situational coding was done according to guide-                as how changes in the input affect learning (Experiment 2).
lines developed by the first author. As the situation con-               The model incrementally takes as input a pair of an utteran-
sists of just one type of activity (playing the game), the            ce (a set of words) and a situation (a set of primitive mean-
set of objects, properties and relations is relatively lim-           ings). The learning algorithm has two phases. In the align-
ited. The most common objects are the bucket, lid,                    ment phase, the words and meanings in the input are prob-
blocks, holes and the two participants, mother and child.                 2 Using  ELAN (Brugman & Russel, 2004).
The feature color={red,green,yellow,blue} was coded                       3 Ideally, one would encode the range of construals of a situation,
for the blocks and the feature shape={square,round,                   including ‘tightness-of-fit’. As a first attempt at relational coding
                                                                      of situations, we opted for convenient, yet widely known, universal
    1 Available at http://childes.psy.cmu.edu/manuals/CHAT.pdf        notions like ‘containment’ and ‘support’.
                                                                  1858

Table 2: A sample of the dataset. The dash-separated abbreviations denote blocks and holes and their properties, where for
blocks the order is b-{red,green,blue,yellow}-{round,star,square,triangular}, and for holes ho-{round,star,square,triangular}
  time      type            coding/transcription
  0m0s      situation       <nothing happens>
            utterance       een. nou jij een.
            translation     one. now you one. “One. Now you try one.”
  0m3s      situation       position(mother, toy, on(toy, floor)) grab(child, b-ye-tr)
                            move(child, b-ye-tr, on(b-ye-tr, floor), near(b-ye-tr, ho-ro)), mismatch(b-ye-tr, ho-ro)
            utterance       nee daar.
            translation     no there. “No, there.”
  0m6s      situation       point(mother, ho-tr, child) position(child, b-ye-tr, near(b-ye-tr, ho-ro)) mismatch(b-ye-tr, ho-ro)
            utterance       nee lieverd hier past ie niet.
            translation     no sweetie here fits he not. “No sweetie, it won’t fit in here.”
abilistically mapped to each other; this process is guided by
                                                                              Table 3: A sample of the lexicon of target words
the conditional probabilities of the meanings given the words
(“the learned meanings”). Second, in the update phase the                type         examples
obtained alignments are used to update the word–meaning as-              action       duwen = force, halen = {move,off,out}
sociations by adding the alignment score to the association.             spatial      in = in, af = off, dicht = {lid,on,bucket}
The word–meaning associations, next, are used to calculate               object       gat = hole, emmer = bucket
the learned meanings, which are then used in the alignment               property     rood = red, ster = star
phase of the next input. These probabilities are based on
the association mass a meaning has for a word, relative to
all other meanings associated with that word. For a formal             this end we need some sort of gold standard, as well as some
explanation, we refer the reader to Fazly et al. (2010).               measure of how well the model approximates this standard.
                                                                          Many words in the utterances have no semantic representa-
        Experiment 1: Exploring the Corpus                             tion in the coded situations (articles, modals, discourse parti-
Using the computational model and the corpus, we aim to                cles). As we cannot expect the model to learn anything about
gain insight into questions such as: what kind of and how              them, we do not consider them in our evaluation. This leaves
much information is derivable from the situational contexts?           us with a small subset of lemmas (n = 41) that do refer to
And is the information equally valuable for different kinds            possible aspects of the situation. These are verbs of manip-
of words (relational words like verbs and prepositions, and            ulation (e.g., pakken ‘grab’) and placing (e.g., stoppen ‘put
non-relational words like adjectives and nouns)?                       into’), spatial relations (e.g., op, ‘on’), object labels (e.g., blok
                                                                       ‘block’) and properties (vierkant ‘square’). As some words
Running the Model                                                      have multiple meanings (stoppen meaning put and in), we
A set of each utterance’s lemmatized word forms is used as             have to determine which set of meanings should be associ-
the linguistic input. As the model takes a set of primitive            ated with each word. Table 3 gives a sample of words and
meanings as the other part of its input, we considered all con-        their relevant gold-standard (true) meanings.
tent elements from the structured meaning annotation of the               We evaluate the learned meanings using two measures.
interval containing the start of the utterance as the set of situ-     First, we look at the summed meaning probabilities over the
ation primitives. An example of an input item is:                      set of true meanings (Summed Conditional Probability or
                                                                       SCP). This measure tells us what proportion of the proba-
   Utterance: {nee lieverd hier passen hij niet}                       bility mass is correctly assigned.
   Situation: { point, mother, hole, triangular, child,                                SCP =                         p( f |w)            (1)
   position, block, yellow, near, round, mismatch }
                                                                                                         ∑
                                                                                               f ∈true meanings(w)
   We set the two smoothing parameters of the model to re-                Second, we look at how high the true meanings are ranked
flect the size of the lexicon, as in Fazly et al. (2010).              among all learned meanings, and do so using Average Preci-
                                                                       sion (AP), calculated as follows:
Evaluation
                                                                                                       n
We need to understand how the model learns various types
                                                                                              AP =    ∑ P(k)∆r(k)                        (2)
of words that refer to aspects of the situational context. To                                         k=1
                                                                   1859

                                                                                                                                             gat , 'hole'                                                      blok , 'block'                                          rond , 'round'
                                                                                                                                                                                       0.00 0.10 0.20
                                                                                                                                         SCP
                                                                                                                          0.8                                                                                                                             0.8
                                                                                                            performance                                                  performance                                                        performance
                      0.35                                                                                                               AP
                                SCP
                                AP                                                                                        0.4                                                                                                                             0.4
                      0.30
                                                                                                                          0.0                                                                                                                             0.0
                      0.25
                                                                                                                                     2        4    6        8   10                                            5       10     15        20                              5     10        15    20
                                                                                                                                     number of occurrences                                                  number of occurrences                                    number of occurrences
        performance
                      0.20
                                                                                                                                         groen , 'green'                                                            uit , 'out'                                             op , 'on'
                      0.15
                                                                                                                                                                                                                                                          0.12
                                                                                                                          0.8                                                          0.08
                                                                                                            performance                                                  performance                                                        performance
                      0.10
                                                                                                                          0.4                                                          0.04                                                               0.06
                      0.05
                                                                                                                          0.0                                                          0.00                                                               0.00
                      0.00
                                                                                                                                 1       2     3   4    5       6    7                                  0     10      20     30        40                        0    10     20    30       40
                                                                                                                                     number of occurrences                                                  number of occurrences                                    number of occurrences
                                      500            1000            1500             2000
                                      number of utterance-situation pairs processed                                                          halen , 'get'                                                     passen , 'fit'                                          draaien , 'turn'
                                                                                                                                                                                       0.12
                                                                                                                          0.20
                                                                                                            performance                                                  performance                                                        performance
                                                                                                                                                                                       0.06                                                               0.06
                                                                                                                          0.10
 Figure 1: Development of the lexicon’s mean SCP and AP
                                                                                                                          0.00                                                         0.00                                                               0.00
                                                                                                                                 0   5       10        20           30                                  0      10       20        30                                    5         10        15
                                                                                                                                     number of occurrences                                                  number of occurrences                                    number of occurrences
where k is the rank, n the total number of ranks, P(k) is the
number of true meanings found up to and including k, divided
by the number of meanings found up to and including k, and                                                   Figure 2: Development of SCP & AP over time for 9 words
∆r(k) is the change in recall between k − 1 and k, which is the
number of true meanings found at k divided by the total num-
ber of true meanings (which is zero in case no true meanings                                            approach AP = 1 (gat), while for most words, the true mean-
are found at rank k). This tells us whether the true meanings                                           ings remain low ranked. There is, however, a development to-
are more or less prominent than the irrelevant ones.                                                    wards a higher AP for many of these words, except for halen
                                                                                                        and uit. The SCP remains low in all cases, even when the true
Results                                                                                                 meaning is ranked first (as in groen and rond), although note
                                                                                                        that for several words there is some improvement in SCP over
Table 4 presents the global results, binned per meaning type
                                                                                                        time. Recall that the model has only seen 2492 utterances at
(properties, objects labels, spatial relations, and actions). We
                                                                                                        this point, and that more data may increase the SCP further.
can see that the meanings of non-relational word meanings
are ranked higher than those of relational word meanings                                                Discussion
(compare AP = 0.81 and AP = 0.25 for properties and object
                                                                                                        In this experiment, the model does not learn most words well.
labels, with AP = 0.19 and AP = 0.15 for spatial relations and
                                                                                                        One potential reason is the small data set, representing only
action labels), although SCP does not differ much between
                                                                                                        three hours of interaction. We observe that many develop-
the categories. In general, the probability distributions of the
                                                                                                        mental curves seem not to have reached their asymptotes yet,
learned meanings do not have very strong peaks: the highest
                                                                                                        suggesting that further learning could occur with more data.
ranking meanings rarely have a learned meaning probability
                                                                                                        We also, admittedly, have the model discard valuable infor-
of more than 0.20. Nevertheless, with 78 primitive meanings,
                                                    1                                                   mation from the data. Both the linguistic structure (syntax)
the model does learn well beyond a baseline of 78     = 0.013.
                                                                                                        and the semantic structure (predicate-argument relations) are
   Looking at the development of the SCP and AP values over
                                                                                                        currently ignored by the model but could be useful in creating
time (Fig. 1), we see strikingly little development in the SCP,
                                                                                                        the mapping.
whereas the AP rises for a time, then shows a slight decline.
                                                                                                           In addition, the highly structured and restricted nature of
   Splitting the developmental curves out over some of the
                                                                                                        the data, which we expected to help by focusing the learn-
words (Fig. 2), we see that the words are learned rather hete-
                                                                                                        ing, may actually be hindering performance. We observe that
rogeneously. Looking at the AP first, some words are ac-
                                                                                                        some words have a very high ranking for their true mean-
quired instantly, with AP = 1 (i.e., the correct meaning rank-
                                                                                                        ings (high AP), yet have low learned probability mass (low
ing first) from early on (groen and rond), others gradually
                                                                                                        SCP). (For example, see the words groen and rond in Fig-
                                                                                                        ure 2.) On the one hand, the structured and restricted nature
                                                                                                        of the blocks game entails that a word’s true meaning often
                             Table 4: Results of Experiment 1                                           consistently appears with it. On the other hand, however, the
                                                                                                        limited nature of the interactions in the data also entails that
                         property           object            spatial            action      total      many irrelevant meanings consistently appear with the word.
    SCP                  0.10               0.05              0.09               0.07        0.08       For example, the object of a grab action is almost always a
    AP                   0.81               0.25              0.19               0.15        0.31       block, so that the learner cannot rule out block as a possible
                                                                                                     1860

meaning of pakken ‘grab’. The lack of situational variability
                                                                                      Table 5: Results of Experiment 2
in the input is thus an obstacle to cross-situational learning,
because it requires a consistent co-occurrence of true mean-               W                     prop.  object spatial action total
ings with a word coupled with variability in the presence of                              SCP    0.10   0.05   0.09 0.07      0.08
irrelevant meanings to help rule them out.                                 Ui : Ui
                                                                                          AP     0.81   0.25   0.19 0.15      0.31
   A first solution that comes to mind is a corpus representing                           SCP    0.10   0.04   0.09 0.07      0.07
a wider variety of activities, with less situational uniformity,           Ui−1 : Ui
                                                                                          AP     0.80   0.17   0.20 0.14      0.31
for true cross-situational learning. The corpus from which                                SCP    0.11   0.06   0.11 0.08      0.08
we drew our dyads here does have a number of other types                   Ui : Ui+1
                                                                                          AP     0.79   0.45   0.24 0.18      0.40
of situations we can include in future annotation. Second,                                SCP    0.08   0.05   0.10 0.08      0.07
even with relatively homogeneous situations, we expect the                 Ui−1 :Ui+1
                                                                                          AP     0.79   0.41   0.22 0.20      0.39
learner’s attentional mechanisms to help filter out irrelevant
meanings. Adding attentional mechanisms, such as the ones
in Nematzadeh, Fazly, and Stevenson (2012), is a next step             is the window-setting used in Experiment 1). The window-
   A final issue we observed with the data is that the true            setting that only draws situational context from the intervals
meanings for words in an utterance are sometimes not present           between the previous utterance and the current one (W =
within the situational interval paired with the utterance. This        Ui−1 : Ui ) does not improve over W = Ui : Ui . As hypothe-
problem is very salient for relational meanings, which are of-         sized, however, due to utterances that refer to future actions,
ten displaced in time from the utterance that refers to them           the results show that having a window that includes meanings
(e.g., Go grab that one! or Don’t take the lid off now!). This         from the intervals up to the next utterance enables the model
might explain why spatial relation terms and verbs display             to learn the object, spatial and action words better (at least
weaker associations with their true meanings than do words             according to our AP measure). The trade-off is a negligible
for objects and their properties. In the case of positive imper-       decline in the learning of property words.
atives, we do find that the actions are often carried out slightly
later than the utterance. In Experiment 2, we explore whether          Discussion
this problem of temporal displacement can be mitigated.                Some important information for acquiring the meaning of re-
                                                                       lational words can be found in the situations unfolding after
  Experiment 2: Widening the Temporal Scope                            the utterance has been produced. Clearly, this needs to be in-
Our hypothesis is that presenting the model with situational           terpreted within the context of playing a game, in which the
meanings only from the time of the utterance impedes the               relevant topics of communication (the game goals) often lie
learning of relational terms. Here we explore expanding the            in the future w.r.t. the moment of communication. While ex-
temporal scope of the situational input to the model.                  panding the situational window adds some irrelevant as well
                                                                       as true meanings, the balance struck by this pragmatically-
Motivation and Set-up                                                  defined windowing approach seems to help the model acquire
Suppose that in word learning, the learner is not narrowly fo-         the meaning of relational terms (as well as objects!) some-
cussed on the situation at exact moment of the utterance, but          what better, with little negative impact on property words.
also considers some of the situational context taking place            Note that the improvement from adding the post-utterance
around that moment. That is: not only the situation at the very        meanings is found mainly in the AP metric: the SCP values
moment of the utterance is cognitively available to a learner,         remain similar across the simulations. Even though the prob-
but also some of the surrounding situations. To make this no-          ability mass of the true meanings is not changed much, they
tion precise, we assume that the learner may consider as rele-         are now more often better than the irrelevant meanings. This
vant to an utterance Ui any meanings in the situational context        means that the probability values are close to each other and
starting from the interval of the previous utterance Ui−1 up to        a very small change may improve the rankings visibly.
and including the interval of the next utterance Ui+1 . (That is,
we assume that the relevance of situations overlaps previous               General Discussion and Future Directions
and subsequent utterances.) We thus evaluate the model on              In this research, we have developed a corpus of caregiver–
three possible “windows” W of situational context for utter-           child interactions in which video is annotated with tran-
ance Ui : all video intervals up to and including the previous         scribed utterances and a precise description of the depicted
and next utterance in the corpus (W = Ui−1 : Ui+1 ); only the          situational context. Unlike other recent multimodal corpora,
interval of Ui−1 up to the current interval (W = Ui−1 : Ui ), or       our annotation of the situational context includes meaning el-
the current interval up to Ui+1 (W = Ui : Ui+1 ).                      ements that correspond not only to objects and their proper-
                                                                       ties, but to relations as well. Thus the meaning annotations
Results                                                                support the learning of various word types, including nouns,
Using the same parameter settings and evaluation metrics as            adjectives, prepositions/particles, and verbs. Our initial work
in Experiment 1, we obtain the results in Table 5 (W = Ui : Ui         has explored how we can use this corpus with a computational
                                                                   1861

model of cross-situational word learning to explore what in-                                  References
formation must be available to the child from the situation to       Akhtar, N., & Montague, L. (1999). Early Lexical Acquisi-
support word learning, and to examine the relative ease or dif-        tion: The Role of Cross-Situational Learning. First Lan-
ficulty of learning various types of words in early acquisition.       guage, 19(57), 347–358.
   Despite the small size of the target lexicon, the model did       Baillargeon, R., & Wang, S.-H. (2002). Event Categorization
not perform robustly in the learning task, revealing a num-            in Infancy. Trends in Cognitive Sciences, 6(2), 85–93.
ber of potential areas of improvement for both the corpus            Brugman, H., & Russel, A. (2004). Annotating Multimedia/
and the model itself. First, due to the cost of annotation, the        Multi-modal resources with ELAN. In Proceedings LREC.
size of the corpus (only 8,000 word tokens) almost certainly         Carletta, J. (1996). Assessing Agreement on Classification
limits the learning. Nonetheless, even this small corpus can           Tasks: The Kappa Statistic. Computational Linguistics,
be a complementary source of information to larger corpora             22(2), 249–254.
that are semantically less naturalistic, or contain only object      Choi, S. (2006). Preverbal Spatial Cognition and Language-
labels. Second, the corpus seems to lack sufficient cross-             Specific Input: Categories of Containment and Support. In
situational variability for many words to be learned. In more          K. Hirsh-Pasek & R. M. Golinkoff (Eds.), Action Meets
general child–caregiver interactions, a word occurs across a           Word. How Children Learn Verbs (pp. 191–207). Oxford,
wider variety of contexts (eating scenes, bed-time procedures          UK: Oxford University Press.
and so on), enabling a child to rule out as possible meanings        Fazly, A., Alishahi, A., & Stevenson, S. (2010). A Prob-
those aspects of the context that are irrelevant to the word.          abilistic Computational Model of Cross-Situational Word
Third, regardless of the uniformity or variability of the data,        Learning. Cognitive Science, 34(6), 1017–1063.
a realistic model of word learning needs to incorporate an at-       Fleischman, M., & Roy, D. K. (2005). Why Verbs are Harder
tentional mechanism that helps it focus on those aspects of            to Learn than Nouns. Initial Insights from a Computational
the situation that are likely to be referred to.                       Model of Intention Recognition in Situated Word Learning.
   Even with this restricted corpus, we find that relational           In Proceedings CogSci.
words (verbs, prepositions) are particularly problematic to          Frank, M. C., Goodman, N. D., & Tenenbaum, J. B. (2009).
learn compared to words for objects and properties, in line            Using Speakers Referential Intentions to Model Early
with a wealth of psycholinguistic observation to this effect           Cross-Situational Word Learning. Psychological Science,
(Gleitman, 1990; Gentner, 1978). Because the situational               20(5), 578–585.
context to which a relational term refers is often displaced,        Gentner, D. (1978). On Relational Meaning: The Acquisition
expanding the temporal window of situational context for               of Verb Meaning. Child Development, 49, 988–998.
each utterance led to an improvement in the learning of rela-        Gleitman, L. (1990). Sources of Verb Meanings. Language
tional terms, but surprisingly led to even greater improvement         Acquisition, 1(1), 3–55.
in the learning of words for objects.                                Hespos, S. J., & Baillargeon, R. (2001). Reasoning about
   Perhaps, following Gleitman, more structured learning is            Containment Events in Very Young Infants. Cognition,
necessary for acquiring the meaning of relational words, but           78(3), 207–45.
the exact source and nature of this structured learning, and its     Medina, T. N., Snedeker, J., Trueswell, J. C., & Gleitman,
integration with methods of cross-situational learning, is an          L. R. (2011). How Words Can and Cannot be Learned by
exciting open issue. Important to look into, and perhaps prob-         Observation. PNAS, 108(22), 9014–9.
lematic, is the high proportion of closed-class items in child-      Needham, A., & Baillargeon, R. (1993). Intuitions about
directed utterances (e.g., pronouns, aspectual and modal aux-          Support in 4.5-Month-Old Infants. Cognition, 47, 121–
iliaries, and particles) that have received little attention in        148.
word-learning models, but may play a crucial role in using the       Nematzadeh, A., Fazly, A., & Stevenson, S. (2012). A Com-
structure of an utterance to help determine the meaning of un-         putational Model of Memory, Attention, and Word Learn-
known lexical items. More research into the degree to which            ing. In Proceedings CMCL.
this information, as found in actual child-directed language,        Roy, D. K., Patel, R., Decamp, P., Kubat, R., Fleischman, M.,
can help is a question in want of an answer, and modeling              Roy, B., et al. (2006). The Human Speechome Project Step-
techniques combined with good data can help us approach it.            ping into the Shoes of Children. In Proceedings CogSci.
                                                                     Roy, D. K., & Pentland, A. P. (2002). Learning Words from
                    Acknowledgments                                    Sights and Sounds: A Computational Model. Cognitive
                                                                       Science, 26, 113–146.
We gratefully acknowledge the funding of BB through NWO              Tomasello, M. (2003). Constructing a Language: A Usage-
of the Netherlands (grant 322.70.001) and AF, AN and SS                Based Theory of Language Acquisition. Cambridge, MA:
through NSERC of Canada, and the Faculty of Arts & Sci-                Harvard University Press.
ence, University of Toronto. We would like to thank Mari-            Yu, C., & Ballard, D. H. (2003). A Multimodal Learning
nus van IJzendoorn and Marian Bakermans-Kranenburg for                 Interface for Grounding Spoken Language in Sensory Per-
making the video data available, and Arie Verhagen and two             ceptions. Proceedings ICMI.
anonymous reviewers for helpful comments.
                                                                 1862

