UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning to perceive coherent objects
Permalink
https://escholarship.org/uc/item/4d26685g
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Dorfman, Nimord
Harari, Daniel
Ullman, Shimon
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                      Learning to Perceive Coherent Objects
                                        Nimrod Dorfman1 (nimrodd@weizmann.ac.il)
                                       Daniel Harari1 (danny.harari@weizmann.ac.il)
                                     Shimon Ullman1 (shimon.ullman@weizmann.ac.il)
                         1
                           Department of Computer Science, Weizmann Institute of Science, Rehovot, Israel
                              Abstract
   Object segregation in a visual scene is a complex perceptual
  process that relies on the integration of multiple cues. The
  task is computationally challenging, and even the best
  performing models fall significantly short of human
  performance. Infants initially have a surprisingly
  impoverished set of segregation cues and their ability to
  perform object segregation in static images is severely
  limited. Major questions that arise are therefore how the rich
  set of useful cues is learned, and what initial capacities make
  this learning possible. Here we present a computational model
  that initially incorporates only two basic capacities known to                            A                            B
  exist at an early age: the grouping of image regions by              Figure 1: Object segregation, infant to adult capacity.
  common motion and the detection of motion discontinuities.           (A): A complex scene, easily segregated by an adult.
  The model then learns significant aspects of object
  segregation in static images in an entirely unsupervised             (B): At 3 months, infants do not appear to divide the
  manner by observing videos of objects in motion.                     figure into two components (after Spelke et al. 1993).
  Implications of the model to infant learning and to the future
  development of object segregation models are discussed.
   Keywords: Visual perception; computational modeling;               start from a surprisingly limited capacity for segregating the
   development; object segregation; figure-ground.                    world into coherent objects, and reach the capability of the
                                                                      adult system. For computational modeling of vision, an
                   Background and Goals                               intriguing possibility is to try to surpass the capabilities of
                                                                      current models by following a strategy similar to human
We naturally perceive the scene around us as containing
                                                                      development, namely, start with the appropriate set of basic
coherent objects, separated from each other and from their
                                                                      capacities and learning mechanisms and allow the model to
background. Even in a complex image such as Figure 1A,
                                                                      develop on its own the final segregation capabilities.
we can count for example the number of distinct cars,
                                                                         In the current study we focus on specific sub-problems
delineate their boundaries, etc. The ability to segregate the
                                                                      within this broad domain. We develop a model that
scene into objects, delineate their boundaries, and determine
                                                                      incorporates simple basic capacities, which are known
occlusion relations (termed here 'object segregation'), relies
                                                                      empirically to already exist in young infants. It uses them to
on a complex set of processes, which integrate multiple cues
                                                                      segregate familiar objects and to extract and use so-called
that are only partially understood.
                                                                      'boundary ownership' cues (indicating boundaries as well as
  Infants' initial ability to segregate scenes into coherent
                                                                      figure/background direction) for static object segregation.
objects is rudimentary and it does not make use of even
                                                                      The model initially has no ability to segregate objects in
basic salient 'Gestalt' properties such as uniformity of
                                                                      static images, but it can compute visual motion and motion
texture, brightness or color, the smooth continuity of
                                                                      discontinuities. It is exposed in an unsupervised manner to
boundary contours, occlusion cues and the like (Spelke et al.
                                                                      video sequences containing moving objects. It uses them to
1993). For instance, infants at 3 months of age do not appear
                                                                      segregate familiar objects in static images and to learn local
to distinguish that the shape in Figure 1B is likely to be
                                                                      boundary ownership cues. These are used as cues for static
composed of two distinct components. The contrast between
                                                                      object segregation, applied to novel objects.
Figures 1A and 1B illustrates the span of learning
                                                                         In the next sections we briefly summarize relevant
accomplished in performing object segregation. The ability
                                                                      background from developmental studies of object
to segregate objects based on multiple cues develops
                                                                      segregation, followed by a presentation of the current
quickly already in the first year of life, but the learning
                                                                      model.
process continues over an extended period of time (Kovaks
et al. 1999). The process of learning object segregation
                                                                      Early Development of Object Segregation
raises fundamental questions for cognitive development and
computational modeling of vision. For cognitive                       Initial object segregation by infants is based almost
development, it is of basic interest to understand the innate         exclusively on dynamic cues, which are then used to learn
capacities and learning mechanisms that allow the system to           static object segregation. We focus below on two main
                                                                  394

aspects of using visual motion for object segregation:             specific objects and object classes. The different cues and
grouping by common motion, and the use of motion                   their integration into a full segregation scheme are still a
discontinuities. We also comment briefly on the use of static      subject of active research in both human studies and
cues.                                                              computational modeling. Yearly competitions and
                                                                   evaluations of natural image segmentation1 show consistent
Common Motion Infants use visual motion to group                   improvements, but current performance is still significantly
together adjacent regions that move together. These grouped        below human performance. Due to space limitations, we
image entities, discovered through motion, are also stored in      will not review here different modeling efforts. The closest
memory and can subsequently be identified in static images         to the current study is the SANE (segmentation according to
(Needham 2001, Needham & Baillargeon 1998, Needham &               natural examples) model by Ross et al. (2009), where, like
Modi 1999, Spelke 1990, Spelke et al. 1989). For example,          in the current study, motion segmentation was used to guide
if 4.5 months old see in a static image a region A next to a       static segmentation. However, the SANE model does not
second region B, their expectations are shaped by their            use the two main components of the current model: learning
recent experience of seeing these regions in motion. If A          boundary-ownership cues near a boundary, and learning
and B moved together, infants will treat them as a unit and        object-based segregation. It uses instead local binary 5×5
will be surprised if they move separately, but not if they saw     boundary elements, with no ownership information, and
A or B moving alone. The grouping of regions into a single         their pair-wise relationships.
unit depends on their common motion: if two regions differ
in their image motion, even if they remain in contact, they        Goals of the Current Study
are treated as separate objects (Spelke 1990, Spelke et al.        As reviewed above, infants are sensitive to motion cues for
1989). Retention in memory of the formed unit is limited in        segregation, but lack sensitivity to most static cues for
time (about 24 hours at 4.5 months of age), but grows              objects identity. It is therefore natural to ask how static
gradually with age (Needham & Baillargeon 1998,                    segregation cues may be learned during development,
Needham & Modi 1999). This use of stored object                    guided by dynamic cues. We focus on two dynamic cues
representations for segregation is termed 'object-based            that are prominent in early infant perception. The first is
segregation', and it can generalize with more experience to        common motion, guiding object-based segmentation. That
other similar objects ('class based' segmentation), provided       is, infants naturally segregate adjacent image regions that
that the differences are initially small (Needham & Modi,          share common motion, and can identify similar
1999). Two regions moving together can also be grouped             configurations in static images. One goal is therefore to
together to form a single unit when they are non-contiguous        model this learning of object-based segregation. Second,
but separated behind an occluder (Kellman & Spelke 1983)           infants are sensitive to dynamic cues created by the
provided that the parts are roughly aligned (Johnson &             boundaries of moving objects, and these are used by the
Aslin 1996).                                                       model to learn useful static boundary cues. Although
                                                                   boundary ownership cues appear to play a major role in
Motion Discontinuities In addition to region grouping              human object segregation (e.g. McDermott 2004, Ghose &
based on common motion, infants are also sensitive from an         Palmer, 2010), they are not usually used in computational
early age (5 months or earlier) to dynamic cues created by         models, in part because it is still unclear which features are
the boundaries of moving objects (e.g., Granrud et al. 1984).      useful for assigning boundary ownership. A possible
                                                                   outcome of a model for the unsupervised learning of
Static Cues In terms of static cues, at 3-5 months                 boundary ownership features could be, therefore, the
contiguous regions that are not separated by a visible gap         extraction and use of such features in future segmentation
tend to be grouped together, and are expected for example to       models and algorithms.
move together rather than separately (Needham &
Baillargeon 1998, Spelke 1990, Spelke et al. 1989). At this                                  The Model
age they show little or no evidence for using grouping
principles based on uniformity of color, texture, and              The current learning model has initially two 'innate'
continuity of bounding contour in object perception. At 9          capacities for using visual motion to learn object
months the effect of such grouping cues is still weak              segregation. The first is the capability to group together
(Spelke et al. 1993). The learning of static cues is gradual,      adjacent regions based on their common motion. A
and appears to depend on familiarity with many objects             representation of the grouped shape is stored and can then
(Needham & Modi 1989, Spelke 1990).                                be used for segregating similar shapes in novel static
                                                                   images. The second capacity is to extract motion
Following extended learning, perceptual organization into          discontinuities. These are used as teaching signals to extract
distinct objects and their boundaries develops into a              image features located along object boundaries, together
complex process that relies on a rich set of cues. In addition     with a labeling of the figure/background sides, and
to image-based, or bottom-up properties, organization into
                                                                      1
objects depends on top-down cues, based on familiarity with             http://www.eecs.berkeley.edu/Research/Projects/CS/vision/gro
                                                                   uping/segbench/
                                                               395

subsequently use them to locate novel object boundaries and
identify the figure direction in new static images. These two
components and how they are used by the learning model
are described in subsequent sections, following a brief
description of the training data used for learning.
Training and Testing Data
Data consisted of 48 movies, each depicting an object (doll,
banana, remote control etc.) moved by hand in front of a
textured background (12 objects, 12 backgrounds). For each
movie, there are 3 other movies showing the same object on
a different background, 3 movies showing different objects
on the same background; the remaining 41 have different
object and a background. Each movie is one minute long
(1500 frames), frame size varies between 520×720 pixels to
576×752 pixels.
Object-based Segregation                                            Figure 2: Examples of object-based segregations produced
The goal of object-based segregation is to learn the                by the algorithm. Bottom right: an erroneous example.
appearance of a specific object, such as the doll, fruit, etc.,
in our movies, and then find the full extent of the object and
separate it from its background under new settings. The part        Segregation of Static Images If the same or similar object
of the model that deals with object-based segregation is            appears in a new image, it can be detected and segregated
based on an object detection model used, with some                  based on the above representation, using the following
variations, in computer vision schemes, termed 'star model'.        algorithm. The new image is represented by its local SIFT
For the purpose of object segregation, the model is                 descriptors. For each descriptor F in the image, we find its
augmented with a 'back projection' stage. Since this part           K=25 nearest neighbors among the descriptors of the stored
relies on existing object detection models it will be               object. Each neighbor Fk, votes for the location of the center
described here briefly.                                             C according to the displacement Vk. Votes are weighted by
   The input to the object-based segregation is an image in a       the similarity between F and Fk, and aggregated over the
movie, together with the visual motion associated with the          image. If an image location C obtains a sufficient number of
image. The scheme used for motion computation was an                total votes, an object is detected, centered at location C. The
available optical flow algorithm (Sun et al. 2010) combined         full object is then segregated by a 'back projection' step: all
with background subtraction, assuming that the camera               image descriptors that contributed their votes to the selected
itself is stationary (as in Ross & Kaelbling, 2009).                location are identified as components of the detected object.
   The motion computation divides the image into two                A final object/background decision is made by an
components: a stationary one, and a set of one or more              automatically set threshold.
moving regions. One of the moving regions is selected for
further processing. The selected region is covered by local         Results – Speed and Generalization In infants, even a few
image descriptors, each one representing the appearance of a        seconds of observing an object in motion already affects
local region. The implementation used the standard SIFT             subsequent segregation of the same object in static images
image descriptor (Lowe 2004) because computationally, it is         (e.g. Needham, & Baillargeon 1998). The segmentation is
robust and efficient, and biologically, it is similar to            effective for images of the same or similar object and
intermediate level units used in modeling (e.g. S2 units in         generalizes gradually to less similar objects (Needham, &
the cortical H-Max model, Riesenhuber & Poggio 1999). A             Baillargeon 1998, Needham & Modi 1999). Object-based
single reference point C is selected at the center of the           segregation in the model showed similar characteristics.
selected region, and for each image descriptor Fi, the              Brief (5 seconds) training was sufficient for learning object
displacement Vi from its location to the center C is stored.        segregation of a specific object in subsequent parts of the
The object defined by the moving region is therefore                movie, with some generalization to a different pose and
represented by its center C, and the set of image descriptors       different background. The object is often grouped by motion
(Fi), each one with its displacement Vi from the object's           with the holding hand; the two can be separated when the
center.                                                             hand is learned as an object on its own (Ullman et al. 2012).
                                                                    Figure 2 shows example segregations.
                                                                396

Figure 3: Detecting object boundaries. Left: Original image, with object-based segregation. Object is located, but boundaries
are inaccurate. Center: Detection of boundary features. Warm colors indicate figure side of boundary, cold colors – ground
side. Both object and background were not seen during training of boundary detector. Right: Combining object-based
segregation with boundary detections. Object is detected with correct boundaries.
   Results were tested by learning an object model in each           Learning Process To learn useful boundary features,
movie using 5 sec and 40 sec segments, and testing on both           motion discontinuities are used to guide the extraction of
later parts of the same movie, as well as the same object in         static boundary features and their figure-ground labeling.
other movies, with different backgrounds and larger                  The learning procedure is simple, proceeding along the
variations in pose and lighting. Agreement between the true          following stages. In each frame of the training movies,
object (extracted by motion) and the model segregation               motion discontinuities are detected, and at each pixel along
were measured by the standard score s = |            | |      |,     the boundary, image patches are extracted at 5 different
where T is the true object and S the segmented. Mean scores          sizes (ranging from 12×12 pixels to 60×60 pixels). Each
for 5-sec training were s = 0.3 vs. 0.23 on same vs. different       patch is represented by a rotation invariant SIFT descriptor,
movies, and for 40-sec training s = 0.49, 0.36 respectively.         producing a fixed-size descriptor regardless of original
Effects of training time and generalization are highly               patch size. The motion signal is also used to label the figure
significant (1-tailed t-test, n=1200, p < 10-6 in all                part (which is moving in the training images) and
comparisons).                                                        background part (which is stationary) in each stored patch.
   The object-based segregation in the model segregates the          From these, a subset of boundary patches is later selected, as
general object region but it does not accurately delineate the       described in the Results section below.
boundaries. Since the object is represented by local
appearance patches, it is sensitive to texture properties            Use In Static Images The learned boundary features are
inside the object, in agreement with infant's object-based           then used to identify likely object boundaries in novel static
segregation (Needham & Modi, 1999). In contrast, the                 images. Given a static input image, local SIFT features are
model shows limited accuracy around object boundaries; it            extracted at the same 5 sizes, densely over the entire image.
will be interesting to test this prediction in infants' vision       For each feature, its 25 nearest neighbors in the stored set of
(see discussion).                                                    trained boundary features are extracted (using a fast
                                                                     approximation algorithm, Arya & Mount 1993). These
Learning Boundary Features                                           neighbors are used to estimate the likelihood of an object
The accurate delineation of boundaries is important for              boundary at this location, and to identify the figure side of
interacting with objects, e.g. for grabbing, finding free space      the potential boundary. Specifically, each neighbor i has a
to place them, etc. This is obtained in the model by a second        SIFT descriptor Di and an object direction θi. For an image
mechanism, which uses motion discontinuities to learn static         patch with descriptor D, we define the predicted object
cues for occluding boundaries, as described next.                    direction θ and a score S as follows:
                                                                 397

Figure 4: 25 examples of top-scoring boundary detection             Figure 5: Object segmentation with the GrabCut algorithm.
features, chosen by cross validation testing over 48 folds.         Left: Segmentation produced by the algorithm using default
Individual features are not reliable on their own – it takes at     initialization. Right: Segmentation results with initialization
least 1,000 features to get good predictions (see text).            by our segregation score maps.
                       ‖     ‖                                      Consequently, it became possible to extract and study a
                ∑                           [       ]               much richer set of boundary features compared with
                                                                    previous studies that used human annotated boundaries
                                                                    (Geisler et al. 2009, Fowlkes et al. 2007). The learning
                            ( )            ‖ ‖                      process produced a rich and varied set of boundary cues.
                                                                    Their analysis revealed the following properties. (i)
Where atan2 is a 4-quadrant arctangent function. σ is set to        Individual boundary features are probabilistic in the sense
0.25. θ, S are then used to estimate the figure/background          that they contribute information to the correct figure
direction at the patch in question. Estimations of all patches      direction, but individual features are usually not definitive
are added together weighted by S and smoothed by a spatial          on their own. When training on 100,000 boundary features,
Gaussian function (positive in the figure, negative in              the correct figure side is predicted in novel boundary
background side). This yields a single total figure-score at        features 78% of the times. (ii) Boundary features are
each image location, where a positive score is likely to be         consistent across image sets and are therefore useful for
the figure side of an object boundary.                              generalization to novel images. Our testing was done in 48
                                                                    cross-validation folds, each time testing one movie, and
Results Examples of boundary detection are shown in                 excluding all movies with the same object or background
Figure 3. We used statistical testing to compare the density        from training data. (iii) There is a large set of useful
of boundary features in a region (10 pixels) around object          boundary features, and using a restricted subset is less
boundaries compared with inside the object and on the               accurate than using the larger set. We selected the best
background. Density was significantly higher around the             performing features by cross-validation folds, and tested
boundaries compared with internal or external regions. In           sets of different sizes, yielding 75% accuracy for 10,000
contrast, object-based segregation produced higher density          patches, 71% for 1,000, 65% for 100, 54% for 50.
in internal regions compared with boundary or external              Nonetheless, the improvement diminishes for very large
regions (1-tailed t-test, n=1200 p < 10-6 in all comparisons).      sets, suggesting that saturation may be reached at some
                                                                    point, and there is no need to memorize every observed
Types And Number Of Boundary Features                               feature. Exploring mechanisms of feature retention is left for
Psychophysical and computational studies of boundary                future work. (iv) Among the top-scoring boundary features
features have suggested several types of informative                (examples in Fig. 4) there is a significant fraction that can
boundary features, including: interposition (T-junctions),          be labeled 'extremal edges'. These have only recently been
surface junctions, such as Y-junctions and arrow-junctions,         found to play a crucial role in human vision (Ghose &
and extremal edges, or folds, (Geisler et al. 2009, Ghose &         Palmer 2010), and have not been tested in infants' object
Palmer 2010) coming from the projection of an occluding             segregation. Our model focuses on learning boundary
edge curving smoothly in 3D, typically creating a highlight         features, and does not model their integration within a fully
or shadow along the curving edge.                                   functional segregation system. To illustrate their
  The current study used automatically labeled object               contribution we therefore used them as input to an existing
boundaries, identified by motion discontinuities.                   algorithm (GrabCut, Rother et al. 2004); results are
                                                                398

illustrated in Figure 5. The figure shows performance of the           Granrud CE, Yonas A, Smith IM, Arterberry ME,
algorithm in its standard form (left), and the same algorithm            Glicksman ML, Sorknes AC. (1984) Infants' sensitivity to
when supplied with our object and boundary scores.                       accretion and deletion of texture as information for depth
                                                                         at an edge. Child Dev. 55(4), 1630-1636.
                         Discussion                                    Ghose, T, & Palmer, S. (2010) Extremal edges versus other
The model demonstrates how static object segregation can                 principles of ﬁgure-ground organization. J. Vision 10(8)3,
be learned effectively guided by two motion based                        1 – 17
mechanisms known to be innate or early learned in infants'             Johnson, SP & Aslin, RN. (1996). Perception of Object
vision: grouping by common motion and sensitivity to                     Unity in Young Infants: The Roles of Motion, Depth, and
motion discontinuities.                                                  Orientation. Cog. Dev. 11, 161-180.
   These mechanisms are used by the model for two                      Kellman, PJ, Spelke, ES, (1989). Perception of Partly
complementary goals: common motion is used for object-                   Occluded Objects in Infancy. Cog. Psych. 15, 483-524.
based segregation, and motion discontinuities are used for             Kovacs, I, Kozma, P, Feher, A, Benedek, G. (1999). Late
learning static occlusion cues. In agreement with infants                Maturation of Visual Spatial Integration in Humans.
learning, the learning of object-based segregation by the                PNAS 961(21), 12204-12209.
model is fast, with initial sensitivity to details of the object's     Lowe, D. (2004) Distinctive image features from scale-
internal texture. It identifies well the region of the object            invariant keypoints. Int. J. Comp. Vis., 60, 2 91-110.
with reduced accuracy near the boundaries. Boundary cues               McDermott, J. (2004) Psychophysics with junctions in real
require more prolonged learning, but they appear to                      images. Perception, 33, 1101 – 1127.
generalize broadly to novel object images. The set of useful           Mount, D. & Arya, S. (2010) ANN: A Library for
boundary features found by the model is large and varied,                Approximate        Nearest        Neighbor        Searching.
including a major contribution from extremal edges, which                http://www.cs.umd.edu /~mount/ANN/
have played a limited role in modeling so far.                         Needham, A,, & Baillargeon, R. (1998). Effects of prior
   The results of the study suggest a number of interesting              experience in 4.5-month-old infants’ object segregation.
directions for further research. In terms of infant studies, it          Inf. Behav. Dev. 21, 1-24.
will be of interest to test their capacity for object                  Needham, A. (2001). Object Recognition and Object
segregation based on extremal cues, which, to the best of                Segregation in 4.5-Month-Old Infants. J. Exp. Child
our knowledge have not been tested so far. Another                       Psychol, (78), 3-24.
prediction that can be tested is whether object-based                  Needham A, Modi A., (1999). Infants' use of prior
segregation by infants, which is sensitive to internal texture,          experiences with objects in object segregation:
will exhibit insensitivity to the object's boundary.                     implications for object recognition in infancy. Adv Child
Computationally, it will be interesting to compile a large set           Dev Behav., 27:99-133.
of useful boundary features that could be used by future               Riesenhuber, M. & Poggio, T. (1999), Hierarchical models
segmentation algorithms. Finally, since scene segmentation               of object recognition in cortex. Nat. Neurosci, 2(11),
in natural images is still a challenging open problem, it will           1019–1025.
be of interest to extend the current approach and examine              Ross, MG & Kaelbling, LP. (2009). Segmentation
whether following human development, by letting object                   According to Natural Examples: Learning Static
segregation (including cues not considered in the current                Segmentation from Motion Segmentation. IEEE PAMI
model) be guided and learned using dynamic cues, could                   31(4), 661-676.
lead to the emergence of models approaching human                      Rother, C., Kolmogorov, V., Blake, A. (2004) GrabCut:
segregation capacities.                                                  Interactive Foreground Extraction using Iterated Graph
                                                                         Cuts. ACM Transactions on Graphics, 23 (3), 309-314.
                                                                       Spelke, ES. (1990). Principles of object perception.
                    Acknowledgments                                      Cognitive Science, 14, 29-56
The work was supported by European Research Council                    Spelke, E.S, Breinlinger, K, Jacobson, K, Phillips, A. (1993)
(ERC) Advanced Grant “Digital Baby” (to S.U.).                           Gestalt relations and object perception: a developmental
                                                                         study. Perception, (22) 1483-1501.
                         References                                    Spelke, ES, von Hofsten, C, Kestenbaum, R. (1989). Object
Arya, S., & Mount, DN (1993). Approximate nearest                        Perception in Infancy: Interaction of Spatial and Kinetic
   neighbor queries in fixed dimensions. ACM-SIAM, 271–                  Information for Object Boundaries. Developmental
   280.                                                                  Psychology, 25(2), 185-196.
Geisler, WS, Najemnik, J, Ing, AD. (2009). Optimal                     Sun, D, Roth, S., Black, M.J. (2010). Secrets of optical flow
   stimulus encoders for natural tasks. J. Vision 9(13), 1–16            estimation and their principles. IEEE CVPR, 2432-2439.
Granrud, CE & Yonas, A. (1984) Infants’ Perception of                  Ullman, S. Harari, D. Dorfman, N. 2012. From simple
   Pictorially Specified Interposition. J. Exp. Child Psych.             innate biases to complex visual concepts. PNAS, 109(44),
   37, 500-511                                                           18215–18220.
                                                                   399

