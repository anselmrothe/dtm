UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Here’s not looking at you, kid! Unaddressed recipients benefit from co-speech gestures
when speech processing suffers
Permalink
https://escholarship.org/uc/item/3t71t1b4
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Holler, Judith
Schubotz, Louise
Kelly, Spencer
et al.
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                     University of California

                                             Here’s not looking at you, kid!
      Unaddressed recipients benefit from co-speech gestures when speech processing
                                                                suffers
                                            Judith Holler (judith.holler@mpi.nl)1,2
                                         Louise Schubotz (louise.schubotz@mpi.nl)1
                                              Spencer Kelly (skelly@colgate.edu)3
                                           Peter Hagoort (peter.hagoort@mpi.nl)1,5
                                       Manuela Schütze (manuela.schuetze@mpi.nl) 1
                                             Aslı Özyürek (asli.ozyurek@mpi.nl)1,4
              1 Max Planck Institute for Psycholinguistics, Wundtlaan 1, 6525XD Nijmegen, The Netherlands
     2 University of Manchester, School of Psychological Sciences, Coupland Building 1, M13 9PL Manchester, UK
 3 Colgate University, Psychology Department, Center for Language and Brain, Oak Drive 13, Hamilton, NY 13346,
                                                                  USA
        4 Radboud University, Centre for Language Studies, Erasmusplein 1, 6525HT Nijmegen, The Netherlands
          5 Radboud University, Donders Institute for Brain, Cognition and Behaviour, Montessorilaan 3, 6525 HR
                                                     Nijmegen, The Netherlands
                              Abstract                                it explores the interplay of these modalities during
                                                                      comprehension in a situated, dynamic social context
  In      human     face-to-face    communication,      language
  comprehension is a multi-modal, situated activity. However,         involving multiple interlocutors in different roles.
  little is known about how we combine information from these              There is, by now, a plethora of empirical evidence
  different modalities, and how perceived communicative               demonstrating that speech and co-speech gestures are
  intentions, often signaled through visual signals, such as eye      semantically integrated during comprehension (e.g., Holle
  gaze, may influence this processing. We address this question       & Gunter, 2007; Holle, Gunter, Rüschemeyer,
  by simulating a triadic communication context in which a            Hennenlotter, & Iacoboni, 2008; Kelly, Özyürek, & Maris,
  speaker alternated her gaze between two different recipients.       2010; Özyürek, Willems, Kita, & Hagoort, 2007; Willems,
  Participants thus viewed speech-only or speech+gesture              Özyürek, & Hagoort, 2007, 2009). However, only two
  object-related utterances when being addressed (direct gaze)
                                                                      recent studies have begun to explore to what extent this
  or unaddressed (averted gaze). Two object images followed
  each message and participants’ task was to choose the object        integration is automatic, and to what extent it is controlled
  that matched the message. Unaddressed recipients responded          and influenced by the pragmatics of communication, such
  significantly slower than addressees for speech-only                as the perceived intentional coupling of gesture and speech
  utterances. However, perceiving the same speech                     (e.g., when observing a gesture performed by one person
  accompanied by gestures sped them up to a level identical to        accompanying speech produced by another) (Kelly, Ward,
  that of addressees. That is, when speech processing suffers due     Creigh, & Bartolotti, 2007; Kelly, Creigh, & Bartolotti,
  to not being addressed, gesture processing remains intact and       2010). The findings suggest that the semantic integration of
  enhances the comprehension of a speaker’s message.                  gesture and speech is indeed sensitive to the intentional
                                                                      coupling of the speech and gesture modalities.
  Keywords: language processing; co-speech iconic gesture; eye
  gaze; recipient status; communicative intent; multi-party                A question that remains is whether this holds when we
  communication.                                                      situate speech and gesture comprehension in a context that
                                                                      is much closer to natural communication, such as in a face-
                                                                      to-face context, where speech and gesture are accompanied
                                                                      by additional nonverbal social cues, such as eye gaze. Due
                          Introduction                                to the saliency of the sclera and the contrast it forms with
   Human face-to-face communication is a multi-modal                  the iris in the human eye, gaze direction is not only
activity and often involves multiple participants. Despite            omnipresent but also an extremely powerful social cue in
this, language comprehension has typically been                       human face-to-face interaction (Senju & Johnson, 2009).
investigated in uni-modal (i.e., just speech) and solitary            While some studies have investigated speech and gesture
(i.e., one passive listener) contexts. The present study              comprehension in the presence of eye gaze, they have
investigates language comprehension in the context of two             typically done so without manipulating eye gaze direction
other modalities omnipresent during face-to-face                      as an independent cue (e.g., Green, Straube, Weis, Jansen,
communication, co-speech gesture and eye gaze. Moreover,              Willmes, Konrad, & Kircher, 2009; Kelly, Kravitz, &
                                                                  2560

Hopkins, 2004; Skipper, Goldin-Meadow, Nusbaum, &                  utterances in the bi-modal condition were always iconic in
Small, 2009; Straube, Green, Jansen, Chatterjee, & Kircher,        nature and depicted a typical feature of the object (such as
2010; Wu & Coulson, 2005, 2007).                                   its function, e.g., a typing gesture). These messages were
     One exception is a recent study by Holler, Kelly,             followed by two object images, one of them having been
Hagoort, and Özyürek (2012). Their study involved one              mentioned in the utterance. The task was simple – speakers
speaker alternating her gaze between two recipients, thus          were asked to indicate as quickly as possible which of the
rendering one of them addressed and the other unaddressed          two images was related to the speaker’s preceding message.
during each message she communicated. Despite this study           This paradigm allows us to test, firstly, how different types
involving multi-modal messages consisting of speech and            of recipients process speech when it is the only modality
gesture, the study was designed to primarily yield insights        carrying semantic information, and, secondly, how they
into the influence of eye gaze direction on the processing of      process semantic messages that are communicated bi-
the gestural component of bi-modal utterances. Thus, while         modally, via speech and co-speech gesture.
showing that addressed and unaddressed recipients process               More specifically, we are also interested in seeing
gestures differently, the findings revealed no effect of eye       whether the findings from our study are in line with the
gaze on the processing of speech. However, as the authors          Competing Modalities Hypothesis proposed by Holler et al.
state themselves, this does not necessarily mean that              (2012). This hypothesis states that unaddressed recipients
addressed and unaddressed recipients do not differ in how          focus more on gesture than do addressed recipients, since
they process speech; one reason being that the paradigm            they are processing information from fewer (visual)
applied in their study required participants to focus attention    modalities overall (i.e., no eye gaze, since the speaker’s
on the verbal modality to make judgements about the                eyes are averted to the other participant). They can therefore
speech they heard. This explicit attentional focus might           devote more cognitive resources to the gestures, and, as a
have masked effects of eye gaze on speech processing that          consequence, they process the gesturally depicted meaning
may be revealed in other contexts.                                 more than addressees. In contrast to Holler et al. (2012),
     There are some studies that provide us with good              whose paradigm was designed to tap primarily into co-
reasons to assume that this is indeed the case. For example,       speech gesture processing, we here test this hypothesis in a
Schober and Clark (1989) showed that overhearers process           paradigm that allows us to measure the processing of both
speech less well than addressees in a referential                  gesture and speech. That is, if, in the present study, we do
communication task. While this study did not involve a             observe an effect of recipient status on the processing of
manipulation of eye gaze direction (nor a face-to-face             speech in a way that is in line with past research (e.g.,
context), it demonstrates that recipient status can have a         Schober & Clark, 1989) - meaning unaddressed recipients
significant impact on how we process language. This                process speech less well - then the enhanced processing of
evidence is complemented by more recent studies that did           co-speech gestures may benefit unaddressed recipients’
investigate speech processing in the context of gaze. For          comprehension of the speaker’s message and compensate
example, Staudte and Crocker (2012) showed that a robot’s          for some (or even all) of the speech processing
eye gaze towards objects in the interlocutors’ environment         disadvantage.
influenced participants’ reference resolution, while                    As an alternative, Holler et al. (2012) proposed the
Knöferle and Kreysa (2012) demonstrated that a person’s            Fuzzy Representation Hypothesis. This hypothesis predicts
eye gaze towards objects influences how participants               that unaddressed recipients perceive gestures as being less
process speech with respect to thematic role assignment and        intended for them than for the gazed at recipient. They
syntax.                                                            therefore process gestures less clearly than addressees, and,
     Based on this earlier research, we predict that social        as a consequence, end up with a fragmented, or fuzzy,
eye gaze, indicating communicative intent and recipient            representation of the gesturally depicted meaning. If the
status in conversation, also influences the processing of          Fuzzy Representation Hypothesis is true, then we should
speech. Thus, the present study investigates how, in a multi-      see no benefitting effect of gestures on the processing of
party setting, different types of recipients (as signaled          speech. Rather, unaddressed recipients might be slowed
through a speaker’s eye gaze direction) process speech, and        down even more when trying to process bi-modal
speech accompanied by gestures. To do so, we developed a           utterances, since not only the speech poses difficulties for
visually focused paradigm that avoids explicit attention to        them, but also the gestures.
speech to allow us to better observe potential differences in           The present study aims to tease apart which of these
addressed and unaddressed recipients’ processing of both           two hypotheses may best explain how addressed and
uni-modal speech-only and bi-modal speech-gesture                  unaddressed recipients (as indicated by the speaker’s eye
utterances.                                                        gaze direction) comprehend multi-modal language in a
     Like Holler et al. (2012), we implement our task in a         pragmatically much richer communication context than has
situated, triadic communicative setting. However, in our           been traditionally investigated, that is, in a context that
task, participants watched a speaker conveying speech-only         bears somewhat more resemblance to the kind of joint
or speech + gesture utterances referring to objects (e.g., ‘he     activity that human communication is (Clark, 1996).
prefers the laptop’). The gestures accompanying these
                                                               2561

                           Method                                   160 stimulus sentences (e.g., a picture of a laptop), and an
                                                                    additional 160 pictures were selected to serve as unrelated
Participants                                                        pictures, such that the ‘laptop’ would be presented
   32 right-handed, native German speakers (16 female)              alongside a ‘towel’, for example (Fig. 2). Object pictures
participated in the experiment (mean age 24.5yrs).                  were searched via Google Images and further edited in
                                                                    Adobe Photoshop to have all objects presented in the same
                                                                    quality and size on a white background.
Design                                                                 Prior to testing, all 320 pictures were judged by two raters
   We used a 2x2 within-participants factorial design,              (female native German speakers who did not participate in
manipulating the gaze direction of the speaker (direct              the main experiment) for their ease of identification.
gaze/addressed       recipient    condition      vs.     averted
gaze/unaddressed recipient condition) as well as the
modality of presentation (speech-only vs. speech+gesture).
Materials and Apparatus
Video clips 160 short sentences of a canonical SVO
structure were constructed. Sentences always referred to an
object combined with a non-action verb (see below for
more detail), e.g. ‘he prefers the laptop’ (‘er bevorzugt den
Laptop’). The iconic gestures accompanying the sentences
always referred to the object that was mentioned in speech                    Figure 2: Example of a pair of object pictures.
and provided information about its shape, function, or size
(see Fig.1, for a gesture depicting the act of typing).                Each participant saw each of the 160 video clips in one of
   In order to guarantee that the gestures unambiguously            the four conditions exactly once, resulting in 160
referred to the objects mentioned, verbs were carefully             experimental trials per participant (40 trials per condition),
selected to be as neutral as possible and were never action         plus 24 filler trials, yielding 184 trials overall. To avoid
verbs. Hence, rather than more commonplace constructions            confounding effects of the order in which the pictures were
like ‘he types on the laptop’ where the typing gesture could        presented on the screen, this order was counterbalanced.
refer to both ‘typing’ and ‘laptop’, verbs like ‘prefer’               Videos and object pictures were presented on a 15”
(‘bevorzugen’), ‘like’ (‘mögen’), or ‘see’ (‘sehen’) were           laptop       screen       using      Presentation     software
used in the sentences. Our manipulation of both gaze                (http://www.neurobs.com). The audio signal of the videos
direction and modality of presentation required each                was presented via high quality Sennheiser headphones.
sentence to be recorded in four versions: 1. direct gaze
(addressed) speech-only, 2. direct gaze (addressed)                 Procedure
speech+gesture, 3. averted gaze (unaddressed) speech-only,             Participants were tested individually. At the beginning of
and 4. averted gaze (unaddressed) speech+gesture (Fig. 1).          each testing session, participants were familiarised with the
                                                                    experimental set-up and the course of the experiment, and
                                                                    were seated in front of the experiment laptop where they
                                                                    received their instructions.
                                                                       Participants were told that they would see a number of
                                                                    pre-recorded video clips of a speaker (in fact a confederate)
                                                                    who, they were told, spontaneously formed short sentences
                                                                    based on line drawings and single words displayed on a
                                                                    screen not displayed in the video shot. They were also told
                                                                    that during the recordings, a second person was present in
                                                                    the room, sitting diagonally across from the speaker. The
                                                                    speaker was supposedly instructed to sometimes address
                                                                    this other (fictitious) participant when producing her
                                                                    utterances (averted gaze condition), and to sometimes
                                                                    address the (actual) participant via a video camera
                                                                    positioned straight across from her (direct gaze condition).
                                                                    Participants were instructed that following each video clip,
    Figure 1: Four different versions of the ‘laptop’ stimulus.     they would see two pictures of objects on the screen, and
   AR = addressed recipient, UR = unaddressed recipient.            that it was their task to indicate via button press which of
                                                                    the two pictures best matched the speaker’s message (left
Object pictures We created a total of 320 object pictures.          button for the left-hand picture, right button for the right-
160 of these were pictures of the objects mentioned in the          hand picture). They were asked to react as quickly and as
                                                                2562

accurately as possible. Reaction times of participants’                recipients did not differ in their processing               of
left/right responses were recorded via a button box, as were           speech+gesture utterances, t(1,31) = 1.112, p = .275.
response accuracies.
   In order to ensure that participants were actually
watching the video clips and not basing their decision on
the spoken part of the message only, they were explicitly
asked to look at the screen during the entire course of the
experiment. This was further enforced by the presence of a
surveillance camera (our checks showed that no participant
had looked away), which all participants agreed to be video-
recorded with during the experiment.
   Before the beginning of the experiment proper,
participants completed a total of six practice trials. As in the
actual experiment, each trial consisted of a video clip,
followed immediately by the two object pictures, which
stayed onscreen until the participants pressed a button.
After their response, participants saw a fixation cross for a
random time interval between 2 and 5 seconds before the
next trial started.
                             Results                                        Figure 3: Addressed recipients’ (AR) and unaddressed
                                                                        recipients’ (UR) reaction times (ms) in the speech-only and
   A total of six trials from two participants were excluded
                                                                            speech+gesture conditions (error bars represent SE).
from the analysis beforehand because of a technical error.
An alpha value of .05 was used throughout our statistical
analyses. All p-values reported are two-tailed.
   For the analysis of the reaction times1, we excluded all
incorrect responses, 83 in total (= 1.62% of all trials). Also                                 Discussion
excluded from the analysis were responses more than 2.5                   This study investigated multi-modal language processing
SD above or below each subject’s mean reaction time (this              in a situated, socially dynamic communication setting
resulted in 118 responses being excluded: 40 in the speech-            involving multiple parties. The specific question we tried to
only condition, direct gaze, 31 in the speech-only condition,          answer is how different types of recipients, as signaled
averted gaze, 23 in the speech+gesture condition, frontal              through a speaker’s eye gaze, process speech and speech
gaze, and 24 in the speech+ gesture condition, averted                 accompanied by iconic gestures, in a triadic communication
gaze).                                                                 scenario. The findings revealed a significant interaction
   Figure 3 shows the reaction time data for the 2 (gaze               between modality and recipient status. More precisely, they
direction: direct vs. averted) x 2 (modality of presentation:          show, first and foremost, that the processing of speech-only
speech-only vs. speech+gesture) repeated measures                      utterances is indeed affected by recipient status in our task,
ANOVA. The results yielded a significant interaction,                  since unaddressed recipients were significantly slower in
F(1,31) = 5.947, p = .021. The main effect of modality was             this condition than were addressed recipients. Crucially,
not significant, F(1,31) = 3.431, p = .074, and neither was            addressed and unaddressed recipients did not differ in their
the main effect of gaze, F(1,31) = .464, p = .501.                     processing of speech+gesture utterances. That is,
   In line with our hypotheses, we calculated two a priori             unaddressed recipients significantly benefitted from the
contrasts (using paired-samples t-tests), comparing                    information depicted in the gestural modality, allowing
addressed and unaddressed recipients’ processing of uni-               them to perform at the same level as addressees when
modal speech-only utterances, as well as their processing of           perceiving bi-modal rather than uni-modal utterances.
the bi-modal speech+gesture utterances. The first                         The findings are thus very much in line with the
comparison showed that unaddressed recipients (M =                     Competing Modalities Hypothesis (Holler et al., 2012).
542ms) were significantly slower than addressees (M =                  Unaddressed recipients appear to focus their cognitive
530ms) at processing speech-only utterances, t(1,31) =                 resources on the processing of co-speech iconic gestures. At
2.547, p = .016. The second comparison, however, showed                the same time, the findings allow us to further refine this
that unaddressed (M = 525ms) and addressed (M = 531ms)                 hypothesis; because we found that unaddressed recipients
                                                                       do not process speech more quickly than addressed
   1
                                                                       recipients, the competition effect seems to apply to the
     The analysis of participants’ error rates yielded a significant   visual modalities (gesture and gaze) only. In other words,
modality effect, with both types of recipients being more accurate
                                                                       due to not having to process eye gaze, unaddressed
in the bi-modal than in the uni-modal condition. No other effects
were significant.                                                      recipients can focus more on gesture and, as a consequence
                                                                   2563

process this information more. Their increased processing                             Acknowledgments
capacity due to the absence of direct gaze does not,
                                                                     We would like to thank the University of Rostock for
however, affect their processing of speech-only utterances.
                                                                  their provision of testing space, the Radboud University and
   The reason as to why, in contrast to Holler et al. (2012),
                                                                  MaxNet initiative for financial support, the Neurobiology of
we found a numerical but no reliable difference between
                                                                  Language Department and the Gesture & Sign Research
addressed and unaddressed recipients’ processing of bi-
                                                                  Group at the Max Planck Institute for Psycholinguistics for
modal utterances (i.e., unaddressed recipients were slightly
                                                                  helpful feedback during discussions of this study, and the
faster in the bi-modal condition than addressed recipients
                                                                  European Commission for funding this research (JH was
were, but not significantly so) is likely to be due to our
                                                                  supported through a Marie Curie Fellowship #255569, and
change in paradigm. As argued in the Introduction, the
                                                                  through ERC Advanced Grant #269484INTERACT).
explicit attentional focus on the verbal modality in Holler et
al.’s (2012) study might have masked differences in the
processing of speech – an assumption that we were able to
corroborate here. In the present study, we purposefully
                                                                                           References
shifted participants’ attention towards the visual modality       Clark, H. (1996). Using language. Cambridge: Cambridge
(by asking them to identify pictures) in order to uncover           University Press.
potentially previously masked differences in speech               Green, A., Straube, B., Weis, S., Jansen, A., Willmes K.,
processing, while being aware that this shift in paradigm           Konrad, K., & Kircher, T. (2009). Neural integration of
might, in turn, reduce differences in the processing of visual      iconic and unrelated coverbal gestures: a functional MRI
(i.e., gestural) information between addressed and                  study. Human Brain Mapping, 30, 3309–3324.
unaddressed recipients. The present study thus                    Holle, H. & Gunter, T. C. (2007). The role of iconic
complements that by Holler et al. (2012) nicely. Together,          gestures in speech disambiguation: ERP evidence.
they offer us a more comprehensive insight into how                 Journal of Cognitive Neuroscience, 19, 1175-1192.
different recipients process uni-modal and bi-modal               Holle, H., Gunter, T. C., Rüschemeyer, S.A., Hennenlotter,
utterances in the presence of eye gaze.                             A., & Iacoboni, M. (2008). Neural correlates of the
   What remains to be investigated are the exact cognitive          processing of co-speech gestures. NeuroImage, 39, 2010-
mechanisms underlying our Competing Modalities account.             2024.
Currently, we are unable to determine whether the iconic          Holler, J., Kelly, S., Hagoort, P., & Özyürek, A. (2012).
co-speech gestures benefit unaddressed recipients’                  When gestures catch the eye: The influence of gaze
processing of speech because they are semantically                  direction on co-speech gesture comprehension in triadic
integrated with the verbal information - thus leading to a          communication. In N. Miyake, D. Peebles, & R. P.
richer, unified mental representation of the concept of             Cooper (Eds.), Proceedings of the 34th Annual Meeting of
‘laptop’, for example – or whether they lead to a stronger          the Cognitive Science Society (pp. 467-472). Austin, TX:
memory trace due to receiving related information from two          Cognitive Society.
different input streams (visual and verbal), with this            Kelly, S. D., Creigh, P., & Bartolotti, J. (2010). Integrating
information being associated but stored separately and not          speech and iconic gestures in a Stroop-like task: Evidence
as a unified representation (much like a dually-coded               for automatic processing. Journal of Cognitive Neuro-
representation à la Paivio (1986)). Future studies, preferably      science, 22, 683-694.
involving on-line measures suitable for dipping directly into     Kelly, S. D., Kravitz, C., & Hopkins, M. (2004). Neural
semantic integration processes, are needed to answer this           correlates of bimodal speech and gesture comprehension.
question.                                                           Brain & Language, 89, 253-260.
   In conclusion, the present study has brought together          Kelly, S. D., Özyürek, A., & Maris, E. (2010). Two sides of
three different modalities in a language processing                 the same coin: Speech and gesture mutually interact to
paradigm, and it advances our understanding of how                  enhance comprehension. Psychological Science, 21, 260-
perceived communicative intent, as signaled through a               267.
speaker’s eye gaze, influences the interplay of these             Kelly, S. D., Ward, S., Creigh, P., & Bartolotti, J. (2007).
modalities during comprehension in a situated, face-to-face-        An intentional stance modulates the integration of gesture
like (rather than solitary) setting. The findings are striking      and speech during comprehension. Brain and Language,
since we have shown that the ostensive cue of eye gaze has          101, 222-233.
the power to modulate how different recipients process            Knöferle, P. & Kreysa, H. (2012). Can speaker gaze
semantic information carried by two concurrent modalities,          modulate syntactic structuring and thematic role
speech and co-speech gestures. Moreover, we have shown              assignment during spoken sentence comprehension?
that in situated face-to-face settings involving multiple           Frontiers in Cognitive Science, 3, 538.
recipients, the gestural modality can benefit unaddressed         Özyürek, A., Willems, R. M., Kita, S., & Hagoort, P.
recipients – when speech processing suffers, gestures help.         (2007). On-line integration of semantic information from
                                                                    speech and gesture: Insights from event-related brain
                                                                    potentials. Journal of Cognitive Neuroscience, 19, 605-
                                                              2564

  616.
Paivio, A (1986). Mental representations: a dual coding
  approach. Oxford, UK: Oxford University Press.
Schober, M. F. & Clark, H. H. (1989). Understanding by
  addressees and overhearers. Cognitive Psychology, 21,
  211-232.
Senju, A. & Johnson, M. H. (2009). The eye contact effect:
  Mechanisms and development. Trends in Cognitive
  Sciences, 13, 127-134.
Skipper, J. I., Goldin-Meadow, S., Nusbaum, H. C., &
  Small, S. L. (2009). Gestures orchestrate brain networks
  for language understanding. Current Biology, 19, 661-
  667.
Staudte, M. & Crocker, M. (2011). Investigating Joint
  Attention Mechanisms through Spoken Human-Robot
  Interaction. Cognition, 120, 268-291.
Straube, B., Green, A., Jansen, A., Chatterjee, A., &
  Kircher, T. (2010). Social cues, mentalizing and the
  neural processing of speech accompanied by gestures.
  Neuropsychologia, 48, 382-393.
Willems, R. M., Özyürek, A., & Hagoort, P. (2007). When
  language meets action: The neural integration of speech
  and gesture. Cerebral Cortex, 17, 2322-2333.
Willems, R. M., Özyürek, A., & Hagoort, P. (2009).
  Differential roles for left inferior frontal and superior
  temporal cortex in multimodal integration of action and
  language. NeuroImage, 47, 1992-2004.
Wu, Y.C. & Coulson, S. (2005). Meaningful gestures:
  Electrophysiological     indices   of    iconic   gesture
  comprehension. Psychophysiology, 42, 654-667.
Wu, Y. C. & Coulson, S. (2007). How iconic gestures
  enhance communication: An ERP study. Brain and
  Language, 101, 234-245.
                                                           2565

