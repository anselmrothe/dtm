UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Blending and choosing within one mind: Should judgments be based on exemplars, rules or
both?
Permalink
https://escholarship.org/uc/item/1v83p74s
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Herzog, Stefan
Von Helversen, Bettina
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                    Blending and Choosing Within One Mind:
                     Should Judgments Be Based on Exemplars, Rules, or Both?
                                       Stefan M. Herzog (herzog@mpib-berlin.mpg.de)
                  Center for Adaptive Rationality, Max Planck Institute for Human Development, Lentzeallee 94
                                                         14195 Berlin, Germany
                                  Bettina von Helversen (bettina.vonhelversen@unibas.ch)
                                 Department of Psychology, University of Basel, Missionsstrasse 62a
                                                         4055 Basel, Switzerland
                             Abstract                                    To our knowledge, however, research in cognitive science
   Accurate judgments and decisions are crucial for success in
                                                                       and judgment and decision making has not previously
   many areas of human life. The accuracy of a judgment or             investigated what kind of interaction between exemplar- and
   decision depends largely on the cognitive process applied. In       rule-based processes leads to accurate judgments, decisions,
   research on judgment, decision making, and categorization,          and categorizations: relying on just one of the two processes
   two kinds of cognitive processes have often been contrasted:        or using both? If both are considered, is it better to choose
   exemplar-based processes, which use similarity to previously        between them depending on the structure of the task, for
   encountered items to make judgments, decisions, and                 instance (Rieskamp & Otto, 2006), or to blend them into a
   categorizations, and rule-based processes, which use
   abstracted cue knowledge. Although most cognitive models            joint response? This paper presents first answers to these
   of judgment and decision processes assume that people rely          questions.
   on both processes, they differ in whether they assume that one        A functional perspective on the interaction between
   process is selected or that both processes are blended into a       exemplar- and rule based processes may be useful for at
   single response. The present research takes a functional            least three reasons. First, examining cognitive models’
   perspective and investigates what kind of interaction between       ability to predict external real-world criteria goes a step
   the two processes leads to accurate responses. Based on cross-
                                                                       further than comparing their ability to describe human
   validated simulations in real-world domains, it shows that
   blending rule- and exemplar-based processes generally leads         behavior in idealized laboratory tasks, by adding a further
   to better judgments than does choosing between them,                evaluation criterion. If one class of cognitive models were
   suggesting that the default strategy should be a blend of both      superior to another in terms of predictive performance, this
   processes, which is abandoned only when feedback justifies          would make them more attractive as plausible models of
   it.                                                                 human behavior (Chater & Oaksford, 1999). Second, many
   Keywords: accuracy; multiple-cue judgments; decision                cognitive models are inspired by or share similarities with
   making; categorization; exemplar models; rules; cognitive           models from research fields interested in predictive
   models; mixtures of experts; simulation.                            performance (such as statistics, artificial intelligence,
                                                                       computer science, and machine learning; see e.g., Jäkel,
                         Introduction                                  Schölkopf, & Wichmann, 2009; Marling, Sqalli, Rissland,
Judging quantities, making decisions, and categorizing                 Munoz-Avila, & Aha, 2002), and a functional perspective
items are crucial elements of successful human behavior. A             provides a common ground that serves to re-connect
vast and diverse literature in cognitive science and judgment          cognitive models with such fields. Third, knowledge of how
and decision making has investigated how people achieve                to profit from the complementary strengths of the two
these tasks (e.g., Ashby & Maddox, 2005; Gigerenzer,                   processes could offer prescriptions for improving human
Hertwig, & Pachur, 2011; Kruschke, 2008; Payne, Bettman,               judgment, decision making, and categorization by
& Johnson, 1993). The many different models and strategies             instructing decision makers on when and how to use the two
proposed can be broadly classified into two categories with            processes.
reference to the cognitive processes they assume: exemplar-
based processes, which use similarity to previously                       Models of Judgment, Decision Making, and
encountered items to make judgments, decisions, and                                         Categorization
categorizations, and rule-based processes, which use                   There are two general approaches to modeling human
abstracted cue knowledge (Hahn & Chater, 1998).                        cognition. First, single general-purpose models have been
   Extensive research has compared the proposed models’                proposed (e.g., Lee & Cummins, 2004). For instance,
ability to describe human behavior. Furthermore, the                   judgment and categorization models assume either only
performance of judgment and decision making strategies in              exemplar-based (e.g., Juslin & Persson, 2002; Kruschke,
predicting real-world criteria has been thoroughly                     1992) or only rule-based processes (e.g., Ashby & Gott,
investigated (e.g., Gigerenzer et al., 2011; Todd, Gigerenzer,         1988; Brehmer, 1994). Second, toolbox approaches have
& the ABC Research Group, 2012).                                       been proposed. These assume that people draw on multiple,
                                                                  2536

different processes to solve the same task (e.g., Gigerenzer       (“competition”)      or      blending     different   sources
& Selten, 2001). The toolbox approach posits that people           (“cooperation”) may lead to better performance.
adaptively select a tool (i.e., strategy) likely to succeed in       On the one hand, choosing a specific strategy allows the
the task at hand from a repertoire of strategies: the              overall decision process to be adapted to environmental
“toolbox” (Gigerenzer & Selten, 2001; Payne et al., 1993;          regularities and thus facilitates good performance (e.g.,
Rieskamp & Otto, 2006; Scheibehenne, Rieskamp, &                   Todd et al., 2012). On the other hand, “blending” (i.e.,
Wagenmakers, 2013). Toolbox approaches have gained                 averaging) different sources can often improve accuracy
popularity particularly in decision making (e.g., Gigerenzer       because errors of different signs cancel each other out. This
& Selten, 2001; Rieskamp & Otto, 2006). Yet also in                “wisdom of crowds” phenomenon (Surowiecki, 2004) has
categorization and judgment research, it is frequently             recently also been applied to individual minds (e.g., Herzog
assumed that people chose the process that is better suited to     & Hertwig, 2009, 2013; Vul & Pashler, 2008). Combining
solving a task (Ashby, Alfonso-Reese, Turken, & Waldron,           exemplar- and rule-based processes can be seen as an
1998; Juslin, Karlsson, & Olsson, 2008; Nosofsky, Palmeri,         implicit “crowd within,” where the two processes constitute
& McKinley, 1994; von Helversen & Rieskamp, 2008). For             two “experts” in one mind that either compete or cooperate
example, COVIS assumes that similarity-based and rule-             in giving a response. To the extent that exemplar- and rule-
based processes “race” for an answer, with the faster one          based processes complement each other in the errors they
determining the response (Ashby et al., 1998).                     commit, combining them may be a successful strategy
   Although toolbox approaches often assume competition            (Herzog & von Helversen, 2013).
between processes, it is also possible that the processes            In the following simulation study, we compare the merits
cooperate. Hybrid or blending models assume that, instead          of single purpose models, a competitive toolbox approach,
of “choosing” a process for a task, two or more processes          and a cooperative toolbox approach. We focus on exemplar-
are executed simultaneously and their responses are                based and rule-based processes as examples of distinctive
integrated. For instance, the categorization model ATRIUM          cognitive processes because of the prominent distinction
(Erickson & Kruschke, 1998) combines both exemplar- and            between the two in the cognitive literature (Ashby et al.,
rule-based processes. Inspired by the “mixtures-of-experts”        1998; Hahn & Chater, 1998; Nosofsky et al., 1994; Persson
approach from machine learning (Jacobs, Jordan, Nowlan,            & Rieskamp, 2009).
& Hinton, 1991), ATRIUM assumes that people have two
“experts” in their mind: an exemplar-based and a rule-based          Different Levels of Interaction: Task or Item
one, whose outputs are processed by a gating mechanism.            Besides differentiating between choosing (competition) and
This gating mechanism can “choose” between these                   blending (cooperation) of cognitive processes, we also
modules or “blend” their outputs by averaging their                consider on which level the interaction takes place: the task
responses. In addition, ATRIUM can learn to rely more              or item level. In the ecological rationality and adaptive
strongly on the more successful module (in terms of the            toolbox approach (Todd et al., 2012), it is (implicitly)
probability of choosing or weighted averaging)—either for          assumed that the selection of strategies happens on the task
the whole task or depending on the item presented (i.e.,           level—that is, that all the decisions within the same task are
depending on its location in psychological space). Modeling        solved using the same strategy (once learning has
and experimental investigations support ATRIUM’s                   completed). However, strategy selection (or integration) can
assumption that exemplar- and rule-based processes                 also happen on the item level—that is, some items may be
simultaneously influence how humans categorize (e.g.,              better solved by a rule, whereas others require memorization
Erickson & Kruschke, 1998; Hahn, Prat-Sala, Pothos, &              (Nosofsky et al., 1994). To account for this level of
Brumby, 2010). There is also evidence for such                     interaction, we compared competition and cooperation on
simultaneous influence in the domain of multiple-cue               the task and the item level.
judgments (von Helversen, Herzog, & Rieskamp, in press).
                                                                   Simulation Study: Should Judgments Be Based
    Blending and Choosing Within One Mind
                                                                               on Exemplars, Rules or Both?
The combination of judgments or decisions from different
sources is a vibrant topic in research fields such as                We investigated the performance of different ways to use
psychology, judgment and decision making, cognitive                exemplar- and rule-based processes in predicting a
science, statistics, artificial intelligence (AI), machine         continuous criterion based on multiple cues. To this end, we
learning, biology, and economics (e.g., Krause, Ruxton, &          conducted cross-validated simulations, informed by
Krause, 2010; Kuncheva, 2004; Larrick, Mannes, & Soll,             ATRIUM’s (Erickson & Kruschke, 1998) cognitive
2012; Lee, Zhang, & Shi, 2011; Marling et al., 2002).              architecture, in five real-world domains. We addressed the
Combining diverse sources (e.g., forecasts from different          following three questions. First, is it better to be equipped
experts) generally improves accuracy because different             with both exemplar- and rule-based processes or is one
sources often compensate for each other’s shortcomings.            process enough to achieve accurate judgments? Second, if
Depending on the circumstances, either choosing between            both processes are used, is it better to choose between them
                                                               2537

                 Table 1: Characteristics of the real-world datasets (adapted from Table 1 in Dana & Dawes, 2004).
 N = number of cases, k = number of cues, ρ = correlation between target variable and predicted values from a multiple linear
     regression, v Vector = zero-order correlation between target variable and cues, ∅rxixj = mean correlation among cues.
                          Dataset        N       k       ρ                    v Vector                   ∅rxixj
                          Abalone      4,177      7     .73  .63 .58 .56 .56 .54 .50 .42                   .89
                          NFL          3,057    10      .54  .46 .43 .37 .34 .33 .27 .21 .07 .05 .05       .21
                          ABC            955      5     .35  .32 .20 .06 .04 .02                           .08
                          NES          1,910      6     .35  .26 .17 .15 .15 .13 .12                       .11
                          WLS          6,385      5     .20  .13 .11 .10 .10 .10                           .15
(competition) or to blend them (cooperation)? Third, for               estimated parameter values to make predictions for the
either choosing between or blending the two processes, is it           items in the test sample (for six different strategies
better to treat all items the same (i.e., integration on the task      described below). We measured estimation accuracy in the
level) or to treat individual items differently (i.e., integration     test sample using the RMSE between the model’s
on the item level)? Item-level integration implies choosing            predictions and the criterion values, a commonly used
between the processes for each item (in the competitive                measure of absolute goodness of fit. Seven different sizes of
approach) or weighting the two processes differently for               learning samples were used (20, 40, 60, 80, 100, 200, and
each item when blending (in the cooperative approach).                 500 items) to vary the amount of experience with a domain;
                                                                       all test samples consisted of 250 items. For each dataset and
Datasets                                                               each of the sizes of learning samples, we ran the simulation
We analyzed datasets previously used to compare the                    1,000 times and averaged the results.
performance of proper and improper linear models (Dana &
Dawes, 2004). The datasets pertain to five domains:                    Using the Rule and Exemplar Models
biology, sports, public opinion, political sentiment, and              We tested six strategies for using rule- and exemplar-based
occupational prestige. In all datasets, a continuous target            processes to make predictions for the test sample.
variable was predicted by several cues. For instance, the
ABC dataset was derived from a 2002 poll of 955 U.S.                   “Exemplar Model” and “Rule Model” The first two
households. Respondents’ confidence that Osama bin Laden               strategies used just one of the two processes exclusively.
would be captured or killed was predicted by five cues,
including the respondent’s age, education, gender, and                 “Choosing-Task” and “Choosing-Item” The third and
patriotism. See Table 1 for details of the statistical structure.      fourth strategy chose either the exemplar or the rule model.
                                                                          On the task level, “choosing-task” selected in each
Cognitive Models                                                       simulation run the model that was superior in the learning
Exemplar Model To represent an exemplar-based judgment                 sample and used it for all items in the test sample. To
process, we used an exemplar model for multiple-cue                    account for differences in model complexity, we used the
judgments (Juslin et al., 2008). The model assumes that                Bayesian Information Criterion as a selection criterion.
judgments are based on the similarity to exemplars stored in              On the item level, “choosing-item” selected in each
memory, where the judgment is an average of the criterion              simulation run and for each item in the test sample the
values of the stored exemplars weighted by their similarity            model that was more likely to be superior for this particular
to the target item. We used a simplified exemplar model                test item—based on the performance on similar items in the
with one single free parameter determining the similarity              learning sample. Specifically, for each test item we
gradient (see von Helversen & Rieskamp, 2008).                         calculated the RMSE that the exemplar and the rule model
                                                                       had on similar items in the learning sample (i.e., we
Rule Model To represent a rule-based process, we used a                weighted the RMSE values of each training item using the
multiple linear regression model. Such models have been                similarity gradient of the exemplar model). The process with
widely used to model human judgment (Brehmer, 1994);                   the lower weighted RMSE was then selected and its
they assume that judgments can be understood as the sum of             prediction for this test item was used.
weighted cue values. The model has a free parameter for
every cue plus an intercept.                                           “Blending-Average” and “Blending-Item” The fifth and
                                                                       sixth strategy blended the outputs of the exemplar and the
Simulation Setup                                                       rule model to make a joint prediction.
                                                                          On the task level, “blending-average” computed for each
For each simulation run, we randomly drew a learning
                                                                       test item the arithmetic mean of the predictions of the rule
sample and a test sample. We then fitted the free parameters
                                                                       and the exemplar model.
of the exemplar and the rule model to the learning sample—
                                                                          On the item level, “blending-item” used in each
minimizing the root mean square error (RMSE) between
                                                                       simulation run and for each item in the test sample a
model predictions and criterion values—and used the
                                                                   2538

                                     Averaged Across Domains                              'Abalone'                     National Football League ('NFL')
                             1.00                                     3.2
                                                   Rule model
                                                   Exemplar model                                                 1.2
                             0.95                  Choosing-task      3.0
                                                   Choosing-item
                             0.90                  Blending-average   2.8                                         1.1
                                                   Blending-item
                             0.85                                     2.6
   Estimation Error (RMSE)
                                                                                                                  1.0
                             0.80                                     2.4
                                                                                                                  0.9
                                                                      2.2
                                     20   40     100    200     500           20    40       100      200   500           20   40     100   200     500
                                    Opinion Poll Bin Laden ('ABC')           Rating Republican Party ('NES')             Occupational Prestige ('WLS')
                             1.10                                                                                 27
                                                                      24
                             1.05                                                                                 26
                                                                      23
                                                                                                                  25
                             1.00                                     22
                                                                                                                  24
                             0.95                                     21
                                                                                                                  23
                                                                      20
                             0.90
                                     20   40     100    200     500           20    40       100      200   500           20   40     100   200     500
                                                                            Size of Learning Sample
    Figure 1: Cross-validated estimation accuracy (Root Mean Squared Error, RMSE) of six strategies in five domains (for
  learning samples of different sizes). The upper left panel averages the normalized data across domains; the RMSE values
          were divided by the largest average RMSE value in each domain. The strategies are explained in the text.
weighted average of both models’ predictions—using the                                   rule model; the exemplar model was somewhat better than
same similarity-weighted RMSEs as in “choosing-item.”                                    the averaged predictions of both models only for very small
The item-specific weight for the exemplar model was                                      learning samples (i.e., 20 items). Second, “blending-
calculated as the proportion of the rule model’s weighted                                average” was generally more accurate than choosing the
RMSE relative to the sum of both models’ weighted RMSEs                                  better model based on its performance in the respective
(i.e., the worse the rule model, the larger the weight on the                            learning sample (“choosing-task”), although choosing was
exemplar model).                                                                         slightly better for very small learning samples (i.e., 20
                                                                                         items) in two of the five datasets. Third, when choosing or
Results & Discussion                                                                     blending, it did not pay off to tune one’s use of the models
Figure 1 shows the generalization performance of the                                     to the type of item. Weighting both processes when
different strategies as a function of the size of the learning                           blending (“blending-item”) was less or equally accurate than
sample for the five domains. Because the datasets differed                               was giving them equal weights (“blending-average”);
in their range of criterion values, which in turn affected the                           similarly, choosing the process depending on the item
scale of the RMSE, it was necessary to normalize the                                     (“choosing-item”) was less or equally accurate than was
RMSEs before aggregating them across datasets. To this                                   using the same process for all items (“choosing-task”).
end, we divided each RMSE by the largest average RMSE                                    Fourth, the differences between strategies decreased as the
value within the respective domain, so that each RMSE                                    size of the learning samples increased.
value could be understood as the relative increase in fit. We                               Let us now answer the three questions motivating this
then constructed a summary learning curve by averaging the                               simulation. First, in the datasets we investigated, it was
normalized RMSEs across the five domains (see Figure 1,                                  generally better to be equipped with both exemplar- and
upper left panel).                                                                       rule-based processes than with just one of the two processes.
  Four results are noteworthy. First, “blending-average”                                 Second, if both processes were used, it was generally better
was generally more accurate than either the exemplar or the                              to blend them than to choose between them. Third, when
                                                                                   2539

choosing between or blending the two processes, it was            processes make similar errors, and this statistical structure
generally better to treat all items the same (and not to          can be ascertained with enough confidence (see Soll &
choose or blend, respectively, depending on the type of           Larrick, 2009). However, we would argue that this is
item; i.e., depending on how much “expertise” the                 typically not the case in real-world domains. It would thus
exemplar- and rule-based processes had about a specific part      seem prudent that human judges and decision makers, as
of the psychological space).                                      modeled, for example, by ATRIUM (Erickson & Kruschke,
                                                                  1998), start with a simple blend of both processes and
                   General Discussion                             deviate from this approach (e.g., by choosing or item-
Many cognitive models of judgment, decision-making, and           specific tuning) only when feedback justifies it.
categorization assume that people can use both exemplar-             Why is combining exemplar- and rule-based processes so
and rule-based processes (e.g., Erickson & Kruschke, 1998).       successful in multiple-cue judgment tasks? The use and the
Yet it remained unclear whether using both processes              performance of exemplar- and rule-based processes in
provides a performance advantage over using just one              multiple-cue judgment tasks seems to depend on the
process and, when both processes are available, whether it is     statistical structure of the task—in particular, the functional
better to choose one process depending on the task (i.e.,         relation between cues and criteria (Juslin et al., 2008; von
competitive toolbox approach) or to blend their responses         Helversen & Rieskamp, 2008). If the criterion can be
(i.e., cooperative toolbox approach). Our simulations in the      approximated by a linear additive combination of the cues,
domain of multiple-cue judgments suggest that combining           rule-based processes predominate. In multiplicative tasks,
the two processes (either by choosing between or blending         by contrast, exemplar-based processes perform better and
them) leads to better judgments than does relying on just         are used more frequently. Simulations using artificially
one of them, and that a simple blend (i.e., equal weighting)      created domains (Herzog & von Helversen, 2013) suggest
of both processes leads to accurate judgments. This latter        that the five real-world domains we analyzed in the present
point is consistent with the success of naïve equal weighting     simulations represent a mixture of these two kinds of
strategies (e.g., Dawes, 1979). In another set of simulations,    statistical structures (i.e., additive and multiplicative).
we investigated the combination (i.e., choosing or blending)      Consequently, neither of the two processes in isolation was
of exemplar- and rule-based processes in the context of           able to capture their statistical structure. To the extent that
making categorizations (using 38 machine learning                 this result generalizes to decision making and
benchmark datasets; Herzog & von Helversen, 2013).                categorization, it suggests one reason why people are
Further broadening the scope of the present analysis, we          equipped with and use both exemplar- and rule-based
found that blending the outputs of an exemplar- and a rule-       processes: because only a combination of the two allows
based process led to successful categorizations.                  people to make successful judgments, decisions, and
   Our results resonate with research in AI and machine           categorizations in the real world.
learning that demonstrates how combining different
representations is often beneficial (Kuncheva, 2004;                                   Acknowledgments
Marling et al., 2002). More specifically, our results             We thank the Swiss National Science Foundation for a grant
suggesting that combining exemplar- and rule-based                to the first author (100014_129572/1).
processes can often increase accuracy in human cognition
dovetail nicely with the successful combination of case-                                    References
based and rule-based reasoning systems in AI (e.g., Marling       Ashby, F. G., Alfonso-Reese, L. A., Turken, A. U., &
et al., 2002; Prentzas & Hatzilygeroudis, 2007).                        Waldron, E. (1998). A neuropsychological theory of
   Besides the general question of whether exemplar- and                multiple systems in category learning, 105, 442–481.
rule-based processes should be “blended” or “chosen”              Ashby, F. G., & Gott, R. E. (1988). Decision rules in the
among, our simulations suggest that it does not pay off to              perception and categorization of multidimensional
tune one’s use of exemplar- and rule-based processes to the             stimuli. Journal of Experimental Psychology: Learning,
type of item one wants to generalize to. This conclusion                Memory, and Cognition, 14, 33–53.
seems inconsistent with empirical studies suggesting that         Ashby, F. G., & Maddox, W. T. (2005). Human category
participants successfully choose between processes in                   learning. Annual Review of Psychology, 56, 149–178.
categorization tasks (e.g., Erickson, 2008). Yet these            Brehmer, B. (1994). The psychology of linear judgement
experimental tasks may be unrepresentative of real-world                models. Acta Psychologica, 87, 137–154.
situations. In many experimental studies—especially in            Chater, N., & Oaksford, M. (1999). Ten years of the rational
categorization research—there is little (or no) doubt about             analysis of cognition. Trends in Cognitive Sciences, 3,
which process is better suited to solving the whole task (or            57–65.
responding to a specific item), and a participant can thus        Dana, J., & Dawes, R. M. (2004). The superiority of simple
learn to choose between or differentially use the two                   alternatives to regression for social science predictions.
processes. We speculate that deviating from a simple                    Journal of Educational and Behavioral Statistics, 29,
blending strategy is generally worthwhile only in domains               317–331.
in which one process is clearly superior to the other, both       Dawes, R. M. (1979). The robust beauty of improper linear
                                                              2540

     models in decision making. American Psychologist, 34,            Sons.
     571–582.                                                    Larrick, R. P., Mannes, A. E., & Soll, J. B. (2012). The
Erickson, M. A. (2008). Executive attention and task                  social psychology of the wisdom of crowds. In J. I.
     switching in category learning: Evidence for stimulus-           Krueger (Ed.), Frontiers in social psychology: Social
     dependent representation. Memory & Cognition, 36,                judgment and decision making. New York, NY:
     749–761.                                                         Psychology Press.
Erickson, M. A., & Kruschke, J. K. (1998). Rules and             Lee, M. D., & Cummins, T. D. R. (2004). Evidence
     exemplars in category learning. Journal of                       accumulation in decision making: Unifying the “take
     Experimental Psychology: General, 127, 107–140.                  the best” and the “rational” models. Psychonomic
Gigerenzer, G., Hertwig, R., & Pachur, T. (2011).                     Bulletin & Review, 11, 343–352.
     Heuristics: The foundations of adaptive behavior.           Lee, M. D., Zhang, S., & Shi, J. (2011). The wisdom of the
     Oxford: Oxford University Press.                                 crowd playing The Price Is Right. Memory &
Gigerenzer, G., & Selten, R. (Eds.). (2001). Bounded                  Cognition, 39, 914–923.
     rationality: The adaptive toolbox. Cambridge, MA:           Marling, C., Sqalli, M., Rissland, E. L., Munoz-Avila, H., &
     MIT Press.                                                       Aha, D. (2002). Case-based reasoning integrations. AI
Hahn, U., & Chater, N. (1998). Similarity and rules:                  Magazine, 23, 69–86.
     Distinct? Exhaustive? Empirically distinguishable?          Nosofsky, R. M., Palmeri, T. J., & McKinley, S. C. (1994).
     Cognition, 65, 197–230.                                          Rule-plus-exception model of classification learning.
Hahn, U., Prat-Sala, M., Pothos, E. M., & Brumby, D. P.               Psychological Review, 101, 53–79.
     (2010). Exemplar similarity and rule application.           Payne, J. W., Bettman, J. R., & Johnson, E. J. (1993). The
     Cognition, 114, 1–18.                                            adaptive decision maker. Cambridge, UK: Cambridge
Herzog, S. M., & Hertwig, R. (2009). The wisdom of many               University Press.
     in one mind: Improving individual judgments with            Persson, M., & Rieskamp, J. (2009). Inferences from
     dialectical bootstrapping. Psychological Science, 20,            memory: Strategy- and exemplar-based judgment
     231–237.                                                         models compared. Acta Psychologica, 130, 25–37.
Herzog, S. M., & Hertwig, R. (2013). The crowd-within and        Prentzas, J., & Hatzilygeroudis, I. (2007). Categorizing
     the benefits of dialectical bootstrapping: A reply to            approaches combining rule-based and case-based
     White and Antonakis (2013). Psychological Science,               reasoning. Expert Systems, 24, 97–122.
     24, 117–119.                                                Rieskamp, J., & Otto, P. E. (2006). SSL: A theory of how
Herzog, S. M., & von Helversen, B. (2013). The benefits of            people learn to select strategies. Journal of
     combining cognitive processes. Manuscript in                     Experimental Psychology: General, 135, 207–236.
     preparation.                                                Scheibehenne, B., Rieskamp, J., & Wagenmakers, E.-J.
Jacobs, R. A., Jordan, M., Nowlan, S., & Hinton, G. (1991).           (2013). Testing adaptive toolbox models: A Bayesian
     Adaptive mixtures of local experts. Neural                       hierarchical approach. Psychological Review, 120, 39–
     Computation, 3, 79–87.                                           64.
Jäkel, F., Schölkopf, B., & Wichmann, F. A. (2009). Does         Soll, J. B., & Larrick, R. P. (2009). Strategies for revising
     cognitive science need kernels? Trends in Cognitive              judgment: How (and how well) people use others'
     Sciences, 13, 381–388.                                           opinions. Journal of Experimental Psychology:
Juslin, P., Karlsson, L., & Olsson, H. (2008). Information            Learning, Memory, and Cognition, 35, 780–805.
     integration in multiple cue judgment: A division of         Surowiecki, J. (2004). The wisdom of crowds: Why the
     labor hypothesis. Cognition, 106, 259–298.                       many are smarter than the few and how collective
Juslin, P., & Persson, M. (2002). PROBabilities from                  wisdom shapes business, economies, societies and
     Exemplars (PROBEX): A “lazy” algorithm for                       nations. Garden City, NY: Doubleday.
     probabilistic inference from generic knowledge.             Todd, P. M., Gigerenzer, G., & the ABC Research Group.
     Cognitive Science, 26, 563–607.                                  (2012). Ecological rationality: Intelligence in the
Krause, J., Ruxton, G. D., & Krause, S. (2010). Swarm                 world. Oxford, UK: Oxford University Press.
     intelligence in animals and humans. Trends in Ecology       von Helversen, B., Herzog, S. M., & Rieskamp, J. (in press).
     and Evolution, 25, 28–34.                                        Haunted by a Doppelgänger: Irrelevant facial similarity
Kruschke, J. K. (1992). ALCOVE: An exemplar-based                     affects rule-based judgments. Experimental Psychology.
     connectionist     model     of    category    learning.     von Helversen, B., & Rieskamp, J. (2008). The mapping
     Psychological Review, 99, 22–44.                                 model: A cognitive theory of quantitative estimation.
Kruschke, J. K. (2008). Models of categorization. In R. Sun           Journal of Experimental Psychology: General, 137,
     (Ed.), The Cambridge handbook of computational                   73–96.
     psychology. New York, NY: Cambridge University              Vul, E., & Pashler, H. (2008). Measuring the crowd within:
     Press.                                                           Probabilistic representations within individuals.
Kuncheva, L. (2004). Combining pattern classifiers:                   Psychological Science, 19, 645–647.
     Methods and algorithms. Hoboken, NJ: John Wiley &
                                                             2541

