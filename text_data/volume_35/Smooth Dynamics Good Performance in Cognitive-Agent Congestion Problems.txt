UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Smooth Dynamics, Good Performance in Cognitive-Agent Congestion Problems
Permalink
https://escholarship.org/uc/item/57b632sz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Reitter, David
Scerri, Paul
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

  Smooth Dynamics, Good Performance in Cognitive-Agent Congestion
                                                          Problems
                                              David Reitter, Penn State University
                                             Paul Scerri, Carnegie Mellon University
                           Abstract                                ing property of the human-based system is adaptation
                                                                   and damping. We will use a multi-agent system in which
   In a congestion game, individuals exhaust a common re-
   source out of selfish behavior. In this scenario, drivers       each agent is implemented as a model of (relevant) hu-
   create traffic jams by choosing the shortest route accord-      man behavior, namely learning from observations and
   ing to their individual knowledge. They can avoid them          information obtained by others, and estimating quanti-
   by communicating their belief states about the traffic
   situation in real-time through a peer-to-peer network,          ties (or categorizing) based on current knowledge.
   assuming unlimited bandwidth. We study two poten-                  In our scenario, individual models repeatedly choose
   tial, cognitively inspired models of human behavior: 1)
   categorization (quantized memorization and communi-             roads in a road network to get them from home to work.
   cation), which dampens communication and belief adop-           The time taken to traverse a road is a function of the
   tion, but leads to undesired oscillations and lower perfor-     number of other agents on the road when the agent be-
   mance. 2) Instance-based blending with memory decay,
   which achieves good dynamics and near-optimal perfor-           gins to traverse the road.
   mance without the same bandwidth needs. We argue                   The first contribution of this work is to apply human-
   that this supports our hypothesis of co-adaptation of
   cognitive function and communicating communities.               inspired instance-based learning (IBL, Gonzalez, Lerch,
                                                                   & Lebiere, 2003) algorithm to the problem of multi-agent
                       Introduction                                learning on the road congestion problem. The IBL model
                                                                   treats each trip from home to work as several instances of
In many situations, crowds of interacting human indi-
                                                                   road segments and weighs instances based on several fac-
viduals share resources such as food, roads, electricity,
                                                                   tors including recency when estimating time to traverse a
internet bandwidth, or airtime in a conversation. Sim-
                                                                   road. We found that the agents using IBL do much bet-
ilarly there are many interesting domains that require
                                                                   ter than the agents using an alternative, category-based
robots or agents to simultaneously learn to utilize com-
                                                                   model and about the same as agents using a communi-
mon resources. When the actions of one agent impact
                                                                   cation intensive averaging model.
the outcomes of another agent, individual learning often
leads to complex system dynamics. A canonical example                 Second, this paper examines the dynamics of mixed
of this problem is cooperative path planning(Burgard,              human-machine systems. We demonstrate that the over-
Moors, Fox, Simmons, & Thrun, 2000), where agents                  all system performance is improved by even a relatively
using the same routes negatively interfere with one                small number of the IBL agents and that there were no
other, but many other domains have been studied includ-            negative effects on either type of agent. When there were
ing soccer(Kalyanakrishnan, Liu, & Stone, 2007) and                only a few IBL agents in the system, they performed rel-
markets(Tesfatsion & Judd, 2006).                                  atively better than the ternary agents, but when there
   Computational-cognitive models of social behavior re-           were many IBL agents all agents performed about the
quire a combination of cognitive architectures and multi-          same.
agent design and analytics. In this paper, we investigate             Finally, the third contribution of this work is to show
the effect of memory decay and instance-based learning             how changes to the underlying system, in addition to
and decision-making a system of communicating, simu-               changes due to learning, impact performance. One might
lated individuals.                                                 expect that more numerical approaches will respond to
   Based on the rational assumption that human cog-                change more quickly and effectively than a learner re-
nitive function has adapted to its environment, we hy-             lying on experiences. However, we found no evidence
pothesize that memory function improves system dy-                 of this. Instead, IBL agents reacted very quickly and
namics in communication networks. We predict that                  appropriately to underlying change, far better than the
forgetting improves, rather than impedes, performance              ternary agents. From these experiments we can see po-
in situations where crowds use finite common resources.            tential for using IBL in multi-agent learning settings and
This is also a core problem in multi-agent learning: one           exploring these other settings is a key area of future fo-
agent’s behavior may impact the outcome for another                cus.
agent. The collective behavior of a learning system and               Cognitive models have been combined to explain
individual reward can vary wildly and unpredictably.               learning in team settings, primarily in a qualitative way
Human-controlled road networks exhibit similar traffic             (Sun, 2008). Reitter and Lebiere (2012) used decay
jams, though rarely with the same catastrophic conse-              in a model implemented within a cognitive architecture
quences the multi-agent simulations suggest. An emerg-             to show that decayed memory improves agent perfor-
                                                               3269

mance in a foraging scenario with multiple, communi-           ested in two primary metrics. First, the average time
cating agents. Instance-based learning within cognitive        it takes for an agent to get from phome to pwork . Sec-
models has been shown to explain human behavior in             ond, as the agents build their models and adapt their
a number of cognitive decision-making tasks (Lebiere,          plans to the changing models, the average transit time
Wallach, & West, 2000; Erev et al., 2010).                     will change. As a secondary measure, we are interested
                                                               in the change in average transit time over time.
                Scenario Framework
The framework for the scenario (Scerri, to appear) con-        Communication Network The agents are organized
sists of agents A, places P and edges G over some num-         into a social network where they can only communicate
ber of iterations. Each agent a ∈ A has some place,            directly with a small subset of the rest of the agents.
phome ∈ P where it starts each iteration and some place        Information is propagated through the network in a peer-
pwork ∈ P where it must get to each iteration or day.          to-peer manner. Unless otherwise noted below, we use a
To get to pwork it must use edges connecting places. In-       random network with degree 5 to connect the agents.
dividual edges g ∈ G connect exactly two places. The
agents task is to get from phome to pwork most quickly
                                                               Model Path Planning
each iteration.                                                The cognitive agents have to choose a path that will most
   The time that it will take an agent to traverse an edge     quickly get them to their destination, based on experi-
depends purely on the number of agents already on the          ences so far and from experiences communicated from
edge when it gets to the edge. Specifically, we choose a       other agents. The optimal strategy might be one that
simplified function to model limited resources that are        considers likely plans by others and the changes they will
affected by congestion: the time taken by an agent is          make, given their previous experiences. However, this is
10 + n3already , where nalready is the number of agents        typically infeasible, and theoretically the game-theoretic
on the edge when the agent reaches it. The simulation          Traveller’s Dilemma (Basu, 1994) applies: If one agent
randomizes the order the agents execute so that in one         A anticipates another agent B’s reaction, A would also
iteration an agent might be the first on the edge and have     anticipate B’s anticipation of A’s reaction, and so on.
a very short travel time and another iteration it might        Rational players will end up with a poor solution (finite
be tenth onto the edge and have a very long travel time,       game), or they will be faced with a computation that
even if none of the agents change their routes.                does not scale.
   This framework has two important features. First,              Any accurate model of crowd cooperation needs
the agents will get very different perspectives on speed       to deal with limited communication bandwidth, learn
of a edge, based on exactly when they get onto the edge.       (quickly) to achieve acceptable performance, adapt to
Hence, either many iterations or cooperation is needed         changing network dynamics. In the following we describe
to create an accurate model. Second, busy edges heavily        two cognitively plausible models for reasoning about the
penalize the agents, just a few extra agents on a edge         road network, as well as an optimal one with high band-
will dramatically slow the last few agents down again          width needs. Earlier work has shown path-planning in
making cooperation important.                                  a model in the cognitive architecture ACT-R (Reitter &
   For experimental purposes, there are only ten different     Lebiere, 2010). However, here we focus on the memory
phome and pwork for 200 agents. This makes for more in-        components only and keep the path planning algorithm
teresting traffic congestion problems, with more extreme       (A* planning) constant to facilitate meaningful compar-
cases, and requires more coordination among the agents,        ison.
but, as was shown in (Scerri, to appear) does not quali-       Categorizing Model: Ternary We include two cog-
tatively change the system dynamics.                           nitively plausible models at the agent level. The first,
   In every iteration, each agent uses a model of the          ternary, forms its belief about a road segment as a cat-
graph to plan a path from phome to pwork . The agents          egory: slow, medium or fast. The model keeps, for each
use a standard A* algorithm (Russell, Norvig, & Artifi-        edge, a normalized frequency distribution of the observed
cial Intelligence, 1995) to do the planning based on their     categories, decayed over time. Specifically, for each edge
current model of edge traversal times. Agents are risk         e, the model is Me = {pslow , pmedium , pf ast }, pslow +
neutral, trying to minimize expected travel time. They         pmedium + pf ast = 1. When an individual gets an ob-
then execute their plan without adapting to observed           servation of a particular category it adds βlocal for a
conditions. At the end of an iteration, the agents can         local observation and β for a communicated observation
communicate about what they observed. The model the            to the relevant p and then normalizes.
agent plans with and the information it communicates              The models assume the most probable category,
are described below.                                           max M for planning. In the following experiments, an
   It is assumed that each agent plans selfishly, but com-     edge in a particular category is assumed to take time 300,
municates truthfully and cooperatively. We are inter-          156 and 12 for pslow , pmedium and pf ast , respectively, cor-
                                                           3270

                                                                                      200
                                     AVERAGE                                                                  AVERAGE
                                     IBL                                                                      IBL
                                                                                      150                     TERNARY
                                     TERNARY
Avg. Travel Time
                   1000
                                                                        Change rate
                                                                                      100
                    500
                                                                                       50
                                                                                        0
                          0     50       100    150   200                                   0         50      100         150      200
                                        Day                                                                   Day
                    Figure 1: Ternary, IBL and Average models.                                  Figure 2: Rate of belief changes.
responding to the average time when approximately 3, 7
and 11 agents also use the edge reasonable approxima-
tion of the typical expectations. When max M changes
for an edge, i.e., when the individual’s belief about an
edge changes categories, it communicates the new cate-
gory to its direct neighbors in the social network.
Instance-based learning model The second model
implements a cognitively motivated aggregation mecha-
nism that forms their beliefs. As in the ternary model,
its communications are quantized and occur whenever
its belief about a road changes. The same A* algorithm
is used to plan paths. However, this model’s estimates
about the speed of each road are based on instance-                 Figure 3: A histogram of the variance in estimates per
based learning. IBL stores a datapoint (episode) with               road for each of the model types.
the speed of a road whenever it is traveled or when
agents receive a communication. (A commute involves
many such roads.) A speed estimate can then be de-                  of episodes R involving the road, then the expected speed
rived as the average of all episodes associated with the            of a road, U (R) is derived as
road, weighted by the episode’s activation. Activation
is determined by a function that rewards experience (a                                                                 ue eAe /T
                                                                                                              P
                                                                                                               e∈R
large set of episodes), but discounts older information                                             U (R) =       P
                                                                                                                      eAe /T
(decay). Activation has been shown to predict the avail-
ability of information in human memory (Anderson &                  T = 0.25 is a parameter (temperature). If R is empty,
Lebiere, 1998).                                                     we assume a default speed, Uβ for the road. The agent’s
   In detail, activation of an episode e consisting of a            performance is sensitive to Uβ , which represents a mea-
road speed (utility) and time, < ue , te > is given as              sure of pessimism (we do not optimize Uβ and choose 0.0
                                 Ae = (t − te )−0.5                 as the most optimistic value).
                                                                      Instance-based learning and the activation function
t is the current time. The decay exponent is the default            have several desirable properties in our context. Acti-
that is empirically realistic in human experiments. Our             vation increases during early iterations and allows the
implementation uses an highly precise approximation of              model to quickly differentiate between fast and slow
the above activation function that omits to store all but           roads. Activation is less affected by presentation of
the n latest episodes. If a road is represented by a series         changes concerning frequently travelled roads.
                                                                 3271

                                                               ment over time and some initial poor performance as the
                                                               space is explored. The highly communication intensive
                                                               and, for a human, computationally challenging Average
                                  COMPLETE
                                                               model and the IBL model achieve about the same final
Avg. Travel Time
                   1000
                                  RANDOM                       level of performance and have about the same initially
                                  RING
                                  SMALLWORLDS                  poor performance. Both do better than the Ternary
                                  NONE                         model in the long run, although the Ternary model more
                                                               quickly finds decent solutions.
                    500
                                                                  Since the IBL and Average models end with about
                                                               the same performance, it is tempting to conclude that
                                                               they work in about the same way. However, Figures 2
                                                               and 3 show that they actually achieve the result with
                                                               quite different dynamics. Figure 2 shows the average
                          0      50    100      150   200      number of agents that change the path they take from
                                      Day                      the day before. The ternary model oscillates because
                                                               beliefs take some time to change. More interestingly,
                                                               IBL consistently changes more than Average. IBL agents
Figure 4: Average travel times for IBL model agents as         change paths substantially more often, but the net result
the communication network is varied.                           is the same as the Average agents. It is infeasible to
                                                               determine exactly what is occurring, but it appears that
Averaging Model The Averaging Model is included                IBL agents switch between approximately equal paths
to provide a form of non-cognitive empirical ceiling: it       due to the noise in their relatively sparse data, while the
is information-hungry, assuming that communication is          Average agents have aggregated more data leading to
free and unconstrained. It is the simplest model an agent      more stable choices.
can have of the graph is to store the average time taken          Figure 3 shows a snapshot of the variance in beliefs
by agents traversing that edge. Since the utilization of       of the agents at the end of the 200 days. Specifically,
an edge will change over time, a moving average is used        for each road segment we computed the variance in the
to keep the model updated with respect to the current          time the agents estimate it would take to traverse that
situation.                                                     road. These variances were then discretized and pre-
   The agents estimate for an edge is simply e0i = αei +       sented as a histogram, with variances > 50 put in the
(1 − α)obs, where ei is the current estimate for the edge      last bin for clarity. The higher the variance the more
and obs is the new observation for the edge, whether           the agents disagreed about how long it would take to
communicated or observed locally. In this paper, we use        traverse the road. Each of the three models lead to dis-
α = 0.95.                                                      tinctly different patterns. The Ternary case often has all
                                                               agents in agreement and never has large disagreements
                              Empirical Evaluation             between agents due to the way beliefs cascade across the
In this section, we empirically examine the three models       network and because the agents only allow a road esti-
on the congestion problem described in Section 2. The          mate to have one of three values. The Average model
evaluation is split out into three parts, with each part       shows slightly lower variance overall than IBL, though
aimed at looking in depth at one of the hypotheses in-         the IBL has many more roads with very high variance,
troduced in Section 1. Unless otherwise stated, for each       indicating complete disagreement. It is insightful to see
experiment below we use the following experimental pa-         that better performance was had when the agents had
rameters.                                                      different models of the environment, many of which must
                                                               actually be wrong. We can conclude that Average and
Instance-Based Multi-Agent Learning                            IBL achieve approximately the same results, with very
The key challenge for multi-agent learning is that all the     different algorithms and with distinctly different internal
agents are simultaneously learning, making the learning        dynamics.
environment non-stationary. Learning from instances               Conceptually, the cognitive model (IBL) does several
in a non-stationary environment is not an intuitively          things differently to Ternary. To try to understand what
effective technique. However, humans, who arguably             the cause of the different behavior was, we manipulated
use a type of IBL, are highly capable of learning in           Ternary in several different ways. First, we artificially
non-stationary environments. Our first experiments are         prevent Ternary agents from changing each step to mimic
aimed at looking at the performance of IBL on the con-         the IBL’s preference for reusing previous paths. Sec-
gestion problem. Figure 1 compares the IBL, Ternary            ond, we decay the learning rate so later data has less
and Average models. Each model shows some improve-             effect on Ternary, to mimic the way IBL instances aggre-
                                                            3272

                   800                                           800                                           800
                                                                                                                           AVERAGE
Avg. Travel Time                              Avg. Travel Time                              Avg. Travel Time
                                                                                AVERAGE                                    IBL
                   600                                           600            IBL                            600         TERNARY
                                                                                TERNARY
                   400                                           400                                           400
                   200                                           200                                           200
                         0   50   100   150                       200   100           105                            160         165
                                  Day                                           Day                                        Day
Figure 5: Travel times for different arrangements (lefts) of IBL and Ternary agents. Adding roads over time (right).
gate. Finally, we change the default value for Ternary                           influencing each other. Hence, it is informative to look at
for unknown roads to match the default for IBL. We                               what happens when IBL and Ternary agents are learning
found that each of these changes improved Ternary per-                           on the network at the same time. Varying the ratio of
formance, but preventing them from changing each step                            IBL to Ternary agents, we found that it takes relatively
had the biggest effect. The qualitative equivalent of this                       few IBL agents to give the whole system an improvement
in human decision-making would be status-quo effects                             in performance. Having different types of learners in
or confirmation biases, while IBL’s implementation more                          the same system not only does not hurt performance, it
directly reflects properties of human memory.                                    actually helps the weaker learners do better.
   Figure 4 shows how communication networks influ-                                 In the case of mixed Random graph networks of IBL
ence the IBL agents. Curiously, blocking communication                           and Ternary agents, we find that when there are only a
works similarly well as communication on fully connected                         few IBL agents they have a noticeable advantage over the
and random network structures. These networks share                              Ternary agents, i.e., although they are using the same
information most evenly across the team, while ring and,                         roads and are all interfering with each other, the IBL
to a lesser degree, small worlds networks compartmental-                         agents do relatively better. This advantage has disap-
ize information into neighborhoods. Although the effect                          peared when there are equal numbers of IBL and Ternary
is not very big, the data represents many simulation runs                        agents. The effect disappears smoothly as the number of
so the differences are not due to noise.                                         IBL agents increases. If we think of IBL agents as being
   We see that complete, random networks do very well.                           similar to humans and Ternary as being more like agents,
A post-hoc explanation is that these networks enable                             this experiment hints that a small number of humans in
the agents to communicate freely; agents have up-to-                             an otherwise agent-dominated environment may do rel-
date information about congested roads. (The random                              atively better than the agents, and that they, as shown
networks were dense - each node has a degree of 5.) The                          above, may improve the whole system’s performance.
networks without connections also do well, perhaps sur-
prisingly so. Here, agents may adapt more slowly, and                            Disruptions
only to first-hand experience. In conjunction with the                           Intuitively, learning from instances is likely to behave
instance-based learner, this may also be a working strat-                        differently to learning moving numerical estimates when
egy to avoid congestion. However, communication helps                            there are changes to the underlying system. Here we
avoid a consistent initial spike, which we expect to be                          look at two different types of disruption to the under-
due to decision-making based on shared ground truth:                             lying system, the addition of roads and the addition of
everyone decides to use the fastest roads.                                       agents, and the effects on the dynamics for each of the
                                                                                 agent types. In the first case, one new road is randomly
IBL and Ternary Models Interacting                                               added every 20 days. The resulting dynamics are shown
IBL agents can be thought of as a simple model for how                           in Figure 5 over 200 days (left) and just for an early (cen-
human learning might occur and Ternary agents can be                             ter) and a late (right) road addition. Both Average and
thought of as a reasonable, low communication agent                              IBL spike dramatically as they try to exploit the new
approach to cooperative learning. Future systems are                             road, but then go back to their original paths after find-
likely to have humans and agents learning together and                           ing it to be unhelpful – for most of them because they
                                                                              3273

                   500
                                                             moving to a Nash Equilibrium, where, at least according
                              AVERAGE                        to their local models, they have no incentive to change
                              IBL
                              TERNARY                        behavior. However even if the agents do reach an equilib-
                   400                                       rium, the outcome may be far from the socially optimal
Avg. Travel Time
                                                             solution (Hagstrom & Abrams, 2001).
                   300                                          Understanding the emerging effects of cognitive
                                                             decision-making in these networked simulations will al-
                                                             low us to spell out clear predictions to investigate
                   200
                                                             crowd behavior empirically. The performance of cogni-
                                                             tive agents that are based on empirically informed con-
                   100                                       straints of memory decay, instance-based learning and
                                                             blending suggests that the mechanisms are not merely
                                                             a rational adaptation to static information in the envi-
                         0     50       100   150   200      ronment, but to dynamic resources and a social commu-
                                    Day
                                                             nication system. They enable us to maintain external,
                                                             distributed memory without the devastating effects of
                                                             cyclic, mutual adaptation.
Figure 6: The impact of adding agents over time on
average performance.                                                                References
                                                             Anderson, J. R., & Lebiere, C. (1998). The atomic compo-
                                                                    nents of thought. Mahwah, NJ: Erlbaum. Paperback.
                                                             Basu, K. (1994). The traveler’s dilemma: Paradoxes of ra-
all tried it at once. The Ternary model is more robust              tionality in game theory. The American Economic Re-
because of the information sharing, but also takes longer           view , 84 (2), pp. 391-395.
to recover. Figure 6 shows the travel times as five new      Burgard, W., Moors, M., Fox, D., Simmons, R., & Thrun, S.
                                                                    (2000). Collaborative multi-robot exploration. In IEEE
Ternary agents are added each 20 days, starting with 150            Conference on Robotics and Automation, ICRA’00
agents to make the result more comparable to other re-              (Vol. 1, pp. 476–481).
sults. Both the Average and IBL agents jump when the         Erev, I., Ert, E., Roth, A. E., Haruvy, E., Herzog, S. M.,
                                                                    Hau, R., et al. (2010). A choice prediction competition:
new agents are added, but then smoothly improve per-                Choices from experience and from description. Journal
formance. The Ternary agents are more dramatically                  of Behavioral Decision Making, 23 (1), 15-47.
effected by the change and do not adapt quickly. As the      Gonzalez, C., Lerch, F., & Lebiere, C. (2003). Instance-
                                                                    based learning in dynamic decision making. Cognitive
environment gets more congested and the original agents             Science, 27 (4), 591-635.
have built up more learning data, it is appears that IBL     Hagstrom, J., & Abrams, R. (2001). Characterizing braess’s
is more affected by the disruptions. This is unsurprising           paradox for traffic networks. In Proc. IEEE Intelligent
                                                                    transportation systems (pp. 836–841).
as its learning rate is effectively lower at this point.     Kalyanakrishnan, S., Liu, Y., & Stone, P. (2007). Half
                                                                    field offense in robocup soccer: A multiagent reinforce-
                         Discussion and Future Work                 ment learning case study. RoboCup 2006: Robot Soccer
                                                                    World Cup X , 72–85.
The cognitive, IBL agents benefit from a relatively sim-     Lebiere, C., Wallach, D., & West, R. L. (2000). A memory-
ple learning model, combining a preference for well-                based account of the prisoner’s dilemma and other 2x2
known roads and exploration of unseen roads. These                  games. In Proceedings of the International Conference
                                                                    on Cognitive Modeling (p. 185-193).
cognitive agents can, with relatively limited communi-       Reitter, D., & Lebiere, C. (2010). A cognitive model of spa-
cation volume, spread across the road network and ef-               tial path planning. Computational and Mathematical
ficiently use shared resources. What may be key to                  Organization Theory, 16 (3), 220-245.
                                                             Reitter, D., & Lebiere, C. (2012). Social cognition: Mem-
the cognitive agent’s performance is limited sharing of             ory decay and adaptive information filtering for robust
knowledge: because agents do not have access to precise             information maintenance. In Twenty-sixth AAAI Con-
road utility estimates of their neighbors, and because              ference on Artificial Intelligence (AAAI-12).
                                                             Russell, S., Norvig, P., & Artificial Intelligence, A. (1995).
they only receive updates when the neighbor’s (quan-                A modern approach. Artificial Intelligence. Prentice-
tized) beliefs change, they may arrive at heterogeneous             Hall, Egnlewood Cliffs.
conclusions about which roads are best. This leads them      Scerri, P. (to appear). Modulating communication to im-
                                                                    prove multi agent learning convergence. In Dynamics
to spread out more, without sacrificing much individual             of information systems: Algorithmic approaches.
performance. Under this scenario, agents do not need to      Sun, R. (Ed.). (2008). Cognition and Multi-Agent Inter-
misrepresent their knowledge states to their neighbors.             action: From Cognitive Modeling to Social Simulation
                                                                    (1st ed.). Cambridge University Press. Paperback.
   When combined in the same system, IBL agents              Tesfatsion, L., & Judd, K. (2006). Handbook of com-
and ternary agents actually helped each other rather                putational economics: Agent-based computational eco-
                                                                    nomics (Vol. 2). North Holland.
than hurting performance. This is promising for future
human-agent systems that will learn with distinctly dif-
ferent approaches. Notice that the agents are generally
                                                          3274

