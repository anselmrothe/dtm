UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Cheap but Clever: Human Active Learning in a Bandit Setting
Permalink
https://escholarship.org/uc/item/5xt5z4tv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Zhang, Shunan
Yu, Angela
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                 Cheap but Clever: Human Active Learning in a Bandit Setting
                                                  Shunan Zhang            Angela J. Yu
                                                         (s6zhang, ajyu@ucsd.edu)
                                Department of Cognitive Science, University of California, San Diego
                                             9500 Gilman Drive, La Jolla, CA 92093-0515
                              Abstract                                     Bandit problems elegantly capture the tension between
                                                                        exploration (selecting an arm about which one is igno-
   How people achieve long-term goals in an imperfectly known
   environment, via repeated tries and noisy outcomes, is an im-        rant) and exploitation (selecting an arm that is known to
   portant problem in cognitive science. There are two inter-           have relatively high expected reward), which is manifest in
   related questions: how humans represent information, both            many real-world decision-making situations involving noise
   what has been learned and what can still be learned, and how
   they choose actions, in particular how they negotiate the ten-       or uncertainty. Bandit problems have been well studied in
   sion between exploration and exploitation. In this work, we          many fields, including statistics (Gittins, 1979), reinforce-
   examine human behavioral data in a multi-armed bandit set-           ment learning (Kaebling, Littman, & Moore, 1996; Sut-
   ting, in which the subject choose one of four “arms” to pull on
   each trial and receives a binary outcome (win/lose). We im-          ton & Barto, 1998), economics (Banks, Olson, & Porter,
   plement both the Bayes-optimal policy, which maximizes the           2013, e.g.), psychology and neuroscience (Daw, O’Doherty,
   expected cumulative reward in this finite-horizon bandit envi-       Dayan, Seymour, & Dolan, 2006; Cohen, McClure, & Yu,
   ronment, as well as a variety of heuristic policies that vary in
   their complexity of information representation and decision          2007; Steyvers, Lee, & Wagenmakers, 2009; Lee, Zhang,
   policy. We find that the knowledge gradient algorithm, which         Munro, & Steyvers, 2011). There is no analytical solution
   combines exact Bayesian learning with a decision policy that         to the general bandit problem, though properties about the
   maximizes a combination of immediate reward gain and long-
   term knowledge gain, captures subjects’ trial-by-trial choice        optimal solution of special cases are known (Gittins, 1979).
   best among all the models considered; it also provides the best      For relatively simple, finite-horizon problems, the optimal
   approximation to the computationally intense optimal policy          solution can be computed numerically via dynamic program-
   among all the heuristic policies.
                                                                        ming (Kaebling et al., 1996), but its computational complex-
   Keywords: Bandit problems; human decision making; hu-                ity grows exponentially with the number of arms and with
   man active learning; knowledge gradient
                                                                        the time horizon. In the psychology literature, a number of
                                                                        heuristic policies, with varying levels of complexity in the
                          Introduction
                                                                        learning and control processes, have been proposed as possi-
How humans achieve long-term goals in an imperfectly                    ble strategies used by human subjects (Daw et al., 2006; Co-
known environment, via repeated tries and noisy outcomes,               hen et al., 2007; Steyvers et al., 2009; Lee et al., 2011). Most
is an important problem in cognitive science. The com-                  models assume that humans either adopt simplistic policies
putational challenges consist of the learning component,                that retain little information about the past and sidestep long-
whereby the observer updates his/her representation of                  term optimization (e.g. win-stay-lose-shift and ε-greedy), or
knowledge and uncertainty based on continual observations,              switch between an exploration and exploitation mode either
and the control component, whereby the observer chooses                 randomly (Daw et al., 2006) or discretely over time as more
an action that somehow balances between the need to ob-                 is learned about the environment (Steyvers et al., 2009).
tain immediate reward and to obtain information that assists
long-term reward accumulation.                                             Here, we analyze a new model for human bandit choice
   A classical task setting used to study sequential decision-          behavior, based on the knowledge gradient (KG) algorithm,
making under uncertainty is the multi-armed bandit prob-                which has been developed by Frazier, Powell, and Dayanik
lem (Robbins, 1952). The bandit problems are a family of                (2008) to solve problems in operations research. At each
reinforcement-learning problems where the decision maker                time step, the KG policy chooses, conditioned on previous
must choose among a set of arms on each trial: the reward               observations, the option that maximizes future cumulative
gained on each trial both has intrinsic value and informs the           reward gain. It is based on the myopic assumption that the
decision maker about the relative desirability of the arms,             next observation is the last exploratory choice, used to learn
which can help with future decisions. In the basic bandit               about the environment, and all remaining choices will be
setting, each arm has an unknown probability of generating              exploitative, choosing the option with the highest expected
a reward on each trial. The problem is called finite hori-              reward by the end of the next trial. Note that this myopic
zon if the total number of trials is finite; it is called infinite      assumption is only used in reducing the complexity of com-
horizon if the number of trials is infinite, in which case one          puting the predicted value of each option, and not actually
either discounts future rewards or tries to maximize average            implemented in practice – the algorithm may end up exe-
reward per unit time. In this work, we focus on stationary,             cuting arbitrarily many non-exploitative choices. Despite a
non-discounted, finite-horizon bandit problems, where the               certain greedy aspect to the KG control policy, it is not com-
underlying reward rates are independent and identically (iid)           pletely short-sighted. In particular, it tends to explore more
distributed across the arms.                                            when the number of trials left is large, because finding an
                                                                     1647

arm with even a slightly better reward rate than the currently
best known one can lead to a large cumulative advantage in
future gain; on the other hand, when the number of trials left          15
                                                                           Ratio: 0.000
                                                                                                15
                                                                                                   Ratio: 0.667
                                                                                                                      15
                                                                                                                         Ratio: 0.667
                                                                                                                                              15
                                                                                                                                                 Ratio: −−
is small, KG tends to exploit and stay with the currently best          14                      14                    14                      14
known option, because it knows that finding a slightly better           13                      13                    13                      13
                                                                        12                      12                    12                      12
option will not lead to large improvement, while the risk of            11                      11                    11                      11
wasting time on a bad option is high. KG is also known to be            10                      10                    10                      10
                                                                         9                       9                     9                       9
exactly optimal in certain special cases (Frazier et al., 2008),         8                       8                     8                       8
such as when there are only two arms.                                    7                       7                     7                       7
                                                                         6                       6                     6                       6
   KG has several advantages over previously proposed algo-              5                       5                     5                       5
rithms. Unlike the simple heuristic algorithms such as win-              4                       4                     4                       4
stay-lose-shift and ε-greedy, and in common with the other               3                       3                     3                       3
                                                                         2                       2                     2                       2
Bayesian learning algorithms (Daw et al., 2006; Steyvers et              1                       1                     1                       1
al., 2009; Lee et al., 2011), KG uses a sophisticated Bayesian           0                       0                     0                       0
posterior distribution as its belief state at each time step.
Unlike the other Bayesian learning algorithms, KG grace-
                                                                                        Figure 3. Basic design of the experimental interface.
fully and gradually transitions from primarily exploring to
primarily exploiting over the course of a finite-horizon ban-       Figure 1: Experiment interface. The four panels correspond
dit experiment. Also unlike previously proposed algorithms,         to the four arms, each of which can be chosen by pressing the
which typically assumes that the stochastic component of            corresponding button. In each panel, successes from previ-
action selection is random or arbitrary, KG also provides           ous trials are shown as green bars, and failures as red bars.
a more sophisticated and discriminating way to explore, by          At the top of each panel, the ratio of successes to failures,
normatively combining immediate reward expectation and              if defined, is shown. The top of the interface provides the
long-term knowledge gain. On the other hand, in contrast to         count of the total number of successes to the current trial,
the optimal algorithm, which scales exponentially in compu-         index of the current trial and index of the current game.
tational complexity with respect to the number of remaining
timesteps, KG is computationally much simpler, incurring a
constant cost regardless of the number of timesteps left.           they only keep a mean estimate (running average) of the re-
   In the following, we first describe the experiment, then         ward rate of the different options, or also uncertainty about
describe all the learning and control models that we con-           those estimates, or indeed more complex meta-information,
sider. We then compare the performance of the models both           such as the general abundance/scarcity of rewards. The de-
in terms of agreement with human behavior on a trial-to-trial       cision component can also differ in complexity in at least
basis, and in terms of computational optimality.                    two respects: the objective the decision policy tries to opti-
                                                                    mize (e.g. reward versus information), and the time-horizon
                              Data                                  over which the decision policy optimizes its objective (e.g.
Participants                                                        greedy versus long-term). In this section, we introduce mod-
                                                                    els that encompass different combinations of learning and
A total of 451 participants completed a series of bandit prob-
                                                                    decision policies.
lems as part of ‘testweek’ at the University of Amsterdam.
                                                                    Bayesian Learning in Beta Environments
Experimental procedure
Each participant completed 20 bandit problems in sequence,          The observations are generated independently and identi-
all problems had 4 arms and 15 trials. The reward rates for         cally (iid) from an unknown Bernoulli distribution for each
all games were generated independently from a Beta(2, 2)            arm. We consider two Bayesian learning scenarios, either
distribution, and were all done prior to data collection. All       subjects have a fixed belief about the distribution from which
participants thus played games with the same sets of reward         the Bernoulli rates are drawn (“basic learning”), or they do
rates, but the order of the games was randomized. Partic-           meta-learning about the parameters of that distribution over
ipants were aware that the reward rates in all games were           time (“meta learning”). We explore the two scenarios below.
drawn from the same environment, but they were not told its         In either case, we assume the distribution that generates the
form, i.e. Beta(2, 2). A representation of the basic experi-        Bernoulli rates is a Beta distribution, Beta (α, β), which is a
mental interface is shown in Fig 1.                                 conjugate prior, and whose two hyper-parameters, α and β
                                                                    are determined by the total number of rewards and failures
                    Modeling Methods                                experienced so far, plus any pseudo-counts associated with
                                                                    the prior.
There exist multiple levels of complexity and optimality in
both the learning and the decision components of decision           Basic Learning Suppose we have K arms with reward
                                                                               g
making models of bandit problems. For the learning com-             rates, θk , k = 1, · · · , K, which are independent and identi-
ponent, we examine whether people learn any abstract rep-           cally drawn from Beta (α, β) for the gth game. On the tth
resentation of the environment at all, and if they do, whether      trial, if the kth arm is chosen, a reward is attained with a
                                                                 1648

                                   t, g                  g
Bernoulli distribution, Rk ∼ Bernoulli θk . Let St, g and                        (up to a fine discretization of the belief state space), and then
Ft, g be vectors of the number of successes and failures at-                     apply the computed policy for each sequence of choice and
tained from each arm at the tth trial of the gth game. The                       observations that each subject experiences. We use the term
model learns the individual reward rates using Bayes’ Rule:                      “the optimal solution” to refer to the specific solution under
                                                                                 α = 2 and β = 2, which is the true experimental design.
Pr θg | α, β, St, g , Ft, g         ∝ Pr St, g , Ft, g | θg Pr (θg | α, β)
                                                           
                                                                                 Win-Stay-Lose-Shift WSLS does not learn any abstract
                             θg ∼ Beta (α, β)
                             t, g                     t, g    t, g g 
                                                                                 representation of the environment, and has a very simple de-
                            Sk     ∼ Binomial Sk + Fk , θk                       cision policy. It assumes that the decision-maker continues
                                                                                 to choose an arm following a reward, but shifts to other arms
   The learner’s belief state at the trial t of the game g, Bt, g ,              (with equal probabilities) following a failure to gain reward.
is the set of posterior Beta distributions for each arm, and the
mean reward on each arm, based on the observed sequence,                         ε-Greedy The ε-greedy model assumes that decision-
is θ̂t, g = (α + St,k g )/(α + β + St,k g + FKt, g ).                            making is driven by a parameter ε that controls the bal-
                                                                                 ance between random exploration and exploitation inherent
Meta Learning We also consider the case that subjects
                                                                                 in bandit problems. On each trial, with probability ε, the
may use observations to learn about the true environmental
                                                                                 decision-maker chooses randomly (exploration), otherwise
reward distribution (the true Beta distribution), correspond-
                                                                                 chooses the arm with the greatest estimated reward rate (ex-
ing to the general abundance/scarcity of resources in the en-
                                                                                 ploitation). ε-Greedy keeps simple estimates of the reward
vironment. In this case, observing an outcome on any arm
                                                                                 rates, but does not track the uncertainty of the estimates. It is
will affect the posterior distribution on all arms because of
                                                                                 not sensitive to the horizon, maximizing the immediate gain
the correlation induced by shared hyper-parameters of the
                                                                                 with a constant rate, otherwise searching for information by
environment (Gelman, Carlin, Stern, & Rubin, 2004):
                                                                                 random selection1 .
                                                                                                                                t, g
Pr θg , α, β|St, g , Ft, g ∝ Pr St, g , Ft, g | θg Pr (θg | α, β) Pr(α, β)
                                                                                  We call the situation k ∈ argmaxk′ θ̂k ‘case 1’, and the
                                                                                 ε-greedy model is implemented as
                                                       t, g
   The belief state on trial t of game g, B , is a joint pos-                                                       
                                                                                                                      (1 − ε)/Mt, g   if case 1
                                                                                          t, g
                                                                                                      ε, θ̂t, g
                                                                                                                
terior distribution over the reward rates and environmental                        Pr   D       = k |             =
                                                                                                                      ε/ (K − Mt, g ) otherwise
parameters, conditioned on the observed sequence.
                                                                                             t, g
                                                                                 where M is the number of arms with the greatest estimated
Decision Policies                                                                value at the tth trial of the gth game.
We consider five different decision policies. We first de-
                                                                                 ε-Infomax The ε-infomax model is similar to the ε-greedy
scribe the optimal model, and then the four heuristic models
                                                                                 model in that it chooses the arm with the greatest estimated
with increasing levels of complexity.
                                                                                 reward rate with probability 1 − ε, and explores with proba-
The Optimal Model The learning and decision problem                              bility ε. The difference is that, instead of random selection
for bandit problems can be instantiated as a Markov Decision                     for exploration, it selects the arm that results in the largest re-
Process with a finite horizon (Kaebling et al., 1996). Due                       duction in the expected total entropy. In our study, the arms
to the low dimensionality of the bandit problem here (i.e.                       are independent given the same environmental distribution,
small number of arms and number of trials per game), the                         and the policy reduces to choose the arm with the largest
optimal policy, up to a discretization of the belief state, can                  uncertainty. We use St,k g + Fkt, g as an approximate, simple
be computed numerically according to Bellman’s dynamic                           measure of the uncertainty associated with arm k given the
programming principle. Let V t (St , Ft ) be the expected total                  state of the game. In this model, an arm may be chosen when
future reward on trial t. The optimal policy should satisfy                      one of the two cases applies: in case 1, it has the greatest es-
the following iterative property:                                                timated reward rate; in case 2, it does not have the greatest
                                                                     t, g
                                                                                 estimated reward rate, but has the least number of times be-
    V t, g (St, g , Ft, g ) = max E V t+1,g (St+1,g , Ft+1,g ) + θ̂k
                                                                
                                 k
                                                                                 ing chosen. We implement ε-infomax as
                                                                                                                     (1 − ε)/Mt, g
                                                                                                                    
                                        t, g                                                                                          if case 1
and the optimal decision, D , is decided by
                                                                                   Pr Dt, g = k | ε, θ̂t, g =         ε/N t, g
                                                                                                                
                                                                                                                                      if case 2
                                                                         t, g
 Dt, g (St, g , Ft, g ) = argmaxk E V t+1,g (St+1,g , Ft+1,g ) + θ̂k                                                  0               otherwise
                                                                                                                  
                                                                                 where Mt, g and N t, g are the number of arms that satisfy case
We solve the equation using dynamically programming,
                                                                                 1 and 2, respectively, at the tth trial of the gth game.
backward in time from the last time step, whose value func-
                                                                                    The ε-infomax model uses both the mean estimates and
tion and optimal policy are known for any belief state, i.e.
                                                                                 measure of uncertainty as criteria for action selection. It is a
any setting of posterior Beta distribution for each of the
arms: it always choose the arm with the highest expected                             1 The ε-Greedy model has a variant, ε-decreasing, where the
reward, θ̂T, g , and the value function is just that expected re-                probability of random selection decreases over trials. However,
                                                                                 previous studies found that ε-decreasing had a poor fit to the same
ward. In the simulations, we compute the optimal policy off-                     data when compared with the ε-greedy model (Zhang & Lee, 2010),
line, for any conceivable setting of belief state on each trial                  so we only consider the ε-greedy model in this study.
                                                                              1649

greedy heuristic, maximizing the immediate reward gain at
                                                                    Model Agreement with Optimal
                                                                                                    1
a constant rate.                                                                                           Basic Learn
                                                                                                   0.9     Meta Learn
Knowledge Gradient The knowledge gradient (KG) al-
gorithm (Ryzhov, Powell, & Frazier, 2012) is an approxi-                                           0.8
mation to the optimal policy, by pretending only one more
                                                                                                   0.7
exploratory measurement is allowed, and assuming all re-
maining choices will exploit what is known after the next                                          0.6
measurement. It evaluates the expected change in each esti-
mated reward rate, if a certain arm were to be chosen, based                                       0.5
on the current belief state. Its mathematical expression is
                                                                                                   0.4
                                                                                                       WSLS            eG         eINFO       KG
        vKG,t
         k    =  E   max  θ̂t+1
                            k′  | Dt
                                     = k, Bt
                                               − max θ̂tk′
                      k′                       k′
                                                                   Figure 2: Model agreement with data simulated by the op-
The first term is the expected largest reward rate on the next     timal solution under the correct prior of the environment.
step if the kth arm were to be chosen, with the expectation        Each bar shows the agreement of a model combining the cor-
taken over all possible outcomes of choosing k. The KG             responding decision policy and the learning framework. For
decision rule is                                                   the ε-greedy (eG), ε-infomax (eINFO) and the KG models,
                                                                   the error bars show the standard errors of the average agree-
        DKG,t,g = argmax θ̂t,g             KG,t, g
                           k + (T − t − 1)vk              (1)      ment based on a 4-fold cross-validation. WSLS has no pa-
                           k                                       rameters to fit and does not rely on any learning framework.
The first term of Equation 1 denotes the expected immedi-
ate reward by choosing the kth arm at t of the gth game,
whereas the second term reflects the expected gain of total                                        0.9      Basic Learn
                                                                    Agreement with People
remaining reward from t + 1 to the last trial of the current                                                Meta Learn
game. The formula for calculating vKG,t,g
                                      k      for the binary                                        0.8
bandit problems can be found in Chapter 5 of Powell and
Ryzhov (2012).                                                                                     0.7
Model Implementation and Agreement Calculation                                                     0.6
We used model agreement as a measure of how well it cap-                                           0.5
tures experimental data, which was calculated as the average
per-trial likelihood, conditioned on the observed game states.                                     0.4
                                                                                                         WSLS     eG          Optimal   eINFO   KG
We fit the models and calculated model agreement across all
participants.
   WSLS is a fully deterministic paradigm, so the per-trial        Figure 3: Model agreement with human data. The figure is
likelihood is 1 for a win-stay decision, 1/3 for a lose-shift      generated in the same way as for Figure 2, except for that the
decision, and 0 otherwise. All other models have at least          optimal model was only implemented with basic learning for
two free parameters, α and β, and the ε-greedy and the             this study.
ε-infomax models have one additional parameter, ε. We
implemented the KG, ε-greedy and ε-infomax models as
Bayesian graphical models under both learning frameworks.                                                                 Results
We used a vague prior for the environmental parameters,
Pr (α, β) = (α + β)5/2 , as suggested by Gelman et al. (2004),     Model Agreement with the Optimal Solution
because it is uniform on the psychologically interpretable
reparameterization, α/ (α + β) and (α + β)−1/2 . We used           As shown in Figure 2, the KG algorithm, under either learn-
uniform prior for ε. Model inference used combined sam-            ing framework, is able to approximate the optimal solution
pling algorithm, with Gibbs sampling of ε, and Metropo-            well in terms of the average number of correct predictions.
lis sampling of α and β. All chains contained 3000 steps,          In this sense, the KG policy is ‘process optimal’. ε-infomax
with a burn-in size of 1000. All chains converged according        outperforms the ε-greedy model, which implies that smarter
the R-hat measure (Gelman et al., 2004). We calculated the         exploration for information gain increases the optimality of
model agreement as the proportion of same choices between          the heuristic. The simple WSLS model achieves model
the model and the data, based on the full posterior predic-        agreement well above 60%. In fact, both WSLS and the opti-
tive distribution of choices given each observed state of the      mal model do win-stay with probability 1. The only situation
game. For this study, we implemented the optimal model             that WSLS does not resemble the optimal behavior is when
only with basic learning because of the heavy computational        it shifts away from an arm that the optimal solution would
load.                                                              otherwise stay with.
                                                                 1650

                         P(stay|win)                P(shift|lose)                 P(best value)              P(least known)
      WSLS
      Human                                   1                             1
                 1                                                                                    0.6
      KG meta
      Optimal                               0.8
      KG basic                                                            0.8                         0.4
               0.8                          0.6
                                            0.4                           0.6
                                                                                                      0.2
               0.6
                                            0.2                           0.4
                  3                    15      3                    15       3                   15      3                  15
                             Trial                     Trial                          Trial                       Trial
Figure 4: Behavioral patterns in the human data and the simulated data from a selection of the best- and worst-performing
models. The four panels show the trial-wise probability of win-stay, lose-shift, choosing the greatest estimated value, and
choosing the least known when it is an exploration trial, respectively. Probabilities are calculated based on simulated data
from each model at their MAP estimate, and are averaged across all games and all participants. The optimal solution shown
here uses the correct prior Beta (2, 2).
Model Agreement with the Human Data                                  cess will cause the environment to appear more rewarding,
Figure 3 shows the average model agreement with human                making other arms more likely to surpass the current best
data. Overall, the type of decision policy, other than the           arm.
learning framework, makes significant differences in the                The second panel shows the trialwise probability of shift-
model agreement. However, a decision policy tends to do              ing away following a previous failure. People, the optimal
better under the meta learning framework — the ε-greedy              solution, and KG show a decline in this probability over trial.
model and the KG model have significantly greater model              When the horizon is approaching, it becomes increasingly
agreement with meta learning.                                        important to stay with the arm that is known to be reasonably
   We next break down the overall behavioral performance             good, even if it may occasionally yield in a failure, because
into four finer measures: how often people adopt win-stay            it is increasingly important to maximize the reward on the
and lose-shift, how often they exploit, and whether they use         current trial.
random selection or search for the greatest amount of infor-            In general, the KG model with meta learning matches the
mation during exploration. We compare three of our models            second-order trend of human data. However, there still exists
that have the highest agreement with human data on these ad-         a big difference on the absolute scale, especially regarding
ditional behavioral criteria. Figure 4 shows the model anal-         the probability of staying with ‘good’ arms — in fact, the
ysis results. We show the patterns of the human subjects, the        KG policy does win-stay and exploitation more often, and
optimal solution, the best performing decision policy (KG)           resembles the optimal solution more than the human data.
under both learning frameworks, and the simplest WSLS.               Model Performance in Cumulative Reward
   The first panel probably contains the most interesting re-
                                                                     Collection
sults. It shows the trialwise probability of staying with the
same arm following a previous success. People show clear             Fig 5 shows a comparison of the distribution of average re-
sub-optimality by not staying with the same arm after an im-         ward per trial achieved by the participants, the optimal so-
mediate reward. In fact, obtaining a reward from any arm             lution, and the knowledge gradient model. When playing at
should always increase the estimated value of the chosen             their best fit parameterization based on the human data, KG
arm. Under the basic learning framework where unchosen               with meta learning and WSLS achieve nearly identical re-
arms do not change in value, this means the optimal deci-            ward distributions as the participants. Moreover, if we let
sion process should always do win-stay. This is consistent           KG with meta learning forward play under the correct prior
with the curve of the optimal solution. As implied by Equa-          knowledge of the environment, i.e. Beta (2, 2), it is able to
tion 1, KG considers the likelihood of an arm surpassing the         achieve a nearly identical distribution as the optimal solu-
known best value upon chosen, and weights this knowledge             tion.
gain more heavily in the early stage of the game. In general,
during the early trials, it chooses the second-best arm with a                                 Discussion
certain probability, not necessarily depending on the previ-         Our analysis supports the KG decision policy under the meta
ous outcome. This explains the drop of the win-stay proba-           learning framework as a good fit to human data in bandit
bility of KG during the early trials. When the learner is also       problems. Our result implies that people might learn the in-
updating its knowledge of the environment, a previous suc-           dividual reward rates as well as the general environment,
                                                                  1651

                 People
                                                                                                 References
                 Optimal                                                  Banks, J., Olson, M., & Porter, D. (2013). An experimental
                 KG B(.1, .8)
                 WSLS
                                                                                analysis of the bandit problem. Economic Theory, 10,
                 Random                                                         55–77.
                 KG B(2, 2)                                               Cohen, J. D., McClure, S. M., & Yu, A. J. (2007). Should
     Frequency
                                                                                I stay or should I go? Exploration versus exploitation.
                                                                                Philosophical Transactions of the Royal Society B: Bi-
                                                                                ological Sciences, 362, 933-942.
                                                                          Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B., &
                                                                                Dolan, R. J. (2006). Cortical substrates for exploratory
                                                                                decisions in humans. Nature, 441, 876-879.
                                                                          Frazier, P., Powell, W., & Dayanik, S. (2008). A knowledge-
                                                                                gradient policy for sequential information collection.
           0.4         0.45      0.5   0.55   0.6   0.65   0.7   0.75
                                Average Reward per Trial                        SIAM Journal on Control and Optimization, 47, 2410-
                                                                                2439.
                                                                          Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B.
Figure 5: Average reward achieved by the KG model for-
                                                                                (2004). Bayesian data analysis (2 ed.). Boca Raton,
ward playing the bandit problems with the same reward
                                                                                FL: Chapman & Hall/CRC.
rates. KG achieves similar reward distribution as the hu-
                                                                          Gittins, J. C. (1979). Bandit processes and dynamic alloca-
man performance, with KG playing at its maximum a pos-
                                                                                tion indices. Journal of the Royal Statistical Society,
teriori probability (MAP) estimate, α = .1 and β = .8. KG
                                                                                41, 148-177.
achieves the same reward distribution as the optimal solu-
                                                                          Kaebling, L. P., Littman, M. L., & Moore, A. W. (1996). Re-
tion when playing with the correct prior knowledge of the
                                                                                inforcement learning: A survey. Journal of Artificial
environment.
                                                                                Intelligence Research, 4, 237-285.
                                                                          Lee, M. D., Zhang, S., Munro, M., & Steyvers, M. (2011).
                                                                                Psychological models of human and optimal perfor-
and the shared, latent environment induces a special type
                                                                                mance in bandit problems. Cognitive Systems Re-
of correlation among the bandit arms. The meta learning
                                                                                search, 12, 164-174.
framework is a psychologically sensible improvement to ba-
                                                                          Powell, W., & Ryzhov, I. (2012). Optimal learning (1 ed.).
sic learning, because correct knowledge of the environment
                                                                                Wiley.
can be critical for achieving the best performance, especially
                                                                          Robbins, H. (1952). Some aspects of the sequential design of
when the environment can change over time or contexts. For
                                                                                experiments. Bulletin of the American Mathematical
the decision component, our results support the KG policy,
                                                                                Society, 58, 527-535.
which optimizes the semi-myopic goal of maximizing future
                                                                          Ryzhov, I., Powell, W., & Frazier, P. (2012). The knowledge
cumulative reward while assuming only one more time step
                                                                                gradient algorithm for a general class of online learn-
of exploration and strict exploitation thereafter (but does not
                                                                                ing problems. Operations Research, 60, 180-195.
actually ever carry out that policy). The KG model under
                                                                          Steyvers, M., Lee, M. D., & Wagenmakers, E.-J. (2009). A
the more general learning framework has the largest pro-
                                                                                bayesian analysis of human decision-making on ban-
portion of correct predictions of human data, and can cap-
                                                                                dit problems. Journal of Mathematical Psychology,
ture the trial-wise dynamics of human behavioral reasonably
                                                                                53, 168-179.
well. KG achieves similar behavioral patterns as the optimal
                                                                          Sutton, R. S., & Barto, A. G. (1998). Reinforcement learn-
model, and is computationally tractable, making it a plausi-
                                                                                ing: An introduction. Cambridge, MA: MIT Press.
ble algorithm for human learning and decision-making
                                                                          Yu, A. J., & Cohen, J. D. (2009). Sequential effects: Super-
  One remaining puzzle why human subjects tend to explore                       stition or rational behavior? In Advances in neural in-
more often than policies that optimize the specific utility of                  formation processing systems (Vol. 21, p. 1873-1880).
the bandit problems. One possibility is that people believe                     Cambridge, MA.: MIT Press.
the task environment can undergo stochastic changesand ex-                Zhang, S., & Lee, M. D. (2010). Cognitive models and
hibit sequential effects due to recent trial history, as in many                the wisdom of crowds. In N. Taatgen & H. van Rijn
other psychological task contexts (Yu & Cohen, 2009) . This                     (Eds.), Proceedings of the 32th annual conference of
would be an interesting line of future inquiry.                                 the cognitive science society. Austin, TX.
                                Acknowledgments
We thank Mark Steyvers and Eric-Jan Wagenmakers for
sharing the data on bandit problems for our analysis, and
three anonymous reviewers for their valuable comments that
helped improve the paper.
                                                                        1652

