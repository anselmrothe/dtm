UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
General Instruction Following in a Large-Scale Biologically Plausible Brain Model
Permalink
https://escholarship.org/uc/item/6p45n14b
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Choo, Xuan
Eliasmith, Chris
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                       University of California

General Instruction Following in a Large-Scale Biologically Plausible Brain Model
                                                    Xuan Choo (fchoo@uwaterloo.ca)
                                              Chris Eliasmith (celiasmith@uwaterloo.ca)
                                        Center for Theoretical Neuroscience, University of Waterloo
                                                        Waterloo, ON, Canada N2L 3G1
                                Abstract                                  Y”, “add 1 to the value in WM area X”). Apart from motor and
                                                                          cognitive commands, actions can also be utilized to change
   We present a spiking neuron brain model implemented in
   318,870 LIF neurons organized with distinct cortical modules,          the values of the model’s states.
   a basal ganglia, and a thalamus, that is capable of flexibly fol-         Rules are conditional statements typically of the form “IF
   lowing memorized commands. Neural activity represents a                X, THEN Y” (e.g. “If you see a 1, then push button A”) where
   structured set of rules, such as “If you see a 1, then push button
   A, and if you see a 2, then push button B”. Synaptic connec-           X is a set of conditions which have to be met for the set of
   tions between these neurons and the basal ganglia, thalamus,           actions Y to be executed. More generally, in Spaun, rules are
   and other areas cause the system to detect when rules should           statistical maps between cortical states and actions.
   be applied and to then do so. The model gives a reaction time
   difference of 77 ms between the simple and two-choice reac-               An instruction is a combination of rules or actions that
   tion time tasks, and requires 384 ms per item for sub-vocal            can be executed sequentially (e.g. “Remember the number 1;
   counting, consistent with human experimental results. This is
   the first biologically realistic spiking neuron model capable of       add 1 to that number; write the result”) or in any order (e.g.
   flexibly responding to complex structured instructions.                “If you see a 1, then push button A; If you see a 2, then push
   Keywords: neural engineering; spiking neuron model; in-                button B”).
   struction following; instruction processing; cognitive con-
   trol; cognitive architectures                                                                       Spaun
                           Introduction                                   The architecture of Spaun (the Semantic Pointer Architecture,
                                                                          or SPA) is composed of 9 distinct but interconnected modules
One of the hallmarks of complex cognition is the ability to
                                                                          (see Figure 1A). Of interest to this paper is how the action
perform a multitude of tasks using the same underlying ar-
                                                                          selection module interacts with the rest of the model. Fun-
chitecture. When given an instruction, the human brain is
                                                                          damentally, the action selection module of Spaun is identical
capable of processing and executing the instruction without
                                                                          to the basal ganglia (BG) based production system described
the need for extensive rewiring of the underlying neural con-
                                                                          in (Stewart, Bekolay, & Eliasmith, 2012), and functions sim-
nections. As far as we are aware, no neural model to date has
                                                                          ilarly to the action selection component of production system
been shown to exhibit this ability.
                                                                          models (e.g. (Anderson, 1996)).
   Eliasmith et al. (2012) describes what is currently the
                                                                             In these systems, action selection is hard-coded by a pre-
world’s largest functional brain model. While the model,
                                                                          defined set of rules. To select an action, the BG monitors
called Spaun (for Semantic Pointer Architecture Unified Net-
                                                                          internal cortical state variables and executes a rule whose an-
work), is able to perform 8 different cognitive tasks with-
                                                                          tecedent best matches the values of the internal state variables
out necessitating changes to its architecture, the knowledge
                                                                          (see Figure 1B). Critically, to encode instructions, the tran-
needed to complete these 8 tasks is hard-coded into the ac-
                                                                          sitions between each rule in the instruction has to be hard-
tion selection mechanism (the basal ganglia) of the model,
                                                                          coded into the BG as well. For example, if the instruction
making it unable to perform any task other than the prede-
                                                                          was to perform ACTION-A followed by ACTION-B, and then
fined 8. In this paper, we propose an extension to the Spaun
                                                                          ACTION-C, the following rules would have to be encoded
action selection component making it capable of processing
                                                                          into the BG:
generic instructions.
                           Terminology                                          IF INIT, THEN state = ACTION-A
Four key concepts are discussed in this paper: states, actions,                 IF state = ACTION-A, THEN state = ACTION-B
rules, and instructions.                                                        IF state = ACTION-B, THEN state = ACTION-C
   States are internal variables that the action selection sys-
tem monitors to figure out what is the best action to perform.            Several ACT-R models (e.g. (Taatgen & Lee, 2003),
States can be both internal (e.g. goal memories, working                  (Taatgen, 1999)) able to follow instructions, however no neu-
memories (WM)) and external (e.g. visual input) to the sys-               ral implementation has been previously discussed.
tem.                                                                         Aside from its architecture, Spaun is also unique in the way
   Actions are atomic commands within the architecture, and               information is represented. Information is encoded and rep-
are typically motor commands (e.g. “write the number X”,                  resented using semantic pointers (Eliasmith, In Press). These
“push the X button”) or cognitive commands (e.g. “remember                representations are used in the SPA to define a type of vec-
the word X”, “route information from WM area X to WM area                 tor symbolic architecture (VSA). In typical VSAs, the vector
                                                                      322

                                                                                                  that binding A with A∗ results in approximately the identity
   A                                         Working Memory
                                                                                                  vector I (A ~ A∗ ≈ I). This property of the approximate in-
                                                                                                  verse can be used to unbind previously bound vectors.
                                                                                                     Both the superposition and binding operations are anal-
   Visual         Information  Transform         Reward       Information   Motor      Motor      ogous to addition and multiplication in scalar mathematics,
   Input            Encoding  Calculation      Evaluation       Decoding  Processing   Output
                                                                                                  and are often associative, commutative, and distributive.
                                                                                                     In the SPA, vector addition is used for superposition, and
                                                                                                  circular convolution is used for binding, bearing close sim-
                                             Action Selection
                                                                                                  ilarity to the Holographic Reduced Representation (Plate,
                                                                                                  2003).
   B
          Vision Module
                                                                                                  Encoding Instructions
                                                                                                  Instructions are encoded using a positional encoding schema
                                            Action Selection
                                          IF ACTION 1 THEN
                                                                                                  similar to that used in Spaun and in the Ordinal Serial Encod-
       Working Memory
                                          IF ACTION 2 THEN
                                                                                                  ing model of serial working memory (Choo, 2010). Each rule
             STATE 1
                                          IF ACTION 3 THEN
                                                                                                  in the instruction is tagged (bound) to a position vector to in-
             STATE 2                                                         Motor Module
                                                                                                  dicate its relative order within the instruction. For example,
                                                                                                  the instruction “1. RULE1; 2. RULE2” is encoded as
             STATE N                      IF ACTION N THEN
                                                                                                             INSTR = P1 ~ RULE1 + P2 ~ RULE2
Figure 1: A) High-level architecture of Spaun. B) Method by
which Spaun chooses an action. The action selection system                                        where P1 and P2 are the position vectors. Importantly, since
monitors cortical state variables (solid arrows), selects an ac-                                  the position vectors are also semantic pointers, the position
tion that bests matches these states, and effects the action on                                   vectors have some relation. That is to say P2 = P1 ~ ADD1,
efferent modules (dotted arrows).                                                                 and likewise for subsequent position vectors.
                                                                                                     Individual rules in the instruction are encoded as a super-
that represents the number ONE and the vector that repre-                                         position of the conditions that make up the antecedent and
sents the number TWO would be chosen from a random dis-                                           the actions that make up the consequence of the rule. For ex-
tribution and thus have no direct relation to each other. In the                                  ample, the rule “IF STATEA THEN ACTIONB” is encoded
SPA however, the semantic pointer for the number TWO is                                           as
computed as the bound combination of the semantic pointer                                                     RULE = ant(STATEA) + ACTIONB
ONE with a vector that represents the concept ADD1, thus
                                                                                                  where ant() is a randomly generated linear operator applied
imparting semantic meaning to each vector:
                                                                                                  to the STATEA vector that serves to disambiguate the an-
                              TWO = ONE ~ ADD1                                                    tecedent and consequent components of the rule.
                                                                                                     State conditions are encoded by binding vectors that de-
Similarly, the semantic pointer for the number THREE can                                          scribe the state being monitored with the state value required
be computed as follows:                                                                           for the rule to be executed. Thus, the state condition “state =
                       THREE = TWO ~ ADD1                                                         A” is constructed as
                                    = ONE ~ ADD1 ~ ADD1                                                               STATEA = STATE ~ A.
                   Vector Symbolic Architectures                                                  Other conditions can also be combined in this state represen-
Vector symbolic architectures have four core properties. First,                                   tation. For example, if the state conditions was “vision = 3
information is represented by high-dimensional vectors usu-                                       and state = A” (i.e. looking at a 3 while in state A), then the
ally chosen from a random distribution.                                                           state representation would be
   Second, vectors can be combined using a superposition
operation (denoted with a +). Of note, the vector result of                                              VIS3&STATEA = VISION ~ 3 + STATE ~ A.
the superposition operation is similar to the original vector
operands, where similarity is measured by a dot product.                                             Actions are encoded by combining the bound result of an
   Third, vectors can be bound together using a binding oper-                                     “action” descriptor with the specific action to be performed
ation (denoted with a ~). Unlike the superposition operator,                                      with an optional bound result of a “data” descriptor with the
the vector result of the binding operation is dissimilar to the                                   specific data to be used with the action. A “write the number
original vector operands.                                                                         2” action is thus represented as
   Last, an approximate inverse operator (denoted with ∗ ,
such that A∗ is the approximate inverse of A) is defined such                                             WRITE2 = ACTION ~ WRITE + DATA ~ 2.
                                                                                              323

   Combining all of the representations above, the full encod-        Conditionally Responsive Decoding of Instructions An
ing of an instruction can be demonstrated. As an example the          instruction can also be decoded using the values of the state
instruction:                                                          conditions. In order to do so, the value of the state condi-
                                                                      tion(s) is bound to its associated state descriptor(s), and the
            1. IF vision = 0, THEN push button A                      inverse of this result is bound to the instruction vector to
            2. IF vision = 1, THEN push button B                      yield the position of the rule that best matches the state con-
                                                                      dition(s). Using Equation 1 as an example, if the vision state
is encoded as                                                         condition had a value of 1, the position of the rule that best
                                                                      matches this can be found like so:
  INSTR = P1 ~ [ant(VISION ~ 0)+
                    ACTION ~ PUSH + DATA ~ BTNA]+                       pos = INSTR ~ (ant(state) ~ state val)∗                       (5)
                                                                                                                    ∗
             P2 ~ [ant(VISION ~ 1)+                                          = INSTR ~ (ant(VISION) ~ 1)
                    ACTION ~ PUSH + DATA ~ BTNB] (1)                         = P1 ~ [ant(VISION ~ 0)...] ~ (ant(VISION) ~ 1)∗ +
                                                                                 P2 ~ [ant(VISION ~ 1)...] ~ (ant(VISION) ~ 1)∗
It is important to note that at the end of this computation,                 ≈ P2 ~ [I + ...] ≈ P2
the instruction is encoded as a single vector with the same
dimensionality as the original atomic components.                     Once the position vector has been retrieved, the sequential
                                                                      instruction decoding equations can then be used to obtain the
Decoding Instructions                                                 action and data associated with the rule.
With the instruction encoding schema presented above, the
instructions can be decoded in one of two ways: by using              The Model
positional information, and by using the values of the states         With the ability to encode and decode general instructions,
in the system.                                                        modifying the existing Spaun action selection module to take
Sequential Decoding of Instructions A rule associated                 advantage of this is straightforward. It only entails the addi-
with a specific position within the instruction can be retrieved      tion of a instruction processing module that implements the
by binding the instruction vector with the inverse of the posi-       instruction decoding equations (Eq 2 – 5) above. The output
tion vector.                                                          of this module then become new state variables which the ac-
                                                                      tion selection system monitors when selecting an appropriate
       rule = INSTR ~ P1∗                                     (2)     action (see Figure 2).
                 ∗                       ∗
            = P1 ~ P1 ~ RULE1 + P1 ~ P2 ~ RULE2
                                                                              Instruction           Vision Module
            = I ~ RULE1 + P1∗ ~ P2 ~ RULE2                                    Processing
                                                                                Module
            ≈ RULE1
                                                                                                   Action Selection
   Given the rule vector, it is possible to retrieve information           Working Memory        IF ACTION 1 THEN
related to the consequent by binding it with the inverse of the                 STATE 1          IF ACTION 2 THEN
“action” descriptor or the “state” descriptor.                                                   IF ACTION 3 THEN
                                                                                STATE 2                                   Motor Module
                                 ∗
      action = rule ~ ACTION                                  (3)
                                                                               STATE N           IF ACTION N THEN
             = [ant(VISION ~ 0) + ACTION ~ PUSH+
                  DATA ~ BTNA] ~ ACTION∗
                                                                      Figure 2: Proposed modification to Spaun’s action selection
             ≈ I ~ PUSH = PUSH                                        system with the addition of an instruction processing module
                                                                      (italicized). As in Figure 1, state monitoring is indicated with
Likewise,
                                                                      a solid arrow, and action effects with a dotted arrow.
       data = rule ~ DATA∗                                    (4)
                                                                         Validation of the model comes in the form of behavioural
            = [ant(VISION ~ 0) + ACTION ~ PUSH+                       analysis as well as matching the model dynamics to human
                 DATA ~ BTNA] ~ DATA∗                                 timing data. The model is implemented with spiking neurons
            ≈ I ~ BTNA = BTNA                                         and biologically realistic synaptic time constants in order to
                                                                      generate realistic temporal dynamics.
   After the rule has been executed, the next rule can be
computed by incrementing the position vector (P2 = P1 ~                                   Neural Representation
ADD1)) and repeating Equations 2, 3 & 4 with this new po-             Fundamental to the SPA is the vector-based representation
sition vector.                                                        of information. We use methods of the Neural Engineer-
                                                                  324

ing Framework (NEF) to accomplish this in spiking neu-                  Neural Implementation
rons (Eliasmith & Anderson, 2003). Georgopoulos et al.                  The model proposed here relies on two key functions: the
(1986) demonstrated that motor neurons are well character-              binding operation and working memory.
ized as having responses driven by their preferred direction               The binding operation is performed by a two step process.
to movement in two dimensions. The NEF generalizes this                 First the Fourier transform (FT) of both input vectors is com-
notion to suggest that neurons can represent any number of              puted, and these are multiplied element-wise. Performing an
dimensions, and the neuron’s preferred direction determines             inverse Fourier transform (IFT) on this result provides the de-
its activity with regards to its input in a given vector space.         sired answer. That is,
Mathematically, the current J flowing into a neuron can be
calculated using as                                                                     A ~ B = IFT (FT (A)     FT (B)),           (12)
                      J(x) = α(e · x) + J bias ,                (6)     where is the element-wise multiplication operation. The
                                                                        FT, IFT and element-wise multiplication are functions that
where α and J bias are neuronal scaling terms, e is the neuron’s
                                                                        can be computed by spiking neurons using the methods dis-
preferred direction (or encoding vector), and x is the vector
                                                                        cussed in the previous section (see Equation 11).
to be represented. The inner product computes the similarity
                                                                           The working memory component is identical to that used
between the encoding and input vector and determines how
                                                                        in Spaun. This component is implemented by a recurrent net-
much current is being fed to the neuron. The leaky integrate-
                                                                        work that is able to stably store information over time. The
and-fire (LIF) neuron model equation is then used to convert
                                                                        storage and retrieval of information is determined by gates
this current into a firing rate.
                                                                        controlled by the basal ganglia.
                                                1
           a(x) = G[J(x)] =                                   (7)                           Response Timing
                                                        Jth
                                 τre f − τRC ln 1 − J(x)
                                                                        In this section we compare the behaviour of the model to two
In the equation above, τre f is the neuron refractory time con-         different tasks: the choice reaction time task, and a sub-vocal
stant, τRC is the neuron RC time constant, and Jth is the neu-          counting task. The choice reaction time task demonstrates
ron threshold firing current. With a population of neurons, it          the model’s ability to perform unordered instructions, while
then possible to derive optimal decoding vectors that can be            the sub-vocal counting task demonstrates the model’s ability
used to convert the neural activity back into the high dimen-           to perform sequential instructions. Note that for both of these
sional vector space. Eliasmith and Anderson (2003) demon-               tasks, the architecture of the model remains the same, with the
strate how these decoders d can be computed.                            only difference being the instruction vector and visual stimuli
                                                                        that it is required to process.
            d = Γ−1 ϒ, where
                 Z                                Z             (8)     Conditionally Responsive Decoding - Two-Choice
          Γi j =   ai (x)a j (x) dx        ϒi =     ai (x)x dx          and Simple Reaction Time Task
                                                                        To test the model’s ability to account for human instruction
An estimate of the original vector x can then be generated by
                                                                        processing time, it was tested with the two-choice (CRT)
multiplying each neuron’s decoding vector with its activity.
                                                                        and simple reaction time (SRT) tasks described in Grice,
                           x̂ = ∑ai (x)di                       (9)     Nullmeyer, & Spiker (1982). Since the input stimuli and mo-
                                  i                                     tor action performed are similar in both of these tasks, any
The encoding and decoding vectors can also be used to de-               difference in reaction time can be attributed to the speed at
termine the optimal connection weights between two neural               which the different instructions are processed.
populations.                                                               In the two-choice reaction time task, the subject is in-
                            wi j = α j e j di                  (10)     structed to push one of two buttons, the identity of which
                                                                        is indicated by some sort of visual stimuli. To simulate this
Taking into account a specific function while solving for the           with the general instruction following model, it is given the
decoding vectors yields the set of connection weights that will         instruction:
cause the neurons in the post-synaptic population to compute
said function. For example,                                                1. IF vision = ZERO, THEN state = Push, motor = A
                                              f                            2. IF vision = ONE, THEN state = Push, motor = B
                         (x) = ∑ai (x)di ,
                        fd                                     (11)
                                    i
                                                                        Figure 3 demonstrates the model performing this instruction.
where d f are the decoding vectors solved with the function f              In the simple reaction time task, the subject is instructed
incorporated into Equation 8. In other words, these equations           to push a single button in response to a single stimulus. This
allow us to build a spiking neuron model that performs ar-              task requires no instruction processing so the rule:
bitrary specified computations. See Eliasmith and Anderson
(2003) for additional details.                                               IF vision = TWO, THEN state = Push, motor = C
                                                                    325

                                             ZERO                                                        ONE
                                                          PUSH                                                   PUSH
                                                           A                                                       B
Figure 3: Neural response data for the two-choice reaction time task. Shown are the decoded representations for two neural
populations (an internal state memory, and the motor output), and the visual stimulus provided. Also displayed is the spiking
neural data associated with each of the neural populations. Note that only the cognitive components (i.e. no input stimuli
processing lag nor motor lag) of the reaction time task are being simulated in this model.
is encoded directly in the basal ganglia. By doing so, the          Simulation Details
model is able to execute the desired action when presented          In total the model is made up of 318,870 spiking LIF neu-
with the appropriate stimuli without requiring any additional       rons, and uses 256-dimensional semantic pointers. It should
processing in the instruction processing module.                    be noted that Spaun utilizes semantic pointers with 512 di-
   The model reports a reaction time difference of 77 ± 34          mensions, and this was reduced for this model to decrease
ms between the two tasks, while Grice, Nullmeyer, & Spiker          the amount of time required to simulate the experiments. It
report a reaction time difference of 81 ± 72 ms for human           takes 275 ± 25 seconds of CPU time to simulate 1 second of
subjects.                                                           simulation time on a machine with a 3.40 GHz Core i7-3770
                                                                    quad-core CPU and 16 GB of RAM.
Sequential Decoding - Sub-vocal Counting
                                                                                              Discussion
For this task, the model was given a sub-vocal counting in-
struction. This instruction is formatted as sequence of ac-         The model presented in this paper demonstrates the ability to
tions, and thus have no antecedent.                                 process and execute generic instructions without needing any
                                                                    changes to the underlying architecture. It is also able to repro-
              1. memory = Store, data = X                           duce response times in human reported ranges based purely
                                                                    on the temporal dynamics of the underlying neural implemen-
              2. state = Add1                                       tation – without the need for data fitting of any kind.
              3. state = Write, motor = memory                         Because the model utilizes semantic pointers to represent
                                                                    information, it is also highly scalable. The maximum number
In the instruction above, the variable X is a vector represent-     of concepts the model is able to represent is dependent on the
ing a digit from 0 to 9. Instructions requiring more than one       dimensionality of the semantic pointer used, and not on the
count (e.g. add 1 twice), have the second action repeated the       number of knowledge nodes present in the model. Crawford,
appropriate number of times (and appropriately renumbered).         Gingerich and Eliasmith (Crawford, Gingerich, & Eliasmith,
Figure 4 illustrates the model peforming the sub-vocal count-       2013) demonstrate that the entirety of WordNet (117,659 con-
ing task for one count.                                             cepts) can be represented using 512 dimensional semantic
   The mean reported count time per item is 384 ± 29 ms             pointers. Increasing the proposed model to utilize 512 di-
which falls well between the reported human range of 344 ±          mensional semantic pointers would add an additional 287,488
135 ms (Landauer, 1962), and provides a much better match           neurons to the model.
to the human data than Spaun’s reported count time per item            One major limitation to this model, however, is its inabil-
time of 419 ± 10 ms (Eliasmith et al., 2012).                       ity to learn frequently executed instructions. In essence, even
                                                                326

                                 SEQ
                                                          ADD
                                           STORE                  ADDING         ADD                  WRITE
                                                            ONE                                    TWO
                                                                                                         TWO
Figure 4: Neural response data for the sub-vocal counting task. Shown are the decoded representations for three neural pop-
ulations (an internal state memory, working memory (WM), and the motor output), and the visual stimulus provided. Also
displayed is the spiking neural data associated with each of the neural populations. Note that the ADD value for the state
variable indicate both the start and end of the number addition action.
if it is presented with multiple instances of the same instruc-         ing: computation, representation, and dynamics in neuro-
tion, it is unable to form an expert action for that instruction.       biological systems. Cambridge, MA: The MIT Press.
This issue is currently being investigated and integrating this       Eliasmith, C., Stewart, T. C., Choo, X., Bekolay, T., Dewolf,
ability in the proposed model remains as future work.                   T., Tang, Y., et al. (2012). A large-scale model of the
   This paper also makes no mention of how the model                    functioning brain. Science, 338(6111), 1202–1205.
could construct a new instruction vector given purely a vi-           Georgopoulos, A. P., Schwartz, A., & Kettner, R. E. (1986).
sual stream of words or symbols. Concurrent work done by                Neuronal population coding of movement direction. Sci-
Stewart and Eliasmith (Stewart & Eliasmith, 2013) provides              ence, 233, 1416–1419.
insight on how this issue can be made tractable.                      Grice, G. R., Nullmeyer, R., & Spiker, V. A. (1982). Hu-
                                                                        man reaction time: Toward a general theory. Journal of
                     Acknowledgments                                    Experimental Psychology: General, 111(1), 135–153.
                                                                      Landauer, T. K. (1962). Rate of implicit speech. Perceptual
This research was supported by the Natural Sciences and En-
                                                                        and Motor Skills, 15, 646.
gineering Research Council of Canada, the Canadian Foun-
                                                                      Plate, T. A. (2003). Holographic reduced representations:
dation for Innovation, and the Ontario Innovation Trust.
                                                                        distributed representations for cognitive structures. Stan-
                                                                        ford, CA: CSLI Publications.
                          References                                  Stewart, T. C., Bekolay, T., & Eliasmith, C. (2012). Learning
Anderson, J. R. (1996). Act: A simple theory of complex                 to select actions with spiking neurons in the basal ganglia.
   cognition. American Psychologist, 51, 355–365.                       Frontiers in Decision Neuroscience, 6(2).
Choo, X. (2010). The ordinal serial encoding model: Serial            Stewart, T. C., & Eliasmith, C. (2013). Parsing sequentially
   memory in spiking neurons. Unpublished master’s thesis,              presented commands in a large-scale biologically realistic
   The University of Waterloo.                                          brain model. In Proceedings of the 35th annual conference
Crawford, E., Gingerich, M., & Eliasmith, C. (2013). Biolog-            of the cognitive science society.
   ically plausible, human-scale knowledge representation. In         Taatgen, N. A. (1999). A model of learning task-specific
   Proceedings of the 35th annual conference of the cognitive           knowledge for a new task. In Proceedings of the twenty-
   science society.                                                     first annual conference of the cognitive science society (pp.
Eliasmith, C. (In Press). How to build a brain: A neural ar-            730–735). Mahwah, NJ: Erlbaum.
   chitecture for biological cognition. New York, NY: Oxford          Taatgen, N. A., & Lee, F. J. (2003). Production composition:
   University Press.                                                    A simple mechanism to model complex skill acquisition.
Eliasmith, C., & Anderson, C. H. (2003). Neural engineer-               Human Factors, 45(1), 61–76.
                                                                  327

