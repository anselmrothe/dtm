UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Getting to the Point: The Influence of Communicative Intent on the Kinematics of Pointing
Gestures

Permalink
https://escholarship.org/uc/item/51j2106d

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Peeters, David
Chu, Mingyuan
Holler, Judith
et al.

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Getting to the Point: The Influence of Communicative Intent on the Kinematics of
Pointing Gestures
David Peeters (david.peeters@mpi.nl)
Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands
International Max Planck Research School for Language Sciences, Nijmegen, The Netherlands
Radboud University, Donders Institute for Brain, Cognition, and Behaviour, Nijmegen, The Netherlands

Mingyuan Chu (mingyuan.chu@mpi.nl)
Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands

Judith Holler (judith.holler@mpi.nl)
Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands
School of Psychological Sciences, University of Manchester, UK

Aslı Özyürek (asli.ozyurek@mpi.nl)
Radboud University, Center for Language Studies, Nijmegen, The Netherlands
Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands

Peter Hagoort (peter.hagoort@mpi.nl)
Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands
Radboud University, Donders Institute for Brain, Cognition, and Behaviour, Nijmegen, The Netherlands

Abstract
In everyday communication, people not only use speech but
also hand gestures to convey information. One intriguing
question in gesture research has been why gestures take the
specific form they do. Previous research has identified the
speaker-gesturer’s communicative intent as one factor
shaping the form of iconic gestures. Here we investigate
whether communicative intent also shapes the form of
pointing gestures. In an experimental setting, twenty-four
participants produced pointing gestures identifying a referent
for an addressee. The communicative intent of the speakergesturer was manipulated by varying the informativeness of
the pointing gesture. A second independent variable was the
presence or absence of concurrent speech. As a function of
their communicative intent and irrespective of the presence of
speech, participants varied the durations of the stroke and the
post-stroke hold-phase of their gesture. These findings add to
our understanding of how the communicative context
influences the form that a gesture takes.
Keywords: Pointing Gesture; Communicative Intent; Gesture
Production; Action Planning; Deixis.

Introduction
In everyday communication, people not only use speech but
also meaningful hand gestures to convey information. One
of the most intriguing questions in gesture research has been
why such gestures take the physical form they do (Bavelas
et al., 2008; Gerwing & Bavelas, 2004; Krauss, Chen, &
Gottesman, 2000). The main focus so far in answering this
question has been on gestures iconic in nature, i.e., gestures
that in form and manner of execution visually resemble the
simultaneously expressed meaning of the linguistic part of
an utterance (McNeill, 1985), such as moving up and down

one’s hand when talking about a basketball game. Typically,
such studies have varied aspects of the communicative
context, such as the visibility of gestures or the knowledge
speaker and listener mutually share. Amongst other things,
these studies have shown that speakers design their gestures
for particular recipients and produce more (e.g., Alibali,
Heath, & Myers, 2001; Bavelas et al., 2008) as well as
larger and more precise gestures when communicative
intentions are enhanced (e.g., Gerwing & Bavelas, 2004;
Holler & Stevens, 2007). Thus, iconic co-speech gestures
seem closely linked to the speaker’s specific communicative
intent, and the particular form an iconic gesture takes
depends on the context-bound communicative relation
between speaker and addressee (see Holler & Wilkin, 2011).
In contrast, it is unclear how the form of pointing gestures
changes as a function of the gesturer’s communicative
intent. Pointing is a foundational building block of human
communication (Kita, 2003) and allows us to directly
connect our communication to the material world that
surrounds us (Clark, 2003). Pointing has received much
attention in the literature from an ontogenetic viewpoint
because of its role in paving the way for the acquisition of
language (Butterworth, 2003; Carpenter, Nagell, &
Tomasello, 1998; Tomasello, Carpenter, & Liszkowski,
2007), as well as from a phylogenetic viewpoint with
respect to declarative pointing being a uniquely human form
of communication in a natural environment (Call &
Tomasello, 1994; Kita, 2003; Tomasello et al., 2007). In
contrast, the exact form parameters that people vary in the
production of pointing gestures in human adult
communication remain largely unknown. Therefore, the
present study aims at contributing further to our

1127

understanding of why adults’ index-finger pointing gestures
take the particular physical form they do in a
communicative context.
There are some preliminary indications that suggest a
relation between the form of a pointing gesture and the
speaker’s communicative intent. Cleret de Langavant et al.
(2011) had participants repeatedly point to objects on a table
in front of them. Two addressees were always sitting next to
the table. At the onset of a communicative block, the
participant was instructed to verbally address one of the
addressees before the block started and was instructed
before each trial of that block to point at a specific object for
that addressee, who named the object after observing the
participant’s gesture. At the onset of a non-communicative
block, the participant addressed nobody and was instructed
before each trial to point at a specific object without having
an addressee (and hence did not receive feedback from an
addressee). Compared to the latter, non-communicative
condition, the former condition yielded pointing gestures
that had a trajectory and endpoint distribution that were
tilted away from the addressee, arguably because the
addressee's perspective on the target object was taken into
account in the form of the gesture.
Everyday pointing gestures generally occur in a context in
which two interlocutors share a joint attentional frame in
which one person directs the attention of another person
towards a location, event, or referent in the perceptual
environment (Tomasello et al., 2007). An important
prerequisite for a successful referential pointing gesture is
that two interlocutors come to perceptually attend to the
same entity or location and are mutually aware of the fact
that they are both attending to the same thing (Clark, 1996).
Therefore, instead of comparing a “communicative”
situation (including addressing a listener and receiving
verbal feedback) to a “non-communicative” situation
(without addressing a listener and verbal feedback), as in
Cleret de Langavant et al. (2011), we here compare two
situations that are both communicative and differ only in the
communicative intent of the speaker-gesturer. As a proxy of
the communicative intent of the speaker-gesturer, we
manipulate the degree of informativeness of the pointing
gesture as a first factor in our design.
A second factor manipulated here is the presence of
speech as a second modality. Pointing gestures often come
with concurrent deictic speech such as spatial
demonstratives (e.g., “this” and “that” in English). In the
production of referring expressions, speech and gesture are
tightly interconnected (Kendon, 2004; McNeill, 1992) and
can be used independently or simultaneously to single out a
referent (e.g., Bangerter, 2004), in contrast with iconic
gestures that canonically come with speech. In the current
study we manipulate the presence of such a second modality
(speech) and explore the yet unaddressed question of
whether the mere presence of speech as a second modality
influences the form parameters people exploit in producing
pointing gestures for their addressee, and whether the

presence of speech interacts with our manipulation of
communicative intent.
The current study looks at different subcomponents (or:
parameters) of the pointing gesture. We focus on the
planning duration of the gesture, the duration of the stroke
and the post-stroke hold-phase, as well as the point in time
at which the apex is reached after the visual presentation of
a referent (Levelt, Richardson, & La Heij, 1985), and the
amount of distance travelled by the pointing finger. Finally
we also look at whether the synchronization of speech and
gesture changes as a function of communicative intent.

Method
Participants
Twenty-four right-handed native speakers of Dutch (12
female; mean age 20.6), studying at Radboud University
Nijmegen, participated in the experiment. They were
compensated with 20€.

Experimental Design and Set-up
Participants were seated at a distance of 100 cm from a
computer screen that was placed back-to-back with another
computer screen (henceforth: the back screen). Stimuli were
four white circles in a horizontal line on the top of the
screen, mirroring four circles on the back screen. The circles
could light up either blue or yellow. A second participant (a
confederate; henceforth: the addressee) looked at the back
screen and the participant’s pointing gesture via a camera.
Figure 1 shows the view of the addressee via the camera
(converted to a grayscale image). On all trials, participants
referred to the circle that lit up. The addressee noted on a
paper form to which of the four circles the participant

Figure 1: The addressee's view of the back screen and the
pointing participant during a non-informative trial.

1128

referred on each trial. In order to make the deictic act
informative in one case but non-informative in the other, the
following set-up was used. In both conditions, via a camera,
the addressee observed the pointing gesture of the
participant, as well as the circles at the back screen
providing the corresponding view of the four circles the
participant was seeing. This way, the addressee saw which
of the four circles the participant pointed at, but without
seeing which circle lit up on the participant’s side of the
screen, a crucial aspect in our manipulation (see below).
We manipulated the informativeness of the gesture
(informative versus non-informative) as well as the
modality of the deictic act (gesture-only versus gesture +
speech) in a 2x2 within participants design. In the
informative condition, a circle turned blue or yellow only on
the participants’ screen but not on the back screen.
Therefore the participant’s pointing gesture was the only
source of information on which the addressee had to base
his decision in selecting the circle referred to by the
participant. In the non-informative condition, the
corresponding circles would light up on both the
participant’s and the addressee’s screen. Thus, the
participant’s pointing gesture was non-informative, because
the addressee saw one of the circles light up on the back
screen at the same moment as the participant saw the
corresponding circle light up (i.e., even before the onset of
the participant’s pointing gesture).
The modality factor was manipulated by having
participants use either one or two modalities in referring to
the circles. In gesture only blocks (G-only), participants
pointed to a circle when it turned blue or yellow without
producing speech. In gesture + speech blocks (G+S)
participants pointed to the circle and said either die blauwe
cirkel (“that blue circle”) or die gele cirkel (“that yellow
circle”), depending on the color of the circle. Note that,
because on every trial only one circle turned blue or yellow,
the speech was never informative (neither in the informative
nor the non-informative blocks). The rationale for this was
that we were interested in the possible effect of the mere
presence of speech as a second modality, independently
from the informativeness of the deictic act that was
manipulated separately in the gesture.
Each trial started with a fixation cross, displayed for 500
ms, followed by the presentation of four white circles. After
a jittered period of 500-1000 ms, one of the circles turned
yellow or blue. At this point, the participant was allowed to
release her finger from a button, pointed to the blue or
yellow circle, and named the circle (in the G+S blocks). The
experiment consisted of 16 blocks of 20 trials each. Every
condition in the experiment was represented by four blocks.
The order of presentation of blocks was counterbalanced
across participants. In half of the trials a circle lit up yellow,
in the other half it lit up blue. The idea behind this was to
create a slightly more complex and varied utterances to
enhance the ecological validity in this very strictly
controlled environment. Each block of 20 trials consisted of
ten circles lighting up yellow and ten lighting up blue,

equally distributed over the four circles and the four
conditions throughout the experiment, in a randomized way.

Procedure
At the arrival of the participant, the experimenter explained
that a second participant (i.e., the confederate addressee)
would perform a behavioral task on the basis of the
participant’s gesture. The experimenter showed the
participant the computer and form to be used by the
addressee and demonstrated that the participant could be
seen on the computer screen via a camera. Also, it was
explained and shown to the participant that the addressee
could only see the arm movement of the participant and the
computer screen that was at the back of the computer screen
that the participant saw. The addressee could not see the
head of the participant, to avoid the participant from
conveying information via the head and face. In order to
keep participants motivated, it was emphasized that they
were in a joint activity with the addressee and that the
success of this joint activity depended on how well they
worked together. The participant was then seated in a
comfortable chair in the dimly-lit experiment room. The
height of the screen was adjusted to the height of the eyes of
the participant. The button used by the participant was
placed at the height of the participant’s elbow, 23 cm in
front of the participant calculated from the vertical axis
corresponding to the position of the participant’s eyes.
Participants were instructed to always rest their finger on
this button, except when making the pointing gesture, which
allowed calculating the duration and onset of the pointing
gesture. A sensor was placed on the participant’s right index
finger nail to allow for motion tracking of the pointing
movements. Participants’ electroencephalogram (EEG) was
recorded continuously throughout the experiment. These
results will be reported elsewhere.
After montage of the motion tracking sensor the
experimenter picked up the addressee. The addressee was
shown the room in which the participant performed the task,
greeted the participant, and was seated in a chair in front of
a computer in a room adjacent to the participant’s room. In
order to familiarize the participant with the different
conditions and the task, thirty-two test-items (eight per
condition) preceded the main experiment as a practice set.
Participants received specific instructions to point with or
without speech before each block. In addition, before each
block, the participant was instructed whether the addressee
could also see the same circles light up at the back screen or
not during that block. Participants were asked to only move
their hand and arm when pointing. During the experiment,
participants were allowed to have a short break after every
fourth block. Before and during the experiment, the
communication between experimenter and addressee was
minimal and fully scripted, in order to be constant across
participants. After the experiment, the addressee was
thanked for participation and left the room. Participants
were debriefed, financially compensated, and thanked for
participation.

1129

Kinematic recording and analysis
Behavioral and kinematic data were acquired throughout the
experiment using experimental software (Presentation,
Neurobehavioral Systems, Inc) and a 60 Hz motion tracking
system and DTrack2 tracking software (both Advanced
Realtime Tracking, Weilheim, Germany). For each trial, the
Gesture Initiation Time (i.e. the moment the participant’s
finger left the button calculated from the moment a circle lit
up) was calculated. This measure thus reflected the time it
took to plan the pointing gesture. In addition, we collected
for each trial the Apex Time (i.e. the moment of the
endpoint of the gesture calculated from the moment a circle
lit up). The endpoint of the gesture was defined as the point
in time where the pointing index finger was at least 7 cm
from the button and only moved forward less than 2 mm for
two consecutive samples. The Stroke Duration was defined
as the interval between the onset of the gesture (i.e., The
Gesture Initiation Time) and the moment the apex was
reached (i.e., the Apex Time). The Incremental Distance
travelled by the pointing index finger was calculated for the
complete stroke (similar to Levelt et al., 1985). Further, the
Velocity of the hand movement was calculated for each trial
on the basis of the Apex Time and the Incremental Distance.
The Hold Duration of the pointing gesture was calculated by
subtracting the Apex Time from the Retraction Time (i.e.,
the moment the index finger moved back in the direction of
the button for at least 2 mm in two consecutive samples). In
the G+S blocks, the Speech Onset Time was calculated from
the moment one of the circles lit up. The Synchronization
Time was defined as the difference between Apex Time and
Speech Onset Time.

Results
Trials on which the Gesture Initiation Time was below 100
ms or above 2000 ms were considered errors and excluded
from all analyses (0.7% of total dataset). In addition, trials
containing hesitations or errors in the participant’s speech
were removed from further analyses (0.2% of all data).
Separate analyses of variance were performed for each

dependent variable with Informativeness (Informative
versus Non-informative) and Modality (Gesture-only or
Gesture+Speech) as within-subject factors.
A first analysis was performed on the Gesture Initiation
Time. This analysis did not yield any significant main or
interaction effect. Next, we analyzed the Stroke Duration.
This analysis yielded a significant main effect of
Informativeness, F (1,23) = 10.97, p = .003, ηp2 = .32. This
effect denoted that the duration of the stroke was
significantly longer in the Informative condition (M = 837
ms) than in the Non-informative condition (M = 823 ms).
No significant main effect of Modality was found. There
was no significant interaction between the two factors. Also
an analysis on the Apex Time showed a significant main
effect of Informativeness, F (1,23) = 8.15, p = .009, ηp2 =
.26. This effect denoted that the apex was reached
significantly later in the Informative condition (M = 1379
ms) than in the Non-informative condition (M = 1359 ms).
No significant main effect of Modality was found. There
was no significant interaction between the two factors.
A further analysis was performed on the Incremental
Distance. No significant main or interaction effect was
found. Because the same amount of distance was travelled
across conditions, but the apex was reached later in the
Informative condition than in the Non-informative
condition, the velocity of the pointing gesture must have
been lower in the Informative condition compared to the
Non-informative condition. Indeed, an analysis on the mean
Velocity yielded a significant main effect of
Informativeness, F (1,23) = 5.75, p = .025, ηp2 = .20. The
velocity of the pointing gesture was significantly lower in
the Informative condition (M = 38.2 cm/s) than in the Noninformative condition (M = 38.7 cm/s). Again, no
significant main effect of Modality or interaction between
the two factors was found. Another analysis, performed on
the Hold Duration, yielded a significant main effect of
Informativeness, F (1,23) = 10.17, p = .004, ηp2 = .31. The
Hold Duration was significantly longer in the Informative
condition (M = 1235 ms) compared to the Non-informative
condition (M = 1143 ms). No significant main effect of

Table 1: Overview of the results per condition in the experiment. Duration in ms is displayed for the Gesture Initiation Time
(GIT), Stroke Duration (Stroke), Apex Time (Apex), Hold Duration (Hold), Speech Onset Time (SOT), and Synchronization
Time (Sync). Further, the Incremental Distance in cm (Dist) and Velocity in cm/s (Velocity) are provided. The standard error
of the mean is indicated between parentheses.
Condition

GIT

Stroke

Apex

Informative
Gesture-only
Gesture + Speech

534 (21)
550 (22)

834 (30)
840 (27)

Non-informative
Gesture-only
Gesture + Speech

532 (22)
541 (24)

819 (29)
826 (27)

Dist

Hold

SOT

Sync

1368 (42) 51 (1) 38.5 (1)
1389 (39) 51 (1) 37.8 (1)

1252 (135)
1219 (121)

1385 (65)

4 (54)

1351 (41) 51 (1) 39.0 (1)
1367 (40) 51 (1) 38.5 (1)

1138 (116)
1149 (106)

1351 (66)

16 (54)

1130

Velocity

Modality was found. There was no significant interaction
between the two factors.
In the G+S conditions, participants referred linguistically
to the circle on the screen while pointing. An analysis on the
Speech Onset Time with Informativeness as the only withinsubject factor revealed a significant main effect, F (1,23) =
6.79, p = .016, ηp2 = .23. This effect reflected that the
speech onset on average took place significantly later in the
Informative condition (M = 1385 ms) than in the Noninformative condition (M = 1351 ms). An analysis on the
Synchronization Time did not show a significant main effect
of Informativeness (p = .16), indicating that the onset of the
speech and the apex of the gesture were aligned similarly
and independently from the informativeness of the gesture.
Table 1 summarizes all results.

Discussion
Research investigating the production of iconic gestures has
found that the form of such gestures changes on the basis of
the communicative intent of the speaker-gesturer.
Importantly, here we show that also in the case of pointing
gestures speaker-gesturers exploit different form parameters
as a function of their communicative intent. First, the
duration of the stroke of pointing gestures was longer in the
informative condition, which led to a gesture with a lower
velocity and delayed the moment at which the apex was
reached. Presumably participants did this in order to be as
precise as possible in pointing to a target, which could be
achieved by pointing more slowly. An additional benefit
would then be that the addressee would have more time to
identify towards which referent the gesture was heading.
Second, the post-stroke hold-phase of the gesture was
maintained longer, presumably in order to assure that the
addressee had enough time to identify which referent the
speaker pointed to. The form parameters under investigation
here were not affected by the presence of deictic speech.
Nevertheless, the onset of speech was synchronized with the
moment at which the pointing gesture reached its apex.
A previous study compared a communicative to a noncommunicative situation and found that people may modify
the trajectory and endpoint location of their pointing gesture
to single out a referent for their addressee (Cleret de
Langavant et al., 2011). The current study takes this
research a step further by comparing two situations that are
both communicative and identical except for the
communicative intent of the gesturer. Cleret de Langavant
et al. (2011) did not find a difference in the duration of the
pointing gesture when comparing their communicative to
their non-communicative condition. Here we did find an
effect of communicative intent on the duration of the stroke
and the post-stroke hold-phase. Thus, in addition to varying
the endpoint location and trajectory of a pointing gesture (as
in Cleret de Langavant et al., 2011), people may also use the
duration of different sub-components of the pointing gesture
in order to communicate effectively.
Participants temporally aligned the onset of the deictic
linguistic expression with the moment the pointing gesture

reached its apex, regardless of whether the gesture was
informative or not. This finding is in line with previous
studies showing such temporal alignment of pointing and
speech (e.g., Levelt et al., 1985; McNeill, 1992) and with
models of speech and gesture production that underline the
synchronization of speech and gesture (e.g., De Ruiter,
2000; Krauss et al., 2000). Here we show that this temporal
synchrony between deictic speech and gesture is maintained
irrespective of the speaker-gesturer’s communicative intent.
We found a similar effect of communicative intent in
situations where people only used gesture to communicate,
compared to situations where speech and gesture were
concurrently produced (Clark, 1996; Kendon, 2004).
However, in our study, speech was purposefully never
informative and very similar across trials, and there is
indeed evidence that deictic speech can interact with the
form of a simultaneously produced gesture (e.g., Gonseth,
Vilain, & Vilain, 2012). It is therefore possible that
whenever speech itself is informative enough to single out a
referent, speaker-gesturers no longer design their concurrent
gesture to be maximally informative. Future research needs
to shed more light on the influence of speech-gesture
interaction on the form of deictic gesture and speech while
manipulating the informativeness of the speech.
In general, the results of our study fit well with models of
speech and gesture production that allow for a role of the
speaker-gesturer’s communicative intent in modulating the
exact form of a gesture, such as the Sketch model (De
Ruiter, 2000) and the Interface model (Kita & Özyürek,
2003). Conversely, our data would argue against models of
speech and gesture production that question whether the
speaker’s communicative intent plays a role in determining
the form of a gesture (e.g., Krauss et al., 2000). In our study,
participants had the communicative intention of producing a
pointing gesture towards a referent, either accompanied with
referential speech or not. The Sketch model, which
explicitly describes the production of pointing (in addition
to other types of gesture), underlines that upon the intention
to produce a pointing gesture, conventions such as which
hand shape and finger to use can be retrieved from a
knowledge store (called a “gestuary” by De Ruiter, 2000) in
memory. This representation of the pointing gesture in the
gestuary is only a template or abstract motor program, and
there are a number of degrees of freedom that can be varied
depending on the context in which the pointing gesture is
performed. According to this model, in our study,
participants retrieved a pointing gesture template from
memory and subsequently exploited the duration of both the
stroke (and as such the velocity and the moment the apex
was reached) and the post-stroke hold-phase of the gesture
as free parameters. Our study thus suggests that duration is a
free parameter that people use to vary the execution of their
pointing gesture, and further specifies in which specific
components of the gesture duration is indeed varied.
The form a pointing gesture takes not only depends on the
gesturer's communicative intent. Research has shown that it
also depends on physical factors such as the spatial location

1131

of a referent. For instance, people may raise their pointing
arm and hand higher when a referent is more distant
(Wilkins, 2003). Furthermore, the form of a gesture depends
on cultural factors. In different cultures, different body parts
are used for pointing (Kita, 2003; Wilkins, 2003). Finally, it
may depend on socio-pragmatic factors. In a corpus study
on Lao speakers, Enfield, Kita, and De Ruiter (2007)
observed a distinction between relatively big points in
which the whole arm is outstretched and relatively small
points in which the hand is the main articulator. They argue
that this difference in form is related to the pragmatic
function of the utterance a gesture occurs in. Big points
would do the primary work of an utterance, such as pointing
out the location of an object, whereas small points would
occur in utterances in which speech is central, adding a
background modifier on the basis of social factors such as
the common ground between interlocutors (p. 1738). Future
studies could investigate interactions between such different
physical, cultural, socio-pragmatic, and communicative
factors.
To conclude, our study showed that people exploit the
duration of the stroke (and as such its velocity and the
moment the apex is reached) and the post-stroke hold-phase
of their pointing gesture to communicate effectively. Thus,
the form of a pointing gesture varies as a function of the
speaker-gesturer’s communicative intent. Similarly to iconic
gestures, the form of pointing gestures is dependent, among
other factors, on the context-bound communicative relation
between speaker-gesturer and addressee.

Acknowledgments
We thank Albert Russel for technical assistance. JH was
supported through Marie Curie Fellowship #255569 as well
as through ERC Advanced Grant #269484 INTERACT.

References
Alibali, M. W., Heath, D. C., & Myers, H. J. (2001). Effects
of visibility between speaker and listener on gesture
production: Some gestures are meant to be seen. Journal
of Memory and Language, 44, 169-188.
Bangerter, A. (2004). Using pointing and describing to
achieve joint focus of attention in dialogue.
Psychological Science, 15, 415-419.
Bavelas, J., Gerwing, J., Sutton, C., & Prevost, D. (2008).
Gesturing on the telephone: Independent effects of
dialogue and visibility. Journal of Memory and
Language, 58, 495-520.
Butterworth, G. (2003). Pointing is the royal road to
language for babies. In S. Kita (Ed.), Pointing. Where
language, culture, and cognition meet. Mahwah, NJ:
Lawrence Erlbaum.
Call, J., & Tomasello, M. (1994). Production and
comprehension of referential pointing by orangutans
(Pongo pygmaeus). Journal of Comparative Psychology,
108, 307-317.
Carpenter, M., Nagell, K., & Tomasello, M. (1998). Social
cognition,
joint
attention,
and
communicative

competence from 9 to 15 months of age. Monographs of
the Society for Research in Child Development, 255, Vol.
63, 1-174.
Clark, H. H. (1996). Using Language. Cambridge:
Cambridge University Press.
Cleret de Langavant, L., Remy, P., Trinkler, I., McIntyre, J.,
Dupoux, E., Berthoz, A., & Bachoud-Lévi, A.-C. (2011).
Behavioral and neural correlates of communication via
pointing. PLoS ONE, 6:3, e17719.
De Ruiter, J. P. (2000). The production of gesture and
speech. In D. McNeill (Ed.), Language and gesture.
Cambridge: Cambridge University Press.
Enfield, N. J., Kita, S., & De Ruiter, J. P. (2007). Primary
and secondary pragmatic functions of pointing gestures.
Journal of Pragmatics, 39, 1722-1741.
Gerwing, J. & Bavelas, J. (2004). Linguistic influences on
gesture’s form. Gesture, 4, 157-195.
Gonseth, C., Vilain, A., & Vilain, C. (in press). An
experimental study of speech/gesture interactions and
distance encoding. Speech communication.
Holler, J., & Stevens, R. (2007). An experimental
investigation into the effect of common ground on how
speakers use gesture and speech to represent size
information in referential communication. Journal of
Language and Social Psychology, 26, 4-27.
Holler, J., & Wilkin, K. (2011). An experimental
investigation of how addressee feedback affects cospeech gestures accompanying speakers’ responses.
Journal of Pragmatics, 43, 3522-3536.
Kendon, A. (2004). Gesture: Visible action as utterance.
Cambridge: Cambridge University Press.
Kita, S. (2003). Pointing. Where language, culture, and
cognition meet. Mahwah, NJ: Lawrence Erlbaum.
Kita, S., & Özyürek, A. (2003). What does cross-linguistic
variation in semantic coordination of speech and gesture
reveal?: Evidence for an interface representation of
spatial thinking and speaking. Journal of Memory and
Language, 48, 16-32.
Krauss, R. M., Chen, Y., & Gottesman, R. F. (2000).
Lexical gestures and lexical access: A process model. In
D. McNeill (Ed.), Language and gesture. Cambridge:
Cambridge University Press.
Levelt, W. J. M., Richardson, G., & La Heij, W. (1985).
Pointing and voicing in deictic expressions. Journal of
Memory and Language, 24, 133-164.
McNeill, D. (1985). So you think gestures are nonverbal?
Psychological Review, 92, 350-371.
McNeill, D. (1992). Hand and Mind: What gestures reveal
about thought. Chicago: University of Chicago Press.
Tomasello, M., Carpenter, M., & Liszkowski, U. (2007). A
new look at infant pointing. Child Development, 78, 705722.
Wilkins, D. (2003). Why pointing with the index finger is
not a universal (in sociocultural and semiotic terms). In S.
Kita (Ed.), Pointing. Where language, culture, and
cognition meet. Mahwah, NJ: Lawrence Erlbaum.

1132

