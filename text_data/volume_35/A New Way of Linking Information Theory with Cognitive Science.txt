UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A New Way of Linking Information Theory with Cognitive Science
Permalink
https://escholarship.org/uc/item/5f15z1gs
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Author
Thorton, Chris
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

     A New Way of Linking Information Theory with Cognitive Science
                                                        Chris Thornton
                                                           Informatics
                                                      University of Sussex
                                                             Brighton
                                                            BN1 9QH
                                                               UK
                                                    c.thornton@sussex.ac.uk
                           Abstract                                other hand, takes the view that the difficulty with them
   The relationship between the notion of information in
                                                                   is they are calculated purely from the perspective of
   information theory, and the notion of information pro-          the recipient. Luce takes the view that information
   cessing in cognitive science, has long been controversial.      theory cannot address questions about structural rep-
   But as the present paper shows, part of the disagreement        resentation of content because the ‘elements of choice
   arises from conflating different formulations of measure-
   ment. Clarifying distinctions reveals it is the context-        in information theory are absolutely neutral and lack
   free nature of Shannon’s information average that is            any internal structure’ (Luce, 2003, p. 185). On the
   particular problematic from the cognitive point of view.        other hand, a community of researchers examines ways
   Context-sensitive evaluation is then shown to be a way
   of addressing the problems that arise.                          in which information-theoretic quantification can explain
                                                                   emergence of structural representation in sensory pro-
                      Introduction                                 cessing (e.g. Attneave, 1959; Barlow, 1961; Uttley, 1979;
One of the longest standing puzzles of cognitive science           Srinivisan et al., 1982; Atick, 1992).
is what to think about information theory. Set out in its             For Haber, it is beyond dispute that ‘the demise of
standard formulation more than 60 years ago, this frame-           information theory in psychology’ has already occurred
work (Shannon and Weaver, 1949) is acknowledged to be              (Haber, 1983, p. 71). But intermediate positions are also
a remarkably general and precise area of mathematics.              common. Barwise notes that while ‘traditional informa-
So it is of great interest to discover whether the notion          tion theory is not a semantic theory at all’ it ‘puts impor-
of information developed in information theory has any-            tant constraints on cognitive theories’ (Barwise, 1983, p.
thing to do with the notion of information processing at           65). Churchland and Churchland (1983) are more pos-
the heart of cognitive science.                                    itive still, seeing information theory as having a signifi-
   In the original publication, Shannon notes that ‘the            cant ‘role to play in an account of cognition’ (Churchland
semantic aspects of communication are irrelevant to the            and Churchland, 1983, p. 67), and arguing the connec-
engineering aspects’ (Shannon and Weaver, 1949, p. 31).            tion can be made through something called ‘calibrational
On the assumption that information processing in cogni-            content’ specifically, where this is defined to be informa-
tive science deals with semantic aspects in particular, a          tionally quantifiable ‘measurement or detection concern-
fundamental disconnect seems implied. But this is mud-             ing the status of the objective world’ (Churchland and
dled somewhat by the qualification (in Weaver’s con-               Churchland, 1983, p. 67). Others are doubtful of there
tribution to the joint publication) that Shannon’s as-             being any connection at all. Dretske, for example, argues
sertion ‘does not mean that the engineering aspects are            that information theory does not even ‘deal with infor-
necessarily irrelevant to the semantic aspects’ (Shannon           mation as it is ordinarily understood’ (Dretske, 1983, p.
and Weaver, 1949, p. 8). Adding to the ambiguity, re-              56).1
searchers such as Meyer (1957/1967), Miller (1953), Gar-              The present paper argues that one of the reasons the
ner (1962), Mackay (1956) and Attneave (1959) note a               situation has become so confused is that the debate has
range of ways in which issues of a semantic nature can             conflated different formulations of measurement.2 The
be addressed in information-theoretic terms. Quinlan               notion of measurement at the heart of the framework is
(1993) and others demonstrate algorithms that operate              the logarithmic principle, originally proposed by Hartley
specifically on this basis. Recent decades have also seen
increasing use of information-theoretic quantification in              1
                                                                         Curiously, this view is part of an informational episte-
cognitive neuroscience (e.g. Tononi et al., 1996; Lun-             mology. However, Dretske’s hard-line is consistent with the
garella et al., 2005; Friston, 2010).                              fact that his account has little in common with information
                                                                   theory (Sayre, 1983). In Kyburg’s view ‘Dretske seeks to
   The range of positions adopted on this issue deep-              clothe a relatively traditional approach to epistemology in
ens the mystery. Haber (1983) argues that information-             new information-theoretic clothes’ (Kyburg and Jr, 1983, p.
theoretic measures cannot address psychological ques-              72).
                                                                       2
                                                                         I ignore areas of the framework (concerned with noisy
tions due to being ‘entirely independent of the recipi-            and/or non-discrete systems) that have not figured in the
ent’ (Haber, 1983, p. 71). Temperley (2007), on the                debate.
                                                               3545

(1928). There is also the probabilistic formulation of          provided the same base is used for logarithm and digits.
the logarithmic principle: − log p. Finally, there is the       The usual approach uses base 2. The quantify of infor-
averaging formula                                               mation can then be stated in terms of ‘bits’ (short for
                       X                                        BInary digiTS). The measure quantifies both the amount
                     −     p(x) log p(x)                        of information obtained, and the number of binary digits
                        x                                       needed to encode the outcome.
This is called the entropy. These formulations build on            On the logarithmic principle, then, the informational
each other. The averaging formula uses the probabilis-          value of anything that reveals one of n outcomes is just
tic formulation, which is itself based on the logarithmic       log n. To obtain a value measured in bits, we take the
principle.3 But the three formulations have different im-       logarithm to base 2. (Use of this base is assumed hence-
plications for the question of connectivity with cognitive      forth.) The process can be illustrated using any set of
science.                                                        mutually exclusive outcomes. Let’s say a new regula-
   The position often taken is that there is one form of        tion requires Wi-Fi hotspots to be classified according
information-theoretic quantification, and it is the aver-       to level of service, with the possible classifications being
aging formula. Information measurement is taken to in-          W1, W2, W3 and W4. Given there are four possible
volve calculations of entropy specifically (e.g. Dretske,       outcomes, the informational value of anything that gives
1983; Sayre, 1983; Luce, 2003). This may be a conse-            the classification of a hotspot is then log 4 = 2 bits. This
quence of the extent to which the results of (Shannon and       is also the number of base 2 (binary) digits required to
Weaver, 1949) are derived by means of this formulation.         identify a classification.
But these results involve objectives of telecommunica-             An advantageous property of the logarithmic principle
tions specifically. Regarding the objectives of cognitive       is that it generalizes straightforwardly to the case where
science, the logarithmic principle and the probabilistic        outcomes have different probabilities. Instead of defining
formulation are equally of interest.                            the information obtained from a one-in-n outcome as
   The present paper reviews the steps that lead from           log n bits, it can be defined more generally as − log p
the logarithmic principle to Shannon’s averaging for-           bits, where p is the probability of the outcome. This
mula. Account is taken of the semantic implications of          accommodates the simple case of equiprobable outcomes,
different stages of the argument. Some aspects of the           since − log n1 = log n. But it also accommodates there
connectivity debate are clarified along the way, and con-       being a mixture of probabilities.
sideration is given to the problems that arise from the            Let’s say Wi-Fi hotspots are classified as W1 with
use of context-free forms of measurement. Derivation of         probability 12 , as W2 with probability 41 , and as W3 and
context-sensitive quantities is shown to be a viable al-        W4 with probability 81 . The discovery that a hotspot
ternative, and some examples are set out that show how          has a W4 classification is more informative in the sense
this approach connects to the representational concerns         of being contrary to expectation, than observing it has
of cognitive science.                                           a W1 classification. This is reflected in the information
                                                                value obtained. The value of a W1 classification is just
         Context-sensitive information                          − log 21 = 1 bit, whereas the informational value of a W4
                                                                classification is − log 81 = 3 bits.
Mathematical quantification of information begins with
                                                                   The probabilistic formulation of the logarithmic prin-
the logarithmic principle. Proposed originally by Hart-
                                                                ciple also provides the means of calculating averages.
ley (1928), this has a number of foundations, as reviewed
                                                                Given p(x) is the probability of outcome x, the aver-
by Shannon (Shannon and Weaver, 1949, pp. 31-33).
                                                                age informational value of an outcome is the weighted
Where an outcome is within a known set, the informa-
                                                                average
tional value must relate to the number of outcomes in
the set. A simple way of measuring the informational                                    X
value of something that reveals a particular outcome is                              −      p(x) log p(x)                (1)
thus in terms of the number of possible outcomes that                                     x
might have been revealed. This is a potential way to            This is the entropy formula, centrepiece of Shannon’s de-
measure the informational value of a ‘message’ to a ‘re-        velopment of the logarithmic approach. It can be used
ceiver’ then, to use Shannon’s own terminology. But as          whenever there is a probability distribution over out-
Hartley points out, it is much better to use a logarith-        comes. The distribution for Wi-Fi hotspots yields an
mic function of the number of outcomes. This yields             average information value of 1.75 bits, for example.
a measurement in which the quantity of information is              The average information has a number of appealing
also the number of digits needed to identify the outcome,       properties. It can be seen as measuring the uncertainty
   3                                                            that exists with respect to the outcomes, in the sense
     In practice, Shannon derives the entropy formula as the
only acceptable way of measuring the ‘choice’ permitted by      of quantifying the ‘choice’ allowed by the distribution
a distribution.                                                 (Shannon and Weaver, 1949, p 48-53). As a weighted
                                                            3546

average, it can also be seen as defining the information      of S is given, and all outcomes have known information
that an outcome is expected to have. Given − log p is an      values. It can also be seen as measuring the degree to
encoding cost, we can also look at the formula as the         which the distribution predicts the outcome in question.
average cost of encoding an outcome. (Shannon proves             Context-free evaluation of information (e.g., Eq. 1) is
the average cost can be no less: Shannon and Weaver,          valid in most situations. Hence the generality of Shan-
1949, p. 62-64).                                              non’s framework. But where subjectivity is a possibility,
   It is important to notice, however, that this approach     context-sensitive evaluation (by Eq. 3) is entailed. The
makes no distinction between subjective and objective         effects of evaluating information inappropriately can be
perspectives. In order for probability p(x) to be what        illustrated using the hotspots example again. Let’s say
fixes the amount of information an agent obtains from         a particular agent expects every hotspot to be a W1.
outcome x, this must be the probability the agent at-         The agent attributes a probability of 1 to the W1 clas-
tributes to x. On this basis, p(x) is subjective. But         sification, and a probability of 0 to W2, W3 and W4.
where it is used in the averaging formula, p(x) becomes       In objective reality, however, not all hotspots are W1:
the objective probability of x. In fact, Shannon’s frame-     at least one is classified as W2. There is a subjective
work makes no distinction between subjective and ob-          context, then, requiring amounts of information to be
jective probabilities. In the telecommunications context      calculated in a context-sensitive way.
that is the framework’s main focus, this makes sense.
                                                                 Should we choose to evaluation information in a
A telecommunications device adopting a personal per-
                                                              context-free way regardless, the results are likely to be
spective would be worthless. In other contexts, however,
                                                              meaningless. The attributed distribution places all prob-
subjective factors may be of more relevance. It is of in-
                                                              ability on one outcome. Its entropy is zero. On the
terest, then, to consider ways in which context-sensitive
                                                              strength of this, the average informational value of each
information values can also be calculated.
                                                              outcome is deemed to be zero bits. This is appropriate
   Consider the case where there is a set of two out-
                                                              in the case of a W1 classification, since the agent deems
comes, both of which have information values calculated
                                                              this to be the outcome in all cases. Unfortunately, it is
in an objective way (i.e., by the logarithmic principle).
                                                              also the value in the case of a W2 classification, which is
A context-sensitive value can then be calculated for any
                                                              a case the agent deems to be impossible.
distribution attributed, and any outcome arising. This
is just the expected value of the distribution in regard         This nonsensical result is a consequence of applying
to the outcome. On the principle that probability at-         context-free evaluation to a situation in which there is a
tributed to the given outcome must increase the distri-       subjective context. On the context-free interpretation,
bution’s value, while probability attributed to any other     there cannot be a W2 classification: its assumed prob-
outcome must decrease it, the expected information is         ability is zero. Given the subjective perspective that is
a weighted average in which outcome values are either         in force, context-sensitive evaluation using Eq. 3 is re-
positive or negative:                                         quired. This produces a result that makes more sense.
                                                              The context-sensitive value is found to be 1 bit for a
                                                              W1 classification, and − 23 bits for any other classifica-
                          
                           I(x)     if x is given
                                                              tion. Notice the potential for negative context-sensitive
                    X
          I(PS ) =     Px                             (2)
                   x∈S
                          
                            −I(x) otherwise                   values, in contrast with context-free (entropy) values,
                                                              which are always non-negative.
Here, S is the set of outcomes, PS denotes the distri-
                                                                 The general difficulty that arises for cognitive science
bution attributed, and I(x) is the informational value
                                                              will then be evident. Situations of interest for this dis-
of outcome x. (Calculated by the logarithmic principle,
                                                              cipline involve subjectivity by definition. The tendency
I(x) = log |S|).) This formula is valid whenever there
                                                              to equate information-theoretic evaluation with context-
are just two outcomes. Where there are more than two,
                                                              free measurement is thus an obstacle. But there is an-
the number of outcomes not given is greater than 1, and
                                                              other aspect to the problem. Both context-free and
thus greater than the number given. It is then neces-
                                                              context-sensitive forms of evaluation are calculated with
sary to ensure commensurability between additions and
                                                              regard to a set of mutually-exclusive outcomes. The
subtractions by normalizing the latter with respect to
                                                              evaluations obtained depend solely on probabilities at-
|S| − 1, the number of non-given outcomes. The general
                                                              tributed, and the number of outcomes in the set. The
form of the context-sensitive evaluation is thus
                                                              difficulty is that each outcome has the potential to sig-
                                      if x is given           nify something completely different. Information values
                           I(x)
                          
         I(PS ) =
                   X
                       Px                             (3)     reflect the original outcomes, rather than any interpreta-
                   x∈S
                              I(x)
                            − |S|−1   otherwise               tions that may be forthcoming, however. Where an addi-
                                                              tional semantics is imposed on outcomes, both context-
This is the expected information value of distribution        free and context-sensitive values may be meaningless in
PS to the attributing agent, where a particular element       relation to the interpretations that apply.
                                                          3547

                                                                       showery 0.2        bright -0.6   showery -0.2       bright 0.6
   Evaluations that are context-sensitive in the sense of
being calculated by Eq. 3 may thus fail to be context-
sensitive with regard to an imposed semantics. There
are thus two ways in which information-theoretic eval-                     0.6                0.8            0.6            0.8
uations can be inadequate from the cognitive point of                             0.2
                                                                                        0.4
                                                                                                                   0.2
                                                                                                                       0.4
view. The semantic disconnect that commentators such
as Luce (2003), Haber (1983) and Dretske (1983) see as
                                                                         rain 1.0           sun 1.0       rain 1.0         sun 1.0
inherent in information theory originates in these two
ways.
                                                                               Figure 2: Derived evaluation of outcomes.
                        Illustrations
A useful context for illustrating context-sensitive eval-
uation is that of weather forecasting. Imagine we live
                                                                      In the illustrated scenario, distributions are signified
in a world where the weather has just two outcomes:
                                                                   by entities (i.e., forecasts) that are themselves outcomes.
rain and sun. Let’s say the forecast issued by the local
                                                                   By definition, these inherit the informational values of
met office for a particular day is showery, and that this
                                                                   the distributions they designate. Any higher-level dis-
signifies 60% chance of rain, and 40% chance of sun. As-
                                                                   tribution must then be evaluated in terms of the de-
sume the outcome is rain. Eq. 3 can then be used to
                                                                   rived values of predicted outcomes. To illustrate, let’s
obtain a context-sensitive value for the attributed dis-
                                                                   say that in a certain season the forecast is always either
tribution given this particular outcome. With the fore-
                                                                   showery or bright, with the latter meaning 20% chance
cast being showery, rain is predicted with probability
                                                                   of rain and 80% chance of sun. Context-sensitive values
0.6. The outcome is in fact rain, and the information
                                                                   for these forecasts are then derived as in Figure 2. Po-
value of each outcome is assumed to be log 2 = 1 bit.
                                                                   tentially there can then be a second level of structure. A
The context-sensitive value of the distribution is thus
                                                                   forecast of unsettled might mean 70% chance of showery
(0.6 × 1) − (0.4 × 1) = 0.2 bits. If the outcome is sun, on
                                                                   and 30% chance of bright. The context-sensitive value
the other hand, the value is (0.4 × 1) − (0.6 × 1) = −0.2
                                                                   of this forecast would then be calculated in terms of the
bits.
                                                                   derived values of showery and bright, rather than values
      showery 0.2                   showery -0.2                   obtained by the logarithmic principle.
                0.6                            0.6
                                                                                      Analysis of representation
                    0.4                            0.4
                                                                   Context-sensitive evaluations can be calculated wherever
    rain 1.0             sun 1.0   rain 1.0            sun 1.0     we have both a distribution and a given outcome. Where
                                                                   one outcome signifies such a distribution itself, the value
             Figure 1: Context-sensitive evaluations.              obtained also belongs to the signifying outcome, as noted
                                                                   above. Context-sensitive evaluations can thus be a way
                                                                   of evaluating probabilistic representation. Such evalua-
   The diagram of Figure 1 illustrates the two cases con-          tions can be made at multiple levels. Where one out-
sidered. In this and ensuing schematics, outcomes are              come signifies a distribution over several others, one of
represented by small circles, labeled with the outcome’s           which does the same thing, there are two levels of rep-
name and informational value. Circles are filled where             resentation. The latter is embedded within the former.
the outcome is given. Circles enclosed within the same             Context-sensitive measurement of information is a way
bar are within the same choice of outcomes: the bar                of evaluating outcomes at multiple levels of representa-
represents the choice. Where one outcome signifies a               tion.
distribution over others—e.g., showery specifying rain                An assembly of signifying outcomes is a kind of repre-
with probability 0.6—the relationships are indicated us-           sentation structure, then. Such structures can take any
ing connecting lines. Annotations placed over these lines          form we like. For example, we might configure a rep-
show the probabilities that are attributed.                        resentation structure in a way that expresses a category
   The figure shows evaluations of the showery distribu-           hierarchy. Let’s say we have three categories as follows: a
tion for the two outcomes rain and sun. Notice how                 fruit category, in which the members are apple and plum;
the values reflect the degree to which the distribution            a bread category in which the members are pita and bun,
predicts the outcome given. The evaluation is negative             and a food category in which the members are fruit and
where the implied distribution mispredicts the outcome,            bread. To express this category hierarchy as a repre-
and positive otherwise. At the same time, its relatively           sentation structure, categories must be treated as linked
indiscriminate nature ensures both values are small com-           outcomes. Since category members are equiprobable in-
pared with those of the outcomes themselves.                       stances of their category, member outcomes are always
                                                               3548

                            food                                                           food 0.0
                             food                                                                  food
                      fruit   0.5                                                         fruit     0.5
                      bread 0.5                                                           bread 0.5
                                                                              fruit 0.67                                         bread -0.67
                 fruit                      bread
                                                                                fruit bread
           fruit  bread                                                 apple    0.5   0.0
     apple 0.5     0.0                                                  plum     0.5   0.0
     plum 0.5      0.0                                                  pita     0.0   0.5
     pita   0.0    0.5                                                  bun      0.0   0.5
     bun    0.0    0.5
                                                                    apple 2.0                                                              bun 2.0
      apple                                         bun              plum 2.0                                                              pita 2.0
       plum                                         pita
                                                                 Figure 4: Context-sensitive evaluation as classification.
Figure 3: Representation structure in the form of a cat-
egory hierarchy.
                                                                ties. Viewed as a representation structure, this is a case
                                                                in which one outcome designates multiple distributions,
equiprobable attributions of the corresponding category         each of which concentrates probability on a single out-
outcome. The representation structure expressing the            come. Such cases can be analyzed using context-sensitive
category hierarchy is thus the one of Figure 3. (Notice         evaluation in the usual way. But in so doing it is nec-
this diagram tabulates the probabilities involved, rather       essary to take the possibility of multiple designations
than displaying them on an individual basis.)                   into account. This must be done in accordance with the
   Regardless of what a representation structure ex-            principle that information can be summed only if is inde-
presses, it retains its capacity for informational evalua-      pendent. Where distributions are not independent, the
tion. This can be a way of explaining the functionalities       evaluation obtained is the maximum (i.e., greatest inde-
that are forthcoming. Where a representation structure          pendent value) rather than the sum of values arising.
is arranged as a category hierarchy, for instance, there
                                                                                            fruitcake 1.67
is the possibility of explaining classifications mathemati-
                                                                                                                            fudgecake 0.0
cally. Classifying an outcome in a particular way can be                                                        fruitcake
                                                                                                      bread        0.0
seen to identify the category outcome with the highest                                                fruit        1.0
                                                                                                      cake         1.0
context-sensitive evaluation.                                                                         crumble      0.0
   Consider the values that are obtained in the repre-
                                                                    bread -0.67                        fruit 0.67         cake 1.0
sentation structure of Figure 3, where apple is given.
                                                                                                                                         crumble 1.0
These are shown in Figure 4. The context-sensitive value                                                    bread   fruit
                                                                                                     apple   0.0     0.5
of the correct classification (fruit) is 0.67 bits, whereas                                          plum    0.0     0.5
                                                                                                     pita    0.5     0.0
the value of the incorrect classification (bread) is -0.67                                           bun     0.5     0.0
bits. The classification can be explained as identifying
the most informative category outcome.                               apple 2.0                                      bun 2.0
   Representation structures can be arranged in a broad               plum 2.0                                      pita 2.0
range of ways and can thus express any model con-
structed in terms of representational relationships. Their      Figure 5: Representation structure with disjunctive and
probabilistic foundation means they can represent con-          conjunctive elements.
ventional Bayesian models, for example. Being able to
incorporate multiple levels of representation, they can
express hierarchical Bayesian models. Another possibil-            Figure 5 extends the bread/fruit scenario to illustrate
ity is schematic models, involving representational rela-       what happens where representation structure includes
tionships of a conjunctive nature.                              conjunctive designations of this type. The bread/fruit
   Consider a schematic model in which a particular en-         structure from the previous diagram is seen here in the
tity is considered to be a combination of other enti-           lower-left corner. At the same level of representation,
                                                            3549

there is a cake/crumble choice. At the top level of rep-       Garner, W. R. (1962). Uncertainty and Structure as
resentation, the outcome fruitcake is specified in a way           Psychological Concepts. New York.
that requires both fruit and cake. The effect is to repro-
duce the conjunctive character of a schema. As previ-          Haber, R. N. (1983). Can information be objectivized?
ously, the evaluations arising can explain classifications.        Behavioral and Brain Sciences, 6 (pp. 70-71).
If apple and cake are both given, the context-sensitive        Hartley, R. L. (1928). Transmission of information. Bell
value of fruitcake is 1.67 bits. In the case of fudgecake,         System Technical Journal (pp. 535).
the value is 0 bits. Classifying a composite of apple and
cake as fruitcake is then explained in terms of this cate-     Kyburg, H. E. and Jr, (1983). Knowledge and the abso-
gory being most informative for the given context.                 lute. Behavioral and Brain Sciences, 6 (pp. 72-73).
                      Conclusion                               Luce, R. D. (2003). Whatever happened to information
                                                                   theory in psychology. Review of General Psychology,
The traditional objection to use of information theory in
                                                                   7, No. 2 (pp. 183-188).
cognitive science has been the assumption that it does
not deal with semantic aspects of information. On close        Lungarella, M., Pegors, T., Bulwinkle, D. and Sporns,
examination, this is found to be an over-simplification.           O. (2005). Methods for quantifying the information
Where information values are calculated by means of the            structure of sensory and motor data. Neuroinformat-
entropy formula, they are context-free in the sense of ig-         ics, 3, No. 3 (pp. 243-262).
noring any element of subjectivity. They may also be
context-free in the trivial sense of ignoring a superim-       Mackay, D. (1956). Towards an information-flow model
posed semantic interpretation. The latter problem can              of human behaviour. Br. J. Psychol, 43 (pp. 30-43).
be resolved simply by outlawing such applications. The
                                                               Meyer, L. B. (1957/1967). Meaning in music and in-
former can be resolved by pursuing evaluation in a way
                                                                   formation theory. Music, the Arts, and Ideas (pp.
that takes subjective context into account. On this basis,
                                                                   5-21). Chicago: University of Chicago Press.
information-theoretic evaluation can be of relevance to
cognitive science. Specifically, it can be a way of math-      Miller, G. A. (1953). What is information measurement?
ematically explaining category representation.                     American Psychologist, 8 (pp. 3-11).
                      References                               Quinlan, J. R. (1993). C4.5: Programs for Machine
Atick, J. J. (1992). Could information theory provide an           Learning. San Mateo, California: Morgan Kauf-
    ecological theory of sensory processing. Network, 3            mann.
    (pp. 213-251).                                             Sayre, K. M. (1983). Some untoward consequences of
Attneave, F. (1959). Applications of Information Theory            Dretske’s “causal theory” of information. Behavioral
    to Psychology. New York: Henry Holt.                           and Brain Sciences, 6 (pp. 78-79).
                                                               Shannon, C. and Weaver, W. (1949). The Mathematical
Barlow, H. B. (1961). Possible principles underlying the
                                                                   Theory of Communication. Urbana, Illinois: Univer-
    transformation of sensory messages. In Rosenblith
                                                                   sity of Illinois Press.
    (Ed.), Sensory communication (pp. 217-234). Cam-
    bridge: MIT Press.                                         Srinivisan, M. V., laughlin, S. B. and A, A. D. (1982).
                                                                   Predictive coding: a fresh view of inhibition in the
Barwise, J. (1983). Information and semantics (com-                retina. Proc. R. Soc. Lond. B, 216 (pp. 427-59).
    mentary on Précis of Knowledge and the Flow of In-
    formation). Behavioral and Brain Sciences, 6 (pp.          Temperley, D. (2007). Music and Probability. Cam-
    65-66).                                                        bridge, Massachusetts: The MIT Press.
Churchland, P. M. and Churchland, P. S. (1983). Con-           Tononi, G., Sporns, O. and Edelman, G. (1996). A com-
    tent: semantic and information-theoretic. Behav-               plexity measure for selective matching of signals by
    ioral and Brain Sciences, 6 (pp. 67-68).                       the brain. Proceedings of the Nat. Academy of Sci-
                                                                   ence, 93 (pp. 3422-3427).
Dretske, F. I. (1983). Précis of Knowledge and the Flow
    of Information. Behavioral and Brain Sciences, 6           Uttley, A. M. (1979). Information Transmission in the
    (pp. 55-90).                                                   Nervous System. London: Academic.
Friston, K. (2010). The free-energy principle: a unified
    brain theory? Nat. Rev. Neurosci, 11, No. 2 (pp.
    127-138).
                                                           3550

