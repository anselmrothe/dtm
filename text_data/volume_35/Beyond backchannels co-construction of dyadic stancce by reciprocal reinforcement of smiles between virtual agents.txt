UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Beyond backchannels: co-construction of dyadic stancce by reciprocal reinforcement of
smiles between virtual agents.
Permalink
https://escholarship.org/uc/item/1np4f1br
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Prepin, Ken
Ochs, Magalie
Pelachaud, Catherine
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

             Beyond backchannels: co-construction of dyadic stance by reciprocal
                                reinforcement of smiles between virtual agents.
                                          Ken Prepin (ken.prepin@telecom-paristech.fr)
                                            LTCI-CNRS/Telecom-ParisTech, 37-39 rue Dareau
                                                              75014 Paris, France
                                       Magalie Ochs (magalie.ochs@telecom-paristech.fr)
                                            LTCI-CNRS/Telecom-ParisTech, 37-39 rue Dareau
                                                              75014 Paris, France
                             Catherine Pelachaud (catherine.pelachaud@telecom-paristech.fr)
                                            LTCI-CNRS/Telecom-ParisTech, 37-39 rue Dareau
                                                              75014 Paris, France
                              Abstract                                    dyadic stances can be inferred (Prepin, Ochs, & Pelachaud,
   When two persons participate in a discussion, they not only            2012) from diachronic alignment between interactants. The
   exchange the concepts and ideas they are discussing, they also         effort of interlocutors to linguistically and non-verbally align
   express attitudes, feelings and commitments regarding their            through time is a marker of stance: it convey stance of mu-
   partner: they express interpersonal stances. Endowed with
   backchannel model, several virtual agents are able to react to         tual understanding, attention, agreement, interest and pleas-
   their partners’ behaviour through their non-verbal behaviour.          antness (Louwerse, Dale, Bard, & Jeuniaux, 2012).
   In this paper, we go beyond this approach, proposing and test-
   ing a model that enables agents to express a dyadic stance,               The description of stance has not only evolved toward
   marker of effective communication: agents will naturally co-           a distinction between individual and co-constructed stance.
   construct a shared dyadic stance if and only if their interper-
   sonal stance is reciprocally positive. We focus on smile, which        It has also evolved from a uniquely linguistic description
   conveys interpersonal stance and is a particularly efficient sig-      (DuBois, 2007; Kielsing, 2009) to a description implying in-
   nal for co-regulation of communication. With this model, a             teractants’ Non-Verbal Behaviours (NVBs) (Scherer, 2005;
   virtual agent, only capable to control its own individual pa-
   rameters, can, in fact, modulate and control the dyadic stance         Prepin et al., 2012). The non-verbal behaviours participate in
   appearing when it interacts with its partner. The evaluation           maintaining contact between interactants and facilitate ver-
   of the model through a user perceptive study has enabled us            bal exchange: they are an integral part of the communication
   to validate that the dyadic stance is significantly perceived as
   more positive (mutual understanding, attention, agreement, in-         process (Paradowski, 2011). NVBs actively convey stances
   terest, pleasantness) when reinforcement of smile is reciprocal.       through paralinguistic features (such as tone of voice, dura-
   Keywords: dyadic interaction; interactive behaviours; dynam-           tion, loudness or prosody), facial expressions, and postures
   ical systems; dyadic stance; smile; virtual agent;                     (Chindamo et al., 2012).
                          Introduction                                       Models of interactive agents have mainly explored the au-
When we consider verbal communication, interlocutors not                  tomatic generation of virtual agent’s behaviour aligned on the
only exchange the concepts and ideas which constitute the                 interlocutor’s behaviour. Buschmeier, S., and Kopp (2010)
subject of their discussion, they also express feelings, judge-           combine a model of lexical alignment with a model gener-
ments or commitments regarding this subject. This “atti-                  ating behaviours based on linguistic information. Bailenson
tude which, for some time, is expressed and sustained in-                 and Yee (2005) model the NVBs alignment of a speaking
teractively in communication, in a unimodal or multi-modal                virtual agent to a listening human. They propose a Digital
manner” corresponds to the stance: Chindamo, Allwood, and                 Chameleon (in reference to the Chameleon effect described
Ahlsén (2012) review the existing definitions and descriptions           by Chartrand and Bargh (1999)). Bevacqua, Hyniewska, and
of stance; they show how these definitions have evolved from              Pelachaud (2010) model the NVBs alignment of a listen-
a focus on individual expression of stance to a more interac-             ing agent to a speaking human: they propose a model of
tive and social description. Individual stance refers to two              backchannels, i.e. NVBs aligned in time and nature, to fa-
types of stance: epistemic and interpersonal stance (Kielsing,            cilitate human users to tell a story.
2009). The epistemic stance is the expression of the rela-                   All these models focus on the adaptation of the virtual
tionship of a person to his/her own talk (for instance “cer-              agent to its interlocutor, but do not take into account the recip-
tain”). The interpersonal stances convey the relationship of a            rocal adaptation of this interlocutor: behaviours are computed
person to the interlocutor (for example “warm” or “polite”).              in reaction to partner’s behaviour, but not in interaction with
Moreover, during an interaction, “stances are constructed                 partner’s behaviour; the dynamical coupling associated to the
across turns rather than being the product of a single turn”              mutual engagement of interactants is not modelled, and crit-
(Chindamo et al., 2012). When interactants with individ-                  ical parameters of interaction such as synchrony and align-
ual epistemic and interpersonal stances are put in presence,              ment which appear as side effects of this coupling (Paolo,
                                                                     1163

Rohde, & Iizuka, 2008; Prepin & Pelachaud, 2011, 2012a),               Characteristics     Amused                     Embarrassed
are missed. In this paper, we aim at going further by propos-                                           Polite smile
                                                                           of smile         smile                         smile
ing a model enabling virtual agents to co-construct their be-          Cheek raising          +              −              −
haviours: agents will be enabled to adapt to each other be-            Open mouth             +              −              −
haviour on the fly (that is in the time scale of the coupling          Lips tension           −              −              +
(Prepin & Pelachaud, 2011)) and to perform a resulting be-             Symmetry               +              +              –
haviour which is a dynamically built mix of each other be-             Amplitude              +              −              −
haviour; agents will also be enabled to modulate how much           Table 1: Smiles characteristics depending on their type (table
their own behaviour is influenced by the behaviour of the           filled based on the results described in (Ochs et al., 2010)): +
other, and doing so, they can control the stance of the dyad.       indicates significantly higher and - significantly lower values
   In the present paper, we propose and test a model that en-       of the characteristic for a given type of smile than the others,
ables virtual agents to co-construct a dyadic stance by tak-
ing advantage of the interactive loop existing between agents       the face, have to be activated to create a smile, and are suf-
and the resulting conjugated effects of reciprocal alignments.      ficient for an observer to recognize a facial expression as be-
Each virtual agent, only capable to control its own individ-        ing a smile. However, subtle differences in dynamics and in
ual parameters, can, in fact, modulate and control the dyadic       muscular activations make smiles convey different messages
stance appearing when it interacts with its partner. We focus       (such as amusement and politeness). Ochs et al. (2010) have
on smile behaviours for three reasons: (P1) a smile is one          studied the characteristics of polite, amused, and embarrassed
of the simplest and most easily recognized facial expressions       smiles of virtual agent’s. Their results are summarized in Ta-
(Ekman & Friesen, 1982); (P2) recent works (Ochs, Niewiad-          ble 1. The amused smiles are mainly characterized by large
moski, & Pelachaud, 2010) have shown that people are able to        amplitude, open mouth, symmetry, and relaxed lips. Most of
distinguish different types of smile when they are expressed        them also contain the activation of the cheek raising, and a
by a virtual character; (P3) in multimodal communication,           long global duration. The polite smiles are mainly character-
smile alignment appears in the form of synchronous smile            ized by small amplitude, a closed mouth, symmetry, relaxed
expressions of interactants (Louwerse et al., 2012). These          lips, and an absence of cheek raising. The embarrassed smiles
three properties of smile enable us to focus on the dynamical       often have small amplitude, a closed mouth, and tensed lips.
mechanisms of smiles alignment to model the co-construction         They are also characterized by the absence of cheek raising
of dyadic stances. For this purpose, based on the first prop-       and an asymmetry in the smile.
erty of smile (P1), we model the sensitivity to partner’s smile
as a motor resonance phenomenon. Considering the sec-
                                                                    Perception-Action mapping
ond property of smile (P2), we implement this model on a            In order to enable virtual agents to modify their facial expres-
dyad of smiling virtual agents. Based on the third property         sions “on the fly” (that is dynamically and in real-time), as
of smile (P3), we enable the virtual agents’ smiles occurring       proposed in (Prepin & Pelachaud, 2012b), facial expressions
synchronously to reinforce each other depending on the two          are updated frame by frame depending on both the speech ex-
agents’ individual stances.                                         pressed and the continuously incoming reactions of its part-
                                                                    ner. When an agent is performing an action (e.g. the display
                    Model description                               of a facial expression), it can have feedbacks concerning this
                                                                    action and can modify it “on the fly”.
In order to create virtual agents able to co-construct a dyadic         Several researches have shown that there is a natu-
stance by taking advantage of the interactive loop they form        ral/structural tendency to imitate the other and to better per-
with their partner, we focus on the agents capacity to mu-          ceive the other when imitating back (Muir, 2005; Nadel, Pre-
tually reinforce their smiles (see Introduction). The agents        pin, & Okanda, 2005). We model this property combining a
will be able to change the influence of their partner’s smile       mapping between the perceptive space and the motor space,
on their own smile: the more their own actions are influenced       and the self-activation of the motor space. Both the percep-
by the perception of their partner’s actions, the easier will be    tive space and the motor space are defined by Action Units
the coupling and the mutual reinforcement of the two agents         (AUs) in the Facial Action Coding System (Ekman & Friesen,
smile; virtual agents will be able to control the dyadic stance     1982) necessary to define smiles.
they co-produce with their interlocutor.                                The self-activation of the motor space, with a weight α <
                                                                    1 (see Figure.1), both simulates a short term memorisation
Smiles descriptions
                                                                    of actions and facilitates the subsequent activation of similar
In the proposed model, we focus on virtual agent’s smiles. On       actions (Schöner & Thelen, 2006). The nearer α is to 1, the
one hand, smile is one of the simplest and most easily recog-       longer the memorisation. We choose here α = 0.95 to ensure
nised facial expression (Ekman & Friesen, 1982), and on the         that this memorisation is “short term”, i.e. that after 1sec.
other hand it is one of the few behaviours often performed          (25 time steps), if there is no other stimulation, the activation
contingently by partners during interaction (Louwerse et al.,       of the AU is decreased by two thirds: AUi (t0 + 25) < 1/3 ·
2012). The two muscles zygomatic major, on either side of           AUi (t0 ).
                                                                1164

   The mapping between perceptive and motor spaces cor-                      For instance, a virtual agent with a cooperative attitude will be
responds to the links between perceived characteristics of                   more sensitive to the interlocutor’s perceived smile. Note that
smiles and generated characteristics of smiles. The mapping                  we do not model any cognitive model or strategy concerning
is based on the results on smiles reported in previous sec-                  the expression of stance, we just model how the interpersonal
tion. More precisely, the nodes in the perceptive and motor                  stance of the virtual agent modifies the way the agent is sen-
spaces correspond to the characteristics of the different types              sitive to its partner’s behaviours: the agent will modify how
of smile1 .                                                                  much it is interactive, engaged and finally cooperative with
   This mapping is represented in Figure 1 by links of dif-                  its partner2.
ferent widths between the perceptive (AU per ) and motor                        We assume here that interpersonal stance is represented as
(AU prod )spaces. The dashed links ending with a circle rep-                 a single variable σ, in [0, 1], which multiplies all the influ-
resent inhibitory links.             Action                                  ences between perceptive and motor spaces (see Fig.2). In
                                                                             the evaluation study, σ only takes two values: σ = 0 when the
                     zygom.    cheeks
                                raised
                                           mouth
                                          opened
                                                     lips
                                                   tension
                                                                             virtual agent is not cooperative, i.e. when its smiles are not
         Motor space                                                         reinforced by its partner’s smiles; and σ = 0.45 when the vir-
                                                                             tual agent is cooperative, i.e. when its smiles are reinforced
                                                            α                by its partner’s smiles. Note here that if σ was higher than
                                                                             0.45, even without any communicative intention stimulating
         Perceptive                                                          smiles, the reciprocal influence between agents would be too
            space    thβ
                     zygom.     cheeks     mouth      lips                   high to let smiles decrease.
                                raised    opened    tension
                                                                             Virtual agents dyad
                                    Perception
                                                                             The last step in the design of our model is to put two virtual
    Figure 1: Perceptive Space and Motor Space mapping.
                                                                             agents in presence, a speaker and a listener (Fig.2). For sake
   The excitatory/inhibitory nature of links and their weight
                                                                             of simplicity and to focus on the dyadic effect of the smile
have been inferred from Table 1. We detail the modelled ef-
                                                                             expressions, the virtual listener has no access to the mean-
fects for each smile characteristic:
                                                                             ing of what the speaker says. The listener only perceives
• Zygomatics: zygomatics appear in every smile and only                      the speaker’s non-verbal behaviour. On the other side, the
   their high amplitude indicates amused smile (Table 1); we                 speaker’s speech directly influences its own actions in the mo-
   assume that their perception will influence their production              tor space (see Fig.2).
   only if the perceived amplitude is over a threshold thβ .
• Lips tension: amused and embarrassed smiles are incom-                                        Agent2’s perceptions are Agent1’s actions
                                                                              Agent1                                                                   Agent2
   patible (they have opposite characteristics, see Table 1);                 Motor                                                                  Perceptive
   we assume that the specific AU of an embarrassed smile                     Space                                                                     Space
                                                                                 zyg. cheeks mouth lips                Interpersonal lips  mouth cheeks zyg.
                                                                                                                          Stance
   (i.e. lips tension) will inhibit and will be inhibited by the
   specific AUs of amused smile (i.e. cheeks raised and zygo-                                                Speech            ×σ2
                                                                                                         α
   matics over thβ ).                                                                                    ×σ1                      α
• Cheek raising: cheeks raising is an exclusive marker of                                                Interpersonal                lips
                                                                                 zyg.  cheeks mouth lips                                   mouth cheeks zyg.
   amused smile (Table 1); we assume that its perception                      Perceptive
                                                                                                            Stance
                                                                                                                                                        Motor
                                                                                Space                                                                    Space
   highly excites all the specific characteristics of amused
   smile (zygomatic above thβ , cheeks raise and mouth open-                                     Agent1’s perceptions are Agent2’s actions
   ing).
                                                                                Figure 2: Scheme of the interactive loop within the dyad.
• Mouth Opened: opening of mouth is not a specific char-
                                                                                We implement our model of virtual agents dyadic stance
   acteristic of smiles. We assume that its perception only
                                                                             generation in the Leto/Prometheus Neural Network (NN)
   influence the opening of mouth production.
                                                                             simulator (Gaussier & Zrehen, 1994), interfaced with the vir-
We stay at the level of a purely reactive model, only using                  tual agent platform SEMAINE (Schröder, 2010). The NN
muscular activations of produced and perceived signals.                      simulator enables to design the architecture neuron by neuron
More cognitive modelling could infer emotions and inten-                     and to control architecture dynamics in real-time (here frame
tions from these muscular activations.                                       by frame). The agent platform computes the communicative
                                                                             intention of the virtual character depending on its speech, and
Interpersonal stance influence                                               directly influences its actions in the motor space accordingly
                                                                             (see Fig.2). For instance, the utterance “I’m happy today” is
Virtual agent’s interpersonal stance (i.e. its stance regarding              automatically said with an amused smile.
its interlocutor) influences the visuo-motor mapping (Fig.2).
                                                                                 2 Other interpersonal stances may influence the mapping between
    1 Note that we have not considered the symmetry of the smile             perceptive space and motor space, such as warm or polite. However,
since this characteristic is difficult to perceive by a user when watch-     a model of the effect of the different stances on the perceptive and
ing a face to face interaction between virtual agents                        motor space is out of the scope of this paper.
                                                                         1165

    In the context of face to face interaction, if both virtual                 to be agreed with each other), mutual interest (the virtual
 agents have a cooperative interpersonal stance, they recipro-                  characters seem to be interested to the discussion), mutual
 cally reinforce their smiles (see Fig.3, (Prepin & Pelachaud,                  pleasantness (the virtual characters seem to spend a pleasant
 2012b)): a snowball effect on shared behaviours (when cou-                     time to interact). These stances have been chosen since re-
 pling occurs) and a decay/alignment of not-shared behaviours                   search (Louwerse et al., 2012) has shown that the mutual un-
 (when coupling is disrupted).                                                  derstanding, attention, agreement, interest and pleasantness
             activation
                                                    Snowball Effect:            are cues of the quality of an interaction between a speaker
                                                     intense smiles
                                                                                and a listener.
cheeks raised th                                                                Hypothesis. The hypothesis we want to validate through the
mouth opened th
                         small smiles                                           evaluation is the following:
  zygomatics th                                                                    The positive dyadic stance is significantly increased
                        non-contingent           contingent             t          when reinforcement of smile is reciprocal.
                           actions                 actions
 Figure 3: Dyadic dynamics of smiles. Solid and dotted lines                    More precisely, the evaluation aims to show that the mutual
 are respectively for Agent1 and Agent2’s intensity of smile.                   reinforcement of the smiles of the two interlocutors (i.e. the
                                                                                speaker and the listener) increases the impression of mutual
    The figure 4 shows the result of such an interaction on one                 understanding, attention, agreement, interest, pleasantness
 agent: the virtual agent’s smile is emphasized.                                compared to an interaction in which only the listener’s smiles
                                                                                are reinforced by the speaker’s smiles (and not in the other
                                                                                way round).
                                                                                   A validation of this hypothesis will enable us to validate the
                                                                                proposed model which simulates virtual characters’ dyadic
                                                                                stances through smiles mutual reinforcement and emerging
                                                                                snowball effect.
                                                                                Procedure. In order to verify the hypothesis, we have per-
          no smile          small smile transition smile      intense smile
                                                                                formed the evaluation on the web. The evaluation was in
 Figure 4: Snowball effect when smile reinforcement is recip-                   French. Four video clips showing two virtual characters
 rocal.                                                                         discussing were presented to participants. For each video
    Finally, the proposed model enables one to simulate an in-                  clip, we asked the participants to answer 5 questions us-
 teraction between two virtual agents with different smiling                    ing a Likert scale of 5 points (from “strongly disagree” to
 behaviour depending on the agents’ interpersonal stance. The                   “strongly agree”). The questions concerned their perception
 resulting interactions reflect different dyadic stances. In addi-              of the mutual understanding, attention, agreement, interest
 tion to cheeks raise and release of lips tension, the main side                and pleasantness of the two virtual characters. An example
 effect of mutual positive interpersonal stance is the snowball                 of a question is “When you watch the two virtual characters
 effect on smiles, i.e. the increase of smiles intensity and du-                discussing, according to you, do they understand each other?”
 ration.                                                                        (translated from French).
    Indeed, considering that NVBs alignment and dynamical                       Video Clips. To evaluate the perception of the interaction
 coupling are marker of the quality of the interaction (see In-                 between virtual characters in one way versus reciprocal con-
 troduction), these side-effects (such as “snowball effect”) are                ditions of smiles reinforcement, we have recorded the two
 the cues that should give an impression of fruitful interac-                   conditions of interaction:
 tion. In order to validate that our model enables one to sim-                  • reciprocal condition: both the speaker and the listener mu-
 ulate interactions between virtual agents that convey differ-                     tually reinforce their smiles depending on the smiles ex-
 ent dyadic stances depending on the mutual reinforcement of                       pressed by each other, “snowball effect” is enabled.
 their smiles, we have performed an evaluation presented in                     • control condition: only the listener reinforces its smiles ac-
 the next section.                                                                 cording to the speaker’s expressed smiles.
                                                                                In the video clips, the virtual characters discuss using an un-
                        Evaluation of the model                                 intelligible verbal language (corresponding to an acoustic de-
 To test that the proposed model enables one to simulate the                    formation of French texts). By this way, we avoid an influ-
 co-construction of different stances, we have performed a                      ence of what the virtual characters said on the user’s percep-
 user perceptive study. Our objective through this evaluation is                tion. We have considered 6 different texts corresponding to
 to show that the smiles mutual reinforcement between two in-                   the situation in which the virtual character tells a joke to its
 teracting virtual characters conveys specific stances. We have                 interlocutor. Given the text and the associated communica-
 focused on the following dyadic stances: mutual understand-                    tive intention, the virtual character expresses a polite smile at
 ing (the virtual characters seem to understand each other),                    the beginning and an amused smile in the middle of the text.
 mutual attention (the virtual characters seem to pay attention                 For each text, we have recorded video clips in the 2 condi-
 to each other), mutual agreement (the virtual characters seem                  tions described above with a virtual character saying this text
                                                                            1166

with an acoustic deformation and another virtual character, in      significantly higher when the speaker and the listener mu-
front, listening. In total, 12 video clips have been recorded.      tually reinforce their smiles according to the other’s smiles
In order to visualize clearly the faces of the two virtual char-    (reciprocal condition) than when only the listener reinforces
acters while keeping the impression that the virtual charac-        its smiles depending on the speaker’s expressed smiles (con-
ters are face to face, we have used a film-making technique         trol condition). The impression of mutual understanding, at-
called split-screen (Fig.5). Before starting the evaluation on      tention, agreement, interest and pleasantness directly depends
the web, to ensure that the instruction, the questions, and the     on the reciprocity of the interaction. These results are consis-
video clips are understandable, the platform of test has been       tent with psychology studies which claim that the interaction
pre-tested with 7 participants.                                     effort must be shared and reciprocal to enable effective com-
                                                                    munication (Nadel et al., 2005; Paolo et al., 2008; Auvray,
                                                                    Lenay, & Stewart, 2009; Fuchs & DeJaegher, 2009). Finally,
                                                                    the results validate the hypothesis described above: The pos-
                                                                    itive dyadic stance is significantly increased when reinforce-
                                                                    ment of smile is reciprocal and “snowball effect” is enabled.
                                                                                             Conclusion
Figure 5: Screen shot of a video clip of the two virtual char-      In the present paper, we have proposed a model enabling vir-
acters interacting                                                  tual agents to co-create different dyadic stances. We have
                                                                    described this model entwining each agent’s ability to con-
Participants. Sixty-six individuals have participated in this
                                                                    trol its cooperation to the interaction and the dyadic effects
evaluation on the web (34 females) with a mean age of
                                                                    emerging from the resulting agents coupling.
34 (SD=13). They were recruited via French mailing lists
                                                                    Agents are able to produce a continuum of smiling be-
on line. The participants were predominantly from France
                                                                    haviours. They can modulate their own smiles depending
(N=63). Each participant was shown and rated 4 video clips
                                                                    directly on their perceptions of their partner’s smiles. They
(two video clips selected randomly for each of the 2 condi-
                                                                    can control the level of this modulation and doing so con-
tions). The order of the presented video clips were counter-
                                                                    trol their interpersonal stance: a highly cooperative agent
balanced to avoid any effect on the results.
                                                                    reinforces its smiles when its interlocutor smiles. Finally
Results (Fig.6). We have collected 264 video clips’ rat-
                                                                    when a speaking agent (which produces smiles in relation to
ings. Independent t-Test was conducted to compare the par-
                                                                    its speech) and a listening agent are put together, their be-
ticipants’ ratings of the video clips in each condition. The
                                                                    haviours modulate each other reciprocally and dynamically
analysis revealed statically significant effects of the condi-
                                                                    form a new behaviour. Performing a user perceptive study,
tion on the participants’ ratings of the mutual understand-
                                                                    we have shown that this dyadic behaviour is the expression of
ing (p < 0.001), the mutual attention (p < 0.01), the mutual
                                                                    the two agents dyadic stance: the specific dyadic dynamics
agreement (p < 0.001), the mutual interest (p < 0.001), and
                                                                    which appear depending on each agent interpersonal stance
the mutual pleasantness (p < 0.001).
                                                                    convey information on agents’ mutual understanding, atten-
                                                                    tion, agreement, interest and pleasantness. The evaluation
                                                                    highlights that the virtual agent’s backchannels (one way re-
                                                                    actions) are less effective than reciprocal reactivity to convey
                                                                    some dyadic stances such as mutual understanding, attention,
                                                                    agreement, interest and pleasantness: The agents’ reactions
                                                                    must be reciprocal, as proposed in our model, to enable side
                                                                    effects of dynamical coupling such as emphasise of smiles,
                                                                    increase in intensity and duration.
                                                                    Future works. One of the aspect of the virtual agents mod-
                                                                    elling we have proposed is the fact that each agent of the
                                                                    dyad, has a different dynamic depending on the other agent
                                                                    stance: the agent’s own smile dynamic (for instance the smile
                                                                    slope) changes according to whether or not the other agent
                                                                    has co-operative interpersonal stance. As a consequence,
Figure 6: Means and standard errors of the dyadic stances’          each agent, knowing its own interpersonal stance and detect-
ratings for the two conditions. The significant differences be-     ing its own smile slope variation, could infer the other agent’s
tween the condition are indicated by ** for (p < 0.001), and        interpersonal stance. Finally each agent can use this signal
* for (p < 0.01)                                                    for modulating its own stance, its model of the other, or the
Discussion of the results. The mutual understanding, at-            way it interacts.
tention, interest, agreement and pleasantness are perceived            One of the next steps is to apply such a model to human-
                                                                1167

virtual agent interaction. For this purpose, we are currently        Louwerse, M., Dale, R., Bard, E. G., & Jeuniaux, P. (2012).
integrating in the SEMAINE platform a system to detect in              Behavior Matching in Multimodal Communication is Syn-
real-time user’s smiles3 . In this condition of direct interac-        chronized. Cognitive Science, 36(8), 1404–26.
tion between user and virtual agent, the user perception of the      Muir, D. (2005). Emotional development. In J. Nadel &
dyadic stances could be different since the user is directly en-       D. Muir (Eds.), (pp. 207–233). Oxford, UK: Oxford Uni-
gaged in the interaction (compared to the studied conditions           versity Press.
in which users have a third person point of view when they           Nadel, J., Prepin, K., & Okanda, M. (2005). Experi-
watch virtual characters interacting).                                 encing contingency and agency: first step toward self-
                                                                       understanding? In P. Hauf (Ed.), Making minds ii: Special
                      Acknowledgements                                 issue of interaction studies 6:3 2005 (pp. 447–462). John
This work has been financed by the European Community                  Benjamins publishing company.
Seventh Framework Program (FP7/2007-2013): the Euro-                 Ochs, M., Niewiadmoski, R., & Pelachaud, C. (2010). How
pean Project VERVE and the European Project NoE SSPNet.                a virtual agent should smile? morphological and dynamic
                                                                       characteristics of virtual agent’s smiles. In Intelligent vir-
                           References                                  tual agent (iva).
Auvray, M., Lenay, C., & Stewart, J. (2009). Perceptual              Paolo, E. A. D., Rohde, M., & Iizuka, H. (2008). Sensitivity
   interactions in a minimalist virtual environment. New ideas         to social contingency or stability of interaction? modelling
   in psychology, 27, 32–47.                                           the dynamics of perceptual crossing. New ideas in psychol-
Bailenson, J., & Yee, N. (2005). Digital chameleons: auto-             ogy, 26, 278–294.
   matic assimilation of nonverbal gestures in immersive vir-        Paradowski, M. B. (2011). The Embodied Language: Why
   tual environments. Psychological Science, 16(10), 814–9.            Language Should Not Be Conceived of in Abstraction from
Bevacqua, E., Hyniewska, S., & Pelachaud, C. (2010, Octo-              the Brain and Body, and the Consequences for Robotics.
   ber). Positive influence of smile backchannels in ecas. In          SSRN eLibrary.
   International workshop on interacting with ecas as virtual        Prepin, K., Ochs, M., & Pelachaud, C. (2012). Mutual stance
   characters (aamas 2010). Toronto, Canada.                           building in dyad of virtual agents: Smile alignement and
Buschmeier, H., S., K. B., & Kopp. (2010). Adaptive ex-                synchronisation. In Proceedings of international workshop
   pressiveness: virtual conversational agents that can align to       on exploring stances in interactions, ase/ieee international
   their interaction partner. In Proceedings of the 9th inter-         conference on social computing (pp. 938–943).
   national conference on autonomous agents and multiagent           Prepin, K., & Pelachaud, C. (2011). Effect of time delays
   systems (pp. 91–98). Richland, SC: IFAAMAS.                         on agents’ interaction dynamics. In Tumer, Yolum, Sonen-
Chartrand, T. L., & Bargh, J. A. (1999). The chameleon                 berg, & Stone (Eds.), The tenth international conference on
   effect: the perception-behavior link and social interaction.        autonomous agents and multi agents systems, aamas2011
   Journal of Personality and Social Psychology, 76(6), 893–           (pp. 1–8). IFAAMAS.
   910.                                                              Prepin, K., & Pelachaud, C. (2012a). Basics of intersubjec-
Chindamo, M., Allwood, J., & Ahlsén, E. (2012). Some sug-             tivity dynamics: Model of synchrony emergence when di-
   gestions for the study of stance in communication. In 2012          alog partners understand each other. In J. Filipe & A. Fred
   ase/ieee international conference on social computing (pp.          (Eds.), Icaart 2011, revised selected papers, series: Com-
   617–622). IEEE Computer Society.                                    munications in computer and information science (ccis)
DuBois, J. W. (2007). The stance triangle. In R. Engelbretson          (Vol. 271, pp. 302–318). Springer-Verlag.
   (Ed.), Stancetaking in discourse (pp. 139–182). Amster-           Prepin, K., & Pelachaud, C. (2012b). Live generation of
   dam/Philadelphia: John Benjamins Publishing Company.                interactive non-verbal behaviours. In Conitzer, Winikoff,
Ekman, P., & Friesen, W. V. (1982). Felt, False, And Miser-            Padgham, & van der Hoek (Eds.), 11th international con-
   able Smiles. Journal of Nonverbal Behavior, 6, 238–252.             ference on autonomous agents and multiagent systems, aa-
Fuchs, T., & DeJaegher, H. (2009). Enactive intersubjec-               mas 2012 (pp. 1–2). IFAAMAS.
   tivity: Participatory sense-making and mutual incorpora-          Scherer, K. (2005, December). What are emotions? and how
   tion. Phenomenology and the Cognitive Sciences, 8(4),               can they be measured? Social Science Information, 44(4),
   465–486.                                                            695–729.
Gaussier, P., & Zrehen, S. (1994). Avoiding the world model          Schöner, G., & Thelen, E. (2006). Using dynamic field the-
   trap: An acting robot does not need to be so smart! Jour-           ory to rethink infant habituation. Psychological Review,
   nal of Robotics and Computer-Integrated Manufacturing,              113(2), 273–299.
   11(4), 279–286.                                                   Schröder, M. (2010). The semaine api: Towards a standards-
Kielsing, S. F. (2009). Stance: Sociolinguistic perspectives.          based framework for building emotion-oriented systems.
   In A. Jaffe (Ed.), (pp. 171–194). Oxford: Oxford Univer-            Advances in Human-Computer Interaction, 2010, 21.
   sity Press.
    3 http://www.iis.fraunhofer.de/en/bf/bsy/produkte/shore.html
                                                                 1168

