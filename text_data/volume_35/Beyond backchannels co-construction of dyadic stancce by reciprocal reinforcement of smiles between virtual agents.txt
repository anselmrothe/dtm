UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Beyond backchannels: co-construction of dyadic stancce by reciprocal reinforcement of
smiles between virtual agents.

Permalink
https://escholarship.org/uc/item/1np4f1br

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)

Authors
Prepin, Ken
Ochs, Magalie
Pelachaud, Catherine

Publication Date
2013-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Beyond backchannels: co-construction of dyadic stance by reciprocal
reinforcement of smiles between virtual agents.
Ken Prepin (ken.prepin@telecom-paristech.fr)
LTCI-CNRS/Telecom-ParisTech, 37-39 rue Dareau
75014 Paris, France

Magalie Ochs (magalie.ochs@telecom-paristech.fr)
LTCI-CNRS/Telecom-ParisTech, 37-39 rue Dareau
75014 Paris, France

Catherine Pelachaud (catherine.pelachaud@telecom-paristech.fr)
LTCI-CNRS/Telecom-ParisTech, 37-39 rue Dareau
75014 Paris, France
Abstract

dyadic stances can be inferred (Prepin, Ochs, & Pelachaud,
2012) from diachronic alignment between interactants. The
effort of interlocutors to linguistically and non-verbally align
through time is a marker of stance: it convey stance of mutual understanding, attention, agreement, interest and pleasantness (Louwerse, Dale, Bard, & Jeuniaux, 2012).

When two persons participate in a discussion, they not only
exchange the concepts and ideas they are discussing, they also
express attitudes, feelings and commitments regarding their
partner: they express interpersonal stances. Endowed with
backchannel model, several virtual agents are able to react to
their partners’ behaviour through their non-verbal behaviour.
In this paper, we go beyond this approach, proposing and testing a model that enables agents to express a dyadic stance,
marker of effective communication: agents will naturally coconstruct a shared dyadic stance if and only if their interpersonal stance is reciprocally positive. We focus on smile, which
conveys interpersonal stance and is a particularly efficient signal for co-regulation of communication. With this model, a
virtual agent, only capable to control its own individual parameters, can, in fact, modulate and control the dyadic stance
appearing when it interacts with its partner. The evaluation
of the model through a user perceptive study has enabled us
to validate that the dyadic stance is significantly perceived as
more positive (mutual understanding, attention, agreement, interest, pleasantness) when reinforcement of smile is reciprocal.
Keywords: dyadic interaction; interactive behaviours; dynamical systems; dyadic stance; smile; virtual agent;

The description of stance has not only evolved toward
a distinction between individual and co-constructed stance.
It has also evolved from a uniquely linguistic description
(DuBois, 2007; Kielsing, 2009) to a description implying interactants’ Non-Verbal Behaviours (NVBs) (Scherer, 2005;
Prepin et al., 2012). The non-verbal behaviours participate in
maintaining contact between interactants and facilitate verbal exchange: they are an integral part of the communication
process (Paradowski, 2011). NVBs actively convey stances
through paralinguistic features (such as tone of voice, duration, loudness or prosody), facial expressions, and postures
(Chindamo et al., 2012).

Introduction
When we consider verbal communication, interlocutors not
only exchange the concepts and ideas which constitute the
subject of their discussion, they also express feelings, judgements or commitments regarding this subject. This “attitude which, for some time, is expressed and sustained interactively in communication, in a unimodal or multi-modal
manner” corresponds to the stance: Chindamo, Allwood, and
Ahlsén (2012) review the existing definitions and descriptions
of stance; they show how these definitions have evolved from
a focus on individual expression of stance to a more interactive and social description. Individual stance refers to two
types of stance: epistemic and interpersonal stance (Kielsing,
2009). The epistemic stance is the expression of the relationship of a person to his/her own talk (for instance “certain”). The interpersonal stances convey the relationship of a
person to the interlocutor (for example “warm” or “polite”).
Moreover, during an interaction, “stances are constructed
across turns rather than being the product of a single turn”
(Chindamo et al., 2012). When interactants with individual epistemic and interpersonal stances are put in presence,

Models of interactive agents have mainly explored the automatic generation of virtual agent’s behaviour aligned on the
interlocutor’s behaviour. Buschmeier, S., and Kopp (2010)
combine a model of lexical alignment with a model generating behaviours based on linguistic information. Bailenson
and Yee (2005) model the NVBs alignment of a speaking
virtual agent to a listening human. They propose a Digital
Chameleon (in reference to the Chameleon effect described
by Chartrand and Bargh (1999)). Bevacqua, Hyniewska, and
Pelachaud (2010) model the NVBs alignment of a listening agent to a speaking human: they propose a model of
backchannels, i.e. NVBs aligned in time and nature, to facilitate human users to tell a story.
All these models focus on the adaptation of the virtual
agent to its interlocutor, but do not take into account the reciprocal adaptation of this interlocutor: behaviours are computed
in reaction to partner’s behaviour, but not in interaction with
partner’s behaviour; the dynamical coupling associated to the
mutual engagement of interactants is not modelled, and critical parameters of interaction such as synchrony and alignment which appear as side effects of this coupling (Paolo,

1163

Rohde, & Iizuka, 2008; Prepin & Pelachaud, 2011, 2012a),
are missed. In this paper, we aim at going further by proposing a model enabling virtual agents to co-construct their behaviours: agents will be enabled to adapt to each other behaviour on the fly (that is in the time scale of the coupling
(Prepin & Pelachaud, 2011)) and to perform a resulting behaviour which is a dynamically built mix of each other behaviour; agents will also be enabled to modulate how much
their own behaviour is influenced by the behaviour of the
other, and doing so, they can control the stance of the dyad.
In the present paper, we propose and test a model that enables virtual agents to co-construct a dyadic stance by taking advantage of the interactive loop existing between agents
and the resulting conjugated effects of reciprocal alignments.
Each virtual agent, only capable to control its own individual parameters, can, in fact, modulate and control the dyadic
stance appearing when it interacts with its partner. We focus
on smile behaviours for three reasons: (P1) a smile is one
of the simplest and most easily recognized facial expressions
(Ekman & Friesen, 1982); (P2) recent works (Ochs, Niewiadmoski, & Pelachaud, 2010) have shown that people are able to
distinguish different types of smile when they are expressed
by a virtual character; (P3) in multimodal communication,
smile alignment appears in the form of synchronous smile
expressions of interactants (Louwerse et al., 2012). These
three properties of smile enable us to focus on the dynamical
mechanisms of smiles alignment to model the co-construction
of dyadic stances. For this purpose, based on the first property of smile (P1), we model the sensitivity to partner’s smile
as a motor resonance phenomenon. Considering the second property of smile (P2), we implement this model on a
dyad of smiling virtual agents. Based on the third property
of smile (P3), we enable the virtual agents’ smiles occurring
synchronously to reinforce each other depending on the two
agents’ individual stances.

Model description
In order to create virtual agents able to co-construct a dyadic
stance by taking advantage of the interactive loop they form
with their partner, we focus on the agents capacity to mutually reinforce their smiles (see Introduction). The agents
will be able to change the influence of their partner’s smile
on their own smile: the more their own actions are influenced
by the perception of their partner’s actions, the easier will be
the coupling and the mutual reinforcement of the two agents
smile; virtual agents will be able to control the dyadic stance
they co-produce with their interlocutor.

Smiles descriptions
In the proposed model, we focus on virtual agent’s smiles. On
one hand, smile is one of the simplest and most easily recognised facial expression (Ekman & Friesen, 1982), and on the
other hand it is one of the few behaviours often performed
contingently by partners during interaction (Louwerse et al.,
2012). The two muscles zygomatic major, on either side of

Characteristics
of smile
Cheek raising
Open mouth
Lips tension
Symmetry
Amplitude

Amused
smile

Polite smile

+
+

−
−
−
+
−

−
+

+

Embarrassed
smile
−
−

+
–
−

Table 1: Smiles characteristics depending on their type (table
filled based on the results described in (Ochs et al., 2010)): +
indicates significantly higher and - significantly lower values
of the characteristic for a given type of smile than the others,
the face, have to be activated to create a smile, and are sufficient for an observer to recognize a facial expression as being a smile. However, subtle differences in dynamics and in
muscular activations make smiles convey different messages
(such as amusement and politeness). Ochs et al. (2010) have
studied the characteristics of polite, amused, and embarrassed
smiles of virtual agent’s. Their results are summarized in Table 1. The amused smiles are mainly characterized by large
amplitude, open mouth, symmetry, and relaxed lips. Most of
them also contain the activation of the cheek raising, and a
long global duration. The polite smiles are mainly characterized by small amplitude, a closed mouth, symmetry, relaxed
lips, and an absence of cheek raising. The embarrassed smiles
often have small amplitude, a closed mouth, and tensed lips.
They are also characterized by the absence of cheek raising
and an asymmetry in the smile.

Perception-Action mapping
In order to enable virtual agents to modify their facial expressions “on the fly” (that is dynamically and in real-time), as
proposed in (Prepin & Pelachaud, 2012b), facial expressions
are updated frame by frame depending on both the speech expressed and the continuously incoming reactions of its partner. When an agent is performing an action (e.g. the display
of a facial expression), it can have feedbacks concerning this
action and can modify it “on the fly”.
Several researches have shown that there is a natural/structural tendency to imitate the other and to better perceive the other when imitating back (Muir, 2005; Nadel, Prepin, & Okanda, 2005). We model this property combining a
mapping between the perceptive space and the motor space,
and the self-activation of the motor space. Both the perceptive space and the motor space are defined by Action Units
(AUs) in the Facial Action Coding System (Ekman & Friesen,
1982) necessary to define smiles.
The self-activation of the motor space, with a weight α <
1 (see Figure.1), both simulates a short term memorisation
of actions and facilitates the subsequent activation of similar
actions (Schöner & Thelen, 2006). The nearer α is to 1, the
longer the memorisation. We choose here α = 0.95 to ensure
that this memorisation is “short term”, i.e. that after 1sec.
(25 time steps), if there is no other stimulation, the activation
of the AU is decreased by two thirds: AUi (t0 + 25) < 1/3 ·
AUi (t0 ).

1164

The mapping between perceptive and motor spaces corresponds to the links between perceived characteristics of
smiles and generated characteristics of smiles. The mapping
is based on the results on smiles reported in previous section. More precisely, the nodes in the perceptive and motor
spaces correspond to the characteristics of the different types
of smile1 .
This mapping is represented in Figure 1 by links of different widths between the perceptive (AU per ) and motor
(AU prod )spaces. The dashed links ending with a circle represent inhibitory links.
Action
zygom.

cheeks
raised

mouth
opened

lips
tension

Motor space

α
Perceptive
space

thβ
zygom.

cheeks
raised

mouth
opened

lips
tension

Virtual agents dyad

Perception

Figure 1: Perceptive Space and Motor Space mapping.
The excitatory/inhibitory nature of links and their weight
have been inferred from Table 1. We detail the modelled effects for each smile characteristic:
• Zygomatics: zygomatics appear in every smile and only
their high amplitude indicates amused smile (Table 1); we
assume that their perception will influence their production
only if the perceived amplitude is over a threshold thβ .
• Lips tension: amused and embarrassed smiles are incompatible (they have opposite characteristics, see Table 1);
we assume that the specific AU of an embarrassed smile
(i.e. lips tension) will inhibit and will be inhibited by the
specific AUs of amused smile (i.e. cheeks raised and zygomatics over thβ ).
• Cheek raising: cheeks raising is an exclusive marker of
amused smile (Table 1); we assume that its perception
highly excites all the specific characteristics of amused
smile (zygomatic above thβ , cheeks raise and mouth opening).
• Mouth Opened: opening of mouth is not a specific characteristic of smiles. We assume that its perception only
influence the opening of mouth production.
We stay at the level of a purely reactive model, only using
muscular activations of produced and perceived signals.
More cognitive modelling could infer emotions and intentions from these muscular activations.

Interpersonal stance influence
Virtual agent’s interpersonal stance (i.e. its stance regarding
its interlocutor) influences the visuo-motor mapping (Fig.2).
1 Note

For instance, a virtual agent with a cooperative attitude will be
more sensitive to the interlocutor’s perceived smile. Note that
we do not model any cognitive model or strategy concerning
the expression of stance, we just model how the interpersonal
stance of the virtual agent modifies the way the agent is sensitive to its partner’s behaviours: the agent will modify how
much it is interactive, engaged and finally cooperative with
its partner2.
We assume here that interpersonal stance is represented as
a single variable σ, in [0, 1], which multiplies all the influences between perceptive and motor spaces (see Fig.2). In
the evaluation study, σ only takes two values: σ = 0 when the
virtual agent is not cooperative, i.e. when its smiles are not
reinforced by its partner’s smiles; and σ = 0.45 when the virtual agent is cooperative, i.e. when its smiles are reinforced
by its partner’s smiles. Note here that if σ was higher than
0.45, even without any communicative intention stimulating
smiles, the reciprocal influence between agents would be too
high to let smiles decrease.
The last step in the design of our model is to put two virtual
agents in presence, a speaker and a listener (Fig.2). For sake
of simplicity and to focus on the dyadic effect of the smile
expressions, the virtual listener has no access to the meaning of what the speaker says. The listener only perceives
the speaker’s non-verbal behaviour. On the other side, the
speaker’s speech directly influences its own actions in the motor space (see Fig.2).
Agent1

Agent2’s perceptions are Agent1’s actions

Interpersonal
Stance

zyg. cheeks mouth lips

α

Speech

×σ1
zyg.

Agent2
Perceptive
Space

Motor
Space

cheeks mouth lips

Perceptive
Space

Interpersonal
Stance

lips

mouth cheeks zyg.

lips

mouth cheeks zyg.

×σ2
α

Motor
Space

Agent1’s perceptions are Agent2’s actions

Figure 2: Scheme of the interactive loop within the dyad.
We implement our model of virtual agents dyadic stance
generation in the Leto/Prometheus Neural Network (NN)
simulator (Gaussier & Zrehen, 1994), interfaced with the virtual agent platform SEMAINE (Schröder, 2010). The NN
simulator enables to design the architecture neuron by neuron
and to control architecture dynamics in real-time (here frame
by frame). The agent platform computes the communicative
intention of the virtual character depending on its speech, and
directly influences its actions in the motor space accordingly
(see Fig.2). For instance, the utterance “I’m happy today” is
automatically said with an amused smile.
2 Other interpersonal stances may influence the mapping between

that we have not considered the symmetry of the smile
since this characteristic is difficult to perceive by a user when watching a face to face interaction between virtual agents

perceptive space and motor space, such as warm or polite. However,
a model of the effect of the different stances on the perceptive and
motor space is out of the scope of this paper.

1165

In the context of face to face interaction, if both virtual
agents have a cooperative interpersonal stance, they reciprocally reinforce their smiles (see Fig.3, (Prepin & Pelachaud,
2012b)): a snowball effect on shared behaviours (when coupling occurs) and a decay/alignment of not-shared behaviours
(when coupling is disrupted).
Snowball Effect:
intense smiles

activation
cheeks raised th
mouth opened th
small smiles
zygomatics th
non-contingent
actions

contingent
actions

t

Figure 3: Dyadic dynamics of smiles. Solid and dotted lines
are respectively for Agent1 and Agent2’s intensity of smile.
The figure 4 shows the result of such an interaction on one
agent: the virtual agent’s smile is emphasized.

no smile

small smile

transition smile

intense smile

Figure 4: Snowball effect when smile reinforcement is reciprocal.
Finally, the proposed model enables one to simulate an interaction between two virtual agents with different smiling
behaviour depending on the agents’ interpersonal stance. The
resulting interactions reflect different dyadic stances. In addition to cheeks raise and release of lips tension, the main side
effect of mutual positive interpersonal stance is the snowball
effect on smiles, i.e. the increase of smiles intensity and duration.
Indeed, considering that NVBs alignment and dynamical
coupling are marker of the quality of the interaction (see Introduction), these side-effects (such as “snowball effect”) are
the cues that should give an impression of fruitful interaction. In order to validate that our model enables one to simulate interactions between virtual agents that convey different dyadic stances depending on the mutual reinforcement of
their smiles, we have performed an evaluation presented in
the next section.

Evaluation of the model
To test that the proposed model enables one to simulate the
co-construction of different stances, we have performed a
user perceptive study. Our objective through this evaluation is
to show that the smiles mutual reinforcement between two interacting virtual characters conveys specific stances. We have
focused on the following dyadic stances: mutual understanding (the virtual characters seem to understand each other),
mutual attention (the virtual characters seem to pay attention
to each other), mutual agreement (the virtual characters seem

to be agreed with each other), mutual interest (the virtual
characters seem to be interested to the discussion), mutual
pleasantness (the virtual characters seem to spend a pleasant
time to interact). These stances have been chosen since research (Louwerse et al., 2012) has shown that the mutual understanding, attention, agreement, interest and pleasantness
are cues of the quality of an interaction between a speaker
and a listener.
Hypothesis. The hypothesis we want to validate through the
evaluation is the following:
The positive dyadic stance is significantly increased
when reinforcement of smile is reciprocal.
More precisely, the evaluation aims to show that the mutual
reinforcement of the smiles of the two interlocutors (i.e. the
speaker and the listener) increases the impression of mutual
understanding, attention, agreement, interest, pleasantness
compared to an interaction in which only the listener’s smiles
are reinforced by the speaker’s smiles (and not in the other
way round).
A validation of this hypothesis will enable us to validate the
proposed model which simulates virtual characters’ dyadic
stances through smiles mutual reinforcement and emerging
snowball effect.
Procedure. In order to verify the hypothesis, we have performed the evaluation on the web. The evaluation was in
French. Four video clips showing two virtual characters
discussing were presented to participants. For each video
clip, we asked the participants to answer 5 questions using a Likert scale of 5 points (from “strongly disagree” to
“strongly agree”). The questions concerned their perception
of the mutual understanding, attention, agreement, interest
and pleasantness of the two virtual characters. An example
of a question is “When you watch the two virtual characters
discussing, according to you, do they understand each other?”
(translated from French).
Video Clips. To evaluate the perception of the interaction
between virtual characters in one way versus reciprocal conditions of smiles reinforcement, we have recorded the two
conditions of interaction:
• reciprocal condition: both the speaker and the listener mutually reinforce their smiles depending on the smiles expressed by each other, “snowball effect” is enabled.
• control condition: only the listener reinforces its smiles according to the speaker’s expressed smiles.
In the video clips, the virtual characters discuss using an unintelligible verbal language (corresponding to an acoustic deformation of French texts). By this way, we avoid an influence of what the virtual characters said on the user’s perception. We have considered 6 different texts corresponding to
the situation in which the virtual character tells a joke to its
interlocutor. Given the text and the associated communicative intention, the virtual character expresses a polite smile at
the beginning and an amused smile in the middle of the text.
For each text, we have recorded video clips in the 2 conditions described above with a virtual character saying this text

1166

with an acoustic deformation and another virtual character, in
front, listening. In total, 12 video clips have been recorded.
In order to visualize clearly the faces of the two virtual characters while keeping the impression that the virtual characters are face to face, we have used a film-making technique
called split-screen (Fig.5). Before starting the evaluation on
the web, to ensure that the instruction, the questions, and the
video clips are understandable, the platform of test has been
pre-tested with 7 participants.

significantly higher when the speaker and the listener mutually reinforce their smiles according to the other’s smiles
(reciprocal condition) than when only the listener reinforces
its smiles depending on the speaker’s expressed smiles (control condition). The impression of mutual understanding, attention, agreement, interest and pleasantness directly depends
on the reciprocity of the interaction. These results are consistent with psychology studies which claim that the interaction
effort must be shared and reciprocal to enable effective communication (Nadel et al., 2005; Paolo et al., 2008; Auvray,
Lenay, & Stewart, 2009; Fuchs & DeJaegher, 2009). Finally,
the results validate the hypothesis described above: The positive dyadic stance is significantly increased when reinforcement of smile is reciprocal and “snowball effect” is enabled.

Conclusion
Figure 5: Screen shot of a video clip of the two virtual characters interacting
Participants. Sixty-six individuals have participated in this
evaluation on the web (34 females) with a mean age of
34 (SD=13). They were recruited via French mailing lists
on line. The participants were predominantly from France
(N=63). Each participant was shown and rated 4 video clips
(two video clips selected randomly for each of the 2 conditions). The order of the presented video clips were counterbalanced to avoid any effect on the results.
Results (Fig.6). We have collected 264 video clips’ ratings. Independent t-Test was conducted to compare the participants’ ratings of the video clips in each condition. The
analysis revealed statically significant effects of the condition on the participants’ ratings of the mutual understanding (p < 0.001), the mutual attention (p < 0.01), the mutual
agreement (p < 0.001), the mutual interest (p < 0.001), and
the mutual pleasantness (p < 0.001).

Figure 6: Means and standard errors of the dyadic stances’
ratings for the two conditions. The significant differences between the condition are indicated by ** for (p < 0.001), and
* for (p < 0.01)
Discussion of the results. The mutual understanding, attention, interest, agreement and pleasantness are perceived

In the present paper, we have proposed a model enabling virtual agents to co-create different dyadic stances. We have
described this model entwining each agent’s ability to control its cooperation to the interaction and the dyadic effects
emerging from the resulting agents coupling.
Agents are able to produce a continuum of smiling behaviours. They can modulate their own smiles depending
directly on their perceptions of their partner’s smiles. They
can control the level of this modulation and doing so control their interpersonal stance: a highly cooperative agent
reinforces its smiles when its interlocutor smiles. Finally
when a speaking agent (which produces smiles in relation to
its speech) and a listening agent are put together, their behaviours modulate each other reciprocally and dynamically
form a new behaviour. Performing a user perceptive study,
we have shown that this dyadic behaviour is the expression of
the two agents dyadic stance: the specific dyadic dynamics
which appear depending on each agent interpersonal stance
convey information on agents’ mutual understanding, attention, agreement, interest and pleasantness. The evaluation
highlights that the virtual agent’s backchannels (one way reactions) are less effective than reciprocal reactivity to convey
some dyadic stances such as mutual understanding, attention,
agreement, interest and pleasantness: The agents’ reactions
must be reciprocal, as proposed in our model, to enable side
effects of dynamical coupling such as emphasise of smiles,
increase in intensity and duration.
Future works. One of the aspect of the virtual agents modelling we have proposed is the fact that each agent of the
dyad, has a different dynamic depending on the other agent
stance: the agent’s own smile dynamic (for instance the smile
slope) changes according to whether or not the other agent
has co-operative interpersonal stance. As a consequence,
each agent, knowing its own interpersonal stance and detecting its own smile slope variation, could infer the other agent’s
interpersonal stance. Finally each agent can use this signal
for modulating its own stance, its model of the other, or the
way it interacts.
One of the next steps is to apply such a model to human-

1167

virtual agent interaction. For this purpose, we are currently
integrating in the SEMAINE platform a system to detect in
real-time user’s smiles3 . In this condition of direct interaction between user and virtual agent, the user perception of the
dyadic stances could be different since the user is directly engaged in the interaction (compared to the studied conditions
in which users have a third person point of view when they
watch virtual characters interacting).

Acknowledgements
This work has been financed by the European Community
Seventh Framework Program (FP7/2007-2013): the European Project VERVE and the European Project NoE SSPNet.

References
Auvray, M., Lenay, C., & Stewart, J. (2009). Perceptual
interactions in a minimalist virtual environment. New ideas
in psychology, 27, 32–47.
Bailenson, J., & Yee, N. (2005). Digital chameleons: automatic assimilation of nonverbal gestures in immersive virtual environments. Psychological Science, 16(10), 814–9.
Bevacqua, E., Hyniewska, S., & Pelachaud, C. (2010, October). Positive influence of smile backchannels in ecas. In
International workshop on interacting with ecas as virtual
characters (aamas 2010). Toronto, Canada.
Buschmeier, H., S., K. B., & Kopp. (2010). Adaptive expressiveness: virtual conversational agents that can align to
their interaction partner. In Proceedings of the 9th international conference on autonomous agents and multiagent
systems (pp. 91–98). Richland, SC: IFAAMAS.
Chartrand, T. L., & Bargh, J. A. (1999). The chameleon
effect: the perception-behavior link and social interaction.
Journal of Personality and Social Psychology, 76(6), 893–
910.
Chindamo, M., Allwood, J., & Ahlsén, E. (2012). Some suggestions for the study of stance in communication. In 2012
ase/ieee international conference on social computing (pp.
617–622). IEEE Computer Society.
DuBois, J. W. (2007). The stance triangle. In R. Engelbretson
(Ed.), Stancetaking in discourse (pp. 139–182). Amsterdam/Philadelphia: John Benjamins Publishing Company.
Ekman, P., & Friesen, W. V. (1982). Felt, False, And Miserable Smiles. Journal of Nonverbal Behavior, 6, 238–252.
Fuchs, T., & DeJaegher, H. (2009). Enactive intersubjectivity: Participatory sense-making and mutual incorporation. Phenomenology and the Cognitive Sciences, 8(4),
465–486.
Gaussier, P., & Zrehen, S. (1994). Avoiding the world model
trap: An acting robot does not need to be so smart! Journal of Robotics and Computer-Integrated Manufacturing,
11(4), 279–286.
Kielsing, S. F. (2009). Stance: Sociolinguistic perspectives.
In A. Jaffe (Ed.), (pp. 171–194). Oxford: Oxford University Press.

Louwerse, M., Dale, R., Bard, E. G., & Jeuniaux, P. (2012).
Behavior Matching in Multimodal Communication is Synchronized. Cognitive Science, 36(8), 1404–26.
Muir, D. (2005). Emotional development. In J. Nadel &
D. Muir (Eds.), (pp. 207–233). Oxford, UK: Oxford University Press.
Nadel, J., Prepin, K., & Okanda, M. (2005). Experiencing contingency and agency: first step toward selfunderstanding? In P. Hauf (Ed.), Making minds ii: Special
issue of interaction studies 6:3 2005 (pp. 447–462). John
Benjamins publishing company.
Ochs, M., Niewiadmoski, R., & Pelachaud, C. (2010). How
a virtual agent should smile? morphological and dynamic
characteristics of virtual agent’s smiles. In Intelligent virtual agent (iva).
Paolo, E. A. D., Rohde, M., & Iizuka, H. (2008). Sensitivity
to social contingency or stability of interaction? modelling
the dynamics of perceptual crossing. New ideas in psychology, 26, 278–294.
Paradowski, M. B. (2011). The Embodied Language: Why
Language Should Not Be Conceived of in Abstraction from
the Brain and Body, and the Consequences for Robotics.
SSRN eLibrary.
Prepin, K., Ochs, M., & Pelachaud, C. (2012). Mutual stance
building in dyad of virtual agents: Smile alignement and
synchronisation. In Proceedings of international workshop
on exploring stances in interactions, ase/ieee international
conference on social computing (pp. 938–943).
Prepin, K., & Pelachaud, C. (2011). Effect of time delays
on agents’ interaction dynamics. In Tumer, Yolum, Sonenberg, & Stone (Eds.), The tenth international conference on
autonomous agents and multi agents systems, aamas2011
(pp. 1–8). IFAAMAS.
Prepin, K., & Pelachaud, C. (2012a). Basics of intersubjectivity dynamics: Model of synchrony emergence when dialog partners understand each other. In J. Filipe & A. Fred
(Eds.), Icaart 2011, revised selected papers, series: Communications in computer and information science (ccis)
(Vol. 271, pp. 302–318). Springer-Verlag.
Prepin, K., & Pelachaud, C. (2012b). Live generation of
interactive non-verbal behaviours. In Conitzer, Winikoff,
Padgham, & van der Hoek (Eds.), 11th international conference on autonomous agents and multiagent systems, aamas 2012 (pp. 1–2). IFAAMAS.
Scherer, K. (2005, December). What are emotions? and how
can they be measured? Social Science Information, 44(4),
695–729.
Schöner, G., & Thelen, E. (2006). Using dynamic field theory to rethink infant habituation. Psychological Review,
113(2), 273–299.
Schröder, M. (2010). The semaine api: Towards a standardsbased framework for building emotion-oriented systems.
Advances in Human-Computer Interaction, 2010, 21.

3 http://www.iis.fraunhofer.de/en/bf/bsy/produkte/shore.html

1168

