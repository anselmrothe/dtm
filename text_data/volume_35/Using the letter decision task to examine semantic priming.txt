UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Using the letter decision task to examine semantic priming
Permalink
https://escholarship.org/uc/item/1c4392hm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 35(35)
Authors
Heyman, Tom
De Deyne, Simon
Storms, Gert
Publication Date
2013-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

             Bayesian Adaptive Estimation of Psychometric Slope and Threshold
                                with Differential Evolution
                              Hairong Gu, Jay I. Myung, Mark A. Pitt, and Zhong-Lin Lu
                                    {Gu.124, Myung.1, Pitt.2, Lu.535}@Osu.Edu
                                          Department of Psychology, Ohio State University
                                           1835 Neil Avenue, Columbus, OH 43210 USA
                           Abstract                                    effects while ignoring the individual variation. However,
                                                                       more and more recognition has been given to the importance
  The adaptive experimentation methodology has been adopted            of individual differences. For example, in drug development,
  in visual psychophysical modeling in the pursuit of efficiency
  in experimental time and cost. The standard scheme only              it is important to know how different people react
  optimizes one design in each experimental stage, although            differently to the same drug to guide the prescription. Thus,
  simultaneous optimization of multiple designs per stage can          experimental designs should not be identical for every
  be beneficial, but difficult to implement because of a surge in      participant.
  computation. In this study, we incorporated the adaptive                To illustrate how experimental designs can be unequally
  experimentation methodology under a Bayesian framework               informative, suppose that a researcher is interested in
  with differential evolution (DE), an algorithm specialized in
                                                                       studying how the rate of detection changes with the
  multi-dimensional optimization problems to explore the
  multiple-designs-per-stage approach. By taking advantage of          brightness of a stimulus. A psychometric function is used to
  parallel computing, DE is computationally fast. The results          describe the probability p of detecting a stimulus of certain
  showed that the multiple-designs-per-stage scheme resulted in        brightness x. A simplified example assumes a sigmoid
  a more stable estimation in the early stages of the parameter        function                              , where x is the design
  estimation.                                                          variable representing the brightness and t is the parameter,
                                                                       threshold, a characteristic associated with a particular
  Keywords: Visual psychophysics, Bayesian inference,
  adaptive estimation, evolutionary computing
                                                                       individual, reflected in the shift of the model in the design
                                                                       dimension. Suppose that there are only 5 possible values of t.
                                                                       The corresponding predictions are depicted as the five lines
    Not All Designs are Equally Informative                            in Figure 1. The red line represents a particular subject’s
   Experimental design is a critical step in carrying out              true t value and the other four blue lines are from the wrong
effective experiments. Traditionally, the practice of                  t values. The researcher conducts an experiment to estimate
experimental design is guided by heuristic norms, using a              the threshold value of that subject by presenting two designs
one-shot design, chosen at the outset, throughout the course           with intensity D1 and D2. Visualization of the model
of the experiment. Although this approach may be adequate              suggests that D1 is a good design because the predictions
in some scientific quests, its shortcomings are obvious. First,        from the five t values are very differentiable so that the
not all experimental designs are equally informative. The              observation can be informative of the true t value. On the
traditional approach does not guarantee that the design,               other hand, D2 would be a bad design because the
including the number of treatments, the values of treatments,          prediction differences are so small that little information
and the number of participants in each treatment, is an                about the exact shift of the true model is given.
optimal choice. A non-optimal design may contribute little                                              1
to the goal of the experiment. Further, the most informative                                                 misspecified
                                                                                                             correct
designs may change as the experiment progresses with more
                                                                               Detection probability
                                                                                                       0.8
responses being observed. Thus, a one-shot design ignores
utilizing what can be learned during the course of an                                                  0.6
experiment.
   Second, the traditional experimental design method                                                  0.4
typically relies on increasing the number of participants or
                                                                                                       0.2
the number of measurements to increase the power of
statistical inference. Obviously, this increases the                                                    0
                                                                                                                            D1      D2
experimental cost, which would matter for experiments that                                               0   3         6      9    12    15
use expensive technology such as fMRI, or research whose                                                              X (design)
target population is difficult to recruit (children, senior              Figure 1: A sample psychometric function with 5 possible
citizens, mentally disordered).                                        parameter values (see text) with the true value indicated by
   Third, the traditional methods of experimental design               the red line and the wrong values by the blue lines. A good
center on randomization, reduction of variation, blocking              design D1 offers the most discriminability, whereas D2 is a
etc., with the purpose of revealing the group or treatment             bad design for a lack of differentiability in prediction.
                                                                    2452

Adaptive Experimentation                                            been implemented in the discrimination of retention models
In practice, we do not possess full knowledge of the                in simulations (Cavagnaro et al., 2010) and human
approximate values of good designs because the model can            experiments (Cavagnaro et al., 2011).
be quite complex and the range of parameters can be much               A promising application of ADO is in psychophysical
larger. In addition, an experiment usually contains multiple        experiments with potential clinical applications that put high
trials, so the best designs at the beginning of an experiment       stake on the reliability of the results and usually have tight
may be different from those at the later trials of the              time restraint on the experiments. In this area, the previous
experiment. Therefore, an efficient experimentation should          studies have only optimized one design in each
adaptively identify the best design for the current trial based     experimental stage. The difficulty of exploring a different
on the responses already collected from the participant.            scheme, multiple designs per stage, lies in a lack of a
Facing these challenges for a better experimental design            smarter algorithm and the increase in computation.
regime, a statistical methodology, dubbed adaptive design              In this paper, we explore ways to improve upon the
optimization (ADO, Cavagnaro et al., 2010; Myung et al.,            current efficiency of ADO by implementing the multiple-
2012) under a Bayesian framework has been developed to              designs-per-stage scheme that is solved with an evolutionary
meet these needs.                                                   computation algorithm known as differential evolution (DE).
   The general framework of ADO is illustrated in Figure 2.         In what follows, we begin with a brief introduction of the
The traditional experimentation starts from a particular            ADO methodology. We will then review past studies in
experimental design, with which data are collected, and it          adaptive experimentation of visual psychophysics, followed
stops at cognitive modeling where data are fit to a proposed        by a discussion of the motivation and application of the
model to make statistical inferences. In contrast, in ADO,          multiple-designs-per-stage scheme and DE. Finally, we
the inference from cognitive modeling continues to                  present and discuss results from ADO simulations.
influence the choice the designs for the next experimental
stage. To put it in another way, the whole experiment is                                  How ADO Works
divided into multiple stages, and in each stage, the design is      In this section, we provide some technical details of ADO.
based on what is learned from the data collected in the             Readers who prefer to skip technicalities may bypass this
previous stages. By doing that, every selected design is the        section. Figure 3 is a schematic illustration of the steps
most imminently useful one for the immediate trial. As such,        involved in ADO. First, the application of ADO requires
ADO is efficient in a way that it reduces the time, cost of         that the model should be formulated as a statistical model
experiments and the number of participants without                  defined as a parametric family of probability distributions,
sacrificing the quality of the statistical inferences.              p(y|θ, d)’s, which specifies the probability of observing an
                                                                    experimental outcome y given a parameter value θ and a
                          Experiment                                design d. As mentioned before, ADO is a circulating process
                                                                    going through design optimization (DO), experiments and
                                                                    cognitive modeling. In each round, the process starts with
                                                                    the assumed or learnt probability distribution of the
                                                                    parameters, the prior distribution p(θ). Next, in the step of
    Design optimization                 Cognitive modeling          DO, the optimal design d* is selected from a design set D
                                                                    by the principle of maximum utility.
Figure 2: Schematic illustration of ADO paradigm.                      In DO, a utility function U(d) is pre-defined to quantify
                                                                    the usefulness of a design d                for the purpose of the
  There are other desirable features of ADO that make it            experiment. For parameter estimation, the utility U(d) of
more attractive to the traditional experimental methods. It is      each design d is the expectation of the local utility u(d, θ , y)
found that bad designs not only increase the cost of                taken over the parameter space and the outcome’s sample
experiments, but also deteriorate the quality of data so as to      space, formally written as
hurt the final inference. ADO adopts an information                       d * = argmax(U (d ))
theoretic computational algorithm to ensure the quality of                        d D
                                                                                                                                   (1)
the selected designs so that the risk of having bad designs is                argmax  u (d ,  , y ) p( y |  , d ) p( )dyd ,
minimized. Additionally, ADO is able to reveal individual                         d D y ,
differences in response strategy or characteristics because
the designs are tailored based on the subject’s responses in        where u(d, θ , y) is defined on a set of particular design d,
each experiment. Classification of participants can also be         parameter value θ and observation y. The goal of parameter
done after individuals’ properties are estimated.                   estimation is to obtain accurate estimation of the true
   Because of its efficiency and versatility, ADO has found         parameter values with the smallest number of experimental
its usage in various disciplines. It has been used for              trials. Functionally, an appropriate utility quantifies the
designing electrophysiology experiments in neuroscience             usefulness of designs in reducing the variation of the
(Lewi et al., 2008), drug dosage assignment in clinical drug        parameter estimates. Or in the language of information
development (Miller, et al. 2007), etc. In psychology, it has       theory, a utility amounts to the information gain or the
                                                                2453

uncertainty reduction of the unknown parameters after                       Adaptive Estimation of Psychometric Function
observations are collected. One formulation of utility that
                                                                           In visual psychophysics, a major interest is to study the
directly quantifies the information gain of the parameter Θ
                                                                           relationship between the intensity of visual stimuli and their
with the observation Y is Mutual Information I(Yd;Θ).
                                                                           perception. This relationship is usually modeled by a
According to the property of mutual information, the utility
                                                                           psychometric function with two parameters, threshold and
U(d) can be written as
                                                                           slope. Accurate estimation of the parameter values on
         U (d )  I (Yd ; )                                               individual level not only provides knowledge of the
                          P(yd |  )                                       underlying psychophysical process, but also assists in the
                 (log             ) p( yd |  ) p( )d dyd ,
                                                                           diagnosis and classification (Lesmes et al., 2010). A major,
                            P( yd )
                                                                           practical challenge is that a large number of experimental
In which log            corresponds to the local utility u(d, θ, y)        trials is often needed to accurately estimate the parameters
in Equation (1).                                                           with the finding that different design schemes of fixed
  Two general methods have been used in ADO to solve the                   patterns produce varying accuracy, precision of parameter
multiple integral problem of Equation (1), grid search and                 estimation and model fit (Wichmann & Hill, 2001).
sequential Monte Carlo (SMC). In grid search, the design                      Addressing this issue, a variety of adaptive experimental
space is discretized and grids are the fixed designs on the                methods have been proposed for efficient parameter
space. To calculate U(d), one way is to discretize the                     estimation while the design dimension was restricted to be
parameter space also and just replace the integral with                    one. ADO, as a more general optimization algorithm, is able
summation. Or we can draw a large sample of (θ, y) from                    to handle large scale, non-linear models with multiple
the model’s prior and sampling distribution, and then                      design variables. Next, within the framework of ADO, the Ψ
calculate Equation (1) by Monte Carlo approximation. On                    method (Kontsevish & Tyler, 1999) was developed that can
the other hand, in SMC, solving ADO is recasted as a                       easily be generalized to incorporate more than one stimulus.
probability density simulation problem. The utility function               It has been applied to such research as diagnosis of visual
is extended to a joint distribution with parameters,                       deficit (Lesmes et al., 2010).
observations and designs. By adopting Metropolis-Hasting
algorithm and simulated annealing procedure, the marginal                  Multiple-designs-per-stage Scheme
distribution of d can be obtained. In this paper, we will                  All the methods mentioned above assume that there is just
present a third method, differential evolution (DE) (Storn &               one design to be optimized and one response to be collected
Price, 1997) as an alternative that is specialized in multi-               in each adaptive estimation stage. It is worthwhile to
dimensional optimization problems.                                         explore if there is any benefit when more than one design is
  After DO, the optimal design ds for the current stage will               optimized simultaneously and executed in each stage, by
be presented to the participant. The responses until the                   which d in Equation (1) becomes a vector. Intuitively, a
current stage will be used to update the knowledge of the                  multiple-designs-per-stage approach can be beneficial
parameters. Mathematically, we calculate the posterior                     because multiple responses are collected jointly in one stage,
probability distribution of the parameters by Bayes’ rule,                 and according to the information theory, the joint entropy or
                                                                           information from a set of random variables is more than or
                                  Then the posterior distribution
                                                                           equal to the sum of entropy from individual variables.
of the parameters of stage g is treated as the prior                       Therefore, we hypothesize that if multiple responses are
distribution of the next stage g+1. And the ADO process                    collected in one stage, the relationship or synergy of the
continues.                                                                 responses can benefit the modeling process more than the
                                                                           case when the responses are collected one by one.
                                                                       g → g+1
                 Priors Pg(θ)                                                                               Posteriors
                                                                                                              Pg+1(θ)
                 Probability
                                                 Global                 Optimal              Observed
                Distribution
                                             Utility U(d)              Design dg             Outcome
                  P y𝜃 d
                                                                                                Ys
                     Local
                                                              Design              Experiment         Bayesian Updating
                    Utility
                                                            Optimization
                  u d𝜃 y
                   Figure 3: Schematic illustration of the steps involved in adaptive design optimization (ADO).
                                                                       2454

  One computational challenge in the application and                GPU-based Parallel Computing
implementation of multiple-designs-per-stage scheme is              Although ADO retains the quality of the data with fewer
the curse of dimensionality. Most published studies on              trials, the heavy computation of ADO is still an issue to
parameter estimation with psychometric functions used               reckon with, especially in real-time experiments. One
brute-force grid search, which is to fix a certain number           solution to speed up the computation lies in parallel
of design points on the design space. Because the                   computing. Traditionally, computer instructions are stored
dimension of the design space increases with the number             and processed by a central processing unit (CPU), and
of designs per stage, the quantity of grids need to enlarge         executed in a serial manner. On the other hand, parallel
exponentially to keep a certain resolution, which causes a          computing employs multiple cores on a single chip to
waste of computing resource because most of the grids               perform many independent numerical operations
are far from the best design and not worth being                    simultaneously. Graphic processing units (GPUs) were
computed in each stage. As such, it begs for a different            originally dedicated to processing graphics. However, in
algorithm that suits multi-dimensional optimization                 recent years, GPUs are being increasingly popular as a
problems in an accurate and efficient way.                          general-purpose parallel computing tool in image
                                                                    processing, data mining, and machine learning.
Differential Evolution Search                                          In our previous work, we have implemented GPU
DE is an evolutionary computation algorithm to optimize             computing to accelerate ADO computing. Compared with
nonlinear and non-differentiable continuous functions by            CPU-based ADO, GPU-based ADO is around 100 times
keeping track of, iteratively evolving and updating                 faster, which substantiates the feasibility of using GPU
multiple particles. A brief explanation of the algorithm is         computing to accelerate the computational speed of ADO
as follows. To search the global maximum of a D-                    computing (Gu, 2012). Given that the DE algorithm is
dimensional space, it keeps track of NP D-dimensional               intrinsically parallelizable, GPU computing can be
vectors                           , where NP is the number of       beneficial for accelerating the computation.
particles and G the generation index. At the beginning,                In the present work, we implemented DE on graphic
the vectors can be randomly selected. Then for each target          processing units (GPUs) to speed up the ADO
vector , a mutant vector vi,G+1 for the next stage is               computation.
generated by                                (         ) where
r1, r2 and r3 are randomly chosen integers from 1 to NP                                          Simulations
except i, and F is a constant factor controlling the                ADO-based parameter estimation of the psychophysical
contribution of the difference of the two randomly chosen           model in Kontsevich and Tyler (1999) was simulated with
vectors. The next step, crossover, creates a trial vector for       artificial data under the assumption that the data are from
each target vector with each element either from the                a stationary process with no variation of lapses or learning.
mutant vector        or the target vector . Then the cost           The data-generating model was defined in the following
function values of both the target vector             and the       equations
mutant vector         are computed. If the mutant vector                                  Y  Binomial (1,  ( x));
   yields a smaller cost, the target vector is set to .
Otherwise, the target vector is retained from the last                                     ( x)  (r ( x) / 2;   0,   1);
generation. DE is illustrated in Figure 4 with a simple toy                               r ( x)  10^ (10s ( x  t )),
example in which DE was used to search the global
maximum of a bimodal distribution.                                  in    which                                 ∫                                Y
                                                                                                          √
                                                                    represents the experimental observation; x, t and s are the
      0.1                            0.1                            design variable and the parameters, threshold and slope,
                                                                    transformed in log decimal scale. The range of x, t and s
     0.05                           0.05                            are set to be (0, 3), (0, 3) and (log100.7, log107),
                                                                    respectively. The prior distributions of t and s are both
        0                              0                            uniform. In the simulation, the true values for t and s are
          -2                             -2                         set to be 1.5 and log103.5 or approximately 0.544.
                                2                          2
         X2
             0             0            X2 0          0
               2       -2    X1               2    -2   X1             Multiple designs are optimized at the same time in one
Figure 4: Illustration of DE algorithm searching for the            stage by DE algorithm. Computationally, DE is used to
global maximum of a 2-dimensional bimodal distribution.             search for the global maximum of the defined utility
Initially (left), the particles are randomly selected. At 30th      function. For a two-alternative forced choice (2AFC)
generation (right), they converged to the larger mode.              problem, the response y is either 0 or 1. So the utility
                                                                    function of an n-dimensional space can be written as
 DE is a natural approach to our problem of optimizing
multiple designs per stage simultaneously. Because                       U (d )     u(d ...d  1    n , y1 ...yn ,  )P( y1 ...yn |  ) P( ),
                                                                                   yi  0,1
different particles can be processed independently in one
                                                                    in which the parameter space θ is also discretized so that
stage, DE can benefit from parallel computing.
                                                                    the integral in Equation (1) becomes a summation. The
                                                               2455

local utility u(d1...dn, y1...yn, θ) is in the form of mutual                                                   iI
                                                                                                                 (    i        true )
information                  .                                                                    bias ( )    i 1
                                                                                                                                             20dB，
                                                                                                                            I
   First   the     two-designs-per-stage       scheme    was                                                     iI
implemented. Five two-dimensional particles were                                                                 (        i    true ) 2
generated and shown to be enough for the convergence,                                             SD( )        i 1
                                                                                                                                               20dB.
                                                                                                                            I 1
which was evaluated by the closeness of the particles at
the last generation. Until the 50th generation, the 5
particles are identical up to the second decimal number,
indicating that 50 generations are enough for DE to locate
the maximum of the utility space. The algorithm was
coded in parallel computing with a single GPU card,
Tesla C2050 by Nvidia, which contains 448 CUDA cores.                                                                                Posterior PDF of threshold t
A third party library in C++, Arrayfire, is called to access
the GPU computing function.
   One experiment contains a total of 150 stages or 300
trials. To visualize the effect of parameter estimation, the
                                                                                              Posterior PDF of slope s
model predictions based on the prior distribution and the
posterior distribution at the last stage is shown in Figure 5.                     Figure 6: The joint and marginal posterior distributions of
On the left, the model prediction is based on the initial                          threshold and slope at the 300th trial.
uniform distribution of the two parameters. On the right,
the prediction is based on the posterior distribution of the                       To compare the two-designs-per-stage scheme with the
150th stage of the two parameters. Compared to the initial                         traditional one-design-per-stage scheme, we ran 100
stage, the range of the likely outcome of the model is                             experiments of 300 stages with one design in each stage
much narrowed and concentrated, indicating the                                     and computed the bias and standard deviation of the
convergence of the estimation.                                                     estimates in each stage. Figure 7(a) shows the comparison
                     Initial state                      150th stage                between the two different schemes. In the later trials, the
              1                                  1
                                                                                   two different schemes do not seem to have significant
              0.9                                0.9                               differences. There is no significant bias at the 300 th trial
                                                                                   for both threshold and slope. The standard deviation of
              0.8                                0.8                               threshold is about 0.2dB and that of slope is about 1.1dB.
       (x)                               (x)                                     Although the two-designs-per-stage scheme has less
              0.7                                0.7                               fluctuation in the early stages in the bias of threshold, the
                                                                                   difference may result from the random effect.
              0.6                                0.6                                 Next, the five-designs-per-stage scheme was
                                                                                   implemented. Because the dimension increases, 200
              0.5
                 0   0.6 1.2 1.8 2.4 3
                                                 0.5                               generations are needed for DE to converge. One hundred
                                                    0   0.6 1.2 1.8 2.4 3
                     x (Contrast)                        x (Contrast)              experiments of 60 stages (300 trials in total still) were run
Figure 5: The model predictions based on the prior                                 and the point estimates were computed for each stage.
distribution (left) and the posterior distribution at the 150th                    Figure 7(b) shows the comparison between the five-
stage (right). Darker colors indicate high probabilities.                          designs-per-stage and the one-designs-per-stage schemes.
                                                                                   We can see that there is much less fluctuation in the bias
The joint and marginal posterior distributions of threshold                        of threshold for five-designs-per-stage than that of one-
and slope at the end of the experiment are shown in                                design-per-stage at the early trials, which is consistent
Figure 6. Both the posterior distributions tend to converge                        with the improvement in the two-designs-per-stage
to the true values of the parameters. Conforming to the                            scheme. Other than that, there is no obvious difference
previous studies, the estimation of the threshold is more                          between the two schemes.
accurate and has less variation in its posterior distribution                         As expected, simply increasing the number of designs
while the estimation of slope is less stable.                                      in one stage while still keeping the total number of trials
   In each stage, one point estimate is computed for each                          constant resulted in improvement in the accuracy of
parameter by calculating the mean of the distribution. 100                         parameter estimation, at least at the early stages. As we
experiments of 150 stages were run. Let be the point                               hypothesized, the relationship or synergy provided by
estimate in each stage, and          be the true parameter                         multiple responses is greater or at least different than the
value, each in log decimal scale. Then we can compute                              sum of the information from single responses. We expect
the average bias and standard deviation of the estimation                          that such improvement can be more obvious when it is
in each stage across the 100 experiments by                                        applied to more complex models because in those cases,
                                                                                   more trials are needed for simply exploring the model in
                                                                            2456

the early stages of an experiment. However, we should              holistics in Gestalt psychology and the principle in
not expect that the performance continues to improve as            information theory, with the multiple responses offering
the number of designs per stage increases. By the                  extra information than the sum of the individual responses.
principle of ADO, a good design should be based on solid             To realize the optimization of multiple designs in one
information conveyed by the participants’ responses. A             stage, we integrated the adaptive design optimization
large number of designs per stage may probe into                   framework with an evolutionary computation algorithm,
unfruitful regions of the design space. A balance must be          differential evolution, which is specialized in searching a
sought in deciding how many designs per stage are good             multi-dimensional space for the purpose of optimization.
for different models.                                              DE can also be naturally applied to models that contain
                                                                   multiple design variables, for which brute-force grid
                                                                   search is usually applied. DE is less computationally
                                                                   demanding than grid search when the design space is
                                                                   large. Other than that, DE can also benefit from parallel
                                                                   computing to accelerate the computation within each
                                                                   experimental stage.
                                                                     As such, DE-based adaptive design optimization has
                                                                   large potential of applications in the future experiments
                                                                   for parameter estimation.
                                                                                           References
                                                                   Cavagnaro, D. R., Myung, J. I., Pitt, M. A., & Kujala, J. V.
                               (a)
                                                                        (2010). Adaptive design optimization: A mutual
                                                                        information based approach to model discrimination
                                                                        in cognitive science. Neural Computation, 22(4):
                                                                        887-905.
                                                                   Cavagnaro, D.R., Pitt, M.A., & Myung, J.I. (2011). Model
                                                                        discrimination through adaptive experimentation.
                                                                        Psychonomic Bulletin & Review, 18(1), 204-210.
                                                                   Gu, H. (2012). Graphic-Processing-Units Based Adaptive
                                                                        Parameter Estimation of a Visual Psychophysical
                                                                        Model. Master thesis submitted to the Department of
                                                                        Psychology of the Ohio State University.
                                                                   Kontsevich, L. & Tyler, C. (1999). Bayesian adaptive
                                                                        estimation of psychometric slope and threshold.
                                (b)                                     Vision Research, 39, 2729-2737.
Figure 7: The comparison of one design per stage with              Lesmes, L.A., Lu, Z., Baek, J., & Albright, T.D. (2010).
two designs per stage (a) and five designs per stage (b) in             Bayesian adaptive estimation of the contrast
the bias and standard deviation of the estimates of                     sensitivity function: the quick CSF method. Journal
threshold and slope.                                                    of Vision, 10(3), 17, 1-21.
                                                                   Lewi, J., Butera, R., & Paninski, L. (2009). Sequential
                        Conclusion                                      optimal design of neurophysiology experiments.
                                                                        Neural Computation, 21, 619-687.
In psychophysical studies, many endeavors have been
                                                                   Miller, F., Dette, H., & Guilbaud, O. (2007). Optimal
made to bring further efficiency to the process in
                                                                        designs for estimating the interesting part of a dose-
parameter estimation. One clear direction is in global
                                                                        effect curve. Journal of Biopharmaceutical Statistics,
optimization or multiple steps ahead to improve the
                                                                        17, 6.
current greedy method that only evaluates the design
                                                                   Myung, J. I., Cavagnaro, D. R. & Pitt, M. A. (2012). A
utilities at the next stage. If global optimization provides
                                                                        tutorial on adaptive design optimization. Manuscript
the ultimate solution, the approach we studied in this
                                                                        submitted for publication.
paper, multiple designs per stage, is an initial step in this
                                                                   Storn, R. & Price, K. (1997). Differential evolution – a
direction. Thus, in this paper, we sought one eclectic
                                                                        simple and efficient heuristic for glabal optimization
choice between the traditional one-shot experimental
                                                                        over continuous spaces. Journal of Global
design at the very beginning of an experiment and the
                                                                        Optimization 11: 341-359.
advanced adaptive experimentation with only one design
                                                                   Wichmann, F. A. & Hill, N. J (2001). The psychometric
per stage. The results showed that multiple designs per
                                                                        function: I. fitting, sampling and goodness of fit.
stage can benefit the estimation in the early stages of an
                                                                        Perception & Psychophysics, 63(8),1293-1313.
experiment. The reason for the benefit is reminiscent of
                                                              2457

