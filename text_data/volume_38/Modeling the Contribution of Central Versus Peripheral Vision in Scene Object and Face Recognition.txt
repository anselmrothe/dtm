  Modeling the Contribution of Central Versus Peripheral Vision in Scene, Object,
                                                      and Face Recognition
                                                  Panqu Wang (pawang@ucsd.edu)
                            Department of Electrical and Engineering, University of California San Diego
                                              9500 Gilman Dr 0407, La Jolla, CA 92093 USA
                                                Garrison W. Cottrell (gary@ucsd.edu)
                        Department of Computer Science and Engineering, University of California San Diego
                                              9500 Gilman Dr 0404, La Jolla, CA 92093 USA
                              Abstract                                  mid-level visual areas (V1-V4), but also in higher-level re-
   It is commonly believed that the central visual field (fovea and     gions, where perception and recognition for faces or scenes
   parafovea) is important for recognizing objects and faces, and       is engaged (Malach, Levy, & Hasson, 2002; Grill-Spector &
   the peripheral region is useful for scene recognition. However,      Malach, 2004). More specifically, Malach et al. (2002) pro-
   the relative importance of central versus peripheral informa-
   tion for object, scene, and face recognition is unclear. Larson      posed that the need for visual resolution is a crucial factor
   and Loschky (2009) investigated this question in the context of      in organizing object areas in higher-level visual cortex: ob-
   scene processing using experimental conditions where a cir-          ject recognition that depends more on fine detail is associated
   cular region only reveals the central visual field and blocks
   peripheral information (”Window”), and in a ”Scotoma” con-           with central-biased representations, such as faces and words;
   dition, where only the peripheral region is available. They          object recognition that depends more on large-scale integra-
   measured the scene recognition accuracy as a function of vi-         tion is associated with peripheral-biased representations, such
   sual angle, and demonstrated that peripheral vision was indeed
   more useful in recognizing scenes than central vision in terms       as buildings and scenes. This hypothesis is supported by
   of achieving maximum recognition accuracy. In this work,             fMRI evidence, which shows that the brain areas that are
   we modeled and replicated the result of Larson and Loschky           more activated for faces (FFA; Kanwisher, McDermott, and
   (2009), using deep convolutional neural networks (CNNs).
   Having fit the data for scenes, we used the model to predict         Chun (1997)) and words (VWFA; McCandliss, Cohen, and
   future data for large-scale scene recognition as well as for ob-     Dehaene (2003)) sit in the eccentricity band expanded by
   jects and faces. Our results suggest that the relative order of      central visual-field bias, whereas buildings and scenes (PPA;
   importance of using central visual field information is face
   recognition>object recognition>scene recognition, and vice-          Epstein, Harris, Stanley, and Kanwisher (1999)) are asso-
   versa for peripheral information. Furthermore, our results pre-      ciated with peripheral bias. More recent studies even sug-
   dict that central information is more efficient than peripheral      gest that the central-biased pathway for recognizing faces and
   information on a per-pixel basis across all categories, which is
   consistent with Larson and Loschky’s data.                           peripheral-biased pathway for recognizing scenes are segre-
   Keywords: face recognition; object recognition; scene recog-         gated by mid-fusiform sulcus (MFS) to enable fast parallel
   nition; central and peripheral vision; deep neural networks          processing (Gomez et al., 2015).
                          Introduction                                     In the domain of behavioral research, studies have shown
Viewing a real-world scene occupies the entire visual field,            that object perception performance is the best around 1◦ -2◦
but the visual resolution across the visual field varies. The           of fixation point and drops rapidly as eccentricity increases
fovea, a small region in the center of the visual field that sub-       (Henderson & Hollingworth, 1999; Nelson & Loftus, 1980).
tends approximately 1◦ of visual angle (Polyak, 1941), per-             For scene recognition, Larson and Loschky (2009) used a
ceives the highest visual resolution of 20 to 45 cycles/degree          ”Window” and ”Scotoma” design (see Figure 1), to test the
(cpd) (Loschky, McConkie, Yang, & Miller, 2005). The                    contributions of central versus peripheral vision to scene
parafovea has a slightly lower visual resolution and extends            recognition. The Window condition (top rows of the right-
to about 4-5◦ eccentricity, where the highest density of rods           hand columns of Figure 1) presents central information at
is found (Wandell, 1995). Beyond the parafovea is generally             various visual angles to the subjects, while the Scotoma con-
considered to be peripheral vision (Holmes, Cohen, Haith, &             dition (second row on the right) blocks it. Using images from
Morrison, 1977), which receives the lowest visual resolution.           10 categories, subjects were required to verify the category
Due to the high density and small receptive field of retinal            in each condition. The recognition accuracy as a function of
receptors,the central (foveal and parafoveal) vision encodes            visual angle is shown in Figure 2. They found that foveal
information of higher spatial frequency and more detail; pe-            vision is not accurate for scene perception, while peripheral
ripheral vision, on the contrary, encodes coarser and lower             vision is, despite its much lower resolution. However, they
spatial frequency information.                                          also found that central vision is more efficient, in the sense
   This retinotopic representation of the visual field is mapped        that less area is needed to achieve equal accuracy. The visual
to visual cortical areas through a log-polar representation.            area is equal at 10.8◦ , and the crossover point, where central
Recent studies have shown that orderly central and periph-              vision starts to perform better than peripheral, is to the left of
eral representations can be found not only in low-level to              that point.
                                                                    1409

   Despite the common belief that central vision is important
for face and object recognition, and peripheral vision is im-
portant for scene perception shown in studies above, a careful
examination of the contribution of central versus peripheral
vision in object, scene, and face recognition is needed. In this
work, we modeled the experiment of Larson and Loschky
(2009) using deep convolutional neural networks. Further-
more, we extended the modeling work to a greater range of
stimuli, and answer the following questions: How does the
model perform as the number of scene categories is scaled
up? Besides scenes, can the model predict the importance
of central vision versus peripheral information in object and
face recognition? What is the result compared to scenes?
   In the following, we show that our modeling results match
the observations of Larson and Loschky (2009), and that it
scales up to over 200 scene categories. By running a sim-            Figure 1: Examples of images used in our experiment. First
ilar analysis for large-scale object and face recognition, our       column: original images. Second column: foveated images.
model predicts that central vision is very important for face        Third to last column: images processed through ”Window”
recognition, important for object recognition, and less impor-       and ”Scotoma” conditions with different radii in degrees of
tant for scene recognition. Peripheral vision, however, serves       visual angle.
an important role for scene recognition, but is less impor-
tant for recognizing objects and faces. Furthermore, across
                                                                     3◦ , 7◦ , 9◦ , 12◦ , and 16◦ . The example Window and Scotoma
all conditions we tried, central vision is more efficient than
                                                                     images are shown in Figure 1.
peripheral vision on a per-pixel basis (when equal areas are
presented), which is consistent with the result of Larson and        Deep Convolutional Neural Networks (CNNs)
Loschky (2009).
                                                                     Deep CNNs are neural networks with many layers that stack
                           Method                                    computations in a hierarchical way, repeatedly performing:
Image Preprocessing                                                  1) 2-dimensional convolutions over the stimulus generated
                                                                     from previous layers using learned filters, which are con-
To create foveated images, we preprocessed the images using
                                                                     nected locally to a small subregion of the visual field; 2) a
the Space Variant Imaging System1 . To mimic human vision,
                                                                     pooling operation on local regions of the feature maps ob-
we set the parameter that specifies the eccentricity at which
                                                                     tained from convolution operation, which is used to reduce
resolution drops to half of the fovea to 2.3◦ . Example images
                                                                     the dimensionality and gain translational invariance; 3) non-
and their preprocessed retinal versions are shown in the first
                                                                     linearities to the upstream response, which is used to gener-
and second columns of Figure 1.
                                                                     ate more discriminative features useful for the task. As layers
   As in the experiments of Larson and Loschky (2009), we
                                                                     go higher, the receptive fields of filters are generally larger,
used the Window and Scotoma paradigms as specified by van
                                                                     and the learned features go from low-level (edges, contours)
Diepen, Wampers, and dYdewalle (1998) to process the input
                                                                     to high-level object-related representations (object parts and
stimulus. The idea of both paradigms is to evaluate the value
                                                                     shapes) (Zeiler & Fergus, 2014). Several fully-connected lay-
of missing information - if the missing information is needed,
                                                                     ers are usually added on top of these computations to learn
then the perception process may be disrupted and recognition
                                                                     more abstract and task-related features.
performance may drop; if the missing information is not nec-
                                                                        We used deep CNNs in our experiments for two reasons.
essary, then the processing remains normal.
                                                                     First, deep CNNs are the best models in computer vision:
   Input images in our experiments are 256 × 256 pixels, and
                                                                     they achieve the state-of-the-art performance on many large-
we assume that corresponds to 27◦ × 27◦ of visual angle,
                                                                     scale computer vision tasks, such as image classification
the number in (Larson & Loschky, 2009). In (Larson &
                                                                     (Krizhevsky, Sutskever, & Hinton, 2012; He, Zhang, Ren,
Loschky, 2009), they used four sets of radius conditions for
                                                                     & Sun, 2015), object detection (Ren, He, Girshick, & Sun,
Windows and Scotomas: 1◦ represents the presence or ab-
                                                                     2015), and scene recognition (Zhou, Lapedriza, Xiao, Tor-
sence of foveal vision; 5◦ represents the presence or absence
                                                                     ralba, & Oliva, 2014). Thus, the models should achieve de-
of central vision; 10.8◦ presents equal viewable area inside
                                                                     cent performance in our experiments. Smaller networks or
the Windows or outside the Scotomas; 13.6◦ presents more
                                                                     other algorithms are not competent for our tasks. Second,
viewable area in the Windows than the Scotomas. In order
                                                                     deep CNNs have been shown to be the best models of the vi-
make the prediction of the model more accurate, we added
                                                                     sual cortex: they are able to explain a variety of neural data
five additional radius conditions in all of our experiments:
                                                                     in human and monkey IT (Yamins et al., 2014; Güçlü & van
    1 http://svi.cps.utexas.edu/software.shtml                       Gerven, 2015; Wang, Malave, & Cipollini, 2015). As a result,
                                                                 1410

 it is natural to use them in our work modeling a behavioral
 study related to human vision.
                         Experiments
 In this section, we first describe our model of the behavioral
 study of Larson and Loschky (2009). We then introduce the
 experiment for measuring the contribution of central versus
 peripheral vision for large-scale scene, object, and face recog-
 nition tasks.
 Modeling Larson and Loschky (2009)
 In Larson and Loschky (2009), scene recognition accuracy
 was measured across 100 human subjects on 10 categories:             Figure 2: Results for scene recognition accuracy as a func-
 Beach, Desert, Forest, Mountain, River, Farm, Home, Mar-             tion of viewing condition (Windows (w) and Scotomas (s))
 ket, Pool, and Street. For each trial in the Windows and Sco-        and visual angle. Left: result of Larson and Loschky (2009).
 tomas conditions, subjects were first presented a scene im-          Right: our modeling result.
 age, and then were asked to press ”yes” or ”no” for the cue
 (category name) presented on the screen. Their experimental          initializing the weights of the last layer to be random with
 result is summarized in Figure 2. They showed that central           zero mean and unit variance. To be compatible with the ”yes”
 vision (5◦ window condition) performs less well than periph-         or ”no” condition in the behavioral experiment, we replaced
 eral vision in terms of getting maximum recognition perfor-          the last layer in the networks with a single logistic unit, and
 mance. They further demonstrated the peripheral advantage            trained the networks for each of the 10 object categories sep-
 is due to more viewing areas in the Scotomas conditions, and         arately, using half of the training images from the target cat-
 central vision is more privileged when given equal viewable          egory and half randomly selected from all other 9 categories.
 areas (10.8◦ ).                                                      As the last layer needs more learning, we set the learning rate
    We obtained the stimuli of the above 10 categories from           of the last layer to 0.001, and all previous layers to 1e−4 . The
 the Places205 database (Zhou et al., 2014), which contains           training set of the 10 scene categories contains a total num-
 205 scene categories and 2.5 million images. All input stim-         ber of 129,210 full resolution images, and we trained all net-
 uli were preprocessed using the retina model described in the        works using minibatch stochastic gradient descent with batch
 above section. As 10 categories is small and can easily lead         size from 32 to 256, using the Caffe deep learning framework
 to overfitting problems in training deep CNNs, we trained            (Jia et al., 2014) on NVIDIA Titan Black 6GB GPUs. All
 our recognition model by performing fine-tuning (or transfer         networks were trained for a maximum number of 24,000 it-
 learning) based on pretrained models. The model pretrained           erations to ensure convergence. Each test set contains 200
 on the Places205 database can be treated as a mature scene           images (100 from target category and 100 from all other cat-
 recognition pathway, and fine-tuning can be thought as addi-         egories), and the label distribution is the same as the training
 tional training for the task. To investigate whether different       set.. Test images were preprocessed to meet each of the Win-
 network architectures, especially depth, have different impact       dows and Scotomas condition. We tested the performance of
 on the modeling result, we applied three different pre-trained       the fine-tuned models on all conditions by reporting the mean
 models, namely:                                                      classification accuracy, which is shown in Figure 2.
                                                                         From Figure 2, we can clearly see our result for all
1. AlexNet (Krizhevsky et al., 2012): A network with 5 con-           three models qualitatively matches the result of Larson and
    volutional layers and 3 fully connected layers, about 60          Loschky (2009). First, for Window and Scotoma conditions,
    million trainable parameters. Achieved 81.10% top-5 ac-           an increasing radius of visual angle (x axis) yields a mono-
    curacy on the Places205 validation set.                           tonic increase or decrease in classification accuracy (y axis).
                                                                      The sharper increase from 1◦ to 5◦ in the behavioral study
2. VGG-16 (Simonyan & Zisserman, 2014): A network with                may be due to the higher efficiency of human central vision.
    13 convolutional layers and 3 fully connected layers, about       Second, we replicated the fact that central vision (less than
    138 million trainable parameters. Achieved 85.41% top-5           5◦ ) is less useful than peripheral vision in terms achieving the
    accuracy on the Places205 validation set.                         best scene recognition performance. Third, however, when
3. GoogLeNet (Szegedy et al., 2015): A network with 21                using equal viewable areas (10.8◦ ), central vision performs
    convolutional layers and 1 fully connected layer, about 6.8       better than peripheral, exhibiting higher efficiency. Fourth,
    million trainable parameters. Achieved 87.70% top-5 ac-           the critical radius (the crossover point where the two con-
    curacy on the Places205 validation set.                           ditions produce equal performance, see Figure 2b) is 8.26◦
                                                                      (averaged across all models), which is within the 8.22◦ -9.24◦
 For all models, the fine-tuning process starts by keeping the        range reported by Larson and Loschky (2009). This suggests
 weights except for the last fully connected layer intact, and        our models are quite plausible.
                                                                  1411

Figure 3: Results for large-scale scene recognition accuracy           Figure 4: Results for large-scale object recognition accuracy
as a function of viewing condition (Windows (w) and Sco-               as a function of viewing condition (Windows (w) and Sco-
tomas (s)) and visual angle. Softmax output is used instead            tomas (s)) and visual angle. Softmax output is used instead
of logistic unit, so chance is 0.005. Left: experiment using           of logistic unit, so chance is 0.001. Left: experiment with
original images. Right: experiment using foveated images.              original images. Right: experiment with foveated images.
   When comparing the performance across the three models
we use, we cannot find a notable difference in terms of perfor-        which contains 1000 object categories and over 1.2 million
mance, though GoogLeNet usually performs slightly better,              training images. We used the pretrained models of AlexNet,
indicating that depth of processing might be the key factor in         VGG-16, and GoogLeNet, which achieve top-5 accuracy of
obtaining better performance.                                          80.13%, 88.44%, and 89.00%, respectively, on the ILSVRC
                                                                       2012 validation set. Similar to scene recognition, we tested
Large-Scale Scene, Object, and Face Recognition                        all models under all Windows and Scotoma conditions, us-
                                                                       ing original and foveated images. The results are shown in
The above modeling work is based on a scene recognition                Figure 4.
task using 10 categories. In real life, however, there are a
much larger number of scene categories. Beyond scenes, gen-               At the first glance of looking at Figure 4, we may draw the
eral object recognition and face recognition are the two most          conclusion that the result is the same as scene recognition:
important recognition tasks that are performed regularly. The          central vision is still more important than peripheral vision.
relative importance of central versus peripheral vision among          However, when we compare the scene and object recogni-
the three categories needs to be examined carefully. Using a           tion results (shown in Figure 5), we can clearly see that cen-
similar modeling approach, we describe our findings in large-          tral information in object recognition is more important than
scale scene, object, and face recognition in the sections below.       that in scene recognition: the accuracy of the Scotoma con-
                                                                       ditions drops much faster for object recognition than scene
Scene Recognition We used all 205 categories in the                    recognition as visual angle increases from 1◦ to 7◦ , suggest-
Places205 dataset. The trained models of AlexNet, VGG-16,              ing that losing central vision causes a greater impairment for
and GoogLeNet are deployed to examine the recognition ac-              object recognition performance. This is consistent with our
curacy on the Place205 validation set, which contains 20, 500          knowledge that central vision plays a more important role in
images, in all Windows and Scotoma conditions. In addition,            object recognition than scenes, as there are more high spa-
we tested the models using images both processed and un-               tial frequency details in objects than scenes. Another find-
processed by the retina model to examine the generalization            ing from this experiment is that AlexNet (8 layers) performs
power of the learned features. The result is shown in Figure 3.        much worse than VGG-16 (16 layers) and GoogLeNet (23
   From Figure 3, we can see the general trend that we ob-             layers), suggesting that depth is important to produce good
served in Figure 2 still holds: peripheral vision is more im-          performance.
portant than central vision, but central vision is more efficient.
                                                                       Face Recognition We performed the face recognition ex-
All models behave similarly. However, we can see the perfor-
                                                                       periment on the Labeled Faces in the Wild (LFW) dataset
mance on images preprocessed through the retina model is
                                                                       (Huang, Ramesh, Berg, & Learned-Miller, 2007), which con-
inferior. Apparently, since there are many more categories in
                                                                       tains 13, 233 labeled images from 5, 749 individuals. As there
this experiment, the foveation has more of an effect. Recall
                                                                       is only 1 image for some identities, researchers usually pre-
that the models are trained using images with full resolution;
                                                                       train their network on larger datasets (not publicly available)
missing the peripheral information may the cause learned fea-
                                                                       and test their models on the LFW dataset. In this experiment,
tures to imperfectly generalize.
                                                                       we tested three pretrained models, namely Lighten-A (10 lay-
Object Recognition We ran our object recognition experi-               ers; (Wu, He, & Sun, 2015)), Lighten-B (16 layers), and
ment on the ILSVRC 2012 dataset (Russakovsky et al., 2015),            VGG-Face (16 layers;(Parkhi, Vedaldi, & Zisserman, 2015)),
                                                                   1412

Figure 5: Comparison results for scene and object recognition        Figure 6: Results for large-scale face recognition accuracy
using the VGG-16 model. Losing central vision decreases              as a function of viewing condition (Windows (w) and Sco-
performance for object recognition more quickly than scene           tomas (s)) and visual angle. Left: experiment with origi-
recognition. Left: original images. Right: foveated images.          nal images. Right: experiment with foveated images. For
                                                                     Lighten-A and Lighten-B models, the visual angle only ex-
on the face verification task for the LFW dataset, where they        pands to 9.5◦ , as the input image is smaller (144 × 144) than
achieve accuracy of 90.33%, 92.37%, and 96.23%, respec-              for the VGG model (256 × 256). The accuracy for face verifi-
tively. Face images were preprocessed so that they occupy            cation task is measured as the true positive rate at Equal Error
the entire visual field (Figure 1). Same as the previous ex-         Rate (EER) point on the ROC curve. Chance is 0.5.
periments, we tested all models using Windows and Scotoma
conditions, with original and foveated images. Results are           presented, but the accuracy still suffers due to the loss of cen-
shown in Figure 6.                                                   tral vision. However, peripheral information is important for
   We see very different performance in Figure 6 compared            object and scene recognition (and more important for scene
to object and scene recognition. First, central information          recognition, as shown in Figure 5).
is obviously much more important than peripheral informa-               These large-scale scene, object, and face recognition mod-
tion for face recognition, given the accuracy at 5◦ is much          eling results suggest there is an order of relative importance
higher for the Window condition than the Scotoma condition           of central versus peripheral vision in those tasks: peripheral
for Lighten models, and very similar with each other for the         vision is most important for scene recognition, less impor-
VGG model. This is consistent with our intuition that face           tant for object recognition, and basically not helpful for face
recognition is a fine-grained discrimination process. Second,        recognition. Central vision, however, plays a crucial role in
the Window performance grows much more slowly after 7◦ ,             face recognition, is important for object recognition, and is
suggesting the more peripheral region provides little addi-          less important for scene recognition.
tional information for recognizing faces, unlike objects and
scenes, which needs lots of peripheral information to obtain                                  Conclusion
the maximal accuracy. Third, the foveated images produce             In this paper, we modeled the contribution of central ver-
nearly identical results as the original image, demonstrating        sus peripheral visual information for scene, object, and face
that face recognition only involves central vision, and the          recognition, using deep CNNs. We first modeled the behav-
blurred peripheral vision is not needed.                             ioral study of Larson and Loschky (2009), and replicated their
   Finally, as central vision appears to be more efficient (on a     findings of the importance of peripheral vision in scene recog-
per-pixel basis) than peripheral vision in all experiments we        nition. In addition, by running a large-scale scene, object, and
tried, we tested the relative efficiency of the central vision       face recognition simulation, our models make testable predic-
over peripheral vision by measuring the recognition accuracy         tions for the relative order of importance for central versus
as a function of viewable area. The result is shown in Fig-          peripheral vision for those tasks.
ure 7.
   From Figure 7, we can clearly see that the recognition ac-                            Acknowledgments
curacy of central vision is always superior than peripheral          This work was supported by NSF grants IIS-1219252 and
vision for all tasks. However, central vision is even more           SMA 1041755 to GWC. PW was supported by a fellowship
efficient when recognizing faces than recognizing objects or         from Hewlett-Packard.
scenes, as viewable areas over 50% of the whole image can
only provide a limited boost for face recognition, while sig-                                 References
nificantly improving the accuracy of object and scene recog-         Epstein, R., Harris, A., Stanley, D., & Kanwisher, N. (1999).
nition. Contrarywise, peripheral information provides little to         The parahippocampal place area: Recognition, navigation,
no help for face recognition, unless over 90% of the image is           or encoding? Neuron, 23(1), 115–125.
                                                                 1413

                                                                     of central versus peripheral vision to scene gist recognition.
                                                                     Journal of Vision, 9(10), 6.
                                                                   Loschky, L., McConkie, G., Yang, J., & Miller, M. (2005).
                                                                     The limits of visual resolution in natural scene viewing. Vi-
                                                                     sual Cognition, 12(6), 1057–1092.
                                                                   Malach, R., Levy, I., & Hasson, U. (2002). The topogra-
                                                                     phy of high-order human object areas. Trends in cognitive
                                                                     sciences, 6(4), 176–184.
                                                                   McCandliss, B. D., Cohen, L., & Dehaene, S. (2003). The
                                                                     visual word form area: expertise for reading in the fusiform
                                                                     gyrus. Trends in cognitive sciences, 7(7), 293–299.
                                                                   Nelson, W. W., & Loftus, G. R. (1980). The functional vi-
                                                                     sual field during picture viewing. Journal of Experimental
Figure 7: Accuracy for object (left), scene (middle) and face        Psychology: Human Learning and Memory, 6(4), 391.
(right) recognition as a function of the percentage of view-       Parkhi, O. M., Vedaldi, A., & Zisserman, A. (2015). Deep
able area presented under Window (blue) and Scotoma (red)            face recognition. In British machine vision conference.
conditions, using original (solid line) and foveated images        Polyak, S. L. (1941). The retina.
(dashed line).                                                     Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster r-
                                                                     cnn: Towards real-time object detection with region pro-
Gomez, J., Pestilli, F., Witthoft, N., Golarai, G., Liberman,        posal networks. In Advances in neural information pro-
   A., Poltoratski, S., . . . Grill-Spector, K. (2015). Func-        cessing systems (pp. 91–99).
   tionally defined white matter reveals segregated pathways       Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
   in human ventral temporal cortex associated with category-        Ma, S., . . . Fei-Fei, L. (2015, April). ImageNet Large
   specific processing. Neuron, 85(1), 216–227.                      Scale Visual Recognition Challenge. International Journal
Grill-Spector, K., & Malach, R. (2004). The human visual             of Computer Vision (IJCV), 1-42. doi: 10.1007/s11263-
   cortex. Annu. Rev. Neurosci., 27, 649–677.                        015-0816-y
Güçlü, U., & van Gerven, M. A. (2015). Deep neural net-         Simonyan, K., & Zisserman, A. (2014). Very deep convo-
   works reveal a gradient in the complexity of neural repre-        lutional networks for large-scale image recognition. arXiv
   sentations across the ventral stream. The Journal of Neuro-       preprint arXiv:1409.1556.
   science, 35(27), 10005–10014.                                   Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep                   Anguelov, D., . . . Rabinovich, A. (2015, June). Going
   residual learning for image recognition. arXiv preprint           deeper with convolutions..
   arXiv:1512.03385.                                               van Diepen, P. M., Wampers, M., & dYdewalle, G. (1998).
Henderson, J. M., & Hollingworth, A. (1999). The role of fix-        Functional division of the visual field: Moving masks and
   ation position in detecting scene changes across saccades.        moving windows. Eye guidance in reading and scene per-
   Psychological Science, 10(5), 438–443.                            ception, 337–355.
Holmes, D. L., Cohen, K. M., Haith, M. M., & Morrison,             Wandell, B. A. (1995). Foundations of vision. Sinauer Asso-
   F. J. (1977). Peripheral visual processing. Perception &          ciates.
   Psychophysics, 22(6), 571–577.                                  Wang, P., Malave, V., & Cipollini, B. (2015). Encoding
Huang, G. B., Ramesh, M., Berg, T., & Learned-Miller, E.             voxels with deep learning. The Journal of Neuroscience,
   (2007). Labeled faces in the wild: A database for study-          35(48), 15769–15771.
   ing face recognition in unconstrained environments (Tech.       Wu, X., He, R., & Sun, Z. (2015). A lightened cnn for deep
   Rep. No. 07-49). University of Massachusetts, Amherst.            face representation. arXiv preprint arXiv:1511.02683.
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J.,        Yamins, D. L., Hong, H., Cadieu, C. F., Solomon, E. A., Seib-
   Girshick, R., . . . Darrell, T. (2014). Caffe: Convolutional      ert, D., & DiCarlo, J. J. (2014). Performance-optimized hi-
   architecture for fast feature embedding. arXiv preprint           erarchical models predict neural responses in higher visual
   arXiv:1408.5093.                                                  cortex. Proceedings of the National Academy of Sciences,
Kanwisher, N., McDermott, J., & Chun, M. M. (1997). The              111(23), 8619–8624.
   fusiform face area: a module in human extrastriate cor-         Zeiler, M. D., & Fergus, R. (2014). Visualizing and under-
   tex specialized for face perception. The Journal of Neu-          standing convolutional networks. In Computer vision–eccv
   roscience, 17(11), 4302–4311.                                     2014 (pp. 818–833). Springer.
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Im-         Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., & Oliva, A.
   agenet classification with deep convolutional neural net-         (2014). Learning deep features for scene recognition using
   works. In Advances in neural information processing sys-          places database. In Advances in neural information pro-
   tems (pp. 1097–1105).                                             cessing systems (pp. 487–495).
Larson, A. M., & Loschky, L. C. (2009). The contributions
                                                               1414

