              Simpler structure for more informative words: a longitudinal study
                                       Uriel Cohen Priva (uriel cohen priva@brown.edu)
                                   Department of Cognitive, Linguistic, and Psychological Sciences
                                                    Brown University, RI 02912, USA
                                            Emily Gleason (emily gleason@brown.edu)
                                   Department of Cognitive, Linguistic, and Psychological Sciences
                                                    Brown University, RI 02912, USA
                               Abstract                                events are more informative. Imagine if every exchange of
   As new concepts and discoveries accumulate over time, the           words in English were recorded. In terms of information the-
   amount of information available to speakers increases as well.      ory, some exchanges would be more informative than others.
   One would expect that an utterance today would be more in-          Predictable utterances provide less information than unpre-
   formative than an utterance 100 years ago (basing information
   on surprisal; Shannon, 1948), given the increase in technol-        dictable ones. Repetitive “how are you”s do not contribute
   ogy and scientific discoveries. This prediction, however, is at     much information, but utterances such as Neil Armstrong’s
   odds with recent theories regarding information in human lan-       “small step for man” do. It is reasonable to expect that a ran-
   guage use, which suggest that speakers maintain a somewhat
   constant information rate over time. Using the Google Ngram         dom sample of exchanges collected today will contain more
   corpus (Michel et al., 2011), we show for multiple languages        information than an equal-sized sample collected a hundred
   that changes in lexical information (a unigram model) are actu-     years ago: the modern sample may contain unfollow, Higg’s
   ally negatively correlated with changes in structural informa-
   tion (a trigram model), supporting recent proposals on infor-       boson and politically correct – lexical items, scientific dis-
   mation theoretic constraints.                                       coveries and social concepts that were first used or discovered
                                                                       within the past 100 years.1
   keywords: information rate, information theory, Google                 If a million word sample collected today contains more
                                                                       information than it did a hundred years ago, the expecta-
                          Introduction
                                                                       tion is that information rate (entropy), will be higher as well.
Most of Campbell’s condensed soup cans in Andy Warhol’s                However, the prediction that information rate has been ris-
famous 1962 work show between four and seven words on                  ing is incompatible with recent findings in psycholinguis-
the front of the can. Currently, a typical Campbell’s can has          tics. It has been proposed that speakers manipulate their
more than ten. The “cream of mushroom with roasted garlic”             speech so that they will not exceed or fall below acceptable
soup is “great for cooking”, and the can additionally specifies        information rates, such as by omitting, reducing, or hypo-
its net weight, as shown in Figure 1. Campbell now offers              articulating low-information linguistic material and expand-
more soup varieties, more than 80, compared to only 32 in              ing or hyper-articulating high-information linguistic material
1962.                                                                  (Aylett & Turk, 2004; Jaeger, 2010; Levy & Jaeger, 2007).
                                                                       Expansion and reduction have been demonstrated for individ-
                                                                       ual segments (Cohen Priva, 2015; R. van Son & van Santen,
                                                                       2005; R. J. J. H. van Son & Pols, 2003), syllables (Aylett
                                                                       & Turk, 2004), morphemes (Kuperman, Pluymaekers, Ernes-
                                                                       tus, & Baayen, 2007; Kurumada & Jaeger, 2015; Pluymaek-
                                                                       ers, Ernestus, & Baayen, 2005), and words (Arnon & Co-
                                                                       hen Priva, 2014; Bell, Brenier, Gregory, Girand, & Jurafsky,
                                                                       2009; Jurafsky, Bell, Gregory, & Raymond, 2001; Mahowald,
                                                                       Fedorenko, Piantadosi, & Gibson, 2013; Piantadosi, Tily, &
                                                                       Gibson, 2011; Seyfarth, 2014). Such effects have even been
                                                                       demonstrated at the edge of clauses (Jaeger, 2010; Levy &
                                                                       Jaeger, 2007; Norcliffe & Jaeger, 2014), suggesting that in-
Figure 1: Andy Warhol’s Campbell’s tomato soup (left), and             formation theoretic considerations are also driven by syntac-
modern Campbell’s cream of mushroom with roasted garlic                tic information. Other studies suggest that higher-level syn-
(right)                                                                tactic considerations affect the duration of individual words
                                                                       within that construction (Gahl & Garnsey, 2004; Kuperman
   On every front, the development of human society is ac-             & Bresnan, 2012). This trend is even suggested in the Camp-
companied by information growth. There are more books                  bell’s soup example presented in the first paragraph– there are
to read today, objects to use, and apps to download. In in-            more possible choices of soup, and additional words are re-
formation theory (Shannon, 1948), the information encoded                  1 This is not to say that some words do not fall out of fashion–
in some event is its negative log probability – unpredictable          see Petersen, Tenenbaum, Havlin, and Stanley (2012).
                                                                   1895

quired to describe most choices, spreading out the amount of         cuses on a more restricted subset of available information, the
information per symbol.                                              entropy of the single-word model would drop. In contrast, the
   What factors contribute to the limitations on information         three-word model factors in both available information and
rate? Current research considers at least two. First, speakers       the structural complexity of the language. If these two val-
may be unable to speak faster or provide more information            ues were independent, they should be positively correlated–
due to speaker-internal limitations, such as time constraints        if context were not available, then the best estimate for the
for motor planning or a cap in cognitive ability (Bell et al.,       trigram model (1) would be the unigram model (2), and there-
2009; the within-speaker model in Jaeger, 2010, pp. 50–              fore a rise in unigram entropy would predict a rise in trigram
51). Alternatively, the limit may focus on the communica-            entropy. However, we instead expect that one would come
tion channel – even if speakers are able to produce high in-         at the expense of the other, and that increasing the amount
formation rates, their listeners may be unable to follow what        of available information should lead to a reduction in trigram
is being said at such rates (Jaeger, 2010; Pate & Goldwater,         entropy to keep information rate within acceptable ranges.
2015). Both explanations predict that information rate will
                                                                   (1) Trigram entropy
not exceed certain thresholds, even if more information does
become available.                                                                   E [− log Pr (word|two previous words)]
   How can information rate be held constant given an in-
crease in available information? Several factors affect the        (2) Unigram entropy
amount of information provided by speakers. Consider the
phrase it is raining. The information provided by an utter-                                   E [− log Pr (word)]
ance as a whole is the negative log probability of observing
the utterance, combining the probability of the content and             Another possible hypothesis is that information rate con-
the probability of the structure. One aspect of this is world        straints would have no effect on textual data. After all, read-
knowledge. If it is -10 degrees outside, then it is raining be-      ers (and writers) can theoretically slow down and speed up as
comes a highly unlikely utterance, whereas it is snowing be-         they will, in order to digest (or produce) denser or more infor-
comes far more likely. Studying this type of information is          mative words and structures (although, see Genzel & Char-
beyond the scope of this paper, but it is possible to measure        niak, 2002, who found evidence for entropy rate constancy
lexical and structural information. Lexical information is de-       in text). This expectation has the same prediction as the null
rived from the frequency or probability of individual words.         hypothesis: increase in unigram information rate would lead
The word precipitating is less frequent than the word rain-          to a rise in structural information rate. A negative correla-
ing (despite denoting a larger set of events). The phrase it is      tion between unigram and trigram entropy would suggest that
precipitating is therefore lexically more informative than it is     writers still tend not to exceed some level of information rate.
raining. Structural knowledge would tell us that it is raining
is more common than raining it is, and so the untopicalized                            Methods and materials
form is more probable and less informative than the second,          The Google Ngram corpus
topicalized form. Therefore, one of the ways language use            Historical spoken data was not systematically collected, but
can change to accommodate the rising amounts of informa-             written data is available. The Google Ngram corpus (Lin
tion is by reducing structural complexity. Increase in avail-        et al., 2012; Michel et al., 2011) provides yearly frequency
able information tends to increase the information provided          counts for sequences of words, and has previously been used
in any utterance, but using more probable (less informative)         to study related phenomena, such as the lifecycle of words
structures would balance this increase. Does language com-           (Petersen et al., 2012). The corpus contains several subsets
pensate for the rising availability of information by reducing       that limit the type of word sequences to words that were pub-
structural complexity?                                               lished in a specific language, or a specific country. For ex-
   We test this hypothesis using a longitudinal study of lan-        ample, the American English subset includes only word se-
guage by contrasting the entropy of a three-word language            quence counts of English books that were published in the
model with the entropy of a single-word language model (un-          United States. A typical datum in the Google Ngram corpus
igram model, Jurafsky & Martin, 2000). A three-word (tri-            for the American English subset might contain a three-word
gram) language model determines the probability of the ap-           sequence, such as “take aerial photographs”, followed by two
pearance of a word using the expected negative log probabil-         numbers, e.g. “1992 23”. This would mean that the sequence
ity of observing a word given the two preceding words (1).           take aerial photographs appeared 23 times in all the books
This method is similar to the one used in Genzel and Char-           scanned by Google that were published in 1992 in English in
niak (2002). A unigram model determines the probability of           the United States.
a word using no context (2). Both models take yearly en-                We focus on data from the 20th century, for which data is
tropy as the weighted average surprisal of all words in the          available for the greatest number of languages. We exclude
corpus for that year. If more information is available, the          data from 2000 and onwards, as suggested by the authors of
diversity of the lexicon will be higher, but if the language fo-     corpus (supplementary material of Michel et al., 2011). We
                                                                 1896

excluded languages that had too little data in the 20th cen-               Statistical method
tury (Simplified Chinese, Hebrew),2 and languages for which
                                                                           For each language we measured the relationship between tri-
no single country is dominant (Spanish).3 English data was
                                                                           gram entropy and unigram entropy in a linear regression, with
split by the corpus into American English and British En-
                                                                           trigram entropy as the predicted variable and unigram entropy
glish, and English was therefore included. The exact same
                                                                           as the main predictor. The log number of unique trigrams per
methodology, as detailed below, was replicated for each of
                                                                           year, the log of the total number of trigrams in the corpus, and
the remaining languages: American English, British English,
                                                                           the log number of unique unigrams were used as controls, as
French, German, Italian, and Russian. For both trigrams and
                                                                           well as the log number of volumes and log number of pages
unigrams, we excluded words that mixed letters and numbers,
                                                                           that were included in the original Google Ngram corpus. The
as too many of those seemed like data from tables, rather than
                                                                           total number of unigrams was identical to the total number of
language use. 4
                                                                           trigrams, and was not used (as unigrams were taken from the
Calculating trigram entropy                                                trigram dataset, see previous section). The greater the unique
                                                                           number of trigrams relative to the total number of trigrams,
Trigram surprisal was estimated as the maximum likelihood                  the higher the entropy is expected to be, everything else being
estimate (MLE) of observing the third word in a three word                 equal (as the entropy of a uniform distribution over n + 1 out-
sequence given all the possible words that could follow the                comes is higher than a uniform distribution over n outcomes).
previous two words. For example, to calculate the probability              There is no concrete prediction for the total number of tri-
of the word photographs appearing in the context take aerial,              grams as a predictor, but it is expected to capture some of the
the frequency of take aerial photographs is divided with the               variance that is associated with having more books (or top-
frequency of take aerial followed by any word. The negative                ics of discussion). The number of unique unigrams expresses
log of the probability provides the number of bits the word                an alternative (though less accurate) estimate for the richness
photographs provided in that context. The average number of                of the lexicon than unigram entropy. All the predictors and
bits per word is the entropy of the corpus given the model.                predicted values are time series, and are not considered to be
   MLE estimates were used rather than models incorporat-                  independent from their previous values (e.g. the correlation
ing smoothing or backoff (Jurafsky & Martin, 2000, ch. 4),                 between trigram entropy in year n and year n-1 is 0.99 for
as such methods explicitly integrate information from lower-               American English). Therefore, the regression used the differ-
order n-grams to the probability calculations of trigrams.                 ences between each pair of consecutive years for all variables.
Thus, they already factor out cases in which a word’s fre-                 All counts-based controls were logged, as the logged values
quency is biased by the context in which it appears (e.g. Fran-            correlated better with trigram entropy (e.g. pearson r 0.88 for
cisco is frequent, but almost always preceded by San). The                 the correlation between log unique number of trigrams and
proposed account predicts that new words are likely to be                  trigram entropy, but only 0.81 for its unlogged counterpart in
structurally accommodated by facilitating (restrictive) con-               American English), and therefore consistute more appropri-
texts. Switching to smoothed models could mask this effect.                ate controls.
Calculating unigram entropy
                                                                                             Results and discussion
For unigram entropy, the first words of each trigram in the tri-
                                                                           For all languages, changes to unigram entropy were strongly
gram model were counted. The first word was chosen since
                                                                           negatively correlated with changes to trigram entropy (Amer-
trigrams in the 2012 version of the Google Ngram data do not
                                                                           ican English: β=-0.44, SE=0.05, t=-8.753, p<10-12 ; British
span sentence boundaries (this is the version used here; Lin
                                                                           English: β=-0.51, SE=0.034, t=-15.05, p<10-15 ; French:
et al., 2012), and we did not want to bias the sample towards
                                                                           β=-0.55, SE=0.055, t=-9.925, p<10-15 ; German: β=-0.46,
sentence-final words, which are likely to be less informative
                                                                           SE=0.07, t=-6.532, p<10-8 ; Italian: β=-0.76, SE=0.058, t=-
if our hypothesis is correct. The surprisal of observing a par-
                                                                           13.148, p<10-15 ; Russian: β=-0.55, SE=0.042, t=-12.945,
ticular word in any context was taken to be the negative log
                                                                           p<10-15 ), suggesting that structural (or transitional) complex-
of the number of times the word was observed, divided by
                                                                           ity is reduced when lexical complexity rises, even when the
the number of times each word was observed (MLE of word
                                                                           size of the corpus is controlled for. Figure 2 plots for Amer-
probability).
                                                                           ican English the partial correlation between changes in un-
    2 Hebrew and Chinese had comparable results to the other lan-          igram entropy and the residual changes in trigram entropy
guages when using only data for the last 30 years of the 20th cen-         after other predictors were controlled for. Figure 3 shows
tury.                                                                      the equivalent relationship for German, the language with the
    3 We did run the study for Spanish, with comparable results to the
other languages.
                                                                           least significant relationship between unigram entropy and
    4 Median total number of trigrams per year after exclusions            trigram entropy.
(in millions): American English: 623.11; British English: 221.1;              For all languages used, the number of unique trigrams
French: 202.8; German: 151.84; Italian: 54.97; Russian: 82.83.             was positively correlated with trigram entropy (American En-
Median number of unique trigrams per year (in millions): Ameri-
can English: 80.22; British English: 35.75; French: 30.11; German:         glish: β=0.6, SE=0.049, t=12.26, p<10-15 ; British English:
26.97; Italian: 14.2; Russian: 19.29.                                      β=0.71, SE=0.035, t=20.624, p<10-15 ; French: β=0.76,
                                                                       1897

                                                                                                                                  SE=0.065, t=11.777, p<10-15 ; German: β=0.86, SE=0.11,
                                                                                                                                  t=7.852, p<10-11 ; Italian: β=0.84, SE=0.078, t=10.794,
                                                                                                                                  p<10-15 ; Russian: β=1.161, SE=0.093, t=12.537, p<10-15 ),
                                                                                                                                  as expected. The total number of trigrams was signifi-
                                                       Changes in unigram and trigram entropy for American English                cantly negatively correlated with trigram entropy in every lan-
                                               0.04                                                                               guage except British English (American English: β=-0.24,
Residual structural (trigram) entropy change
                                                                                                                                  SE=0.058, t=-4.142, p<10-4 ; French: β=-0.23, SE=0.051,
                                                                                                                                  t=-4.531, p<10-4 ; German: β=-0.22, SE=0.054, t=-3.983,
                                               0.02                                                                               p<0.001; Italian: β=-0.47, SE=0.05, t=-9.286, p<10-14 ; Rus-
                                                                                                                                  sian: β=-0.36, SE=0.057, t=-6.326, p<10-8 ). In all languages
                                                                                                                                  except Italian and Russian, the number of unique unigrams
                                               0.00
                                                                                                                                  had no effect on trigram entropy (positive correlation for Ital-
                                                                                                                                  ian: β=0.28, SE=0.12, t=2.376, p<0.05; negative correla-
                                                                                                                                  tion for Russian: β=-0.31, SE=0.11, t=-2.687, p<0.01). The
                                                                                                                                  number of books was positively correlated with trigram en-
                                               -0.02
                                                                                                                                  tropy for Russian only (β=0.066, SE=0.03, t=2.19, p<0.05),
                                                                                                                                  and the number of pages was positively correlated with tri-
                                                                     -0.02                   0.00               0.02
                                                                             Lexical (unigram) entropy change                     gram entropy for only Italian (β=0.23, SE=0.065, t=3.454,
                                                                                                                                  p<0.001). No other languages were affected by book or page
Figure 2: A plot showing the relationship in American En-                                                                         count.
glish between changes in unigram entropy on the x-axis, and                                                                          In all languages, unigram and trigram entropy change over
residual changes in trigram entropy on the y-axis, after other                                                                    time. There are clear drops in trigram entropy, e.g. all West-
predictors are controlled for.                                                                                                    ern world countries have a drop in trigram entropies in the
                                                                                                                                  1970s, despite an increase in the size of the corpus. This in
                                                                                                                                  itself, prior to controlled analysis, is an interesting finding. It
                                                                                                                                  would suggest that the acceptable range for information rate
                                                                                                                                  may sometimes drop. Figure 4 plots the change in residual
                                                                                                                                  unigram entropy over time, controlling for the total number
                                                                                                                                  of unigrams and the number of unique unigrams in the cor-
                                                                                                                                  pus. Figure 5 plots the change in residual trigram entropy
                                                                                                                                  over time, controlling for the total number of trigrams and
                                                                                                                                  the number of unique trigrams in the corpus.
                                                           Changes in unigram and trigram entropy for German
                                                                                                                                                                                   Unigram entropy over time in American English
Residual structural (trigram) entropy change
                                               0.04
                                                                                                                                                                    0.10
                                                                                                                                  Residual unigram entropy (bits)
                                                                                                                                                                    0.05
                                               0.02
                                                                                                                                                                    0.00
                                               0.00
                                                                                                                                                                    -0.05
                                                                                                                                                                    -0.10
                                               -0.02
                                                                -0.050              -0.025          0.000              0.025
                                                                             Lexical (unigram) entropy change                                                               1900          1925          1950         1975          2000
                                                                                                                                                                                                    Time (years)
Figure 3: A plot showing the relationship in German be-                                                                           Figure 4: Residual values of unigram entropy in American
tween changes in unigram entropy on the x-axis, and residual                                                                      English during the 20th century. The x-axis is years. The y-
changes in trigram entropy on the y-axis, after other predic-                                                                     axis is the residual unigram (lexical) entropy after controlling
tors are controlled for.                                                                                                          for parameters signifying the size of the corpus: log number
                                                                                                                                  of unique unigrams, log number of unigrams.
                                                                                                                                     Because of the individual language differences, we addi-
                                                                                                                                  tionally combined all languages in a mixed-effects regres-
                                                                                                                               1898

                                                 Trigram entropy over time in American English             information also predict a decrease in complex grammatical
                                  0.06
                                                                                                           structures, such as complex clauses? Our hypothesis would
                                                                                                           suggest so. The Google Ngram corpus does contain basic
Residual trigram entropy (bits)
                                  0.03                                                                     syntactic information, in the form of rough part-of-speech
                                                                                                           tags (e.g. noun vs. verb, but not preterite vs. participle; Lin
                                                                                                           et al., 2012), and those too can perhaps be used to infer com-
                                  0.00                                                                     plexity in future studies. In a separate study, Cohen Priva
                                                                                                           (under revision) shows that speech rate (another form of in-
                                                                                                           formation rate) is negatively correlated with both lexical and
                                  -0.03
                                                                                                           syntactic information rates.
                                                                                                              Both unigram and trigram information rates rose during
                                                                                                           certain years and fell in others. There were large scale dips in
                                  -0.06
                                          1900          1925          1950          1975         2000      both unigram and trigram entropy in all languages at differ-
                                                                  Time (years)
                                                                                                           ent times, suggesting the existence of additional factors that
                                                                                                           play a part in determining the acceptable information rate for
Figure 5: Residual values of trigram entropy in American En-                                               a language at any particular time. The timing of the drops
glish during the 20th century. The x-axis is years. The y-axis                                             in information rate is potentially quite telling– in most of the
is the residual trigram (structural) entropy after controlling                                             languages analyzed here, there are large dips in trigram en-
for parameters signifying the size of the corpus: log number                                               tropy around 1915-1920 and 1935-1945, perhaps correspond-
of unique trigrams, log number of trigrams.                                                                ing to the two world wars. If information rate corresponds to
                                                                                                           large-scale societal attitudes, then this would give us a new
                                                                                                           tool to study societal change. Fluctuations in information rate
sion, post-hoc, using random effects for language, with tri-
                                                                                                           may be used to track society-level mood, or even predict fu-
gram entropy as a random slope. The controls for this re-
                                                                                                           ture events. For instance, does information rate rise before
gression were identical for those used in the individual lin-
                                                                                                           democratic revolutions or following them? Being allowed to
ear regressions, and the results were nearly identical. Crit-
                                                                                                           express one’s opinion will mean that more opinions will be
ically, changes to unigram entropy were still strongly nega-
                                                                                                           expressed, but perhaps the expression of new opinions is what
tively correlated with changes to trigram entropy (β=-0.55,
                                                                                                           brings about democratic revolutions in the first place.
SE=0.0161, t=-34.359, p<10-15 ). The number of unique tri-
grams and number of volumes were positively correlated with
trigram entropy (β=0.96, SE=0.021, t=45.410, p<10-15 and
                                                                                                                              Acknowledgments
β=0.048, SE=0.0107, t=4.496, p<10-5 , respectively). The to-                                               This work greatly benefited from discussions with Fiery
tal number of trigrams and the number of unique unigrams                                                   Cushman, Dan Jurafsky, Elinor Amit, Scott AnderBois,
were negatively correlated with trigram entropy (β=-0.29,                                                  Laura Kertz, and David Badre.
SE=0.0191, t=-14.916, p<10-15 and β=-0.088, SE=0.0238,
t=-3.705, p<0.001, respectively). Number of pages was not                                                                          References
significant.                                                                                               Arnon, I., & Cohen Priva, U. (2014). Time and again:
                                                                                                                 The changing effect of word and multiword fre-
                                                               Summary                                           quency on phonetic duration for highly frequent se-
Changes to unigram entropy and trigram entropy were neg-                                                         quences. The Mental Lexicon, 9(3), 377–400. doi:
atively correlated, the opposite of what the null hypothesis                                                     10.1075/ml.9.3.01arn
expects: When amounts of lexical information rise, structural                                              Aylett, M., & Turk, A. (2004). The smooth signal redun-
information drops. This constitutes strong evidence for ac-                                                      dancy hypothesis: a functional explanation for relation-
counts that expect language to restrict the amount of infor-                                                     ships between redundancy, prosodic prominence, and
mation provided at a given time (Aylett & Turk, 2004; Jaeger,                                                    duration in spontaneous speech. Language and Speech,
2010; Levy & Jaeger, 2007). It is quite surprising that the                                                      47(1), 31–56.
transitional or structural properties of language should change                                            Bell, A., Brenier, J., Gregory, M., Girand, C., & Jurafsky, D.
in response to the increasing amount of information, as pre-                                                     (2009). Predictability effects on durations of content
dicted by information theoretic accounts, and yet this is the                                                    and function words in conversational English. Journal
case for all the languages studied here.                                                                         of Memory and Language, 60(1), 92–111.
   These findings open the door to studies of other trade-offs                                             Cohen Priva, U. (2015). Informativity affects consonant du-
in long term information rate. We use transitional probabili-                                                    ration and deletion rates. Laboratory Phonology, 6(2),
ties here as an estimate of structural complexity (more com-                                                     243–278.
plex transitions would indicate more complex structure), but                                               Cohen Priva, U. (under revision). Not so fast: Fast speech
it would also be interesting to use parsed corpora to look at the                                                correlates with lower lexical and structural informa-
frequency of different grammatical constructs. Does a rise in                                                    tion. Brown University manuscript.
                                                                                                        1899

Gahl, S., & Garnsey, S. M. (2004). Knowledge of gram-               Pate, J. K., & Goldwater, S. (2015). Talkers account for
      mar, knowledge of usage: syntactic probabilities affect             listener and channel characteristics to communicate ef-
      pronunciation variation. Language, 80(4), 748–775.                  ficiently. Journal of Memory and Language, 78, 1–17.
Genzel, D., & Charniak, E. (2002). Entropy rate constancy                 doi: http://dx.doi.org/10.1016/j.jml.2014.10.003
      in text. In Proceedings of the Association for Compu-         Petersen, A. M., Tenenbaum, J., Havlin, S., & Stanley, H. E.
      tational Linguistics (pp. 199–206).                                 (2012). Statistical laws governing fluctuations in word
Jaeger, T. F. (2010). Redundancy and reduction: Speakers                  use from word birth to word death. Scientific Reports,
      manage syntactic information density. Cognitive Psy-                2.
      chology, 61(1), 23–62.                                        Piantadosi, S. T., Tily, H. J., & Gibson, E. (2011). Word
Jurafsky, D., Bell, A., Gregory, M. L., & Raymond, W. D.                  lengths are optimized for efficient communication.
      (2001). Probabilistic relations between words: Evi-                 Proceedings of the National Academy of Sciences.
      dence from reduction in lexical production. In J. L. By-      Pluymaekers, M., Ernestus, M., & Baayen, R. H. (2005).
      bee & P. Hopper (Eds.), Frequency and the emergence                 Articulatory planning is continuous and sensitive to in-
      of linguistic structure (pp. 229–254). Amsterdam:                   formational redundancy. Phonetica, 62, 146–159.
      Benjamins.                                                    Seyfarth, S. (2014). Word informativity influences acoustic
Jurafsky, D., & Martin, J. (2000). Speech and language pro-               duration: Effects of contextual predictability on lexi-
      cessing: an introduction to Natural Language Process-               cal representation. Cognition, 133(1), 140–155. doi:
      ing, computational linguistics, and speech recognition.             10.1016/j.cognition.2014.06.013
      New York: Prentice Hall.                                      Shannon, C. E. (1948). A mathematical theory of commu-
Kuperman, V., & Bresnan, J. (2012). The effects of con-                   nication. The Bell System Technical Journal, 27, 379–
      struction probability on word durations during spon-                423.
      taneous incremental sentence production. Journal              van Son, R., & van Santen, J. (2005). Duration and spec-
      of Memory and Language, 66(4), 588–611. doi:                        tral balance of intervocalic consonants: a case for ef-
      10.1016/j.jml.2012.04.003                                           ficient communication. Speech Communication, 47,
Kuperman, V., Pluymaekers, M., Ernestus, M., & Baayen, H.                 100–123.
      (2007). Morphological predictability and acoustic du-         van Son, R. J. J. H., & Pols, L. C. W. (2003). How efficient
      ration of interfixes in Dutch compounds. The Journal of             is speech? Proceedings of the Institute of Phonetic
      the Acoustical Society of America, 121(4), 2261–2271.               Sciences, 25, 171–184.
      doi: 10.1121/1.2537393
Kurumada, C., & Jaeger, T. F. (2015). Communicative effi-
      ciency in language production: Optional case-marking
      in Japanese. Journal of Memory and Language, 83,
      152–178. doi: 10.1016/j.jml.2015.03.003
Levy, R., & Jaeger, T. F.             (2007).  Speakers opti-
      mize information density through syntactic reduction.
      In B. Scholkopf, J. Platt, & T. Hofmann (Eds.),
      Advances in neural information processing systems
      (NIPS) (Vol. 19, pp. 849–856). Cambridge, MA: MIT
      Press.
Lin, Y., Michel, J.-B., Aiden, E. L., Orwant, J., Brockman,
      W., & Petrov, S. (2012). Syntactic annotations for the
      google books ngram corpus. In Proceedings of the acl
      2012 system demonstrations (pp. 169–174).
Mahowald, K., Fedorenko, E., Piantadosi, S. T., & Gibson,
      E. (2013). Info/information theory: Speakers choose
      shorter words in predictive contexts. Cognition, 126(2),
      313–318. doi: 10.1016/j.cognition.2012.09.010
Michel, J.-B., Shen, Y. K., Presser Aiden, A., Veres, A., Gray,
      M. K., Pickett, J. P., . . . Lieberman Aiden, E. (2011).
      Quantitative analysis of culture using millions of digi-
      tized books. Science, 331(6014), 176–182.
Norcliffe, E., & Jaeger, T. F. (2014). Predicting head-
      marking variability in yucatec maya relative clause pro-
      duction. Language and Cognition, FirstView, 1–39.
      doi: 10.1017/langcog.2014.39
                                                                1900

