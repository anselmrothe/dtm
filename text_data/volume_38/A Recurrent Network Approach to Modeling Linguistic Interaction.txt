              A Recurrent Network Approach to Modeling Linguistic Interaction
                                                     Rick Dale (rdale@ucmerced.edu)
                                 Cognitive and Information Sciences, University of California, Merced
                      Riccardo Fusaroli (fusaroli@dac.au.dk), Kristian Tylén (kristian@dac.au.dk)
                                  Interacting Minds Centre & Center for Semiotics, Aarhus University
                                  Joanna Ra̧czaszek-Leonardi (joanna.leonardi@gmail.com)
                                 Institute of Psychology, Polish Academy of Sciences, Warsaw, Poland
                                       Morten H. Christiansen (christiansen@cornell.edu)
                                                Department of Psychology, Cornell University
                                                 Interacting Minds Centre, Aarhus University
                               Abstract                                    happens in the “here-and-now” (Christiansen & Chater, in
                                                                           press) and thus must satisfy a plurality of constraints at the
   What capacities enable linguistic interaction? While several
   proposals have been advanced, little progress has been made in          same time: from the perceptuomotor level all the way “up” to
   comparing and articulating them within an integrative frame-            social discourse and pragmatics (Abney et al., 2014; Fusaroli
   work. In this paper, we take initial steps towards a connec-            et al., 2014; Louwerse, Dale, Bard, & Jeuniaux, 2012). Ac-
   tionist framework designed to systematically compare differ-
   ent cognitive models of social interactions. The framework              cordingly, there remains a need to systematically compare
   we propose couples two simple-recurrent network systems                 and articulate the contributions of the suggested mechanisms
   (Chang, 2002) to explore the computational underpinnings of             in an integrative model of interactional language performance
   interaction, and apply this modeling framework to predict the
   semantic structure derived from transcripts of an experimen-            (Dale, Fusaroli, Duran, & Richardson, 2013).
   tal joint decision task (Bahrami et al., 2010; Fusaroli et al.,            In this paper, we propose a computational framework that
   2012). In an exploratory application of this framework, we              enables flexible combination and comparison of different
   find (i) that the coupled network approach is capable of learn-
   ing from noisy naturalistic input but (ii) that integration of pro-     cognitive constraints. We show that coupled simple-recurrent
   duction and comprehension does not increase the network per-            networks are capable of learning sequential structure from
   formance. We end by discussing the value of looking to tra-             latent-semantic analysis (LSA) representation of wordforms
   ditional parallel distributed processing as flexible models for
   exploring computational mechanisms of conversation.                     in interactive transcripts. As a first case study, we use a tradi-
   Keywords: language; interaction; neural networks; produc-               tional neural-network approach to test the role of production-
   tion; comprehension                                                     comprehension integration during natural language perfor-
                                                                           mance (MacDonald, 2013; Pickering & Garrod, 2014).
                           Introduction
What capacities enable linguistic interaction? There are a
                                                                              Production, Comprehension, and Prediction
large number of extant theoretical proposals. A glance at                  We look to production-comprehension integration to illustrate
the literature reveals a host of proposed mechanisms that sup-             this framework. The relationship between production and
port conversation and other sorts of interactive tasks. Some               comprehension is a key factor in most theories of language
of these are specific to social or linguistic cognition, such              processing. In research on conversational or task-based in-
as mirroring and simulation (Oberman & Ramachandran,                       teraction, these two systems are granted considerable and of-
2007), mind or intention reading (Tomasello, Carpenter, Call,              ten distinct attention. Does language production vary more
Behne, & Moll, 2005), linguistic alignment (Garrod & Pick-                 as a function of internal constraints of the speaker, or more
ering, 2004), and use of common ground (Clark, 1996). Oth-                 in response to the needs of his or her listener (for some re-
ers have drawn on domain-general cognitive processes, in-                  view, among many, see Brennan & Hanna, 2009; Ferreira &
cluding memory resonance of social identity (Horton & Ger-                 Bock, 2006; Jaeger, 2013)? A prominent recent theory takes
rig, 2005), perceptuomotor entrainment (Shockley, Richard-                 these systems to be deeply intertwined. Pickering and Gar-
son, & Dale, 2009), synergies (Fusaroli, Ra̧czaszek-Leonardi,              rod (2014; see also MacDonald, 2013) have argued that an
& Tylén, 2014), one-bit information integration (Brennan,                 integration of production and comprehension is critical in un-
Galati, & Kuhlen, 2010), coupled oscillatory systems (Wilson               derstanding the mechanistic underpinnings of interaction.
& Wilson, 2005), executive control (Brown-Schmidt, 2009),                     Experimental and neuroimaging work suggests simultane-
brain-to-brain coupling (Hasson, Ghazanfar, Galantucci, Gar-               ous involvement of both aspects of language processing dur-
rod, & Keysers, 2012), and situated processes (Bjørndahl,                  ing linguistic interactions (e.g, Silbert, Honey, Simony, Poep-
Fusaroli, Østergaard, & Tylén, 2014).                                     pel, & Hasson, 2014). However, explicit cognitive modeling
   Many of these proposals are individually supported by rig-              can more directly reveal the extent to which, for example,
orous experimentation or corpus analysis. However, language                prediction and understanding are improved as a function of
                                                                       901

                  1                                                                                                                                      2
                  Comprehension                                                       Production
                                                                                                                                                                           Comprehension                                                     Production
                     LSA output (t+1)                                                     LSA output (t+1)
                                                                                                                                                                                          local lexical output (t+1)                                        local lexical output (t+1)
                       hidden layer            context layer                                hidden layer                context layer                     context layer                           hidden layer                                                      hidden layer                      context layer
                                                                                                                                                                                            local lexical input (t)                                            local lexical input (t)
                       LSA input (t)                                                        LSA input (t)
                  3                                                                                                                                      4                                                           Interlocutor A
                  Interlocutor A                                                      Interlocutor B
                                                                                                                                                                        local lexical output (t+1)                                       local lexical output (t+1)
                    Comprehension     Production                                        Comprehension      Production                                                          hidden layer                              context layer           hidden layer                          context layer
                                        local lexical output (t+1)                                           local lexical output (t+1)
                                                                                                                                                                          local lexical input (t)                                           local lexical input (t)
                                                hidden layer            context layer                               hidden layer           context layer
                                                                                                                                                                          local lexical output (t+1)                                       local lexical output (t+1)
                                           local lexical input (t)                                             local lexical input (t)
                                                                                                                                                                                 hidden layer                              context layer          hidden layer                          context layer
                                                               Interlocutor A speaking to interlocutor B
                                                                                                                                                                            local lexical input (t)                                          local lexical input (t)
                                                              Interlocutor B speaking to interlocutor A                                                                                                              Interlocutor B
Figure 1: (1) A representation of two coupled simple-recurrent networks (SRN) inspired by Chang (2002). A conversant is
modeled as a two-SRN agent. A pair of coupled subnetworks is referred to as an agent network. (2) In the original Chang
(2002) model, production did not influence comprehension. We model the complete integration of production-comprehension
by having these two subnetworks share internal states. (3) A conversation can be modeled as a coupling between two such “nets
of nets,” leading to a second-order recurrent network. Each agent receives input from the other, and shares the hidden states of
its comprehension subnetwork with the input layer of its production when it is its turn. We refer to this second-order network as
a dyad network. (4) This framework can be parameterized to investigate, for example, the effect of explicitly externally shared
information between interlocutors, akin to emerging common ground (black box with dotted lines), or the extent to which one
network is facilitated by having access to the “internal state” of another network (thick solid line).
tighter integration. In what follows, we describe one way to                                                                         in Fig. 1, panel 3. On a turn-by-turn basis, we can switch
model human interaction using parallel distributed process-                                                                          who is doing the producing and comprehending. The net-
ing (PDP). Inspired by a predictive approach to language, we                                                                         works are trained to predict word sequences in this way, in
adapt the models of Elman (1990) and Chang (2002) to cou-                                                                            the context of a coupled “conversation.” As shown in Fig. 1,
ple neural networks into two interacting systems, and show                                                                           panel 3, there are two levels of coupling in this model. These
that such a model can be parameterized in various ways to                                                                            first-order networks (agent network) are coupled in their com-
test computational claims.                                                                                                           prehension and production subnetworks in some way. Inter-
                                                                                                                                     action is modeled as a coupling between two such networks,
         Higher-Order Recurrent Dynamics                                                                                             as a second-order recurrent network (dyad network).
We draw inspiration from the successful PDP model of El-                                                                                This model can be readily adapted to parameterize con-
man (1990) and adapted by Chang (2002) to investigate sen-                                                                           straints on processing. In Fig. 1, panel 2, we show that we can
tence processing in a single cognitive agent. The architec-                                                                          “complete the circuit” in the dyads by connecting production
ture of this simple-recurrent network (SRN) is shown in Fig.                                                                         to comprehension in the same way. This simple modification
1, panel 1. This network receives input in a comprehension                                                                           inspired two conditions in a preliminary simulation. First, we
subnetwork. In Chang (2002), this was modeled as a set of                                                                            studied the ability of dyad nets to predict words in interac-
input sentence primes. The hidden state of the comprehen-                                                                            tion under the original formulation, with only comprehension
sion network (activation of nodes at the hidden layer) then                                                                          constraining production. We then tested the contribution of
constrains the production subnetwork, and influences its sub-                                                                        full comprehension-production integration by completing the
sequent performance. Such a network has been shown to ef-                                                                            circuit, and compared its performance to the original formu-
fectively model syntactic priming effects (Chang, 2002).1                                                                            lation.
   Each person in an interaction can be represented as a pair                                                                           Like any cognitive model, this framework requires an in-
of SRNs – receiving input and generating output with pro-                                                                            put space that provides structure to the task. Elman (1990)
duction and comprehension subnetworks. Modeling conver-                                                                              used simulated sentences generated by a simple grammar, and
sation then involves coupling these neural network architec-                                                                         Chang (2002) used hand-coded semantic and syntactic rep-
tures into a “dyad.” We couple these nets by taking the output                                                                       resentations in a simplified grammar. To get input vectors
of “speaker” and use it as the input of the “listener,” as shown                                                                     for our model, we used transcripts from an interactive task in
    1 Note in Fig. 1 that Chang’s original model only included the                                                                   which two participants communicate to jointly solve a per-
constraint on production from prior comprehension.                                                                                   ceptual task (Fusaroli et al., 2012). Taking the word-by-word
                                                                                                           902

sequences in these transcripts, we created input activations               properties. In order to propagate error back, we differenti-
based on latent semantic analysis representations. This re-                ate the tanh function at the output nodes. Because derivative
duces the dimensionality and sparsity of the input space and               d tanh = 1 − tanh2 , we obtain
makes the learning problem more tractable for the network.
It also tests the framework with complex naturalistic data.                                  δo = o ◦ do ◦ e = o(1 − o ◦ o) ◦ e              (1)
          Input Corpus: LSA Word Vectors                                   Where o is the output vector of the network, e is the error as-
                                                                           sociated with each node, and ◦ represents elementwise mul-
The corpus consists of 16 dyads (32 Danish-speaking individ-               tiplication. δo reflects the error assignment to output nodes.
uals) totaling more than 1,600 joint decisions, 25,000 word                Once this is calculated, we can modify the weights Wh→o with
tokens and 1,075 unique word types.2 Given the sparsity of
the lexical space, we transformed the corpus into a latent se-                                       ∆Wh→o = αhT δo                          (2)
mantic analysis (LSA) representation (Landauer & Dumais,
1997). This projects words into a lower dimensional feature                α represents the learning rate parameter, and h the hidden unit
(vector) space based on how the words occur in the corpus.                 activations of a given subnetwork. We used this approach to
We define a word’s relative cooccurrence to another word by                modify the weights connecting hidden and output layers. All
using a simple 1-step window, so that the cooccurrence of                  other layers were treated in the common way with the sigmoid
word wi with word w j is the total number of times they fol-               function and its derivative, in accordance with traditional it-
lowed each other, f (i, j) = NP(wi,t , w j,t+1 ), where N is the           erated backpropagation.
total number of words in sequence, and P the joint prob-                      In order to train the networks using LSA vectors as they
ability that words i and j occurred together at times t and                interact in dyads, we follow the process illustrated in Fig.
t + 1, respectively. This count serves as an entry in a 1,075              2. In a turn-by-turn fashion, the production subnetwork of
× 1,075 matrix M, as the entry at the ith row and jth column.              one agent net would be trained to predict its “spoken” output,
This matrix is, of course, quite sparse, because most words                while the comprehension subnetwork of the other agent net
do not cooccur with every other word. LSA was employed                     would receive this output as input and predict it in a word-by-
as a means to overcome such sparsity, providing a lower-                   word fashion.
dimensional representation of word similarity based on these
distributional patterns: [U, S,V ] = SVD(M).
                                                                                               Simulation Procedure
   The left eigenvector matrix (U) provides a more compact                 Training and Testing To assess how well the models cap-
representation for individual words. Rather than a complete                ture interactional structure of the empirical data, we trained
(but sparse) representation across all 1,075 of its column                 16 dyad networks in each of two conditions (comprehension
entries, the SVD solution that LSA uses allows us to take                  to production only vs. full integration). Each network was
a much smaller number of columns of U instead. These                       trained on one pass on the full transcripts of 15 dyads (almost
columns represent the most prominent sources of variance in                25,000 word presentations) and then tested on the remaining
the distributional patterns of the word usage.                             target dyad. We set α to .01, and the number of hidden units
   When inspecting the singular values (S) of the SVD so-                  across all subnetworks to 10.3 We built a baseline control for
lution in an LSA model, we find that word usage across all                 each test dyad by shuffling its word order, thus disrupting the
transcripts can be captured by about 7 of these columns of U.              sequential structure that the networks were expected to learn.
A schematic of how we use these feature vectors is shown in                The ‘A’ or ‘B’ designation of interlocutors was randomly as-
Fig. 2, which illustrates a pattern of activity across 7 nodes as          signed, but used here for convenience of presentation.
the input for these networks. This gives us a 7-dimensional                   Our performance measure was based on the common mea-
representation of words, where activations can be negative or              sure of cosine between the output and target vectors. Co-
positive, which requires some modification to the training of              sine is commonly used with the LSA model, since it captures
our SRN subnetworks.                                                       whether word vectors are pointing in the same direction in
                                                                           “semantic space.” Cosine varies from [−1, 1], with higher
                      Training with LSA                                    values reflecting better predictions by the network.
Because common backpropagation assumes an activation                       Predictions First, we expected that the “content” shared be-
range of [0, 1], we had to modify the input and output ac-                 tween speaker and listener, projected in LSA space, should
tivation transformations to suit a [−1, 1] range. To do this               allow the networks to learn the statistical structure of in-
we changed the standard sigmoid function, used as output                   teraction. Second, we contrasted three hypotheses about
activation function, to a tanh function that has the desired               production-comprehension integration. (H1) Fully inte-
                                                                           grated production-comprehension systems would benefit per-
    2 Space limitations prevent us from fully describing the construc-     formance, as the networks are able to receive “more in-
tion of this semantic representation, but we note that we also in-
cluded a “turn end” marker to ensure that words adjacent across                3 Space restricts our parameter search, but we found, in general,
turns were not treated as if they were spoken in the same sequence         that hidden layer size did not greatly impact performance in any con-
of words by one person.                                                    ditions in our explorations.
                                                                       903

                                          A            B                     A                               B
                                       "ja lidt"   "aah aah"   "aah aah hvad goer vi"                   "vi maa spille ...
                           . . .                                                                                             . . .
                                   C          P    C       P         C               P                   C           P
                  Participant A
                                   C          P    C       P         C               P                   C           P
                  Participant B
Figure 2: We organized network training by interactive turn. For a given turn, one participant (A or B) is doing the talking.
We take the LSA vectors (visualized as a distributed pattern of activation) and have the production network of the speaker on
that turn predict its output, and the comprehension network of the other participant predict its input. Within each dyad, the
subnetworks of each participant take turns learning to predict the LSA vectors.
formation,” in that the comprehension net is now receiving
input from production. (H2) Fully integrated production-
comprehension systems degrade performance as they intro-                                 0.6
duce noise to the network and an additional set of weights that
                                                                                                                                               ●
the network has to learn. (H3) There will be no difference be-
tween these networks: Our simplified task has the production                             0.5    ●
                                                                                                                 ●
                                                                                                                 ●
                                                                                                                                      ●
                                                                                                                                               ●
                                                                                                ●                ●
                                                                                                                 ●                    ●        ●
                                                                                                                                               ●
                                                                                                ●                ●                    ●        ●
                                                                                                                                               ●
                                                                                                ●
and comprehension networks doing very similar things, and                                      ●●                ●                    ●       ●●
                                                                                                ●
                                                                                                ●
                                                                                                ●             ●  ●
                                                                                                                 ●                   ●
                                                                                                                                      ●
                                                                                                                                      ●        ●
                                                                          cos(t,o)
                                                                                                ●                ●
so we may not observe any divergence in their performance.                                      ●                ●
                                                                                                                 ●                   ●●
                                                                                                                                               ●
                                                                                                                                               ●
                                                                                         0.4    ●                ●                    ●
                                                                                                                                      ●
                                                                                                                                      ●        ●
                                                                                                                 ●                             ●
                                                                                                                                               ●
                                                                                                ●
                                                                                                ●                                              ●
                                                                                                ●                ●                             ●
                                                                                                                                               ●
                                                                                                ●                ●                    ●        ●
                                                                                                                                               ●
                           Results                                                             ●●
                                                                                                ●
                                                                                                ●
                                                                                                ●             ●
                                                                                                                 ●
                                                                                                                 ●
                                                                                                                 ●
                                                                                                                 ●
                                                                                                                                      ●
                                                                                                                                      ●
                                                                                                                                      ●
                                                                                                                                      ●
                                                                                                                                      ●
                                                                                                                                              ●●
                                                                                                                                               ●
                                                                                                ●                ●
                                                                                                                 ●
                                                                                                                 ●                   ●●
                                                                                                                                      ●
                                                                                                                                               ●
                                                                                         0.3    ●                ●                             ●
                                                                                                ●                                     ●
                                                                                                                                      ●
Can Dyad Nets Learn Sequential Structure? When com-                                                              ●
                                                                                                                                      ●
                                                                                                                                      ●
                                                                                                                                      ●
                                                                                                                                               ●
paring networks in both conditions, it appears that they are
very similarly effective at predicting word-by-word LSA vec-
                                                                                         0.2
tors in unseen interactions, and that they also show much bet-
ter performance than the control baseline, in which words are                                  A prod        A comp                  B prod   B comp
shuffled. This means that networks are processing the or-
der of LSA features, and not simply capturing the activation                                                             Subnetwork
space in which these LSA features reside. This learning ef-               Figure 3: Dyad networks are capable of learning interactional
fect is quite large, and is shown in Fig. 3. The appropriate              structure. The cosine for agent subnetworks trained on se-
test here is a paired-sample t-test, since each network and its           quential structure show greatly increased scores relative to
control are trained on matched sets of words with the same                baseline subnetworks, for which temporal order of the LSA
network. A t-test across all four subnetworks shows the ex-               training vectors are shuffled. In general, agent nets with com-
pected result, for both conditions: t’s > 25 and p’s < .000001.           prehension ⇒ production (circle) do not perform differently
Does Integration Improve Prediction? The average co-                      from agent nets with integration (triangle). They do both
sine performance did not differ between the two network                   show better performance than the control (red). The models
conditions, using the same paired-sample t-test across lay-               are both learning to predict LSA vector sequences. cos(t, o)
ers, t(63) = 0.33, p = .74. This absence of an effect is quite            stands for the cosine of target and observed output vectors.
evident in Fig. 3. No reliable difference emerges in direct
comparison of any of the subnetworks, either.
                                                                    904

                                                                                ●
                                                                       ●
                                                                       ●
  integrationwins
              wins>>
                                                          ●
Integration
                                                                                ●
                                                                                ●
                              0.04           ●
                                                                                               nal’ production-comprehension coupling is in fact not facili-
                                             ●
                                             ●
                                             ●                                  ●              tating mutual prediction in this context. This could indicate
                                             ●
                                             ●            ●                     ●
                                                                                ●              that recurrent (and thus predictive) structure resides on lev-
                              0.02
                                                          ●            ●
                                                                       ●
                                                          ●                     ●
                                             ●            ●
                                                          ●
                                                                                               els other than the turn-by-turn organization of the conver-
                                                          ●
                                                          ●                                    sation. In fact, a recent study (relying on the same corpus)
                                                          ●            ●        ●
                                                                                ●
                              0.00
                                                          ●                     ●              suggests that linguistic patterns critical to performance in the
                                                          ●
< integration losesnet wins
  < Basic Chang
                                            ●             ●            ●
                                                                       ●                       task tend to straddle interlocutors and speech turns making
                                             ●                         ●
                                                                       ●
                                             ●                         ●                       turn-by-turn alignment secondary to recurrent structural pat-
                              −0.04 −0.02
                                                          ●
                                                                       ●
                                             ●
                                                          ●
                                                                       ●        ●
                                                                                ●              terns at the level of the conversation as a whole (Fusaroli &
                                             ●
                                                                       ●
                                                                                               Tylén, 2016). A future implementation of the model could
                                             ●
                                             ●            ●
                                                                       ●
                                                                       ●
                                                                                               directly test such ideas (sometimes referred to as the interper-
                                             ●            ●                                    sonal synergy model of dialogue: Fusaroli et al., 2014) and
                                                                                               compare the performance to other types of conversational in-
                                            A prod       A comp       B●prod   B ●comp         teraction that might entail different functional organization.
                                                                                ●
                                                                                ●
                                                                                                  These results might also be contingent upon a number of
                                                          ●   Subnetwork                       methodological limitations that will need to be overcome in
Figure 4: Difference between integrated and unintegrated                                       future developments. First, the sample size is not impres-
agent network conditions relative to their respective base-                                    sive and a bigger corpus would possibly enable better train-
lines. It reflects how much more one network can be ex-                                        ing of the networks. Second, in order to deal with the sparse
pected to exceed its baseline relative to the other condition.                                 lexical space of real conversations, we reduced the input to
If integrating production and comprehension improves per-                                      LSA vectors. As a consequence both the comprehension and
formance, we expect a positive value on the y-axis.                                            production subnetwork end up dealing with the same kind of
                                                                                               data. Integrating comprehension does therefore not add in-
                                                                                               formation that is not already contained in the LSA vectors
Does Integration Improve Prediction above Baseline?                                            processed in production subnetworks. Thus, the integration
These results are shown in Fig. 4. In general, as might be                                     is at least partially redundant and cannot be expected to add
expected from the prior analyses, the models are not differ-                                   much to the performance of the model.
ent from each other in most subnetwork performance. All                                           There are also more general limitations to overcome. For
results are non-significant, with the initial agent net config-                                example, anticipatory dynamics in agent networks should al-
uration not different from its baseline relative to that of the                                low overlap at the turn level, as seen in natural interactions.
fully integrated networks, |t|’s < 1.                                                          This is a critical feature for modeling the higher-order dy-
                                                                                               namics of interaction. The PDP approach embraces such
                                                     General Discussion                        computational extensions. For example, networks could be
In this paper, we described a flexible computational frame-                                    gated, such that off/on states of the production subnetwork
work to investigate the cognitive mechanisms underlying lin-                                   will have to be learned by agents. The recurrent property of
guistic interaction. The first step in this direction is the im-                               these networks should allow them to predict forthcoming turn
plementation of coupled neural networks to learn from inter-                                   switches. The approach offers much in the way of extension,
action data. We demonstrated that this adaptation of Chang                                     as these networks are, after all, nonlinear function approxima-
(2002) is capable of learning the sequential semantic structure                                tors over any arbitrary sets of temporal constraints. For exam-
in raw, noisy input.                                                                           ple, we could also develop other input spaces, such as multi-
   Based on the current debate on interactive alignment,                                       modal constraints from nonverbal aspects of interaction, and
we manipulated their internal cognitive structure to contrast                                  add them to the verbal components we have explored here.
two theoretically motivated models: (i) a model with full                                         This flexibility also permits more focused theoretical ex-
comprehension-production integration, and (ii) a model with-                                   plorations. The constraints on these networks have theoreti-
out integration. These alternative coupled networks were then                                  cal implications that can be readily adapted to further com-
used to model real conversational data in order to investigate                                 pare and integrate proposed mechanisms, the topic that began
hypothesized prediction benefits of full integration. Our re-                                  this paper. For example, Fig. 1, panel 4 showcases how we
sults did not reveal an effect of full integration. Put simply,                                might develop the framework to test combinations of other
hypothesis (H3) seems to have been supported here: In this                                     constraints on interaction, such as “common ground.” An-
computational system, full integration does not bring great                                    other example is how internal constraints from one agent net-
gains, if any. Why did we not observe clearer results? To                                      work might constrain, and possibly facilitate, the dynamics
conclude, we outline theoretical and methodological consid-                                    of the agent to which it is coupled in the dyad network. The-
erations that hint at possible explanations and motivate future                                oretically motivated manipulations of this kind would allow
implementations of the framework.                                                              more explicit tests of the relationship among these various
   First, the results can be interpreted to suggest that ‘inter-                               proposals for the mechanisms of interaction, and compar-
                                                                                         905

isons to related computational frameworks (e.g., Buschmeier,       Fusaroli, R., Ra̧czaszek-Leonardi, J., & Tylén, K. (2014).
Bergmann, & Kopp, 2010; Reitter, Keller, & Moore, 2011).             Dialog as interpersonal synergy. New Ideas in Psychology,
                                                                     32, 147–157.
                    Acknowledgments                                Fusaroli, R., & Tylén, K. (2016). Investigating conversa-
Thanks to the Interacting Minds & Center for Semiotics at            tional dynamics: Interactive alignment, interpersonal syn-
Aarhus University for its support in bringing the co-authors         ergy, and collective task performance. Cognitive Science,
together for a meeting last year in January, 2015 to discuss         40, 145-171.
this work. Thanks also to Andreas Roepstorff for fun and           Garrod, S., & Pickering, M. J. (2004). Why is conversation
productive discussions during our visit. The co-authors vi-          so easy? Trends in Cognitive Sciences, 8(1), 8–11.
brantly discussed the theoretical status of such a PDP frame-      Hasson, U., Ghazanfar, A. A., Galantucci, B., Garrod, S., &
work, and did not come to a consensus about that status. It          Keysers, C. (2012). Brain-to-brain coupling: a mechanism
did not detract from the fun.                                        for creating and sharing a social world. Trends in Cognitive
                                                                     Sciences, 16(2), 114–121.
                         References                                Horton, W. S., & Gerrig, R. J. (2005). The impact of memory
Abney, D. H., Dale, R., Yoshimi, J., Kello, C. T., Tylén, K.,       demands on audience design during language production.
  & Fusaroli, R. (2014). Joint perceptual decision-making: a         Cognition, 96(2), 127–142.
  case study in explanatory pluralism. Frontiers in Psychol-       Jaeger, T. F. (2013). Production preferences cannot be un-
  ogy, 5.                                                            derstood without reference to communication. Frontiers in
Bahrami, B., Olsen, K., Latham, P. E., Roepstorff, A., Rees,         Psychology, 4.
  G., & Frith, C. D. (2010). Optimally interacting minds.          Landauer, T. K., & Dumais, S. T. (1997). A solution to
  Science, 329(5995), 1081–1085.                                     plato’s problem: The latent semantic analysis theory of ac-
Bjørndahl, J. S., Fusaroli, R., Østergaard, S., & Tylén, K.         quisition, induction, and representation of knowledge. Psy-
  (2014). Thinking together with material representations:           chological Review, 104(2), 211.
  joint epistemic actions in creative problem solving. Cogni-      Louwerse, M. M., Dale, R., Bard, E. G., & Jeuniaux, P.
  tive Semiotics, 7(1), 103–123.                                     (2012). Behavior matching in multimodal communication
Brennan, S. E., Galati, A., & Kuhlen, A. K. (2010). Two              is synchronized. Cognitive Science, 36(8), 1404–1426.
  minds, one dialog: Coordinating speaking and understand-         MacDonald, M. C. (2013). How language production shapes
  ing. Psychology of Learning and Motivation, 53, 301–344.           language form and comprehension. Frontiers in Psychol-
Brennan, S. E., & Hanna, J. E. (2009). Partner-specific adap-        ogy, 4.
  tation in dialog. Topics in Cognitive Science, 1(2), 274–        Oberman, L. M., & Ramachandran, V. S. (2007). The sim-
  291.                                                               ulating social mind: the role of the mirror neuron system
Brown-Schmidt, S. (2009). The role of executive function in          and simulation in the social and communicative deficits of
  perspective taking during online language comprehension.           autism spectrum disorders. Psychological Bulletin, 133(2),
  Psychonomic Bulletin & Review, 16(5), 893–900.                     310.
Buschmeier, H., Bergmann, K., & Kopp, S. (2010). Mod-              Pickering, M. J., & Garrod, S. (2014). Neural integration of
  elling and evaluation of lexical and syntactic alignment           language production and comprehension. Proceedings of
  with a priming-based microplanner. In Empirical methods            the National Academy of Sciences, 111(43), 15291–15292.
  in natural language generation (pp. 85–104). Springer.           Reitter, D., Keller, F., & Moore, J. D. (2011). A compu-
Chang, F. (2002). Symbolically speaking: A connectionist             tational cognitive model of syntactic priming. Cognitive
  model of sentence production. Cognitive Science, 26(5),            Science, 35(4), 587–637.
  609–651.                                                         Shockley, K., Richardson, D. C., & Dale, R. (2009). Con-
Christiansen, M. H., & Chater, N. (in press). The now-or-            versation and coordinative structures. Topics in Cognitive
  never bottleneck: A fundamental constraint on language.            Science, 1(2), 305–319.
  Behavioral and Brain Sciences.                                   Silbert, L. J., Honey, C. J., Simony, E., Poeppel, D., & Has-
Clark, H. H. (1996). Using language. Cambridge university            son, U. (2014). Coupled neural systems underlie the pro-
  press.                                                             duction and comprehension of naturalistic narrative speech.
Dale, R., Fusaroli, R., Duran, N., & Richardson, D. C. (2013).       Proceedings of the National Academy of Sciences, 111(43),
  The self-organization of human interaction. Psychology of          E4687–E4696.
  Learning and Motivation, 59, 43–95.                              Tomasello, M., Carpenter, M., Call, J., Behne, T., & Moll,
Ferreira, V. S., & Bock, K. (2006). The functions of struc-          H. (2005). Understanding and sharing intentions: The ori-
  tural priming. Language and Cognitive Processes, 21(7-8),          gins of cultural cognition. Behavioral and Brain Sciences,
  1011–1029.                                                         28(05), 675–691.
Fusaroli, R., Bahrami, B., Olsen, K., Roepstorff, A., Rees,        Wilson, M., & Wilson, T. P. (2005). An oscillator model of
  G., Frith, C., & Tylén, K. (2012). Coming to terms quanti-        the timing of turn-taking. Psychonomic Bulletin & Review,
  fying the benefits of linguistic coordination. Psychological       12(6), 957–968.
  Science, 931–939.
                                                               906

