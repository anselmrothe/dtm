                    Desirable difficulties in the development of active inquiry skills
                                        George Kachergis, Marjorie Rhodes, & Todd Gureckis
                                         {george.kachergis, marjorie.rhodes, todd.gureckis}@nyu.edu
                                                Department of Psychology, New York University
                                                                 New York, NY
                                  Abstract                               to search efficiently for information given a particular set of
    This study explores developmental changes in the ability to          hypotheses but have trouble updating their beliefs correctly
    ask informative questions. We hypothesized an intrinsic link         given new evidence. In this sense active inquiry is like a
    between the ability to update beliefs in light of evidence and       bicycle: when all the elements are properly functioning and
    the ability to ask informative questions. Four- to ten-year-old
    children played an iPad game asking them to identify a hidden        aligned the bike moves forward. However, misalignment of
    bug. Learners could either ask about individual bugs, or make        any one component can be catastrophic.
    a series of feature queries (e.g., “Does the hidden bug have            The present study joins with some recent work which at-
    antenna?”) that could more efficiently narrow the hypothesis
    space. Critically the task display either helped children inte-      tempts to decompose the component processes involved in ac-
    grate evidence with the hypothesis space or required them to         tive inquiry (e.g., Bonawitz & Griffiths, 2010). In particular,
    perform this operation themselves. Although we found that            we tasked four to ten-year olds to identify a hidden bug in a
    helping children update their beliefs improved some aspects of
    their active inquiry behavior, children required to update their     simple iPad variant of the classic “Guess Who?” game. Chil-
    own beliefs asked questions that were more context-sensitive         dren asked questions to try to identify the hidden bug. Across
    and thus informative. The results show how making a task             conditions, we manipulated whether the computer program
    more difficult may actually improve children’s active inquiry
    skills, thus illustrating a type of desirable difficulty.            helped children to use the new evidence that resulted from
    Keywords: question asking, information search, active in-            their queries to narrow down the hypothesis space, or whether
    quiry, hypothesis testing, scientific reasoning                      children had to use the new evidence to update the hypothesis
                                                                         space on their own. Our expectation was that helping chil-
                             Introduction                                dren to update their beliefs accurately following the receipt of
A central aim of science education is to teach students how to           new information would free up cognitive resources and lead
approach the task of understanding their environment. Rather             to more effective question-asking. Interestingly, our results
than teaching only a catalogue of facts about the biological             opposed our main hypothesis in that elements which ostensi-
and physical worlds, current standards emphasize teaching                bly made our task more difficult actually improved the quality
the conceptual and analytic skills that underlie science: de-            of children’s inquiry behavior.
tecting patterns in environments that initially appear chaotic,
                                                                         How the ability to ask revealing questions develops
abstracting the general principles that can be used to under-
stand and predict events, and importantly, learning how to ask           Experimental tasks based on the “Guess Who?” game have
informative questions to reveal these patterns and principles            often been used to study question asking and active inquiry
when they are not immediately obvious (Bransford, Brown, &               with both children and adults. In the game, the asker (partic-
Cocking, 2000; Donovan & Bransford, 2005; Duschl, Schwe-                 ipant) tries to determine a hidden object known only to the
ingruber, & Shouse, 2007).                                               the answerer (experimenter) (e.g., “What animal am I think-
    Many of the cognitive skills required for active scientific          ing of?”) by asking a series of yes-or-no questions. Mosher
inquiry follow protracted developmental trajectories. For ex-            and Hornsby (1966) identified two broad question types com-
ample, in tasks designed to assess scientific reasoning abil-            monly used in the game: hypothesis- scanning questions
ities, children in the older elementary school years (ages 8-            test a single hypothesis (e.g., “Is it a monkey?”), whereas
10) often have difficulty adopting systematic strategies, such           constraint-seeking questions attempt to constrain the hypoth-
as testing the effects of one variable at a time or selecting            esis space faster by querying features that are present or ab-
interventions that will lead to determinate evidence (Chen               sent in multiple objects (e.g., “Is it soft?”), but that do not
& Klahr, 1999). Although children in the older elementary                directly identify the answer except by virtue of elimination.
school years can be taught to engage in these strategies via di-            A classic finding in this literature is that younger children
rect instruction (Klahr & Nigam, 2004; Kuhn & Dean, 2005),               (e.g., aged 6) tend to ask more hypothesis-scanning questions,
it is notable how difficult it is for them to discover and imple-        while older children (e.g., aged 11) use more constraint-
ment them on their own.                                                  seeking questions, and also tend to find the answer after
    One reason for the difficulties children show may be that            fewer questions (Mosher & Hornsby, 1966). One explana-
active inquiry depends on the coordination of a variety of               tion is that only older children have developed the ability
component cognitive processes (belief updating, decision                 to focus on the high-level features that group the hypothe-
making, hypothesis generation, etc.). Inefficiencies in any              ses, whereas younger children focus on individual stimuli.
or all of these interrelated processes may serve as develop-             Consistent with this viewpoint, manipulations that help chil-
mental limitations. For example, young learners may be able              dren focus on these higher-level features (such as cuing them
                                                                     2477

with basic level category labels instead of exemplar names            Stimuli On each round, sixteen bugs with the same body
(Ruggeri & Feufel, 2015) increase the likelihood that young           shape but with varying features were used as stimuli. Bugs
children will generate constraint-seeking questions (see also         were defined by the presence or absence of 9 features: green
Herwig (1982). Further, although young children are often             body, orange eyes, antennae, big spots, tiny spots, legs,
relatively less likely than older children to ask constraint-         leaves, water droplets, and blue “fur”. Figure 1 shows an
seeking questions, even younger children (ages 7-9) are more          example of two of the body shapes used, each with all of
likely to do so when such questions are particularly infor-           the binary features present. One of the sixteen possible bugs
mative, such as when the hypothesis space is large and there          was chosen as the “hidden bug” on each trial which chil-
several equally probable solutions remaining (e.g., Ruggeri &         dren attempted to identify by asking questions. The hidden
Lombrozo, 2015; 2015).                                                bug was randomly selected on each round, and each round
   Whereas previous work has focused on developmental                 had differently-shaped bug bodies, selected from a pool of 16
changes in when children generate informative, hypothesis-            unique body shapes. The bug task was used to fit thematically
scanning questions, less prior work has considered possible           with the content of the AMNH Discovery Room activities.
developmental changes in how children make use of the new
evidence that their questions reveal. As described above, ef-
fective active inquiry involves the coordination of multiple
cognitive processes–representing the hypothesis space, gen-
erating an informative query, updating one’s representation
of the hypothesis space in light of the data produced by the
query, and so on. As suggested by prior work, hypothe-
sis scanning questions might be easier for young children to
generate because they do not require abstracting informative          Figure 1: Examples of two bug types with all 9 of the binary features
higher level features (features to query that group classes of        present. Each round used one of the 16 body shapes.
hypotheses together and might allow them to be eliminated
at once). Yet, another reason why hypothesis scanning ques-
tions might be easier for young children is that they produce         Design Across the sixteen items, some feature were more
evidence that is easier for them to process. As a hypothe-            frequent than others (relevant to eight of the possible bugs),
sis scanning question is answered, children are told directly         while some were very infrequent (relevant to only 1 bug),
whether the item they queried is correct or not. If instead chil-     with an abstract structure shown in Figure 2. This introduced
dren ask about a feature (as in a constraint-seeking question),       strong differences in the informational utility of each feature.
additional cognitive processing is required–children have to          For example, given no other information it would be infor-
take that new information (e.g., that a hidden animal has an-         mative to ask about feature F1 because is it shared with half
tennae) and consider each remaining possible exemplar in              the possible bugs. In contrast, feature F9 is less informative
light of this information (e.g., check if each one has the an-        on the first trial because most of the bugs do no have this
tennae) and eliminate from the hypothesis space any that are          feature. The abstract features in Figure 2 were randomly
ruled out by the new information. This process could be cog-          assigned to the visual features for each participant, and then
nitively taxing, and also prone to errors. Thus, although con-        remained consistent across rounds. This gave participants the
straint seeking questions are often more informative in theory,       opportunity to learn the structure across rounds, for example
they might not always be so to young children, particularly if        to perhaps figure out which visual features are most relevant
children have difficulty using the obtained information to up-        to ask about first.
date their representation of the hypothesis space accurately.            Each of these features was represented on a button, avail-
To address these issues, in the present study we manipulated          able for participants to query. Before participants were al-
whether children received assistance in updating their hypoth-        lowed to begin, the experimenter explained at least three of
esis space or had to undertake this process on their own, fol-        these buttons, randomly selected. An additional feature but-
lowing the receipt of new evidence obtained by their queries.         ton depicted a particular body shape that was not relevant to
                                                                      the bugs on display. Instead of choosing a feature button,
                        Experiment                                    participants could at any time query an exemplar to deter-
                                                                      mine if it was the hidden bug or not. This paradigm thus en-
Methods
                                                                      abled us to investigate both the qualitative strategies used by
Participants Participants in this experiment were 134 chil-           participants (constraint-seeking feature queries or hypothesis-
dren between the ages of 5 and 10 years old who were re-              scanning exemplar queries) and to quantify how efficiently
cruited at the American Museum of Natural History’s Dis-              participants searched the hypothesis space, within and across
covery Room. Of the 134 children recruited, we analyze the            rounds as they learn a novel structured stimulus space. More-
data from 121 children (21 5-year-olds, 20 6-year-olds, 22            over, we introduced a novel manipulation: after making a fea-
7-year-olds, 20 8-year-olds, 20 9-year-olds, and 18 10-year-          ture query, participants in the manual-update condition had
olds) who completed 5 or more rounds of the game.                     to select the hypotheses that were consistent with the feed-
                                                                  2478

 Exemplar F1         F2    F3     F4    F5    F6    F7     F8   F9        be deselected by tapping again. Only when participants were
      A        1      0     0      1     0     0     0      0    0        done selecting bugs did the experimenter press the “Elimi-
      B        1      1     0      1     0     0     0      0    0        nate” button, which eliminated any bugs that were not se-
      C        0      1     0      1     0     0     0      0    1        lected. Although manual-update participants received train-
      D        0      1     0      1     0     0     0      0    0        ing for the manual elimination in the dog house training task,
      E        1      0     0      0     0     0     0      0    0
                                                                          as well as gentle reminders in the first round of the bug game,
      F        1      1     0      0     0     0     0      0    0
      G        0      1     0      0     0     0     1      1    0
                                                                          it should be noted that it was possible for mistakes to be made
      H        0      1     0      0     0     0     1      0    0        during manual updating–unlike in the automatic condition.
      I        1      0     1      0     0     1     0      0    1
      J        1      0     1      0     0     1     0      0    0
      K        0      0     1      0     0     0     0      0    0
      L        0      0     0      0     0     0     0      0    0
      M        1      0     1      0     1     0     0      1    0
      N        1      0     1      0     1     0     0      0    0
      O        0      0     1      0     1     0     0      0    0
      P        0      0     0      0     1     0     0      0    0
Figure 2: The abstract feature structure of the 16 exemplars used in
each round. Each participant had these abstract features randomly
assigned to the visual features, but had a consistent assignment used
round-to-round.
back, whereas participants in the automatic-update condition
had the hypothesis space automatically updated. This ma-
nipulated the ease of updating the hypothesis space: a diffi-
cult step in the cycle of active inquiry that has not been well-
studied.
Procedure After being trained by an experimenter on a                     Figure 3: Task overview: in the upper left, a feature button is used,
simpler version of the task with unrelated stimuli (a dog                 asking if the bug hidden under the rug is green. Given feedback
searching dog houses), participants played 5 or more rounds               (“Yes!”), participants in the manual update condition select the bugs
of an iPad game asking them to identify which one of 16 bugs              that are consistent with this new information (upper right), whereas
                                                                          in the automatic condition the consistent bugs are selected by the
is hidden under a rug (see Figure 3). The task alternated be-             game. Players in both conditions press the red button to return to
tween the query phase and the elimination phase. In the query             the button phase, and again either choose a feature button or query a
phase, players either queried individual bugs, or used feature            single bug.
queries (e.g., “Is the hidden bug green?”) to find out whether
the hidden bug had a particular feature. If a single exemplar             Results
was queried by tapping on it, feedback was immediate: if it               Overall We analyzed the first 10 rounds from each child
happened to be the hidden bug, a smiley face appears and the              (only 8 children played more than 10 rounds, including one
round was done, whereas if the tapped exemplar was not the                who played 51 rounds). This covers 722 rounds from 121
hidden bug, a red “X” appeared on top of the tapped bug and               children. The mean number of total queries (feature and ex-
the bug becomes grayed out (i.e., eliminated). After a feature            emplar) taken to complete a round was 6.5 in the automatic-
query, the bug gave feedback, saying “Yes!” (it has the fea-              update condition, and 7.6 in the manual-update condition.
ture; narrated by the experimenter), or “No!” (it does not have           Although the median queries to complete a round in each
the feature). This was followed by the elimination phase, dur-            condition was 6, the distributions were significantly different
ing which bugs that are inconsistent with the feedback were               (Kolmogorov-Smirnov test, D = 0.13, p < .01). For compar-
eliminated, thus narrowing the hypothesis space.                          ison, we simulated 700 rounds of the game with an agent that
                                                                          clicked randomly in the task. This agent took on average 8.9
   Participants were assigned in counterbalanced order to one
                                                                          queries (median: 9) to complete a round.
of two hypothesis-updating conditions. In the automatic-
update condition, after the feedback from a feature query,                Response Times Participants’ median RT for each button
subjects merely pressed the “Eliminate” button and all the                type (feature and exemplar) was computed and these data
irrelevant bugs are eliminated (grayed out), and the game re-             were subjected to an ANOVA with condition (automatic,
turns to the guessing phase. In the manual-update condition,              manual) and age group (5-7, 8-10) as between-subjects fac-
after a subject made a feature query and saw feedback, they               tors and button type as a within-subject factor. There were
had to select each bug that was consistent with the feedback              significant main effects of button type (F(1,229) = 42.52,
for that feature, as shown in the top right of Figure 3. Bugs             p < .001) and condition (F(1,229) = 4.14, p < .05), but not a
were selected (denoted by a green box) by tapping, and could              significant main effect of age group (F(1,229) = 0.73). On av-
                                                                      2479

erage, participants took longer to make queries in the manual                                      other interactions were significant (all Fs < 1).
condition (4800 ms) than in the automatic condition (4000                                             Figure 5 shows the average number of query types used
ms). Overall, participants took much longer to make fea-                                           per round for participants by age group. Both age groups
ture queries (7,470 ms) than to press an exemplar button                                           in the manual-update condition used more exemplar queries
(2,680 ms), perhaps indicating more thought before making                                          than feature queries. In comparison to the manual condition,
more complex queries. There was also a significant interac-                                        there were fewer exemplar queries in the automatic condi-
tion effect of button type and condition (F(1,229) = 12.89,                                        tion (Mman = 5.0, Mauto = 3.2, t(103.5) = 4.1, p < .001),
p < .001). Figure 4 shows the mean of subjects’ median RTs                                         while there were more feature queries in the automatic con-
for each button type, split by condition. Feature queries were                                     dition (Mauto = 3.8) (Mman = 3.3, t(102.9) = 2.1, p < .05).
slower in the manual-update condition (7900 ms vs. 5430 ms                                         These query rates were all lower than the simulated random
in automatic), which could indicate 1) more careful thought                                        rounds’ mean number of feature queries (6.5) and exemplar
given to features in this condition, and/or 2) general hesitance                                   queries (5.3), but above the optimal.1 Older participants used
to use feature queries, perhaps because it is time-consuming                                       a greater proportion of feature queries than younger partic-
(even difficult) to manually update hypotheses. Exemplar                                           ipants in both the automatic (M5−7 = .50 vs. M8−10 = .66,
queries were faster in the manual-update condition (1850 ms                                        t(57.2) = 3.12, p < .01) and manual conditions (M5−7 = .39
vs. automatic: 2570 ms), which could be greater readiness to                                       vs. M8−10 = .50, t(50.3) = 2.30, p < .05). Thus, both con-
use the simpler strategy.                                                                          ditions replicate the Mosher and Hornsby (1966) finding that
                                                                                                   older children use a greater proportion of constraint-seeking
                                                                                                   questions.
                                             automatic              manual
                                  8000                                                                                                           automatic       manual
                                                                                                       Mean Number of Queries per Round
           Mean of Ss Median RT
                                  6000
                                                                                                                                          7.5
                                                                                   button_type
                                                                                      exemplar
                                  4000
                                                                                      feature                                                                               Query Type
                                                                                                                                          5.0
                                                                                                                                                                               exemplar
                                                                                                                                                                               feature
                                  2000
                                                                                                                                          2.5
                                    0
                                         exemplar   feature   exemplar   feature
                                                       Query Type
                                                                                                                                          0.0
Figure 4: Mean of participants’ median RT for each condition and                                                                                5−7    8−10    5−7   8−10
query type. Exemplar queries were faster than feature queries, which                                                                                    Age Group
represent a more complex strategy and thus likely required more
thought. Feature queries were slower in the manual-update con-                                     Figure 5: Mean number of queries of each type per round by age
dition: it seems the difficulty of updating in this condition made                                 and condition. Error bars show +/-1SE.
participants think even more carefully about using feature queries.
Error bars show +/-1SE.
                                                                                                      In summary, it is clear that the manual-update condition re-
                                                                                                   sults in fewer feature queries and more reliance on exemplar
Querying Behavior Participants’ mean number of queries                                             queries. Manual-update participants may be loathe to use fea-
per round were subjected to an ANOVA with condition and                                            ture queries for at least two reasons: 1) it demands more time
age group (5-7 vs. 8-10) as between-subjects factors and                                           and cognitive effort to manually update the hypothesis space
query type as a within-subject factor. This analysis indicated                                     after a feature query than in the automatic-update condition,
significant main effects of condition (F(1,229) = 4.60, p <                                        and 2) the manual update process is error-prone, and any mis-
.05) and age group (F(1,229) = 12.20, p < .001), and no sig-                                       takes may in turn lead to more exemplar queries in order to
nificant main effect of query type (F(1,229) = 0.10, p = .75).                                     recover.2 Therefore we proceed to investigate errors in man-
Overall, older children required fewer total queries to com-
plete a round (M5−7 = 4.2, M8−10 = 3.3), also evidenced by                                            1 Although there were at first more exemplars (16) than feature
a significant negative correlation with age (t(119) = 3.24,                                        buttons (10), after the first 1-2 clicks there would likely be few ex-
                                                                                                   emplars remaining, thus the expected number of exemplar queries is
p = .001, r = −.28). There were significant interactions of                                        lower than the expected number of feature queries in the simulation.
condition and query type (F(1,229) = 22.18, p < .001), and                                            2 If the correct answer is mistakenly eliminated, additional clicks
age group and query type (F(1,229) = 12.25, p < .001). No                                          on the grayed-out bugs were needed to find it and finish the round.
                                                                                                2480

ual updating, as well as information theoretic analyses that
will indicate whether the quality of feature queries varied in                              I(X;Y ) = H(X) − H(X|Y )                       (2)
the two update conditions. Although the qualitative analy-
                                                                           The Expected Information Gain (EIG) of a query Q is the
ses have thus far revealed interesting effects that build on the
                                                                        weighted average of the information possible from each pos-
previous literature, as the game unfolds the utility of different
                                                                        sible answer to the query, weighted by the current probabil-
query types (and specific queries) changes, and can be best
                                                                        ity of receiving that answer. This will be 0 (or near-0) for
quantified using a more sophisticated model-based approach
                                                                        queries that can be expected to eliminate none or just one or
to understanding the quality of children’s question asking.
                                                                        two hypotheses in a large space, and more positive for queries
Manual Update Mistakes The manual-update condition                      that are likely to eliminate a larger number of hypotheses. In
allows participants to commit two types of error during hy-             this task, EIG is maximal (1) for a feature query that will
pothesis updating: a miss is defined as a failure to eliminate          eliminate half the remaining hypotheses. Such a query is al-
a bug, and a false alarm is a failure to keep a hypothesis that         ways available at the beginning of any round, and due to the
was consistent with the query. Note that a miss is an error             partially-nested feature structure used, maximal EIG queries
of commission–i.e., the bug had to be tapped to be kept–                are often available at other stages of the round.
whereas a false alarm is an error of omission (i.e., failing
to tap a bug), and thus we expect more of the latter. Com-                                EIG(Q) = − ∑ p(Y |Q)I(X;Y )                      (3)
                                                                                                           Y
paring the manual-update subjects’ mean number of errors
of each type per round, indeed there were more false alarms                EIG has often been proposed as a model of how children
(M = 6.9, sd = 1.9) than misses (M = 1.8, sd = 1.3; paired              might evaluate the quality of possible queries. For exam-
t(58) = 19.8, p < .001). A MANCOVA to determine if error                ple, Nelson, Divjak, Gudmundsdottir, Martignon, and Meder
rates were related to age did not find a significant effect for ei-     (2014) found that 8-10 year-old children can search a famil-
ther misses (F(1,56) = 0.77, p > .05) or false alarms (F(1,56)          iar structured domain (people with varying gender, hair color,
= 0.23, p > .05). Given the fairly high rate of errors in manual        etc.) fairly efficiently, tending to ask about frequent real-
updating, it is perhaps unsurprising that fewer feature queries         world features that roughly bisected the search space. Like-
and more exemplar queries were made in this condition than              wise, Ruggeri, Lombrozo, Griffiths, and Xu (2015) found ev-
under automatic updating of the hypothesis space. However,              idence that children’s patterns of search decisions were well-
RT analyses indicated that feature queries took longer under            explained in terms of EIG.
manual updating: is this just reluctance, or could it be that              In our study, the EIG for each participants’ feature queries3
feature queries were more carefully considered in this con-             were computed, and their mean EIG was subjected to an
dition than under the ease of automatic updating? The ex-               ANOVA with condition and age group (5-7 vs. 8-10) as
pected information gain of children’s feature queries provides          between-subjects factors. This ANOVA indicated significant
a measure of their sensitivity to the information structure in          main effects of condition (F(1,115) = 55.0, p < .001) and age
the stimuli.                                                            group (F(1,115) = 12.42, p < .001), with no significant in-
                                                                        teraction effect (F(1,115) = 0.2, p > .05).4 Figure 6 shows
Expected Information Gain Each successive query re-                     mean EIG per feature query by age group and condition.
duces the size of the remaining hypothesis space to some de-            Mean EIG of feature queries for each subject was marginally
gree: on the first move, querying the appropriate feature (F1)          correlated with age (t(116) = 1.77, p = .08, r = .16), sug-
can cut the space in half. When two hypotheses remain, even             gesting that older children tended to use more relevant fea-
an exemplar query will cut the space in half. The appropriate           ture queries. The feature queries made by participants in the
way to analyze the contextual sensitivity (i.e., are they choos-        automatic condition had significantly lower EIG than those
ing a feature that is present for half of the remaining exem-           made in the manual condition (Mauto = .60, Mman = .74,
plars, thus quickly reducing the hypothesis space?) of partic-          t(116) = 5.49, p < .001). Thus, although manual-update
ipants’ queries is to calculate the Expected Information Gain           participants used fewer feature queries overall, and tended
(EIG) of the query they made. We first introduce key terms              to make mistakes during hypothesis updating, the greater
used to define EIG. Entropy measures uncertainty about the              amount of time they spent when choosing a feature query
outcome of a random variable X. Entropy is 0 when there is              tended to pay off: manual-update participants queried fea-
only one possible outcome, and maximal when all possible                tures with higher expected information gain than automatic-
outcomes are equiprobable (i.e., a uniform distribution).
                                                                            3 Exemplar query EIGs are less interesting, as they are a simple
                                                                        function of how many remaining hypotheses there are. Participants’
                 H(X) = − ∑ p(x) · log(p(x))                   (1)      choice of feature query, on the other hand, indicates how sensitive
                              x                                         they are to the relevance of each feature–and to the context of their
   Mutual information gain measures the change in entropy               current situation, as it is based on the remaining bugs’ features.
                                                                            4 The same significant effects and similar mean EIG values were
as we receive a new piece of information Y , i.e., how much             obtained when analyzing only the first two feature queries per round,
does our uncertainty about X change given that we know Y?               when manual- and automatic-update participants were on more
                                                                        equal footing (i.e., before further manual errors–which could raise
This occurred rarely, happening in < 10% of rounds.                     or lower the EIG of the remaining feature queries).
                                                                    2481

                                                                                             of which feature query to use, and ultimately a better choice.
                                        0.8                                                  Indeed, response times for feature queries were slower un-
       Mean Expected Information Gain
                                                                                             der manual updating, perhaps indicating that greater thought
                                                                                             went into making those choices. Indeed, slower feature query
                                        0.7                                                  RTs were correlated with higher EIG. In both conditions,
                                                                         condition           older children made more informative feature queries, but
                                                                          ●   automatic      even 5-7 year-olds asked far more informative questions than
                                                        ●
                                        0.6                                   manual         a simulation that chose a random sequence of queries, show-
                                                                              random
                                                                                             ing some efficiency in navigating an unfamiliar domain even
                                               ●
                                                                                             after only a few minutes of experience.
                                                                                                In summary, this study provides evidence that hypothesis
                                        0.5
                                                                                             updating is a difficult, error-prone step in the active inquiry
                                                                                             process. Moreover, children are sensitive to the difficulty of
                                                                                             this step: if aided in hypothesis updating, they will ask more
                                        0.4                                                  constraint-seeking questions than if they must manually up-
                                              5−7     8−10   simulated                       date the space. However, we also uncovered evidence of a de-
                                                    Age Group
                                                                                             sirable difficulty in this step, for manual updating resulted in
Figure 6: Mean expected information gain for feature queries by age                          more informative, context-sensitive constraint-seeking ques-
group and condition, with simulated subjects making random feature                           tions than the supported update process. Future work will aim
queries for comparison. Manual- update subjects had higher EIG                               to reduce errors in hypothesis updating and discover other
than automatic-update subjects, and both were better than random–
but suboptimal (1). Older children had higher EIG than younger                               bottlenecks–or desirable difficulties–in active inquiry.
children. Bars show +/-1SE.
                                                                                                                  Acknowledgments
                                                                                             This work was supported by the John Templeton Foundation
                                                                                             “Varieties of Understanding” grant to TMG and MR.
update participants. Indeed, there was a weak but significant
correlation of participants’ mean feature query RT and EIG                                                             References
(r = .20, t(116) = 2.17, p < .05), verifying that longer RTs                                 Bonawitz, E., & Griffiths, T. (2010). Deconfounding hypothesis
                                                                                               generation and evaluation in bayesian models. In S. Ohlsson &
are associated with more informative feature queries.                                          R. Catrambone (Eds.), Proceedings of CogSci 32. Austin, TX.
                                                                                             Bransford, J., Brown, A., & Cocking, R. R. (Eds.). (2000). How
                                               General Discussion                              people learn: Brain, mind, experience, and school. National Re-
The present study asked children 5-10 years of age to learn                                    search Council.
                                                                                             Chen, Z., & Klahr, D. (1999). All other things being equal: Chil-
feature distributions in an unfamiliar hypothesis space, and                                   dren’s acquisition of the control of variables strategy. Child De-
examined both their qualitative questioning strategies, and                                    velopment, 70(5), 1098–1120.
how efficiently they were able to search that space. Impor-                                  Donovan, M., & Bransford, J. (Eds.). (2005). How students learn:
                                                                                               Science in the classroom. Nat’l Research Council.
tantly, we manipulated the support children were given while                                 Duschl, R., Schweingruber, H., & Shouse, A. (Eds.). (2007). Taking
updating the hypothesis space: after a feature query, partic-                                  science to school: Learning and teaching science in grades k-8.
ipants in the automatic update condition were shown which                                      Washington, D.C.: National Research Council.
                                                                                             Herwig, J. A. (1982). Effects of age, stimuli, and category recogni-
bugs were eliminated at the press of a button, whereas man-                                    tion factors in children’s inquiry behavior. Journal of Experimen-
ual update participants were required to select the bugs that                                  tal Child Psychology, 33, 196–206.
were consistent with the feedback.                                                           Klahr, D., & Nigam, M. (2004). The equivalence of learning paths
                                                                                               in early science instruction: Effects of direct instruction and dis-
   In line with previous research (Mosher & Hornsby, 1966;                                     covery learning. Psychological Science, 15(10), 661–667.
Ruggeri & Lombrozo, 2014), older children (ages 8-10)                                        Kuhn, D., & Dean, D. (2005). Is developing scientific thinking
                                                                                               all about learning to control variables? Psychological Science,
asked a higher proportion of constraint-seeking questions                                      16(11), 866–870.
than younger children (ages 5-7), who relied more on                                         Mosher, F. A., & Hornsby, J. R. (1966). Studies in cognitive growth.
hypothesis-scanning (i.e., exemplar queries), in both condi-                                   In (chap. On asking questions). New York, NY: Wiley.
                                                                                             Nelson, J. D., Divjak, B., Gudmundsdottir, G., Martignon, L. F., &
tions. These qualitative analyses also found that children use                                 Meder, B. (2014). Children’s sequential information search is
more constraint-seeking questions (i.e., feature queries) in the                               sensitive to environmental probabilities. Cognition, 130, 74–80.
automatic-update condition. On the surface then, these chil-                                 Ruggeri, A., & Feufel, M. A. (2015). How basic-level objects facil-
                                                                                               itate asking efficient questions in a categorization task. Frontiers
dren were using a more efficient strategy than the manual-                                     in Psychology, 6(918), 1–13.
update children.                                                                             Ruggeri, A., & Lombrozo, T. (2014). Learning by asking: How
   However, in terms of expected information gain, a context-                                  children ask questions to achieve efficient search. In Proceedings
                                                                                               of CogSci 36. Cognitive Science Society.
sensitive measure of how well a chosen feature bisects the                                   Ruggeri, A., & Lombrozo, T. (2015). Children adapt their questions
remaining hypothesis space, it turned out that children in                                     to achieve efficient search. Cognition, 143, 203–216.
the automatic-update condition made less informative feature                                 Ruggeri, A., Lombrozo, T., Griffiths, T., & Xu, F. (2015). Children
                                                                                               search for information as efficiently as adults, but seek additional
queries. We suggest that the greater mental effort required by                                 confirmatory evidence. In D. C. Noelle et al. (Eds.), Proceedings
manual updating actually lead to more careful consideration                                    of cogsci 37.
                                                                                          2482

