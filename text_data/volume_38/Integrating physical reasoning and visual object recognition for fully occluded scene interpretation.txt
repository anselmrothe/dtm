                      Perceiving Fully Occluded Objects via Physical Simulation
                                                   Ilker Yildirim*1 (ilkery@mit.edu)
                            BCS, MIT and The Laboratory for Neural Systems, The Rockefeller University
                                                    Max H. Siegel* (maxs@mit.edu)
                                                 Joshua B. Tenenbaum (jbt@mit.edu)
                                                                     BCS, MIT
                               Abstract
   Conventional theories of visual object recognition treat objects
   effectively as abstract, arbitrary patterns of image features.
   They do not explicitly represent objects as physical entities in
   the world, with physical properties such as three-dimensional
   shape, mass, stiffness, elasticity, surface friction, and so on.
   However, for many purposes, an object’s physical existence is
   central to our ability to recognize it and think about it. This
   is certainly true for recognition via haptic perception, i.e., per-
   ceiving objects by touch, but even in the visual domain an ob-
   ject’s physical properties may directly determine how it looks
   and thereby how we recognize it. Here we show how a physi-
   cal object representation can allow the solution of visual prob-
   lems, like perceiving an object under a cloth, that are other-                    Figure 1: Two objects occluded by cloths.
   wise difficult to accomplish without extensive experience, and
   we provide behavioral and computational evidence that people
   can use such a representation.
   Keywords: physical object representations; analysis-by-
   synthesis; object perception; occlusion; psychophysics                  and image generation. We posit that objects are represented,
                                                                           at a minimum, by attributes including three-dimensional ge-
                                                                           ometry; rigidity; mechanical material properties; and optical
             The common and almost despairing feeling . . . was that       material properties.
             practically anything could happen in an image and
             furthermore that practically everything did.
                                                                              Consider the covered objects displayed in Figure 1. Both
                                                    David Marr, Vision     objects are completely occluded, but it is easy to say which of
                                                                           them might be a chair. These images were generated by using
                           Introduction                                    a physics simulator to drop a simulated cloth onto two sepa-
                                                                           rate 3D models (one a chair), then using a rendering engine
Object perception is notoriously difficult, in part because the
                                                                           to produce images from the resulting scene.
appearance of an object can vary in almost any way. The
problem has been studied in neuroscience, cognitive psychol-                  This describes a process for generating the image, but the
ogy, and artificial intelligence, leading to a loose consen-               same process can also be used to interpret an image. When
sus that object perception can be solved by the brain (or a                asked which image has a chair in it, we can simulate drop-
computer) learning to “untangle” or become “invariant to”                  ping cloths onto a chair mesh, and compare the rendered re-
sources of variation in the image (DiCarlo, Zoccolan, & Rust,              sults with each candidate (this is an intuitive sketch of a pro-
2012; LeCun, Bengio, & Hinton, 2015). On this account, sen-                cedure; it can be made precise with Bayes’ theorem, which
sory input is repeatedly transformed, ideally leading to a (bio-           specifies how to turn a forward model into an inverse model.
logical or artificial) neural code that is diagnostic for a partic-        See also (Battaglia, Hamrick, & Tenenbaum, 2013)).
ular object regardless of variation in the image (Riesenhuber
& Poggio, 1999).                                                              There are several notable differences between the latter
   We study an alternative solution to the object perception               approach (which we will call analysis-by-synthesis) and the
problem, which is enabled by a different representation for                ”consensus” approach (which we will call the invariant fea-
objects and a different attitude towards variation. The ba-                tures approach). First and most notably, invariant features ap-
sic idea is to model the causal processes that lead to an ob-              proaches must learn each kind of scene transformation inde-
served image, explaining and reproducing image variation                   pendently. We explained above how knowledge about chairs
rather than attempting to ignore it. More specifically, we take            and cloths can be combined, in the analysis-by-synthesis ap-
an object to be represented (at least in part) by a set of phys-           proach, to recognize a chair underneath a cloth. By contrast,
ical attributes necessary for supporting physical interaction              invariant features approaches cannot directly leverage exist-
                                                                           ing knowledge to recognize the compound object. They must
    1 indicates equal contribution.                                        be trained, separately, to discount the cloth in order to recog-
                                                                       1265

nize the chair2 . Second, while invariant features approaches         without texture, and at a random viewing angle, and (3) fully
are mostly agnostic to the kinds of image transformations that        occluded with a cloth draped over it, and with the mesh ran-
might exist, analysis-by-synthesis implicitly handles a wide          domly rotated.
variety of transformations without being explicitly trained or           For (2) and (3), the rotation was sampled from the full
taught to do so.                                                      sphere with a slight preference around the canonical view-
   Analysis-by-synthesis in vision has a long history (Yuille         point – viewpoint of (1). More specifically, a rotation angle
& Kersten, 2006; Tu, Chen, Yuille, & Zhu, 2005), and has              of ±35◦ of the canonical viewpoint on all three axes was 1.5
recently seen increased attention (Kulkarni, Kohli, Tenen-            more likely than the rest of the sphere. For (3), we simulated
baum, & Mansinghka, 2015; Yildirim, Kulkarni, Freiwald, &             a cotton-like cloth draped over the rotated mesh for a total of
Tenenbaum, 2015; Erdogan, Yildirim, & Jacobs, 2015). Our              100 simulation steps, and obtained a rendering of the very last
work focuses primarily on two less-studied aspects: First,            step of the simulation.
while most work in object perception studies unoccluded or               We used a total of 197 meshes to generate 100 five-tuples
partially occluded objects, we are interested in objects that         of one study item of unoccluded object rendered at a canon-
are fully occluded, so that the only perceptible effect of the        ical viewpoint with texture, and four test items consisting of
object is on its occluder. Said another way, perceiving ob-           two unoccluded objects without texture each rendered after
jects through cloths requires an observer to do without most          randomly rotating the meshes, and two objects rendered af-
or all of the visual information that one normally uses. Sec-         ter randomly rotating each and then occluding with a cloth.
ond, unlike most previous work, we are interested primarily           The unoccluded study items were never seen twice, but the
in how the object representation enables this kind of percep-         test items were repeated multiple times, each at a different
tion. If objects are represented geometrically in a way that          rotation or viewing angle. On 57 of the 100 tuples, the dis-
can interact with physics, then the procedure outlined above          tractors were of the same category, and 43 of the 100 tuples,
shows how to solve the cloth task without more training.              the distractors were of different category. Example pairs of
   The object-under-cloth task is most interesting in the case        unoccluded study items and occluded test items are shown in
of novel cloth-object pairs (such as an airplane under a cloth).      Figure 2a.
We predict that both people and analysis-by-synthesis models
will perform well on this task, but invariant features models         Procedure
will not without further training. We ran a large scale study         Both experiments were match-to-sample tasks where both the
to assess how well people can perform the object-under-cloth          study and the test items were presented simultaneously and
task. After describing the task and the results of the study,         all stayed on the screen until the participants responded. In
we will evaluate the performance of each of the candidate             Experiment 1 (N = 27), the study item was an unoccluded
models, and discuss the implications of our findings.                 image of an object from a canonical viewpoint; the test items
                                                                      were images of two unoccluded objects after randomly rotat-
                        Experiments                                   ing each. Participants had to choose which of the test items
We performed two experiments where participants needed to             contained the study item (Figure 2b).
generalize from a single view of 3D object shown at a canon-             In Experiment 2 (N = 31), the study item was also an un-
ical view to either a novel view of that object (Experiment 1)        occluded image of an object; but the test items were images
or to a fully occluded image of that object again at a novel          of two objects rotated randomly and occluded with a cloth.
view (Experiment 2).                                                  Again, participants had to choose which of the test images
                                                                      contained the study item (Figure 2b).
Participants                                                             In each of the experiments, participants completed 10 prac-
58 participants were recruited from Amazon’s crowdsourcing            tice trials before moving onto 90 experimental trials. Partici-
web-service Mechanical Turk. The experiment took about 20             pants were provided with their running average performance
minutes to complete. Each participant was paid $1.50.                 at every 5th trial throughout the experiment.
Stimuli                                                               Results
                                                                      Results of the experiments are shown in Figure 3a. Partici-
The stimuli were generated using a subset of the meshes from
                                                                      pants performance on Experiment 1 (with the unoccluded test
the ShapeNet (Chang et al., 2015) database using Blender
                                                                      items) were high overall (93%).
(Blender Online Community, 2015), a 3D modeling and ren-
                                                                         Participants performance was surprisingly high in Experi-
dering program. The meshes we used represented objects
                                                                      ment 2 with the occluded test items at 80% (Figure 3a), and
from five different categories: guns, cars/buses, bicycles, lap-
                                                                      as expected, their performance was lower when compared to
tops and pillows.
                                                                      Experiment 1.
   We rendered each mesh in three ways: (1) unoccluded,
                                                                         The type of the distractor (whether it is of the same cat-
with texture, and from a canonical viewpoint, (2) unoccluded,
                                                                      egory as the study item or different category) introduced a
   2 Alternatively, models might be ”retrained” or similar; these     much stronger decline in performance in Experiment 2 than
methods also require more training.                                   in Experiment 1 (Figure 3b).
                                                                  1266

Figure 2: (A) Pairs of images of meshes and simulation results after rotating the mesh randomly and draping a cloth over it. (B)
Screenshot of an example trial in the unoccluded experiment. (C) Screenshot of an example trial in the occluded experiment.
                          Models                                  object factors (e.g., 3D shape, mass, friction, soft-body dy-
                                                                  namics, soft-body and rigid-body interaction); and graphics
We considered two models as potential accounts of our sub-
                                                                  (e.g. rotation and lighting direction). When each of these
ject’s performance: a physics-based analysis-by-synthesis
                                                                  factors are specified, we end up with a likelihood function
model, and a model derived from features learned by a deep
                                                                  that gives the probability of an image given latent parameters,
convolutional neural network trained to classify millions of
                                                                  P(I|Ψ).
unoccluded images.
                                                                     The model maps input images to the underlying scene pa-
Physics-based analysis-by-synthesis
                                                                  rameters using Bayesian inference. Bayes’ rule enables us to
We developed a Bayesian computational model that uses             use the (forward model) likelihood along with a prior distri-
knowledge of the causal processes underlying image forma-         bution (here taken to be uniform) to get the posterior distribu-
tion to interpret new images. Aspects of (synthetic) image        tion of the parameters given the image. The posterior, which
formation may be divided into two categories: physics-based       includes beliefs about the underlying mesh, is the object of
                                                             1267

                                                                        The uncertainty about the rotation or pose, R (a vector
                                                                     of length three of Euler angles), was taken to be higher
                                                                     for occluded test scenes than for unoccluded test scenes.
                                                                     For unoccluded test images, we modeled rotation as R ∼
                                                                     N(Rtrue , 0.025), whereas for occluded test images, we mod-
                                                                     eled it as R ∼ N(Rtrue , 0.1), where Rtrue is the true rotation
                                                                     of the test items. We approximated the rotation uncertainty
                                                                     using five randomly chosen samples from the normal distri-
                                                                     bution.
                                                                        We took a sampling-based approach to simulate partici-
                                                                     pants from our experiments. For a given simulated partici-
                                                                     pant and trial number, we took one joint sample of shape and
                                                                     rotation from a pool of 25 samples (5 possible shapes × 5
                                                                     rotations). Also using the true cloth parameters for occluded
                                                                     trials (i.e., rest of the parameters in Ψ), the model generated
                                                                     a sampled image, Is , compared it to each of the test images
                                                                     using correlation as its similarity metric, and returned the test
                                                                     image that was more similar to Is . We simulated 40 subjects in
                                                                     each of the occluded and unoccluded experiments. We report
                                                                     the average performance of these 40 simulated participants.
                                                                     Deep convolutional network
                                                                     We also evaluated a pre-trained deep convolutional neural
Figure 3: (A) Average performace of the participants in the          network, which has been shown to succeed at a number of
two experiments. (B) Performance of the subjects divided by          challenging visual recognition tasks (Jia et al., 2014). The
whether the distractor is of the same or different category as       network, like all convolutional neural networks, is a feed-
the study item. Error bars indicate standard error of the mean.      forward hierarchical model that performs a series of con-
                                                                     volutions and nonlinearities; such models can contain (as
interest for inference tasks.                                        does the model we evaluate) millions of learnable parame-
   Bayes’ rule states that the posterior is proportional to the      ters. The model was trained to classify objects using the Im-
likelihood times the prior, that is,                                 agenet (Deng et al., 2009) dataset; it is representative of a
                                                                     wide class of neural network models that find application in
                                                                     computer vision.
                  P(Ψ|I) ∝ P(I|Is , Ψ)δ p(·) δg(·)           (1)
   where Ψ are the latent variables (e.g. 3D shape, mass,            Results
friction, soft-body dynamics, rotation, lighting) that drive the     Figure 4 show the performance of the two models. The
physics and graphics engines; δ p(·) denotes a physics engine        pre-trained network performs worse than human participants
and δg(·) denotes a graphics engine (here, we used Blender           (Figure 4, left). Furthermore, unlike the human participants,
for both engines); P(Ψ|I) denotes the posterior distribution         the decline in the performance from by the type of the distrac-
over physical object representations; Is denotes the gener-          tor category is similar for both the occluded and unoccluded
ated image given Ψ, g(·), and p(·); and P(I|Is , Ψ) denotes          stimuli sets. The network performs at chance in the most dif-
the image log-likelihood for a given set of physical object          ficult condition of occluded and same-category distractor tri-
parameters (we assumed a Gaussian likelihood function with           als.
sigma = 0.01; N(I|Is , σ).                                              The physics-based analysis-by-synthesis model captures
   In our implementation, we deterministically assigned the          the average performance of the participants across the two ex-
cloth properties such as stiffness, mass and friction to their       periments accurately (Figure 4, right). Moreover, our model
true values, but assumed uncertainty for the exact 3d shape,         captures the details of the subjects performance when broken
S, of the study item and the rotation, R, of the test items.         down by the distractor type.
We approximated shape uncertainty as a uniform distribution
over the most similar five shapes given a study item, I and its                                  Discussion
underlying shape, SI , from the ShapeNet dataset. Similarity         Can humans recognize objects even when they are fully oc-
between a pair of meshes from the ShapeNet was determined            cluded? The behavioral study presented here indicates that
by correlating the concatenated images of each mesh with-            the answer is yes, at least when the candidates are known
out any texture at four view points – the canonical viewpoint        (we suspect that people can often do so otherwise, as in the
where each mesh is already aligned at in the dataset, and the        chair example in the introduction). What underlies our par-
three orthogonal viewpoints (top, right, front).                     ticipants’ performance? We think that this ability reflects the
                                                                 1268

Figure 4: (Left) Average performance of the physics-based analysis-by-synthesis model on the unoccluded and occluded stimuli
sets, and the breakdown of its performance by the tyep of the distractor category. Error bars indicate standard error of the mean.
(Right) Average performance of the pre-trained network on the unoccluded and occluded stimuli sets, and the breakdown of its
performance by the type of the distractor category. Dashed line shows the chance-level performance
presence of causal theories in human perception - in this case,       using basic knowledge about the world to derive and under-
theories about how soft and rigid objects bodies interact, and        stand how the world can influence our percepts. We consid-
about how the resultant shape gets rendered into a visual per-        ered the problem of perceiving objects under cloths. Invariant
cept. This approach is different from the standard model of           features theories of perception face a problem in this domain,
visual perception. Most accounts of visual object recognition         because cloth-draped objects differ dramatically in appear-
assert that the brain maps images to identity or class labels via     ance from their unoccluded states. Treating perception as an
increasingly more abstract feature hierarchies (DiCarlo et al.,       inverse compositional process provides a solution.
2012; Krizhevsky, Sutskever, & Hinton, 2012; Riesenhuber
& Poggio, 1999). In this approach, objects are not explic-
itly represented – they do not have physical properties such
as mass and friction or 3D shape. The cost of not having ex-             There are a number of future directions that we wish to
plicit and causal representations of the scenes is an inability       explore. First, our match-to-sample task is only one way of
of composition – or in other words, requirement for data (lots        getting at people’s abilities to perceive objects under heavy
of data) in the face of every new scene setting such as the           occlusion and in physical settings. We plan to build upon
setting presented here: objects fully occluded under cloths.          this paradigm for future experiments. In the Introduction we
                                                                      posed a question without answering - “which of these oc-
   These issues are not just theoretical: we found that a state-      cluded objects is a chair?”. This is perhaps the most inter-
of-the-art neural network (albeit trained only on unoccluded          esting future direction to pursue; it requires accessing and
images) had trouble with our behavioral task. In particular, it       manipulating the concept of a chair, instead of imagining ob-
could not approach human performance. We believe that any             ject transformations as we studied here. Second, we plan to
similar architecture (supervised learning) will have similar          build hybrid architectures that involve fast and feedforward
problems, because such approaches do not attempt to model             neural network pipelines for broad stroke comprehension of
the causal structure that gives rise to percepts.                     the scene and top-down physics-based architectures for in
   We suggested an alternative solution. Instead of relying           depth physical interpretation (Wu, Yildirim, Lim, Freeman,
on the environment to teach us about variation, we proposed           & Tenenbaum, 2015).
                                                                  1269

                      Acknowledgments                              Wu, J., Yildirim, I., Lim, J. J., Freeman, B., & Tenenbaum,
                                                                     J. (2015). Galileo: Perceiving physical object properties
This research was supported by the Center for Brains Minds           by integrating a physics engine with deep learning. In Ad-
and Machines (CBMM), funded by NSF STC award CCF-                    vances in neural information processing systems (pp. 127–
1231216 and by ONR grant N00014-13-1-0333.                           135).
                                                                   Yildirim, I., Kulkarni, T. D., Freiwald, W. A., & Tenenbaum,
                          References                                 J. B. (2015). Efficient analysis-by-synthesis in vision: A
Battaglia, P. W., Hamrick, J. B., & Tenenbaum, J. B. (2013).         computational framework, behavioral tests, and modeling
   Simulation as an engine of physical scene understanding.          neuronal representations. In Thirty-seventh annual confer-
   Proceedings of the National Academy of Sciences, 110(45),         ence of the cognitive science society.
   18327–18332.                                                    Yuille, A., & Kersten, D. (2006). Vision as bayesian infer-
                                                                     ence: analysis by synthesis? Trends in cognitive sciences,
Blender Online Community. (2015). Blender - a 3d mod-
                                                                     10(7), 301–308.
   elling and rendering package [Computer software man-
   ual]. Blender Institute, Amsterdam. Retrieved from
   http://www.blender.org
Chang, A. X., Funkhouser, T., Guibas, L., Hanrahan, P.,
   Huang, Q., Li, Z., . . . Yu, F. (2015). ShapeNet:
   An Information-Rich 3D Model Repository (Tech. Rep.
   No. arXiv:1512.03012 [cs.GR]). Stanford University —
   Princeton University — Toyota Technological Institute at
   Chicago.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei,
   L. (2009). Imagenet: A large-scale hierarchical image
   database. In Computer vision and pattern recognition, ieee
   conference on (pp. 248–255).
DiCarlo, J. J., Zoccolan, D., & Rust, N. C. (2012). How does
   the brain solve visual object recognition? Neuron, 73(3),
   415–434.
Erdogan, G., Yildirim, I., & Jacobs, R. A. (2015). From
   sensory signals to modality-independent conceptual repre-
   sentations: A probabilistic language of thought approach.
   PLoS Comput Biol, 11(11), e1004610.
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J.,
   Girshick, R., . . . Darrell, T. (2014). Caffe: Convolutional
   architecture for fast feature embedding. In Proceedings of
   the acm international conference on multimedia (pp. 675–
   678).
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Im-
   agenet classification with deep convolutional neural net-
   works. In Advances in neural information processing sys-
   tems (pp. 1097–1105).
Kulkarni, T. D., Kohli, P., Tenenbaum, J. B., & Mansinghka,
   V. (2015). Picture: A probabilistic programming lan-
   guage for scene perception. In Proceedings of the ieee
   conference on computer vision and pattern recognition (pp.
   4390–4399).
LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning.
   Nature, 521(7553), 436–444.
Riesenhuber, M., & Poggio, T. (1999). Hierarchical models
   of object recognition in cortex. Nature neuroscience, 2(11),
   1019–1025.
Tu, Z., Chen, X., Yuille, A. L., & Zhu, S.-C. (2005). Im-
   age parsing: Unifying segmentation, detection, and recog-
   nition. International Journal of computer vision, 63(2),
   113–140.
                                                               1270

