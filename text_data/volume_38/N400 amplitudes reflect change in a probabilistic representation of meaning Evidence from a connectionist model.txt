       N400 amplitudes reflect change in a probabilistic representation of meaning:
                                        Evidence from a connectionist model
                                          Milena Rabovsky (rabovsky@stanford.edu)
                                          Steven S. Hansen (sshansen@stanford.edu)
                                      James L. McClelland (mcclelland@stanford.edu)
                                            Department of Psychology, Stanford University
                              Abstract                                     The N400 is a negative deflection at centro-parietal
  The N400 component of the event-related brain potential is
                                                                        electrode sites peaking around 400 ms after the presentation
  widely used in research on language and semantic memory,              of a meaningful stimulus. N400 amplitudes have been
  but the cognitive functions underlying N400 amplitudes are            shown to be modulated by a wide variety of variables. For
  still unclear and actively debated. Recent simulations with a         example, the N400 is modulated by contextual fit, with
  neural network model of word meaning suggest that N400                larger amplitudes to incongruent as compared to congruent
  amplitudes might reflect implicit semantic prediction error.          sentence continuations, such as when the sentence ‚ÄúHe
  Here, we extend these simulations to sentence                         spread the warm bread with‚Ä¶‚Äù is completed by the word
  comprehension, using a neural network model of sentence
  processing to simulate a number of N400 effects obtained in           ‚Äúsocks‚Äù instead of ‚Äúbutter‚Äù. The N400 also shows larger
  empirical research. In the model, sequentially incoming words         amplitudes to congruent continuations with lower as
  update a representation capturing probabilities of elements of        compared to higher cloze probability; decreasing amplitudes
  sentence meaning, not only reflecting the constituents                over the course of a sentence; smaller amplitudes for targets
  presented so far, but also the model‚Äôs best guess at all features     after semantically or associatively related as compared to
  of the sentence meaning based on the statistical regularities in      unrelated primes; smaller amplitudes for repeated words as
  the model‚Äôs environment internalized in its connection
                                                                        compared to a first presentation; and smaller amplitudes for
  weights. Simulating influences of semantic congruity, cloze
  probability, a word‚Äôs position in the sentence, reversal              words of high as compared to low lexical frequency.
  anomalies, semantic and associative priming, categorically               Despite the large body of data on N400 amplitude
  related incongruities, lexical frequency, repetition, and             modulations and widespread agreement that the N400 is
  interactions between repetition and semantic congruity, we            related to semantic processing, the computational principles
  found that the update of the predictive representation of             and processing mechanisms underlying N400 amplitude
  sentence meaning consistently patterned with N400                     generation are as yet unclear. Various verbally-formulated
  amplitudes. These results are in line with the idea that N400
  amplitudes reflect semantic surprise, defined as the change in
                                                                        theories are currently under active debate proposing, e.g.,
  the probability distribution over semantic features in an             that N400 amplitudes reflect lexical and/or semantic access,
  integrated representation of meaning occasioned by the arrival        semantic integration/ unification, semantic binding, or
  of each successive constituent of a sentence.                         semantic inhibition (reviewed by Kutas & Federmeier,
  Keywords: neural network model; sentence comprehension;
                                                                        2011). The development of an explicit computational
  language; event-related potentials; N400; semantic surprise           account that addresses this diverse range of phenomena
                                                                        would thus appear to be a worthwhile goal.
                         Introduction                                      There are at least two ways in which one could seek to
                                                                        better understand the N400 component by means of
Language and meaning processing have been investigated                  computational modeling. First, one might try to implement a
with event-related brain potentials (ERPs), providing                   neurobiologically realistic model of the brain processes
continuous time-resolved measures of electrical brain                   underlying the N400 component, an approach that makes it
activity, and with models that characterize the processing of           possible to also model the morphology of the ERP
language and meaning, either in terms of the principles that            waveform (Laszlo & Plaut, 2012; Laszlo & Armstrong,
govern these processes or the processes that implement                  2014). Another approach is to directly relate variations in
them. Integration of these approaches could constrain                   N400 amplitudes to measures obtained from functional-
selection among alternative models of the computations and              level models of cognitive processes. While this approach
the processes that implement them, while also providing for             may entail losing some possibly interesting information
a possible integrated explanation of the diverse set of                 with respect to neural realization, it allows the modeling
empirical phenomena that have been discovered through                   process to focus on the goal of better understanding the
ERP research. In this spirit, the current work builds on other          cognitive functions underlying N400 amplitudes. Many
work described below, aiming to contribute to a better                  neural network models are of this functional type, in that the
understanding of the computational principles and                       model is viewed as conforming to a computational principle
functional processes underlying the N400 ERP component,                 characterized at the functional level. The principle is often
an electrophysiological indicator of meaning processing (see            articulated in terms of the goal to maximize consistency
Kutas & Federmeier, 2011, for review).
                                                                    2045

with information in the environment which can then be              representations consistent with the sentence through the nth
formalized in terms of the more specific goal of minimizing        word and the distribution consistent with the sentence
prediction error (Hinton, 1987; Rumelhart et al., 1995).           through the preceding word, as measured by the Kullbach-
Focusing on this functional level, Rabovsky & McRae                Leibler divergence:
(2014) used an attractor network model of word meaning to
investigate which measure in the model covaries with N400                  ùëÜùëíùëöùëÜ! =       ùëù(ùëü|ùëõ) log ! ùëù(ùëü|ùëõ) ùëù(ùëü|ùëõ ‚àí 1)
amplitudes over a series of typical N400 word processing
                                                                                      !
paradigms. They consistently observed a close                         Here r indexes alternative possible patterns of semantic
correspondence between N400 amplitudes and network                 features of the event being described by the sentence and
error at the semantic layer of representation, and took this       p(r|n) and p(r|n-1) denotes the probability of that pattern
correspondence to suggest that N400 amplitudes reflect             given the sentence up through word n and n-1 respectively.
implicit semantic prediction error. Here we extend this               The change in activation at the hidden (SG) layer of the
approach to the processing of words in sentences.                  model reflects this semantic surprise, and, as shown in a
   What kind of model would be most appropriate to                 series of simulations of empirical N400 effects described
simulate N400 amplitudes in sentences? Simple recurrent            below, models the N400, suggesting that the N400 is itself a
network models (SRNs; Elman, 1990) are typically trained           measure of semantic surprise.
to predict the next word in sentences based on the preceding
context so that network error in these models reflects an                        The Sentence Gestalt model
implicit prediction error which could correlate with N400
amplitudes. Indeed, such a correlation was recently reported       The SG model does not assume that sentences are
by Frank et al. (2015) who used four information measures          represented in a specific propositional format. Instead, it is
derived from three probabilistic language models as                based on the idea that the task of sentence processing
predictors for six ERP deflections (including the N400).           consists in processing sequences of incoming words to build
However, the prediction error in SRNs trained to predict the       representations enabling correct responses to various probes,
next input based on the preceding context is not specific to       and the model is allowed to find the best way to build these
semantics but rather reflects word surprisal (the negative         representations in order to meet the imposed demands
log of the probability of a word in a specific context) and is     through adjustments of connections between simple
affected by both syntactic and semantic expectation                processing units organized in layers. A detailed description
violations (Levy, 2008). As the N400 is a functionally             of the model is provided elsewhere (McClelland et al.,
specific indicator of meaning processing (Kutas &                  1989); we briefly sketch it here. For the current simulations,
Federmeier, 2011) while syntactic violations typically             the model was re-implemented in the PDPTool software, V3
modulate different ERP components, we therefore decided            (http://web.stanford.edu/group/pdplab/pdphandbookV3/).
against using such an SRN, and instead simulated N400
amplitudes using a model that is specifically trained to           Architecture. The model can be conceptualized as
understand and predict sentence meaning, the Sentence              consisting of two parts (see Fig. 1). The first part
Gestalt (SG) model (McClelland et al., 1989).                      sequentially processes each incoming word (presented at the
   The SG model minimizes the mismatch between its                 input layer) to update activation in the SG layer which
estimates of the probability of semantic features of events        represents the model‚Äôs best guess interpretation of the
given the words presented so far in a sentence and the             meaning of the sentence as a whole, using the previous
observed probabilities of these features in the meanings of        activation of the SG layer together with the activation
sentences, such that, once it has learned, its estimates after     induced by the new incoming word to produce the updated
each new word encountered as a sentence unfolds should             SG layer activation. The second part of the model is used
come close to matching the true probabilities. Thus the            primarily for performance assessment and training,
model can be characterized as an implicit probabilistic            decoding the content from the SG layer by probing it
model of sentence comprehension: The model‚Äôs outputs can           concerning the event described by the sentence.
be seen as representing conditional probability distributions
over possible semantic features of the events described by         Environment and training. It is important to note that the
the sentence up to and including the latest word.                  statistical regularities underlying the model‚Äôs best guess
Furthermore, the magnitude of the update of the hidden unit        interpretation of the meaning of the sentence at a given
state produced by the presentation of the latest word can be       point in its presentation are determined by the training set so
characterized as reflecting the change in this probability         that the effects on semantic surprise depend on the training
distribution produced by the word. We use a measure of this        set as well. There are two different approaches to training
update we call semantic surprise, based on a measure that          which are complementary in that they each have strengths
has been called Bayesian surprise (Itti & Baldi, 2005).            and weaknesses. First, models can be trained on large-scale
Formally, the semantic surprise (SemS) produced by the nth         training corpora approximating real life language
word in a sentence is defined as the difference between the        environments of human participants. While this approach
probability     distribution    over     semantic      feature     allows for simulation of empirical experiments with the
                                                                   exact same stimuli on a single-trial basis, the factors
                                                               2046

responsible for the effects produced by the model may               one more semantic feature (‚Äòcan fly‚Äô) and then the canary
remain somewhat opaque. A second possible approach is to            had two individuating features (‚Äòcan sing‚Äô and an item-
train models on a synthetic training set which implements           unique individuating feature) so that the robin and the
variation among certain dimensions considered to be                 canary shared three of their five semantic features while the
relevant for the target empirical phenomenon (the N400, in          salmon and the canary shared two features, the rose and the
our case) and/or the theory advanced to explain the                 canary shared only one feature, and the jeans and the canary
empirical phenomenon (semantic surprise, in our case).              did not share any features. While the labels for the features
While this approach is limited in its capacity to fully explain     are irrelevant for model behavior, the aim in constructing
specific empirical data points, it is more transparent              the representations was to create graded similarities between
concerning the general factors and principles responsible for       concepts roughly corresponding to real world similarities. A
the effects produced by the model. Because the main aim of          comparison between a similarity matrix of the concepts
the current study is to advance a theory concerning the             based on the hand-crafted semantic representations and
functional basis of N400 amplitudes by highlighting the             representations based on semantic word vectors derived
common core shared by the different dimensions that have            from co-occurrences in large text corpora (Pennington,
been shown to modulate them, this transparency concerning           Socher, & Manning, 2014) suggested a reasonable
the responsible factors is of primary importance to our             correspondence (r = .73). Such feature-based semantic
goals. Thus, we trained our model on a small synthetic              representations were also employed in the original version
training set, aiming to create statistical regularities in the      of the model; this allows us to capture the influence of
training set that allowed us to run simulation experiments          semantic similarity over and above the influence of co-
containing manipulations corresponding to manipulations in          occurrence in language as implemented via the presented
empirical N400 experiments. We observe, based on these              sentences (enabling simulation of categorically related
simulation experiments, that variables or dimensions that           semantic incongruities; Sim. 1).
influence N400 amplitudes in the world influence semantic              After each presented word, the model is probed for each
surprise in our model in the same way, suggesting that N400         thematic role and each filler of each role-filler pair involved
amplitudes reflect semantic surprise.                               in the described event, and the model‚Äôs activation at the
                                                                    output layer is compared with the correct output. Error is
                                                                    then back-propagated through the entire network and
                                                                    connections are adjusted to minimize the difference between
                                                                    model-generated and correct output (we used cross-entropy
                                                                    error, a learning rate of 0.00005 and momentum of 0.9).
                                                                    Because the model is probed concerning the described event
                                                                    after every single presented word, it anticipates the meaning
                                                                    of each sentence as early as possible, so that the activation
                                                                    at the SG layer (and accordingly at the output layer in
                                                                    response to the presented probes) becomes tuned to the
           Fig. 1: The Sentence Gestalt (SG) model.                 regularities in the corpus. For example, the model learns that
                                                                    a sentence beginning ‚ÄúThe woman writes‚Ä¶‚Äù more often
   The model environment consists of sentences (presented           describes the woman writing an email than an sms, and
word by word at the input layer) such as ‚ÄòAt breakfast, the         encodes this regularity in the connection weights, resulting
man eats eggs‚Äô each paired with a corresponding event (a set        in probabilistic pre-activations of units in the SG layer
of role-filler pairs, e.g., agent ‚Äì the man), probabilistically     before email or sms appear in the sentence. Indeed the
generated online during training according to pre-specified         model‚Äôs connection weights capture the base-rate
constraints. After each presented word (represented by a            probabilities of the semantic features of each of the roles in
word-specific unit at the input layer), the model is probed         the sentence, so that when probed with a role prior to the
concerning the event described by the sentence. Responding          presentation of the first word of a sentence the pattern over
to a probe consists in completing a role-filler pair when           the filler units corresponds approximately to the overall
probed with either a thematic role (i.e., agent, action,            probability across the entire environment that the feature
patient, location, or situation; each represented by an             will be present in the filler of the probed-for role.
individual unit at the probe and output layer) or a filler of a        Since the minimum of the cross-entropy error is reached
thematic role (e.g., the man, to eat, the eggs, etc.). For the      when the network‚Äôs estimates of feature probabilities match
filler concepts, we used feature-based semantic                     the actual probabilities of those features, the change in the
representations that were handcrafted so that members of            network‚Äôs estimates occasioned by each successive word
the same semantic category shared some semantic features.           should match the change in these actual probabilities
For example, somewhat similar to the representations used           (Rumelhart et al., 1995). Treating the semantic feature
by Rogers and McClelland (2008), all living things shared a         probabilities as conditionally independent given the words
semantic feature (‚Äòcan grow‚Äô), all animals shared an                seen so far, this change in estimates of feature probabilities
additional semantic feature (‚Äòcan move‚Äô), all birds shared
                                                                2047

can be shown to correspond to the SemSn measure defined in          congruent continuations, t(9) = 6.30, p < .0001, in line with
the introduction.                                                   N400 amplitudes.
  Thus far we have described how changes in the activation             The model also successfully captures other typical N400
of semantic features in the model‚Äôs output layer should             effects in sentences such as cloze probability effects, with
correspond to the semantic surprise. However, we do not             larger N400 for low cloze as compared to high cloze
assume that these semantic feature activations are actually         probability sentence continuations (Sim. 2; Kutas &
computed during sentence processing. Instead, we propose            Hillyard, 1984), sentence position effects with decreased
that the pattern of activation over the SG layer (together          N400 amplitudes over the course of a sentence (Sim. 3; Van
with the connection weights in Part 2 of the model)                 Petten & Kutas, 1990), and influences of so-called semantic
implicitly represent this probability distribution in such a        illusions or reversal anomalies, i.e. only a very slight
way that the update at the SG layer mirrors the update in the       increase of N400 amplitudes in sentences such as ‚ÄòFor
actual probability distribution over features. We use the           breakfast, the eggs eat‚Ä¶‚Äô as compared to ‚ÄòFor breakfast, the
following cross-entropy measure to characterize this update:        boys eat‚Ä¶‚Äô while the increase in ‚ÄòFor breakfast, the boys
                                                                    plant‚Ä¶‚Äô is much larger (Sim. 4; Kuperberg et al., 2003; also
                  ùëé! ùëõ                             1 ‚àí ùëé! ùëõ         simulated by Brouwer, 2014, PhD thesis, see below). Even
      ùëé! ùëõ log             + 1 ‚àí ùëé! ùëõ log ¬†
                ùëé! ùëõ ‚àí 1                        ¬†1 ‚àí ùëé! ùëõ ‚àí 1       though the SG model is designed as a model of sentence
   !
                                                                    processing, word pairs and isolated words should be
  Here i ranges over all of the SG layer units, ai(n)               processed by the same system so that we also used the
represents the activation of unit i based on the current word       model to simulate N400 effects outside of sentence context.
and ai(n-1) represents the activation of unit i based on the        We describe the simulation of semantic priming in detail.
previous word. Similar results are obtained using the sum
over SG units of the absolute value of the difference               Sim. 5: Semantic priming. Bentin et al. (1985) observed
between ai(n) and ai(n-1).                                          smaller N400 amplitudes to target words presented after
                                                                    semantically related primes (i.e., primes from the same
                                                                    semantic category as the targets) as compared to unrelated
                        Simulations
                                                                    primes. To simulate these data, we presented the model with
Sim. 1: Categorically related semantic incongruities.               10 word pairs where the referenced concepts were members
Federmeier and Kutas (1999) presented sentence pairs such           of the same semantic category and thus shared semantic
as ‚ÄúThey wanted to make the hotel look more like a tropical         features at the output layer (e.g., monopoly ‚Äì chess) and 10
resort. So along the driveway they planted rows of‚Ä¶‚Äù and            word pairs where the primes and targets from the related
observed gradually increasing N400 amplitudes from                  pairs were re-assigned such that there was no semantic
congruent sentence continuations (‚Äúpalms‚Äù) to unexpected            similarity between prime and target. As shown in Fig. 2B,
continuations which were members of the same semantic               semantic surprise was smaller for targets after semantically
category as the expected continuation and thus shared               related as compared to unrelated primes, t(9) = 5.14, p <
semantic features (‚Äúpines‚Äù) to incongruent continuations.           .0001, in line with N400 amplitudes (Bentin et al., 1985).
   To simulate these data, we trained the model such that           The SG model additionally captures several other N400
one member of each semantic category (i.e., trees, drinks,          effects in word processing such as associative priming, with
etc.) was never presented in the same sentence context (i.e.,       smaller amplitudes to targets after associatively related (e.g.,
as a patient of the same action) as the other category              play ‚Äì chess) as compared to unrelated primes (Sim. 6;
members so that it was completely unexpected in that                Kutas & Hillyard, 1989), repetition priming, i.e. smaller
context. For the simulation experiment, we presented the            N400 amplitudes to immediately repeated words as
model with 10 such unexpected sentence continuations                compared to target words presented after unrelated primes
which were categorically related to the congruent                   (Sim. 7; Nagy & Rugg, 1989), and smaller amplitudes to
continuations, as well as 10 congruent continuations,               words of high as compared to low lexical frequency (Sim. 8;
presented with a probability of .8 during training when the         Van Petten & Kutas, 1990), captured through the encoding
specific combination of agent and action had been                   of base rate probabilities of features in the model‚Äôs
presented, and 10 incongruent sentence continuations which          connection weights.
were never presented as patients of the specific action                Finally, probability distributions can change and
during training and did not share any semantic features with        anticipatory preparedness to likely upcoming features
the congruent continuations.                                        depends on constant adaptation of represented probabilities
  As shown in Fig. 2A, semantic surprise induced by the             based on new experiences. In neural network models, this
critical words gradually increased from congruent                   adaptation is driven by the difference between expected and
continuations to unexpected continuations categorically             observed outcomes. Thus, if N400 amplitudes reflect this
related to the congruent continuations, t(9) = 5.23, p < .0001,     difference then larger N400 amplitudes should entail
and from those to unexpected continuations unrelated to the         enhanced adaptation. Simulation 9 focuses on this relation.
                                                                2048

   Fig. 2: Influences of (A) categorically related semantic incongruities, (B) semantic priming, and (C) repetition X congruity.
Sim. 9: Semantic incongruity and repetition. The                   small increase of N400 amplitudes in sentences such as ‚ÄòFor
influence of semantic incongruity on N400 amplitudes is            breakfast, the eggs eat‚Ä¶‚Äô (Kuperberg et al., 2003) to
reduced by repetition due to a stronger repetition-induced         indicate that the N400 component does not reflect semantic
reduction of amplitudes for incongruent continuations              integration but the retrieval of lexical information. He
(Besson et al., 1992). Repetition effects have been simulated      further suggests that semantic integration is linked to the
as consequences of connection adjustments induced by the           P600 component (which is increased in reversal anomalies).
prior presentations of the item within the experiment. We          Our account differs from Brouwer‚Äôs in that it specifies a
thus presented the 10 congruent and 10 incongruent                 single integrated representation of meaning which is
sentences from Sim. 1 twice while learning was operative           updated whenever a word is presented, with the extent of
(learning rate = 0.0005) so that the first presentation served     this update reflected in N400 amplitudes. When the word is
not only as an experimental condition but also produced            presented in a sentence, the update is seen as an update of an
connection adjustments.                                            integrated representation of the meaning of the sentence.
   As shown in Fig. 2C, the difference in semantic surprise        The integration process is relatively heuristic and may not
between the critical congruent vs. incongruent words was           accord with syntactic constraints in constructions such as
smaller during repetition. A rmANOVA confirmed a                   reversal anomalies. Indeed, analysis of our model‚Äôs output
significant interaction between repetition and congruity,          layer suggests that it experiences a ‚Äòsemantic illusion‚Äô in
F(1,9) = 87.18, p < .0001, reflecting a stronger influence of      that it continues to assign the eggs to the patient instead of
repetition for incongruent, F(1,9) = 82.35, p < .0001, Œ∑p¬≤ =       the agent role even after the word ‚Äòeat‚Äô, in line with the
.90, as compared to congruent, F(1,9) = 16.47, p = .003, Œ∑p¬≤       suggestion that language processing can be shallow
= .65, sentence continuations, in line with N400 data.             (Ferreira et al., 2002). Our model does not address the P600.
   These results nicely illustrate the intrinsic relationship      It is possible that the P600 effect in reversal anomalies
between semantic surprise and adaptation. However, there is        reflects a re-assignment of the eggs to an agent role.
a subtle issue with this simulation, namely that the output        Alternatively, the comprehender may continue to see the
layer which drives learning represents the events described        eggs as being eaten, with the P600 reflecting detection of a
by the sentences such that the simulation assumes these            syntax error. As a third possibility, the P600 may reflect
events to be observed while processing the sentences. This         conflict monitoring triggered by competing interpretations,
is not true for the empirical experiment. Thus, in a               one arising from a heuristic process and the other arising
prospective version of this simulation the error signal that       from a controlled process (van Herten et al., 2006). Further
drives learning should be derived from the difference              research seems required to better understand the P600.
between SG activation before and after the critical words.            As noted above, the SG representation together with the
                                                                   model‚Äôs weights latently predict the semantic features of
                         Discussion                                each role filler in the sentence based on prior constituents,
The goal of the present study was to investigate the               and the update of the SG due to the next constituent adjusts
functional basis of the N400 ERP component by relating             these latent predictions. Latent prediction in that sense
N400 amplitudes to a computational model of sentence               means that the SG model (and presumably the brain)
comprehension, the Sentence Gestalt (SG) model. Across a           becomes tuned through experience to be prepared to
series of simulations of N400 effects, we consistently             respond to likely upcoming semantic features with little
observed a correspondence between N400 amplitudes and              additional effort. This kind of latent prediction seems to
semantic surprise as represented by the magnitude of the           range from the pre-activation of specific semantic features
update of hidden unit activations that implicitly represent        in sentence context (e.g., for the categorically related
probability distributions over semantic features.                  incongruities, where less semantic update is necessary when
   N400 amplitudes have been previously linked to changes          an unexpected sentence continuation shares semantic
in lexical activation (Brouwer, 2014, PhD thesis; see also         features with an expected continuation; Sim. 1) to the latent
Rabovsky & McRae, 2014, for discussion, and Crocker et             structure in connection strengths and default activation that
al., 2010, for a model of the P600 component). Brouwer             leads to less semantic update when processing a high
(2014) focused on reversal anomalies and took the very             frequent as compared to a low frequent word in isolation
                                                               2049

(Sim. 8). This characterization should make clear that              Friston, K. (2005). A theory of cortical responses. Philos T
prediction in that sense does not refer to explicit intentional        R Soc B, 360, 815-836.
prediction of specific items but rather to a general                Hinton, G. E. (1989). Connectionist learning procedures.
configuration of the system optimized for upcoming                     Artif Intell, 40(1), 185-234.
semantic information. This entails that semantic activation         Itti, L. & Baldi, P. F. (2005). Bayesian surprise attracts
changes induced by new incoming input primarily reflect                human attention. In NIPS (Vol. 19), 547-554, Cambridge,
the discrepancy between probabilistically anticipated and              MA: MIT Press.
encountered features, i.e. semantic surprise, in accordance         Kuperberg, G.R., Sitnikova, T., Caplan, D., & Holcomb,
with predictive coding (Friston, 2005). In line with this              P.J. (2003). Electrophysiological distinctions in
view, N400 amplitudes in many paradigms appear to have                 processing conceptual relationships within simple
one thing in common, namely that they seem to be a                     sentences. Cogn Brain Res, 17, 117-129.
function of the fit between the semantic features that are          Kutas, M. & Federmeier, K. (2011). Thirty years and
implicitly expected based on previously experienced                    counting: Finding meaning in the N400 component of the
regularities and those activated by the current stimulus.              event-related brain potential (ERP). Annu Rev Psychol,
   In sum, the present study aimed to contribute to a better           62, 621-647.
understanding of the functional basis of the N400 ERP               Kutas, M. & Hillyard, S.A. (1980). Reading senseless
component by relating N400 amplitudes to an implemented                sentences: Brain potentials reflect semantic incongruity.
model of sentence comprehension. Across a series of                    Science, 207, 203-205.
simulations of N400 effects, we consistently observed a             Kutas, M. & Hillyard, S.A. (1984). Brain potentials during
correspondence between N400 amplitudes and the update of               reading reflect word expectancy and semantic association.
conditional probability distributions over semantic features.          Nature, 307, 161-163.
Besides demonstrating that the SG model naturally captures          Kutas, M. & Hillyard, S.A. (1989). An electrophysiological
electrophysiological indicators of internal cognitive                  probe of incidental semantic association. JoCN, 1, 38-49.
dynamics during language comprehension, these results are           Laszlo, S. & Armstrong, B. (2014). PSPs and ERPs:
in line with the idea that N400 amplitudes reflect semantic            Applying the dynamics of post-synaptic potentials to
surprise as the extent of change induced by an incoming                individual units in simulation of ERP reading data. Brain
stimulus in a probabilistic representation of meaning.                 Lang, 132, 22-27.
                                                                    Laszlo, S. & Plaut, D. (2012). A neurally plausible parallel
                    Acknowledgments                                    distributed processing model of event-related potential
Funded by the European Union‚Äôs Horizon 2020 programme,                 word reading data. Brain Lang, 120, 271-281.
Marie Sklodowska-Curie grant 658999 to Milena Rabovsky.             Levy, R. (2008). Expectation-based syntactic comp-
                                                                       rehension. Cognition, 106, 1126‚Äì1177.
                                                                    McClelland, J.L., St. John, M., & Taraban, R. (1989).
                          References                                   Sentence comprehension: A parallel distributed
Bentin, S., McCarthy, G., & Wood, C.C. (1985). Event-                  processing approach. Lang Cognitive Proc, 4, 287-335.
   related potentials, lexical decision and semantic priming.       Nagy, M.E. & Rugg, M.D. (1989). Modulation of event-
   Electroen Clin Neuro, 60, 343-355.                                  related potentials by word repetition: The effects of inter-
Besson, M., Kutas, M., & Van Petten, C. (1992). An event-              item lag. Psychophysiology, 26, 431-436.
   related potential (ERP) analysis of semantic congruity and       Pennington, J., Socher, R., & Manning, C.D. (2014). GloVe:
   repetition effects in sentences. JoCN, 4, 132-149.                  Global vectors for Word Representation. EMNLP.
Brouwer, H. (2014). The electrophysiology of language               Rabovsky, M. & McRae, K. (2014). Simulating N400
   comprehension: A neurocomputational model. PhD                      amplitudes as semantic network error: Insights from a
   thesis.                                                             feature-based connectionist attractor model of word
Crocker, M.W., Knoeferle, P., & Mayberry, M.R. (2010).                 meaning. Cognition, 132, 68-89.
   Situated sentence processing: The coordinated interplay          Rumelhart, D. E., Durbin, R., Golden, R., & Chauvin, Y.
   account and a neurobehavioral model. Brain Lang, 112,               (1995). Backpropagation: The basic theory. Backprop-
   189-201.                                                            agation: Theory, Architectures and Applications, 1-34.
Elman, J.L. (1990). Finding structure in time. Cognitive            Van Herten, M., Chwilla, D.J., & Kolk, H.H.J. (2006).
   Science, 14, 170-211.                                               When heuristics clash with parsing routines: ERP
Federmeier, K. & Kutas, M. (1999). A rose by any other                 evidence for conflict monitoring in sentence perception.
   name: Long-term memory structure and sentence                       JoCN, 18, 1181- 1197.
   processing. J Mem Lang, 41, 469-495.                             Rogers, T.T. & McClelland, J.L. (2008). Pr√©cis of semantic
Ferreira, F., Bailey, K.G.B., & Ferraro, V. (2002). Good-              cognition: A parallel distributed processing approach.
   enough representations in language comprehension.                   Behav Brain Sci, 31, 689-749.
   Current Directions in Psychological Science, 11, 11-15.          Van Petten, C. & Kutas, M. (1990). Interactions between
Frank. S.L., Otten, L.J., Galli, G., & Vigliocco, G. (2015).           sentence context and word frequency in event-related
   The ERP response to the amount of information conveyed              brain potentials. Mem Cognition, 18, 380-393.
   by words in sentences. Brain Lang, 140, 1-11.
                                                                2050

