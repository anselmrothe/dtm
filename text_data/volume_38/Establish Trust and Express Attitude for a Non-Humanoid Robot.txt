Establish Trust and Express Attitude for a Non-Humanoid Robot
Mei Si (sim@rpi.edu)
Department of Cognitive Science, Rensselaer Polytechnic Institute
Troy, NY 12180 USA

Joseph Dean McDaniel (mcdanj2@rpi.edu)
Department of Computer Science, Rensselaer Polytechnic Institute
Troy, NY 12180 USA
Abstract

to create a positive experience with trust and comfort. Furthermore, we want to have the capacity of creating different
“personalities for the robot. Here we do not necessarily need
to build an active or intriguing character, but rather to present
meaningful and consistent behavior patterns that the user can
infer from the robots actions for enhancing the believability of the robot as a human-like character. Breazeal studied
the requirements to “promote the illusion of a socially aware
robotic creature and found that to socially engage a human,
its behavior must address issues of believability such as conveying intentionality, promoting empathy, being expressive,
and displaying enough variability to appear unscripted while
remaining consistent” (Breazeal, 2000). Similar arguments
have been given for creating digital companions (Bickmore
& Rosalind, 2005), and for assistant robot. It has been found
that even though assistant robot does not need to represent a
social character, having a “personality helps the user to understand and predict its behaviors (Severinson-Eklundh, Kerstin
and Green, Anders and Httenrauch, Helge , 2003).
Both facial expressions and non-verbal behaviors play important roles in social interactions. In particular, it has been
shown that for human-like robots, showing appropriate nonverbal behaviors such as gaze, head nod, and gestures can
improve people’s performance in a collaborative task because
people can better understand the robot’s intention (Breazeal,
Kidd, Thomaz, Hoffman, & Berlin, 2005). Many robots used
by rescue teams, law enforcement, and the military are designed with functionality as a higher priority, and thus, they
do not appear humanlike and have limited means to support
natural human-robot interaction. While these robots are often
modified to have a facial display to express affect, researchers
have found that their body movements may play a more important role (Bethel & Murphy, 2008). Our modified Baxter
robot belongs partially to this category. We face the challenge
of the robot not having an exact human-like shape. Because
of the robot’s physical strength and size, potential negative
misinterpretations of the robot’s intentions will become particularly problematic.
In this work, we designed two experiments for studying
the expression of attitudes using the robot’s body movements.
In the first experiment, we examined how using humaninformed gestures and social dialogue, and having a digital
face can make people feel more comfortable to interact with
the robot. In the second experiment, we further investigated
expressing two different attitudes relaxed and proud using

In recent years, there has been an increasing interest in designing social robots to interact with people to provide therapy and
companionship. Most social robots currently being used are
light-weight and much smaller in size compared to people. In
this work, we investigate designing interactions for larger and
more physically capable robots as they have more potential to
assist people physically. A modified version of Baxter robot
was used, by sitting Baxter on top of an electronic wheelchair.
Two experiments were designed for studying the role of facial
expressions and body movements in establishing trust with the
user and for expressing attitudes. Our results suggest that the
robot is capable of expressing fine and distinguishable attitudes
(proud vs. relaxed) using its body language, and the coupling
between body movements and speech is essential for the robot
to be viewed as a person.
Keywords: Robot Human Interaction; Gesture; Trust

Introduction
In recent years, there has been an increasing interest in designing social robots to interact with people, as a tutor or
a companion. For example, Sony’s AIBO robotic dog has
been used for improving autistic children’s play, reasoning,
and affective skills (François, Powell, & Dautenhahn, 2009).
Autom is a humanoid robot, which is capable of establishing eye contact with the user and making small talk. It was
used for helping people keep track of their weight loss history (Kidd & Breazeal, 2008). Nao and Hanson robots have
also been widely used for research, tutoring, and entertainment purposes.
Most social robots currently being used are light-weight
and much smaller in size compared to people. This makes
them naturally look non-threatening. On the other hand, they
have limited movability and physical strength, which limits
their potential for physically interacting with and assisting
people. In this work, we want to investigate designing interactions for robots, which are larger and more physically
capable. We start this exploration using a modified version
of Baxter, a dual-arm robot by Rethink Robotics. The robot
by itself is 3’1” (93.98 cm) in height. With modification, the
robot sits on top of an electronic wheelchair. Its actual height
is similar to an adult sitting on a chair. The robot’s arm can
reach 41” (104 cm). Its body looks sturdy, with a weight of
165 lbs (75 kg).
To enable long-term natural interaction between robots
and users, we strive to not only rely on safe, dependable
robotic movements but also leverage existing research studying human-computer interaction and human social interaction

860

Guided by research in proxemics (Hall et al., 1968), we positioned the participants within the social zone of Baxter, between 4’ and 12’ away: specifically, 9’ (2.7 m) away. When
the participants approached Baxter to shake hands, they left
the social zone and entered the intimate zone. For safety reasons, the experimenter was in the same room during the interaction.
Two control groups were used: one in which participants
saw no arm movement during the routine (No Gestures, N =
9) and one in which participants saw arbitrary arm movements at the start of the routine, not tied to any particular behavior or gesture (Arbitrary Gestures, N = 8). Neither control
group saw a virtual face on Baxter. The first control group is
a baseline to compare against all other groups that add a form
of gesture to the routine. The second control group is used
to determine whether any arm movement elicits the same response as arm movements that are performed to couple with
the dialogue.
Two experimental groups were used: one in which participants saw meaningful gestures without a face (Meaningful
Gestures, N = 13), and one in which participants saw meaningful gestures accompanied by a virtual human face (Meaningful Gestures with Face, N = 13).
We adopted a between-group design, and the participants
were randomly assigned to one of four different groups. For
dependent variables, we measured a categorical dependent
variable, “hesitation,” which represents how much the participant hesitated before approaching Baxter when prompted
to shake hands. A score of “low” was given to the participants who waited less than one second to approach Baxter. A
score of “medium” was given to the participants who waited
between one and two seconds, and a score of “high” was
given when the participants waited more than two seconds
or looked to the experimenter for clarification that they could
approach Baxter if needed. This measurement was taken by
the experimenter.
For the participants’ subjective reports about the robot,
we used three dependent variables. During the interaction,
one of the questions asked by Baxter was how friendly it
was (“friendliness”). After the interaction, all participants
responded to two additional questions asked by the experimenter during the debriefing phase: how comfortable would
the participant be interacting with Baxter at a close distance
(“comfortability closeness”), and how comfortable would the
participant be letting Baxter touch them with one of its arms
(“comfortability touch”). Similar to answering Baxter’s questions, the participants answered these two questions using a
7-point Likert scale.

Figure 1: Baxter with Virtual Human Face
the robot’s body movements.

Experiment 1
In this experiment, we examined how placing Baxter’s movements into a social context with its dialogue affects people’s
perceptions of its safety and friendliness. We studied the importance of having meaningful gestures and a face display. In
particular, it is natural that people feel more comfortable interacting with a novel device after seeing how it operates or
moves. To study whether there is a difference between this
phenomenon and the effect of viewing the robot as a person,
we designed an experimental condition in which the robot’s
arm movements are not coupled with its speech and compared
the participants’ responses in this condition against other conditions.

Experiment Design and Subjects
Forty-three undergraduate students from Rensselaer Polytechnic Institute were recruited to participate. Their participation was compensated by giving course credits. The independent variable is the way the robot interacted with the
participants, which has four conditions.
Common to all the conditions, we designed an interaction
routine in which the robot talked about the research projects
it is involved with and its various capabilities. Baxter’s introductory statements consisted of fifteen sentences. Baxter then
asked the participants ten questions about his/her personal interests, e.g. what kind of music do they like (to which they
replied verbally on 7-point Likert scales) and commented on
the participants’ responses after all questions had been answered. The interaction with each participant lasted approximately five minutes. Afterward, Baxter prompted the participant to approach it and shake its right hand but assured
the participant that he/she could decline to shake hands if
he/she felt uncomfortable doing so. Then as the participant
approached, Baxter extended its arm towards him/her. Before this gesture, the participant had not physically touched
or been touched by the robot. Therefore, though extending
one’s arm towards the other person is natural for handshakes,
it could be perceived as an unexpected event from the robot.
We expected the participants to feel less apprehension – measured by how much hesitation they had to continue the handshake – if they treated the robot like another person.

Implementation
Figure 2 shows the system architect we used to create the
Baxter “character” in this experiment. The robot’s gestures
were created manually to complement each line of the dialogue, informed by human nonverbal behaviors, in particular, McNeill’s work on hand gestures (McNeill, 1992). Baxter uses deictic gestures such as pointing to the participant

861

Figure 3: Hesitation before Handshake
Figure 2: System Architect
or pointing to the general gesture space. Baxter uses iconic
gestures when talking about activities. For instance, Baxter
asks the participants, ”To what extent do you enjoy working on pieces of art, or writing?” and moves its left hand in
small circles as if it is writing on imaginary paper. It uses
metaphoric gestures when discussing concepts like creating
and expanding by bringing its hands together and then moving them apart. Finally, beat gestures are used with greetings
and exclamations to punctuate Baxter’s emotional intent.
We recorded Baxter’s arm movements using ROS “Robot
Operating System” and Robot Raconteur, a communication
library for robotic systems (Wason & Wen, 2011). A ROS
service records the joint positions of Baxter’s arms as we
manually manipulate the positions of the arms. We then systematically reduced the recording files down to keyframes to
reduce any jittery movement, and stored the result of interpolating joint positions between key frames to be used for
driving the robot’s movements during the interaction.
In the condition which included a virtual face on Baxter’s
display screen, we utilized the Virtual Human Toolkit (VHT)
(Hartholt et al., 2013). We used the face of a female character named Rachel, shown on Baxter as in Figure 1. During
the interaction, Rachel’s facial expressions remained mostly
neutral. Rachel’s voice was generated using Text-to-Speech.
We created a custom Windows Forms application written
in C#/.NET as a driver for VHuman and Baxter during the
interaction. It sent arm movement, speech, and facial expression commands to their respective destinations for each
segment of the routine. This system also allowed the experimenters to utilize a “Wizard of Oz -style approach. When
the participants were asked questions by Baxter, the experimenters could enter their answers into the application for
Baxter to comment on later in the routine.

Figure 4: Mean Ratings of “Comfortability Closeness”,
“Comfortability Touch”, and “Friendliness”

Of the standardized residual values, shown in Table 1, the
differences were significant at the 0.05 level between the
“Meaningful Gestures” condition and “No Gestures” and “Arbitrary Gestures” conditions for participants in the low and
medium hesitation categories. Overall, participants in the
“Meaningful Gestures” condition hesitated least in approaching Baxter, and participants in the control groups “No Gestures” and “Arbitrary Gestures” hesitated more. A comparison of hesitation measures of all conditions is shown in Figure 3.
One-way analysis of variance(ANOVA) tests were conducted to evaluate the differences between all groups on three
ordinal measures: comfortability interacting with Baxter at
a close distance (“comfortability closeness”), comfortability
letting Baxter use its arm to touch the participant (“comfortability touch”), and how friendly the robot seemed during the
routine (“friendliness”). A summary of the ANOVA tests can
be found in Table 2, and the mean scores for each condition
in all measures can be seen in Figure 4.
There was a significant difference among groups on how
comfortable the participants would be interacting with Baxter
at a close distance after having seen the routine, F(3, 39) =
3.154, p < 0.05. Post hoc comparisons using the Fisher LSD
test indicated that the mean score for the “No Gestures”
group (M = 4.67, SD = 1.12) was significantly lower than
the “Meaningful Gestures” group (M = 5.77, SD = 1.09),
p < 0.05. The mean score for the “Meaningful Gestures”
group was significantly higher than the “Meaningful Gestures
with Face” group (M = 4.38, SD = 1.33), p < 0.01.
Groups differed significantly with how comfortable the
participants would be letting Baxter touch them with one of
its arms after having seen the routine, F(3, 39) = 3.865, p <
0.05. Post hoc comparisons using Fisher LSD indicated that
the mean score for the “Meaningful Gestures” group (M =
5.46, SD = 0.88) differed significantly from the “No Ges-

Results
A chi-square test of independence was conducted to examine
the relation between the experiment condition and amount of
time that participants hesitated before approaching Baxter to
shake hands. The test showed a significant relationship between which version of the routine participants observed and
their amount of hesitation to approach Baxter, X 2 (6) = 15.46,
p = 0.02. No participants declined to shake hands with Baxter, but they differed significantly in the amount of time to
approach Baxter after the request.

862

Finally, the participants rated Baxter as being friendlier
when they saw meaningful gestures instead of arbitrary gestures or when the virtual face was not present. There are many
possible explanations for this result. The digital human like
face might introduce the uncanny valley effect. Moreover, because the rest of the robot’s body is not human like, the participants might experience a disconnection between the face
and the body. Combining with the fact that displaying the
virtual face did not help reduce the participants’ hesitations
to approach Baxter, this result suggests that we should use
caution when using a 3D realistic virtual face for this robot.

Table 1: Standardized Residual Values of Participants Hesitation in Experiment 1
Low Medium High
No Gestures
-1.5
1.8
-0.4
Arbitrary Gestures
-1.3
1.0
0.6
Meaningful Gestures
1.4
-0.9
-0.8
Meaningful Gestures w. Face
0.9
-1.4
0.6

Table 2: ANOVA Tests for Experiment 1
Measure
df MS
F
comfortability closeness 3
4.54 3.15
comfortability touch
3
4.51 3.86
friendliness
3
4.45 3.74

p
.035
.017
.019

Experiment 2
This experiment evaluates whether people can differentiate
and appropriately label the robot’s attitudes based on the
robot’s body movements. Unlike in Experiment 1, we utilized
the software CrazyTalk Animator by Reallusion to create the
facial animations. An original face was drawn as a boxy, outlined appearance that is much less realistic than the human
face in Experiment 1.

tures” group (M = 4.22, SD = 0.67), p = 0.01, and from the
“Arbitrary Gestures” group (M = 4.00, SD = 0.76), p < 0.01.
The final ANOVA test revealed a significant difference
among groups with how friendly participants perceived Baxter to be, F(3, 39) = 3.742, p < 0.05. Post hoc comparisons using Fisher LSD indicated that the mean score for the
“No Gestures” group (M = 5.67, SD = 1.00) was significantly
higher than the “Meaningful Gestures with Face” group (M =
4.69, SD = 1.18), p < 0.05. Additionally, the mean score for
the “Meaningful Gestures” group (M = 5.85, SD = 1.07) was
significantly higher than the mean scores for both the “Arbitrary Gestures” group (M = 4.63, SD = 1.06), p < 0.05, and
the “Meaningful Gestures with Face” group, p = 0.01.

Experiment Design and Subjects
We modeled two routines for Baxter: a Proud routine (sterner
and more bragging) and a Relaxed routine (more relaxing
and soothing). We picked these two attitudes because they
are likely to be useful in health care, and educational domains. Moreover, the difference between these two attitudes
is subtler than more commonly perceived emotions like happiness or anger, and therefore, the success of this experiment
will provide us more confidence in using the modified Baxter
robot to express emotions and attitudes in the future. Similar
to Experiment 1, in these routines, Baxter talked about student life on campus and the many things that the university
offers.
Twenty-four undergraduate students were recruited to participate. This time, we used a within-group design. The participants observed the dual-arm Baxter robot equipped with
a cartoonish face discuss a topic twice, using different nonverbal behaviors in each routine. Each participant saw both
routines in random order. Each routine lasted approximately
one and a half minutes. Baxter’s speech was controlled to be
similar between routines, with the main differences being the
displayed nonverbal behavior and facial expressions.
For each routine, the participants need to pick the most appropriate emotion/attitude label from a list. HUMAINE Emotion Annotation and Representation Language (EARL) consolidates emotion labels into ten categories (Schröder, Pirker,
& Lamolle, 2006). Pride and Relaxed belong to the “Positive Thoughts” and “Quiet Positive” categories respectively.
We also picked one relevant label from each other categories.
The ten options for the participants are anger, fear, relaxation,
frustration, sadness, shock, happiness, affection, pride, and
surprise.

Discussion
In this study, the participants were more likely to approach
Baxter without hesitation when they witnessed gestural arm
movement beforehand, with or without the addition of a virtual human face on Baxter’s display screen. Introducing irrelevant, arbitrary arm movement in the second control group
demonstrated either no impact (when comparing to the first
control group) or a negative impact (when compared to the
two experiment groups) on the participants’ hesitations, suggesting that meaningful arm movement was key in reducing
the participants’ hesitations. This confirms our hypothesis
that gestures associated with a social context are most effective in increasing people’s trust and feeling of comfort in interacting with the robot. We want to point out that the positive impact of accompanying speech with meaningful gestures took place after just a few minutes of interaction. This
provides strong evidence for the benefit of embodying a robot
as a social character.
Although the “Arbitrary Gestures” group reported a mean
score for “comfortability closeness” similar to the experimental groups, this behavior negatively impacted ratings of how
likely participants would be to let Baxter make physical contact with them. Participants in both control groups reported
lower scores for “comfortability touch” than participants in
the “Meaningful Gestures” group.

863

The test showed a significant difference for which emotions were chosen for each routine based on the Pearson ChiSquare value, X 2 (7) = 22.80, p = 0.002. Of the standardized
residual values, the differences were significant at the 0.05
level among the three labels – pride, affection, and anger.
Pride accounted for the largest amount of difference in the
distribution, with affection second and anger third.
Figure 6 shows the distribution of chosen emotions for
each routine among the twenty-four participants. The participants mostly matched the Proud routine to the intended
proud emotion at 41.70% (10 participants), with happiness
second at 20.80% (5 participants) and anger coming in third
at 16.70% (4 participants). For the Relaxed routine, participants mostly matched the routine to both happiness and affection at 29.20% (7 participants) each, with relaxation and
sadness at 16.70% (4 participants) each.

Figure 5: Baxter with CrazyTalk Cartoonish Face

Discussion
The spread of participant labels for Baxter’s emotional intent
was more even for the Relaxed routine than for the Proud
routine. In the Proud routine, the gestures were firmer and
more aggressive and were more tightly coupled with precise
moments in the dialogue, which may have contributed to this
result. In post-experiment interviews, many participants reported that Baxter spoke about itself and the university with
high praise and that the audience should agree with Baxter’s
comments – as opposed to the perceived inclusive, friendly
tone of the Relaxed routine. Also, being relaxed can often be
a secondary element of another perceived primary emotion
such as joy (Ekman & Friesen, 1967). Relaxed and calmness
is conveyed through slow, flowing movements, often associated with voice tone in ratio to the face and body (Ekman
& Friesen, 1967, 1969). Pride, on the other hand, can be
displayed more readily through use of nonverbal behavior.
Therefore, the relaxed attitude may have been too vague to be
successfully identified. In our previous work using a TurtleBot, we have observed that the noise made by the robot when
it is moving can intensify the expression of negative and high
dominance emotions such as anger, and affect the expression
of low dominance emotions such as shyness (Barron & Si,
2013). Though the participants did not specifically comment
on the noise as a factor affecting their impressions about the
robot, we suspect the noise played a similar role in this study,
i.e. lead some participants to believe the robot was angry.
We also observed an interesting ordering effect. The participants were more likely to see a greater contrast when they
saw the Relaxed routine first and then labeled the Proud routine with a negative emotion. Feedback from these participants mentioned that Baxter seemed less friendly in the second routine. Labels applied to the Relaxed routine were more
consistent regardless of the ordering of the routines.
Another interacting observation was that several participants mentioned hearing a change in the voice inflection between the two routines even though we used the same Text-toSpeech engine. Though not exactly the same as the McGurk
effect, this also demonstrates what people see can affect what

Figure 6: Emotion Labels for Each Routine

Implementation
As in Experiment 1, we recorded Baxter’s arm movements using ROS and Robot Raconteur (Wason & Wen, 2011). When
modeling pride, we want the robot to seem highly aroused
and actively engaging the participant during most of the routine (Ekman & Friesen, 1967). Therefore, in the Proud routine, gestures using Baxter’s arms were more focused toward
the observing human participant. For example, during the
initial introduction, Baxter says, “Hello, I am Baxter, one of
the amazing robots in the robotics department, raises its right
arm, and pats its torso in a highly precise, confident manner.
In contrast, the Relaxed routine was associated with positivity (Ekman & Friesen, 1967). Movements for the Relaxed
routine included sweeping the arms in slow arcs and keeping
the hands rotated inwardly toward Baxter’s body, with more
frequent pauses and less overall movement than the Proud
routine. For example, the initial greeting includes a small,
tentative wave of the hand.
Similarly as in Experiment 1, we identified keyframes for
each recorded file and interpolated the joint positions for creating smoother animations. Figure 5 shows Baxter with one
of the faces from CrazyTalk. This face has limited capacity of expressing emotions with subtle differences. Also, we
wanted to prevent the participants from forming their judgments mainly based on the robot’s face. Therefore, the facial
expressions remained mostly neutral for both routines. We
used a male Text-to-Speech voice for the robot.

Results
A chi-square test of independence was performed to examine
the relation between the designed routine and the attitude associated with it by participants. The labels shock and surprise
were not chosen by participants for either routine, so we do
not include them in the results.

864

they hear, even in such a brief and simple social interaction
scenario.

C: Applications and Reviews, IEEE Transactions on, 38(1),
83–92.
Bickmore, T., & Rosalind, J. (2005). Establishing and maintaining long-term human-computer relationships. ACM
Transactions on Computer-Human Interaction (TOCHI),
12(2), 293-327.
Breazeal, C. (2000). Sociable machines: Expressive social
exchange between humans and robots. Unpublished doctoral dissertation, Massachusetts Institute of Technology.
Breazeal, C., Kidd, C. D., Thomaz, A. L., Hoffman, G., &
Berlin, M. (2005). Effects of nonverbal communication
on efficiency and robustness in human-robot teamwork.
In Intelligent robots and systems, 2005.(iros 2005). 2005
ieee/rsj international conference on (pp. 708–713).
Cassell, J., Gill, A., & Tepper, P. (2007). Coordination in
conversation and rapport. In Proceedings of the workshop
on embodied language processing (p. 41-50).
Ekman, P., & Friesen, W. V. (1967). Head and body cues in
the judgment of emotion: A reformulation. Perceptual and
motor skills, 24(3), 711–724.
Ekman, P., & Friesen, W. V. (1969). The repertoire of nonverbal behavior: Categories, origins, usage, and coding. Semiotica, 1(1), 49–98.
François, D., Powell, S., & Dautenhahn, K. (2009). A longterm study of children with autism playing with a robotic
pet: Taking inspirations from non-directive play therapy to
encourage children’s proactivity and initiative-taking. Interaction Studies, 10(3), 324–373.
Hall, E. T., Birdwhistell, R. L., Bock, B., Bohannan, P.,
Diebold Jr, A. R., Durbin, M., . . . Vayda, A. P. (1968).
Proxemics [and comments and replies]. Current anthropology, 83–108.
Hartholt, A., Traum, D., Marsella, S. C., Shapiro, A., Stratou,
G., Leuski, A., . . . Gratch, J. (2013). All together now. In
Intelligent virtual agents (pp. 368–381).
Kidd, C. D., & Breazeal, C. (2008). Robots at home: Understanding long-term human-robot interaction. In Intelligent
robots and systems, 2008. iros 2008. ieee/rsj international
conference on (pp. 3230–3235).
McNeill, D. (1992). Hand and mind: What gestures reveal
about thought. University of Chicago Press.
Schröder, M., Pirker, H., & Lamolle, M. (2006). First suggestions for an emotion annotation and representation language. In Proceedings of lrec (Vol. 6, pp. 88–92).
Severinson-Eklundh, Kerstin and Green, Anders and Httenrauch, Helge . (2003). Social and collaborative aspects of
interaction with a service robot. (Tech. Rep.). Royal Institute of Technology (KTH).
Wason, J. D., & Wen, J. T. (2011). Robot raconteur: A
communication architecture and library for robotic and automation systems. In Automation science and engineering
(case), 2011 ieee conference on (pp. 761–766).

Conclusions and Future Work
In this work, we examined the effects of designing meaningful arm movements for a modified version of Baxter robot.
Our results suggest that it is feasible to create a social character using this robot even though it does not have an exact human-like shape. We demonstrated that designing body
movements that are timely coupled with the robot’s speech
makes people more likely to trust the robot and feel comfortable in its presence comparing to demonstrating the robot’s
arm movements independently from its speech or not demonstrating how the robot movements at all. Moreover, this difference takes place in just a few minutes. Finally, the Baxter
robot can use body language in a similar way as humans for
expressing distinguishable attitudes, more specifically, being
either proud or relaxed. However, its proud manner has the
danger of being misinterpreted as anger, and its relaxed attitude is very easily confused with other positive emotions such
as happy and affection.
Future work would further examine how using meaningful arm and face movements to express emotional intent affects people’s willingness to approach and interact with Baxter in close proximity and over a longer period. Right now
the robots body movements are planned independently from
the participants. In real life, when two conversational partners have established rapport, their body movements are often coupled (Cassell, Gill, & Tepper, 2007). For example, one
will often nod when the other pauses in his/her speech. One
of our future directions is to enable such interactions for the
robot. Secondly, we want to observe how people’s attitudes
toward the robot change over longer terms of interaction, and
at moments when the robot makes a seemingly threatening
action. In the latter case, we want to investigate whether the
same techniques people use for recovering trust can be applied to the robot. Finally, compared to digital characters,
when working with a physical robot, there is always the additional challenge of synchronizing the movements of its various parts, such as the two arms, or between the face and the
arms. Even with the same set of commands, it is possible
the movements become out of sync for a variety of reasons,
which may break the image of the robot as a social character. We are interested in developing a software framework
for helping with this challenge. When the robots body movements are delayed, we can also slightly delay its facial expressions and speech, or replan for another set of body movements, facial expressions, and speech.

References
Barron, M., & Si, M. (2013). Augment interactive storytelling with cognitive robot. In Proceedings of the 6th digital games research association (digra) conference.
Bethel, C. L., & Murphy, R. R. (2008). Survey of
non-facial/non-verbal affective expressions for appearanceconstrained robots. Systems, Man, and Cybernetics, Part

865

