                       Grounded Distributional Semantics for Abstract Words
                                         Katsumi Takano (katsumi@utm.inf.uec.ac.jp)
                                                 Akira Utsumi (utsumi@uec.ac.jp)
                                Department of Informatics, The University of Electro-Communications
                                         1-5-1, Chofugaoka, Chofushi, Tokyo 182-8585, Japan
                             Abstract                                  (2009) used perceptual features collected as featural norm
                                                                       to ground a language-based topic model on perceptual ex-
   Since Harnad (1990) pointed out the symbol grounding prob-
   lem, cognitive science research has demonstrated that ground-       perience, and demonstrated that the integrated model outper-
   ing in perceptual or sensorimotor experience is crucial to lan-     formed the language-based topic model. Although the featu-
   guage. Recent embodied cognition theories have argued that          ral norm used in their study is represented by language, recent
   language is more important for grounding abstract than con-         attempts have been directed toward multimodal distributional
   crete words; abstract words are grounded via language. Dis-
   tributional semantics has recently addressed the embodied na-       semantics (for a review, see Baroni, 2016), in which textual
   ture of language and proposed multimodal semantic models.           information is integrated with multimodal information com-
   However, these models are not cognitively plausible because         puted directly from nonlinguistic inputs such as visual (Bruni,
   they do not address the recent embodiment view of abstract
   concepts. Therefore, we propose a novel multimodal distribu-        Tran, & Baroni, 2014; Kiela, Hill, Korhonen, & Clark, 2014),
   tional semantics in which abstract words are represented indi-      auditory (Kiela & Clark, 2015), or olfactory (Kiela, Bulat, &
   rectly through grounded representations of their semantically       Clark, 2015) ones.
   related concrete words. A simulation experiment demonstrated           Their primary research purpose is to improve the accuracy
   that the proposed model achieved better performance in com-
   puting the word similarity than other multimodal or text-based      of distributional semantic models by using multimodal infor-
   distributional models. This finding suggests that the indirect      mation and it has often been demonstrated that multimodal
   embodiment view is plausible and contributes to the improve-        information can improve the performance of language-based
   ment of multimodal distributional semantics.
                                                                       distributional semantics. However, this is not always the case.
                                                                       Kiela et al. (2014) showed that, although the inclusion of vi-
                          Introduction                                 sual input improves the representations of concrete words,
Distributional semantics is a computational approach to con-           it degrades the representations of abstract words. This find-
structing semantic representations of words by observing dis-          ing is highly important, because it seems to suggest that ab-
tributional statistics of word occurrence in large collections         stract words may not be embodied in the same way as con-
of text. The lexical meaning of a word is represented by a             crete words are embodied. It has also been pointed out by
high-dimensional vector in a semantic space, and the degree            recent development of the embodied view of language (e.g.,
of semantic relatedness between any two words can be eas-              Louwerse, 2011; Borghi & Cangelosi, 2014; Thill, Padó, &
ily computed from their vectors, for example as the cosine             Ziemke, 2014). These views argue that abstract words are
of two vectors. Because of its simplicity and versatility, dis-        embodied differently from concrete words, and acquisition
tributional semantics has recently received enormous atten-            of abstract words depends more heavily on linguistic expe-
tion, and a variety of methods have been proposed and refined          rience. To provide a psychologically more plausible model,
(Turney & Pantel, 2010; Jones, Willits, & Dennis, 2015). Re-           distributional semantics must deal with the problem of ab-
cent advances of neural network language models or word                stract words, and at the same time, such a theory can provide
embedding models (Mikolov, Chen, Corrado, & Dean, 2013)                an effective computational framework for testing the validity
are also regarded as notable approaches of distributional se-          of the embodiment theory of language.
mantics.                                                                  In this paper, therefore, we propose a novel distributional
   However, distributional semantics has been criticized as            semantic model by taking into account the recent embod-
psychologically implausible because it is ungrounded or dis-           ied view of abstract concepts. We focus on visual images
embodied (Glenberg & Robertson, 2000). Embodied cog-                   as a source of perceptual information, as used in many other
nition theory argues that language is grounded in our sen-             studies on multimodal distributional semantics, and propose a
sory perception and experience, but distributional semantic            mechanism of embodiment of abstract concepts via language,
models learn from only linguistic information and do not               in which an embodied vector representation of an abstract
use sensorimotor information. Although it is controversial             word is obtained from the embodied representations of con-
whether distributional semantics cannot essentially simulate           crete words that are semantically related to that abstract word.
human semantic memory, it is no doubt that it cannot repre-            We then test the psychological plausibility of the proposed
sent the meaning of some kinds of words, in particular con-            method using human similarity ratings of English word pairs.
crete words, just as they are represented in human semantic
memory.                                                                           Embodiment of Abstract Words
   For this reason, recent studies of distributional seman-            Abstract concepts pose a serious challenge to the embodied
tics have addressed the use of sensorimotor or perceptual              theory of language. As Dove (2015) pointed out, it is difficult
information to learn psychologically plausible lexical rep-            to see how representations grounded in perceptual experience
resentation. For example, Andrews, Vigliocco, and Vinson               can capture the content of abstract concepts or words such as
                                                                   2171

truth and justice. Even if abstract concepts somewhat depend         to-world mapping procedure that combines linguistic obser-
on perceptual experience, the way in which such abstract con-        vations with cooccurring perceptual experience. From the
cepts are grounded seems to differ from concrete concepts.           distributional semantics side, Louwerse (2011) also put for-
This intuition has been supported by a number of empirical           ward the indirect embodiment view, namely the symbol in-
findings obtained in cognitive neuroscience. Neuroimaging            terdependency hypothesis, to refute a criticism against distri-
studies have demonstrated that processing of abstract con-           butional semantics made by the embodied view. According
cepts elicits greater activation of the left perisylvian language    to the symbol interdependency hypothesis, language compre-
network (such as the left inferior frontal gyrus and the left        hension is symbolic through interdependencies of amodal lin-
superior temporal cortex) as compared to processing of con-          guistic symbols, while it is indirectly embodied through the
crete concepts (e.g., Binder, Desai, Graves, & Conant, 2009;         references linguistic symbols make to perceptual representa-
Wang, Conder, Blitzer, & Shinkareva, 2010). Vigliocco et al.         tions. Hence, “language has evolved to become a commu-
(2014) also found greater engagement of the rostral anterior         nicative short-cut for language users and encodes relations in
cingulate cortex, an area associated with emotional process-         the world, including embodied relations (ibid., p.279).”
ing, for abstract than concepts words.                                  However, despite receiving much attention from cognitive
   Some embodied theories claim a general mechanism of               science over the recent years, the indirect embodiment view
embodiment common to both concrete and abstract concepts.            of abstract words has not been addressed by computational
For example, Barsalou’s (1999) theory of perceptual symbol           studies on distributional semantics, as pointed out in the intro-
systems argues that concepts (and meanings as well) are not          duction. Although multimodal distributional semantics incor-
amodal, formal symbols, but rather inherently modal, per-            porates the embodied view of language into an algorithm for
ceptual symbols, which refer to perceptual states (or neural         constructing word vectors, it cannot deal with the problem of
representations) in sensorimotor systems. These perceptual           abstract words. Kiela et al.’s (2014) approach clearly shows
symbols become integrated into a simulator for a concept that        this difficulty; to improve the accuracy of multimodal word
enables us to simulate sensorimotor experience of perceiv-           vectors, they excluded visual information when constructing
ing and/or acting on exemplars of that concept. According            vectors of abstract words, while including visual information
to Barsalou (1999), abstract concepts are represented in the         for concrete words. For distributional semantics to be a psy-
same perceptual symbol systems, although the simulator of            chologically more plausible model, it is highly necessary to
abstract concepts is constructed from not only sensorimotor          integrate the indirect embodiment view into multimodal dis-
but also introspective states or events. Barsalou (1999) ex-         tributional semantics. This is what we aim to accomplish in
plains this by using examples such as truth and negation, but        this paper.
the explanation is relatively vague and limited to some logical
or affective concepts.                                                        Grounded Distributional Semantics
   Recently, a number of studies have attempted to resolve the       Our grounded distributional semantic model, whose algo-
problem of abstract concepts while maintaining the embod-            rithm is shown in Figure 1, is based on the indirect embod-
ied view by emphasizing the role of language in processing           iment view that grounding of abstract words is mediated by
of abstract concepts. For example, Word-as-Tools (WAT) the-          language. Therefore, different grounding methods are used
ory (Borghi & Cangelosi, 2014) claims that words function as         for concrete and abstract words. For a concrete word, its vec-
tools to extend our cognition and such the linguistic function       tor representation is constructed by combining a text-based
is more crucial for abstract than for concrete words represen-       vector (computed from a text corpus) with a visual vector
tation. Dove (2014) argues for a similar view that language          computed from images for that word (i.e., Steps 3(a) and 4
provides an important means of extending our cognitive or            of Figure 1). This method does not differ from Kiela et al.’s
mental capabilities by enabling access to an embodied repre-         (2014) and other standard methods for multimodal distribu-
sentational system that exists independently of language. The        tional semantics. On the other hand, the vector representation
basic idea underlying these views is that abstract words are         of an abstract word is constructed by combining a text-based
grounded in sensorimotor or perceptual experiences, but the          vector with visual vectors of concrete words semantically re-
embodiment is indirect, rather than direct in the case of con-       lated to that abstract word (i.e., Steps 3(b) and 4).
crete words. This “indirect embodiment” view is proposed                An important part of this algorithm is the classification be-
more clearly by Thill et al. (2014). They propose a “division        tween concrete and abstract words at Step 2. In this paper,
of labor” approach between a perceptual layer that associates        we use the image dispersion method proposed by Kiela et al.
basic, concrete concepts with perceptual features and a rela-        (2014). This method is motivated by the intuition that the
tional layer that grounds more complex and abstract concepts         diversity of images retrieved for a particular word depends
in relation to basic concepts. Gleitman et al.’s (2005) expla-       on its concreteness. It is generally expected that images for
nation of how “hard words” are acquired is closely related to        concrete concepts such as “apple” are more similar to one
the indirect embodiment view. In the early stage of lexical ac-      another than those for abstract concepts such as “happiness.”
quisition, the meaning of concrete words is acquired directly        The degree of image dispersion for a concept is defined as the
from perceptual information via word-to-world pairing. In            average pairwise cosine similarity between all the vector rep-
the later stage, the meaning of hard words, which is not eas-        resentations in the set of images for that concept. All words
ily accessible through perception, is acquired by a structure-       are sorted in descending order of image dispersion, and top
                                                                 2172

1. Construct a linguistic semantic space DSML from a text            353 (Finkelstein et al., 2001) and SimLex-999 (Hill, Re-
     corpus.                                                         ichart, & Korhonen, 2015), which are well-known gold-
2. Divide the vocabulary V into concrete words VC and ab-            standards widely used in the computational linguistics com-
     stract words VA (i.e., V = VC ∪VA and VC ∩VA = ϕ ).             munity. These data contain English word pairs (353 pairs
                                                                     in WordSim-353 and 999 pairs in SimLex-999) with human
3. Construct a visual semantic space DSMV using visual im-
                                                                     similarity ratings. The similarity ratings in WordSim-353 are
     ages.                                                           regarded as measures of semantic relatedness (or in other
  (a) For a concrete word wCi ∈ VC , compute a visual vector         words, associative similarity), while the ratings in SimLex-
        y(wCi ) directly from the images labeled with wCi .          999 reflect semantic similarity (or taxonomic similarity). For
  (b) For an abstract word wAi ∈ VA , compute a visual vector        example, a pair of clothes – closet is rated 8.00 in WordSim-
                                                                     353, but 1.96 in SimLex-999. These two words are related
        y(wAi ) as ∑wC ∈SN(wA ) y(wCj ) where SN(wAi ) ⊂ VC is a
                       j     i                                       because the word “closet” refers to a small room or cabinet
        set of n semantic neighbors of (i.e., n concrete words       for hanging “clothes,” but their meanings are not similar.
        semantically related to) wAi . Semantic neighbors of wAi
        are computed in the linguistic semantic space DSML .         Multimodal Semantic Space
4. Construct a grounded semantic space DSMG by combin-               We generated a text-based semantic space (i.e., DSML in Fig-
                                                                     ure 1) from the British National Corpus with 100M word to-
     ing DSML and DSMV as follows. For each word wi ∈ V ,
                                                                     kens as follows. First, we constructed a word-cooccurrence
     normalize x(wi ) ∈ DSML and y(wi ) ∈ DSMV , and con-
                                                                     matrix with 43,528 different words (and thus the dimensions
     catenate them.
                                                                     of the matrix were 43,528 × 43,528). The cooccurrence fre-
Figure 1: Overall algorithm of the proposed distributional se-       quency was counted within a sentence. Second, the word-
mantic model.                                                        cooccurrence matrix was weighted by positive pointwise mu-
                                                                     tual information (PPMI), which generally achieves better per-
100pabs (0 < pabs < 1) percent are judged to be abstract and         formance (Bullinaria & Levy, 2007). Finally, the weighted
constitute VA . Kiela et al. (2014) confirmed that the degree        matrix was smoothed by singular value decomposition (SVD)
of image dispersion is a valid measure for concreteness by           and the dimension of row word vectors was reduced to 300.
showing a high correlation between concreteness ratings and             To construct a visual semantic space (i.e., DSMV in Fig-
dispersion values. Furthermore, as compared to concreteness          ure 1), we collected 30 images for each word using Flickr
rating data, image dispersion is more appropriate for judg-          image retrieval, from which a visual vector for that word was
ing the importance of embodiment in the acquisition of word          constructed. As a method for visual vector computation, we
meanings, because the consistency among images for a word            used the well-known Bag of Visual Words (BoVW) approach
is likely to imply that this word refers to a discernible object     (Sivic & Zisserman, 2003). BoVW computes a vector rep-
or action in the world that we can easily recognize through          resentation for each image by clustering into visual words
perceptual experience.                                               a set of local descriptors obtained from the image dataset,
   Determining semantic neighbors (or in other words, “me-           and then by computing a histogram of visual word counts. In
diator words”) of an abstract word is also a key step for suc-       this paper, we extracted SURF descriptors (Bay, Tuytelaars,
cessful modeling. We apply a simple method that chooses n            & Van Gool, 2006) using a dense keypoint sampling method.
concrete words most similar to a target abstract word. How-          Each local descriptor is represented by a 3 × 128-dimensional
ever, this simple method may not work well in some cases.            vector, because a SURF feature is normally represented in a
Some highly abstract words (e.g., truth, wisdom) may not             128-dimensional vector and computed for each of the three
have semantically similar concrete words, but this method            components of HSV. All the SURF feature vectors were clus-
forces such abstract words to be represented by visual vec-          tered into 300 visual words, and each image is represented by
tors of n concrete words, which may be relatively more simi-         a 300-dimensional histogram vector of visual words. Finally,
lar than other concrete words but absolutely dissimilar to the       a visual vector for each word is computed by summing the
abstract words. Hence, we consider another method, in which          visual vectors of all the images retrieved by that word, and
N(> n) semantic neighbors of an abstract word are selected           then by weighting the summed vector by PPMI.
from the whole vocabulary V , and then n most concrete words
                                                                     Method
are selected from N semantic neighbors according to the de-
gree of image dispersion. If n concrete words cannot be se-          According to the proposed method shown in Figure 1, we
lected (i.e., N semantic neighbors contain less than n concrete      constructed several grounded semantic spaces by changing
words), no visual vector is attached to that abstract word in        the values of the parameters pabs (from 0.10 to 0.95 in steps
the same way as Kiela et al.’s (2014) method.                        of 0.05), n (from 1 to 5), and N (from 10 to 50 in steps of
                                                                     10, 100, and all). To evaluate the relative performance of
                   Evaluation Experiment                             the proposed model, we also generated the standard multi-
                                                                     modal representations and Kiela et al.’s (2014) filtering-based
Test Data                                                            representations. In the standard method, every word is rep-
As test data for evaluating distributional semantic mod-             resented by concatenating its linguistic vector with a visual
els, we used two word similarity norms, namely WordSim-              vector computed directly from images of that word. In Kiela
                                                                 2173

Table 1: Correlation coefficients between the cosine similarity computed by the distributional semantic models and the pairwise
similarity ratings of two datasets. For the proposed model, the highest correlation is shown boldfaced for each of the two
datasets. For the other models (including the proposed model with N =all), results obtained with the same parameters as the
proposed model are selectively shown.
                     Model                                                         WordSim-353 SimLex-999
                     Text-based DSML                                                   0.525            0.248
                     Visual DSMV , Standard                                            0.227            0.118
                     Visual DSMV , Proposed (pabs = 0.50, n = 2, N =all)               0.257            0.112
                     Visual DSMV , Proposed (pabs = 0.95, n = 2, N =all)               0.185            0.085
                     Grounded DSMG , Standard                                          0.476            0.235
                     Grounded DSMG , Kiela et al. (2014) (pabs = 0.50)                 0.510            0.262
                     Grounded DSMG , Kiela et al. (2014) (pabs = 0.95)                 0.532            0.245
                     Grounded DSMG , Proposed (pabs = 0.50, n = 2, N =all)             0.478            0.220
                     Grounded DSMG , Proposed (pabs = 0.95, n = 2, N =all)             0.390            0.179
                     Grounded DSMG , Proposed (pabs = 0.95, n = 2, N = 30)             0.551            0.260
                     Grounded DSMG , Proposed (pabs = 0.80, n = 5, N = 30)             0.535            0.278
                     Note. pabs = the proportion (i.e., threshold) of abstract words; n = the number of se-
                     mantic neighbors (i.e., mediator words) from which visual vectors of abstract words are
                     computed; N = the size of population from which n mediator words are chosen.
et al.’s (2014) filtering-based method, concrete words, which        bodiment of abstract concepts. Hence, the observed better
are determined by image dispersion, are represented in the           performance of our model against Kiela et al.’s (2014) model
same way as the standard method (i.e., concatenation of their        demonstrates the plausibility of our refined representations
linguistic and visual vectors), while abstract words are repre-      and the indirect embodiment view.
sented only by their linguistic vectors.                                On the other hand, the proposed method yielded worse per-
   The performance of each semantic space was measured by            formance when all abstract words are represented using the
Pearson’s correlation coefficient between the computed co-           visual vectors of their semantic neighbors (i.e., in the case
sine values and the semantic similarity ratings in the test data.    of N = all in Table 1). This result implies that indirect em-
If the proposed semantic space can improve the performance           bodiment via language is not always effective; Kiela et al.’s
as compared to the standard multimodal and Kiela et al.’s            (2014) method of not including visual information works bet-
(2014) semantic spaces as well as to the text-based seman-           ter in some cases. In other words, some abstract words (e.g.,
tic space, it would demonstrate that the indirect embodiment         truth, wisdom) need not to be grounded in perceptual experi-
view of abstract words is supported computationally.                 ence and their meanings may be able to be captured primarily
                                                                     by linguistic information.
Result
                                                                        The proposed grounded model achieved the best perfor-
Table 1 shows the correlation coefficients between the cosine        mance at very high pabs (pabs = 0.95 for WordSim-353 and
similarity computed by the distributional semantic models            pabs = 0.80 for SimLex-999). This result indicates that only
and the similarity ratings of the test data. Overall, the pro-       a relatively small number of words need to be grounded di-
posed grounded semantic space achieved better performance            rectly in perceptual information; most words are likely to
on both datasets than the text-based model, standard multi-          be grounded via language or need not to be grounded. It
modal model, and Kiela et al.’s (2014) model. This result            may also implies that word concreteness does not necessar-
indicates that the proposed model is made psychologically            ily determine the necessity of grounding in perceptual ex-
more plausible by language-mediated visual representations           perience. Table 2 lists some example words that are com-
of abstract words, thus providing computational evidence for         puted as concrete and abstract by the proposed model. Some
the indirect embodiment view of abstract words.                      highly concrete words (e.g., baby, computer, seafood) whose
   The result that the standard multimodal model did not out-        concreteness ratings (1 to 5) (Brysbaert, Warriner, & Kuper-
perform the text-based model is consistent with Kiela et al.’s       man, 2014) are higher than 4.5 and many moderately con-
(2014) result; this indicates that inclusion of perceptual input     crete words (e.g., currency, interview, summer) whose con-
does not always improve the quality of lexical meaning repre-        creteness ratings are higher than 3.0 are classified as abstract.
sentations. To overcome this difficulty, Kiela et al. (2014) did     Nevertheless, indirect grounding is effective in representing
not use perceptual information to represent image-dispersed          these words. The proposed model improved the accuracy of
words, but we use language-mediated perceptual information           similarity computation for word pairs in WordSim-353 such
for those words, in accordance with the recent idea of em-           as governor – interview, currency–market, summer–drought,
                                                                 2174

                                                                    and then many other words are learned through the knowl-
Table 2: Examples of concrete and abstract words in                 edge of easy words, leading to a rapid vocabulary growth.
WordSim-353 classified by the proposed DSMG (pabs = 0.95,
n = 2, N = 30). For an abstract word, its mediator words                                        Discussion
(i.e., concrete words used for computing its visual vectors)
are also shown in parentheses. Note that no mediator words          In this paper, inspired by the recent view of the embodiment
are shown for some abstract words because N semantic neigh-         of abstract words, we have proposed a novel approach to mul-
bors of those words do not include n concrete words.                timodal distributional semantics. In the proposed method, the
                                                                    grounded visual representation of an abstract word is learned
   Concrete words in VC                                             through semantically related mediator words, rather than di-
       animal, bread, car, carnivore, collection, food, for-        rectly from images for that word as used in the standard mul-
       est, impartiality, jaguar, mammal, marathon, market,         timodal distributional semantics. The plausibility of the pro-
       moon, office, shower, television, tiger, water, zoo          posed method is tested and justified by comparing the cosine
   Abstract words in VA                                             similarity computed by distributional semantics with human
       activity, alcohol, astronomer (lunar, saturn), baby,         semantic similarity ratings. This finding can be interpreted as
       bird, book (collection, print), center, chance, chil-        supporting the indirect embodiment view of abstract words.
       dren (playgroup, whist), cognition (representational,           However, our model is not so sophisticated and rather pre-
       intentionality), computer, currency (ecu, lira), dis-        liminary. It did not achieve a statistically significant improve-
       covery, drought (soil, rainforest), experience, fam-         ment over the existing multimodal semantic models. We can
       ily (home, bear), flood (stream, storm), fruit (bean,        point out several issues to improve the quality of the model.
       salad), girl, governor (governorship, appoint), index,       One important issue to address for future research is a method
       interview (quote, television), king, law, liquid (water,     for choosing mediator words whose visual vectors form the
       bubble), love, marriage, media (magazine, television),       grounded representation of an abstract word (at Step 3(b) of
       museum (collection, painting), music, psychology,            Figure 1). In this paper, we simply apply semantic similarity
                                                                    computed by the text-based model as a measure of choosing
       reason, seafood (salad, pasta), sprint (marathon, ath-
                                                                    mediator words, but a number of empirical studies have sug-
       lete), summer (garden, lake), treatment, word
                                                                    gested that some other factors affect the embodiment of ab-
                                                                    stract words. For example, emotion (or affect) is suggested to
  summer:                                                           play an important role in processing abstract words (Kousta,
                                                                    Vigliocco, Vinson, Andrews, & Del Campo, 2011; Vigliocco
                                                                    et al., 2014). Affective experience is crucial in the ground-
  garden:                                                           ing of abstract concepts, and thus it is likely that affectively
                                                                    similar words are effective mediators of embodiment for ab-
                                                                    stract words. Age-of-acquisition may also be related to the
  lake:                                                             embodiment of abstract words, because basic words learned
                                                                    at the early stage of lexical acquisition are represented primar-
                                                                    ily perceptually, while other words learned at the later stage
                                                                    are acquired through the knowledge of basic words (Gleitman
Figure 2: Examples of images for an indirectly grounded             et al., 2005; Thill et al., 2014).
word summer and its mediator words garden and lake used                Another important research topic is the use of motor in-
in our multimodal distributional semantic model.                    formation for distributional semantics. Multimodal semantic
                                                                    spaces learned from static images are less than grounded in
and seafood–food, as compared to the text-based model and           motor experience. To overcome this difficulty, we can con-
the standard multimodal model. For example, as shown in             sider distributional semantics using feature vectors extracted
Figure 2, a variety of images associated with the word sum-         from videos or continuous image sequences.
mer seem not to represent one discernible concept, and thus            Further refinement can be achieved in the proposed model
they are somewhat harmful for an appropriate visual repre-          if we consider different ways of indirect grounding accord-
sentation of summer. On the other hand, images for the me-          ing to the degree of abstractness for words. For example, the
diator words garden and lake are highly consistent, and these       number of mediator words can be determined depending on
visual information can represent some aspects of summer.            the abstractness of words. We can also combine an original
    Hence, it may be inappropriate that we refer to the par-        visual vector directly computed from the images for an ab-
titioning of vocabulary (VA and VC at Step 2 of Figure 1) in        stract word with those for mediator words if the original vec-
terms of abstractness. Rather, what this partitioning really        tor is somewhat useful for representing the meaning of the
reflects is the necessity of grounding in perceptual experi-        abstract word.
ence. Note that this finding is consistent with the existing           Through these improvements, we can obtain a psycholog-
findings on lexical acquisition (e.g., Gleitman et al., 2005).      ically more plausible distributional model, and multimodal
“Easy” words (mostly nouns), which express concrete basic-          distributional semantics would hold promise as the computa-
level concepts, are learned at the early stage of word learning,    tional basis for a psychologically testable theory of embodied
                                                                2175

cognition and symbol grounding.                                     Hill, F., Reichart, R., & Korhonen, A. (2015). SimLex-999:
                                                                       Evaluating semantic models with (genuine) similarity es-
                   Acknowledgments                                     timation. Computational Linguistics, 41(4), 665–695.
This research was supported by JSPS KAKENHI Grant Num-              Jones, M. N., Willits, J., & Dennis, S. (2015). Mod-
bers JP15H02713, JP15K12045, and SCAT Research Grant.                  els of semantic memory. In J. R. Busemeyer, Z. Wang,
                                                                       J. T. Townsend, & A. Eidels (Eds.), Oxford handbook
                         References                                    of mathematical and computational psychology (pp. 232–
Andrews, M., Vigliocco, G., & Vinson, D. (2009). Integrat-             254). New York, NY: Oxford University Press.
   ing experiential and distributional data to learn semantic       Kiela, D., Bulat, L., & Clark, S. (2015). Grounding seman-
   representations. Psychological Review, 116, 463–498.                tics in olfactory perception. In Proceedings of the 53rd
Baroni, M. (2016). Grounding distributional semantics in the           annual meeting of the association for computational lin-
   visual world. Linguistic Issues in Language Technologies,           guistics and the 7th international joint conference on nat-
   10(1), 3–13.                                                        ural language processing (pp. 231–236).
Barsalou, L. (1999). Perceptual symbol systems. Behavioral          Kiela, D., & Clark, S. (2015). Multi- and cross-modal seman-
   and Brain Sciences, 22, 577–660.                                    tics beyond vision: Grounding in auditory perception. In
Bay, H., Tuytelaars, T., & Van Gool, L. (2006). SURF:                  Proceedings of the 2015 conference on empirical methods
   Speeded up robust features. In A. Leonardis, H. Bischof,            in natural language processing (emnlp2015) (pp. 2461–
   & A. Pinz (Eds.), Computer vision — ECCV 2006 (pp.                  2470).
   404–417). Springer.                                              Kiela, D., Hill, F., Korhonen, A., & Clark, S. (2014). Im-
Binder, J. R., Desai, R. H., Graves, W. W., & Conant, L. L.            proving multi-modal representations using image disper-
   (2009). Where is the semantic system? a critical review             sion: Why less is sometimes more. In Proceedings of the
   and meta-analysis of 120 functional neuroimaging stud-              52nd annual meeting of the association for computational
   ies. Cerebral Cortex, 19, 2767–2796.                                linguistics (pp. 835–841).
Borghi, A. M., & Cangelosi, A. (2014). Action and language          Kousta, S.-T., Vigliocco, G., Vinson, D. P., Andrews, M., &
   integration: From humans to cognitive robots. Topics in             Del Campo, E. (2011). The representation of abstract
   Cognitive Science, 6, 344–358.                                      words: Why emotion matters. Journal of Experimental
Bruni, E., Tran, N. K., & Baroni, M. (2014). Multimodal                Psychology: General, 140(1), 14–34.
   distributional semantics. Jounral of Artificial Intelligence     Louwerse, M. M. (2011). Symbol interdependency in sym-
   Research, 49, 1–47.                                                 bolic and embodied cognition. Topics in Cognitive Sci-
Brysbaert, M., Warriner, A. B., & Kuperman, V. (2014). Con-            ence, 3, 273–302.
   creteness ratings for 40 thousand generally known English        Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Ef-
   word lemmas. Behavior Research Methods, 46, 904–991.                ficient estimation of word representations in vector space.
Bullinaria, J. A., & Levy, J. P. (2007). Extracting seman-             In Proceedings of workshop at the international confer-
   tic representations from word co-occurrence statistics: A           ence on learning representation (iclr).
   computational study. Behavior Research Methods, 39(3),           Sivic, J., & Zisserman, A. (2003). Video Google: A text re-
   510–526.                                                            trieval approach to object matching in videos. In Proceed-
Dove, G. (2014). Thinking in words: Language as an em-                 ings of ieee international conference on computer vision
   bodied medium of thought. Topics in Cognitive Science,              (ICCV 2003), volume 2 (pp. 1470–1477).
   6, 371–389.                                                      Thill, S., Padó, S., & Ziemke, T. (2014). On the importance of
Dove, G. (2015). Three symbol ungrounding problems: Ab-                a rich embodiment in the grounding of concepts: Perspec-
   stract concepts and the future of embodied cognition. Psy-          tives from embodied cognitive science and computational
   chonomic Bulletin & Review, Online First Articles.                  linguistics. Topics in Cognitive Science, 6, 545–558.
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan,    Turney, P. D., & Pantel, P. (2010). From frequency to mean-
   Z., Wolfman, G., & Ruppin, E. (2001). Placeing search in            ing: Vector space models of semantics. Journal of Artifi-
   context: The concept revisited. In Proceedings of the 10th          cial Intelligence Research, 37, 141–188.
   international conference on world wide web (pp. 406–             Vigliocco, G., Kousta, S.-T., Rosa, P. A. D., Vinson, D. P.,
   414).                                                               Tettamanti, M., Devlin, J. T., & Cappa, S. F. (2014). The
Gleitman, L. R., Cassidy, K., Nappa, R., Papafragou, A., &             neural representation of abstract words: The role of emo-
   Trueswell, J. C. (2005). Hard words. Language Learning              tion. Cerebral Cortex, 24, 1767–1777.
   and Development, 1(1), 23–64.                                    Wang, J., Conder, J. A., Blitzer, D. N., & Shinkareva, S. V.
Glenberg, A., & Robertson, D. (2000). Symbol grounding                 (2010). Neural representation of abstract and concrete
   and meaning: A comparison of high-dimensional and em-               concepts: A meta-analysis of neuroimaging studies. Hu-
   bodied theories of meaning. Journal of Memory and Lan-              man Brain Mapping, 31, 1459–1468.
   guage, 43, 379–401.
Harnad, S. (1990). The symbol grounding problem. Physica
   D, 42, 335–346.
                                                                2176

