         Comparing Predictive and Co-occurrence Based Models of Lexical Semantics
                                               Trained on Child-directed Speech
          Fatemeh Torabi Asr                                       Jon A. Willits                              Michael N. Jones
      Cognitive Science Program                            Department of Psychology                   Psychological and Brain Sciences
  Indiana University, Bloomington                     University of California, Riverside             Indiana University, Bloomington
          fatorabi@indiana.edu                                  jon.willits@ucr.edu                         jonesmn@indiana.edu
                                Abstract                                      reinforcement learning, and probabilistic inference (see
   Distributional Semantic Models have been successful at                     Jones, Willits, & Dennis, 2015, for a review).
   predicting many semantic behaviors. The aim of this paper is                    The majority of distributional models in the cognitive
   to compare two major classes of these models ‚Äì co-                         science literature are from the family of co-occurrence
   occurrence-based models, and prediction error-driven models
   ‚Äì in learning semantic categories from child-directed speech.              models. Models of this family tend to apply unsupervised
   Co-occurrence models have gained more attention in                         learning mechanisms to a frequency count of how often
   cognitive research, while research from computational                      words co-occur with each other in a context (paragraph,
   linguistics on big datasets has found more success with                    document, or an n-word moving window). There are many
   prediction-based models. We explore differences between                    specific models that differ in theoretically meaningful ways
   these types of lexical semantic models (as representatives of              in terms of the learning mechanisms they apply, but all
   Hebbian       vs.     reinforcement      learning      mechanisms,
                                                                              members of this family share the assumption that the learner
   respectively) within a more cognitively relevant context: the
   acquisition of semantic categories (e.g., apple and orange as              is basically counting observed co-occurrences of stimuli in
   fruit vs. soap and shampoo as bathroom items) from linguistic              the environment. Hence, they are all based on error-less
   data available to children. We found that models that perform              Hebbian-type learning mechanisms.
   some form of abstraction outperform those that do not, and                      Perhaps the best-known co-occurrence model in the
   that co-occurrence-based abstraction models performed the                  cognitive literature is Latent Semantic Analysis (LSA;
   best. However, different models excel at different categories,             Landauer & Dumais, 1997). LSA basically applies a
   providing evidence for complementary learning systems.
                                                                              dimensionality reduction mechanism to a sparse word-by-
                                                                              document frequency matrix computed from a large corpus
                            Introduction                                      of text. The resulting dense vectors emphasize higher-order
Distributional models of lexical semantics have had a large                   statistical relationships among words: Two words that occur
impact on cognitive science over the past two decades. In                     in similar contexts across language will have similar
general, these models formalize the distributional hypothesis                 representations, even if they never directly co-occurred in a
(Firth, 1957; Harris, 1970), and attempt to learn distributed                 context. LSA vectors have been used to model how words‚Äô
representations for word meanings from statistical                            semantic similarity predicts performance in vocabulary
regularities across a large corpus of linguistic input. The                   acquisition, categorization, reading, summarization, and a
resulting representations have been enormously valuable to                    number of other cognitive tasks (see Landauer, 2006, for a
researchers wanting to select and calibrate word stimuli                      review). Other popular models count the co-occurrences of
balanced on semantic dimensions. They have also been                          words within a moving window, or how frequently words
successfully used as semantic representations in models of                    occur with predefined context words. For a review of the
cognitive processes (e.g., word recognition, reading), and in                 various co-occurrence algorithms and their performance on
a wide variety of applications ranging from automated                         semantic tasks, we refer the reader to Bullinaria and Levy
tutoring to open question answering.                                          (2007) or Riordan and Jones (2011).
      Due in part to their practical successes, the algorithms                     A second family of predictive distributional models has
that distributional models use to build semantic                              been around for decades, based on principles of predictive
representations have also been hypothesized to be related to                  encoding and error-driven learning core to theories of
the cognitive mechanisms potentially used by humans to                        reinforcement learning. For example, early recurrent neural
learn semantic representations from regularities in their                     networks studied by Elman (1990) and St. John and
language input1. The various learning mechanisms posited                      McClelland (1990) learn distributed representations for a
include simple co-occurrence learning, episodic abstraction,                  word‚Äôs co-occurrence history across their hidden layers.
                                                                              Feedforward networks studied by Rogers and McClelland
                                                                              (2004) similarly learn distributed semantic representations,
   1
     To be fair, the transfer between practical algorithm and human
                                                                              representations that even contain hierarchical taxonomic
mechanism has been bidirectional: Knowledge of how humans learn was           relations. These model architectures rely on predicting a
used to inform the design of distributional algorithms, and the subsequent    distributed set of features for each input word, then deriving
success of certain algorithms on practical tasks has then fed back to help    an error signal from the difference between the prediction
narrow the range of likely cognitive mechanisms that humans use.
                                                                          1092

and the observed context. Errors are backpropagated                 important to note that current tests of W2V have been on
through the network to increase the likelihood that the             very large corpora‚Äîthe tests by Mikolov et al. (2013) were
correct output will be predicted given the input in the future.     trained on over 1-billion words of text, and the comparisons
     The mechanisms for predictive learning core to these           by Baroni et al. (2014) were trained on a corpus of almost 3-
models have been studied a great deal within reinforcement          billion words. While estimates of the number of tokens a
learning, and there is a considerable literature in cognitive       human will read/hear in a lifetime vary greatly, both of
neuroscience exploring how this type of learning is driven at       those corpora are orders of magnitude beyond the upper
the neural level by dopaminergic error signals, and is              limit of a single human‚Äôs experience.
moderated by the basal ganglia, cingulate, and                           Hence, our approach here is to scale the problem way
hippocampus. In practice, these predictive models have only         down. We train co-occurrence and predictive models on the
been applied to small toy datasets to discover distributional       real-world speech that children experience from birth to age
structure; their capabilities have not been studied when            5 using the CHILDES corpus (MacWhinney, 1998), a
applied to large, naturalistic corpus-based materials such as       resource used in previous work on computational modeling
those that are used by simple co-occurrence models.                 of early word categorization (e.g., Asr et al., 2013). This
However, there has been a recent resurge of interest in             linguistic experience is a very different test for the models,
predictive models of distributional semantics. Howard et al.,       and allows us to explore how their learning mechanisms
(2011) trained a predictive version of the Temporal Context         might deal with the noisy data on which children must build
Model (pTCM), a recurrent model of error-driven                     their semantic representations. Given the superiority of
hippocampal learning, on a large text corpus and                    W2V over co-occurrence models on large data and practical
demonstrate impressive performance on word association              semantic tasks, and the similarity of its learning algorithm to
tasks. Similarly, Shaoul et al. (2016) used a Na√Øve                 popular error-driven models of development, how do the
Discrimination Learning (NDL) procedure (a single-layer             two families of models compare on their ability to learn
network trained using the Rescorla-Wagner learning                  complex structure on the same impoverished data that
procedure) to show that semantic representations can be             children receive?
learned from a relatively small sample of spoken language.
Despite the many appealing properties of predictive                                    Co-occurrence Models
distributional models, the bottom line is that they are             To evaluate the performance of co-occurrence based
heavily outperformed by co-occurrence models at                     models, two models were selected that have both performed
accounting for human data on every lexical semantic task            well in previous evaluations, but that use different learning
that has been tested.                                               algorithms (thus providing breadth of coverage of different
     A new type of context-prediction model (a.k.a. ‚Äúneural         types of co-occurrence based models). One model uses an
embedding model‚Äù) has emerged in the past few years; as             abstraction mechanism, whereas the other operates with
Baroni et al. (2014) put it, these models are ‚Äúthe new kids         simple summation of surface-level word co-occurrences.
on the distributional semantics block.‚Äù The predictive model        PCA-Based Vector Model
of Mikolov et al. (2013), referred to in the literature as          The first model tested is notable for its use of principle
word2vec (W2V), is a feedforward neural network model               components analysis (PCA) as the primary method of
with a hidden layer that uses error backpropagation to              knowledge abstraction. This model computed co-
maximize the likelihood of either predicting context given a        occurrences in a 12-word moving window (12 words in both
word, or predicting a word given the context. In this sense,        the forward and backwards directions) for the 10,000 most
W2V behaves much like the networks studied by Rogers                frequent words in the corpus, resulting in a 10,000-by-
and McClelland (2004), but applied to massive amount of             10,000 co-occurrence count matrix. These values were then
linguistic data, and with some tricks to improve training           normalized into positive point-wise mutual information
efficiency. W2V has made a huge stir in the machine                 values (Bullinaria & Levy, 2007). This matrix was then
learning literature for its ability to outperform every other       reduced using PCA, and the first 30 principle components
semantic model on benchmark tasks, and to achieve this              were retained, resulting in 30-element vectors for each of
impressive performance using an architecture that had been          the 10,000 words. This model is composite of several pre-
written off in the cognitive and linguistic literatures. Baroni     existing models, such as the HAL model (Lund & Burgess,
et al. (2014) undertook a careful comparison of state-of-the-       1996), the COALS model (Rohde et al., 2005) and models
art co-occurrence models and W2V, testing them on the               by Bullinaria & Levy (2007).
same input corpus and with a large battery of different
                                                                    Sparse Random Vector Accumulator
semantic tasks. They concluded that the hype surrounding
                                                                    The second co-occurrence model we tested was from the
W2V is warranted: Even under these very well controlled
                                                                    family of Random Vector Accumulators (RVAs; see Jones,
comparisons, W2V outperformed the current top performers
                                                                    et al., 2015, for a review). RVAs are essentially distributed
studied by Bullinaria and Levy (2007). Since W2V has a
                                                                    count models. They initialize a unique random vector for
similar architecture to many ‚Äútoy‚Äù connectionist models that
                                                                    each word prior to learning, and the word‚Äôs memory vector
have been popular in cognitive science, its success on
                                                                    is then updated across learning as the sum of the vectors
practical tasks is exciting to the field. However, it is
                                                                    representing the words with which it has co-occurred.
                                                                1093

Hence, RVAs are incremental learners with no abstraction          between target words and its cues (i.e. the other words in the
mechanism (compared to PCA). We use a sparse-distributed          window) that predict it. In addition to a long history of use
RVA here (Recchia et al., 2015). To equate with the other         in the psychology of learning (see Miller et al., 1995, for a
models, the RVA was trained on the same 10k x 10k word            review), NDL models are now being explored as models of
co-occurrence matrix as was the PCA model. Each word              infant word segmentation and how children learn the
was initially represented by an 8,000-element sparse ternary      meanings of words (Baayen et al., 2015).
environment (E) vector. The memory (M) vector was then
the frequency-weighted sum of the E vectors for all the                                         Method
words with which the target word co-occurred within a 12-         Corpus
word context window. For example, if dog had a co-                We used the entire child-directed speech data in the
occurrence frequency of 3, 1, 0, 2, with cat, shoe, bunny,        American English subset of the CHILDES corpus
and run, respectively, then Mdog = 3*Ecat + 1*Eshoe + 2*Erun.     (MacWhinney,            1998).    This        collection       includes
                                                                  conversations between children (4 to 60 months of age) and
                     Predictive Models                            their parents, care-givers and other children. Utterances
To compare to the RVA and PCA co-occurrence models, we            directed to the target children were combined to create a
tested two error-driven models. The primary objective was         corpus representative of the linguistic input of children from
to evaluate the performance of the W2V predictive learning        these ages. The resulting corpus consisted of 4,568 sub-
algorithm, but we also tested a second model of the error-        corpora (transcribed documents), containing 36,170 distinct
correction family based on the classic Rescorla-Wagner            word types and 8,323,266 total word tokens. The corpus is
model of discrimination learning.                                 relatively well-distributed across ages and generally forms a
W2V                                                               decent snapshot of the input children get at various ages.
As described above, W2V is a multilayer neural network            Little pre-processing was done to the corpora beyond simple
(with an input layer, an output layer, and one hidden layer)      word tokenization. To equate comparisons across the
that learns word vectors by iterating through sample              models, from this corpus, we selected the 10,000 most
contexts. W2V comes with two slightly different                   frequently occurring words and used only those words as
architectures of a neural network for learning word               inputs into the four models. Words below this rank were
embeddings: (1) cbow (context bag-of-words) and (2)               excluded due to their low frequency in the corpus (<7).
skipgram. In the cbow architecture, a word is predicted as        Evaluation Task
the output from its context input. During training, the input     We evaluated the performance of the models based on a
layer produces a weighted sum over the context words              word categorization task. For this task, we used 1,244 high
within a fixed adjacency window of the target word. Output        frequency nouns from the corpus that unambiguously
activations are converted to a probability distribution over      belonged to a set of 30 categories (like mammal, clothing,
the vocabulary (softmax) and the weight matrices are              etc.). Thus, each non-identity pair of words either belonged
updated through backpropagation of the errors. The network        to the same category (as in dog-cat for mammals and shoe-
topology is similar for skipgram, except that in this             sock for clothing) or to different categories (as in dog-shoe,
architecture, a target word is used to predict the context in     dog-sock, cat-shoe, and cat-sock).
which it appears. In skipgram, several context vectors can             The category membership of each pair was predicted
be sampled from a certain window of adjacent words (e.g.,         using each model‚Äôs similarity score in a signal detection
given the input sentence ‚Äúshe found a cute cat in the             framework. For each word pair, the similarity score was
garden‚Äù, the target word ‚Äúcat‚Äù can be used to predict context     compared to a decision threshold. If the similarity score was
unigrams ‚Äúcute‚Äù, ‚Äúin‚Äù, ‚Äúa‚Äù and ‚Äúthe‚Äù when a window of size        above that threshold, the pair was predicted to belong to the
two (from each side) is considered. After training the model      same category (classified a ‚Äúhit‚Äù if this prediction was
on a text corpus, the weight matrix between the hidden layer      correct and a ‚Äúfalse alarm‚Äù if this prediction was wrong). If
and the output layer in cbow or the one between the input         the similarity score was below the threshold, the pair was
layer and the hidden layer in skipgram represents the             predicted to belong to different categories (classified a
embeddings for the vocabulary words: V * N, where V is            ‚Äúcorrect rejection‚Äù if this prediction was correct and a
the size of vocabulary and N is the dimension of word             ‚Äúmiss‚Äù if this prediction was wrong). For each model, a
vectors. In our experiments, we used a python                     single decision threshold was chosen, the threshold that
implementation of the W2V model from Gensim (≈òeh≈Ø≈ôek              maximized categorization accuracy for that model. Each
& Sojka, 2010).                                                   model‚Äôs overall performance was assessed by computing
Na√Øve Discrimination Learner                                      balanced accuracy (BA) using the formula below:
Like W2V, the Na√Øve Discrimination Learning (NDL)                            1         ‚Ñéùëñùë°ùë†               ùëêùëúùëüùëüùëíùëêùë° ùëüùëíùëóùëíùëêùë°ùëñùëúùëõùë†
model learns to make predictions about a target word given              ùêµùê¥ =
                                                                             2
                                                                               ‚àó(
                                                                                  ‚Ñéùëñùë°ùë† + ùëöùëñùë†ùë†ùëíùë†
                                                                                                +
                                                                                                  ùëêùëúùëüùëüùëíùëêùë° ùëüùëíùëóùëíùëêùë°ùëñùëúùëõùë† + ùëìùëéùëôùë†ùëí ùëéùëôùëéùëüùëöùë†
                                                                                                                                    )
its lexical context, or vice-versa. Unlike W2V, NDL has
only an input layer and an output layer (with no hidden                                     Experiment 1
layer), and uses the Rescorla-Wagner learning procedure           Our first experiment focused on exploring the parameter
(Rescorla & Wagner, 1972) to learn a set of weights               space of the W2V model to see how an error-driven
                                                              1094

distributional model learns from the specific characteristics          The second observation concerned the effect of vector
of child-directed speech data: short sentences, simple             dimensionality. While adding nodes to the hidden layer
structure, less ambiguity, etc.                                    increased the categorization accuracy in lower scales (i.e.,
                                                                   from 30 to 50) the effect disappeared in higher scales (100
W2V Setup                                                          to 300). In fact, the best performing model overall is the
The skipgram architecture is known to perform better on            skipgram with 200 hidden layer nodes (accuracy = 74.9%),
smaller corpora and modeling rare words due to repeated            not the one with 300. This can be due to the small and
sampling from a fixed window of context words, whereas             relatively less ambiguous vocabulary in the child-directed
cbow is trained faster, thus is more suitable for learning         corpus, for which a smaller hidden layer would suffice and
word embeddings from big text corpora. We tried both               might generalize better to capture similarities between
architectures in this experiment to see if this held true in our   words within the coarse-grained categorization defined in
task. We also manipulated two other parameters in the              our evaluation task.
model: the window size for collecting context words, and                                        1.00
the hidden layer size, i.e., dimensionality of the resulting                                              v30   v50     v100   v200   v300
                                                                                                0.95
word embeddings. We examined window sizes of 2 and 12
                                                                      Classification Accuracy
                                                                                                0.90
words from each side of the target word. Previous                                               0.85
experiments showed that context words in a smaller window
                                                                                                0.80
convey syntactic information about the target word,
                                                                                                0.75
whereas, context words in a larger window convey more
                                                                                                0.70
topical information (Levy & Goldberg, 2014a). Vector
                                                                                                0.65
dimensionality (size of hidden layer) was set to 30, 50, 100,
                                                                                                0.60
200 or 300 nodes. Larger hidden layers provide a finer-
                                                                                                0.55
grained distributional representation, and thus can be
                                                                                                0.50
beneficial in learning word similarities to perform a                                                  cbow     skipgram       cbow      skipgram
categorization task. In contrast, smaller hidden layer sizes
                                                                                                       w=2        w=2          w=12          w=12
force the network to develop more abstract representations         Figure 1. Mean classification accuracy (averaging across the 30
of meaning. Frequency cut-off was set to 1. Other                  semantic categories, error bars represent 95% CI) of the various
parameters in the W2V training function were held constant         W2V models, as a function of the window size (2 vs. 12), the size
across the experiments (for a complete description of the          of the hidden layer vector (30, 50, 100, 200, 300), and architecture
default parameter values please check the Gensim package).         (cbow, skipgram).
Results
Figure 1 shows the accuracy of the model with different
parameter considerations in the categorization task. As
expected, the skipgram architecture learned similarity
between the words better than the cbow architecture,
resulting in superior categorization performance.
Interestingly, considering a larger context window size
(changing from 2 to 12 context words from each side of the
target word) did not enhance the performance of the cbow
model significantly. On the other hand, the skipgram model
did benefit from more context words. When compared
against the cbow results, this suggests that the size of our
corpus is small for the connectionist model to learn the
similarities between the words in our categorization dataset.
That is why sampling smaller n-grams from a bigger                 Figure 2. Mean classification accuracy on the 30 different
                                                                   categories, comparing the best performing cbow model to the best
window of adjacent words (the mechanism used only in the
                                                                   performing skipgram model (in both cases, w=12, v=200).
skipgram model) results in a significant boost in the
backend performance. This observation is in line with              The best performing W2V setup will be used in our next
previous experiments on large corpora (Mikolov et al. 2013)        experiment in comparison to other models. Thus before we
where the skipgram model outperformed cbow in semantic             proceed, it is good to take a closer look at the learning of
association tasks. The cbow model, on the other hand,              each word category by either of the architectures. Figure 2,
recognizes syntactic associations (plural/singular) slightly       compares the best cbow and skipgram models on every
better. This can be due to the way every training instance is      category within the evaluation dataset. First, the two models
used in either model: in skip-gram, unigrams are sampled           seem to be learning qualitatively different types of
from the context window and used one-by-one together with          information: cbow captured the within-category similarity
the target word in several training steps, whereas cbow uses       of furniture and household items relatively better than it did
the entire context frame of a word at once.                        on dessert, fruit, vegetable and meat items. In contrast,
                                                               1095

household and furniture were among the less accurate            context windows (thus its peak performance is that of it
categories when detected by the skipgram model. Both            being trained on 3 context words from each side).
models performed very well in categorization of numbers,                                     1.00
days and months, but had difficulty with other categories
                                                                   Classificaiton Accuracy
                                                                                             0.90
such as plants. This suggests that the distributional                                               0.816
properties of different semantic categories might be                                         0.80           0.749
different. A correlation analysis of system performance                                                             0.676
                                                                                                                            0.667
across categories with token frequency, word types and                                       0.70
entropy of the category items revealed that cbow learned                                     0.60
categories with larger token frequency better (p<0.05),
while the performance of skipgram on a category was                                          0.50
statistically independent of these factors.                                                         PCA     W2V     RVA     NDL
                                                                Figure 3. Mean classification accuracy (averaging across the 30
                     Experiment 2                               different categories and 95% CI) for the four DSMs.
In the second experiment we compare different
distributional models in the word categorization task.
Setup of the Models
We trained each model with its optimal parameter settings.
As we found in Experiment 1, for W2V this meant using the
skipgram architecture with a window size of 12 and 200
nodes for the hidden layer. For comparison, the optimal
performing NDL model had a window size of 3, and like
W2V, performed better predicting contexts from words
rather than vice-versa (Shaoul et al., 2016). NDL has no
hidden layer, and therefore is not comparable in terms of
vector size. Since the co-occurrence models do not use
error-driven learning, direction (word predicting context vs.
context predicting word) was not a feature of these models.
However, these models were both trained using a window          Figure 4. Mean classification accuracy on the 30 different
size of 12 (making them comparable to the better                categories for the four DSMs.
performance of W2V). The PCA model has a parameter                   Finally, a comparison between the overall performances
equivalent to the number of hidden units in W2V (i.e. the       of the best co-occurrence model (PCA) and the best
amount of abstraction that is used): the number of principle    predictive mode (W2V) in this task revealed the superiority
components retained. The peak performance of this model         of the co-occurrence model. PCA includes a relatively
was obtained with 30 principle components. The RVA              sophisticated post-processing of the raw co-occurrence
model also has a parameter, which is the size of the random     vectors, whereas W2V involves an error-based learning.
environment vectors. The peak performance of the RVA            Inputs to the both models were very similar, that is the co-
model was obtained with a random vector size of 8000.           occurrence information was collected from a window size of
Results                                                         12 from each side of a target word. As we mentioned above,
Figure 3 compares the performance of each of the four           the number of principal components in the PCA was a good
models in the word categorization task. The two co-             equivalent to the size of the hidden layer in the W2V. If we
occurrence models, PCA and RVA performed very                   compare it against the skipgram with 30 nodes in the hidden
differently. The PCA model had the advantage of learning        layer (in our previous experiment), we find an even bigger
latent relationships in the dataset, which likely helped it     difference in the performance of the two models.
perform better on a small dataset such as the CHILDES                How models differ from one another qualitatively?
corpus. Between the two predictive models, W2V was              Figure 4 provides an answer to this question by showing
superior to the NDL. As with the PCA model, W2V‚Äôs               performance on each category separately. The models
hidden layer allows it learn abstract, latent relationships     showed some variance in terms of what categories they
from the co-occurrence data. In machine learning,               learned better and what categories they learn worse. The
introducing a hidden layer to a neural network topology is      less accurate models (NDL and RVA) tended to align fairly
considered as a method for capturing nonlinear correlations     highly, doing relatively well and poorly on the same
between the input and output of the model. In addition to       categories, and showing no significant difference in overall
NDL missing the hidden layer, the input to this model was       performance (t(29) = 1.48, p = 0.15). W2V outperformed
different from that of the W2V skipgram used in this            both NDL and RVA, with a significantly higher overall
experiment. In fact, like the cbow model tested in our          performance (t(29) = 4.20, p < 0.001), and beating NDL on
previous experiment, NDL did not benefit from larger            28/30 categories and RVA on 27/30 categories. The PCA
                                                                model performed the best, having a significantly higher
                                                            1096

overall performance than all three models (all p‚Äôs < .0001),                                      References
and beating W2V on 30/30 categories, RVA on 29/30                  Asr, F. T., Fazly, A. and Azimifar, Z. (2013). From cues to categories: a
categories, and NDL on 30/30 categories. Accuracy of a                computational study of children‚Äôs early word categorization. Cognitive
                                                                      Aspects of Computational Language Acquisition. (pp. 81-103).
category did not correlate with its types or token frequency.
                                                                   Baayen, R. H., Shaoul, C., Willits, J. and Ramscar, M. (2015).
                    General Discussion                                Comprehension without segmentation: A proof of concept with naive
                                                                      discrimination learning. Language, Cognition, and Neuroscience.
     Our analyses compared two models that involved the            Baroni, M., Dinu, G., & Kruszewski, G. (2014). Don't count, predict! A
abstraction of latent representations (PCA and W2V) versus            systematic comparison of context-counting vs. context-predicting
two models that do not perform abstraction (RVA and                   semantic vectors. In ACL (pp. 238-247).
                                                                    Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic representations
NDL) when trained on child-directed speech. Both models
                                                                      from word co-occurrence statistics: A computational study. Behavior
that perform abstraction strongly outperformed those that do          Research Methods, 39, 510-526.
not. Our analyses also compared two co-occurrence models           Elman, J.L. (1990). Finding structure in time. Cognitive Science, 14, 179-
(PCA and RVA) with two predictive models (W2V and                     211.
                                                                   Firth, J.R. (1957). A synopsis of linguistic theory 1930-1955. In Studies in
NDL). In previous work, W2V has been demonstrated to
                                                                      Linguistic Analysis, (pp. 1-32).
surpass standard co-occurrence models when trained on               Harris, Z. (1970). Distributional structure. In Papers in Structural and
large amounts of data (e.g., Baroni et al., 2014; Mikolov,            Transformational Linguistics (pp. 775‚Äì794).
2013). However, we found an interesting paradox: W2V               Howard, M. W., Shankar, K. H., & Jagadisan, U. K. (2011). Constructing
was actually outperformed by a rather simple PCA-based                semantic representations from a gradually changing representation of
                                                                      temporal context. Topics in Cognitive Science, 3, 48-73.
co-occurrence model when applied to child-directed speech          Jones, M. N., Kintsch, W., & Mewhort, D. J. K. (2006). High-dimensional
and with a classification of children‚Äôs concepts. It is               semantic space accounts of priming. Journal of Memory and Language,
important to note that our PCA model was also in the list of          55, 534-552.
                                                                    Jones, M. N., Willits, J., Dennis, S., & Jones, M. (2015). Models of
models in Baroni et al. that were compared to W2V. Hence,
                                                                      semantic memory. Oxford Handbook of Mathematical and
the story isn‚Äôt as simple as saying one model is ‚Äúbetter‚Äù than        Computational Psychology, 232-254
the other. Despite the equivalence of the objective function       Landauer, T. K. (2006). Latent semantic analysis. Encyclopedia of
optimization in W2V to the matrix factorization process of            Cognitive Science.
                                                                   Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato's problem:
the PCA model (as pointed out in Levy & Goldberg 2014b),
                                                                      The latent semantic analysis theory of acquisition, induction, and
W2V involves an incremental error-driven learning process.            representation of knowledge. Psychological review, 104(2), 211.
It operates very well and efficiently with large amounts of        Levy, O. and Goldberg, Y., (2014a). Dependency-Based Word
data; also might be considered as a better simulation of the          Embeddings. In ACL (pp. 302-308).
cognitive processes. On the other hand, our experiments            Levy, O., & Goldberg, Y. (2014b). Neural word embedding as implicit
                                                                      matrix factorization. In Advances in Neural Information Processing
show that PCA performs better on small and sparse                     Systems (pp. 2177-2185).
linguistic data that children learn from.                          Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic
     Future work needs to focus on correlating the models‚Äô            spaces from lexical co-occurrence. Behavior Research Methods,
                                                                      Instruments, & Computers, 28(2), 203-208.
predictions with children‚Äôs response data. In addition, our
                                                                   MacWhinney, B. (2000). The CHILDES project: The database (Vol. 2).
target words were all from the same syntactic category             Miller, R. R., Barnet, R. C., and Grahame, N. J. (1995). Assessment of the
(nouns). Different distributional model architectures have            rescorla-wagner model. Psychological Bulletin, 117(3). 363-386.
been shown to vary in the type of information they learn           Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S. and Dean, J. (2013).
                                                                      Efficient estimation of word representations in vector space. In ICLR.
best (e.g., syntactic vs. semantic association; Mikolov et al.,
                                                                   O'Reilly, R. C. (1998). Six principles for biologically based computational
2013, or different types of semantic relations; Baroni et al.,        models of cortical cognition. Trends in cognitive sciences, 2, 455-462.
2014; Jones, Kintsch, & Mewhort, 2006). Our classification         Recchia, G. L., Sahlgren, M., Kanerva, P., & Jones, M. N. (2015).
task was one that required the models to learn paradigmatic           Encoding sequential information in vector space models of semantics:
similarity well. Interestingly, in the survey of Baroni et al.        Comparing holographic reduced representation and random permutation.
                                                                      Computational Intelligence & Neuroscience.
(2014), paradigmatic tasks were among the cases where              Rescorla, R. A. and Wagner, A. R. (1972). A theory of pavlovian
W2V did not outperform the co-occurrence models. Hence,               conditioning: Variations in the effectiveness of reinforcement and
our results may point to the benefit of having                        nonreinforcement. In Black, A. H. and Prokasy, W. F., editors, Classical
                                                                      conditioning II: Current research and theory, pages 64-99.
complementary learning systems when constructing a
                                                                    Riordan, B., & Jones, M.N. (2011). Redundancy in perceptual and
semantic representation: An error-driven predictive                   linguistic experience: Comparing feature-based and distributional
mechanism and a mechanism applied to direct co-                       models of semantic representation. TopiCS, 3, 303-345.
occurrences. There is no reason that the two learning              ≈òeh≈Ø≈ôek, R., & Sojka, P. (2011). Gensim - Statistical Semantics in Python.
mechanisms need to be mutually exclusive‚Äîthere is a great          Rogers, T. T., & McClelland, J. L. (2004). Semantic cognition: A parallel
                                                                      distributed processing approach. MIT press.
deal of evidence that humans use both Hebbian and error-           Rohde, D. L., Gonnerman, L. M., & Plaut, D. C. (2006). An improved
driven learning (e.g., O‚ÄôReilly, 1998). It is a reasonable            model      of     semantic     similarity   based    on     lexical    co-
presumption that perhaps the two systems work together to             occurrence. Communications of the ACM, 8, 627-633.
construct semantic memory from episodic experience.                Shaoul, C., Willits, J. A., Ramscar, M. Milin, P., & Baayen, R. H. (2016).
                                                                      A discrimination-driven model for the acquisition of lexical knowledge
                                                                      in auditory comprehension. Manuscript under review.
                       Acknowledgement                              St. John, M., & McClelland, J. L. (1990). Learning and applying
This research was funded by grant R305A140382 from the                contextual      constraints    in    sentence    comprehension. Artificial
Institute of Education Sciences.                                      Intelligence, 46, 217-257.
                                                               1097

