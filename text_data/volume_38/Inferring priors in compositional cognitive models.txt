                             Inferring priors in compositional cognitive models
                                Eric J. Bigelow                                       Steven T. Piantadosi
                    Dept. of Brain & Cognitive Sciences,                    Dept. of Brain & Cognitive Sciences,
                            University of Rochester                                 University of Rochester
                             Rochester, NY 14627                                     Rochester, NY 14627
                        ebigelow@u.rochester.edu                            spiantadosi@bcs.rochester.edu
                              Abstract                               from behavioral data, much in the spirit of work recovering
                                                                     priors from behavioral data in psychophysics (Stocker & Si-
   We apply Bayesian data analysis to a structured cognitive
   model in order to determine the priors that support human         moncelli, 2006; Paninski, 2005). We provide a freely modi-
   generalizations in a simple concept learning task. We mod-        fiable implementation in Python (S. T. Piantadosi, 2014) for
   eled 250,000 ratings in a “number game” experiment where          further use and extension.
   subjects took examples of a numbers produced by a program
   (e.g. 4, 16, 32) and rated how likely other numbers (e.g. 8          We assume that the prior parameters specify a genera-
   vs. 9) would be to be generated. This paper develops a data       tive model, namely a Probabilistic Context Free Grammar
   analysis technique for a family of compositional “Language of     (PCFG). For instance, in the context of logic, we might have
   Thought” (LOT) models which permits discovery of subjects’
   prior probability of mental operations (e.g. addition, multi-     separate PCFG parameters corresponding to the production
   plication, etc.) in this domain. Our results reveal high cor-     of a rule with disjunction (∨) versus conjunction (∧). These
   relations between model mean predictions and subject gener-       parameters determine the relative likelihood of each opera-
   alizations, but with some qualitative mismatch for a strongly
   compositional prior.                                              tion; by inferring their values, we are able to determine how
   Keywords: Concepts and categories; learning; Bayesian mod-        strongly subjects believe that each will be used in a novel,
   eling; machine learning                                           unobserved concept.
                                                                        Our analysis technique relies on Bayesian tools, allowing
                          Introduction                               us to infer both the likely parameters and the likely ranges of
Structured “Language of Thought” (LOT) models have re-               parameters from subjects’ data. This allows us to determine
cently become popular cognitive theories across a wide va-           exactly how much behavioral data tells us about the prior;
riety of domains (Goodman, Tenenbaum, Feldman, & Grif-               we might discover that the behavioral data is not informative
fiths, 2008; Katz, Goodman, Kersting, Kemp, & Tenen-                 about subjects’ priors, resulting in high variance in the pos-
baum, 2008; Kemp, Goodman, & Tenenbaum, 2008; Kemp,                  terior on PCFG parameters. Alternatively, we might discover
2012; Ullman, Goodman, & Tenenbaum, 2012; S. Pianta-                 that the prior probability of some but not all operations can
dosi, Tenenbaum, & Goodman, 2012; S. Piantadosi, Good-               be recovered from the data. This type of statistical inference
man, & Tenenbaum, under revision). In most of these ac-              permits inferences that are “just right” from subjects’ data,
counts, learners are assumed to generate compositionally             indicating what a scientist should believe about otherwise un-
structured hypotheses in order to explain observed data. For         observable cognitive operations.
instance, in Goodman et al. (2008), learners infer compo-               The structure of this paper is as follows. First, we introduce
sitions of boolean operations and featural primitives (e.g.          the Number Game, an induction task providing a simple do-
RED ∨ (DOT T ED ∧ SMALL)) to explain observed data, a                main of concept-learning. Then, we discuss the structure and
model that explains several key phenomena in rule learning           expressive potential of probabilistic context-free grammars as
as the consequence of Bayesian rule induction. Other work            a representation for concept hypotheses. After this we present
models number word learning as the discovery of a counting           our method for Bayesian data analysis of grammar parame-
algorithm, capturing children’s developmental progression as         ters and apply it to three complementary LOT formalizations.
a consequence of inferring the correct composition of opera-
tions to perform on sets (S. Piantadosi et al., 2012).                                   The Number Game
   These types of LOT models typically assume fixed priors           We consider concept learning in the Number Game
on hypotheses, which in turn provide an inductive bias for           (Tenenbaum, 1999, 2000), a simple domain of cognitively-
learners to prefer “simpler” compositions of primitives, con-        interesting induction. In the number game, concepts corre-
sistent with behavioral tendencies (Feldman, 2000, 2003).            spond to subsets of integers from the domain {1, ..., 100}.
   Of course, these priors amount to substantial assumptions         Subjects observe some numbers D (data) in an unobserved
about people’s expectations at the start of rule learning. To        concept C and are asked which other numbers are in C. For in-
test these assumptions, different particular LOTs have been          stance, given observed data D = {16, 2, 64, 8}, subjects might
tested to compare, for instance, LOT theories with distinct          induce that the concept used to generate these was “powers
types of quantification or varying sets of boolean operations        of two”. This an interesting domain because this problem
(Kemp, 2009, 2012; S. Piantadosi, 2011; S. T. Piantadosi,            is under-determined, meaning that there are many solutions
Tenenbaum, & Goodman, under review). Here, we develop a              (“all numbers” and “even numbers” are both consistent with
method for directly inferring the parameters of an LOT prior         the data, for instance). Despite this, subjects often have strong
                                                                 2597

                                                                      Independent Model Grammar                 Compositional Model Grammar
intuitions that some concepts are more likely.                               λ 0
                                                                      Start −→     Math                         Start −→ Set
   Tenenbaum (Tenenbaum, 1999) captures these intuitions                    1−λ                                      λ
                                                                      Start −→0 Interval                        Set −→0
                                                                                                                           Set ∪ Set
using a Bayesian model which computes P(C | D) according                                                             λ1
                                                                                                                Set −→     Set ∩ Set
to Bayes rule, or P(C | D) ∝ P(C)P(D | C). Here, P(C) is                                                             λ2
                                                                                                                Set −→     Set \ Set
a prior on concepts representing subjects’ beliefs about likely                                                      λ
                                                                                                                Set −→3
                                                                                                                           Math
concepts before D is observed. P(D | C) is a likelihood model                                                        λ
                                                                                                                Set −→4
                                                                                                                           Interval
of how likely D would be if C were the true concept. This                   λ1−9
                                                                      Math −→ Powers of n       2 ≤ n ≤ 10      Math −→ Map(λx.Expr, Interval) 1
work makes a strong sampling assumption that the elements                   λ10−19
                                                                      Math −→ Multiples of n        3 ≤ n ≤ 12  Expr −→
                                                                                                                        λ 5
                                                                                                                              Expr · Expr
of D are chosen by sampling uniformly from the set specified                λ20−29
                                                                      Math −→ Ends with n         0≤n≤9         Expr −→
                                                                                                                        λ 6
                                                                                                                              Ends with(Expr, Expr)
by C. Thus                           |D|
                                       1
                                                                            λ30−39
                                                                      Math −→ Contains Digit n        0≤n≤9     Expr −→
                                                                                                                        λ 7
                                                                                                                              Contains Digit (Expr, Expr)
                         P(D | C) =                         (1)              λ40
                                                                      Math −→ Prime numbers                     Expr −→
                                                                                                                        λ 8
                                      |C|                                    λ41                                        λ
                                                                                                                              Prime(Expr)
                                                                                                                          9
                                                                      Math −→ Even Numbers                      Expr −→       ExprExpr
   This explains why, for instance, D = {16, 2, 64, 8} sug-                  λ42                                       λ10
                                                                      Math −→ Odd Numbers                       Expr −→ Expr + Expr
gests “powers of two” rather than “even numbers”. If C                Math −→
                                                                             λ 43
                                                                                   Squares                      Expr −→
                                                                                                                       λ 11
                                                                                                                              x
is “powers of two”, this D would be chosen out of the set                    λ44
                                                                      Math −→ Cubes                             Expr −→
                                                                                                                       λ 12
                                                                                                                              ConstE
C = {2, 4, 8, 16, 32, 64}, giving a likelihood of (1/6)4 . If C                                                          λ13−27
                                                                                                                ConstE −→ n           1 ≤ n ≤ 15
were {2, 4, 6, . . . , 100}, then the likelihood would be much                    1.0
                                                                      Interval −→ Range[m, n] : 1 ≤ m ≤ n ≤ 100 Interval −→ Range[ConstI ,ConstI ]
less, (1/50)4 . This critical assumption that the likelihood of                                                           1.0
                                                                                                                ConstI −→ n 1 ≤ n ≤ 100 2
data depends on the cardinality of C is known as the size prin-
ciple and is a natural consequence of the strong sampling as-       Table 1: Three PCFG constructions: a Mixture Model (not shown),
                                                                    a simple model with independent probabilities for each rule (“Inde-
sumption (Tenenbaum, 1999).                                         pendent Model”), and a recursive compositional rule model (“Com-
   Given D, inferences about whether a new number x is in the       positional Model”). The number above each arrow gives the un-
                                                                    normalized probability of each rule expansion. The Mixture Model
concept are made by integrating across all possible concepts:       Grammar is identical to the Independent with all parameters λk are
                                             P(D | C)P(C)           fixed to 1.0 except λ0 .
          ∑ P(x | C)P(C | D) = ∑ P(x | C)        P(D)
                                                            (2)
            C                     C                                 deduce some further lessons about the structure of the PCFG
   Here, we estimate P(D) by summing over a large number            over and above fitting probabilities. In particular, we consider
of concepts, representing the vast majority of posterior proba-     three grammars capable of modeling number game concepts
bility mass (see below). In our implementation, both P(x | C)       (see Table 1). Next, we describe these models in more detail.
and P(D | C) also include a noise parameter α = 0.90 that           We will consider as an example the prior associated with the
generates elements of C 90% of the time, and elements from          “odd numbers” concept, for each of the three grammars.
{1, . . . , 100} uniformly 10% of the time.
                                                                    Mixture Model
The Language of Thought in the Number Game                          The Mixture Model implements a very simple PCFG which
In a formalization of learning as inductive inference over          combines two types of grammatical productions: those gen-
a LOT representation language, a probabilistic context-free         erating mathematical rules and those generating ranges of
grammar (PCFG) can be used to model concepts as com-                numbers, called interval-based concepts. This setup follows
positions of simple primitives. For our purposes, each tree         Tenenbaum (Tenenbaum, 1999, 2000), who constructed a
generated by a LOT PCFG represents a concept hypothesis             concept hypothesis space for the range of numbers 1 through
C. Generation probability for a tree gives the concept’s prior      100 according to representative concepts generated by addi-
probability p(C), calculated as p(C) = ∏k λk , where λk is the      tive clustering within the domain of numbers 1 through 10
probability of the k’th rule used to generate the tree. As in       (Tenenbaum, 1996). The only distinctions between this con-
all PCFGs, this probability is conditioned on the parent of the     cept hypothesis space and that of our model, are that our
generated node. This prior allows us to implicitly specify an       space includes two additional types of rule-based concepts:
infinite concept space and assign higher probability to more        “Ends in n” and “Contains Digit n”, and that where Tenen-
concise LOT expressions.                                            baum assigns an Erlang prior across interval concepts, we as-
   The assumed PCFG represents a hypothesis about what              sign a uniform prior for simplicity.
mental representations might be like, as well as what types            The Mixture Model grammar has a single parameter λ0
of concepts people intuitively find probable. By inferring the      which determines the relative probability of using a rule-
{λk } from data, we are assuming part of the representation         based or a interval-based grammatical production. Once the
(the structure of the PCFG) and discovering part (the specific      type of the production is chosen, the specific production is
probabilities) from human data. Note that we may find that
                                                                        1 While other λ here refer to parameters, this refers to the abstrac-
some λk are close to zero, meaning that we have assumed
                                                                    tion operator λ.
rules which are not psychologically justifiable. Additionally,          2 In order to narrow the extremely broad hypothesis space of
we may write down different PCFGs and compare their per-            the compositional model, priors of 5.0 are assigned to constants
formance in explaining human behavior. This allows us to            1, 10, 20, 30, . . . , 90, 100.
                                                                2598

chosen uniformly as in (Tenenbaum, 1999, 2000). For exam-              wide range of complexities with fewer free parameters. For
ple, to generate an “odd numbers” concept, we begin at Start           example, the following three concepts are ordered by increas-
and traverse to Math with probability λ0 . Following the al-           ing complexity when generated by the Compositional gram-
lowed rules in Table 1, we expand Math to “Odd Numbers”                mar: “powers of 2”, “powers of 2 plus 1”, “only primes from
with probability 1/44 , yielding an overall prior of p(C) = λ440 .     the set of powers of 2 plus 1”
Independent Probabilities Model                                                            Bayesian Data Analysis
The Independent Probabilities grammar generates the                    The primary contribution of this paper is to apply data anal-
same set of concepts as the Mixture Model grammar, but                 ysis techniques to an interesting domain of inductive con-
with the key difference that each rule-based concept is pro-           cept learning in cognitive science, assessing parameters of the
duced by a rule with an independent probability parameter              prior, {λk }, given subjects’ behavioral data. The prior is the
λk . This grammar also has a mixture parameter λ0 that biases          most psychologically laden aspect of LOT models because it
the grammar towards interval- or rule-based concepts. As in            specifies people’s assumptions without any data, as well as—
the Mixture Model, interval concepts are assigned a uniform            in compositional models—their subjective expectations about
prior. The parameter space for the Independent Model allows            how likely each operation is to be used. Similar data analysis
a more intricate representation of individuals’ priors, where          has been used previously to infer the parameters of the prior’s
relative bias associated a priori with particular concepts can         PCFG (S. Piantadosi, 2011; S. T. Piantadosi et al., under re-
be inferred. For example, in the Independent Model we might            view), although largely with the goal of comparing different
infer that an individual has a stronger bias associated with the       languages. Here, we focus on the complementary question of
concept “multiples of 4” over the concept “multiples of 7”.            inferring in detail the relative expectations among different
   To generate the “odd numbers” concept, the same rules are           primitives, or different classes of primitives.
used to generate the concept as in the Mixture Model but with             To do this, we present a method of Bayesian data analysis
different associated priors: p(C) = 44λ42λ .                           (Kruschke, 2010) to infer a posterior distribution on each rule
                                      ∑k=1 k
                                                                       probability, given the human responses. We’ll use yi to repre-
Compositional Model                                                    sent the number of “yes” counts for whether a query xi (e.g.
                                                                       xi = 16) is in the concept, given data Di (e.g. Di = {2, 4, 8}).
The Compositional grammar is a recursive LOT model that                We’ll use Ni to denote the total number of human responses
expresses concepts by freely composing primitives, based on            for query xi , fixed by experimental design. Let G = {λk } be
the general approach of (Goodman et al., 2008). The Com-               the set of rule production probabilities. For a given G, we
positional Model has a significantly wider concept hypothesis          implicitly define a space of concept hypotheses that may be
space, and its parameter space reflects the probability of using       generated by this grammar. We wish to find
each primitive operation. These primitives include values and
operations that can compose to create a range of numerical                      P(G | y, D, N, x) ∝ P(G)P(y, | G, D, N, x)
concepts, including all those of the other two models, as well                                        = P(G) ∏ P(yi , | G, Di , Ni , xi ).             (3)
as set operations between mathematical expressions and in-                                                          i
tervals. For example, one might infer that humans have a par-             Where the product comes from assumed independence of
ticularly high bias associated with the “times” operator, and a        responses, conditioned on Di and xi . In this equation, P(G)
very low prior with the “powers of n” operator. Note that by           is a prior on parameters. We assign a Gamma(2, 1) prior to
the assumed statistical structure of a PCFG, every place that          each λk , representing its unnormalized probability. Human
“times” is used, it will have the same probability. Thus, de-          responses are then assumed to come from a binomial likeli-
spite having a richer concept space, the Compositional gram-           hood P(yi , | G, Di , Ni , xi ),
                                                                            
mar cannot model some of the concept-specific priors of the                  Ni
                                                                                 [P(yi , | G, Di , Ni , xi )]yi [1 − P(yi , | G, Di , Ni , xi )]Ni −yi (4)
Independent Model — e.g. the context free assumptions of                     yi
the PCFG mean that 2 in (2 · x) and 2 in (2 + x) both are                 Together, these equations specify a data analysis model
equally likely (i.e. irrespective of their use in the context of       over grammar probabilities that can take human inferences
addition or multiplication).                                           (e.g. 9 out of 10 people believe 16 is in the concept {4, 8}
   This grammar requires a considerably deeper tree to rep-            came from), and work backwards to discover the relative
resent the “odd numbers” concept. From Start we go to Set,             probability of compositional PCFG components behind these.
then Math with a probability λ3 , which maps an Expr across               For the Compositional Model, we use a Markov Chain
the range of integers 1 to 100. This Expr goes to Expr +Expr           Monte Carlo sampling technique, the Metropolis-Hastings
with probability λ10 ; one of the two Expr terminates at 1 with        (MH) algorithm, to generate a representative subset of the in-
probability λ13 , and the other goes to Expr · Expr. This final        finite concept hypothesis space when computing the marginal
expression terminates with x and 2, with respective probabil-          (integral) (2). In computing the marginal, we ran 255 inde-
ities λ11 & λ14 . In plain English, “multiples of 2, plus 1, for       pendent MH chains at 300,000 steps each, with each chain
integers one to one-hundred.” While this may seem compli-              conditioned according to a unique concept example D from
cated, an advantage of this model is its capacity to capture a         our data. All chains were initialized with the same uniform
                                                                   2599

                                                                           find 3 subjects saying “yes” and 3 saying “no” to 15, condi-
                                                                           tioned on {16} being in the concept. In this case, λ0 is biased
                                                                           towards 0, representing a preference for interval-based con-
                                                                           cepts. A bias emerges here because mathematical concepts
                                                                           have trouble explaining this data, so little is gained by assign-
(a) 15 in concept?      (b) 15 in concept?       (c) 15 in concept?        ing them a high prior. However, if most subjects say “no”
h3 yes, 3 noi           h3 yes, 30 noi           h30 yes, 3 noi            to 15 (Figure 1b), the model correctly implies that λ0 must
                                                                           be higher: rejecting 15 means that people likely down-weight
                                                                           the prior probability of ranges, many of which include 15.
                                                                               Therefore, more “yes”es than “no”s will indicate that
                                                                           interval-based concepts probably have a high prior bias. We
                                                                           see the posterior distribution in Figure 1c reflects this expec-
(d) 64 in concept?      (e) 64 in concept?       (f) 64 in concept?        tation, since the distribution over mixture ratios is greatly
h3 yes, 3 noi           h3 yes, 30 noi           h30 yes, 3 noi            skewed towards interval-based concepts (low λ0 ). We also
                                                                           see this skew towards interval-based concepts in Figure 1a,
Figure 1: Posterior distribution of parameter space for the Mixture
Model grammar p(λ0 | D, x, y, N). D = {16} for each example here,          but with a more uniform distribution. With very few data
and x, y, N are each single-item sets, for either x = {15} (top row) or    points, it is unclear whether these responses are truly reflec-
x = {64} (bottom row).                                                     tive of the prior, or simply result from noise. With a more
prior G0 (approximately uniform, with higher probability as-               proportional mixture ratio, we should see fewer “yes”es than
signed to terminal rules). The 1000 hypotheses with highest                “no”s due to an initial preference towards rule-based con-
posterior scores for each chain (thus containing nearly all pos-           cepts, as reflected in the distribution of Figure 1b.
terior probability mass) were all joined by union, yielding a                  Imagine you are instead asked whether 64 is in the target
total set size of 9, 946. This formed a finite hypothesis space            concept given D = {16}. Maybe the pattern is “square num-
that well-approximated each posterior (P(C | D)).                          bers” or “powers of 2” - each of these has a small set size in
                                                                           the domain of 1 to 100, so it is likely that 16 would be output
   MH was also used to infer the grammar parameters G in
                                                                           if one of these was the concept. Since “rule-based” concepts
3 for all models. Ten chains each were run for the Com-
                                                                           have such small set sizes, these are normally preferred over
positional and Independent Models, each for 50,000 steps,
                                                                           interval concepts given very little data (Tenenbaum, 1999).
with 50,000 steps of burn-in; convergence was usually ob-
                                                                           We therefore expect that with a proportional bias between in-
served for these after less than 20,000 steps. Convergence in
                                                                           terval and rule concepts, a majority of observers should as-
the Mixture Model (which has only one parameter) was usu-
                                                                           sume 64 is in the target concept given D = {16}, as seen in
ally reached after less than 500 samples were drawn, so four
                                                                           Figure 1f. This will not be the case only when interval con-
chains for this model were each run for 10,000 steps.
                                                                           cepts are given a strong bias over rule concepts (Figure 1e).
An example statistical inference                                           Analysis of a large-scale Number Game experiment
To demonstrate our approach, we first apply this data analysis             We analyzed a large dataset of human responses for our ex-
to a simple toy example. Imagine you see a single example                  periment, comprised of 606 participants with demograph-
from a concept: {16} and are asked whether 15 or 64 are part               ics typical of Amazon Mechanical Turk workers (Bigelow
of the same concept. 15 is close to 16 in magnitude, the metric            & Piantadosi, 2016). Subjects were tested on input sets D
of “similarity” considered by Tenenbaum. On the other hand,                that were generated by using a small set of “primordial”
64 can be grouped with 16 according to numerical rules—for                 sets (all integers, evens, odds, squares, cubes, and primes)
example, “even numbers”, “powers of two”, “powers of 4”,                   and then mapping a variety of functions (e.g. f (n) = n + 1,
or ”perfect squares” are all candidate concepts.                            f (n) = n + 2, f (n) = n − 1, f (n) = 2 · n, etc.) across each.
   We ran this simulated data set on the Mixture Model,                    Data sets D were generated by randomly sampling sets of
shown in Table 1. We then used our data analysis to infer                  length 2, 3, and 4 from each resulting concept. On each trial,
this model’s single λ0 parameter from a few data sets with                 participants were told a set D was generated by a specific pro-
intuitive answers, serving as a proof of principle for our data            gram, then indicated whether it was likely the program would
analysis methods. For instance, if subjects assume 15 is in                generate each target from a random sequence of 30 numbers
the concept (given only that they know 16 is), we should in-               in the range {1, . . . , 100}. No feedback was given as to the
fer that λ0 is low, meaning that similarity (distance) based               correctness of responses. Responses were trimmed for the 31
concepts are the most psychologically salient ones. On the                 subjects with the lowest total typicality values - measured as
other hand, if our simulated data has that subjects believe 64             log(p) for “yes” responses and log(1 − p) for “no”, where p
is likely instead, we should see a high λ0 , indicating that the           is the fraction of positive responses - yielding a total 258,750
psychologically highest prior concepts are rule-like.                      generalizations across 255 unique concepts.
   Figure 1 shows the posterior distribution on the λ0 for six                 Figure 2 shows an overall model summary of our results,
artificial datasets. Figure 1a shows the distribution on λ0 if we          giving the relationship between the model’s predictive distri-
                                                                       2600

                                                                                                           (a) Independent Probabilities Grammar
bution (x-axis) and humans’ probability of responding “yes”                                                                                            (b) Compositional Grammar                                                                                     (c) Mixture Parameter
(y-axis), to each of the generalizations in the data. The left                                                                                  10.0                                                                                                                      1.00
column shows the initial grammar parameters — i.e., before
                                                                                                                                  Probability
                                                                                                                                                 7.5
{λk } has been fit to human data; the right column shows the                                                                                     5.0
                                                                                                                                                                                                                                                                          0.75
MAP grammars. This plot makes clear several aspects of our                                                                                       2.5                                                                                                                 λ0   0.50
data. First, the correlations are high in the left column, mean-                                                                                 0.0
                                                                                                                                                                                                                                                                          0.25
                                                                                                                                                                                  Prime
                                                                                                                                                       Contains Digit
                                                                                                                                                                        Ends in
                                                                                                                                                                                                  Plus
                                                                                                                                                                                                         Times
                                                                                                                                                                                                                 Constant
                                                                                                                                                                                                                            1   2   3   4   5   6   7   8   9   10
ing that the default model fits the human data relatively well.                                                                                                                           Power
However, the model fit does improve in the right column, in-                                                                                                                                                                                                              0.00
                                                                                                                                                                                                                                                                                 Compositional
                                                                                                                                                                                                                                                                                                   Independent
dicating that there is some variance to be gained by adjusting                                                                                                                                                                                                                                                   Mixture
these parameters. Note that in all subplots the reported R2 val-
                                                                                                                                                                                                                                                                                                 Model
ued are on the box plot means, meaning that they do not rep-
resent the full variance in the data. The box plot also shows                                                                  Figure 3: Posterior distribution of unnormalized grammar produc-
                                                                                                                               tion probabilities λk . Bar width in this graph represents sample den-
that there is considerable variance in responses for each bin,                                                                 sity relative to a specific parameter λk . Note that in the Independent
meaning that there is a substantial variability over and above                                                                 Model, all rules shown are normalized relative to each other. In the
the means that the model does not capture.                                                                                     Compositional Model, constant primitives (1-10) are weighted rel-
                                                                                                                               ative to other constants, and operators (“Ends in”, “Times”, “Con-
                                                                                                                               stant”) are weighted relative to other operators (see Table 1).
                        1.00                                                       1.00
                               R2mean = 0.976                                             R2mean = 0.982
                                                                                                                                  The overall shape of these plots tells us whether the model
    Human Probability                                          Human Probability
                        0.75                                                       0.75
                        0.50                                                       0.50
                                                                                                                               makes qualitatively correct predictions. The Mixture Model
                        0.25                                                       0.25
                                                                                                                               does well in this respect, as does the Independent Model, al-
                        0.00                                                       0.00
                                                                                                                               though the Independent Model must be interpreted with care
                                                                                                                               since it freely fits the prior on a large number of concepts.
                                  0
                               0.05
                                0.1
                               0.15
                                0.2
                               0.25                                                          0
                                                                                          0.05
                                                                                           0.1
                                                                                          0.15
                                                                                           0.2
                                                                                          0.25
                                0.3
                               0.35
                                0.4
                               0.45
                                0.5
                               0.55                                                        0.3
                                                                                          0.35
                                                                                           0.4
                                                                                          0.45
                                                                                           0.5
                                                                                          0.55
                                0.6
                               0.65
                                0.7
                               0.75
                                0.8
                               0.85                                                        0.6
                                                                                          0.65
                                                                                           0.7
                                                                                          0.75
                                                                                           0.8
                                                                                          0.85
                                0.9
                               0.95                                                        0.9
                                                                                          0.95
                                          Model Probability                                        Model Probability
                                                                                                                               The initially good fits of the Mixture and Independent Mod-
               (a) Mixture Model Init.                         (b) Mixture Model MAP                                           els indicate that an approximately uniform prior across these
                        1.00
                               R2mean   = 0.977
                                                                                   1.00
                                                                                          R2mean = 0.989                       models’ hypothesis spaces fits the data well; clearly, inferring
    Human Probability                                          Human Probability
                        0.75                                                       0.75
                                                                                                                               MAP G will improve fit for some models more than others.
                        0.50                                                       0.50
                                                                                                                                  The Compositional Model seems to miss the qualitative
                        0.25                                                       0.25
                                                                                                                               match, which means falling far from the line y = x even
                        0.00                                                       0.00
                                  0
                               0.05
                                0.1
                               0.15
                                0.2
                               0.25
                                0.3
                               0.35
                                0.4
                               0.45
                                0.5
                               0.55
                                0.6
                               0.65
                                0.7
                               0.75
                                0.8
                               0.85
                                0.9
                               0.95
                                                                                             0
                                                                                          0.05
                                                                                           0.1
                                                                                          0.15
                                                                                           0.2
                                                                                          0.25
                                                                                           0.3
                                                                                          0.35
                                                                                           0.4
                                                                                          0.45
                                                                                           0.5
                                                                                          0.55
                                                                                           0.6
                                                                                          0.65
                                                                                           0.7
                                                                                          0.75
                                                                                           0.8
                                                                                          0.85
                                                                                           0.9
                                                                                          0.95
                                                                                                                               though the general increasing trend yields a respectable cor-
                                          Model Probability                                        Model Probability
                                                                                                                               relation. The qualitative mismatch is particularly salient for
   (c) Independent Model Init.                                (d) Independent Model MAP                                        model predictions < 0.5, meaning the Compositional Model
                        1.00
                               R2mean   = 0.893
                                                                                   1.00
                                                                                          R2mean = 0.928
                                                                                                                               seems to systematically mischaracterize people’s likely “no”
                                                                                                                               responses. This may be because the form of the prior is highly
    Human Probability                                          Human Probability
                        0.75                                                       0.75
                        0.50                                                       0.50                                        constrained in the Compositional Model to follow the PCFG.
                        0.25                                                       0.25                                        For example, assigning a high prior to the concept “odd num-
                        0.00
                                  0
                               0.05
                                0.1
                               0.15
                                0.2
                               0.25
                                0.3
                               0.35
                                0.4
                               0.45
                                                                                   0.00
                                                                                             0
                                                                                          0.05
                                                                                           0.1
                                                                                          0.15
                                                                                           0.2
                                                                                          0.25
                                                                                           0.3
                                                                                          0.35
                                                                                           0.4
                                                                                          0.45
                                                                                                                               bers” (2n + 1) requires high priors on its four components
                                0.5                                                        0.5
                                                                                                                               “times”, “plus”, “2”, and “1”. But high priors on these would
                               0.55
                                0.6
                               0.65
                                0.7
                               0.75
                                0.8                                                       0.55
                                                                                           0.6
                                                                                          0.65
                                                                                           0.7
                                                                                          0.75
                                                                                           0.8
                               0.85
                                0.9
                               0.95                                                       0.85
                                                                                           0.9
                                                                                          0.95
                                          Model Probability                                        Model Probability
  (e) Compositional Model Init. (f) Compositional Model MAP                                                                    also assign high prior to 1n + 2 (“numbers greater than two”)
                                                                                                                               due to the independence assumptions of a PCFG. It is pos-
Figure 2: Box plots showing the distribution of human responses                                                                sible that our Compositional Model performs poorly because
(y) binned by model predictions (x). Model predictions are shown                                                               we have included the wrong primitives or wrong structure in
for initial (a,c,e) and MAP (b,d,f) grammar priors. Red diamonds
show mean values; box plots show 25th , 50th , & 75th percentiles,                                                             our particular PCFG. Fits such as these may in principle be
and whiskers show 5th and 95th . Correlations were computed on                                                                 used to quantitatively compare hypothesized LOTs.
mean values; box plots show high variance over and above these.
                                                                                                                           2601

   These results highlight the impact of grammar design on          Feldman, J. (2000). Minimization of boolean complexity in
determining the hypothesis space for models, and we expect            human concept learning. Nature, 407(6804), 630–633.
this to be an area of future work. Another area of future work      Feldman, J. (2003). The simplicity principle in human con-
may be to explore the limitations of PCFGs. It may be that            cept learning. Current Directions in Psychological Science,
using a context-sensitive grammar will be necessary to fully          12(6), 227.
capture human data, or models of another family entirely.           Goodman, N., Tenenbaum, J., Feldman, J., & Griffiths, T.
   The most interesting aspect of our analysis is the posterior       (2008). A Rational Analysis of Rule-Based Concept Learn-
distribution on grammar productions for each model, giving            ing. Cognitive Science, 32(1), 108–154.
what we should believe about people’s relative probability of       Katz, Y., Goodman, N., Kersting, K., Kemp, C., & Tenen-
each kind of operation. Figure 3 shows these posterior dis-           baum, J. (2008). Modeling semantic cognition as logical
tributions. The independent probabilities model shows that            dimensionality reduction. In Proceedings of Thirtieth An-
human data is best fit when the highest priors are associated         nual Meeting of the Cognitive Science Society.
with “Even Numbers”, “Multiples of 11” (numbers with re-            Kemp, C. (2009). Quantification and the language of thought.
peated digits), “Multiples of 5”, “Odd numbers.” Interest-            Advances in neural information processing systems, 22.
ingly, nonzero weight is also assigned to “Primes”, “Multi-         Kemp, C. (2012). Exploring the conceptual universe. Psy-
ples of 12”, and “Multiples of 4”.                                    chological Review, 119, 685–722.
   In the Compositional Model, human data is best fit by high       Kemp, C., Goodman, N., & Tenenbaum, J. (2008). Learning
priors associated with the “Ends In”, “Prime”, and “Multi-            and using relational theories. Advances in neural informa-
ples” (a · b) operations. The highest prior among operations          tion processing systems, 20, 753–760.
is associated with “Constant”, which is to be expected as this      Kruschke, J. (2010). Doing bayesian data analysis: A tutorial
allows for smaller hypothesis trees. Among the constants,             introduction with r. Academic Press.
the highest prior is associated with 2, followed by a general       Paninski, L. (2005). Nonparametric inference of prior proba-
decreasing trend for large numbers that is reminiscent of the         bilities from bayes-optimal behavior. In Advances in neural
frequency distribution of numbers in language (Dehaene &              information processing systems (pp. 1067–1074).
Mehler, 1992; S. T. Piantadosi, in press).                          Piantadosi, S. (2011). Learning and the language of thought.
   Figure 3c shows that λ0 is much higher for the Composi-            Unpublished doctoral dissertation, MIT.
tional Model than the others, as to be expected since many          Piantadosi, S., Goodman, N., & Tenenbaum, J. (under revi-
of its hypotheses are redundant with interval concepts (e.g.          sion). Modeling the acquisition of quantifier semantics: a
n + 2 corresponds to “numbers greater than 2”, equivalent to          case study in function word learnability.
the interval [3, 100]). The Independent Model’s λ0 is higher        Piantadosi, S., Tenenbaum, J., & Goodman, N. (2012). Boot-
than the Mixture Model’s, which is also expected as λk for            strapping in a language of thought: a formal model of nu-
each rule-based concept was conditioned on the same data.             merical concept learning. Cognition, 123, 199–217.
                                                                    Piantadosi, S. T. (2014). LOTlib: Learning and in-
                          Conclusion                                  ference in the language of thought.           available from
Our work has shown how a structured inductive LOT model               https://github.com/piantado/LOTlib.
may be combined with a Bayesian data analysis to infer likely       Piantadosi, S. T. (in press). A rational analysis of the approx-
parameters of human subject’s priors. Our analysis has re-            imate number system. Psychonomic Bulletin and Review.
vealed both an ability to freely infer priors on concepts (the      Piantadosi, S. T., Tenenbaum, J., & Goodman, N. (under re-
Independent Model) as well as those that use the PCFG more            view). The logical primitives of thought: Empirical foun-
productively, decomposing a concept’s prior into a product of         dations for compositional cognitive models.
the priors of its parts (the Compositional Model). In these         Stocker, A. A., & Simoncelli, E. P. (2006). Noise character-
cases, Figure 3 represents our inference of these values from         istics and prior expectations in human visual speed percep-
subject data. While the mean predictions of the Composi-              tion. Nature neuroscience, 9(4), 578–585.
tional Model are highly correlated with human means, the            Tenenbaum, J. B. (1996). Learning the structure of similarity.
qualitative fit is worse. However, the method developed here          Advances in neural information processing systems, 3–9.
of analyzing large-scale experiments with Bayesian data anal-       Tenenbaum, J. B. (1999). A bayesian framework for concept
ysis provides a way forward for fitting, refining, and compar-        learning. Doctoral dissertation, Massachusetts Institute of
ing LOT models of human cognition.                                    Technology.
                                                                    Tenenbaum, J. B. (2000). Rules and similarity in concept
                          References                                  learning. Advances in neural information processing sys-
Bigelow, E., & Piantadosi, S. (2016). A large dataset of gen-         tems, 12, 59–65.
   eralization patterns in the number game. Journal of Open         Ullman, T., Goodman, N., & Tenenbaum, J. (2012). Theory
   Psychology Data, 4(1).                                             learning as stochastic search in the language of thought.
Dehaene, S., & Mehler, J. (1992). Cross-linguistic regular-           Cognitive Development.
   ities in the frequency of number words. Cognition, 43(1),
   1–29.
                                                                2602

