 Developmentally plausible learning of word categories from distributional statistics
                        Daniel Freudenthal1, Julian M. Pine1, Gary Jones2 and Fernand Gobet1
                                    1
                                      Department of Psychological Sciences, University of Liverpool
                                        2
                                          Division of Psychology, Nottingham Trent University
                              Abstract                               language-learning children. However, it has been argued that
                                                                     it works less well for languages with relatively free word
   In this paper we evaluate a mechanism for the learning of word
   categories from distributional information against criteria of    order such as Dutch (Erkelens, 2009) and German (Stumper
   psychological plausibility. We elaborate on the ideas             et al. 2011). It has also been argued that its all-or-none nature
   developed by Redington et al. (1998) by embedding the             makes it overly sensitive to noise - the presence of a small
   mechanism in an existing model of language acquisition            number of items that do not fit the dominant word class for a
   (MOSAIC) and gradually expanding the contexts it has access       frame has large effects on classification accuracy
   to in a developmentally plausible way. In line with child data,   (Freudenthal et al. 2013).
   the mechanism shows early development of a noun category,
   and later development of a verb category. It is furthermore
                                                                        Redington et al. (1998) express the contexts in which
   shown that the mechanism can maintain high performance at         words occur as vectors containing counts of frequent context
   lower computational overhead by disregarding token                words in preceding and following position. Similarity
   frequency information, thus improving the plausibility of the     between words is then expressed as the (rank order)
   mechanism as something that is used by language-learning          correlation between these vectors, and the matrix of
   children.                                                         correlations is used as input to a cluster analysis. The
   Keywords: Word class acquisition; Distributional analysis         probabilistic nature of Redington et al.’s approach makes it
                                                                     naturally resistant to noise, and thus less prone to error.
                         Introduction                                However, Redington et al.’s approach requires children to
A key issue in understanding how children acquire language           track the frequency of large numbers of words and has been
is how they build word class categories such as verb or noun.        criticized for carrying a high computational overhead (St.
Recent computational work has shown that there is a great            Clair et al. 2011), thus making it less appealing as a
deal of information in the distributional properties of              mechanism that language-learning children might employ.
different languages that can be used to distinguish between          The plausibility of this approach would thus be greatly
instances of different word class categories in the input. For       increased if it could be shown that its computational overhead
example, in English, words that are preceded by determiners          can be reduced significantly without affecting performance.
such as a and the and followed by auxiliary verbs such as can           While both approaches are intended as mechanisms that
and will tend to be nouns, whereas words that are preceded           language-learning children employ, neither considers the fact
by nominative pronouns such as I and You and followed by             that word classes are likely to be gradually built up over time.
determiners such as a and the tend to be verbs.                      Thus, they collect statistics from complete utterances and
   Several approaches to this problem have been proposed,            across large corpora, and focus on building large word classes
but they tend to focus on building large word classes with           with high accuracy. However, it is debatable whether young
high accuracy rather than developing mechanisms that can be          children represent statistics that reflect all of the input they
plausibly applied by, and fit developmental data from                have encountered. Children’s early utterances are just one or
language-learning children. Thus, mechanisms for                     two words long, and only gradually increase in length over a
distributional analysis routinely collect data from large            period of years. While children may well attend to longer
corpora and entire utterances, and make limited contact with         utterances than they produce, it seems unlikely that they
the (developmental) child data. In this paper, we explore a          would process entire utterances from a young age. A
more developmentally plausible mechanism by embedding it             mechanism that tracks complete utterances may therefore
in an existing model of language acquisition that learns to          employ statistics that are not available to language-learning
produce increasingly long utterances, and thus gradually             children in the early stages of development. A
expands the contexts over which statistics are computed in a         developmentally more plausible mechanism would build
way that is consistent with children’s processing biases.            word classes gradually by slowly expanding the contexts
   The two dominant approaches to distributional analysis are        from which statistics are collected.
those of Mintz (2003) and Redington et al. (1998). Mintz                This suggestion is further supported by experimental
introduces the notion of a ‘frequent frame’ - a combination of       evidence which suggests that children’s productive use of
two words with one word intervening (You X A). Mintz                 words develops at different speeds for different word classes.
identifies the most (typically 45) frequent frames for a given       In particular, production studies suggest that children develop
corpus, and finds that the words that co-occur within a frame        a category of noun earlier than they develop a category of
tend to belong to the same grammatical category. The                 verb (Akhtar & Tomasello, 1997; Olguin & Tomasello, 1999;
approach of Mintz seems intuitively appealing since its              Tomasello & Olguin, 1993).
relative simplicity means it is well the capabilities of                The main aim of this paper is to investigate if such
                                                                     developmental effects can be simulated by gradually
                                                                   674

expanding the contexts from which statistics are collected.       that have been preceded by {a, the} and {the, green}
This will be done by embedding variants of Redington et al.’s     respectively, the similarity is 1/3.
mechanism in an existing computational model of language             Disregarding token frequency reduces the computational
acquisition (MOSAIC; Freudenthal et al. 2007, 2010, 2015),        overhead associated with the mechanism considerably: there
that successfully simulates a number of key phenomena in          is no need to collect large numbers of counts, and
language acquisition. Full details of MOSAIC and the way in       computation of the Jaccard distance is a mathematically
which it is trained are provided in Freudenthal et al. (2015).    simpler operation than the computation of a rank order
   MOSAIC is a simple learning model that takes as input          correlation. While the simplified mechanism uses less
orthographically transcribed corpora of child-directed            information than the original approach, this may only have
speech. Training in MOSAIC takes place by feeding the input       limited effects on performance, since the rank order
corpus through the model multiple times. A key feature of         correlation used by Redington et al. only utilizes a limited
MOSAIC is that it builds up its representation of the             amount of frequency information.
utterances to which it is exposed by starting at the right edge      A final question concerns how well the mechanism is
of the utterance and slowly working its way to the left. Thus,    capable of capturing developmental effects in the building of
with each exposure to the input MOSAIC represents                 word classes. If the mechanism could be shown to display
increasingly long utterance-final phrases that become             early emergence of a noun class and late emergence of a verb
increasingly adult-like. The utterance-final bias is              class, this would increase its developmental plausibility.
MOSAIC’s key mechanism for simulating cross-linguistic            Such a developmental pattern may arise naturally from
differences in children’s early speech and is thus                MOSAIC’s utterance-final bias. Nouns frequently occur near
independently motivated. MOSAIC thus provides a natural           the end of utterances, whereas verbs tend to occur in medial
framework for testing the notion that word classes develop        position. Contexts for nouns may therefore register earlier
gradually as the contexts from which statistics are collected     than for verbs, resulting in the emergence of a noun category
are expanded. MOSAIC employs a mechanism for                      before a verb category.
distributional analysis that borrows from the ideas of               In summary, the main aim of this paper is to develop a
Redington et al., and thus allows us to investigate the           psychologically plausible mechanism for learning word
performance of different implementations of the mechanism         classes from distributional information by 1. assessing the
in a developmental setting.                                       performance of Redington et al.’s mechanism when gradually
   A first analysis will investigate how well Redington et al.’s  extending its access to input in a developmentally plausible
mechanism performs when statistics are collected from             way, 2. comparing the performance of the original
increasingly long utterance-final phrases. This will be done      mechanism with a greatly simplified one that disregards
by training MOSAIC on corpora of child-directed speech and        token frequencies, and 3. determining if the developmental
producing output at several stages of development. The            variation results in a developmentally plausible pattern of
output (of increasing average length) from MOSAIC will            noun and verb linkage. These questions are investigated by
then be used to derive the counts of context words on which       applying the two variants of Redington et al.’s mechanism to
the mechanism is based, and thereby test the performance of       the increasingly long phrases encoded by MOSAIC models
the mechanism in a developmentally more plausible setting.        in different stages of development. The main dependent
   A second analysis will investigate how well a substantially    variables are the standard measures of accuracy (number of
simplified version of Redington et al.’s approach performs.       correct classifications), and completeness (number of
For all words that are to be classified (the target words),       classifications), both overall and for nouns and verbs
Redington et al.’s mechanism collects counts for a number         separately. Additionally, a measure of noun richness is
(typically the 150 most frequent words for a corpus) of           computed to track the emergence of noun and verb
context words in preceding and following position. The            categories.
vectors of counts are then concatenated and similarity
between words is expressed as the rank order correlation                               The simulations
between concatenated vectors. The mechanism thus collects         The first set of analyses was aimed at determining the
counts for large numbers of context words, but only uses a        performance of Redington et al.’s approach in a
limited amount of this frequency information. Here, we            developmental setting. To this end, MOSAIC was trained on
investigate the performance of a variant of Redington et al.’s    the child-directed speech for each of the 12 children in the
mechanism that disregards token frequency information             Manchester corpus (Theakston et al., 2001). Each model was
altogether. Rather than collecting counts for target words in     exposed to the input a total of 50 times. The average length
preceding and following position, the approach simply notes       of the utterances that MOSAIC represents increases from
the identity of the words in preceding and following position.    zero to approximately five words over training. The model’s
The context for a given target word is therefore expressed as     ability to classify words was assessed at several points in the
a list of words (types) rather than a vector of counts (tokens).  model’s training. As in Redington et al.’s original
Similarity is then expressed as a measure formally known as       formulation, target and context words were restricted to the
the Jaccard distance: the length of the intersection of two       1000 and 150 most frequent words for each corpus (based on
contexts divided by the length of the union - for two words
                                                                 675

corpus-wide counts). Target and context words were fixed             The main developmental effect in these simulations is that
throughout development.                                           the number of linked items in the corpus-wide analysis is
   At each point in development, context vectors for the target   lower than it is for most earlier developmental stages, which
words were generated by determining how often the context         link more items with lower accuracy. This pattern seems
words occurred in the position directly before or after the       implausible. Children’s early language use has been
target words. A complicating factor here is that MOSAIC           characterized as relatively rote and lexically specific, and is
does not represent duplicate utterances, and may thus             thought to become more productive with age – the opposite
underestimate how often a context and target word co-occur.       of the pattern in Table 1. It is also apparent from Table 1 that,
This was remedied by taking each utterance in the input           while the mechanism manages relatively high accuracy for
corpus, determining the largest utterance-final phrase from       nouns, it is far less accurate in linking verbs, particularly for
that utterance that was represented in the model, and adding      the early stages – verb accuracy exceeds 0.6 for just 4 out of
this utterance-final phrase to the pool of utterances over        the 12 cells in Table 1. Taken together, these findings suggest
which statistics were computed. Thus, if the input corpus         that the mechanism is not sufficiently constrained in the early
contained three instances of “it’s a dog”, and two instances of   stages of development.
“that’s a dog”, and the model only represents the utterance-
final phrase “a dog”, then 5 instances of the phrase “a dog”           Table 1: Performance of Redington et al.’s approach at
were added to the pool of utterances. The pool of utterances           thresholds of 0.5 and 0.6, averaged over 12 models.
was then searched for the target words, and any occurrence           Runs       # of links       Acc.        Noun         Verb
of context words in preceding and following position noted.                                                   Acc.        Acc.
The rationale behind this procedure was to generate accurate          0.50
counts for an input corpus based on the utterance-final                36         1,954          0.68         0.74        0.37
fragments from that corpus that were represented in the                38         4,580          0.73         0.77        0.41
model. As training proceeds, and MOSAIC represents more                40         5,219          0.72         0.75        0.43
and longer utterances, the counts generated in this manner             44         3,258          0.79         0.80        0.57
will become a closer approximation of the counts that would            50         2,454          0.81         0.81        0.71
be generated from a corpus-wide analysis.                             All         2,382          0.82         0.81        0.73
   Vectors containing counts of content words in preceding            0.60
and following position were concatenated and rank order
                                                                       36         1,328          0.71         0.76        0.39
correlations were computed for every pair of target words.
                                                                       38         2,645          0.73         0.78        0.42
For the current analysis, two words were considered of the
                                                                       40         2,493          0.71         0.74        0.42
same class if the rank order correlation exceeded a certain
                                                                       44         1,132          0.79         0.79        0.50
threshold. That is, the cluster analysis performed by
Redington et al. was omitted for ease of interpretation.               50          708           0.82         0.81        0.69
Results are reported for two levels of the threshold: 0.5 and         All          667           0.83         0.81        0.70
0.6. The accuracy of the resulting classification was scored
against the (most common) grammatical class assigned to           The reason for the mechanism’s initial lack of constraint is
each word based on the morphology (MOR:) line of the              that, early in development, contexts for the target words are
transcripts. The main dependent variables were the overall        derived from short, utterance-final phrases, that may only be
accuracy of the classification and the number of items classed    two words long. This means not only that a limited number
together (number of links). Within class accuracy for nouns       of contexts may be available for a given target word, but also
and verbs was computed separately by dividing the number          that the available contexts may be biased towards preceding
of noun-noun (or verb-verb) pairs over the number of pairs        or following words. Since vectors for preceding and
containing at least one noun (or verb).                           following contexts are concatenated (and zeros are
   Table 1 shows the results of these analyses, averaged over     effectively ignored), this means that the mechanism initially
the 12 different models. The top rows present the data for a      links items on the basis of just preceding or following
threshold of 0.5, and the lower rows a threshold of 0.6.          contexts. With increasing exposure to the input, MOSAIC
Results are reported for 5 developmental stages, ranging from     will represent longer phrases that extend further to the left of
36 to 50 exposures to the input. For completeness, the rows       the utterance. As a result, the mechanism registers not only
labeled ‘all’ provide data for a corpus-wide analysis, that       more contexts, but a better mix of preceding and following
includes all complete utterances. In line with the current        contexts. The mechanism thus becomes more constraining,
practice in MOSAIC, only declarative utterances were              and links (fewer) items, with higher accuracy – in particular
included in the analysis. As can be seen in Table 1, the          for verbs.
mechanism manages to maintain high overall accuracy                  These observations suggest that the practice of
throughout, which tends to be higher in the later stages. The     concatenating vectors for preceding and following contexts
number of links is higher for the threshold of 0.5 than for 0.6,  may be inappropriate from a developmental perspective.
though accuracy scores are not much different.                    They also illustrate the value of embedding the mechanism in
                                                                  an existing developmental model of acquisition as the
                                                                 676

constraint provided by developmental data may lead to                     explores the mechanism’s performance when frequency
insights that remain hidden in corpus-wide analyses.                      information is disregarded completely.
   The data in Table 2 show that a more plausible
developmental pattern results when preceding and following                            Disregarding Token Frequencies
contexts are separated. For this analysis, the rank order                 Table 3 shows the results for 12 new MOSAIC models
correlation was computed separately for preceding and                     trained on the individual corpora for children in the
following contexts, and two items were considered of the                  Manchester corpus. Similarities between words were
same category only if both correlations were sufficiently                 computed on the basis of the Jaccard distance or the rank
high. Table 2 reports results for thresholds of 0.4 and 0.51.             order correlation. The rank order correlation was computed
The pattern of results in Table 2 differs starkly from that in            as in the previous analysis: based on counts for the 150 most
Table 1. Rather than becoming more constraining with                      frequent contexts words in (separated) preceding and
development, the mechanism shows a steady increase in the                 following contexts. The Jaccard distance is defined as the
number of links over development, a pattern that is consistent            length of the intersection divided over the length of the union
with children’s language use becoming more productive with                of two sets. As with the rank order correlation, preceding and
age. The mechanism also achieves higher accuracy, in                      following contexts are considered separately. The Jaccard
particular for verbs – verb accuracy is lower than 60% for just           distance disregards token frequencies, and thus greatly
two out of twelve cells.                                                  reduces the computational complexity of the mechanism, as
                                                                          it is no longer necessary to collect counts for context words.
    Table 2: Performance of Redington et al.’s approach with              Since context is represented as a simple list of word types,
         separated vectors at thresholds of 0.4 and 0.5.                  there is also no need to restrict context words to the most
   Runs       # of links        Acc.          Noun          Verb          frequent words in the corpus. Thus, while in practice most
                                              Acc.           Acc.         context words will be contained in the 150 most frequent
    0.40                                                                  words, in principle, any word can act as a context word.
     36            50           0.65           0.64          0.33
     38          254            0.81           0.85          0.61               Table 3: Performance of Jaccard distance and rank
     40          756            0.81           0.83          0.60                       order at thresholds of 0.2 and 0.45
     44         1,426           0.84           0.84          0.77              Runs         # of       Acc.        Noun          Verb
     50         1,658           0.86           0.84          0.83                          links                    Acc.         Acc.
     All        1,691           0.86           0.85          0.84            Jaccard
    0.50                                                                        36           27        0.78         0.83         0.50
     36            21           0.74           0.73          0.29               38          140        0.83         0.84         0.55
     38          107            0.85           0.89          0.60               40          370        0.87         0.88         0.68
     40          267            0.83           0.83          0.62               44          648        0.89         0.86         0.82
     44          405            0.86           0.85          0.78               50          717        0.91         0.88         0.87
     50          433            0.89           0.87          0.86               All         738        0.91         0.88         0.87
     All         442            0.89           0.87          0.88              Rank
                                                                               order
   The data in Table 2 thus suggest that, when similarity is                    36           35        0.68         0.76         0.46
computed separately for preceding and following contexts,                       38          181         0.8         0.83         0.53
Redington et al.’s mechanism can be applied in a                                40          488        0.82         0.83         0.63
developmental setting, which increases its plausibility as a                    44          846        0.85         0.84         0.77
mechanism that could be employed by language learning                           50          913        0.87         0.85         0.84
children. However, separating preceding and following                           All         925        0.87         0.85         0.85
contexts does not alter the basic statistics over which
correlations are computed: counts for the 150 most frequent                  As in previous analyses, the criterion for considering two
context words. As was argued in the introduction, the need to             words to be of the same category was fixed: 0.2 for the
collect these counts increases the complexity of the                      Jaccard distance and 0.45 for the rank order to allow for a
mechanism considerably. Thus, once a child has identified                 meaningful comparison of accuracy and completeness. As
the most frequent context words, it needs to track their                  can be seen in Table 3, results for the two measures are quite
frequency before and after all target words. However, since               similar, although the rank order measure tends to have lower
the similarity between words is expressed using a non-                    accuracy – so may be more appropriately set at a slightly
parametric measure (i.e. a rank order correlation), much of               higher threshold. Data from Table 2, however, suggests that
the frequency information is discarded. The following section             this may cause completeness to fall below that for the Jaccard
   1 The value of these thresholds differs from those used for the first
analysis, and was chosen to enable a meaningful overall comparison
in terms of accuracy and completeness.
                                                                         677

distance. Nevertheless, the conclusion that can be drawn from
the data in Table 3 is that the Jaccard distance performs as    As can be seen in Table 4, the measures show the highest
well as (if not slightly better than) the rank order measure,   noun richness score for the second or third developmental
and thus that the computational overhead of the mechanism       phase (run 38/40). Noun richness scores subsequently
can be reduced significantly without affecting performance.     decrease for all three measures. This decrease is smallest for
                                                                the concatenated rank order (0.07), intermediate for the
                        Noun Richness                           separated rank order (0.17) and largest for the Jaccard
The previous analyses showed that Redington et al.’s            distance (0.23). Table 5 lists the number of noun-noun and
approach can be applied in a developmental setting provided     verb-verb links for the Jaccard distance and shows that both
similarity is computed for preceding and following contexts     the number of noun and the number of verb links increase
separately. Its computational overhead can also be              with development, but verb links are added at a greater rate
significantly reduced without affecting performance by          in the later stages. Results of these analyses thus confirm that
disregarding frequency information. These changes increase      computing similarity separately for preceding and following
the mechanism’s plausibility as something that could be         contexts aids verb learning, and leads to a more plausible
employed by language-learning children. This plausibility       developmental pattern of noun and verb linkage. This pattern
would be further enhanced if the relative emergence of word     is most pronounced for the computationally simplest measure
classes displayed by the mechanism corresponded to that         - the Jaccard distance.
found in language-learning children. Several authors have
argued that children show evidence of a productive noun                                 Conclusions
category before a productive verb category. The relative        The main conclusion to be drawn from the analyses reported
emergence of noun and verb classes was investigated using a     here is that Redington et al.’s (1998) approach can be adapted
measure of noun richness. Noun richness is a measure of the     in such a way that it provides a developmentally plausible
relative size of the noun and verb category and is computed     account of how children build grammatical word classes on
by dividing the number of noun-noun links over the number       the basis of distributional information. Thus, the mechanism
of noun-noun plus verb-verb links. A model that shows early     is able to yield developmentally plausible results when the
emergence of a noun category and late emergence of a verb       length of the utterances over which it computes statistics is
category would thus show decreasing noun richness. Table 4      gradually increased. In addition, it was shown that, when
shows the development of noun richness for the two models       statistics are computed separately for preceding and
reported in Table 3 – the Jaccard distance at a threshold of    following contexts, the mechanism is able to reach high
0.2 and the rank order correlation at a threshold of 0.45. For  levels of accuracy for both nouns and verbs, even in early
completeness, noun richness for the rank order measure for      stages of development when few contexts may be available.
concatenated contexts (at a threshold of 0.6, data from Table   When similarity is computed for joint (concatenated)
1) are reported as well. Table 5 lists the number of noun-noun  preceding and following contexts, the mechanism is too
and verb-verb links) for the Jaccard distance.                  liberal in the early stages, when it tends to link items based
                                                                on just preceding or following contexts.
     Table 4: Noun richness scores for Jaccard distance,           It was also shown that disregarding token frequencies of
      separated rank order and concatenated rank order.         context words substantially reduces the computational
       Runs        Jaccard,        Rank         Concatened      overhead of the mechanism without affecting its
                      0.2         Order,        Rank Order,     performance. Thus, rather than representing contexts as
                                   0.45              0.6        vectors of counts for context words, the mechanism simply
        36           0.72          0.68             0.87        represents a list of word types that have appeared in
        38           0.80          0.80             0.89        preceding and following position. This latter finding is
        40           0.80          0.82             0.88        significant, as it has been suggested that the computational
        44           0.64          0.71             0.87        complexity of the original mechanism makes it implausible
        50           0.57          0.65             0.82        as a mechanism used by language-learning children.
        All          0.57          0.68             0.81           Finally, it was shown that the mechanism can simulate a
                                                                plausible pattern of noun and verb linkage, with a noun
        Table 5: Number of noun-noun and verb-verb              category emerging ahead of a verb category, as is evident in
                   links for Jaccard distance                   language-learning children. This pattern was least
                 Runs         Nouns          Verbs              pronounced in the variant that employed concatenated
                  36           13.75          3.58              vectors, and most pronounced for the simplest variant
                  38           93.33          16.92             (Jaccard distance), thus providing additional evidence that
                                                                computing similarity separately for preceding and following
                  40          254.83          53.67
                                                                contexts is particularly helpful for classifying verbs.
                  44           353.5         191.25
                                                                   Taken together, the analyses reported here suggest that
                  50          357.75         256.92             distributional analysis can be a viable approach to the
                  All         364.42         266.92             learning of word classes from the earliest stages of
                                                               678

development. Thus, while Redington et al. applied their           Liverpool, for which support of the Economic and Social
method to complete utterances and corpora containing              Research            Council        [ES/L008955/1]           is
millions of word tokens, current analyses suggest that it’s       gratefully acknowledged. This research was supported by
possible to accurately classify words on the basis of just a few  ESRC Grant ES/J011436/1.
contexts, provided these are drawn from both the preceding
and following position. That is: two words that have overlap                               References
in both the preceding and following contexts are very likely      Akhtar, N., & Tomasello, M. (1997). Young children’s
to be of the same category, particularly if these are the only       productivity with word order and verb morphology.
known contexts for these words.                                      Developmental Psychology, 33, 952-965.
   The current approach could be argued to combine the best       Erkelens, M. A. (2009). Learning to categorize verbs and
features of Mintz (2003) and Redington et al.’s (1998)               nouns. Unpublished PhD Thesis, Universiteit van
approaches. The great strength of Mintz’s approach is its            Amsterdam, Amsterdam.
ability to classify (large numbers of) items on the basis of      Freudenthal, D., Pine, J. M., Aguado-Orea, J. & Gobet, F.
very little information (meaning it can be plausibly used by         (2007). Modelling the developmental patterning of
language-learning children), while the probabilistic nature of       finiteness marking in English, Dutch, German and Spanish
Redington et al.’s approach means it is naturally resistant to       using MOSAIC. Cognitive Science, 31, 311-341.
noise, and thus likely to be more accurate. The analyses          Freudenthal, D., Pine, J. M. & Gobet, F. (2010). Explaining
reported here suggest that a system of intermediate                  quantitative variation in the rate of Optional Infinitive
complexity can quickly begin to classify items with high             errors across languages: A comparison of MOSAIC and the
accuracy on the basis of a small number of contexts and              Variational Learning Model. Journal of Child Language,
gradually expand these contexts as they become available.            37, 643-669.
This necessarily means that the number of items classified in     Freudenthal, D., Pine, J.M., Jones, G. & Gobet, F. (2013):
the early stages will be considerably lower than that classified     Frequent frames, flexible frames and the noun-verb
by Mintz’s approach. However, while the great strength of            asymmetry. In: M. Knauf, M. Pauen, N. Sebanz E I.
Mintz’s approach is its ability to classify large numbers of         Wachsmuth (Eds.), Proceedings of the 35th annual meeting
items on the basis of very little information, it is typically       of the Cognitive Science Society. (pp. 2327-2332). Austin,
applied to corpus-wide statistics and not subject to the             TX: Cognitive Science Society.
developmental manipulations that were studied here, and           Freudenthal, D., Pine, J.M., Jones, G. & Gobet. F. (2015).
which provide support for the amended mechanism of                   Simulating the cross-linguistic pattern of Optional
Redington et al. Freudenthal et al. (2013) also suggest that the     Infinitive errors in children’s declaratives and Wh-
number of classified items may be increased by including             questions. Cognition, 143, 61-76.
utterance boundaries as framing elements.                         MacWhinney, B. (2000). The CHILDES project: Tools for
   While disregarding token frequencies for context words            analysing talk (3rd Edition). Mahwah, NJ: Erlbaum.
makes the mechanism considerably simpler (and hence more          Mintz, T. H. (2003). Frequent frames as a cue for
psychologically plausible), it retains many of the desirable         grammatical categories in child directed speech. Cognition,
properties of Redington et al.’s approach. Since similarity is       90, 91-117.
expressed probabilistically, it is naturally resistant to noise   Olguin, R., & Tomasello, M. (1993). Twenty-five-month-old
and appears better placed to deal with languages whose word          children do not have a grammatical category of verb.
order is more flexible than English.                                 Cognitive Development, 8, 245-272.
   The results presented here also illustrate the value of        Redington, M., Chater, N. & Finch, S. (1998). Distributional
evaluating distributional learning mechanisms against                Information: A powerful cue for acquiring syntactic
developmental criteria. Using an established model of                structures. Cognitive Science, 22, 425-469.
language acquisition we show that gradually expanding the         St. Clair, M.C. Monaghan, P., & Christiansen, M.H. (2010).
contexts over which similarity is computed can result in a           Learning grammatical categories from distributional cues.
developmentally plausible pattern of noun and verb linkage           Flexible frames for language acquisition. Cognition, 116,
with high accuracy, even when frequency information is               341-360.
discarded. However, the fact that few contexts are available      Stumper, B., Bannard, C., Lieven, E., & Tomasello, M.
early in development means that distributional statistics can        (2011). Frequent frames in German child-directed speech:
become overly liberal unless similarity in preceding and             A limited cue to grammatical categories. Cognitive
following contexts is considered separately, a finding, that is      Science, 35, 1190-1205.
likely to remain hidden in analyses that compute statistics       Theakston, A. L., Lieven, E. V. M., Pine, J. M. & Rowland,
over complete utterances and large corpora.                          C. F. (2001). The role of performance limitations in the
                                                                     acquisition of Verb-Argument structure: An alternative
                      Acknowledgements                               account. Journal of Child Language, 28, 127-152.
Daniel Freudenthal, Julian Pine, and Fernand Gobet are            Tomasello, M., & Olguin, R. (1993). Twenty-three-month-
members of the International Centre for Language and                 old children have a grammatical category of noun.
Communicative Development (LuCiD) at the University of               Cognitive Development, 8, 451-464.
                                                                 679

