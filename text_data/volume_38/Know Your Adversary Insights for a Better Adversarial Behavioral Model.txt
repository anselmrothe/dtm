         Know Your Adversary: Insights for a Better Adversarial Behavioral Model
                               Yasaman D. Abbasi1, Noam Ben-Asher3, Cleotilde Gonzalez2,
                               Debarun Kar1, Don Morrison2, Nicole Sintov1, Milind Tambe1
             1
               {ydehghan,dkar,sintov,tambe}@usc.edu; 2{coty, dfm2}@cmu.edu; 3nbenash@us.ibm.com
             1
               941 Bloomwalk, SAL 300, University of Southern California, SAL (300), Los Angeles, CA 90089, USA
     2
       Social and Decision Sciences, 5000 Forbes Avenue, BP 208, Carnegie Melon University, Pittsburg, PA 15213, USA
3
  US Army Research Labs & IBM T.J.Watson Research Center, 1101 route 134 Kitchawan Rd, Yorktown Heights, NY 10598
                              Abstract                                theory, such as the Quantal Response behavior model
                                                                      (McFadden 1976, Camerer 2003), but typically a
   Given the global challenges of security, both in physical and      homogeneous adversary population is assumed, and a single
   cyber worlds, security agencies must optimize the use of their     adversary behavior model is prescribed (Kar et al., 2015).
   limited resources. To that end, many security agencies have
   begun to use "security game" algorithms, which optimally plan
                                                                         In contrast to this previous work which often assumes a
   defender allocations, using models of adversary behavior that      homogeneous adversary population with a single behavioral
   have originated in behavioral game theory. To advance our          model, this paper focuses on the heterogeneity in adversary
   understanding of adversary behavior, this paper presents           behavior. Our results are based on the study conducted in
   results from a study involving an opportunistic crime security     Opportunistic Security Games (OSGs). In that experiment,
   game (OSG), where human participants play as opportunistic         (Abbasi et al., 2015) evaluated behavioral game theory
   adversaries against an algorithm that optimizes defender           models assuming a homogeneous adversary population.
   allocations. In contrast with previous work which often
   assumes homogeneous adversarial behavior, our work                 However, our results show that adversaries can be naturally
   demonstrates that participants are naturally grouped into          categorized into distinct groups based on their attack patterns.
   multiple distinct categories that share similar behaviors. We      For instance, while one group of participants (about 20% of
   capture the observed adversarial behaviors in a set of diverse     the population) is seen to be highly rational and taking reward
   models from different research traditions, behavioral game         maximizing action, another group (nearly 50%) is seen to act
   theory, and Cognitive Science, illustrating the need for           in a completely random fashion. We show through
   heterogeneity in adversarial models.
                                                                      experiments that considering distinct groups of adversaries
   Keywords: Human Behavioral Modeling, Opportunistic                 leads to interesting insights about their behavioral model,
   Security Game, Cognitive Models, Heterogonous Adversaries          including the defender strategies being generated based on
                                                                      the learned model.
                          Introduction                                   There are two strands of previous work related to this
   Given the global challenges of security, optimizing the use        paper. First, in behavioral game theory models in security
of limited resources to protect a set of targets from an              games, mostly homogenous adversary models have been
adversary has become a crucial challenge. In terms of                 studied, but some recent research has considered the
physical security, the challenges include optimizing security         heterogeneity of human adversarial behavior. They have
resources for patrolling major airports or ports, screening           achieved it by either assuming a smooth distribution of the
passengers and cargo, scheduling police patrols to counter            model parameters for the entire adversary population (Yang
urban crime (Tambe 2011; Pita et al., 2008; Shieh et al.,             et al., 2014), such as a normal distribution or by utilizing a
2012). The challenge of security resource optimization                single behavioral model for each adversary (Haskell et al.,
carries over to cybersecurity (Gonzalez, Ben-Asher,                   2014; Yang et al., 2014). However, they have not categorized
Oltramari & Lebiere, 2015), where it is important to assist           the adversaries into distinct groups based on their attack
human administrators in defending networks from attacks.              patterns. In this paper, we show that adversaries can be
   In order to build effective defense strategies, we need to         categorized into multiple distinct groups, and each such
understand and model adversary behavior and defender-                 group can be represented by distinct degrees of rationality.
adversary interactions. For this purpose, researchers have               The second strand of related work is with respect to the
relied on the insights from Stackelberg Security Games                exploration of available options, which is an important aspect
(SSGs) to provide ways to optimize defense strategies                 of decision making in many naturalistic situations (Pirolli &
(Korzyk, Conitzer, & Parr, 2010; Tambe, 2011). SSGs model             Card, 1999; Todd, Penke, Fasolo, & Lenton, 2007; Gonzalez
the interaction between a defender and an adversary as a              & Dutt, 2011). In line with previous work (Hills & Hertwig,
leader-follower game (Tambe 2011). A defender plays a                 2010; Gonzalez & Dutt, 2012), in this paper, we show that
particular defense strategy (e.g., randomized patrolling of           there is a negative relationship between exploration behavior
airport terminals) and then the adversary takes an action after       and maximization of rewards. However, in their work, they
having observed the defender’s strategy. Past SSG research            did not contrast behavioral models with cognitive models and
often assumed a perfectly rational adversary in computing the         did not provide insights for behavioral game theory models
optimal defense (mixed or randomized) strategy. Realizing             which we provide. In particular, we study the relationship
the limitation of this assumption, recent SSG work has                between exploration and human reward maximization
focused on bounded rationality models from behavioral game            behavior by parameters of bounded rationality models of
                                                                  1391

                                                  Figure 1. Game Interface
human adversaries. Our observations are also with respect to          There are two moving officers, each protecting three
the security games domain where this kind of relationship          stations. The probability of their presence at each station or
between exploration behavior and maximization of rewards           route, i.e. patrolling strategy, is determined beforehand using
has not been studied before. Furthermore, in our work              an optimization algorithm similar to the one presented in
participants were shown all relevant information, such as          (Zhang et al., 2014). The algorithm optimizes defender
rewards about all the alternative choices, while in earlier        strategies given an opportunistic adversary behavior model.
work participants had to explore and collect information              The stationary coverage probabilities for each station and
about various alternatives.                                        trains are revealed to the players. This means that players can
   To model the different categories of human behavior, we         see the percentage of the time that officers spend on average
provide a family of behavioral game theory and cognitive           at each station and on the train, so they can determine the
models. In behavioral game theory models, we have explored         chance of encountering an officer at a station. However,
models such as the popular Quantal Response (McKelvey &            during the game, the players cannot observe where officers
Palfrey 1995) and the Subjective Utility Quantal Response          are actually located unless they encounter the officer at a
models (Nguyen et al., 2013). These models have been shown         station.
to successfully capture human rationality in decision making          The game can finish either if the player uses up all the 100
in the security games domain (Tambe, 2011). In addition,           units of available time in each game, or the game is randomly
based on the tradition of Cognitive Science, we use a model        terminated after a station visit, which may happen with a 10%
derived from a well-known cognitive theory, the Instance-          probability after each station visit. The random termination
Based Learning Theory (IBLT) (Gonzalez, Lerch, & Lebiere,          encourages players to choose each action carefully, as there
2003), developed to explain human decision making behavior         is a chance the game may terminate after each visit.
in dynamic tasks and used to detect adversarial behaviors             The player’s objective is to maximize his total reward in
(Ben-Asher, Oltramari, Erbacher & Gonzalez, 2015). This is         limited time. Players must carefully choose which stations to
the first such use of cognitive models in security games. In       visit, considering the available information about rewards,
summary, in this paper we build on the existent literature of      officers’ coverage distribution on stations and time to visit
security games and adversary behavior modeling by: (i)             the station.
investigating the heterogeneity of adversarial behavior in an         Procedures. Each participant played eight games in total;
experimental study designed for OSGs, by categorizing              starting with two practice rounds to become familiar with the
adversaries into groups based on their exploration patterns;       game, followed by two validation rounds (two simple
(ii) comparing computational models and showing the impact         versions of the main games), in which the participants were
of heterogeneity on future behavior prediction; and (iii)          presented with obvious choices to confirm they understood
showing the impact of considering heterogeneity on the             the rules and game’s objective, and finally, four main games
defender strategies generated.                                     from which we collect and use the data for our analyses. To
                                                                   ensure that the collected data is independent of the graph
            A behavioral study in an OSG                           structure, the four main games were played on four different
To collect data regarding adversarial behavior from playing        graphs, presented in a random order to the participants. Each
an OSG repeatedly, we used data collected from experiments         graph had six stations with a different route structure and
by (Abbasi et al., 2015) using a simulation of urban crime in      patrolling strategy.
a metro transportation system with six stations (Figure 1).           Participants. The participants were recruited from
                                                                   Amazon Mechanical Turk. They were eligible if they had
Methods                                                            previously played more than 500 games and had an
                                                                   acceptance rate of minimum 95%. To motivate the subjects
Game Design. The players’ goal is to maximize their score
                                                                   to participate in the study, they were rewarded based on their
by collecting rewards (represented by stars in Figure 1) while
                                                                   total score ($0.01 for each gained point) in addition to a base
avoiding officers on patrol. Each player can travel to any
                                                                   compensation ($1). In total, 70 participants took part in the
station, including the current one, by train as represented by
the dashed lines in Figure 1.
                                                               1392

                100%                                                50%         Graph1     Graph2
                                                                                                                                  6
                                       ALL       P1
                                                              % of
        % of Attacks
                                                                                                                                 5.5
                                                                                                                 Utility Rank
                                                                                Graph3     Graph4
                                                                                                                                  5
                                       P2        P3                                                                              4.5
                                       P4        P5                                                                               4
                                                                                                                                 3.5
                                                          partcipants…
                                                                                                                                  3
                                                                                                                                 2.5
                                                                                                                                  2
                       0%                                                                                                        1.5
                              1   2 3 4 5         6                      0%                                                       1
                                                                                                                                       1        2        3
                                  Utility Rank                                Cluster1   Cluster2   Cluster3                                Clusters
      Figure 2: % of Attacks on utility rank                      Figure 3: Clustering Distribution                             Figure 4: Utility Rank by Cluster
game and went through a validation test. Data from 15
participants who did not pass validation were excluded.                                     Figure 3 shows the percentage of participants belonging to
                                                                                         each cluster for four different graphs (Graph 1 to Graph 4).
Human Adversarial Behavior                                                               The percentage of participants belonging to each cluster is
Using data from all the main games, Figure 2 illustrates the                             nearly consistent across all graphs: approximately, 20% in
distribution of attacks (i.e., moves) from all participants                              Cluster1, 30% in Cluster2 and 50% in Cluster3.
(black bars) on stations ranked by the participants’ expected                               In Figure 4, using the data from all the graphs per cluster,
utility (average earning per time) 1, as well as attacks of five                         we show the distribution of utility ranks for each of the three
randomly selected individuals (P1 to P5). To normalize the                               clusters. Interestingly, mobility scores were highly correlated
utility scores among graphs, we have used the ranking of                                 with the utility ranks of the attacked stations (𝑅2 = .85 & 𝑝 <
stations’ utility (utility rank) instead of its absolute value (the                      .01). We observe that participants in Cluster1 (the lowest
highest utility in a graph is ranked 1). The graph illustrates                           mobility scores), attacked stations with the highest utility
significant heterogeneity behavior among individuals (line                               (average utility rank of 1.04). In contrast, participants in
charts), and comparison to the average behavior (bar chart).                             Cluster3 (the highest mobility score), attacked stations that
   Given this heterogeneous behavior, we have applied the                                varied more widely in the utility rank (average utility rank of
Hierarchical Clustering algorithm (Johnson, 1967) on                                     3.3). Participants in Cluster2 also attacked a variety of
different features related to an individuals’ exploration                                stations but were leaning (on average) towards higher utility
behavior and found that mobility score was the best feature to                           rank stations (average utility rank of 1.7). These observations
cluster the participants. The mobility score is a measure of                             provide interesting insights for building defender strategies,
exploration: it is a ratio of the number of movements between                            as illustrated in Section Model Results.
stations over the number of trials (total number of
movements) by a participant in the game. Figure 5: % of                                       Models of Adversarial Behavior in OSG
participants based on the Mobility Score                                                 In what follows, we present a series of models that have been
   shows the distribution of participants based on their                                 proposed recently to represent adversarial behavior.
mobility score for each graph. The mobility score varied
widely (0% to 100%) with a significant proportion of                                     Quantal Response Model (QR)
participants at the two extremes. Informally, the exploration                            Quantal Response models the bounded rationality of a human
behavior seems to fall into three categories: (i) those who did                          player by capturing the uncertainty in the decisions made by
no exploration; (ii) those who always explored and (iii) those                           the player (McKelvey & Palfrey 1995; McFadden 1976).
who engaged in a middling level of exploration. Indeed, the                              Instead of maximizing the expected utility, QR posits that the
clustering algorithm resulted in three groups of participants:                           decision-making agent chooses an action that gives high
participants whose mobility score is less than 10% belong to                             expected utility, with probability higher than another action
Cluster1, participants with 10% to 80% mobility score belong                             which gives a lower expected utility. In the context of OSG,
to Cluster2, and participants whose mobility score is greater                            given the defender’s strategy 𝑠 (e.g., stationary coverage
than 80% belong to Cluster3.                                                             probability at station 𝑖 (𝑠𝑖 ) shown in Figure 1), the probability
                                                                                         of the adversary choosing to attack target 𝑖 when he is in
                       50%
                                      Graph1          Graph2                             target 𝑗 and when the defender’s coverage is 𝑠, 𝑞𝑖,𝑗 (𝑠), is
          % of         30%            Graph3          Graph4                             given by the following equation:
                       10%                                                                                                 𝑒 𝜆 ∗ 𝐸𝑈𝑖,𝑗 (𝑠)
      Participants     -10%    0%
                               8%
                              16%
                              24%
                              32%
                                                                                                          𝑞𝑖,𝑗 (𝑠) =
                                                                                                                       ∑1≤𝑘≤6 𝑒 𝜆 ∗ 𝐸𝑈(𝑘,𝑗)(𝑠)
                              40%
                                                                                         where 𝜆 is his degree of rationality and 𝐸𝑈𝑖,𝑗 (𝑠) is the
                              48%
                              56%
                              64%
                              72%
                              80%
                              88%
                              96%
                                         Mobility Score                                  expected utility of the adversary as given by:
                                                                                                                           𝑟𝑖
                                                                                                        𝐸𝑈𝑖,𝑗 (𝑠) =                ∗ (1 − 𝑠𝑖 )
  Figure 5: % of participants based on the Mobility Score                                                              𝑡𝑖𝑚𝑒 (𝑖, 𝑗)
  1   𝐸𝑈 = (1 − 𝑠𝑡𝑎𝑡𝑖𝑜𝑛𝑎𝑟𝑦 𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒) ∗ 𝑟𝑒𝑤𝑎𝑟𝑑/ 𝑡𝑖𝑚𝑒
                                                                                    1393

where 𝑟𝑖 is the number of stars at station 𝑖, 𝑡𝑖𝑚𝑒 (𝑖, 𝑗) refers                       and more recently. For example, if an unguarded nearby
to time taken to attack station 𝑖 when player is in station 𝑗                          station with many starts (reward) is observed many times, the
                                                                                       activation of this instance will increase, and the probability
Subjective Utility Quantal Response (SUQR)                                             of selecting that station in the next round will be higher.
The SUQR model combines two key notions of decision                                    However, if this instance is not observed often, the memory
making: Subjective Expected Utility, SEU, (Fischhoff et al.,                           of such station will decay with the passage of time (the
1981) and Quantal Response; it essentially replaces the                                parameter d, the decay, is a non-negative free parameter that
expected utility function in QR with the SEU function                                  defines the rate of forgetting). The noise component  is a
(Nguyen et al., 2013). In this model, the probability that the                         free parameter that reflects noisy memory retrieval.
adversary chooses station 𝑖 when he is at station j, when the
defender’s coverage is 𝑠, is given by 𝑞𝑖,𝑗 (𝑠). 𝑆𝐸𝑈𝑖,𝑗 (𝑠) is a                                                 Model Results
linear combination of three key factors. The key factors are                           We aggregated the human data and divided the data set into
(a) 𝑟𝑖 , (b) 𝑠𝑖 , and (c) 𝑡𝑖𝑚𝑒𝑖,𝑗 , 𝑤 = <𝑤𝑟 , 𝑤𝑠𝑡𝑎 , 𝑤𝑡𝑖𝑚𝑒 > denotes                   two groups: training and test datasets. The data from the first
the weights for each decision making feature:                                          three graphs played by the participants were used for training
                                          𝑆𝐸𝑈𝑖,𝑗 (𝑠)
                                                                                       and the last graph played was used for testing the models.
                                        𝑒
                       𝑞𝑖,𝑗 (𝑠) =                       where                          This resulted in 1322 instances in the training set and 500
                                    ∑𝑡′∈𝑇 𝑒 𝑆𝐸𝑈𝑖,𝑡 (𝑆)                                 instances in the test data set.
                   𝑆𝐸𝑈𝑖,𝑗 (𝑠) = 𝑤𝑟 . 𝑟𝑖 + 𝑤𝑠𝑡𝑎 . 𝑠𝑖 + 𝑤𝑡𝑖𝑚𝑒 . 𝑡𝑖𝑚𝑒𝑖,𝑗                     For comparison of different models, we use Root Mean
                                                                                       Squared Error (RMSE) and Akaike Information Criterion
                                                                                       (AIC) metrics. RMSE represents the deviation between
Instance-Based Learning Model                                                          model’s predicted probability of adversary’s attack (𝑝̂ ) and
The IBL model of an adversary in the OSG makes a choice                                the actual proportion of attacks of participants from each
about the station to go to, by first applying a randomization                          station to others (p).
rule at each time step:                                                                                                               1          2
   If draw form U(0,1) >= Satisficing threshold                                            𝑅𝑀𝑆𝐸(𝑝̂ ) =√𝑀𝑆𝐸(𝑝̂ ) where MSE (𝑝̂ ) = ∑(𝑝̂ − 𝑝)
                                                                                                                                      𝑛
              Make a random choice                                                        AIC provides a measure of the relative quality of statistical
   Else;                                                                               models; the lower the value, the better the model. The metric
              Make a choice with the highest Blended value.                            rewards goodness of fit (as assessed by the likelihood
   This rule aims at separating highly exploratory choices                             function), and penalizes overfitting (based on a number of
from those made by the satisficing mechanism of the IBL, the                           estimation parameters).
Blended Value. Satisficing is a parameter of this model. The                              𝐴𝐼𝐶 = 2 ∗ # 𝑚𝑜𝑑𝑒𝑙 ′ 𝑠 𝑝𝑟𝑎𝑚𝑡𝑒𝑟𝑠 − 2 ∗ ln(𝑙𝑖𝑘𝑒𝑙𝑖ℎ𝑜𝑜𝑑)
Blended value V represents value of attacking each station                                Table 1 shows the results on the full data set. The model
(option j ):                                                                           parameters obtained from the training data set were used to
                                          𝑛
                                                                                       make predictions on the test dataset. The prediction errors
                                 𝑉𝑗 = ∑ 𝑝𝑖𝑗 𝑥𝑖𝑗                                        from all the models are relatively similar, even though they
                                         𝑖=1                                           provide different perspectives. QR and SUQR predict the
where 𝑥𝑖𝑗 refers to the value (payoff) of each station (the                            stable state transition probabilities of the attacker while the
number of stars divided by time taken) stored in memory as                             IBL is a process model that captures learning and decision
instance i for the station j, and 𝑝𝑖𝑗 is the probability of                            dynamics over time. We also examine the parameter values
retrieving that instance for blending from memory (Gonzalez                            and performance of the models for each cluster (Table 2).
& Dutt, 2011; Lejarraga et al., 2012) defined as:
                                         𝐴𝑖          𝐴𝑙
                              𝑝𝑖𝑗 = 𝑒 𝜏 ⁄∑ 𝑒 𝜏                                               Table 1: Metrics and Parameter on the full data set
                                                   𝑙
where 𝑙 refers to the total number of payoffs observed for                                   Model               Parameters        RMSE2      AIC
station 𝑗 up to the last trial, and 𝜏 is a noise value defined as                              QR                   0.4188          0.25      3962
𝜎 ∙ √2. The 𝜎 variable is a free noise parameter. The                                         SUQR          <3.97,-2.51,-2.55>3     0.23      3685
activation of instance i represents how readily available the
                                                                                               IBL             <1.4, 3.2,0.3>4      0.24      4359
information is in memory:
                                                                        1 − 𝛾𝑖,𝑡
𝐴𝑖 = 𝑙𝑛       ∑      (𝑡 − 𝑡𝑝 )−𝑑 +     ∑       𝑃(𝑀𝐴𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒 − 1) + 𝜎ln(          )        The value of λ (higher value of λ corresponds to higher
              𝑡𝑝
                                                                          𝛾𝑖,𝑡
                                    𝐴𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒
          𝜖 𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑               𝜖 𝑆𝑖𝑡𝑢𝑎𝑡𝑖𝑜𝑛                                         rationality level) in the QR model decreases significantly
   Please refer to (Anderson & Lebiere, 1998) for a detailed                           from Cluster1 (high value of λ=1.81) to Cluster3 (λ=0). These
explanation of the different components of this equation. The                          findings are consistent with our observation of the utility
Activation is higher when instances are observed frequently
   2 the average is over 288 entries, representing moves from any of                      3 𝑤 = <𝑤𝑟𝑒 , 𝑤𝑠𝑡𝑎 , 𝑤𝑡𝑖𝑚𝑒 >
6 stations to any other station, in four graphs, and for two cases                        4
                                                                                            <noise, decay, Satisficing threshold >
where the player observes the officer or not
                                                                                   1394

ranks of targets chosen by adversaries in each cluster, as             captured in the IBL model by a meta-rule with a Satisficing
shown in Figure 5. This is significant because past research           parameter. This meta-rule is not part of the IBL model, but it
has assumed that all participants either behave based on an            helps to overpass the natural choice by Blending (similar to
average value of λ or that each individual's value of λ can be         the Inertia meta-rule used in Gonzalez & Dutt, 2011). This
sampled from a smooth distribution. In this study, however,            meta-rule was added to explicitly account for random
we show that a significant number of participants (70%: 20%            exploratory behavior observed in the OSG. Therefore, the
in Cluster1 plus 50% in Cluster3) have values of λ which fall          Satisficing parameter helps in selecting between the two
at two extreme ends of the spectrum, thus modeling perfectly           modes of behavior to form the different clusters. The
rational and completely random adversaries respectively.               Satisficing parameter is highest in Cluster1, lower in
   Moreover, considering the fact that SUQR weights indicate           Cluster2, and lowest in Cluster3. Cluster1 results from most
the importance of each attribute to the decision maker, the            choices being made by the IBL’s Blending while Cluster3
results of SUQR parameter extraction for different clusters            results from a random choice. However, this parameter
reveal some interesting points. First, the fact that Cluster1 has      interacts with the IBL model’s decay and noise parameters.
the largest weights for all attributes (in the absolute terms)         For example, in Cluster1, most decisions are made for the
implies that Cluster1 participants are very attracted to the           station with highest Blended value, and there is a need for a
stations with high rewards and highly repelled by high                 high noise value to introduce the variability found in human
defender coverage; which conforms with the observed                    behavior. In contrast, choices in Cluster3 are mostly done
behavior of Cluster1 participants in maximizing the expected           randomly, but in the rare occasions when the model makes
utility. Second, although SUQR outperforms QR overall and              choices based on the highest Blended value, it attempts to
in Cluster2 and 3, QR has lower prediction error (statistically        benefit from recent past experiences (i.e., low decay) and
significant for paired t-test at t (288) = 02.34, p<0.01) on data      with low noise to the decision processes. Therefore,
for Cluster1. This is intuitive if participants are utility            identifying such meta-rules for accounting for explicit
maximizers, this would be captured better when in the QR               descriptive information in addition to the IBL model’s
model. On the other hand, a model like SUQR, which reasons             learning mechanisms is an important aspect of capturing
based on different features of the game capture better the             adversary behavior in security games.
propensity of the participants to switch between stations, and            It is interesting to observe that the behavioral game theory
hence perform better on Clusters 2 and three where                     models provide a significantly better fit in Cluster1,
participants do not have a clear movement pattern. Therefore,          compared to the IBL cognitive model, while the values of
identifying different groups of adversaries gives us valuable          behavioral game theory models are comparable to those of
insight into the types of behavioral models that can be used           the IBL model in Clusters 2 and 3. The IBL model, being a
in different scenarios to generate accurate future predictions         learning model, is poor at making highly accurate decisions
                                                                       with little or no experience as in the OSG study.
            Table 2: Metrics and Parameters on each Cluster               Finally, to demonstrate the impact of considering distinct
                                                                       heterogeneous groups of adversaries, we consider one of the
              Model          Parameters              RMSE     AIC      most recent works (Kar et al., 2015) which advocated the use
                                                                       of a homogeneous adversary model. We show on data
               QR               1.81                 0.01     52
 Cluster                                         3
                                                                       collected from their domain that there is a significant
             SUQR       <7.16,-4.53, -13.43>         0.06     67       difference between the defender strategies generated by a
    1                                        4                         homogeneous (SUQR) and a heterogeneous model which
               IBL         <2.3, 0.9, 0.9>           0.27     238
                                                                       considers three distinct clusters (Bayesian SUQR). The bar
               QR              0.6582                0.28   1023       charts in Figure 6 shows the percentage of change in defender
 Cluster2
                                                 3                     strategy, for example, for target 16, the change in coverage
             SUQR        <5.63,-3.14 ,-4.16>         0.27     927
                                                                       probability from defender strategy generated against a
                                             4
               IBL         <0.9, 1.4, 0.8>           0.30   1821       homogeneous to that against a heterogeneous model is 110%.
 Cluster
              QR                  0                  0.26   2188           % of change in
             SUQR         < 1.9,-1.1,0.13>3          0.23   2007
    3
              IBL         <0.01, 1.8, 0.1>4          0.27   2529            defender’s
                                                                              strategy
  The results from the IBL model suggest that the categories                                             Targets
of adversaries found in this study do not emerge naturally                 Figure 6: Strategy against homogenous & heterogeneous
from the learning process. Indeed, in this study participants
had little opportunities to learn. Instead, it appears that                                   Conclusions
participants either use the information readily available to
them in the OSG and attempt to maximize their gains, or they             Significant research has been conducted towards
explore the choices randomly which may lead them to less               understanding adversary behavior in security games, which
optimal decisions. These two modes of behavior were                    has led to several deployed real-world applications (Tambe
                                                                       2011), such as PROTECT for the protection of major ports in
                                                                    1395

the US (Shieh et al. 2012) and ARMOR for scheduling of               Gonzalez, C., & Dutt, V. (2011). Instance-based learning:
police patrols at major airports such as LAX (Pita et al. 2008).        Integrating decisions from experience in sampling and
Although researchers in security games have relied on                   repeated choice paradigms. Psychological Review.
modeling adversaries via a single homogeneous model, or a            Gonzalez, C., Ben-Asher, N., Martin, J. & Dutt, V. (2015). A
heterogeneous model with a smooth distribution over model               cognitive model of dynamic cooperation with varied
parameters, in this paper, we showed the heterogeneity in               interdependency information. Cognitive Science.
adversary behavior by clustering adversaries into distinct           Gonzalez, C., Ben-Asher, N., Oltramari, A., & Lebiere, C.
groups based on their exploration patterns. Three clusters           (2015). Cognition and Technology. Cyber defense and
emerged based on the adversaries’ exploration patterns, two          situational awareness.
of which fall at two extreme ends of the parameter spectrum,         Gonzalez, C., Lerch, F. J., & Lebiere, C. (2003). Instance-
capturing perfectly rational and completely random behavior.            based learning in dynamic decision making. Cognitive
We also observed that in our OSG domain, exploration is                 Science.
negatively correlated with utility maximization.                     Haskell, W., Kar, D., Fang, F., Tambe, M., Cheung, S., &
  We demonstrate that accounting for the diversity of                   Denicola, L. E. (2014). Robust protection of fisheries with
adversary behavior leads to different model parameters and              compass. IAAI
can provide more accurate predictions of future behavior.            Hills, T. T., & Hertwig, R. (2010). Information Search in
Specifically, we show on data collected based on an                     Decisions From Experience Do Our Patterns of Sampling
Opportunistic Security Game that: (i) QR captures the                   Foreshadow Our Decisions, Psychological Science
behavior of utility maximizing adversaries much better than          Johnson, S. C. (1967). Hierarchical clustering schemes.
SUQR or IBL based models; (ii) the behavioral and cognitive             Psychometrika, Chicago
models have similar prediction performance for adversaries           Kar, D., Fang, F., Delle Fave, F., Sintov, N., Tambe, M.
who do not act in a perfectly rational fashion. Furthermore,            (2015) “A Game of Thrones”: When Human Behavior
we show that considering the heterogeneity in adversary                 Models Compete in Repeated Stackelberg Security Games.
behavior leads to different defender strategies being                   (AAMAS).
generated. The effectiveness of such strategies is an                Korzhyk, D., Conitzer, V., & Parr, R. (2010, July).
important area of future work.                                          Complexity of Computing Optimal Stackelberg Strategies
                                                                        in Security Resource Allocation Games. InAAAI.
                    Acknowledgments                                  Lejarraga, T., Dutt, V., & Gonzalez, C. (2012). Instance-
   This research was partly supported by the Army Research              based learning: A general model of repeated binary choice.
Laboratory under Cooperative Agreement Number                           Journal of Behavioral Decision Making.
                                                                     McFadden, D. L. (1976). Quantal choice analaysis: A survey.
W911NF-13-2-0045 (ARL Cyber Security CRA) to Cleotilde
                                                                        In Annals of Economic and Social Measurement.
Gonzalez. The views and conclusions contained in this
                                                                     McKelvey, R.D., Palfrey, T.R. (1995) Quantal response
document are those of the authors and should not be
                                                                        equilibria for normal form games. Games and Economic
interpreted as representing the official policies, either               Behavior.
expressed or implied, of the Army Research Laboratory or             Nguyen, T.M., Yang R., Azaria A., Kraus S., Tambe M.
the U.S. Government. This research is also supported by                 (2013). Analyzing the Effectiveness of Adversary
MURI grant W911NF-11-1-0332, and award no. 004525-                      Modeling in Security Games, In AAAI.
00001 by US-Naval Research laboratory.                               Pita, J., Jain, M., Ordónez, F., Portway, C., Tambe, M.,
                                                                        Western, C., and Kraus, S. (2008). ARMOR Security for
                         References                                     Los Angeles International Airport. In AAAI
Abbasi, Y. D., Short, M., Sinha, A., Sintov, N., Zhang, Ch.,         Shieh, E., An, B., Yang, R., Tambe, M., Baldwin, C., ... &
  Tambe, M. (2015). Human Adversaries in Opportunistic                  Meyer, G. (2012). Protect: A deployed game theoretic
  Crime Security Games: Evaluating Competing Bounded                    system to protect the ports of the United States. AAMAS.
  Rationality Models. Advances in Cognitive Systems.                 Simon, H. A. (1955). A behavioral model of rational
Anderson, J. R., & Lebiere, C. (1998). The atomic                       choice. The quarterly journal of economics.
  components of thought. Lawrence Erlbaum Associates.                Tambe, M. (2011). Security and Game Theory: Algorithms,
Ben-Asher, N., Oltramari, A, Erbacher, R.F., and Gonzalez,              Deployed Systems, Lessons Learned. Cambridge
  C. (2015). Ontology-based Adaptive Systems of Cyber                   University Press.
  Defense. (STIDS).                                                  Yang, R., Ford, B., Tambe, M., & Lemieux, A. (2014).
Camerer, C.F. (2003) Behavioral game theory, Experiments                Adaptive resource allocation for wildlife protection against
  in strategic interaction. Princeton University Press                  illegal poachers. AAMAS.
Fischhoff, B., Goitein, B., and Shapira, Z. (1981). Subjective       Zhang, C., Jiang, A.X., Short, M.B., Brantingham, J.P. and
  utility function: A model of decision-making. American                Tambe, M. (2014). Defending Against Opportunistic
  Society of Information Science.                                       Criminals: New Game-Theoretic Frameworks and
                                                                        Algorithms Gamesec.
                                                                 1396

