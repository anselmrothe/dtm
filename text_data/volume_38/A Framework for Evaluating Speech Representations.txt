                        A Framework for Evaluating Speech Representations
                        Caitlin Richter1 , Naomi H. Feldman2∗ , Harini Salgado3 , Aren Jansen4
                         1 Department
                                  of Linguistics, University of Pennsylvania, Philadelphia, PA 19104
                  2 Department
                            of Linguistics and UMIACS, University of Maryland, College Park, MD 20742
                                        3 Pomona College, Claremont, CA, 91711
            4 Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore, MD 21211
                                      *Address for correspondence: nhf@umd.edu
                                                                                                              Unnormalized Vowels
                            Abstract                                                           3500
                                                                                               3000
  Listeners track distributions of speech sounds along percep-
  tual dimensions. We introduce a method for evaluating hy-                                    2500
  potheses about what those dimensions are, using a cognitive
                                                                               F2              2000
  model whose prior distribution is estimated directly from speech                                                                        /æ/ (bat)
  recordings. We use this method to evaluate two speaker nor-                                  1500                                       /a/ (bot)
  malization algorithms against human data. Simulations show                                                                              /ɔ/ (bought)
                                                                                               1000
  that representations that are normalized across speakers predict                                                                        /ɛ/ (bet)
  human discrimination data better than unnormalized representa-                                500                                       /e/ (bait)
                                                                                                       400     600   800 1000 1200 1400
                                                                                                                      F1                  /ɝ/ (Bert)
  tions, consistent with previous research. Results further reveal                                                                        /ɪ/ (bit)
  differences across normalization methods in how well each                                    2.5
                                                                                                              Normalized Vowels
                                                                                                                                          /i/ (beet)
  predicts human data. This work provides a framework for                                        2                                        /o/ (boat)
  evaluating hypothesized representations of speech and lays the                               1.5                                        /ʊ/ (book)
                                                                               Normalized F2
  groundwork for testing models of speech perception on natural                                  1                                        /ʌ/ (but)
  speech recordings from ecologically valid settings.                                          0.5                                        /u/ (boot)
                                                                                                 0
  Keywords: speech perception, speaker normalization,                                          -0.5
  Bayesian modeling, approximate inference                                                      -1
                                                                                               -1.5
   Listeners track statistical distributions of sounds in their                                 -2
                                                                                                  -2     -1        0      1       2   3
language. Adults are sensitive to these distributions when per-                                                  Normalized F1
ceiving speech (Clayards, Tanenhaus, Aslin, & Jacobs, 2008),
and infants’ discrimination is influenced by these distributions        Figure 1: Acoustic characteristics of vowels produced in hVd
(Maye, Werker, & Gerken, 2002). Statistical properties of the           contexts by men, women, and children from Hillenbrand et
input can differ depending on the dimensions that listeners             al. (1995), plotted as raw formant frequencies (top) and z-
extract from the speech signal. For example, acoustic char-             scored formant frequencies (bottom). If listeners’ perception
acteristics of vowels are highly variable when represented by           is biased toward peaks in these distributions, these feature
their formant frequencies, but the variability is greatly reduced       spaces make different predictions about listeners’ performance
when they are represented by the z-score of their formant               in perceptual discrimination tasks.
frequencies relative to other vowels by the same speaker (Fig-
ure 1; see also Cole, Linebaugh, Munson, & McMurray, 2010).             approaches to evaluating speech dimensions have instead fo-
Because the distributional characteristics of speech depend so          cused on categorization tasks, in which listeners decide which
heavily on the dimensions used, understanding the dimensions            category a sound belongs to (Apfelbaum & McMurray, 2015;
that listeners extract from the speech signal is a critical part of     McMurray & Jongman, 2011). Discrimination provides sev-
understanding phonetic learning and perception.                         eral advantages over categorization: it is a more fine-grained
   In this paper we introduce a novel approach to evaluating            measure than categorization, and can be reliably measured
hypotheses about the dimensions that support listeners’ per-            in both adults and infants, even when listeners do not have
ception. We adopt a cognitive model of speech perception                well-formed categories for a given set of sounds. In addi-
from Feldman, Griffiths, and Morgan (2009), which predicts              tion, whereas building a categorization model requires speech
that listeners’ perception is biased toward peaks in the acoustic       recordings to have been labeled with phoneme identities, build-
distribution of sounds in their input. This model provides a            ing a discrimination model does not, as we explain below.
formal link between the distribution of sounds in the input                As an initial case study, we examine speech dimensions
and listeners’ discrimination abilities. We measure the input           defined by two speaker normalization techniques: z-scoring,
from a speech corpus and use the model to predict listeners’            which has been proposed in the cognitive science literature
discrimination behavior. Different ways of representing the             (e.g., Lobanov, 1971), and vocal tract length normalization
speech signal imply different distributions of sounds in the            (VTLN), which is widely used in automatic speech recognition
corpus, yielding different predictions about listeners’ discrimi-       (ASR) systems (e.g., Wegmann, McAllaster, Orloff, & Peskin,
nation. We are interested in learning which representations of          1996). We find that both normalization methods yield a robust
speech best predict listeners’ actual discrimination.                   improvement over unnormalized representations in predicting
   We model AX discrimination tasks, in which listeners de-             listeners’ discrimination, consistent with previous research.
cide whether two sounds are acoustically identical. Previous            We also find that VTLN predicts human data better than z-
                                                                     1919

scoring, despite being less effective at eliminating speaker                Table 1: Effect of normalization on K-L divergence.
variability. These results illustrate our method for evaluating                                MFCCS               Z-score          VTLN
these dimensions against discrimination data and provide clues                              (unnormalized)       normalized      normalized
to the dimensions that guide listeners’ perception. Adapting a          Gender KLDiv             7.84                4.58            6.14
                                                                        Dialect KLDiv            4.41                2.09            4.19
cognitive model to operate over speech recordings also lays the
groundwork for testing models of speech perception in more
ecologically valid settings, by enabling cognitive scientists to      capturing information about the spectral envelope, reflecting
make use of the same rich corpus data that is often used by           vocal tract shape. Thus, they capture information similar to
researchers working in automatic speech recognition.                  formant frequencies, but have the advantage that they can be
                                                                      computed automatically from the speech signal, without being
                 Speaker normalization                                subject to the error inherent in automatic formant tracking.1
We begin by characterizing the speech representations tested          Effects of normalization Our method for testing hypoth-
in this paper. Speech contains commingled effects of linguis-         esized representations of speech against human perception
tic, paralinguistic, and purely physical sources of variation.        relies on the idea that different representations of speech yield
The goal of speaker normalization is to find representations          different distributions of sounds in the input. To examine
that diminish some of the variability in the speech signal, like      the distribution of sounds in the input, we computed MFCCs,
that from the speaker’s body, while retaining task-appropriate        z-scored MFCCs, and MFCCs with VTLN from vowel record-
information such as the variability that represents different         ings in the Vowels subcorpus of the Nationwide Speech Project
phonemes. While some models have questioned whether lis-              (NSP; Clopper & Pisoni, 2006). This corpus contains ten dif-
teners normalize across speakers (Johnson, 1997), most ev-            ferent vowels pronounced in the context hVd (hid, had, etc.)
idence suggests some degree of normalization. Normalized              by 5 female and 5 male speakers from each of 6 dialect regions
representations have been found to improve phonetic catego-           of the United States. Each of these 60 speakers repeats each
rization (Cole et al., 2010; Nearey, 1978), increase a catego-        of the 10 hVd words 5 times, for a total of 3000 hVd tokens
rization model’s match with human behavior (Apfelbaum &               balanced across vowel, gender, and dialect.2
McMurray, 2015; McMurray & Jongman, 2011), and improve                   We characterize the effects of each normalization method
the performance of speech recognizers (Wegmann et al., 1996;          on the distribution of vowels in the NSP corpus by computing
Povey & Saon, 2006). We test the effects of two specific meth-        symmetrized Kullback-Leibler divergence, a measure of differ-
ods for speaker normalization, z-scoring and vocal tract length       ence between two probability distributions (Wang, Kulkarni,
normalization, in a cognitive model of vowel discrimination.          & Verdú, 2006). Lower K-L divergence indicates greater simi-
   Within-speaker z-scoring (Lobanov, 1971) has been sug-             larity between two distributions. We estimated K-L divergence
gested for descriptive sociolinguistic research (Adank, Smits,        across gender and across dialect (averaged over 15 pairwise
& Hout, 2004), as it highlights learned linguistic content while      comparisons of 6 dialect regions). Male and female speakers
removing speaker-body confounds from vowel formants. A                differ in their vocal tract lengths, and thus normalization algo-
related manipulation, linear regression, has also been shown to       rithms would be expected to increase similarity between their
improve cognitive models of fricative perception (Apfelbaum           vowel productions. Dialects also differ in their pronunciations
& McMurray, 2015; McMurray & Jongman, 2011).                          of different vowels; although this variation is not related to
                                                                      vocal tract length, it may nevertheless be impacted by normal-
   Vocal tract length normalization (VTLN) is a technique de-
                                                                      ization algorithms that seek to neutralize speaker differences.
veloped for automatic speech recognition. VTLN compensates
                                                                         K-L divergence between genders and between dialect pairs
for speaker differences in vocal tract length by stretching or
                                                                      is highest in MFCCs with no speaker normalization, reflecting
compressing the frequency axis of the productions of each
                                                                      the effects these factors have on the original acoustic signal
speaker (Wegmann et al., 1996). The aim of VTLN is to ad-
                                                                      (Table 1). Both VTLN and z-scoring reduce K-L divergence
just the corpus so that it is as if all the speakers had identical
                                                                      between genders, as predicted, so that male and female speak-
vocal tract lengths. VTLN is widely successful in ASR sys-
                                                                      ers saying the same vowel appear more similar after either of
tems, where it substantially decreases the word error rate (e.g.,
                                                                      these normalizations than they are in unnormalized MFCCs.
Giuliani, Gerosa, & Brugnara, 2006). In performing this nor-
                                                                      Z-scoring using all 10 NSP vowels also reduces K-L diver-
malization, we use a procedure from Wegmann et al. (1996)
                                                                      gence between dialect pairs. In contrast, VTLN matching
adapted for an unsupervised setting, which selects frequency
adjustments for speakers on the basis of their /i/ productions            1 Z-scoring has previously been applied to formant frequencies,
by maximising the similarity of all /i/ tokens across speakers.       but we show in the next section that it is also effective at normalizing
                                                                      across speakers when applied to MFCCs. Vowel-intrinsic normal-
   We apply both normalization methods to vowels that are             ization methods such as formant ratios were not tested here because
represented by mel frequency cepstral coefficients (MFCCs;            they are not straightforward to apply to MFCCs.
Davis & Mermelstein, 1980). MFCCs are widely employed                     2 While this corpus does not correspond exactly to listeners’ ex-
as an input representation in ASR systems (although we do             perience with language, conducting initial simulations with a corpus
                                                                      of vowels in neutral contexts allows us to investigate algorithms for
not implement an ASR system here). MFCCs are a 12-                    speaker normalization while sidestepping issues of how listeners
dimensional vector that describe a timepoint of speech by             generalize across phonological contexts.
                                                                  1920

                                                 Tested on familiar speakers                                         Tested on new speakers
                                      −200                                                                −200
                                      −300                                                                −300
                                      −400                                                                −400
                     Log likelihood                                                      Log likelihood
                                      −500                                                                −500
                                      −600                                                                −600
                                                                           MFCC                                                               MFCC
                                      −700                                                                −700
                                                                           z−scored                                                           z−scored
                                                                           VTLN                                                               VTLN
                                      −800                                                                −800
                                             2    4       6       8       10    12                               2   4       6       8       10    12
                                                   Feature dimensionality                                             Feature dimensionality
Figure 2: Model fit to human data, when generalizing to familiar speakers (left) and new speakers (right). Results for
1-dimensional features are not shown on this scale as they are extremely poor, with log likelihoods of −1300 to −1600.
across speakers on the basis of /i/, which differs little across                            corrupted by a noise process defined by a Gaussian likelihood
the dialects (Clopper & Pisoni, 2006), has minimal effects                                  function, p(S|T ) = N (T, ΣS ). Both T and S are d-dimensional
on dialect K-L divergence: cross-dialect differences remain                                 vectors, where d is the dimensionality of the feature space. In
nearly as distinct after VTLN. Overall, when z-scoring by                                   our simulations, the feature space is defined by either MFCCs,
speaker K-L divergence is lowest (though nonzero; gender                                    z-scored MFCCs, or MFCCs with VTLN.
and dialect information remains in the representation); VTLN                                   Listeners hear S and reconstruct T by drawing a sample
removes somewhat less of the gender and dialect variation.                                  from the posterior distribution, p(T |S) ∝ p(S|T )p(T ). We
   In summary, MFCCs, z-scored MFCCs, and MFCCs with                                        refer to a listener’s sample from the posterior distribution as
VTLN each correspond to different distributions of vowels in                                a percept. The percept is a continuous acoustic value, rather
the input, with z-scoring being the most effective at increasing                            than a category label; this is consistent with a large body of
the overlap between the distributions of vowels spoken by dif-                              evidence showing that listeners recover fine-grained acoustic
ferent speakers. The next section describes a cognitive model                               detail from the speech signal (e.g., Pisoni & Tash, 1974).
that uses these input distributions to quantitatively predict                                  The model can be used to predict listeners’ discrimination
listeners’ vowel discrimination in the laboratory.                                          behavior in the laboratory. In AX discrimination tasks, listen-
                                                                                            ers hear two stimuli and decide whether they are acoustically
                     Cognitive model                                                        identical. The model assumes that for each stimulus, listen-
The model of discrimination we adopt has been shown to                                      ers sample a percept from their posterior distribution, p(T |S).
accurately predict listeners’ discrimination of both vowels                                 They then compute the distance between their percepts of the
and consonants (Feldman et al., 2009; Kronrod, Coppess, &                                   two stimuli and compare it to a threshold ε. If the percepts
Feldman, 2012), but has not yet been implemented directly on                                are separated by a distance less than ε, listeners respond same;
speech recordings. The model formalizes speech perception as                                otherwise they respond different. Given these assumptions,
an inference problem. Listeners perceive sounds by inferring                                the proportion of same responses for two stimuli, S1 and S2 , is
the acoustic detail of a speaker’s target production through                                predicted to follow a binomial distribution whose parameter is
a noisy speech signal. Because listeners need to correct for                                the probability that the percepts for the two sounds are within
uncertainty in the speech signal, their perception is biased                                a distance ε of each other, p(|T1 −T2 |<ε|S1 , S2 ). The noise
toward acoustic values that have high probability under their                               covariance ΣS and the response threshold ε are free parameters
prior distribution. This creates a dependency between the                                   which are optimized to best predict discrimination data.
listeners’ prior distribution and their perception of sounds.                                  Whereas previous work with this model has estimated lis-
   Formally, speakers and listeners share a prior distribution                              teners’ prior distribution from perceptual categorization data,
over possible acoustic values that can be produced in the lan-                              here we estimate listeners’ prior distribution directly from
guage, p(T ). Prototypical sounds in the language have highest                              production data in the NSP corpus; we make the simplify-
probability under this distribution, but the distribution is non-                           ing assumption that the prior distribution directly mirrors the
zero over a wide range of acoustic values, corresponding to                                 distribution of sounds in the input. Previous work has also
all the ways in which speech sounds might be realized. When                                 assumed that listeners’ prior distribution is a mixture of Gaus-
producing a sound, speakers are assumed to sample a target                                  sians, with one Gaussian distribution corresponding to each
production T from this distribution. The target production                                  phonetic category. We avoid making this assumption by using
can carry meaningful information aside from category identity,                              an exemplar-based implementation of the model.
such as dialect information or coarticulatory information about                                Shi, Griffiths, Feldman, and Sanborn (2010) showed that
upcoming sounds, making its acoustic value something that                                   exemplar models provide a way of approximating Bayesian
listeners wish to recover. The stimulus S heard by listeners                                inference. Specifically, exemplar models implement a form
is similar to the target production T , but is assumed to be                                of approximate inference known as importance sampling. To
                                                                                      1921

use importance sampling for our simulations, we need a set             midpoint. Although speech recognition systems typically use
of exemplars {T (i) } that are sampled from listeners’ prior dis-     12 MFCC dimensions, we additionally include simulations
tribution p(T ). We assume the vowels in the NSP constitute            that omit subsets of the higher dimensions, as the lower di-
this set of exemplars. We then weight each exemplar by its             mensions are better able to capture information from formants
likelihood with respect to a stimulus S, p(S|T (i) ), and select       traditionally used to describe vowel quality.
an exemplar according to its weight. An exemplar from the                 The NSP is an ideal case for the z-scoring normalization,
corpus sampled in this way behaves as though it were drawn             because each speaker says the same tokens the same number
from the posterior distribution p(T |S). This method does not          of times. However, MFCCs for the stimulus ‘speaker’ were
require us to know a parametric form for the prior distribu-           only available for the /i/-/e/ vowel continuum. Because the
tion p(T ), because the prior distribution is represented only         stimuli were originally synthesized according to average for-
through exemplars. In addition, it does not require the exem-          mant values for male speakers, we handled this missing data
plars from the corpus to have category labels, as the weights          by normalizing the stimuli according to average z-scoring fac-
p(S|T (i) ) are defined by the model’s Gaussian likelihood func-       tors of the 30 male NSP speakers. Due to the reliance of our
tion, corresponding to the speech signal noise.                        VTLN procedure on only /i/ tokens, this normalization was
   We estimate the model’s probability of responding same on           straightforward to apply to the stimuli.
each trial by using importance sampling to obtain 100 pairs of
                                                                       Fitting parameters The NSP corpus was divided into two
percepts corresponding to the pair of stimuli presented in that
                                                                       balanced, equally sized sets of exemplars. Two methods were
trial. The proportion of these pairs of percepts that are within
                                                                       used for dividing the corpus. In one case, the two halves
distance ε of each other provides an estimate3 of the probabil-
                                                                       contained different exemplars from the same speakers, while
ity of responding same on that trial. We use these probabilities
                                                                       in the other case the two halves contained exemplars from
to predict listeners’ actual same-different responses in an exper-
                                                                       different speakers (balanced for gender and dialect region).
iment. We implement the model several times with different
                                                                       Each division of the corpus was created once, and used for
speech representations: MFCCs, z-scored MFCCs, or MFCCs
                                                                       simulations with all speech feature types.
with VTLN. Comparing model likelihoods across the three
                                                                          In each simulation, one set of exemplars was used to fit
speech representations allows us to ask which representations
                                                                       parameters: the response threshold ε and the noise variance ΣS
best predict listeners’ discrimination.
                                                                       (constrained to be diagonal) were selected using Markov chain
                          Simulations                                  Monte Carlo to maximize model likelihoods given perceptual
                                                                       data.4 The other set of exemplars was used to compute model
Simulations implemented the perceptual model with normal-              likelihoods at test. The roles of the two sets of exemplars were
ized and unnormalized representations, comparing model pre-            then reversed, resulting in 2-fold cross-validation. Each set of
dictions to human discrimination data. To the extent that              exemplars served as a test set for 10 simulations. Points and er-
different representations of speech yield different distributions      ror bars in Figures 2 and 4 represent means and standard error
of sounds in a corpus, they should make different predictions          calculated across all 20 simulations; the relatively small error
about the biases that listeners will exhibit in a speech percep-       bars indicate that results were consistent across replications.5
tion experiment. Representations that yield more accurate
predictions can be assumed to contain information more simi-           Results Model performance is assessed by computing the
lar to the dimensions that listeners use in speech perception.         log likelihood of the model, given the human data. Higher log
                                                                       likelihoods indicate a closer match to perceptual data.
Human perceptual data We use vowel discrimination data                    Previous work with categorization models suggests that
from an AX discrimination task conducted by Feldman et al.             normalized representations are more consistent with listeners’
(2009) in quiet listening conditions. Twenty participants heard        perception than unnormalized representations (Apfelbaum &
a continuum of 13 isolated vowels that were synthesized to             McMurray, 2015; McMurray & Jongman, 2011) . We replicate
simulate a male speaker. First and second formants of these            this result with our method: in almost all cases, unnormalized
stimuli ranged linearly in mels from /i/ (as in ‘beet’) to /e/ (as     MFCCs have the lowest likelihoods among the three represen-
in ‘bait’). Participants heard all ordered pairs of stimuli and        tations tested (Figure 2). We also find that MFCCs normalized
judged whether each pair was acoustically identical. MFCCs,            by VTLN outperform z-scored MFCCs, although z-scoring
z-scored MFCCs, and MFCCs with VTLN computed from                      within speakers neutralizes more inter-speaker variation as
these thirteen stimuli serve as input to the model, as the stim-       measured by K-L divergence (Table 1). Thus, the better per-
ulus S. Model predictions are then compared with listeners’            formance of the VTLN models is not merely an artifact of
same-different responses to each pair of stimuli.                      acoustic similarity among vowels in the corpus; this also may
Speech representations The exemplars that serve as the                 imply that human representations underlying vowel discrimi-
model’s prior distribution are vowels from the Vowels subcor-              4 Z-scoring and VTLN were applied prior to parameter fitting;
pus of the NSP. Vowels were represented either as MFCCs,               therefore neither adds free parameters to the model, and no optimiza-
z-scored MFCCs, or MFCCs with VTLN, computed at their                  tion to fit human data is involved in their application.
                                                                           5 Numerical values fit for model parameters were also consistent
    3 We use add-one smoothing to compute this estimate.               across the 20 replications for each speech feature type.
                                                                   1922

                                                                         use of female exemplars, indicating that this representation
                                                                         does not recognize very much similarity between male and
                                                                         female speakers saying the same vowel. Models with z-scored
                                                                         features are closest to sampling 50% female exemplars; this
                                                                         confirms that the z-scored representation is highly effective at
      Human data                            MFCC                         neutralizing difference between speakers of different genders
                                                                         (Table 1), although it is not the representation that gives the
                                                                         best match to human perceptual performance (Figure 2).
                                                                            While simulations with 2 through 6 dimensions consistently
                                                                         treated the experimental stimuli as being most similar to high
                                                                         front vowels in the corpus, simulations with higher orders of
                                                                         cepstral coefficients did not (Figure 4), reinforcing the impor-
        z-scored                            VTLN                         tance of the lower MFCC dimensions in capturing listeners’
                                                                         perception of these stimuli. We suspect that this behavior
Figure 3: Confusion matrices for how often each pair of stim-            emerged due to the artificial synthesis of the experimental
uli is judged to be the same (black) vs. different (white) by            stimuli, which resulted in these high front vowel stimuli being
humans and by 4-dimensional models tested on familiar speak-             most similar to low back vowels from the corpus in two of
ers. Axis labels denote stimulus numbers from an AX trial.               the higher MFCC dimensions. This can cause the model to
                                                                         perceive stimuli as low back vowels in cases where it general-
nation do not completely normalize across speakers. Examples             izes along those dimensions. This underscores the difficulty
of same-different responses from models using each type of               of bringing together ecologically valid speech corpora with
feature are shown in Figure 3, together with human data.                 the type of controlled stimuli typically used in experimental
   Models using speech representations of one dimension are              settings, and illuminates areas in which future research may
always extremely poor; the first cepstral coefficient on its             provide insight by addressing these challenges.
own does not provide enough information for the perceptual                  Finally, we can compare the log likelihoods from Figure 2
task. In some simulations, such as for z-scored features, test           to a benchmark showing ideal model performance. Feldman et
likelihood decreases with higher numbers of dimensions be-               al. (2009) estimated listeners’ prior distributions for the /i/ and
yond a certain point, likely due to overfitting of parameters to         /e/ categories from perceptual categorization data, rather than
particular exemplars.6 It appears that the lower MFCC dimen-             from speech recordings. Using their estimate, the model yields
sions, particularly the second dimension, contain information            average log likelihood of -233, somewhat higher than those
relevant to listeners’ discrimination of an /i/-/e/ continuum.           obtained here. A corpus-based model should approach this
   Although the NSP’s speaker and phoneme labels are not                 value as the distribution of sounds in the corpus approaches
used by our cognitive model, we take advantage of this in-               the prior distribution that listeners use in perceptual tasks.
formation in analyses for insight on the types of exemplars
sampled as percepts by the model. Across all models, the                                           Discussion
100 percepts drawn from the posterior distribution for each
                                                                         In this paper a novel method was introduced for evaluating hy-
stimulus contained on average 30 different exemplars, indicat-
                                                                         potheses regarding the dimensions that guide listeners’ speech
ing that a number of exemplars (from different speakers) are
                                                                         perception. A cognitive model of AX discrimination was im-
treated as linguistically similar to each other. As a measure of
                                                                         plemented directly on speech recordings and used to evaluate
model quality and interpretability, we examine the identity of
                                                                         two speaker normalization methods. Both normalization meth-
the exemplars sampled by the model during each perceptual
                                                                         ods improved the model’s fit to perceptual data, consistent with
judgment (Figure 4). The percentage of samples that belong to
                                                                         previous research. Between the two normalization methods,
the classes of vowels along the /i/-/e/ continuum (NSP ‘heed’,
                                                                         VTLN outperformed z-scoring, despite being less effective at
‘hayed’, and ‘hid’ tokens; henceforth referred to as high front
                                                                         collapsing gender and dialect differences.
vowels) gives information on model quality, because all the
stimuli are perceived by US English speakers as falling along               The advantage of VTLN over z-scoring in modeling human
this continuum. The proportion of times a model samples                  perceptual data suggests that VTLN allows the model to gen-
female exemplars to recover the linguistic target for this ex-           eralize across speakers in a way that is more similar to human
periment’s male-speaker stimuli gives an indication of the               perceivers. For example, listeners may track statistical distribu-
model’s ability to generalize linguistic content across genders.         tions of speech in ways that allow them to collapse across gen-
   Models using unnormalized MFCCs tend to make the least                der while retaining differences across dialects. Nevertheless,
                                                                         a prior distribution estimated from perceptual categorization
   6 The corresponding training likelihoods for z-scored dimensions      data still outperforms the prior distributions measured from
remained stable or even increased at higher numbers of dimensions.       a corpus; none of the three representations tested here match
Similarly, the low likelihood observed at six dimensions for MFCCs
was due to several runs that achieved high likelihood on the training    human perception exactly. Instead, these simulations provide
exemplars and low likelihood on the test exemplars.                      initial clues to the dimensions that guide listeners’ perception
                                                                     1923

                                                                     Vowel quality                                                    Gender distribution
                                                100                                                                        60                                MFCC
                                                                                                                                                             z−scored
                                                 90                                                                                                          VTLN
                       % high front exemplars
                                                                                                                           50
                                                                                                      % female exemplars
                                                 80
                                                 70                                                                        40
                                                 60                                                                        30
                                                 50
                                                          MFCC                                                             20
                                                 40       z−scored
                                                          VTLN                                                             10
                                                 30
                                                 20                                                                         0
                                                      2      4       6       8       10   12                                    2   4       6       8       10    12
                                                              Feature dimensionality                                                 Feature dimensionality
Figure 4: Secondary evaluations, showing how often the stimuli were correctly perceived as high front vowels (left) and how
often the model based perceptions on female exemplars (right). Data are averaged across the familiar and new speaker test cases,
which were very similar on these measures.
while also allowing us to validate a novel method for assessing                                   Cole, J., Linebaugh, G., Munson, C. M., & McMurray, B. (2010).
speech representations against human data.                                                               Unmasking the acoustic effects of vowel-to-vowel coarticula-
                                                                                                         tion: A statistical modeling approach. Journal of Phonetics,
   Our method provides a general tool for investigating propos-                                          38, 167-184.
als regarding the dimensions that guide listeners’ perception.                                    Davis, S. B., & Mermelstein, P. (1980). Comparison of parametric
The model’s prior distribution can in principle be estimated                                             representations for monosyllabic word recognition in continu-
                                                                                                         ously spoken sentences. Proceedings of the IEEE, 357-366.
from any speech corpus, and does not require phoneme labels.                                      Feldman, N. H., Griffiths, T. L., & Morgan, J. L. (2009). The influ-
This ability to make use of unlabeled corpora provides an ad-                                            ence of categories on perception: Explaining the perceptual
vantage over evaluation methods that rely on categorization.                                             magnet effect as optimal statistical inference. Psychological
                                                                                                         Review, 116, 752-782.
In addition, the importance sampling approximation used here                                      Giuliani, D., Gerosa, M., & Brugnara, F. (2006). Improved automatic
can be implemented on any speech representation for which a                                              speech recognition through speaker normalization. Computer
likelihood function p(S|T ) can be computed between stimuli                                              Speech & Language, 20(1), 107–123.
                                                                                                  Hillenbrand, J., Getty, L. A., Clark, M. J., & Wheeler, K. (1995).
and exemplars and for which a distance metric between exem-                                              Acoustic characteristics of American English vowels. Journal
plars in the corpus can be compared to a threshold ε, and thus                                           of the Acoustical Society of America, 97, 3099-3111.
can be used even for representations that lack a fixed or finite                                  Johnson, K. (1997). Speech perception without speaker normaliza-
                                                                                                         tion: An exemplar model. In K. Johnson & J. W. Mullennix
set of dimensions. This flexibility makes it a promising tool                                            (Eds.), Talker variability in speech processing (p. 145-165).
for exploring cross-linguistic differences in listeners’ sensitiv-                                       New York: Academic Press.
ity to perceptual dimensions, as well as for evaluating theories                                  Kronrod, Y., Coppess, E., & Feldman, N. H. (2012). A unified model
                                                                                                         of categorical effects in consonant and vowel perception. Pro-
of dimension learning against children’s discrimination data.                                            ceedings of the 34th Annual Conference of the Cognitive Sci-
   To our knowledge, this is also the first time the model from                                          ence Society.
Feldman et al. (2009) has been implemented on speech record-                                      Lobanov, B. M. (1971). Classification of Russian vowels spoken
                                                                                                         by different speakers. Journal of the Acoustical Society of
ings. Modifying models to operate over corpora of natural                                                America, 49, 606-608.
speech allows them to make use of ecologically valid datasets,                                    Maye, J., Werker, J. F., & Gerken, L. (2002). Infant sensitivity to
and can thus facilitate a richer understanding of the way in                                             distributional information can affect phonetic discrimination.
                                                                                                         Cognition, 82, B101-B111.
which listeners’ perception is shaped by their environment.                                       McMurray, B., & Jongman, A. (2011). What information is necessary
Acknowledgments We thank Josh Falk for help piloting the model,                                          for speech categorization? harnessing variability in the speech
Phani Nidadavolu for help computing speech features, and Hynek                                           signal by integrating cues computed relative to expectations.
Hermansky, Bill Idsardi, Feipeng Li, Vijay Peddinti, Amy Wein-                                           Psychological Review, 118, 219-246.
berg, and the UMD probabilistic modeling reading group for helpful                                Nearey, T. M. (1978). Phonetic feature systems for vowels (Vol. 77).
comments. This work was supported by NSF grant BCS-1320410.                                              Indiana University Linguistics Club.
                                                                                                  Pisoni, D. B., & Tash, J. (1974). Reaction times to comparisons
                                                                                                         within and across phonetic categories. Perception and Psy-
                                                 References                                              chophysics, 15, 285-290.
Adank, P., Smits, R., & Hout, R. v. (2004). A comparison of                                       Povey, D., & Saon, G. (2006). Feature and model space speaker
      vowel normalization procedures for language variation re-                                          adaptation with full covariance Gaussians. Proceedings of
      search. Journal of the Acoustical Society of America, 116,                                         Interspeech.
      3099-3107.                                                                                  Shi, L., Griffiths, T. L., Feldman, N. H., & Sanborn, A. N. (2010).
Apfelbaum, K. S., & McMurray, B. (2015). Relative cue encoding                                           Exemplar models as a mechanism for performing Bayesian
      in the context of sophisticated models of categorization: Sepa-                                    inference. Psychonomic Bulletin and Review, 17, 443-464.
      rating information from categorization. Psychonomic Bulletin                                Wang, Q., Kulkarni, S. R., & Verdú, S. (2006). A nearest-neighbor ap-
      and Review, 22, 916-943.                                                                           proach to estimating divergence between continuous random
Clayards, M., Tanenhaus, M. K., Aslin, R. N., & Jacobs, R. A.                                            vectors. Convergence, 1000(1), 11.
      (2008). Perception of speech reflects optimal use of prob-                                  Wegmann, S., McAllaster, D., Orloff, J., & Peskin, B. (1996).
      abilistic speech cues. Cognition, 108, 804-809.                                                    Speaker normalization on conversational telephone speech.
Clopper, C. G., & Pisoni, D. B. (2006). The nationwide speech                                            Proceedings of the IEEE International Conference on Acous-
      project: A new corpus of American English dialects. Speech                                         tics, Speech, and Signal Processing, 339-341.
      Communication, 48, 633-644.
                                                                                               1924

