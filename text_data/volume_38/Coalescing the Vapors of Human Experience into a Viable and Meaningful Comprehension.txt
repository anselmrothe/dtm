                                  Coalescing the Vapors of Human Experience
                                 into a Viable and Meaningful Comprehension
                  Tomer D. Ullman (tomeru@mit.edu)                                Max Siegel (maxs@mit.edu)
              Department of Brain and Cognitive Science, MIT            Department of Brain and Cognitive Science, MIT
                 Joshua B. Tenenbaum (jbt@mit.edu)                  Samuel J. Gershman (gershman@fas.harvard.edu)
              Department of Brain and Cognitive Science, MIT              Department of Psychology, Harvard University
                              Abstract                                  Schulz called for both empirical and theoretical research on
                                                                        the issue. On the theoretical side, everyday creative thought
   Models of concept learning and theory acquisition often in-
   voke a stochastic search process, in which learners generate         poses a problem for computational models that have been de-
   hypotheses through some structured random process and then           veloped to capture more radical theory change (Goodman,
   evaluate them on some data measuring their quality or value.         Ullman, & Tenenbaum, 2011; Ullman, Goodman, & Tenen-
   To be successful within a reasonable time-frame, these mod-
   els need ways of generating good candidate hypotheses even           baum, 2012). Briefly put, these models see humans as rea-
   before the data are considered. Schulz (2012a) has proposed          soning over a hypothesis space (“theory space”) that contains
   that studying the origins of new ideas in more everyday con-         explanations and concepts. People do not know in advance
   texts, such as how we think up new names for things, can pro-
   vide insight into the cognitive processes that generate good hy-     what the right concepts and theories are, so they must stochas-
   potheses for learning. We propose a simple generative model          tically search through these large (potentially infinite) spaces,
   for how people might draw on their experience to propose             adjusting and discarding their concepts as they go. If a newly
   new names in everyday domains such as pub names or action
   movies, and show that it captures surprisingly well the names        proposed concept or theory better fits the data, and is more
   that people actually imagine. We discuss the role for an anal-       likely under a general prior favoring such things as ‘simplic-
   ogous hypothesis-generation mechanism in enabling and con-           ity’, then that newly proposed concept will be accepted.
   straining causal theory learning.
   Clerk: Occupation?
                                                                           Such a process might make sense for theory-change that
   Comicus: Stand-up philosopher. I coalesce the vapors of hu-          takes years to unfold, but surely (Schulz argues) it is under-
   man experience into a viable and meaningful comprehension.           constrained when it comes to everyday thought. The search
   Clerk: Oh, a bullshit artist!                                        spaces are too large, and the process does not take into ac-
        (History of the World, Part 1. Dir. Mel Brooks)
                                                                        count the specific constraint of a task. For example, suppose
                                                                        a friend asks you to come up with a name for their hip new
                           Introduction                                 Thai restaurant, located near a cluster of start-ups. You may
                                                                        never have been faced with such a problem before, but after
How do people come up with new concepts, causal models or
                                                                        some thought you might come up with “Thai-Tech”. It is not
theories, to make sense of their world? Whether it is scien-
                                                                        a particularly good name, but it is at least a relevant one. It
tists trying to explain the natural world with formal theories
                                                                        is better than “Mummified Ragdoll Fifteen” or “Croatian de-
of physics or chemistry, or children and adults trying to ex-
                                                                        light”, or any of the other infinite combinations of words that
plain their experience with common-sense theories such as
                                                                        language affords. Such proposals would not just be rejected
folk biology or folk psychology, the question of how funda-
                                                                        if proposed, they would not be thought of in the first place.
mentally new ways of thinking unfold over time and change
in response to evidence has long been of interest to cogni-                That people do not waste time with nonsense (in the sense
tive science (Carey, 2009; Gopnik & Wellman, 2012; Schulz,              of proposing gibberish answers used above) seems almost
2012b).                                                                 trivial, and suggesting a concept-production algorithm that
   This paper is about the more everyday aspects of this pro-           runs through all combinations in the English language for
cess: New thoughts of on-the-fly explanations and causal                any given task seems like a clear non-starter. And yet, many
models to make sense of everyday problems and puzzles. The              stochastic search algorithms for hypothesis generation suf-
difference between radical conceptual change and prosaic                fer from exactly this problem. How can such algorithms be
concept generation is one of degree, like the difference be-            amended to not consider ‘obviously wrong’ proposals with-
tween coming up with a general explanation for how objects              out actually proposing them first?
balance and fall – a difficult process that may take months                The challenge of everyday thinking is the challenge of re-
or years (Baillargeon, 2008) – and coming up with an off-               ducing hypothesis spaces quickly and on-the-fly, in response
the-cuff explanation for why soda tastes fizzy, or a plausible          to a task that might never have been considered before. The
name for a new action movie.                                            reduced hypothesis spaces might still be large, and they might
   While everyday creativity is not categorically different             still contain ‘bad’ ideas and concepts. But these concepts and
from scientific creativty, the specific challenge of explaining         ideas would at least be relevant to the task. They would be
creativity in everyday idea generation has been pointed out             capable of being wrong, in the sense that a human would rec-
by Schulz (Magid, Sheskin, & Schulz, 2015; Schulz, 2012a).              ognize them as a bad or good response to the task, as opposed
                                                                    1493

to completely unconnected.                                          Tenenbaum, & Goodman, 2012), and in principle it is pos-
   In this paper we take up the computational challenge of          sible to stochastically search through such spaces by propos-
everyday reasoning, and propose a structured approach for           ing and rejecting amendments to the current hypothesis. But
narrowing hypothesis spaces by inferring a generative model         the problem of everyday thinking suggests that these propos-
over relevant examples taken from memory. The result-               als must be strongly constrained by abstract domain know-
ing “Bounded-Space” model (or, BS model, for short) can             eldge, such that most of the proposals that can potentially be
generate reasonable proposals for learning or problem solv-         considered (a-priori of any data or constraint) will never be
ing within a domain, but it does not exempt a thinker from          considered, and our actual proposals focused efficiently on
stochastically searching within the reduced space and evalu-        candidates that have some hope of being useful.
ating the proposals. Ideas generated in the reduced space may          To see the problem more clearly, try to come up with a
be bad ones. They may even be, for lack of a more polite term,      name for a new pub. Perhaps you never faced such a task,
mostly bullshit. But by quickly cutting down the space of pos-      but presumably you can do it with some degree of success.
sible thoughts from ‘mostly nonsense’ to, at worst, ‘mostly         Such a thought process could be implemented by a stochas-
bullshit’, everyday thinking and learning can be powerfully         tic search through all the possible phrases in the English lan-
and usefully constrained.                                           guage, but this would lead to a ridiculous proportion of not
                                                                    only bad proposals, but completely irrelevant proposals. In
                          Towards a Better                          Figure 1 we consider a fictional space of all possible names
                        Designed Belgian Office                     for things. Only a tiny portion of that space can even be called
                                                                    ‘Pub Space’.
                                                                       Assuming a thinker (such as yourself) was never asked to
                                                                    come up with a new pub name before, how can they quickly
                                                                    reduce the space of all possible new names to just ones rel-
                                                                    evant for pubs? Presumably you thought of a better pub
                  The Red Queen                                     name than “Towards a Better Designed Belgian Office”, and
                                                                    if someone posed that as a suggestion you could evaluate it
                                                                    as a bad proposal. 1 But “Towards a Better Designed Belgian
                                                                    Office” is a potentially good answer to a different question.
                                                                    Again, the issue is that before any particular question, puz-
Figure 1: A space of possible names for things. Only a small
                                                                    zle or problem is posed, we wish to have a large search space
subregion contains names relevant for a given task (dark area,
                                                                    that can generate many possible concepts, thoughts and so-
in this case pub names). A stochastic search over the entire
                                                                    lutions. But after a particular task is set, we wish to restrict
space would result in many examples that are not even bad.
                                                                    the space to only the relevant solutions. How can we know
                                                                    ahead of time what counts as a ‘relevant’ solution, without
   We develop and test our BS model using the relatively sim-
                                                                    first proposing it and evaluating it?
ple task of coming up with new names for items, specifically
                                                                       We propose a method (Bounded-Space, or BS) for con-
movies of different genres and pubs (this is similar to a task
                                                                    structing new, relevant concept spaces on-the-fly, illustrated
proposed in Magid et al., 2015). We compare its proposals to
                                                                    through the pub example in Figure 2. For any particular task
those of people, and to real world data. We find that the ba-
                                                                    the thinker first draws from memory several examples that
sic BS model fares worse than people, but is already within a
                                                                    match the desired concept (Figure 2.1). For example, if asked
reasonable range. A cognitively plausible extension to the BS
                                                                    to come up with a new pub name, the thinker might first draw
model that evaluates a small number of samples before select-
                                                                    some known pub names from memory. We assume that rele-
ing one is surprisingly consistent with the range of people’s
                                                                    vant examples are available in memory, even if they are rel-
novel name generation behavior. The BS model provides an
                                                                    evant in a broad sense. “Broad sense” here means that the
initial proposal for capturing everyday thought, but extend-
                                                                    examples match the general structure of the task. For exam-
ing it beyond the (relatively) simple task of proper names to
                                                                    ple, if the task is coming up with an explanation for why the
tasks such full-blown explanations is not trivial, a point that
                                                                    Roman Empire fell, relevant examples can include causal ex-
we take up in the discussion.
                                                                    planations in general (“State changes in X can be caused by
             Modeling on-the-fly thought                            an outside Y”), rather than particular reasons why the Roman
                                                                    Empire fell (“Crises of legitimacy”). Without any relevant ex-
Generating new thoughts and concepts can be seen as a               amples that come to mind the question itself is a non-starter,
stochastic search through a hypothesis space, guided by some        e.g. “Can you ganoosh a new Floop?”. The initial retrieval of
metric of success or quality (Ullman et al., 2012). Ideally,        relevant examples might rely on associative memory, but is a
the hypothesis space should be large enough to encompass            problem outside the scope of this paper.
any possibly correct or useful theory or concept. Infinite             The thinker then uses inverse inference, conditioned on the
hypothesis spaces that fit this bill can be easily specified
through grammars over conceptual primitives (Piantadosi,                1 Or rather, as “not even bad’, just ridiculous.
                                                                1494

                   1. Recall examples                             examples, noticing for instance that all the examples share a
                                                                  certain abstract causal structure. In our particular case study,
                                 The White Bear                   which is restricted to new names for pubs and movies, we
                        The Rose               The King's         consider a grammatical analysis of the examples from mem-
                         & Crown                   Arms
                                                                  ory (Figure 2.1), although many other structural constraints
                The Horse                                         are possible in the general case. For example, the thinker
                 & Groom                                          would notice that “The Rose and Crown” share the same syn-
                                                                  tactic structure as “The Horse and Groom”, namely “The
                                                                  [Noun] and [Noun]”. The thinker then stores the relevant
               The Six Bells
                                               The Lord           underlying structure without the particular terminals (Figure
                                 The Bull       Nelson            2.3). This database of structures is a proxy for a generative
                                                                  model over the space containing the examples.
                                                                     In order to come up with a new example (Figure 2.4),
                                                                  the thinker chooses a structure in proportion to the number
                                                                  of times it was used in any of the examples from memory.
                2. Structural inference                           For instance, the thinker might generate from “The [Noun]
                                                                  and [Noun]”. When reaching any particular terminal (e.g.
                   S                    S
                                                                  [Noun]), the speaker either reuses an appropriate part from
                  NP                   NP
                                                                  a stored example (e.g. any of the nouns in the examples) or a
             Det       NP         Det       NP        ...         semantically similar part (“Ox” rather than “Horse”). In this
             The
                  NN   CC   NN
                                  The
                                       NN   CC   NN               paper we use high-dimensional word embeddings via Global
                 Horse and Groom      Rose and Crown
                                                                  Vectors (Pennington, Socher, & Manning, 2014) to imple-
                                                                  ment a similarity space. Word-vectors that are close together
                                                                  in this space (in a Euclidean or cosine distance sense) tend
                                                                  to be semantically similar in psychological tasks. At the end
                                                                  of this process, the thinker might for instance come up with
                    3. Store structures                           “The Bear and Ox” as a plausible pub name.
                       S                        S                    It is important to stress that this model only implements
                                                                  the proposal stage of a conceptual search process. An addi-
                      NP                       NP
                                                                  tional evaluation step – not modeled here – is necessary to ac-
               Det         NP
                                          Det   JJ   NN           cept or reject a particular proposal. There may be additional
                                                                  constraints that one considers in the evaluation, such as the
                      NN   CC     NN
                                                                  catchiness of the name. “The Bear and Ox” might be a terri-
                                                                  ble name for a pub for any number of reasons under additional
                                                                  evaluation, but at least it appears reasonable, as opposed to “A
                                                                  Floral and Tasty Essence Wrapped in Chamomile”2 . The role
            4. Generate new examples                              of the BS proposal model is to get the thinker within a space
                                                                  where good and bad proposals can be evaluated, rather than
                                                                  having to spend the majority of the time with non-starters.
                                                                     Under the assumption that an evaluation function exists, it
                 The Bear & Ox                                    is possible to extend the model so that the thinker draws a
                                                                  number of examples at a time, and only reports one of them.
                                                                  This is natural if the thinker must provide an example or se-
                                                                  ries of examples, and cannot choose to simply refuse to pro-
                                                                  vide an example even if it is evaluated as poor. We consider
                                                                  such an extension by having the model draw k samples (BS-
                                                                  k), and choosing among them in proportion to their quality
                                                                  (provided by the assumed evaluation function), relative to the
                                     The White Duke               total quality of the sample. If k = 1, we recover the original
                                                                  BS model. The k parameter has the psychological interpre-
                                                                  tation of ’the number of examples people draw and evaluate
Figure 2: An illustration of the BS model applied to the do-
                                                                  internally before reporting a single answer’. In order to com-
main of pub names.
                                                                  pare this model to people, we consider the task of coming up
examples, to construct a generative model that can produce
                                                                      2 Which is better as an example response for ‘make up a wine
these examples. Such a thinker might focus on the form of the
                                                                  review’.
                                                              1495

with new names for things, specifically movies of different              similarly: the mean ratings were 3.12 for Production and 3.07
genres and pubs.                                                         for Real. People rated the BS model names lower on average
                                                                         (mean rating was 2.57, difference is significant at p < 0.001).
                          Experiment                                     The BS-k model with k = 5 achieves a mean rating of 2.99,
Participants, materials and methods                                      which is still statistically different from the average rating for
                                                                         Production and Real names, but the difference is now much
Two groups of participants, Producers (N = 40, 18 female,                smaller (we expand on why we chose k = 5 below).
median age 29) and Raters (N = 50, 16 female, median age                    Figure 3 shows the comparison in more detail, as a dis-
33), were recruited through Amazon’s Mechanical Turk ser-                tribution over the ratings from 1 to 5. A χ2 test shows the
vice and paid a small monetary sum for their participation.              distributions for the Production names and the Real names are
   The Producers were asked to come up with 5 new names                  not distinguishable, while the BS and BS-5 models are highly
for 4 different movie genres (action, horror, comedy and ro-             distinguishable from both and from each other (p < 0.0001).
mance give 20 movie names in total per participant), and 5               Table 1 illustrates examples of high quality, low quality and
new names for pub names. Producers entered their responses               average names for different genres.
using a free-form text field. The Raters were asked to rate
names for different categories on a 1-5 scale (“Very Bad” to
“Very Good”). Names were category specific, meaning a par-
ticular question might be “How good is the title Parade of
Bullets as a name for a action movie?”
   In order to construct different names, we used an equal
mixture of names from the Production experiment, names
from real instances of the category, and names from the BS
model. To keep the task manageable for Raters, we used 25
names from each source (Production, Real, BS) per category,
creating 375 names in total. Each Rater saw half of these
names, such that each name was rated by 25 raters.
   The Production names were selected by randomly choos-
ing 5 Producers for each category (we chose this method,
rather than randomly selecting from all the production data,
in order to asses whether Producers come up with better or
                                                                         Figure 3: The distribution of ratings for different names, by
worse names as their guesses progress). The real names were
                                                                         the source of the name.
selected by randomly choosing among all Wikipedia entries
for that category (for movies) and from a list of popular pub               The ordinary BS model is not as successful at producing
names (for pubs). The model names were selected by first                 new examples as people, but it is not far. As mentioned, the
choosing examples at random from the Wikipedia entries for               idea of the BS model is to construct a reasonable space to
movie genres, and a list of popular pub names. There were                sample from, as a first step towards in a propose-evaluate cy-
on the order of 1,000 examples for each movie genre, and                 cle. It is likely that people are also considering bad names
260 pub names.3 These examples were syntactically parsed                 but not reporting all of them, and this evaluation step is not
using a variant of the NLTK package in Python (Bird, Klein,              captured by the BS model. The BS-k extention of the model
& Loper, 2009), and a library of syntax trees was built for              uses the rating of the model names as a proxy for an evalua-
each genre. Syntax trees were then chosen in proportion to               tion step, considering k samples at a time and choosing one
how often they appear in the examples, and terminals in each             in proportion to its ratings relative to the total rating in the k
tree were chosen from the appropriate part of speech as it               samples. If k = 1, we recover the original model. But as k
appears in the examples, or randomly replaced with a seman-              grows larger, we come closer to the rating distribution of the
tically similar part of speech using GloVe (Pennington et al.,           Production and Real names (Figure 4), where ‘closer’ means
2014) with probability Preplacement = 0.2. Once examples                 a lower Kullback-Leibler divergence (KL), between the distri-
are available, generating a new BS example takes less than a             bution of quality scores for the names produced by the BS-k
second.                                                                  model and the Production or Real names. As Figure 4 indi-
                                                                         cates, considering only around 3 to 7 samples at a time can
Results                                                                  make a marked improvement to the BS model. Larger values
Participants rated both the Production names (those made up              of k provide diminishing returns, and so for the analyses here
by people) and the Real names (those taken from Wikipedia)               we considered k = 5. Note that the limiting behavior of in-
                                                                         creasing k is not to produce higher and higher rated names,
    3A  more psychologically plausible version of example recall         but rather to converge on a steady-state distribution reflecting
would use likelihood sampling over the space of movies, and would
require asking a different group of subjects to recall actual movies     a two-stage generate-and-evaluate loop for new names. It is
in response to genre prompts.                                            striking how quickly this distribution converges to that of the
                                                                     1496

names made up by people (or the Real names), suggesting              short titles to include such things as wine reviews (”The fin-
that the BS-k model represents at least a plausible first guess      ishing notes are not supported by the light body”). Outside
for how people come to produce the new names they do.                language, a similar approach to idea generation can take ad-
                                                                     vantage of other structures used to organize thoughts. For ex-
                                                                     ample, a causal-explanation BS model could analyze the un-
                                                                     derlying Bayes-net structure of examples from memory, and
                                                                     use those to propose new explanations from a common struc-
                                                                     ture (say, X ∧ Y − > Z, the Roman Empire fell because of a
                                                                     combination of tribal invasions and Jewish thought4 ). Ap-
                                                                     plying this approach to generate good hypotheses for causal
                                                                     learning and intuitive theory formation is, we hope, a promis-
                                                                     ing next step – perhaps wrong, but at least not ridiculous, as
                                                                     an account of where learners’ new concepts come from.
                                                                     Acknowledgments We are grateful to Laura Schulz for
                                                                     stimulating discussions. This material is based upon work
                                                                     supported by the Center for Brains, Minds and Machines
                                                                     (CBMM), funded by NSF STC award CCF-1231216.
                                                                                              References
Figure 4: Kullback-Leibler divergence (KL) between the dis-
                                                                     Baillargeon, R. (2008). Innate ideas revisited: For a principle
tribution of ratings over model names and Production names,
                                                                        of persistence in infants’ physical reasoning. Perspectives
for different values of k in the BS-k model. Error bars show
                                                                        on Psychological Science, 3, 2–13.
95% confidence intervals.
                                                                     Bird, S., Klein, E., & Loper, E. (2009). Natural language
                                                                        processing with python. ” O’Reilly Media, Inc.”.
                         Discussion                                  Carey, S. (2009). The Origin of Concepts. Oxford University
There is something magical about everyday thought, and                  Press.
something odd about how people can respond quickly and               Goodman, N. D., Ullman, T. D., & Tenenbaum, J. B. (2011).
reasonably to new questions they never heard with answers               Learning a theory of causality. Psychological review,
no one else has heard. A child could take years to come up              118(1), 110.
with an intuitive biology that unites trees and animals into one     Gopnik, A., & Wellman, H. M. (2012). Reconstructing
concept (Carey, 2009), but a 5-year-old can answer ’What                constructivism: Causal models, bayesian learning mech-
makes the wind?’ at the speed of thought. And even if their             anisms, and the theory theory. Psychological Bulletin, 138,
answer is wrong it might be amusing (”The trees waving their            1085.
arms make the wind”), rather than absurd (”The wind blows            Magid, R. W., Sheskin, M., & Schulz, L. E. (2015). Imag-
because the moon is bigger than a bag of Doritos”).                     ination and the generation of new ideas. Cognitive Devel-
   Such magical stuff deserves hard-nosed experimental                  opment, 34, 99–110.
scrutiny (Magid et al., 2015), and a better computational an-        Pennington, J., Socher, R., & Manning, C. (2014). Glove:
swer than ”People search randomly through all possible con-             Global vectors for word representation. EMNLP, 12, 1532–
cepts and explanations and evaluate each candidate by how               1543.
well it explains the data” (Ullman et al., 2012). Here we con-       Piantadosi, S. T., Tenenbaum, J. B., & Goodman, N. D.
sidered one particular proposal for constructing a reasonable           (2012). Bootstrapping in a language of thought: A for-
hypothesis space on the fly by using a structural analysis of           mal model of numerical concept learning. Cognition, 123,
examples that match the task at hand. This BS model pre-                199–217. doi: 10.1016/j.cognition.2011.11.005
supposes that thinkers have a way of carrying out such a struc-      Schulz, L. (2012a). Finding new facts; thinking new
tural analysis (in our particular case, we assumed a thinker            thoughts. Rational constructivism in cognitive develop-
can use grammar to recognize the structural similarity be-              ment. Advances in child development and behavior, 43,
tween instances, using ”Saving Private Ryan” and ”Chasing               269–294.
Amy” to construct a general ”VERBing PROPER-NAME”                    Schulz, L. (2012b). The origins of inquiry: Inductive infer-
movie schema).                                                          ence and exploration in early childhood. Trends in Cogni-
   The BS model serves as just one part in a propose-evaluate           tive Sciences, 16, 382–389.
search, where the evaluation step is difficult to capture and        Ullman, T. D., Goodman, N. D., & Tenenbaum, J. B. (2012).
can involve a large amount of world knowledge. Still, for               Theory learning as stochastic search in the language of
the limited task considered it appears to produce reasonable-           thought. Cognitive Development, 27, 455–480.
sounding new examples for a given category. Within the lan-              4 http://courses.washington.edu/rome250/gallery/
guage domain, the model can potentially be extended beyond           ROME%20250/210%20Reasons.htm
                                                                 1497

                Rating              BS Model                           Production                               Real
                           The Raging V (3.7)             War Hawks (4.1)                        Bloodfist II (3.9)
                High
                           Phoenix First (3.4)            Retribution (3.9)                      Marked for Death (3.8)
                           Drop (3.0)                     Stunt Man (3.1)                        Blue Steel (3.2)
 ACTION         Average
                           Conspiracy Mars (2.7)          Run Faster (3.1)                       Bullet in the Head (3.1)
                           Of Art (1.9)                   The State of Iraq (2.6)                I Come in Peace (2.3)
                Low
                           Teenage Chinese (1.5)          Eat It (2.1)                           Curry and Pepper (1.5)
                           The Dawn (3.5)                 The House of Death (4.2)               The Dunwich Horror (4.1)
                High
                           The Prophecy on One (3.4)      Cabin of the Dead (4.0)                Murders in the Rue Morgue (4.0)
                           Sharks (2.7)                   Silence (3.1)                          The Headless Eyes (3.3)
 HORROR         Average
                           Seed 5 (2.4)                   Blood Draws (3.0)                      The Wizard of Gore (3.0)
                           Space Vegas (1.9)              Paid Maidens 3000 (2.1)                Count Yorga, Vampire (2.7)
                Low
                           Family (1.5)                   Cat Napping (1.7)                      Mephisto Waltz (2.2)
                           Love Punch Drunk (3.0)         Hot Mess (3.5)                         Above the Limit (3.0)
                High
                           All American Rita (2.8)        Tame Your Own Shrew (3.4)              Mr. Flip (3.0)
                           Puddles of Max (2.6)           Come on Man (2.8)                      The Enchanted Drawing (2.6)
 COMEDY         Average
                           America Dave (2.6)             Match This (2.7)                       Those Awful Hats (2.6)
                           Princess Year (2.2)            Jane 2 (2.1)                           New Pillow Fight (2.3)
                Low
                           West (2.1)                     Ha Ha Ha (1.8)                         Clowns Spinning Hats (2.0)
                           Love in a Separation (3.5)     When You Least Expect It (3.7)         At First Sight (4.0)
                High
                           Private Woman (3.4)            Daydreaming in New York (3.7)          Bed of Roses (3.7)
                           The Pearl Rollercoaster (2.7)  Take it Slow (3.3)                     Bitter Moon (3.2)
 ROMANCE        Average
                           Walls of Sky (2.5)             Cool Happiness in New York (3.0)       Ballistic Kiss (2.9)
                           A Speckled McKee (2.2)         Cheeky (2.5)                           1871 (2.3)
                Low
                           Death (1.4)                    Red Beans and Rice (1.9)               The 5th Monkey (2.0)
                           The Hound’s Head (3.5)         The Old Barrel (3.8)                   The Rose & Crown (3.8)
                High
                           The Royal Hood (3.1)           The Rusty Spur (3.8)                   The White Lion (3.7)
                           The Green Bay (2.6)            The Dark Forest (3.0)                  The Castle (3.2)
 PUBS           Average
                           The Garter (2.4)               The Dog’s Ear (3.0)                    The Cross Keys (3.1)
                           The Cow (2.2)                  The Cat’s Meow (2.6)                   The White Hart (2.5)
                Low
                           The Pear (1.9)                 The Paper Cut (2.3)                    The New (1.8)
Table 1: Examples of different names, organized by source and average rating, with their rating indicated in parenthesis. The
names were assigned as Low, Average and High based on their relative position compared to the median rating for that source
and genre. 90 names are shown out of the total 375.
                                                           1498

