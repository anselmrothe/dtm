   Bifurcation analysis of a Gradient Symbolic Computation model of incremental
                                                               processing
                                                 Pyeong Whan Cho (pcho4@jhu.edu)
                                               Paul Smolensky (smolensky@jhu.edu)
                                      Department of Cognitive Science, Johns Hopkins University
                                           3400 N. Charles Street, Baltimore, MD 21218 USA
                             Abstract                                   (b) when and why the model fails to build a target structure.
                                                                        It will turn out that the model’s failure looks like either the
   Language is ordered in time and an incremental processing
   system encounters temporary ambiguity in the middle of sen-          garden path effect (Frazier, 1987; Frazier & Rayner, 1982) or
   tence comprehension. An optimal incremental processing sys-          the local coherence effect (Tabor, Galantucci, & Richardson,
   tem must solve two computational problems: On the one hand,          2004) both of which are well established in the psycholin-
   it has to keep multiple possible interpretations without choos-
   ing one over the others. On the other hand, it must reject           guistics literature.
   interpretations inconsistent with context. We propose a re-             The remainder of the paper is organized as follows. First,
   current neural network model of incremental processing that          we briefly describe a phrase structure grammar of our inter-
   does stochastic optimization of a set of soft, local constraints
   to build a globally coherent structure successfully. Bifurcation     est. Second, we present an overview of the model; summarize
   analysis of the model makes clear when and why the model             the results without technical details; and discuss the implica-
   parses a sentence successfully and when and why it does not—         tion of the model. The technical details of the analysis are
   the garden path and local coherence effects are discussed. Our
   model provides neurally plausible solutions of the computa-          presented in “Bifurcation analysis of the GSC model”, which
   tional problems arising in incremental processing                    is followed by a conclusion.
   Keywords: Harmonic Grammar; Gradient Symbolic Compu-
   tation; neural networks; incremental processing; parsing; dy-                         A Grammar of Interest
   namical systems theory; bifurcations
                                                                        In the present study, we investigate the Gradient Symbolic
                          Introduction                                  Computation (GSC) model (Smolensky, Goldrick, & Mathis,
Sentence comprehension requires building a hierarchical con-            2014) that implements a phrase structure grammar G, consist-
stituent structure from sequentially presented words. Psy-              ing of four rewrite rules: G = {S1 → A B, S2 → A C, S3 → D
cholinguists agree that human language comprehension is in-             B, S4 → D C} where S1, S2, S3, S4 are the starting symbols.
cremental, which means the processing system builds up an               Grammar G generates a finite language presented in the in-
interpretation with partial input before a whole sentence is            troduction. We designed the simple language for the follow-
presented. An optimal1 incremental processing system must               ing reasons: (a) the language shares the same computational
solve two closely related computational problems: (a) it must           problems with more complex languages. (b) The GSC model
be able to keep multiple possibilities consistent with the infor-       implementing a more complex grammar is more difficult to
mation that has been processed, say context. (b) At the same            analyze. The simplicity of the grammar makes it possible to
time, it has to reject the interpretations inconsistent with con-       investigate the model thoroughly. (c) Parsing a sentence of
text which may be consistent with bottom-up input.                      the language looks trivial but can be difficult for continuous-
   For clarity, consider a finite language consisting of four           time recurrent neural network models that utilize highly local
sentences {“cats sleep”, “cats yawn”, “dogs sleep”, “dogs               constraints and do not explicitly monitor complete structures.
yawn”}. A first word (e.g., “cats”) of a sentence creates tem-
porary ambiguity which can be resolved after processing a                        Overview of the Model and Results
second word (e.g., “sleep”) of the sentence. An optimal sys-            The GSC model is a brain-like, continuous-time continuous-
tem must be able to consider both grammatical continuations             state dynamical systems model that can gradually build dis-
without choosing one over the other after processing the first          crete symbolic structures. There is a transparent mapping
word. Simultaneously, the system must be able to reject im-             between the symbolic structures and the activation patterns
possible interpretations before reading a second word (e.g.,            (vectors in a continuous vector space) (Smolensky, 1990) so
“sleep”) such that it does not take a wrong interpretation (e.g.,       it allows us to study the nature of the intermediate states in
“dogs sleep”).                                                          the middle of sentence comprehension.2 The GSC model
   In this study, we propose a neural network model of incre-           does stochastic optimization of constraints implementing the
mental processing and show (a) how the proposed model can
solve two computational problems in a principled way and                    2 A popular connectionist alternative is the Simple Recurrent
                                                                        Network (Elman, 1990; Frank, 2009). Although it has been suc-
    1 Human language processing may not be optimal due to perfor-       cessful in modeling human sentence comprehension performance,
mance issues. In this study, we investigate the competence of the       the model works in discrete time and develops a rather hard-to-
proposed model.                                                         understand internal representation.
                                                                    1487

grammar and those implementing a force quantization push-                         ics can be understood thoroughly not with simulations but
ing towards those neural states that encode fully discrete sym-                   based on principles of dynamics (see the next section)—the
bolic trees (for details, see Smolensky et al., 2014). The                        GSC model is not a blackbox. Unlike probabilistic symbolic
model maximizes Total Harmony, a measure of goodness of                           models of sentence comprehension (Hale, 2001; Levy, 2008),
an activation state: H(a, γ) = HG (a) + HB (a) + γHQ (a) where                    the GSC model describes how the state changes in continuous
a is an activation state and γ (≥ 0) is quantization strength.                    time and proposes the solutions of computational problems
Grammar Harmony HG measures how much a state satisfies                            at the algorithmic level. More importantly, the GSC model
grammatical constraints and is defined by principles (Hale &                      is not an implementation of structural probabilistic models
Smolensky, 2006). Bowl Harmony HB measures how close a                            where the structural hypothesis space is discrete. A blend
state is to a baseline activation state. Quantization Harmony                     state in the GSC model—an intermediate state that is located
HQ , roughly speaking, measures how close a state is to dis-                      between the states encoding fully discrete structures—is not
crete symbolic states.3                                                           the representation of a probability distribution across discrete
   In incremental processing, we assume: (a) the model reads                      symbolic structures.
the first word of a sentence at 0 of γ. (b) γ increases mono-                        The GSC model proposes an interesting way of keeping
tonically in time without assuming any specific function. (c)                     past and predicting the future. In the GSC model, the present
The model reads the second word at a certain value of γ (= γc )                   (blend state) contains past. Unlike the memory-based model
and then γ continues to increase. The topic of the paper is a                     proposed by Lewis, Vasishth, and Van Dyke (2006), there
formal understanding of how γ’s temporal trajectory results                       is no need of retrieval processes.4 Given a word input, the
in correct parsing, garden path, or local coherence errors.                       present contains the future as well. In the next section, it will
   We performed numerical bifurcation analysis of the model                       be shown that the GSC model travels along a one-dimensional
(for a brief introduction, see Meijer, Dercole, & Oldeman,                        manifold (a subspace of the full representation space) which
2009)—the details are presented in the next section. The re-                      can evolve to multiple grammatical structures consistent with
sults suggest that there is a range of γc values (γ1 < γc < γ2 )                  both top-down context and bottom-up word input.
that guarantees accurate parsing (case 1). If the model reads                        The proposed GSC model is not complete. Processing dif-
the first word (e.g., ‘A’) of a sentence (e.g., ‘A B’) too long                   ficulty in sentence comprehension is typically measured in
such that γc > γ2 , the model randomly chooses one structure                      reading times rather than in parsing accuracy in behavioral
(e.g., [S2 A C]) over the other (e.g., [S2 A B]) before it reads                  experiments but the model does not make a prediction on
the second word, althouth both structures are consistent with                     word reading times directly.5 First, we point out that psy-
the first word. It is a garden path error (case 2). If the model                  cholinguistic models such as the garden path model (Frazier,
reads the second word too quickly (γc < γ1 ), the model does                      1987) and the unrestricted race model (Traxler, Pickering, &
not reject the structures (e.g., [S3 D B] or [S4 D C]) inconsis-                  Clifton Jr., 1998) propose longer reading times indicate the
tent with the first word (e.g., ‘A’). Thus, when the model reads                  revision of the initial parse that turns out to be wrong. At
the second word at γc (< γ1 ), sometimes the model fails to                       this point, the GSC model does not have the ability to revise
build the target structure because a non-target structure (e.g.,                  its interpretation. Parsing failures (cases 2 and 3) overviewed
[S3 D B]) is consistent with the second word (’B’) and it is                      in this section (see also paths [2] and [3] in Figure 4) should
still considered by the model as a possible grammatical struc-                    be interpreted as the initial parsing failure rather than the fi-
ture. We argue it is an local coherence error (case 3).                           nal product. If we assume the reanalysis requires more time,
   We argue our model has a potential as a processing model                       there is a close relationship between the accuracy of initial
of incremental structure building. Our model provides neu-                        parsing and the reading times. One way to implement the
ral/mathematical solutions of the computational problems                          revision of the initial interpretation is to allow the model to
arising in incremental processing; we know when and why                           reduce γ when it detects the present blend state is inconsis-
the model parses a sentence correctly and when and why it                         tent with the bottom-up input—probably by detecting a sud-
does not. Unlike other constraint satisfaction models (Spivey                     den decrease in Grammar Harmony. Second, slow reading
& Tanenhaus, 1998; Tabor & Hutchins, 2004; Vosse & Kem-                           times may not necessarily suggest the revision of the ini-
pen, 2000), the GSC model is contructed in a principled way                       tial parse. For example, in the constraint-satisfaction mod-
using Harmonic Grammar (Prince & Smolensky, 1997) and                             els (MacDonald, Pearlmutter, & Seidenberg, 1994; Spivey &
tensor product representation (Smolensky, 1990). Its dynam-                       Tanenhaus, 1998; Tabor & Hutchins, 2004), processing delay
                                                                                  is observed when multiple interpretations compete with each
    3 H (a) = 0.5aT Wa + bT a + extT a; H (a) = −0.5β||a − z||2 ;
       G                                        B
                                                                                  other without parsing failure. In the GSC model, there may
HQ (a) = −0.5 ∑r∈R (∑ f ∈F a2f ,r − 1)2 − 0.5 ∑ f ∈F,r∈R a2f ,r (a f ,r − 1)2
                                                                                      4 We do not argue retrival processes do not involve in human sen-
where W and b are the weight matrix and the bias vector which are
constructed by Harmonic Grammar. a is an activation vector, ext                   tence comprehension. The GSC model suggests retrieval may not be
is an external input vector, a f ,r is an acitvation value of a filler/role       the core process in incremental structure building.
binding (Smolensky, 1990), F and R are the index sets of fillers and                  5 The model has a potential to make reading time predictions but
roles, z (= 0.5 1) is the baseline activation vector, β is the strengh to         at this point, we do not know with what information the model can
pull a state to z and was set to 10. HQ implements a constraint that              make a decision when to read a next word. The model may be able
only one filler must occupy a role.                                               to make such decisions by monitoring γ.
                                                                              1488

be a manifold which takes longer to travel along than other             The equilibrium points may be created, destroyed, or
manifolds.                                                           change their stability when a parameter’s value changes.
    We conclude the GSC model can provide a different ap-            Such qualitative change in the dynamics of the system is
proach to incremental processing and may improve our un-             called bifurcation (Strogatz, 1994) and the parameter values
derstanding of the process.                                          at which bifurcations happen are called bifurcation points.
                                                                     When a bifurcation occurs, the topology of the disconnec-
        Bifurcation analysis of the GSC model                        tivity graph changes. In this stuy, we use the continua-
Background                                                           tion method (Meijer et al., 2009) to discover the equilibrium
                                                                     points at different parameter values and detect bifurcations.
                                                                     At the same time, we construct a disconnectivity graph from
                                                                     the set of equilibrium points detected at a particular parameter
                                                                     value to visualize the toplogy of the energy landsacpe. The
                                                                     combination of the two techniques allows us to understand
                                                                     the dynamics of the GSC model thoroughly.
                                                                     Method
                                                                     The GSC model maximizes Total Harmony H(a; γ, α), or
                                                                     equivalently minimizes energy E(a; γ, α) = −H(a; γ, α) in
Figure 1: An arbitrary energy landscape (energy = -Total Har-        which a is a 27-dimensional activation state vector, γ (≥ 0)
mony) and a disconnectivity graph constructured from the en-         is the quantization strength parameter, and α is a variable in-
ergy landscape. p1-p4 indicate the local minima and E1-E5            dicating whether the model is reading the first (α = 0) or the
(E1 < · · · < E5) indicate the energy levels.                        second (α = 1) word of a sentence ‘A B’.6
                                                                        First, we used the continuation method to discover the one-
    Consider an arbitray energy function E(x, p) in which x is       dimensional manifold of equilibrium points in the 28 dimen-
a state variable and p is a parameter. The left side of Fig-         sional vector space (27 state variables + 1 parameter γ) when
ure 1 presents an arbitrary energy landscape (energy = -Total        the first word ‘A’ was presented (i.e., α = 0). To use the con-
Harmony) when p is set to a certain value. Now consider a            tinuation method, we should know the equilibrium points at a
system that minimizes the energy function. When no noise             particular γ value. In the GSC model, it is not difficult. When
is assumed, the system rolls down the hills of the energy            γ = 0, we can ignore nonlinear quantization dynamics and
landscape (i.e., gradient descent): dx/dt = −dE(x, p)/dx =           at the setting, the GSC model is a linear dynamical system
 f (x, p). At a local minimum or a local maximum, the sys-           which was constructed to have a single global optimum. The
tem will not change anymore because the gradient vanishes            equilibrium point was discovered by the Newton’s method.
there. Those states x∗ where f (x∗ , p) = 0 are called equilib-      Alternatively, the global attractor can be discovered by in-
rium points (or fixed points). We can check what happens if          tegrating the differential equations ∇a H(a, γ, α) numerically.
a system is slightly displaced from an equilibrium point x∗ .        Once an equilibrium state was discovered, we used the con-
If the system approaches x∗ , the state (e.g., the local minima      tinuation method to discover the one-dimensional manifold
p1-p4) is asymptotically stable and often called an attractor.       of equilibrum points.
If the system moves away from x∗ , the equilibrium state (e.g.,         We iterated the process with every attractor discovered
the local maxima) is unstable.                                       when γ = 200. When γ is very large, the equilibrium points
    If the system is high dimensional, it is not easy to visual-     are mostly determined by Quantization Harmony. The func-
ize the energy landscape. In that situation, we can focus on         tion was designed to have the attractors at a subset of the ver-
the topology of the energy landscape by constructing a dis-          tices of the unit hypercube [0, 1]27 . The actual attractors must
connectivity graph (Wales, Miller, & Walsh, 1998; Becker &           be close to those vertices. By applying the Newton’s method,
Karplus, 1997). The terminal nodes of the disconnectivity            we could find all the attractors when γ = 200. Besides the
graph (the right side of Figure 1) correspond to the attractors      attractors, there are many unstable equilibrium points at the γ
(local minima of the energy landscape, or equivalently local         value and finding those points is much more difficult. Instead
maxima of the harmony surface). The nonterminal nodes in-            of trying to find those unstable fixed points at the paramter
dicate the height of a ridge (called energy barrier) between         setting, we used the continuation method to discover the un-
two local minima. The magnitude of noise required to cross           stable equilibrium points, assuming that the unstable equilib-
over an energy barrier is determined by the barrier’s height         rium points are connected to the stable equilibrium points on
(Chiang, Hwang, & Sheu, 1987). For example, the height of            the manifold of the equilibrium points when γ is allowed to
the energy barrier from p1 to p2 (= E4-E2) is greater than the
                                                                         6 Given that all four sentences of the language are symmetric, we
height of the energy barrier from p2 to p1 (= E4-E3), sug-
                                                                     focus on the case in which the model is reading a sentence ‘A B’ and
gesting there is a certain level of noise at which the p2-to-p1      investigate when and how the model can build the target structure [S1
transition is possible while the p1-to-p2 transition is not.         A B] successfully.
                                                                 1489

change. If we start to follow the manifold from a stable equi-                From Figure 2, we can predict the state change while the
librium point, we will reach an unstable equilibrium point.                model is reading the first word ‘A’. Recall that γ is assumed to
   The numerical bifurcation analysis with the continuation                increase in time. The system has a single global attractor in
method was performed by using a MATLAB package, the                        the region where γ < γ1 . Thus, regardless of the initial state,
Continuation Core and Toolboxes (COCO)7 . After perform-                   the system will reach the global equilibrium point before γ
ing bifurcation analysis, we chose specific γ values (γ = 20,              passes γ1 . At γ1 , two more attractors emerge from a saddle-
25, 35, 70) and constructed a disconnectivity graph at each γ              node bifurcations but they are separated from the equilibrium
value to investigate the topology of the energy landscape.                 point along branch (5) where the system is. The system keeps
   We did not perform bifurcation analysis and construct the               following the major branch until γ passes γ2 at which a sub-
disconnectivity graphs when the model reads the second word                critical pitchfork bifurcation occurs and the equilibrium point
‘B’ (α = 1). Instead, the equilibrium points and the topology              loses its stability. Thereafter, even with a very small noise,
of the harmony landscape were inferred, based on symme-                    the system moves to either branch (1) or (2).
try, from those discovered from numerical bifurcation analy-                  Figure 3 shows the topology of the energy landscape at four
sis when the model reads the first word ‘A’.                               different γ values (see also Figure 2). When γ = 20 (Fig-
                                                                           ure 3a), there is a single attractor. When γ = 25 (Figure 3b),
Result                                                                     there are three local optima on branches (1), (2), and (5). If
                                                                           the system has followed the major branch, the system will
The continuation method discovered 795 branches of the
                                                                           be at the attractor on branch (5) represented by the leftmost
equilibrium points including 1589 equilibrium points at 200
                                                                           terminal node. The attractor is separated from other attrac-
of γ when α = 0.8 Most branches are not relevant in our dis-
                                                                           tors on branches (1) and (2) by energy barriers. When γ = 35
cussion of the model dynamics so we focus on a small num-
                                                                           (Figure 3c), the energy landscape has more attractors but all
ber of important branches (see Figure 2). First, look at the
                                                                           newly emerged attractors are separated from the three attrac-
branches (1), (2), and (5). A saddle-node bifurcation occurs
                                                                           tors on branches (1), (2), and (5) by high energy barriers. The
at γ1 ≈ 21.3 and a subcritical pitchfork bifurcation occurs at
                                                                           bottommost part of the graph inside the square has the same
γ2 ≈ 59.6 (for introduction, see Strogatz, 1994). Second, look
                                                                           local structure as Figure 3b. The blend state on branch (5) is
at the branches (3) and (4) which evolve to the states repre-
                                                                           still stable so the system will be at the equilibrium point. Fig-
senting [S3 D B] and [S4 D C] both of which are inconsistent
                                                                           ure 3d presents the disconnectivity graph when γ = 70. The
with the first word ‘A’. Those branches are disconnected from
                                                                           graph is very complex and not easy to read. The squred re-
the other branches. We suspect the branches evolving to the
                                                                           gion at the bottom is magnified in the third panel of the top
states representing grammatical/ungrammatical structures in-
                                                                           row in Figure 4. At that time, the equilibrium point on branch
consistent with the input word are disconnected from the ma-
                                                                           (5) is not stable any more. The system moves to the branches
jor branch evolving to the states representing the grammatical
                                                                           of stable equilibrium points, either (1) or (2).
structures consistent with the input.
                                                                              Recall that each word in the language is consistent with two
                                                                           grammatical structures and inconsistent with the other two
                                                                           grammatical structures. Given the symmetry, the equilibrium
                                                                           states when the model reads the second word ‘B’ (α = 1) can
                                                                           be inferred from the equilibrium states when the model reads
                                                                           the first word ‘A’. More specifically, when the model reads the
                                                                           second word ‘B’, the system forms a different set of branches
                                                                           of equilibrium points which has the same structure as shown
                                                                           in Figure 2. At this time, the equilibrium points along branch
                                                                           (2) evolves into the state representing [S3 D B] and the equi-
Figure 2: The bifurcation digram. The solid and dashed lines               librium points along branch (5) evolves into the blend of the
indicate stable and unstable equilibria, respectively. Only the            blend of [S1 A B] and [S3 D B]. Equilibrium points on branch
branches evolving to (1) [S1 A B], (2) [S2 A C], (3) [S3 D                 (1) still evolves into the state representing [S1 A B].
B], (4) [S4 D C], and (5) a blend of S1 and S2 are presented.                 Now consider what will happen if the model reads the sec-
The vertical lines indicate the γ values at each of which a                ond word ‘B’ at γc . Figure 4 shows three qualitatively differ-
disconnectivity graph was constructued (see Figure 3).                     ent cases in incremantal processing.
    7 http://sourceforge.net/projects/cocotools/
                                                                              Case 1 (see path [1] in Figure 4): The model reads ‘B’
    8 The COCO could not finish continuation successfully along            when γ1 < γc < γ2 ; γc = 35 in the current example. While γ
96 of 891 branches. All those cases occurred when the software             increases from 0 to γc , the state quickly approaches a global
switched to a new branch at a branch point (where multiple branches        attractor and follows the major branch (see branch (5) in Fig-
intersect each other) and tried to follow a differnet branch; the          ure 2). The state change is indicated by the arrow (annotated
branch before switching was connected to an ungrammatical struc-
ture (e.g., [C D ]) at 200 of γ. Those cases need further investigation    as [1]/[2]) from panel (a) to panel (b) in Figure 4. When the
but those branches are not important in the discussion of the model.       model reads ‘B’ at γc = 35, the harmony landscape changes
                                                                       1490

      (a)                               (b)                              (c)                              (d)
        Figure 3: Topological change in the harmony landscape (given a first word ‘A’) as γ increases (γ = 20, 25, 35, 70)
abruptly. The previously stable equilbrium state becomes un-         ing to the target structure or a branch leading to a non-target
stable so the state moves to a local optimum when α = 1.             structure [S3 D B] (see the arrows [3] from panel (e) to panel
It turns out the previous equilibrium point is located inside        (f)). The latter case is an extreme version of the local co-
the basin of attraction of an equilibrium point along a branch       herence effect; the system failed to use top-down information
corresponding branch (1) in Figure 2. The state change is in-        (context provided by the first word ‘A’) and relied only on
dicated by the arrow (annotated as [1]) from panel (b) to panel      the bottom-up information to choose the locally coherent but
(e). As γ increases further, the new equilibrium point evolves       globally incoherent structure.
to the state representing the target structure [S1 A B]; see the
                                                                             γ = 20               γ = 35                 γ = 70
arrow (annotated as [1]) from panel (e) to panel (f). When                              (a)                    (b)                   (c)
small noise is assumed, the model builds the target structure
with certainty.
                                                                                      [1]/[2]
   Case 2 (garden path; see path [2] in Figure 4): The model                                                  [2b]
reads ‘B’ when γc > γ2 ; γc = 70 in the example. As in Case 1,
                                                                                                              [2a]
the system reaches the global attractor and follows the mani-         α=0
                                                                              [3]                 [1]
fold of the global attractors in the beginning. However, when
γ passes γ2 (≈ 59.6), the equilibrium point on branch (5) loses                         (d)                    (e)              [2a] (f)
its stability. With a small noise, the system moves to a new                                                          [2b]
equilibrium point, either one on branch (1) or one on branch
(2); see the arrows from panel (b) to panel (c). As suggested                          [3]                   [3b]
in Figure 3, the system cannot move to other attractors be-
cause they are separted by high energy barriers. When the                                                     [3a]
model reads the second word at γc = 70, the harmony land-             α=1                                       [1]
scape changes abruptly. The attractors (1) and (2) in the top
right panel are located in the basins of attraction of the new
                                                                     Figure 4: Parsing accuracy depends on γc . The arrows indi-
attractors (1) and (2) in the bottom right panel, respectively.
                                                                     cate the state change expected while the model processes a
The state will be at either (1) or (2) in the bottom right panel
                                                                     sentence ‘A B’ ([1] γc = 35, [2] γc = 70, [3] γc = 20). The
depending on where the system was (see the arrows from
                                                                     panels in the second and third columns correspond to the re-
panel (c) to panel (f). It suggests if the system was following
                                                                     gions included in the panels (c) and (d) in Figure 3.
branch (2) while processing the first word, the system will
build a non-target structure [S2 A C] with the second word in-
put, although it is inconsistent with the bottom-up input. This                               Conclusion
case is like the garden path effect in that the system chose one     In this study, we investigated how the GSC model handles
possibility over the others when it encounters ambiguity.            temporary ambiguity to parse a sentence successfully in in-
   Case 3 (local coherence; see path [3] in Figure 4): The           cremental processing. The model considers all structures
model reads ‘B’ when γc < γ1 ; γc = 20 in the example. As in         consistent with context (the words that the model has pro-
Case 1, the system reaches the global attractor and follows the      cessed) without choosing one over the others by being at a
manifold of the global attractors in the beginning. When the         stable blend state which can evolve into the possible gram-
model reads ‘B’ at γc = 20, the state moves to a new global          matical structures, suggesting the usefulness of the continu-
optimum; see the arrow [3] from panel (a) to panel (d). As           ous representation space. We point out that the blend state
γ increases, the system follows the major branch of equilib-         does not represent a disjunctive set of those structures so
rium points (see the arrow from panel (d) to panel (e)) until        the GSC model is not an implementation of the probabilistic
γ passes γ2 when the system moves to either a branch lead-           models. On the other hand, the model rejects the structures
                                                                 1491

inconsistent with the input by developing high energy barriers      MacDonald, M. C., Pearlmutter, N. J., & Seidenberg, M. S.
between attractors.                                                   (1994). The lexical nature of syntactic ambiguity resolu-
   More importantly, the model explains when and why the              tion. Psychological Review, 101(4), 676–703.
model fails to parse a sentence correctly. When γ increases         Meijer, H., Dercole, F., & Oldeman, B. (2009). Numeri-
too quickly, the model chooses one interpretation over the            cal bifurcation analysis. In R. A. M. Ph.D (Ed.), Ency-
other possible ones. When γ increases too slowly, the model           clopedia of Complexity and Systems Science (pp. 6329–
fails to separate globally incohrerent interpretations from           6352). Springer New York. (DOI: 10.1007/978-0-387-
globally coherent ones. The present study suggests that ac-           30440-3 373)
curate parsing in the GSC model requires an optimal control         Prince, A., & Smolensky, P. (1997). Optimality: From neural
of quantization strength γ.                                           networks to universal grammar. Science, 275(5306), 1604–
                                                                      1610.
                     Acknowledgments                                Smolensky, P. (1990). Tensor product variable binding and
We thank Geraldine Legendre, Akira Omaki, Kyle Rawlins,               the representation of symbolic structures in connectionist
Ben Van Durme, Colin Wilson, Laurel Brehm, Nick Becker,               systems. Artificial Intelligence, 46(1), 159–216.
Belinda Adam and especially Matthew Goldrick for their con-         Smolensky, P., Goldrick, M., & Mathis, D. (2014). Opti-
tributions to this work, and acknowledge the support of NSF           mization and quantization in gradient symbol systems: a
INSPIRE grant BCS-1344269.                                            framework for integrating the continuous and the discrete
                                                                      in cognition. Cognitive Science, 38(6), 1102–1138.
                          References                                Spivey, M. J., & Tanenhaus, M. K. (1998). Syntactic ambigu-
Becker, O. M., & Karplus, M. (1997). The topology of multi-           ity resolution in discourse: Modeling the effects of referen-
   dimensional potential energy surfaces: Theory and applica-         tial context and lexical frequency. Journal of Experimen-
   tion to peptide structure and kinetics. The Journal of Chem-       tal Psychology: Learning, Memory, and Cognition, 24(6),
   ical Physics, 106(4), 1495–1517.                                   1521–1543.
Chiang, T., Hwang, C., & Sheu, S. (1987). Diffusion for             Strogatz, S. H. (1994). Nonlinear dynamics and chaos: With
   global optimization in Rn . SIAM Journal on Control and            applications to physics, biology, chemistry, and engineer-
   Optimization, 25(3), 737–753.                                      ing. Westview Press.
Elman, J. L. (1990). Finding structure in time. Cognitive           Tabor, W., Galantucci, B., & Richardson, D. (2004). Effects
   Science, 14(2), 179–211.                                           of merely local syntactic coherence on sentence processing.
Frank, S. L. (2009). Surprisal-based comparison between a             Journal of Memory and Language, 50(4), 355–370.
   symbolic and a connectionist model of sentence process-          Tabor, W., & Hutchins, S. (2004). Evidence for self-
   ing. In Proceedings of the 31st annual conference of the           organized sentence processing: digging-in effects. Jour-
   cognitive science society (pp. 1139–1144).                         nal of Experimental Psychology: Learning, Memory, and
Frazier, L. (1987). Sentence processing: A tutorial review.           Cognition, 30(2), 431–450.
   In M. Coltheart (Ed.), Attention and Performance XII: The        Traxler, M. J., Pickering, M. J., & Clifton Jr., C. (1998).
   Psychology of Reading (pp. 559–586). Lawrence Erlbaum              Adjunct attachment is not a form of lexical ambiguity reso-
   Associates.                                                        lution. Journal of Memory and Language, 39(4), 558–592.
Frazier, L., & Rayner, K. (1982). Making and correcting             Vosse, T., & Kempen, G. (2000). Syntactic structure assem-
   errors during sentence comprehension: Eye movements in             bly in human parsing: A computational model based on
   the analysis of structurally ambiguous sentences. Cognitive        competitive inhibition and a lexicalist grammar. Cognition,
   Psychology, 14(2), 178–210.                                        75(2), 105–143.
Hale, J. (2001). A probabilistic Earley parser as a psycholin-      Wales, D. J., Miller, M. A., & Walsh, T. R. (1998). Archety-
   guistic model. In Proceedings of the Second Meeting of             pal energy landscapes. Nature, 394(6695), 758–760.
   the North American Chapter of the Association for Com-
   putational Linguistics on Language Technologies (pp. 1–
   8). Stroudsburg, PA, USA: Association for Computational
   Linguistics.
Hale, J., & Smolensky, P. (2006). Harmonic Grammars and
   harmonic parsers for formal languages. In P. Smolensky &
   G. Legendre (Eds.), The harmonic mind: From neural com-
   putation to optimality-theoretic grammar. Volume I: Cogni-
   tive architecture (pp. 393–416). The MIT Press.
Levy, R. (2008). Expectation-based syntactic comprehen-
   sion. Cognition, 106(3), 1126–1177.
Lewis, R. L., Vasishth, S., & Van Dyke, J. A. (2006). Compu-
   tational principles of working memory in sentence compre-
   hension. Trends in Cognitive Sciences, 10(10), 447–454.
                                                                1492

