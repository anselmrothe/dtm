     Working memory encoding of events and their participants: a neural network
       model with applications in sensorimotor processing and sentence generation
                     Martin Takac (takac@cs.otago.ac.nz)1,2 , Alistair Knott (alik@cs.otago.ac.nz)1
Dept of Computer Science, University of Otago, New Zealand1 , Centre for Cognitive Science, Comenius University, Bratislava2
                               Abstract                                      In the current paper, we will introduce a new network
   In this paper we present a model of how events and their partic-       model of semantic WM, which makes some new proposals
   ipants are represented in working memory (WM). The model’s             about the binding operations that create event representations.
   central assumption is that events are experienced through se-          We show that the event representations in this model are well
   quentially structured sensorimotor (SM) routines—as are the
   individuals that participate in them. In the light of this assump-     suited to support accounts of the role of semantic WM in on-
   tion, we propose that events and individuals are stored in WM          line SM experience and in sentence processing.
   as prepared SM routines. This proposal allows a new mech-
   anism for binding representations of individuals to semantic
   roles such as AGENT and PATIENT. It also enables a novel                     Background: a model of event perception
   account of how expectations about forthcoming events can in-           WM representations of experienced events have to be created
   fluence SM processing in real time as events are perceived.
   Finally, it supports an account of the interface between WM            during experience. Events take time to occur, so the SM pro-
   representations and language.                                          cesses through which they are experienced must be similarly
   Keywords: event perception; working memory; embodied                   extended in time. The founding assumption in our model is
   cognition; neural networks; syntactic heads                            that event-perception processes have a well-defined tempo-
      Introduction: semantic working memory                               ral structure—and that the mechanism representing events in
                                                                          WM capitalises on this well-defined structure. In this section
In this paper we present a model of how the brain encodes
                                                                          we outline what this structure is; for details, see Knott (2012).
events and their participants in working memory (WM). The
                                                                             We argue that perceiving an episode involves a relatively
WM medium that stores events was dubbed the ‘episodic
                                                                          discrete sequence of SM operations. This assumption rests
buffer’ by Baddeley (2000). We adopt the slightly broader
                                                                          on some well-accepted findings about perceptual processes.
term semantic WM, because our model represents individu-
                                                                          Firstly, there is good evidence that focal attention must be al-
als and their properties as well as events. (The term also em-
                                                                          located to an individual in order to process it in any detail
phasises the non-phonological character of the WM medium
                                                                          (see Walles et al., 2014 for a summary). If an event involves
we are modelling.) For Baddeley, the episodic buffer is an
                                                                          several participant individuals, therefore, the observer must
interface medium, linking to three distinct neural systems:
                                                                          attend to them one by one, rather than in parallel. Secondly,
the sensorimotor (SM) system, through which events are di-
                                                                          when an event is perceived, participants playing certain se-
rectly experienced, the episodic memory system, in which
                                                                          mantic roles are recognised first. For transitive events, we
they are stored in long-term memory (LTM), and the lan-
                                                                          argue the AGENT participant must be attended to before the
guage system, through which they are communicated. Here
                                                                          PATIENT (Knott, 2012).1 If the observer is executing the ac-
we consider the semantic WM system as it interfaces with the
                                                                          tion, this is because the decision to act must precede selection
SM system and language; in a companion paper (Takac and
                                                                          of a target; if the agent is watching an action, it is because s/he
Knott, this volume), we consider its interface with LTM.
                                                                          must monitor the agent to identify the intended target (Webb
   Experimental work on the WM system has focussed on rel-
                                                                          et al., 2010). Thirdly, a representation of the motor action
atively simple representations: representations of spatial lo-
                                                                          cannot be evoked until the target object has been attended
cation, visual properties, and prepared motor actions. How-
                                                                          to. In action execution, the agent must activate a represen-
ever, semantic WM is also assumed to be the medium where
                                                                          tation of the target object before its motor affordances can
such representations are combined or bound into complex
                                                                          be computed (Johansson et al., 2001); in action perception,
semantic structures. A binding mechanism is crucial for rep-
                                                                          the observer must compute the trajectory of the agent’s hand
resenting events: in particular, representations of individuals
                                                                          onto the target (e.g. Oztop et al., 2004). If these assump-
must be bound to semantic roles such as AGENT and PATIENT.
                                                                          tions, which are individually quite well accepted, are brought
Note that representations of individuals have their own inter-
                                                                          together, an interesting model of event perception emerges,
nal structure, which must be created through some form of
                                                                          in which apprehending a transitive event involves a sequence
binding. An individual has properties (shape, type etc), but
                                                                          of three SM operations: attention to the agent, then attention
also a spatial location. And it can be a singular entity, or a
                                                                          to the target, then activation of a motor programme. The idea
group. It is important that the mechanism binding individuals
                                                                          that events have a characteristic temporal structure is certainly
to semantic roles can operate on compositionally structured
                                                                          present in other models of event perception, in particular that
representations of individuals, as well as on atoms. Experi-
                                                                          of Reynolds et al. (2007). For Reynolds et al., these sequen-
ments have not revealed much about this binding mechanism,
but it is a key topic for neural network research (e.g. van der               1 Our terms ‘agent’ and ‘patient’ refer to Dowty’s (1991) more
Velde and de Kamps, 2006; Stewart and Eliasmith, 2012).                   general concepts ‘proto-agent’ and ‘proto-patient’.
                                                                      2345

tial regularities relate primarily to the structure of an agent’s       attentional or motor operations are represented, particularly
movements: they are the kind of regularities that the ‘biolog-          in macaques. The relevant representations are predominantly
ical motion’ system becomes attuned to. In our model, such              in prefrontal cortex (PFC), which is also a key site for seman-
regularities are encoded within the action representation sys-          tic WM. A particularly interesting result is from Averbeck et
tem, as discrete actions. But there is more to an event than            al. (2002). They showed that the PFC assembly that stores a
an action. In our model, experiencing an event also involves            prepared sequence of SM operations contains sub-assemblies
a higher-level sequence, of relatively discrete SM operations.          representing each individual operation—and moreover, that
One of these is the activation of an action representation. But         these sub-assemblies are active in parallel in the structure
this operation must be preceded by an action of attention to            representing a planned sequence, even though they represent
the agent, and then an action of attention to the patient (if           operations that are active one at a time. Our model will make
there is one). In our model, the notions of agent and patient           use of this finding.
are in fact defined by the serial order of attentional operrations
in this SM sequence: the (proto-)agent is the first individual                A model of semantic role-binding using
attended to; the (proto-)patient is the second.                            sequentially-structured WM representations
   Alongside this model of event perception, we also assume
                                                                        Modelling semantic WM representations as prepared se-
that the perception of each participant in an event involves its
                                                                        quences suggests a novel account of how semantic roles are
own canonically-structured sequence of SM operations. It is
                                                                        bound to participants in representations of events. Our ac-
well established that in order to classify an object, an observer
                                                                        count makes use of three ideas, which we introduce below.
must first direct focal attention to the region of space it occu-
pies. But observers can also attend to a region of space con-              The key idea is that the binding mechanism is implemented
taining a homogeneous group of objects. Walles et al. (2014)            as part of the active process of rehearsing SM routines, rather
argue that in between focal attention and object classifica-            than within a static representational structure. The classic
tion there is an intervening attentional operation that selects         binding problem arises because the SM media representing
a spatial scale at which the classifier will be deployed, de-           an individual’s properties (location, shape etc) naturally rep-
termining whether the classifier identifies the local or global         resent just one individual: if the properties of several indi-
form (Navon, 1977) of the attended stimulus. This operation             viduals are represented, it is hard to specify which properties
determines whether a single individual is classified or a ho-           belong to which individual. If a WM event representation
mogeneous group of individuals. In summary, perception of               supports the simulation of a sequential SM routine in which
an individual involves a SM routine comprising three oper-              representations of agent and patient are active in these media
ations: selection of a salient region of space, then selection          at different times, many of these problems go away.
of a classification scale (determining whether a singular or               Of course, the event representation must still make refer-
plural stimulus will be classified), and finally activation of an       ence to both participants, so it can activate these temporally
object category. Event perception, in turn, is a higher-level           separated representations. The second idea in our binding
sequential SM routine, some of whose elements have their                scheme is that event representations represent participants us-
own sequential structure.                                               ing pointers into the medium representing individuals—and
                                                                        that there are separate pointers for agent and patient. The
                                                                        pointers are active simultaneously in a WM event representa-
   WM representations as prepared sequences
                                                                        tion, but they are only followed sequentially, when an event is
We propose that representations in semantic WM exploit the              rehearsed. (Event representations thus conform to the prop-
sequential structure of perceptual processes. Specifically,             erties of prepared sequences identified by Averbeck et al.) In
we propose that WM representations of both individuals and              neural networks terms, agent and patient are coded ‘by place’
episodes take the form of prepared sequences. This proposal             in our WM event representations, in separate groups of units.
is attractive for several reasons. For one thing, it offers a clear     Place coding of this kind is not normally seen as a viable way
account of how semantic WM representations can influence                of implementing role-binding: a simple place-coding scheme
SM processing: a prepared SM sequence is an ‘executable’                suffers from the fact that there is nothing in common between
structure, that can initiate sequentially structured SM activity        representations of John-as-agent and John-as-patient. But if
(including actions). For another thing, it suggests an account          the place-coded representations of agent and patient just hold
of a puzzling recent finding: stimuli held in WM appear to              pointers into the medium representing individuals, which are
be transiently reactivated in SM areas during the delay pe-             activated at different times, this problem does not arise.
riod (see e.g. Meyers et al., 2008). If WM representations                 The third idea in our binding scheme is that the place-
are prepared SM routines, that can be executed in simulation,           coded pointers in WM event representations do not point di-
then active simulation processes could occur during the de-             rectly to SM media representing individuals, but rather to a
lay period, resulting in these transient patterns of SM activ-          WM medium holding representations of individuals. Recall
ity. Finally, the proposal places semantic WM representations           that representations of individuals also have internal struc-
within a class of neural representation that is relatively well         ture: we proposed above that the WM representation of an
understood. We know a lot about how prepared sequences of               individual is also stored as a prepared, replayable SM routine.
                                                                    2346

                                                                                                                                                                 predicted next
               WM media                                 CANDIDATE                                                   LTM media              CANDIDATE
                                                                                                                                                                 episode
                                                        WM INDIVIDUALS                                                                     EPISODES
                                                        (cWM-ind)                                                                          (c-ep) SOM
                                                                                                                                                                     current
                                                                                                                                                                     situation
                          WM INDIVIDUAL                                                                                 WM EPISODE
                                                         number       properties                           status            agent                patent                action
                           location
                            person         place          sing         gender    type          colour       new               person/number        person/number
                             1    2  3         ...                                    ...           ...
                                                          plur                                              old              properties/status    properties/status      ...
                                                                                                                  copy operations
                SM media     parietal cortex
                             (spatial attention)
                                                       temporoparietal junction
                                                       (scale attention)
                                                                                   inferotemporal cortex
                                                                                   (object classification)
                                                                                                                                                 parietal/premotor cortex
                                                                                                                                                 (action execution/perception)
                                                   Figure 1: Architecture of the model of semantic WM
In our model, WM representations of recently-perceived indi-                                    As noted above, the agent and patient media hold content-
viduals are held in a separate WM medium: the agent and pa-                                     addressed pointers to representations in the WM individual
tient representations in a WM event point to, and sequentially                                  medium. All the media within a WM episode are active in
re-activate, representations within this WM medium. Dur-                                        parallel, but when a WM episode is executed or rehearsed, the
ing rehearsal of a WM episode, these sequentially reactivated                                   representations they point to become active sequentially: the
representations create opportunities for secondary rehearsal                                    ‘agent’ and ‘patient’ media activate two successive represen-
operations, simulating the steps involved in perceiving the                                     tations in the WM individual medium, and then the ‘action’
participant individuals. This scheme introduces a measure                                       medium activates a representation in the (pre)motor system.
of hierarchy in the model of role-binding, enabling the repre-                                        The prepared operations in a WM episode also provide in-
sentations filling semantic roles to have a degree of internal                                  put to a layer holding episode representations learned over a
structure—an important requirement, as noted earlier.                                           longer timespan, the candidate episodes (c-ep) layer. This
                                                                                                layer is a self-organising map (SOM): when exposed to train-
 A model of WM episodes and WM individuals                                                      ing episodes, it learns to represent episodes as localist units,
Our model is illustrated in Figure 1. SM media are below the                                    organised so that similar episodes are close together in the
grey line; WM media are above it. The WM system repre-                                          map. Each unit can encode a particular combination of rep-
senting individuals is on the left, and that representing events                                resentations in the agent, patient and action media, and thus
(or ‘episodes’, as we call them here) is on the right. The copy                                 can represent a complete episode by itself. Note this localist
operations implementing pointers are highlighted in red.                                        scheme is enabled by our model of binding: the ‘agent’ and
   The WM medium on the left holds a representation of a                                        ‘patient’ fields of a WM episode index their fillers by place,
single selected individual, a WM individual, stored as a pre-                                   so carry information about both roles and fillers. Clearly, we
pared sequence of a location, a number (i.e. classification                                     cannot represent every possible episode using localist units.
scale) and a set of perceptual properties. These three rep-                                     But that is not the purpose of the c-ep SOM: its role is rather
resentations are activated in parallel in the WM medium, but                                    to represent the episodes that occur frequently, so these can
when the prepared sequence is executed or rehearsed, they ac-                                   provide a top-down bias on SM processing during experience.
tivate associated first-order representations in the attentional                                Since the c-ep SOM uses localist representations, it can also
and classification systems one at a time, as discussed above.                                   represent multiple expected episodes simultaneously: a use-
   The media representing a WM individual provide input to                                      ful property, as we will show.
another layer, the candidate WM individuals (cWM-ind)                                                 A final component of the network is a layer representing
layer, which stores combinations of location, number and                                        the current situation. In our model, this is the hidden layer
type over a short time period, and thus represents the set of                                   of a recurrent network that learns to predict the next episode,
recently-attended individuals. A partially specified WM indi-                                   given the episode that has just occurred, plus a copy of its
vidual can function as a query to the cWM-ind layer: if we                                      hidden layer at the previous time point. The current situa-
specify a location, we may be able to retrieve an associated                                    tion network learns to predict a distribution of possible next
type and number (and vice versa). If we can, then the indi-                                     episodes in the c-ep SOM (exploiting its ability to represent
vidual retrieved is classed as ‘old’; if we cannot, it is classed                               multiple episodes). (It is somewhat analogous to Reynolds
as ‘new’. These attributes are recorded in a status field of the                                et al.’s (2007) recurrent network for event representation, but
WM individual, which is not part of the prepared sequence.                                      Reyonlds et al.’s network predicts the next component of an
Queries formed from partially-specified WM individuals can                                      episode, rather than the next discrete episode.)
be used to generate expectations about the location or proper-                                        One useful feature of our model is that that the c-ep SOM
ties of individuals in the current scene, as we discuss below.                                  can learn generalisations over episodes. One kind of general-
   The WM episodes system is structurally similar to the WM                                     isation is hard coded in the model: the copies of WM individ-
individuals system. It holds a representation of a single se-                                   uals created in the agent and patient fields ignore location in-
lected episode, a ‘WM episode’, stored as a planned sequence                                    formation, so representations of episodes in the SOM abstract
of operations activating an agent, a patient and an action.                                     over the location of participants. In our model, expectations
                                                                                          2347

about the locations of objects are dealt with in the WM indi-         We begin by describing the properties of the individuals that
viduals system, as we will illustrate below. This step consid-        featured in episodes. We created a fixed population of to-
erably reduces the combinatorial possibilities that need to be        ken individuals: each with a type, a number, and location and
represented in the SOM. But the SOM also learns generalisa-           colour properties that are stochastically chosen based on its
tions of its own. The ability to generalise is a standard feature     type. Locations are quasi-randomly generated as positions on
of learning in SOMs, since episodes that are sufficiently sim-        a 100×100 grid (which in the system’s 6×6 location medium
ilar will activate the same localist unit. In particular, since       are represented using coarse coding). Colours are stochasti-
the representations of agents and patients providing input to         cally generated from Gaussian distributions centred on 11 ba-
the SOM are distributed, the SOM can learn to abstract away           sic colours. We then generated a stream of episodes involving
from the properties of token individuals and represent the par-       these token individuals. Each episode is presented to the WM
ticipants of episodes as types, as we will show.                      system as a sequence of SM input items. Episodes are of three
   Here are some technical details about the network’s ar-            types: transitive (agentpatienttrans-action), intransitive
chitecture. The WM individual layer consists of localist              (agentintrans-action) and causative (agentpatientcause-
sets of feature units for person (1/2/3), number (Sg/Pl), gen-        signalunaccusative-action). In each case the agent and pa-
der (Male/Female/Neuter) and status (new/old). Each set               tient signals has a sequential structure of their own, namely
of units can either encode a single property unambiguously,           locationnumbertype/properties. Each of these latter se-
or a probability distribution over properties. The type area          quences is sent to the WM individuals medium, activating the
also contains feature sets coding animacy, and object type            different components of a WM individual representation one
(person/dog/cat/bird/cup/ball/chair). Object location, situ-          by one. When complete, the WM individual was first passed
ated on a 100×100 grid, is coded by a population of 6×6               as a query to the cWM-ind layer, to find out whether the in-
neurons with Gaussian receptive fields evenly covering the            dividual it represents has already been encountered. For each
grid. Colour is coded by a population of 11 neurons with              candidate unit currently active in the cWM-ind layer, we com-
Gaussian receptive fields in 3D RGB space maximally re-               pute the likelihood that it corresponds to the current stimulus
sponding to standard 11 basic colours (see Figure 3a). Such           in the WM individual (Jazayeri and Movshon, 2006). This
population coding is neurally plausible and there is a straight-      reduces to the average pairwise Kullback-Leibler divergence
forward mathematical way of computing the likelihoods of              between the respective areas of the WM individual and the
different stimuli given the activities of neurons in the pop-         candidate unit weights (see Takac and Knott, 2016 for de-
ulation (Jazayeri and Movshon, 2006). SM representations              tails). If a likely-enough candidate is returned, it is updated
(below the grey line in Figure 1) are isomorphic to the WM            if necessary and the WM individual’s status is set to ‘old’;
areas they interface with. Likewise, agent and patient lay-           otherwise a new entry in the layer is created and the WM in-
ers of a WM episode are isomorphic to the relevant parts of           dividual’s status is set to ‘new’ (candidate units that have not
a WM individual. The ‘action’ area consists of 22 localist            been updated for N episodes are removed). The WM indi-
units for actions (see the x-axis legend in Figure 2a) and 11         vidual is then copied (along with its status) to the appropriate
units for their distributed featural codes. The cWM-ind layer         layer in the WM episode medium: either the ‘agent’ layer
is a variable-sized convergence zone of units fully connected         or the ‘patient’ layer. When a complete episode has been pre-
with the WM individual layer. When a novel candidate in-              sented to the system, the layers in the WM episode are passed
dividual is encountered, a new unit in the cWM-ind layer is           as input to the c-ep SOM. This SOM learns in the standard
recruited and the current values of WM individual units are           way. Note that while learning in the cWM-ind layer happens
copied into its connection weights (one-shot learning). The           in a ‘one-shot’ manner, it only happens gradually in the c-ep
c-ep layer is a SOM with 400 units. Each unit also main-              SOM, current situation and next episode media.
tains a scalar weight reflecting the frequency of ‘hits’ for this
unit, i.e. the number of times it was the most active unit.           Testing the sequence-based binding scheme To demon-
These frequency weights serve as priors for computing the             strate the new binding scheme, we must show how the WM
Bayesian probability that the current input corresponds to an         representations created during experience of an episode allow
episode represented by a particular unit (for details see Takac       it to be replayed. To test this, after each episode is presented,
and Knott, 2016). The network that represents the current             the WM episode medium is used as input to a replay process,
situation is a recurrent SOM (a ‘Merge SOM’, Strickert and            in which the layers in this medium activate the representations
Hammer, 2005). This provides input to a layer of linear per-          they point to one by one. When the activated representations
ceptrons which are trained to predict the next episode. Details       are in the WM individual layer, they are used as a query to
of all these networks can be found in Takac and Knott (2016).         retrieve a location representation (recall that the location of
                                                                      individuals is not copied to the WM episode). If the binding
         Training and testing of the network                          scheme is effective, this process should regenerate the same
                                                                      sequence of first-order SM signals that was presented to the
Training We trained the network by simulating SM experi-              network during experience of the episode. In our tests, the
ence of a sequence of episodes. Each episode is represented           sequence was perfectly reconstructed for 99.6% of episodes;
in the SM system as a complex sequence of SM operations.              this shows that our proposed binding mechanism is effective.
                                                                  2348

               1                                                                                                      (Woman) Dog ...
              0.5                                                                       1
                                                                                      0.8                                                    cWM-ep
               0
                                                                           Activity
               1                                                                      0.6                                                   cWM-ind
   Activity
              0.5                                                                     0.4
                                                                                      0.2
               0
               1                                                                        0
                                                                                            White   Yellow
                                                                                                                      Red   Pink
                                                                                                                                   Purple
                                                                                                                                            Blue
                                                                                                             Orange                                Green   Grey
                                                                                                                                                                  Brown   Black
              0.5
               0
                       Grab
                         Hit
                       Push
                       Walk
                        Run
                         Lie
                          Sit
                       Sing
                        See
                     Snore
                    Sneeze
                      Sleep
                                                                                                                       (Man) Dog ...
                       Hold
                        Hug
                        Bite
                        Kick
                      Break
                       Stop
                       Hide
                         Go
                         Pat
                     Stroke
                                                                                        1
                       (a)                          (b)                               0.8
                                                                           Activity
                                                                                      0.6
                                                                                      0.4
                                                                                      0.2
Figure 2: (a) Action types predicted in the c-ep layer for                              0
                                                                                            White   Yellow   Orange
                                                                                                                      Red   Pink
                                                                                                                                   Purple
                                                                                                                                            Blue
                                                                                                                                                   Green   Grey
                                                                                                                                                                  Brown   Black
3 episode fragments. From top to bottom: mandog?,
mancat?, bird?. (b) Patient types predicted for Se-
quence A (top) and Sequence B (bottom).                                                                                 (a)                                                       (b)
                                                                   Figure 3: (a) Expectations about the colour of the patient
                                                                   generated by the c-ep and cWM-ind layers for episodes
Testing the network’s prediction/generalisation abilities
                                                                   womandog (top) and mandog (bottom). (b) Expecta-
The network can make several kinds of prediction; we will fo-
                                                                   tions about location of the patient generated by the cWM-
cus on three progressively more complex predictions. Firstly,
                                                                   ind layer for these episodes. Darker areas mean stronger ex-
the c-ep SOM can make predictions about the episode cur-
                                                                   pectations. Black dots represent actual locations of currently
rently being experienced, as experience is under way. Predic-
                                                                   present white dogs (top) and black dogs (bottom).
tions about actions are easiest to demonstrate, since it repre-
sents actions directly. To evaluate these predictions, we intro-
duced some regularities in the episodes presented to the sys-
                                                                   of the resulting WM individual expectation. The system cor-
tem. Birds always sang (birdsing); also when people inter-
                                                                   rectly predicts a colour centred on black in RGB space for
acted with dogs and cats, they always patted dogs and stroked
                                                                   mandog episodes, and on white for womandog episodes.
cats (persondogpat, personcatstroke). We presented
                                                                   Importantly, the cWM-ind layer can also generate expecta-
the c-ep SOM with episodes involving these participants,
                                                                   tions about the location of the dog—see Figure 3b. There is
leaving the action field blank, and generated a distribution
                                                                   a general bias towards the quadrant containing animals, since
of expected episodes in the SOM; from this we reconstructed
                                                                   dogs always appear in this quadrant. But there are also spe-
a distribution of expected actions, by linear combination of
                                                                   cific biases towards the location of the black or white dogs
the weight vectors of SOM units, weighted by unit activity.
                                                                   that the system has recently encountered, that are based on
Figure 2a shows these distributions are correctly weighted to-
                                                                   its expectations about the colour of the patient dog.
wards the actions encountered during training.
                                                                      Finally, the network can make predictions about the next
   The c-ep SOM can also make predictions about the agents
                                                                   episode, using its representation of the current situation.
and patients of episodes. These are more complex, because
                                                                   To show this, we presented it with a sequence of training
its predictions are relayed to the WM individuals system,
                                                                   episodes, with constraints on transitions between episodes:
which refines them based on its own knowledge. To illus-
                                                                   when a person hit a dog and then patted the same dog (‘se-
trate this process, we introduced a further regularity into the
                                                                   quence A’), the dog always bit the person; when a person
training episodes: in all episodes involving people interact-
                                                                   patted a dog after some other episode (‘sequence B’), any
ing with dogs, the dogs were black if the agent was a man,
                                                                   random episode could follow. We tested the network by pre-
and white if the agent was a woman; additionally, people al-
                                                                   senting sequences A and B, propagating activity through the
ways appeared in the top-left quadrant of the spatial array,
                                                                   Current situation and Next episode prediction layers to ob-
and animals in the top-right quadrant. We then generated
                                                                   tain a prior distribution over predicted next episodes in the
an underspecified representation in the WM episode: in the
                                                                   c-ep layer. From this we reconstructed an expected distribu-
agent part, we activated a representation of a person (either
                                                                   tion of agents, patients and actions in the WM episode. The
man or woman), and in the patient part we activated the type
                                                                   network correctly predicted ‘dog-bite-man’ after Sequence A,
‘dog’, unspecified for colour. We used this representation to
                                                                   and made a more neutral prediction after Sequence B (see
generate a distribution in the c-ep SOM, from which we re-
                                                                   Figure 2b). The learned update rule encodes something like
constructed a predicted distribution of patient features. This
                                                                   ‘If a man hits a dog then pats it, the dog bites the man.’
whole distribution was copied to the WM individual medium,
where it activated a distribution of units in the cWM-ind net-
                                                                      Roles for the network in language processing
work. This distribution was used top-down to reconstruct a
distribution over expected locations for the patient individual,   As discussed at the outset, semantic WM representations
and to refine the distribution over expected types and proper-     must do service in language processing as well as in SM pro-
ties. Figure 3a shows activity in the colour-coding features       cessing. Our WM model was designed with this in mind.
                                                               2349

We envisage several linguistic roles for the network. The                      One question concerns space requirements. Our place-coding
cWM-ind medium can function as the medium storing salient                      scheme for event participants requires creating several sepa-
discourse referents, and the current situation medium can be                   rate copies of the WM individual medium, which is expen-
understood as holding a representation of the current dis-                     sive as regards storage space. However, storage is within
course context; for details on these ideas, see Takac and Knott                acceptable limits when scaled up to a memory of realistic
(2016). We also envisage that the WM model plays a role in                     size, as discussed in Takac and Knott (2016). Another ques-
sentence processing. Specifically, we propose that generat-                    tion concerns recursive representations. While the model de-
ing a sentence reporting an episode stored in WM involves                      scribed here implements a notion of hierarchical representa-
rehearsing this episode, in a special cognitive mode in which                  tions, these representations are not recursive. Again, this is-
SM/WM representations can trigger output phonology. We                         sue is addressed in Takac and Knott (2016), where we argue
built a neural network model of sentence generation imple-                     that the current scheme extends well to an account of com-
menting this idea (Takac et al., 2012). Within this model,                     plement clauses, subordinate clauses, and relative clauses.
we argue that the syntactic concept of a head can actually
be derived from the architecture of the semantic WM sys-                       Acknowledgements
tem. In syntactic theory, information conveyed by a syntac-                    This work was supported by the New Zealand Marsden Foun-
tic head spreads through its local syntactic domain (e.g. a                    dation through Grant 13-UOO-048. Martin Takac was also
clause or noun phrase (NP)). This spreading process is seen                    partially supported by Slovak KEGA grant 017UK-4/2016.
most clearly in agreement phenomena: for instance, subject-
verb agreement within a clause, or determiner-noun agree-                                                 References
                                                                               Averbeck, B., Chafee, M., Crowe, D., and Georgopoulos, A. (2002).
ment within a NP). In a NP, agreement rules relate to the                         Parallel processing of serial movements in prefrontal cortex.
head features person, number, type, semantic gender and                           PNAS, 99(20), 13172–13177.
definiteness: exactly the information that is maintained ton-                  Baddeley, A. (2000). The episodic buffer: A new component of
ically in a WM individual during its rehearsal.2 Heads in a                       working memory? TICS, 4(11), 417–423.
clause can convey all this information about the subject and                   Dowty, D. (1991). Thematic proto-roles and argument selection.
                                                                                  Language, 67(3), 547–619.
object, and additionally information about the type of the ac-                 Jazayeri, M. and Movshon, A. (2006). Optimal representation of
tion: exactly the type of information maintained tonically in                     sensory information by neural populations. Nature Neuroscience,
a WM episode during its rehearsal. We argue that syntac-                          9(5), 690–696.
tic heads have an extended syntactic domain because they are                   Johansson, R., Westling, G., Backstrom, A., and Flanagan, J. (2001).
                                                                                  Eye-hand coordination in object manipulation. Journal of Neuro-
read from WM representations that are tonically active during                     science, 21(17), 6917–6932.
rehearsal, and so can influence phonology at multiple points.                  Knott, A. (2012). Sensorimotor Cognition and Natural Language
Again see Takac and Knott (2016) for further discussion.                          Syntax. MIT Press, Cambridge, MA.
                                                                               Meyers, E., Freedman, D., and Kreiman, G. et al. (2008). Dynamic
                                                                                  population coding of category information in inferior temporal
                              Discussion                                          and prefrontal cortex. J.Neurophys, 100, 1407–19.
In this paper we propose that events and their participants are                Navon, D. (1977). Forest before trees: The precedence of global
                                                                                  features in visual perception. Cognitive Psychology, 9, 353–383.
represented in WM as prepared SM sequences. This has sev-
                                                                               Oztop, E., Bradley, N., and Arbib, M. (2004). Infant grasp learning:
eral benefits. It permits a direct account of how WM event                        a computational model. Exp. Brain Research, 158, 480–503.
representations are created during experience, and of how                      Reynolds, J., Zacks, J., and Braver, T. (2007). A computational
they in turn influence event perception by generating top-                        model of event segmentation from perceptual prediction. Cogni-
down expectations. It enables a new model of role-binding,                        tive Science, 31, 613–643.
                                                                               Stewart, T. and Eliasmith, C. (2012). Compositionality and biologi-
that allows hierarchical representations of event participants,                   cally plausible models. In The Oxford Handbook of Composition-
and localist representations of candidate events, through a                       ality. Oxford University Press, New York.
novel use of indexing and place-coding. This in turn allows                    Strickert, M. and Hammer, B. (2005). Merge SOM for temporal
the model to represent large distributions of expected events:                    data. Neurocomputing, 64, 39–71.
                                                                               Takac, M. and Knott, A. (2016). A simulationist model of episode
a very useful ability, which the models of van der Velde and                      representations in working memory. Technical Report OUCS-
de Kamps (2006) and Stewart and Eliasmith (2012) do not                           2016-01, Dept of Computer Science, University of Otago.
have. Finally, the network supports several aspects of a model                 Takac, M., Benuskova, L., and Knott, A. (2012). Mapping senso-
of language processing; most interestingly, it creates a frame-                   rimotor sequences to word sequences: A connectionist model of
                                                                                  language acquisition and sentence generation. Cognition, 125,
work within which aspects of syntactic structure can seen as                      288–308.
deriving from the structure of the semantic WM system.                         van der Velde, F. and de Kamps, M. (2006). Neural blackboard
    The model as illustrated here leaves many open questions.                     architectures of combinatorial structures in cognition. Behavioral
                                                                                  and Brain Sciences, 29, 37–108.
    2 ‘Person’ is conveyed as a special kind of location information:          Walles, H., Robins, A., and Knott, A. (2014). A perceptually
the location can be a point in external space (in which case the point            grounded model of the singular-plural distinction. Language and
is also specified) but it can also be a direct reference to the speaker or        Cognition, 6, 1–43.
interlocutor, whose location is presumed to be independently given.            Webb, A., Knott, A., and MacAskill, M. (2010). Eye movements
Type and semantic gender are expressed within the complex of per-                 during transitive action observation have sequential structure.
ceptual properties. Definiteness is expressed by status (new or old).             Acta Psychologica, 133, 51–56.
                                                                           2350

