   Should Moral Decisions be Different for Human and Artificial Cognitive Agents?
                                           Evgeniya Hristova (ehristova@cogs.nbu.bg)
                                             Maurice Grinberg (mgrinberg@nbu.bg)
                      Research Center for Cognitive Science, Department of Cognitive Science and Psychology
                                                        New Bulgarian University
                                                21 Montevideo Str., Sofia 1618, Bulgaria
                               Abstract                                approaches to morality when it concerns artificial agents –
  Moral judgments are elicited using dilemmas presenting
                                                                       consequentialist (utilitarian) and deontological (see also
  hypothetical situations in which an agent must choose                Allen, Smit, & Wallach, 2005). Concerning moral
  between letting several people die or sacrificing one person in      evaluation, these approaches give quite different
  order to save them. The evaluation of the action or inaction of      perspectives on moral agency for artificial cognitive agents
  a human agent is compared to those of two artificial agents –        (Wallach & Allen, 2009). The utilitarian approach is not
  a humanoid robot and an automated system. Ratings of                 concerned with the protagonist or the reason of a moral
  rightness, blamefulness and moral permissibility of action or        action but only on the utility of the outcome, so it does not
  inaction in incidental and instrumental moral dilemmas are
  used. The results show that for the artificial cognitive agents      differentiate between human and artificial cognitive agents.
  the utilitarian action is rated as more morally permissible than     On the other hand, the deontological approach considers the
  inaction. The humanoid robot is found to be less blameworthy         nature of the agent and it implies that different agents (e.g.
  for his choices compared to the human agent or to the                human or artificial) can have different kind of duties.
  automated system. Action is found to be more appropriate,               This distinction in the approach to moral choice and its
  morally permissible, more right, and less blameworthy than           evaluation is used in this paper to investigate how people
  inaction only for the incidental scenarios. The results are
                                                                       perceive artificial agents while making moral decisions. If
  interpreted and discussed from the perspective of perceived
  moral agency.                                                        participants have a more utilitarian attitude, they are
                                                                       expected to rate agents’ behavior similarly, based on the
   Keywords: moral dilemmas; moral judgment; artificial                perceived utility of the outcome. If participants have a more
   cognitive agents; moral agency
                                                                       deontological attitude, they would rate differently the
                                                                       human and the artificial agents depending on the degree to
                                                                       which they consider them to be moral agents and hence,
                           Introduction                                morally responsible.
Moral Dilemmas and Artificial Cognitive Agents                         Moral Agency and Artificial Cognitive Agents
Moral judgments and evaluation of moral actions have been              The possibility for moral agency of artificial agents has been
of great interest to philosophers and psychologists. Apart             a matter of hot debate (e.g. see Anderson & Anderson,
from the practical importance of better understanding moral            2011; Wallach & Allen, 2008; Johnson, 2006). It is debated
judgments and related actions, morality is an essential part           if the robots should be authorized to kill in moral dilemma
of human social and cognitive behaviour. Recently, the                 situations, and if so, what rules should govern the real-time
behaviour of artificial cognitive agents became central to             decisions that are necessary to determine whether killing
research and public debate in relation to the rapidly                  any particular person is justified (Sparrow, 2007; Wallach &
increasing usage of robots and intelligent systems in our              Allen, 2008).
everyday life. Several important questions must find their                In this paper, we want to explore the differences in moral
answers as the use of artificial cognitive agents has many             agency evaluation depending on the type of agent - human
benefits but also many risks. Some of those questions                  or artificial. So, it is important to take into account the
concern moral agency - if those agents should be allowed to            attribution of mind and moral agency to artificial cognitive
make moral decisions and how such decisions are judged                 systems (Waytz, Gray, Epley, & Wegner, 2010).
and evaluated.                                                            In the study of Gray et al. (2007), participants had to
  Moral judgments can be studied in their purest form using            evaluate several characters including a human, a robot, and
moral dilemmas – situations in which there is a conflict               a computer with respect to the degree of possessing various
between moral values, rules, rights, and agency (Foot, 1967;           cognitive capacities. The authors further established that
Thomson, 1985). Moral dilemmas used in the paper are                   moral judgments about punishment correlated more with
hyopothetical situations in which several people will die if           one of the revealed dimensions – ‘agency’ – than with the
the agent does not intervene in some way. The intervention             second dimension – ‘experience’. In Gray et al. (2007), the
will lead to the death of another person but also to the               human obtains the highest scores on the ‘experience’ and
salvation of the initially endangered people.                          ‘agency’ dimensions while the robot has practically zero
  Analogously to the two main approaches to human                      score on the ‘experience’ and half the maximal score on the
morality, Gips (1995) identifies two basic theoretical                 ‘agency’ scales. Following the interpretation given by the
                                                                   1511

authors, this implies that robots are judged as less morally           Another goal of the study is to explore the influence of
responsible for their actions compared to humans.                   the so-called ‘instrumentality’ of harm on moral judgments.
                                                                    The instrumentality of harm is an important factor in moral
Moral Judgments about the Actions of Artificial                     dilemma research (e.g., Borg et al., 2006; Hauser et al.,
Cognitive Agents                                                    2007; Moore et al., 2008). It draws attention to the fact that
Until recently, research involving moral dilemmas                   harm could be either inflicted intentionally as a ‘mean to an
considered mainly a human agent. Only during the last               end’ (instrumental harm) or it could be a ‘side effect’
years, several empirical studies appeared, exploring the            (incidental harm) from the actions needed to save more
moral judgments about the actions of artificial cognitive           endangered people. It has been found that the unintended
agents in moral dilemmas (Scheutz & Malle, 2014; Malle,             incidental harm (although being foreseen) was judged as
Scheutz, Arnold, Voiklis, & Cusimano, 2015; Hristova &              more morally permissible than the intended instrumental
Grinberg, 2015).                                                    harm (Hauser et al., 2007; Moore et al., 2008).
   Malle et al. (2015) compared moral judgments about a                The utilitarian action is expected to be rated as more
human and a state-of-the-art robot agent choosing the               appropriate, more right, more morally permissible, and less
utilitarian action or inaction. They used a modified version        blameworthy when the harm is incidental (compared to
of the Trolley problem in which the death of the person to          instrumental). However, the discussion about perceived
be killed is a side-effect of the action undertaken by the          moral agency suggests that the difference in moral
agent. They found that it is more permissible for a robot           judgments for the artificial and human agents will be greater
(compared to a human) to do the utilitarian action. It was          when the harm is instrumental, as such actions involve more
also found that a human agent is blamed more the utilitarian        responsibility and direct infliction of harm.
action than for inaction while the robot was equally blamed            The experiment collected ratings on the rightness, moral
for both.                                                           permissibility, and blameworthiness of performing or not
   In another study (Scheutz & Malle, 2014), a means-end            the utilitarian action. The various questions asked can target
scenario was used. In was found that the utilitarian action is      different aspects of the evaluation of moral choices, as some
judged to be both more morally wrong and more blameful              studies suggest (Christensen & Gomila, 2012; Cushman,
than inaction for the human and the robot agent.                    2008). According to Cushman (2008), answers to questions
   While the goal of Scheutz & Malle (2014) and Malle et al.        about punishment and blame concern the harm agents have
(2015) was to study the expectations of people of state-of-         caused, whereas answers to questions about rightness and
the-art robots and inform future robot design, Hristova &           moral permissibility are related to the agent's intentions.
Grinberg (2015) had the goal to explore the moral agency            Thus, depending on the type of agents – human or artificial
ascribed to hypothetical future artificial cognitive agents,        – the different questions can give information about
which are indistinguishable from humans except for being            different aspects of people’s perception of moral agency.
built from inorganic materials. The study of Hristova &                If people evaluate action/inaction in the moral dilemmas
Grinberg (2015) contains only the results concerning the            in a pure utilitarian way (in which the outcome is
judgment of the utilitarian choice of the agents.                   important), one could expect that the ratings for all agents
   The present paper combines the results presented in              will be the same (Wallach & Allen, 2009). On the other
Hristova & Grinberg (2015) with results about the                   hand, if evaluation is based on perceived moral agency of
judgments of inaction by the agents and compares them.              the agents, differences in the judgments are expected.
Additionally, a test for moral agency, specifically
concerning the agents used in the present study, has been                                      Method
carried out whose results are included in the discussion.
                                                                    Stimuli and Design
                  Goals and Hypotheses                              Moral judgments are studied in a 3×2×2 factorial design
The goal of this paper is to compare the moral judgments            with identity of the agent (human vs. humanoid robot vs.
about the choices of a human agent, a humanoid robot (who           automated system), agent’s choice (action vs. inaction), and
is exactly like a human in terms of experiences and mind but        instrumentality of harm (instrumental vs. incidental) as
has a non-organic body), and an automated system.                   between-subjects factors.
   Table 1 contains the description of the agents. The only            Two hypothetical scenarios are used – an instrumental
difference between the human and the humanoid robot,                one and an incidental one. Both scenarios present one and
presentation of the agents, is the material the latter is built     the same situation and require one and the same action.
from. The automated system, on the other hand, is described         The difference between the scenarios is only in the harm
as autonomous, free, and adaptable but lacks experiencing.          inflicted to the person to be killed: in the instrumental
   The expectation is that despite the fact that the humanoid       scenario the body of the person is the ‘instrument’
robot supposedly has all the features for full morally              preventing the death of the five endangered people; while in
responsible agency, people will evaluate its action or              the incidental scenario, the death of the person is a by-
inaction differently compared to those of a human agent.            product.
                                                                1512

                                      Table 1: Stimuli used in the experiment.                            In each scenario, the identity of the agent is varied (a
                                                                                                       human, a robot, or an automated system) by providing a
                               [Human] No description is given, just the name is provided –            name for the protagonist and an additional description in the
                               Cyril – a common male name in Bulgarian.                                case when the protagonist is a robot or an automated system.
                               [Humanoid robot] The year is 2050. Humanoid robots that                    For each scenario and each agent, the agent choice is
                               look like people are being manufactured and used, but are               either action (the utilitarian action) or inaction.
Description of the agent
                               made from inorganic materials. Robots have extremely high
                                                                                                          Each participant read only one of the resulting 12
                               performance – they perceive, think, feel, and make decisions
                               as humans do. Keido is such a humanoid robot that completely
                                                                                                       scenarios given in Table 1.
                               resembles a human – he looks like a human; perceives, thinks,
                               feels and make decisions like a human.                                  Dependent Measures and Procedure
                               [Automated system] The year is 2050. MARK21 is a fully                  The flow of the presentation of the stimuli and the questions
                               automated management system, which independently makes                  is the following.
                               its own decisions, based on the most advanced algorithms and               First, the scenario is presented (description of the agent,
                               technologies. Such systems are widely used in metallurgical
                                                                                                       the situation and the possible resolution, see Table 1) and
                               plants. They completely independently perceive and assess the
                               environment and the situation, make decisions, manage the               the participants answer a question assessing the
                               movement of cargo and all aspects of the manufacturing                  comprehension of the scenario.
                               process.                                                                   Then, before knowing what the agent has chosen, the
                               Cyril/Keido/MARK21 manages the movement of mine                         participants make a judgment about the appropriateness of
                               trolleys     with      loads     in     a    metallurgical     plant.   the possible agent’s choice (action/inaction) answering a
                               Cyril/Keido/MARK21 noticed that the brakes of a loaded                  question about what the agent should do (possible answers
Situation
                               trolley are not functioning and it is headed at great speed             are ‘should activate the control button’ or ‘should not
                               toward five workers who perform repair of the rails. They do            activate the control button’).
                               not have time to escape and they will certainly die.
                                                                                                           Next, the participants read a description of the choice of
                               Nobody, except for Cyril/Keido/MARK21, can do anything in
                               this situation.
                                                                                                       the agent (action – the agent activates the control button, or
                               The only thing Cyril/Keido/MARK21 can do is to activate a               inaction – the agent does nothing) and the resolution of the
                               control button and to release                                           situation. After that, they give ratings about the rightness,
                               [Instrumental scenario] the safety belt of a worker hanging             the moral permissibility, and the blameworthiness of the
Possible resolution
                               from a platform above the rails. The worker will fall onto the          described agent’s choice.
                               rails of the trolley. Together with the tools that he is equipped          The Likert scales used are respectively: for the rightness
                               with, the worker is heavy enough to stop the moving trolley.            of the choice – from 1 = ‘completely wrong’ to 7 =
                               [Incidental scenario] a large container hanging from a                  ‘completely right’; for the moral permissibility of the choice
                               platform. It will fall onto the rails of the trolley. The container     – from 1 = ‘not permissible at all’ to 7 = ‘it is mandatory’;
                               is heavy enough to stop the moving trolley. On the top of the
                                                                                                       and for the blameworthiness of the agent – from 1 = ‘not at
                               container there is a worker who will also fall on the rails.
                               He will die, but the other five workers will stay alive.                all blameworthy’ to 7 = ‘extremely blameworthy’.
                               Agent choosing the utilitarian action                                      All data is collected using web-based questionnaires.
                               Cyril/Keido/MARK21 decides to activate the control button
                               and to release                                                          Participants
                               [Instrumental scenario] the safety belt of the worker hanging           Three hundred seventy (370) participants answered the on-
                               from the platform. The worker falls onto the rails of the trolley       line questionnaires.    42 participants failed to answer
                               and as together with the tools that he is equipped with, the
                                                                                                       correctly the question assessing the reading and the
                               worker is heavy enough, he stops the moving trolley. He dies,
                                                                                                       understanding of the presented scenario and their data was
Agents choice and resolution
                               but the other five workers stay alive.
                               [Incidental scenario] the container hanging from the platform.          discarded. So, the responses of 328 participants (230 female,
                               It falls onto the rails of the trolley and as the container is heavy    98 male; 148 students, 180 non-students) were analyzed.
                               enough, it stops the moving trolley. The worker onto the top            Between 26 and 31 participants took part in each
                               of the container dies, but the other five workers stay alive.           experimental condition.
                               Agent choosing not to do the utilitarian action                                                   Results
                               Cyril/Keido/MARK21 decides not to activate the control
                               button that could release
                               [Instrumental scenario] the safety belt of the worker hanging
                                                                                                       Decisions about the Agent’s Action
                               from the platform. The worker hanging from the platform                 The proportion of participants choosing the option that the
                               stays alive, but the trolley continues on its way and the five          agent should carry out the utilitarian action (activating a
                               workers on the rails die.                                               control button, thus sacrificing one person, and saving five
                               [Incidental scenario] the container hanging from the platform.          people) is presented in Table 2.
                               The worker onto the top of the container stays alive, but the             The data was analyzed using a logistic regression with
                               trolley continues on its way and the five workers on the rails
                                                                                                       instrumentality of harm and identity of the agent as
                               die.
                                                                                                       predictors. Wald criterion demonstrated that only
                                                                                                       instrumentality of harm is a significant predictor of the
                                                                                                   1513

participants’ choices (p < .001, odds ratio = 3.03). Identity           No other main effects or interactions were found to be
of the agent is not a significant predictor.                         statistically significant.
                                                                        In summary, for the artificial agents (a humanoid robot or
Table 2: Proportion of the participants choosing the option
                                                                     an automated system), choosing the utilitarian action is rated as
that the utilitarian action should be done by the agent
                                                                     more right than not choosing it, while there is no such
  Agent                     Instrumental Incidental         All
                                                                     difference for the choices of the human agent.
                            harm            harm
  Human                     0.60            0.79            0.70     Moral Permissibility of the Agent’s Choice
  Humanoid robot            0.60            0.85            0.74     Mean ratings of the moral permissibility of the agent’s
                                                                     choice were analyzed in a factorial ANOVA with agent’s
  Automated system          0.65            0.85            0.75     choice (action vs. inaction), identity of the agent (human vs.
                                                                     humanoid robot vs. automated system), and instrumentality
  All                       0.62            0.83
                                                                     of harm (instrumental vs. incidental) as between-subjects
                                                                     factors.
   More participants stated that the utilitarian action should be       There is a main effect of agent’s choice on the ratings of
undertaken when the harm is incidental (83% of the                   the moral permissibility of the agent’s choice (F(1, 316) =
participants) than when it is instrumental (62% of the               6.22, p = .013). In general, if the agent chooses to do the
participants). This effect is expected based on previous research    utilitarian action, he gets higher moral permissibility ratings
(Borg et al., 2006; Hristova et al., 2014; Moore et al., 2008).      (M = 4.25, SD = 1.8) than when the agent does not choose
                                                                     the utilitarian action (M = 3.75, SD = 1.5).
Rightness of the Agent’s Choice                                         Marginally significant interaction between identity of the
Mean ratings of the rightness of the agent’s choice were             agent and agent’s choice (F(1, 316) = 2.386, p = .094) was
analyzed in a factorial ANOVA with agent’s choice (action            also found (see Figure 2). For the humanoid robot, action
vs. inaction), identity of the agent (human vs. humanoid             (M = 4.5, SD = 1.9) is rated as more morally permissible
robot vs. automated system) and instrumentality of harm              than inaction (M = 3.8, SD = 1.5), F(1, 112) = 5.75, p =
(instrumental vs. incidental) as between-subjects factors.           .018. For the automated system, again the action (M = 4.5,
   There is a main effect of agent’s choice on ratings of the        SD = 1.7) is rated as more morally permissible than inaction
rightness of the agent’s choice (F(1, 316) = 35.8, p < .001).        (M = 3.7, SD = 1.7), F(1, 108) = 5.42, p = .022. For the
In general, if the agent chooses to do the utilitarian action,       human agent, there is no significant difference in the ratings
he gets higher approval (M = 4.6, SD = 1.7) compared to the          for the moral permissibility of the agent’s choice (p = .76,
situations in which the agent chooses not to perform the             M = 3.7 for action, M = 3.8 for inaction).
utilitarian action (M = 3.5, SD = 1.6).
 Figure 1: Mean ratings with standard errors of the rightness           Figure 2: Mean ratings with standard errors of the moral
      of the agent’s choice on a 7-point Likert scale (1 =            permissibility of the agent’s choice on a 7-point Likert scale
          ‘completely wrong’, 7 = ‘completely right’).                     (1 = ‘not permissible at all’, 7 = ‘it is mandatory’).
   However, this main effect is qualified by a significant              In summary, for the artificial agents (a humanoid robot or
interaction between identity of the agent and agent’s choice         an automated system), choosing the utilitarian action is rated
(F(2, 316) = 3.21, p = .042, see Figure 1). For the humanoid         as more morally permissible than not choosing it; while
robot, action (M = 4.6, SD = 1.9) is rated higher than inaction      there is no such a difference in the ratings for the choices of
(M = 3.4, SD = 1.4), F(1, 112) = 14.53, p < .001. For the            the human agent.
automated system, again the action (M = 4.9, SD = 1.8) is rated         Marginally significant interaction between instrumentality of
higher than inaction (M = 3.3, SD = 1.8), F(1, 108) = 28.15, p       harm and agent’s choice (F(1, 316) = 3.59, p = .059) was also
< .001. For the human agent there is no significant difference in    found (see Figure 3). For the incidental harm scenario,
the ratings for the rightness of the agent’s choice (p = .13, M =    choosing the utilitarian action is rated as more morally
3.7 for action, M = 4.2 for inaction).                               permissible (M = 4.5, SD = 1.7) than not choosing it (M = 3.7,
                                                                 1514

SD = 1.4), F(1, 170) = 11.88, p = .001. For the instrumental
harm scenario, there is no difference in the moral permissibility
ratings for choosing or not choosing the utilitarian action (p =
.58, M = 3.8, for action; M = 4.0, for inaction).
                                                                             Figure 5: Mean ratings with standard errors of the
                                                                      blameworthiness of the agent on a 7-point Likert scale (1 =
                                                                        ‘not at all blameworthy’, 7 = ‘extremely blameworthy’).
                                                                     Perceived Moral Agency
  Figure 3: Mean ratings with standard errors of the moral
 permissibility of the agent’s choice on a 7-point Likert scale      To test directly the moral agency ascribed to the agents,
      (1 = ‘not permissible at all’, 7 = ‘it is mandatory’).         which is central for this study, additional data was gathered
                                                                     from a group of 32 students. Here, due to the lack of space,
Blameworthiness of the Agent                                         only the most relevant results are presented (the full study
                                                                     will be reported elsewhere). Participants were asked to rate
Mean ratings of the blameworthiness of the agent were                each agent’s description on a variety of rating scales among
analyzed in a factorial ANOVA with agent’s choice (action            which scales describing capacities and abilities related to
vs. inaction), identity of the agent (human vs. humanoid
                                                                     moral agency. Overall, the human agent is rated higher than
robot vs. automated system) and instrumentality of harm
                                                                     the humanoid robot and the automated system despite the
(instrumental vs. incidental) as between-subjects factors.
                                                                     fact that humanoid robot is described as identical to the
   ANOVA showed a main effect of the identity of the
                                                                     human apart from his building materials.
agent, F(2, 316) = 5.386, p = .005 (see Figure 4). Post hoc
                                                                        For instance, on the scale ‘The agent can tell right from
tests using the Bonferroni correction revealed that the
                                                                     wrong (1 = completely disagree, 7 = completely agree)’, the
humanoid robot is rated as less blameworthy (M = 2.5, SD =
                                                                     human agent is rated higher (M = 4.6) than the humanoid robot
1.5) than the automated system (M = 3.2, SD = 1.9), or the           (M = 3.3) and the automated system (M = 2.9), p = .008 and p
human (M = 3.1, SD = 1.6), with p = .007 and p = .058,               = 0.001, respectively. On the scale ‘The agent is responsible for
respectively.                                                        his actions (1 = completely disagree, 7 = completely agree)’,
                                                                     the human agent is rated again higher (M = 5.9) than the
                                                                     humanoid robot (M = 3.6) and the automated system (M = 3.3),
                                                                     p < 0.001 for both comparisons.
                                                                        Thus, it seems that although the description of the humanoid
                                                                     robot was meant to make him as close as possible to a human
                                                                     agent, people still considered him to have lower moral agency
                                                                     (comparable to that of an automated system).
                                                                                      Summary and Discussion
                                                                     The paper investigated how people evaluate moral
                                                                     judgments of human and artificial agents in instrumental
      Figure 4: Mean ratings with standard errors of the             and incidental moral dilemmas. This was achieved by
 blameworthiness of the agent on a 7-point Likert scale (1 =         asking participants to evaluate the appropriateness,
   ‘not at all blameworthy’, 7 = ‘extremely blameworthy’).           rightness, moral permissibility, and blameworthiness of the
                                                                     utilitarian action or inaction in a set of moral dilemmas. The
   A significant interaction between the instrumentality of          questions were chosen to explore different aspects of
harm and agent’s choice, F(1, 316) = 7.3, p = .007. For the          ascribed moral agency.
incidental harm scenario, choosing the utilitarian action is            As expected, the utilitarian action is found to be more
rated as less blameworthy (M = 2.6, SD = 1.4) than inaction          appropriate when the harm is incidental than when it is an
(M = 3.2, SD = 1.9), F(1, 170) = 6.397, p = .012. For the            instrumental one. Doing the utilitarian action is found to be
instrumental harm scenario, there is no statistically                more morally permissible, more right, and less blameworthy
significant difference in the blameworthiness ratings for            than the inaction only for the incidental scenarios.
action and inaction (p = .24, M = 3.1 and 2.8).                         Based on previous research, it was expected that
                                                                     participants would perceive differently the human and non-
                                                                 1515

human agents in terms of moral agency although the                         Armstrong, W. (2006). Consequences, action, and intention
humanoid robot was described to be identical to a human                    as factors in moral judgments: an FMRI investigation.
with respect to moral agency. These differences were                       Journal of Cognitive Neuroscience, 18(5), 803–817.
expected to be larger for instrumental than for incidental            Christensen, J. F., & Gomila, A. (2012). Moral dilemmas in
moral dilemmas.                                                            cognitive neuroscience of moral decision-making: A
   The results concerning the appropriateness of action, rated             principled review. Neuroscience and Biobehavioral
before the action or inaction of the agent is known, show no               Reviews, 36(4), 1249–1264.
effect of the type of agent. However, for the artificial              Cushman, F. (2008). Crime and punishment: Distinguishing
cognitive agents (a humanoid robot and an automated                        the roles of causal and intentional analyses in moral
system), the utilitarian action is found to be more morally                judgment. Cognition, 108(2), 353–380.
permissible and more right than the inaction. No such effect          Foot, P. (1967). The Problem of Abortion and the Doctrine
is found for human agents.                                                 of Double Effect. Oxford Review, 5, 5–15.
   This is consistent with the interpretation of rightness and        Gray, H. M., Gray, K., & Wegner, D. M. (2007). Dimensions
moral permissibility as related to intentions (Cushman,                    of mind perception. Science, 315(5812), 619.
2008) and agency (Gray et al., 2007). The results presented           Gips, J. (1995). Towards the ethical robot. Android
here can be interpreted by assuming that participants were                 Epistemology, 243–252.
more favorable to the actions of the artificial cognitive             Hauser, M., Cushman, F., Young, L., Kang-Xing, J., &
agents because they are perceived to have lower moral                      Mikhial, J. (2007). A Dissociation Between Moral
agency than the human agent has.                                           Judgments and Justifications. Mind & Language,
   If people think that artificial cognitive agents have little or         22(1), 1–21.
no moral values or rules, they probably expect that such              Hristova, E., & Grinberg, M. (2015). Should Robots Kill?
agents base their decisions on utilitarian calculations using              Moral Judgments for Actions of Artificial Cognitive
reasoning. Therefore, they cannot be blamed or judged from                 Agents. In Proceedings of EAPS 2015.
a moral point of view. This was supported by the results              Hristova, E., Kadreva, V., & Grinberg, M. (2014). Moral
obtained in the additional study using a questionnaire                     Judgments and Emotions: Exploring the Role of
measuring the perceived moral agency of the agents used in                 'Inevitability of Death'and “Instrumentality of Harm”
the study. The results show that the human agent has higher                (pp. 2381–2386). Austin, TX: Proceedings of the
scores in moral agency than the humanoid robot and the                     Annual Conference of the Cognitive Science Society.
automated system.                                                     Malle, B. F., Scheutz, M., & Voiklis, J. (2015). Sacrifice
   However, the humanoid robot is found to be less                         One For the Good of Many? People Apply Different
blameworthy for his decisions compared to the human agent                  Moral Norms to Human and Robot Agents.
and to the automated system. One could have expected similar               Proceedings of the Tenth Annual ACM/IEEE
results for the robot and the autonomous system, or that the               International Conference on Human-Robot Interaction.
automated system is even less blameworthy than the robot. But         Moore, A. B., Clark, B. A., & Kane, M. J. (2008). Who
apparently, at some point too low moral agency shifts the                  shalt not kill? Individual differences in working
responsibility from the artificial agent to its designers and the          memory capacity, executive control, and moral
automated system has the same rating as the human agent.                   judgment. Psychological Science, 19(6), 549–557.
   The results presented here show that the exploration of moral      Scheutz, M., & Malle, B. (2014). May Machines Take Lives
agency using moral dilemma is very promising. A systematic                 to Save Lives? Human Perceptions of Autonomous
review, including all available results (including Scheutz &               Robots (with the Capacity to Kill). A paper presented
Malle, 2014; Malle et al., 2015) should be done in order to                at The Ethics of the Autonomous Weapons Systems
establish firm basis for future research. Also (as suggested by            conference, 2014.
one of the reviewers) the experiment should be replicated in          Sparrow, R. (2007). Killer Robots. Journal of Applied
order to check for possible confounding due to the fact that in            Philosophy, 24(1), 62–77.
the description of the human agent no year is provided.               Strait, M., Briggs, G., & Scheutz, M. (2013). Some
                                                                           correlates of agency ascription and emotional value and
                    Acknowledgements                                       their effects on decision-making. In Affective
We gratefully acknowledge the financial support by New                     Computing and Intelligent Interaction, 505-510. IEEE.
Bulgarian University.                                                 Sullins, J. (2006). When is a robot a moral agent?
                                                                           International Review of Information Ethics, 6, 23-30.
                         References                                   Thomson, J. J. (1985). The Trolley Problem. The Yale Law
                                                                           Journal, 94(6), 1395–1415.
Allen, C., Smit, I., & Wallach, W. (2005). Artificial morality:       Wallach, W., & Allen, C. (2008). Moral Machines: Teaching
      Top-down, bottom-up, and hybrid approaches. Ethics                   Robots Right from Wrong. Oxford University Press.
      and Information Technology, 7(3), 149–155.                      Waytz, A., Gray, K., Epley, N., & Wegner, D. M. (2010).
Anderson, M., & Anderson, S. L. (2011). Machine ethics.                    Causes and consequences of mind perception. Trends
   Cambridge University Press.                                             in Cognitive Sciences, 14(8), 383–388.
Borg, J., Hynes, C., Van Horn, J., Grafton, S., & Sinnott-
                                                                  1516

