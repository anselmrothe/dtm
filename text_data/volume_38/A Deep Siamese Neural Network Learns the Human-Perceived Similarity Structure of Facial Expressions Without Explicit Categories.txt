         A Deep Siamese Neural Network Learns the Human-Perceived Similarity
                    Structure of Facial Expressions Without Explicit Categories
                                            Sanjeev Jagannatha Rao (sjrao@ucsd.edu)
                        Department of Computer Science and Engineering, University of California San Diego
                                                 9500 Gilman Dr, La Jolla, CA 92093 USA
                                                 Yufei Wang (yuw176@eng.ucsd.edu)
                       Department of Electrical and Computer Engineering, University of California San Diego
                                                 9500 Gilman Dr, La Jolla, CA 92093 USA
                                                Garrison W Cottrell (gary@ucsd.edu)
                        Department of Computer Science and Engineering, University of California San Diego
                                                 9500 Gilman Dr, La Jolla, CA 92093 USA
                              Abstract                                   emotion, suggesting that the representation of facial expres-
   In previous work, we showed that a simple neurocomputa-               sions is divided into discrete categories. Once an expression
   tional model The Model, or TM) trained on the Ekman &                 has been categorized, the subtleties of the expression are lost.
   Friesen Pictures of Facial Affect (POFA) dataset to catego-
   rize the images into the six basic expressions can account for           An opposing theory suggests that perception of facial ex-
   wide array of data (albeit from a single study) on facial ex-         pressions is not as discrete as suggested by data supporting
   pression processing. The model demonstrated categorical per-          categorical perception. This notion of facial expression per-
   ception of facial expressions, as well as the so-called facial
   expression circumplex, a circular configuration based on MDS          ception suggests that while some facial expressions have full
   results that places the categories in the order happy, surprise,      membership in one of the six basic emotion classes (happy,
   fear, sadness, anger and disgust. Somewhat ironically, the cir-       disgust, angry, sad, fear, surprise), that nevertheless there is
   cumplex in TM was generated from the similarity between the
   categorical outputs of the network, i.e., the six numbers rep-        an underlying similarity structure to the expressions. Rus-
   resenting the probability of the category given the face. Here,       sell is the strongest advocate of this view, and has presented
   we extend this work by 1) using a new dataset, NimsStims,             results that support this notion of perception of facial ex-
   that is much larger than POFA, and is not as tightly controlled
   for the correct Facial Action Units; 2) using a completely dif-       pressions (Russell & Bullock, 1986; Russell, 1980; Russell,
   ferent neural network architecture, a Siamese Neural Network          Lewicka, & Niit, 1989). This and other related research sug-
   (SNN) that maps two faces through twin networks into a 2D             gests that there is a continuous underlying multidimensional
   similarity space; and 3) training the network only implicitly,
   based on a teaching signal that pairs of faces are in either in       perceptual space in which there are clear neighborhood rela-
   the same or different categories. Our results show that in this       tionships between expression categories, where each expres-
   setting, the network learns a representation that is very similar     sion is closer to some expressions than others.
   to the original circumplex. Fear and surprise overlap, which
   is consistent with the inherent confusability between these two          Dailey et al. (2002) developed a neural network model
   facial expressions. Our results suggest that humans evolved           trained to classify facial expressions into six basic emotions
   in such a way that nearby emotions are represented by similar
   appearances.                                                          (this model is referred to as “The Model” (TM) in (Cottrell &
   Keywords: facial expressions; similarity structure; deep              Hsiao, 2011)). The model was able to fit data usually taken to
   siamese neural network; multidimensional scaling (MDS); fa-           support each of the two competing theories of facial expres-
   cial expression circumplex                                            sion recognition (Young et al., 1997). It displayed categori-
                           Introduction                                  cal perception as well as graded reaction times near category
                                                                         boundaries, and responses indicating that the model was sen-
According to Darwin, facial expressions of emotion evolved
                                                                         sitive to mixed-in emotions even on the opposite side of the
and adapted to prepare the organism to deal with its environ-
                                                                         category boundary.
ment and to also serve to communicate the internal state of
the organism (Darwin, 1872; Hess & Thibault, 2009). If fa-                  Dailey et al. performed MDS on the human forced-choice
cial expressions of emotion are an outward manifestation of              responses published by (Ekman & Friesen, 1976) and on their
an internal state, then similar internal states should lead to           model’s responses to the same stimuli. These are shown in
similar expressions, in order to make the outward manifesta-             Figures 7 and 8, respectively. They showed that the order-
tions consistent and easy to understand. At the same time,               ing of emotions is the same in both the cases, a result that is
expression of different emotions should also be sufficiently             unlikely to have occurred by chance (the probability of this
distinguishable in order to make it possible to properly re-             outcome is 1/60, or 0.017). This reflects clear neighborhood
spond to them.                                                           relationships between facial expressions.
   How are facial expressions represented in the brain? There               In this work, we aim to reproduce these results from MDS,
are two competing theories. One theory is based on exper-                albeit under more restrictive training conditions. In particular,
imental evidence of categorical perception of expressions of             the model is only told which faces are in the same category
                                                                     217

and which faces are in different categories and is not explic-        explicit category labels to each input data point. Siamese neu-
itly given the categories themselves. To the best of our knowl-       ral networks fit these modeling requirements perfectly.
edge, this is the first time the circumplex has been shown to            Siamese neural networks are comprised of two neural net-
arise from facial expression data under such restrictive train-       works that take a pair of images as input and share a common
ing conditions.                                                       contrastive loss function. Like siamese twins who share or-
   We design a siamese neural network and train it to learn           gans, the two networks of a siamese neural network are iden-
a 2D representation of facial expressions. The network is             tical to each other in their architecture, and they share the
trained on pairs of images with a binary label that indicates         same weights.
if the two images belong to the same or different facial ex-             Figure 1 shows the layout of our siamese neural network.
pression categories. In essence, this model is not explicitly         It receives a pair of images that are resized to 227 x 227 as
trained on the number of facial expressions categories in the         input in its first layer. Each of these inputs is then processed
underlying data, or on the relative relationships among the           through a dedicated 6 layer feed forward network as shown
facial expressions. The low dimensional representation pro-           in the Figure. The first three layers are convolutional and the
duced by the siamese network replicates to a large extent the         last three layers are fully connected. The activations of the
circumplex found by Dailey et al. (Dailey, Cottrell, Padgett,         last layer from each network are used to compute the loss.
& Adolphs, 2002).                                                        The loss function is an energy-based one that is designed
   Our results suggest that facial expressions of emotion have        to move the representations of pairs inputs that are supposed
evolved to make their appearance easily discriminable, and            to be “the same” closer together, and ones that are supposed
that compatible inner states produce similar expressions. The         to be different farther apart. We use the loss developed
similarity structure in the low-dimensional space discovered          in (Hadsell, Chopra, & LeCun, 2006). Let X1 and X2 be two
by the network indicates that human expressions of similar            images presented to the system, one to each network. Y is
emotions are closer to each other when compared to the dis-           a binary label assigned to the pair, with Y = 0 if the images
similar ones. The inherent confusability between our per-             supposed to be similar, and Y = 1 if they are supposed to
ception of facial expressions is explained by the overlapping         be different. G1 and G2 are the activation vectors of the last
clusters in our representations. For example, the siamese             layer of each network, just before the contrastive loss function
network overlaps the Fear and Surprise clusters, which are            in Figure 1. Let Dw = ||G1 − G2 || be the Euclidean distance
known to be prone to confusion. The fact that we obtain               between these vectors, where the subscript indicates the de-
distinct clusters in our similarity structure demonstrates our        pendence on the weights W of the network. Then the loss
ability to express dissimilar emotions in a differentiable way.       function is:
            Siamese Neural Network Model                                                     1            1
                                                                               L = (1 −Y ) (Dw )2 +Y ( max(0, m − DW ))2            (1)
                                                                                             2            2
Dimensionality Reduction and Siamese Neural
Networks                                                              where m > 0 is a margin. This loss function is inspired by
Two classical methods for dimensionality reduction are Prin-          an analogy to springs, where minimizing the first term cor-
cipal Component Analysis (PCA) and Multidimensional                   responds to a spring pulling G1 and G2 closer together, and
Scaling (MDS). PCA finds a linear projection of the input             the second term corresponds to a repulsing spring, pushing
data to a low dimensional space that maximizes the explained          G1 and G2 farther apart. This loss function can be optimized
variance. MDS arranges the data in the low dimensional                by gradient descent. In order to map the faces into a two-
space in a manner that best preserves the pairwise distances          dimensional space, G1 (and hence G2 ) are composed of two
between input points. However, facial expression images               units.
pose several challenges, similar to those posed in any com-              Siamese networks have been shown to work well in face
puter vision application. Changes in lighting can make im-            verification (Chopra, Hadsell, & LeCun, 2005), where the
ages of dissimilar emotions more similar, and similar ones            categories are not known in advance, since there are an un-
different. In emotion recognition, the identity of the individ-       bounded number of faces. The networks in this case map
ual is a confound; identity is noise with respect to expression,      faces of the same person to nearby places in the representa-
and vice-versa. This suggests that a nonlinear embedding is           tional space (the last layer), and faces of different people far-
required. MDS provides this, but it does not provide a map-           ther apart. Siamese neural networks have also been shown to
ping of new data into the same space, so it is difficult to check     work well at dimensionality reduction (Hadsell et al., 2006).
for generalization.                                                   We take our loss function from the latter publication. Both
   We require a dimensionality reduction technique that is ro-        these models use deep convolutional neural network archi-
bust to these changes in input, and that provides a way to gen-       tectures to extract features from the input images.
eralize to new images in order to check that the embedding is
consistent. In this work, we aim to learn the low dimensional
                                                                      Dataset
structure of facial expressions data without relying on the to-       We use facial images corresponding to the six basic emo-
tal number of categories in our data and without associating          tions, Happy, Surprise, Fear, Sad, Anger and Disgust from
                                                                  218

                                          Figure 1: Siamese Neural Network Architecture
the NimStim dataset (Tottenham et al., 2009) for our analy-          Dealing with Limited Data
sis. We create all possible pairs from the images correspond-        Deep convolutional neural networks (CNN) are trained on
ing to these six basic emotions and use that as input to our         several hundred thousands of images. A large data set is re-
siamese network. In all, we train on 126,756 pairs of images.        quired to learn the large number of parameters in the network.
The breakdown of category-wise pair counts is given in Ta-           We are constrained by the relative small size of our dataset. A
ble 1. Originally, we tried to balance the number of similar         workaround for a small dataset is to initialize our model with
and dissimilar pairs, however, we ended up losing a signifi-         a pre-trained model that will generalize to our problem.
cant amount of data and the model did not generalize well to            The winning model of the ImageNet LSVRC-2012 contest
unseen data. Hence we used all of the data, as shown in the          (Krizhevsky, Sutskever, & Hinton, 2012) (dubbed “Alexnet”)
table.                                                               broke new ground in CNNs by using a 8 layer deep convo-
   Approximately 10% of the subjects in the dataset are set          lutional neural network. This model was trained to classify
aside for a validation set and 10% for a test set. The remaining     natural images into 1000 different categories. This model,
80 percent of the subjects contribute to the training set.           along with its weights are publicly available. We use the first
                                                                     three convolutional layers as a starting point to build and train
                                                                     our siamese neural network.
                                                                     Architecture
                                                                     We experimented with several architectures, and we report
                                                                     the one that gave the minimum loss on the training set.
                                                                     Though we present only one architecture, all architectures
           (a) Angry        (b) Disgust        (c) Fear              that resulted in a significant reduction in loss during training
                                                                     yielded essentially the same representation. The network con-
                                                                     tains 6 layers, the first three are convolutional, and the next
                                                                     three are fully connected. Two such networks together form
                                                                     our siamese architecture. The output of the last fully con-
                                                                     nected layer serves as input to our loss function. We build our
                                                                     first three convolutional layers from the pre-trained weights
          (d) Happy           (e) Sad        (f) Surprise            on the ImageNet LSVRC-2012 dataset. We found this initial-
                                                                     ization to work really well for our purposes and has helped us
            Figure 2: Sample Images from NimStim                     cope with our limited dataset.
                                                                    1. The first convolutional layer filters the 227 × 227 × 3 input
                                                                 219

    image with 96 kernels of size 11 × 11 × 3 with a stride of 4         The model generalizes to unseen data within the NimStim
    pixels.                                                           dataset. Its performance on test set, shown in Figure 6, is
                                                                      similar to that on the training set.
2. The second convolutional layer takes as input the
    (response-normalized and pooled) output of the first con-
    volutional layer and filters it with 256 kernels of size                    Table 1: Image Pairs by Emotion Catogory
                                                                        Emotion     Angry    Disg.   Fear     Happy    Sad      Surpr.
    5 × 5 × 96.                                                         Angry       3741     7134    6960     11049    7308     3828
                                                                        Disgust              3321    6560     10414    6888     3608
3. The third convolutional layer has 384 kernels of size 3 ×            Fear                         3160     10160    6720     3520
    3 × 256 connected to the (normalized, pooled) outputs of            Happy                                 8001     10668    5588
    the second convolutional layer.                                     Sad                                            3486     3696
                                                                        Surprise                                                946
4. The fourth, fifth, and sixth layers are fully connected with
    1024, 256, and 2 units, respectively.                                The reader should compare the organization of the facial
                                                                      expressions in Figure 5 with that seen in Figures 7 (the MDS
    The loss function takes its inputs from the 2 units of the        of human responses) and 8 (the MDS of The Model’s re-
 sixth layer from each of the two individual networks. We have        sponses). The human MDS is derived from the human sub-
 chosen to have two units in the last fully connected layer in        jects’ averaged six-alternative forced choice responses for
 order to extract a two dimensional representation of our data.       each face in the POFA dataset, as published by (Ekman &
    The siamese neural network was developed using the Caffe          Friesen, 1976) and on The Model’s responses to the same
 deep learning framework (Jia et al., 2014). We use ReLU              stimuli as reported by Dailey et al. (2002).
 (recitified linear units) activation functions throughout our           Of particular interest here is the ordering of the represen-
 architecture. We use a base learning rate of 0.0001, a step          tations of facial expressions in two dimensions. The order-
 learning rate policy with a step size of 5000. The margin m          ing of the emotions in the results reported by Dailey et al.
 in Equation 1 is set to 1. We use dropout after the fourth           and in the representations produced by the siamese network
 and the fifth fully connected layers. Training the model on          model are very similar. However, we do not get a perfect
 a single GPU took around 5 hours. The layers were initial-           circumplex. Surprise and fear images completely overlap in
 ized with Xavier initialization and stochastic batch gradient        our representation, which is consistent with the inherent con-
 descent was used during training.                                    fusability between them. Disgust and anger are just barely
                                                                      separated, and these too are expressions that are confused by
                             Training                                 human subjects.
 We started with pre-trained weights on the first three convo-           The resulting order is unlikely to have happened by chance.
 lutional layers and trained only the subsequent layers. We           The probability of a random ordering of six emotions match-
 stopped training when there was no additional improvement            ing the representation in Figure 7 is only 1/60: starting with
 in the performance on the validation set. We then fine tuned         any emotion, there are 5 to choose from next, 4 after that, etc.
 the first three convolutional layers as well. Fine tuning was        This gives 120 possibilities, but whether they are clockwise
 done for 5000 epochs at which point the loss did not reduce          or counterclockwise does not matter, so there are 60 possible
 any further. The representations learned through the process         events.
 of training are shown in figure 3. We plot the activations in           To compute the probability of the current results, we can
 the last layer of the network for each image in the training set     consider that we have a failure to separate two emotions, so
 to generate these plots.                                             the results are consistent with an ordering of Happy, Sur-
                                                                      prised, Fearful, Sad, Angry and Disgusted, or Happy, Fearful,
                           Evaluation                                 Surprised, Sad, Angry and Disgusted. Since each of these or-
 Figure 3 shows how the different categories are organized            dering have a probability of 1/60, both together have a prob-
 by the network during the course of training. Each point in          ability of 1/30, or 0.033.
 the plots represents one image in the NimStim dataset corre-            Dailey et al. (2002) found that happy faces were the easiest
 sponding to one of the six basic emotions of happy, sad, an-         to classify fear faces the most difficult to classify, consistent
 gry, fearful, surprised and disgusted. At the start of training,     with the literature (Katsikitis, 1997; Ekman & Friesen, 1976;
 the network is unable to differentiate the facial expressions as     Matsumoto, 1992). The results of the siamese network model
 shown in Figure 3(a). The six basic emotions begin to form           are consistent with these patterns. Happy images have been
 clusters around the 4000 epoch mark (Figure 3(d)), and be-           pushed into a tight cluster in the two dimensional representa-
 come distinct after 5000 epochs as shown in Figure 4. Until          tion, such that they are essentially linearly separable from the
 this point the convolutional layers were fixed at their initial      others, even in this very low dimensional space, and fear is
 values, and at 5000 epochs the loss reached its minimum. At          completely overlapping with surprise, making it impossible
 this point, we started fine-tuning the pre-trained layers and        to separate from the others.
 there is a further drop in loss. As expected, the representation        The siamese network model has not been trained to classify
 becomes more distinct after fine tuning as seen in Figure 5.         the emotions into the six categories used by humans; rather it
                                                                  220

        (a) Start of Training           (b) 2000 epochs
         (c) 3000 epochs                 (d) 4000 epochs
                                                                     Figure 5: Representation after fine tuning. Refer Figure 4 for
                                                                     legend.
Figure 3: Representations during training. Refer Figure 4 for
legend.
                                                                      Figure 6: Test set representation. Refer Figure 4 for legend.
Figure 4: Representation after training for 5,000 epochs with-
out fine tuning. Legends used in Figures 3-6: an: Angry, fe:         internal states, then the model predicts that anger and disgust
Fear, di: Disgust, ha: Happy, sa: Sad, sp: Surprise.                 have similar internal states, and fear and surprise also signal
                                                                     similar internal states.
                                                                        Our results suggest that, through evolution, our facial ex-
has simply been trained on what humans consider “same” or            pression of emotions and their perception have developed to
“different” categories. These results, therefore, suggest that       communicate an inner state that is easily differentiable, and
the similarity structure learned by the network is inherent in       that associated emotional states are communicated similarly.
the similarity structure of the faces and the fact that some are     Disgust and anger are often combined in everyday life, and
different from others. We further hypothesize that, not only         in more exciting, if unfortunate, circumstances, fear and sur-
have the expressions evolved to be discriminable, but similar        prise are highly compatible and tend to co-occur. Our net-
emotions have similar expressions.                                   work has no access to these notions, no access to culture, yet
                                                                     it places these pairs of emotions either next to each other (as
                           Conclusions                               in disgust and anger), or overlapping (as in fear and surprise).
We have presented a siamese neural network model that de-            The fact that we obtain relatively distinct clusters in our sim-
rives low dimensional representations of facial expressions          ilarity structure suggests that our emotional expressions are
under restrictive training conditions.                               inherently differentiable.
   The network is only given same/different information
about the images, it is not given any similarity information,                             Acknowledgments
so the structure of the clusters reflects the similarity between
                                                                     This work was supported in part by NSF grant SMA 1041755
the expressions themselves. In Dailey et al., 2002, the cir-
                                                                     to the Temporal Dynamics of Learning Center, an NSF Sci-
cumplex was derived from the softmax output of the network,
                                                                     ence of Learning Center. We are also grateful to members of
which also reflects confusability, but here, the network is de-
                                                                     Gary’s Unbelievable Research Unit (GURU) for their help.
riving the similarity structure solely from the images and the
information that some are the same, and some are different.
It is never told which categories are similar to each other, so
                                                                                               References
that is induced by the network from the similarity in the im-        Chopra, S., Hadsell, R., & LeCun, Y. (2005). Learning a sim-
ages. If Darwin is correct, and emotional expressions signal            ilarity metric discriminatively with application to face ver-
                                                                 221

                                                                      tion, 26, 613-626.
                                                                    Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Im-
                                                                      agenet classification with deep convolutional neural net-
                                                                      works. Advances in neural information processing systems,
                                                                      1097-1105.
                                                                    Matsumoto, D. (1992). American–japanese cultural dif-
                                                                      ferences in the recognition of universal facial expressions.
                                                                      Journal of Cross-Cultural Psychology, 23, 72-84.
                                                                    Russell, J. (1980). A circumplex model of affect. Journal of
                                                                      Personality and Social Psychology, 39, 1161-1178.
                                                                    Russell, J., & Bullock, M. (1986). Fuzzy concepts and the
                                                                      perception of emotion in facial expressions. Social Cogni-
Figure 7: Human MDS representation of facial expressions.             tion, 4, 309-341.
                                                                    Russell, J., Lewicka, M., & Niit, T. (1989). A cross-cultural
                                                                      study of circumplex model of affect. Journal of Personality
                                                                      and Social Psychology, 57, 848-856.
                                                                    Tottenham, N., Tanaka, J., Leon, A., McCarry, T., Nurse, M.,
                                                                      Hare, T., . . . Nelson, C. (2009, August). The nimstim face
                                                                      stimulus set. Development of the MacBrain Face Stimulus
                                                                      Set was overseen by Nim Tottenham and supported by the
                                                                      John D. and Catherine T. MacArthur Foundation Research
                                                                      Network on Early Experience and Brain Development.
                                                                      Please contact Nim Tottenham at tott0006@tc.umn.edu for
                                                                      more information concerning the stimulus set.
                                                                    Young, A. W., Rowland, D., Calder, A. J., Etcoff, N., Seth,
                                                                      A., & avid I. Perrett, D. (1997, June). Facial expression
                                                                      megamix: tests of dimensional and category accounts of
                                                                      emotion recognition. Cognition, 63, 271–313.
Figure 8: The Model’s (Dailey et al., 2002) MDS Represen-
tation
   ification. Computer Vision and Pattern Recognition, 539-
   546.
Cottrell, G., & Hsiao, J. (2011). Neurocomputational mod-
   els of face processing. In The Oxford Handbook of Face
   Perception. Oxford, UK: Oxford University Press.
Dailey, M. N., Cottrell, G. W., Padgett, C., & Adolphs, R.
   (2002). Empath: A neural network that categorizes fa-
   cial expressions. Journal of Cognitive Neuroscience, 14(8),
   1158-1173.
Darwin, C. (1872). The expression of emotions in man and
   animals. New York.
Ekman, P., & Friesen, W. (1976). Pictures of facial affect.
   Consulting Psychologist Press.
Hadsell, R., Chopra, S., & LeCun, Y. (2006). Dimensional-
   ity reduction by learning an invariant mapping. Computer
   Vision and Pattern Recognition, 2, 1735-1742.
Hess, U., & Thibault, P. (2009). Darwin and emotion expres-
   sion. American Psychologist.
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J.,
   Girshick, R., . . . Darrell, T. (2014). Caffe: convolutional
   architecture for fast feature embedding. Proceedings of the
   ACM International Conference on Multimedia, 675-678.
Katsikitis, M. (1997). The classification of facial expressions
   of emotion: A multidimensional scaling approach. Percep-
                                                                222

