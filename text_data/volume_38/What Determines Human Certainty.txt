                                        What Determines Human Certainty?
                                                   Louis Martı́ (lmarti13@gmail.com)
                                                Francis Mollica (mollicaf@gmail.com)
                                            Steven Piantadosi (spiantadosi@gmail.com)
                                                Celeste Kidd (celestekidd@gmail.com)
                                 Department of Brain and Cognitive Sciences, University of Rochester,
                                                          Rochester, NY 14627 USA
                               Abstract                                     Here, we test whether individuals’ subjective certainty
                                                                         while acquiring novel concepts is driven by objective prob-
   Previous work on concept learning has focused on how con-             abilities. Are learners as certain as they should be given the
   cepts are acquired without addressing metacognitive aspects
   of this process. An important part of concept learning from           data, or is their subjective sense of certainty driven by other
   a learner’s perspective is subjectively knowing when a new            factors (e.g., accuracy, quantity of observed data)? This ques-
   concept has been effectively learned. Here, we investigate            tion has implications for understanding the subjective experi-
   learners’ certainty in a classic Boolean concept-learning task.
   We collected certainty judgements during the concept-learning         ences that accompany concept discovery, which may them-
   task from 552 participants on Amazon Mechanical Turk. We              selves interact with future learning.
   compare different models of certainty in order to determine
   exactly what learners’ subjective certainty judgments encode.         Boolean concept learning as a prototypical domain
   Our results suggest that learners’ certainty is best explained by
   local accuracy rather than plausible alternatives such as total       Historically, Boolean concept-learning tasks have been used
   entropy or the maximum a posteriori hypothesis of an idealized
   Bayesian learner. This result suggests that certainty predomi-        to study concept acquisition because they allowed researchers
   nately reflects learners’ performance and feedback, rather than       to study the mechanisms of the learning process in a sim-
   any metacognition about the inferential task they are solving.        plified domain with a known, limited hypothesis space (e.g.,
   Keywords: Concepts; metacognition; learning; human exper-             Bruner, Goodnow, & Austin, 1967; Feldman, 2000; Good-
   imentation; symbolic computational modeling; certainty; ideal         man et al., 2008; Shepard, Hovland, & Jenkins, 1961).
   learning model
                                                                            As an example, consider a classic Boolean concept-
                                                                         learning paradigm. In this task, participants are asked to re-
                           Introduction                                  spond yes or no to a series of images and are given feedback
Most of us are certain that we landed on the moon, but many              after each response. Each image has a shape, size, and color
of us are far less certain about who will win this year’s pres-          that has one of two values, resulting in a total of eight differ-
idential election. Ideally, our certainty would be a direct              ent images. The accuracies of the participants’ yes or no re-
reflection of the evidence we observe, but is our sense of               sponses are determined by an unstated concept such as “large
certainty actually calibrated to reality? Several studies have           black square” or “triangle” that subjects must discover. This
demonstrated that individuals presented with disconfirming               latent concept is a Boolean rule which can easily be stated in
evidence can become even more entrenched in their original               logic, meaning that it is straightforward to quantify represen-
beliefs. Tormala and Petty (2004, 2011) found that when in-              tations that learners are likely to be using. Feldman (2000)
dividuals were confronted with messages that they perceived              tested participants on 41 different concepts spread across six
to be strong (e.g., from a expert source) but went against               families corresponding to the number of positive examples
their existing beliefs, their certainty regarding those beliefs          and feature dimensions used in each stimulus. His results
increased instead of decreased. In contrast, within the vi-              showed that performance in learning decreased as Boolean
sual domain, there is evidence that individuals not only cal-            complexity increased, indicating that concept difficulty was
culate their own subjective measure of visual uncertainty, but           directly proportional to the number of Boolean operators in
that their subjective uncertainty is predictive of objective un-         the shortest logically equivalent expression. For example, the
certainty (Barthelme & Mamassian, 2009). In other words,                 concept “large red triangle” should be more difficult than ”red
individuals’ certainty of visual stimuli reflects veridical prob-        triangle” since the former incorporates an additional feature.
abilities.                                                               The idea of simplicity-driven concept learning was put into
   The Dunning-Kruger effect provides further evidence of                a probabilistic setting by Goodman et al. (2008), who con-
a miscalibration between reality and certainty (Dunning                  structed a Bayesian learner that tried to acquire concepts h
& Kruger, 1999). Dunning and Kruger demonstrated a                       from data d according to an idealized model of P(h | d). The
metacognitive inability of unskilled individuals to recognize            prior in this model favored simplicity, and the likelihood fa-
their own incompetence. This results in an inflated sense of             vored hypotheses that explained the observed labels. In this
certainty regarding their own performance and aptitude. The              way, it was able to combine a formalization of a simplicity
inverse was also found, in which highly competent individu-              preference with Bayesian inference from the observed data,
als would be less certain regarding their own abilities in rela-         providing a close fit to empirical learning.
tion to others.                                                             These tasks provide an ideal domain for us to study cer-
                                                                     698

tainty since there exist well-tested models and theories of the
processes that guide learning in such simple domains. We
collect subjective measurements of learners’ certainty or con-
fidence throughout a novel concept-learning task. We then
compare learners’ self-reported certainty throughout the task
to a set of models we constructed to represent several a pri-
ori plausible, formal theories about what quantitative mea-
sure subjective certainty might reflect. We constructed a rule
learning model similar to Goodman et al. (2008) in order to
provide an idealized quantification of formal measures of cer-
tainty, which we then compare to human judgements.
                           Methods
We tested participants in a novel concept-learning task during
which we measured their knowledge of the concept (via yes
or no responses) and their certainty throughout the learning
process.
   We recruited 552 participants on Amazon Mechanical
Turk. Participants clicked to consent to the study before view-
ing the task instructions. The instructions explained that the
participant’s task was to figure out the meaning of a word that
represented a certain concept. Participants practiced on eight
practice trials to ensure that they understood the task before       Figure 1: Participants saw 24 trials (as above) in sequen-
proceeding to the actual study. During the practice trials, par-     tial order, randomized between-conditions. After responding,
ticipants saw either a cat or a dog, and had to guess whether        feedback was displayed for one second (if correct) or two sec-
each item fit the undisclosed concept for a novel word or not        onds (if incorrect) before the next stimuli was shown. Previ-
by responding yes or no. In addition to guessing, participants       ous feedback was not displayed at any time.
had to report whether or not they were certain about the con-
cept for the novel word. After each guess, participants re-
ceived feedback about whether or not their guess was correct.        Like the practice trials, each participant gave a “yes” or “no”
For the practice trials, the novel word always referred to the       answer as to whether the current image was part of the con-
concept of “cat”.                                                    cept. On each trial, each participant also indicated whether
   For the experimental trials (see Figure 1), participants saw      they were certain regarding their answer, providing a binary
one of ten conditions, each composed of of 24 trials. Each           forced choice judgment. After their responses, participants
condition represented one unique concept, such that each par-        received feedback on their responses. Correct responses re-
ticipant made judgements for only one concept. In a partial          ceived one second of feedback before the next trial com-
replication of the Shepard et al. study (1961), the observa-         menced. Incorrect responses were penalized with a slower
tions spanned three binary dimensions: shape (square or tri-         two seconds of feedback before the next trial to incentivize
angle), color (red or green), and size (large or small). A total     attention to the task.
of eight images were used across all conditions which exhaus-
tively spanned the space. Across all conditions participants                            Ideal learning model
would see these eight images in blocks of three with the or-         We aim to address the question of whether learners’ subjec-
dering of the images assigned randomly per condition. Each           tive sense of certainty reflects veridical probabilities. In other
condition tested for a different concept with varying complex-       words, do learners feel as certain as is justified by the ob-
ity (see Table 1).                                                   served data? Addressing this question requires us to use an
   Concepts 1, 5, 6, 7, 8 and 9 (Table 1) were identical to con-     ideal learning model in order to determine how confident an
cepts used in both the Shepard et al. (1961) and Feldman             ideal learner should be (given the uncertainty of the model).
(2000) experiments. These concepts exhaustively spanned              Here we use an ideal learning model that has already been
across the concept family consisting of three features and           used to formalize concept learning in a probabilistic setting
four positive examples. Additional conditions were added             in which notions of certainty and uncertainty (e.g., Shannon,
to test for potential differences between operators. Concept         1948) are well defined. Though ideal learning models of this
9—“(green and large and triangle) or (green and small and            type have been used before to understand novel concept ac-
square) or (red and large and square) or (red and small and          quisition, they have not previously been applied towards un-
triangle)”—is predicted to be the most difficult as it essen-        derstanding learners’ subjective sense of certainty.
tially transforms that condition into a rote memorization task.         The ideal learning model was developed using Python
                                                                 699

                                            Concept
                 1      RED                 red
                 2      AND                 red and small
                 3      OR                  red or small
                 4      XOR                 red xor small
                 5      AND OR AND          (red and small) or (green and large)
                 6      Complex 1           (green and large and triangle) or (green and large and square) or (green and small and triangle) or (red and large and square)
                 7      Complex 2           (green and large and triangle) or (green and large and square) or (green and small and triangle) or (red and large and triangle)
                 8      Complex 3           (green and large and triangle) or (green and large and square) or (green and small and triangle) or (red and small and square)
                 9      Memorization        (green and large and triangle) or (green and small and square) or (red and large and square) or (red and small and triangle)
                 10     XOR XOR             red xor small xor square
                                       Table 1: Concepts presented to subjects in the experiment.
     Rule                                                                               mation learners have yet to discover about which hypothesis
     START → PREDICATE                                                                  is the true generator of the data. However, it also may be
     START → TRUE                                                                       the case that learners tend to pick probable hypotheses, and
     START → FALSE                                                                      their uncertainty reflects only the probability of the best hy-
     PREDICATE → and(PREDICATE, PREDICATE)                                              pothesis. We refer to this as the MAP model. We may also
     PREDICATE → or(PREDICATE, PREDICATE)                                               consider maximum likelihood in which the prior is ignored,
     PREDICATE → not(PREDICATE)                                                         corresponding to a maximum likelihood model over the struc-
     PREDICATE → red(x)                                                                 tured, compositional hypothesis space. Beyond these ideal-
     PREDICATE → green(x)                                                               ized model-based theories, it is critical to include a variety of
     PREDICATE → triangle(x)                                                            trial-level alternatives. It could be for instance that partici-
     PREDICATE → square(x)                                                              pants just become more confident as they complete more of
     PREDICATE → large(x)                                                               the experiment, a model we refer to as Trial. Alternatively,
     PREDICATE → small(x)                                                               certainty may just reflect a measure of their performance so
                                                                                        far, reflecting a lack of objective self-awareness of how much
Table 2: Grammar used to generate logical rules in the ideal-                           certainty they should have. Total Accuracy quantifies per-
ized learning model. The variable x is the current object.                              formance on all previous trials of the experiment. We also
                                                                                        include Local Accuracy measures of how well subjects have
                                                                                        done on the previous N trials (N = 2, 3, 4, 5), potentially in-
and the Language Of Thought library, LOTlib (Piantadosi,                                corporating their performance on the current trial (e.g. the
2014). This model defines a probabilistic context-free gram-                            one they are responding to), called Local Accuracy Cur-
mar (PCFG) with a set of primitives: red, green, triangle,                              rent. If this predictor beat out the others, it would indicate
square, large, and small, and logical operations (shown in                              that learners are only certain when they anticipate being able
Table 2). The PCFG serves as a prior over hypotheses and                                to guess accurately on the current trial. The Current Accu-
specifies an infinite hypothesis space.                                                 racy model is a baseline that simply quantifies whether par-
   To establish a tractable hypothesis space, the model drew                            ticipants were right on the next trial. Its performance as a pre-
1,000,000 samples from the posterior distribution of hypothe-                           dictor shows whether subjective certainty is well-calibrated to
ses (i.e., hypotheses scored by simplicity and fit to the data)                         true accuracy on the next item, regardless of the underlying
using tree-regeneration Metropolis-Hastings (Goodman et al.,                            computational processes.
2008) and stored the best 1,000 hypotheses at each data                                      Logarithmic transformations are common in psy-
amount that subjects saw. The model incorporated parame-                                chophysics (Stevens, 1957). Therefore, each of these
ters for the noise in the data (alpha) and a power law memory                           predictors was considered in its standard form, as well
decay on the likelihood of previous data1 (beta), best fit as                           as under a logarithmic transformation, yielding a total of
0.64 and 0 respectively.                                                                32 models. The accuracy predictors used a log(1 + x)
                             Analysis                                                   transformation to avoid problems with zeroes.
We considered and compared several different models of                                                                              Results
what might drive uncertainty. Perhaps the most natural is
                                                                                        Certainty and accuracy by concept
that learners might use the uncertainty of an idealized learn-
ing model, as quantified by the posterior entropy (Shannon                              We composed and evaluated plots of participants’ certainty
1948). This provides a measure of the number of bits of infor-                          and accuracy over the course of the experiment for each con-
                                                                                        cept in order to determine (1) whether certainty and accuracy
   1 Weighting the log likelihood of an example n back by (n+1)−β .                     improved over the course of the experiment, (2) whether the-
                                                                                  700

                                                                                                                  Local Accuracy 3 Back Current (A3)                                       Local Accuracy 3 Back (B3)
                                                                                                                              1.00                                                       1.00
                                             Certainty and Accuracy by Condition
                       RED                              AND                       OR         XOR
  1.00                                                                                                                        0.75                                                       0.75
                                                                                                                  Certainty                                                  Certainty
  0.75                                                                                                                        0.50                                                       0.50
  0.50
                                                                                                                              0.25                                                       0.25
  0.25
                                                                                                                              0.00                                                       0.00
                                                                                                                                            1            2      3                                0             1              2              3
  0.00
                                                                                                                                                Local Accuracy                                          Local Accuracy
                 AND OR AND                        Complex1                     Complex2   Complex3
  1.00                                                                                                                                     Total Correct (E)                                             Log Trial (G)
                                                                                                                              1.00                                                       1.00
  0.75
                                                                                                                              0.75                                                       0.75
  0.50
                                                                                                                  Certainty                                                  Certainty
                                                                                                                              0.50                                                       0.50
  0.25
  0.00
                                                                                                                              0.25                                                       0.25
                  XOR XOR                         Memorization
  1.00                                                                                                                        0.00                                                       0.00
                                                                                                                                     0          5       10     15     20                         0            1           2              3
  0.75                                                                                                                                              Total Correct                                              Log Trial
                                                                                                                                         Log Model Entropy (I)                                       Log MAP No Prior (J)
  0.50
                                                                                                                              1.00                                                       1.00
  0.25
                                                                                                                              0.75                                                       0.75
                                                                                                                  Certainty                                                  Certainty
  0.00
         0   5    10         15   20   250    5    10         15   20    25                                                   0.50                                                       0.50
                                                                        Trial
                                                                                                                              0.25                                                       0.25
Figure 2: Mean certainty (blue) and mean accuracy (red)
across concept conditions                                                                                                     0.00
                                                                                                                                           −1.0         −0.5   0.0     0.5
                                                                                                                                                                                         0.00
                                                                                                                                                                                                −8            −6          −4                 −2
                                                                                                                                           Log Model Entropy                                Log MAP Hypothesis (no prior)
                                                                                                                                         Current Accuracy (K)                                        MAP Hypothesis (N)
                                                                                                                              1.00                                                       1.00
oretically harder concepts (according to Feldman 2000) were,
                                                                                                                              0.75                                                       0.75
in fact, more difficult for participants, and (3) whether partic-
ipants’ certainty correlated with their accuracy in general.                                                      Certainty                                                  Certainty
                                                                                                                              0.50                                                       0.50
   Figure 2 shows participants’ certainty and accuracy (y-                                                                    0.25                                                       0.25
axis) over trials of the experiment (x-axis). The increasing                                                                  0.00                                                       0.00
                                                                                                                                           0.25         0.50   0.75   1.00                             0.25        0.50           0.75
trend of the accuracy curves reaches ceiling for some con-                                                                                  Current Accuracy                                            MAP Hypothesis
cepts, indicating that participants successfully acquired them.                                                                                     (a)                                                       (b)
In other conditions, participants did not reach ceiling, indi-
cating that they did not acquire the target concept. This is                                                Figure 3: Visualizations of several key model fits, giving the
actually beneficial to our analysis as it allows us to analyze                                              participant response means for each concept and trial (gray)
conditions and trials in which participants should have high                                                and binned model means in each of five quantiles (blue)
uncertainty. The certainty curves follow a generally increas-                                               for certainty rating (y-axis) as a function of model (x-axis).
ing trajectory, but only reach high values (ceiling probabil-                                               Straight lines with low variance correspond to models which
ity of a participant reporting being certain) in conditions in                                              accurately capture human performance.
which participants also achieved high accuracy. The increas-
ing trend of certainty in conditions for which accuracy does
not go above 50% may be reflective of overconfidence.                                                       has a similar shape and dispersion but is not a good predictor
                                                                                                            due to situations in which the participant is very uncertain but
Predictors of certainty
                                                                                                            happens to get the trial correct by chance. Finally, log model
Figure 3 shows certainty (y-axis) over several different key                                                entropy and the MAP plots perform very poorly. Data points
predictors of certainty (x-axis). Local accuracy models have                                                are completely scattered and the predictors perform poorly
low dispersion, meaning that individuals with low local ac-                                                 because of this. For example, although there are many likely
curacy have low certainty and individuals with high local ac-                                               MAP hypotheses for which participant certainty is high, there
curacy are highly certain. There are no cases where an indi-                                                are just as many for which certainty is low. This could be due
vidual is highly accurate and highly uncertain and no cases                                                 to complicated concepts for which the ideal learner model
where an individual has low accuracy and is highly certain.                                                 does well but participants do not.
On the other hand, total correct and log trial are highly lin-
ear but have high dispersion. This is likely due to condition                                               Model comparison results
effects. In conditions where the concept is extremely simple                                                Before examining how well the certainty models predict hu-
(e.g. ”red”) participants might reach high certainty extremely                                              man certainty, it is important to verify that the ideal learning
quickly and, due to a low level of negative feedback (incor-                                                model predicts human accuracy during the task. A logistic re-
rect responses), remain highly confident. Current accuracy                                                  gression predicting behavioral accuracy from model accuracy
                                                                                                      701

                  Model                                  AIC      Pseudo R2     Log Likelihood     Beta   Standard Error
            A3    Local Accuracy 3 Back Current        14753.0       0.11           -7374.5        1.11          0.03
            A4    Local Accuracy 4 Back Current        14778.8       0.11           -7387.4        0.85          0.02
            B3    Local Accuracy 3 Back                14786.6       0.11           -7391.3        1.33          0.04
            B4    Local Accuracy 4 Back                14803.0       0.11           -7399.5        0.99          0.03
            A5    Local Accuracy 5 Back Current        14816.3       0.11           -7406.1        0.68          0.02
            B5    Local Accuracy 5 Back                14838.8       0.11           -7417.4        0.76          0.02
            A2    Local Accuracy 2 Back Current        14872.6       0.11           -7434.3        1.44          0.04
            B2    Local Accuracy 2 Back                14892.2       0.11           -7444.1        1.91          0.05
            C3    Log Local Accuracy 3 Back Current    14948.6       0.10           -7472.3        3.40          0.10
            D3    Log Local Accuracy 3 Back            14990.2       0.10           -7493.1        3.24          0.10
            C4    Log Local Accuracy 4 Back Current    15010.9       0.10           -7503.5        2.92          0.09
            C2    Log Local Accuracy 2 Back Current    15018.9       0.10           -7507.4        3.78          0.11
            D2    Log Local Accuracy 2 Back            15037.6       0.10           -7516.8        3.83          0.11
            D4    Log Local Accuracy 4 Back            15051.6       0.10           -7523.8        2.73          0.08
            C5    Log Local Accuracy 5 Back Current    15078.1       0.09           -7537.0        2.53          0.08
            D5    Log Local Accuracy 5 Back            15121.0       0.09           -7558.5        2.32          0.07
            A1    Local Accuracy 1 Back Current        15215.5       0.09           -7605.8        1.88          0.05
            C1    Log Local Accuracy 1 Back Current    15338.3       0.08           -7667.1        3.94          0.12
            B1    Local Accuracy 1 Back                15346.5       0.08           -7671.3        2.92          0.09
              E   Total Correct                        15389.5       0.08           -7692.7        0.14          0.00
            D1    Log Local Accuracy 1 Back            15430.1       0.07           -7713.1        4.32          0.14
              F   Log Total Correct                    15525.0       0.07           -7760.5        0.77          0.03
             G    Log Trial                            15911.6       0.04           -7953.8        0.70          0.03
             H    Trial                                16014.1       0.04           -8005.1        0.07          0.00
              I   Log Entropy                          16205.5       0.03           -8100.8        -1.05         0.05
              J   Log Maximum likelihood               16207.4       0.03           -8101.7        0.30          0.02
             K    Current Accuracy                     16339.6       0.02           -8167.8        0.69          0.04
              L   Log Current Accuracy                 16339.6       0.02           -8167.8        1.00          0.06
             M    Entropy                              16389.6       0.02           -8192.8        -0.49         0.03
             N    MAP                                  16545.0       0.01           -8270.5        0.93          0.10
             O    Log MAP                              16593.9       0.00           -8295.0        0.29          0.04
              P   Maximum likelihood                   16609.7       0.00           -8302.8        5.02          0.90
             Table 3: Performance of predictors in determining subjective certainty. All were significant at p < .001.
results in a significant relationship, B = 3.206, p <.001.           and local accuracy models implies that people’s certainty is
   Table 3 shows the full model results, giving the perfor-          largely influenced by their own perception of how well they
mance of each model in predicting certainty ratings. These           are doing on the task. It is important to note that all the mod-
have been sorted by the primary measure of performance,              els which use either local accuracy or total correct outperform
AIC, which quantifies the fit of each model penalizing its           all other models.
number of free parameters (closer to −∞ is better). In this             The relative poor performance of the number of Trials in
case, the AIC is simply the AIC score of a logistic regression       predicting performance (model G) indicates that participants
including the variable of interest. This table also provides a       are not simply becoming more certain over time regardless of
pseudo R2 measure, giving a rough measure of the “amount             performance. It also excludes the possibility that learners are
of variance” accounted for by each model (this is not literally      waiting for exhaustive data before becoming certain. If they
an R2 since amount of variance does not have a clear ana-            were, a sudden spike of certainty would be visible at trial 8,
log in logistic models). The reported numbers also include           when in our experimental design they have seen all possible
a β giving the regression coefficient, its standard error, and a     feature dimensions and outcomes.
two-tailed p-value comparing it to zero.                                Strikingly, the poor performance of of the Entropy and
   As this table makes clear, the Local accuracy models out-         MAP models rules out that subjective certainty is calibrated
perform any of the alternatives, a pattern which is robust to        with an ideal learner. The poor performance of these models
the way in which local accuracy is quantified (e.g. the number       is consistent with the theory that learners are likely not main-
back that are counted or whether the current trial is included).     taining more than one hypothesis in mind—perhaps they store
The quantitatively best model A3 tracks accuracy over the            a sample from the posterior, but do not have access to the full
past three trials and includes the future accuracy on the next       posterior distribution. Such a failure of metacognition is con-
trial. One possible explanation for this is that individuals are     sistent with the poor performance of Current accuracy, a
simply deciding their own certainty based on recent perfor-          measure of whether or not the participant got the next trial
mance and whether or not they think they know the answer to          correct. Subjective certainty does not accurately predict ac-
the current item.                                                    curacy on the current example, or vice versa.
   Interestingly, model E, the Total Correct count of re-               While entropy is not a major predictor of certainty, a gen-
sponses, is the second best predictor of certainty outside of        eralized linear mixed model fit by maximum likelihood pro-
the local accuracy models. The high performance of this              vides evidence that it is a significant factor when controlling
                                                                 702

for the most predictive model (Local Accuracy 3 Back Cur-            It is possible that a large component of certainty could reflect
rent), B = 0.097, p = .005. However, despite being signif-           factors that are almost entirely removed from the veridical
icant, the effect of entropy is small, especially when com-          probabilities, such as the context of the judgement or differ-
pared to the effect of local accuracy, B = 1.264, p <.001.           ences in individual learners’ overall self-confidence or mood.
This evidence does not rule out the possibility of unknown
factors fully mediating the relationship between entropy and                              Acknowledgments
certainty.                                                           The authors would like to thank the Kidd Lab, and the Com-
                                                                     putation and Language Lab for providing feedback.
              Conclusions and Discussion
                                                                                                References
Our analyses revealed that local accuracy is the best predictor
                                                                     Barthelmé, S., & Mamassian, P. (2009). Evaluation of objec-
of certainty in our simple concept-learning task. Further, this
                                                                        tive uncertainty in the visual system. PLoS Comput Biol,
effect is robust to exactly how accuracy is computed. This
                                                                        5(9), e1000504–e1000504.
means that participants seem to be basing their certainty on
                                                                     Feldman, J. (2000). Minimization of boolean complexity in
their immediate performance—inferring certainty from their
                                                                        human concept learning. Nature, 407(6804), 630–633.
own behavior and feedback. Specifically, participants seem to
                                                                     Goodman, N. D., Tenenbaum, J. B., Feldman, J., & Griffiths,
be assessing their performance on the past several items along
                                                                        T. L. (2008). A rational analysis of rule-based concept
with a guess on whether or not they know the current item.
                                                                        learning. Cognitive Science, 32(1), 108–154.
This general pattern is consistent with metacognitive studies
                                                                     Johansson, P., Hall, L., Sikström, S., & Olsson, A. (2005).
showing that often subjects do not understand—or perhaps
                                                                        Failure to detect mismatches between intention and out-
even remember—the causes of their own behavior (Johansson
                                                                        come in a simple decision task. Science, 310(5745), 116–
et al., 2005; Nisbett & Wilson, 1977). Subjects don’t directly
                                                                        119.
observe their own cognitive processes and are often blind to
                                                                     Nisbett, R. E., & Wilson, T. D. (1977). Telling more than we
their internal dynamics. This appears to be true in the case of
                                                                        can know: Verbal reports on mental processes. Psycholog-
subjective certainty reports. They do not appear to reflect an
                                                                        ical review, 84(3), 231.
awareness of how much certainty subjects should have.
                                                                     Piantadosi, S. T. (2014). LOTlib: Learning and In-
   The analyses also help inform us about which factors do              ference in the Language of Thought. available from
not drive certainty, and several of these results are surpris-          https://github.com/piantado/LOTlib.
ing. For example, one reasonable theory of certainty posits          Shannon, C. (1948). A mathematical theory of communities.
that participants could be basing their certainty off of their          bell.. 8)/st. Techn. J, 27, 379–423.
confidence in the MAP hypothesis under consideration (as in          Shepard, R. N., Hovland, C. I., & Jenkins, H. M. (1961).
hypothesis-testing accounts of learning). Our analyses do not           Learning and memorization of classifications. Psychologi-
support this account. If participants were basing their cer-            cal Monographs: General and Applied, 75(13), 1.
tainty off of the MAP hypothesis that they were considering,         Smith, M. E., & Farah, M. J. (2011). Are prescription stim-
the MAP predictors would perform much better. Since the                 ulants smart pills? the epidemiology and cognitive neu-
MAP predictors do not form well, it is unlikely that learners’          roscience of prescription stimulant use by normal healthy
certainty relies on internal estimates of the probabilities that        individuals. Psychological bulletin, 137(5), 717.
most Bayesian learning accounts assume.                              Stevens, S. S. (1957). On the psychophysical law. Psycho-
   Our analyses also reveal that there is still a lot that we do        logical review, 64(3), 153.
not understand about human certainty. We tested many major,          Tormala, Z. L., Clarkson, J. J., & Henderson, M. D. (2011).
reasonable hypotheses about the factors that drive human cer-           Does fast or slow evaluation foster greater certainty? Per-
tainty, yet the proportion of variance explained by the highest         sonality and Social Psychology Bulletin, 37(3), 422–434.
performing predictor here is only 11%. Thus, although local          Tormala, Z. L., & Petty, R. E. (2004). Source credibility and
accuracy performs better than other predictors, it cannot be            attitude certainty: A metacognitive analysis of resistance to
the whole story. The low performance of the proposed pre-               persuasion. Journal of Consumer Psychology, 14(4), 427–
dictors here is surprising given that our hypotheses spanned            442.
major hypotheses involving metacognitive awareness of both
uncertainty and task performance—factors that most people
have previously assumed are major drivers of certainty in
learners. Further, these results hint that non-metacognitive
factors may play a surprisingly substantial role in influencing
human certainty. As an example, neurochemical changes in
the brain induced by stimulants such as Adderall and Ritalin
are known to robustly influence self-reported confidence on
performance in cognitive tasks without actually boosting ob-
jective measures of task performance (Smith & Farah, 2011).
                                                                 703

