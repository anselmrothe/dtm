                                 Which Learning Algorithms Can Generalize
                                        Identity-Based Rules to Novel Inputs?
                                                       Paul Tupper (pft3@sfu.ca)
                                           Department of Mathematics, Simon Fraser University
                                                       Burnaby, BC, V5A 1S6, Canada
                                                Bobak Shahriari (bshahr@cs.ubc.ca)
                                   Department of Computer Science, University of British Columbia
                                                     Vancouver, BC, V6T 1Z4, Canada.
                              Abstract                                    to both the forms YY and YZ, since neither of them have any
                                                                          similarity to the training words in a manner that is deemed
   We propose a novel framework for the analysis of learning al-
   gorithms that allows us to say when such algorithms can and            relevant by the algorithms. Important classes of such algo-
   cannot generalize certain patterns from training data to test          rithms include basic connectionist algorithms (Rumelhart &
   data. In particular we focus on situations where the rule that         McClelland, 1988) and the “Plain” (Baseline version) of the
   must be learned concerns two components of a stimulus being
   identical. We call such a basis for discrimination an identity-        UCLA Phonotactic Learner (Hayes & Wilson, 2008). There
   based rule. Identity-based rules have proven to be difficult or        are ways to modify these algorithms to perform better on such
   impossible for certain types of learning algorithms to acquire         tasks, for example, by introducing copying (Colavin, Levy, &
   from limited datasets. This is in contrast to human behaviour
   on similar tasks. Here we provide a framework for rigorously           Rose, 2010), special representations of identical segments in
   establishing which learning algorithms will fail at generalizing       the input (Gallagher, 2013), or weight sharing across connec-
   identity-based rules to novel stimuli. We use this framework           tions as is done in convolutional neural networks (LeCun &
   to show that such algorithms are unable to generalize identity-
   based rules to novel inputs unless trained on virtually all possi-     Bengio, 1995).
   ble inputs. We demonstrate these results computationally with             There are many informal arguments given for why the ba-
   a multilayer feedforward neural network.                               sic versions of these algorithms cannot learn identity-based
   Keywords: phonology; learning algorithms; symmetries; con-             rules. Such algorithms are unable to generalize “outside the
   nectionism
                                                                          training space” (Marcus, 2003), or “do not instantiate vari-
                                                                          ables” (Berent, 2012). Though these terms describe a gen-
                          Introduction                                    uine limitation of such algorithms, they suffer the drawback
Suppose a subject is asked to learn an artificial language in             of not being defined formally. Even though computational
which all words consist of two letters. They are told that CC,            learners themselves are clearly defined, whether a particular
AA, HH, EE, and RR are all examples of valid words in the                 algorithm is able to learn identity relations or instantiate vari-
language but that GA, EH, RA, ER, MG are not valid words.                 ables is impossible to determine precisely since the criterion
Now suppose that the learner is asked whether YY and YZ                   for these conditions is not formalized. Our present goal is to
could be valid words in the language. Presumably they will                provide a rigorous framework for these informal statements
say that YY could be a valid word in the language whereas                 about algorithms, and to provide criteria for when an algo-
YZ could not be. The obvious feature that all the valid words             rithm cannot generalize identity-based rules to novel inputs.
have in common is that they consist of two identical letters.                In the following we define learning algorithms, symmetries
This feature is not shared by the invalid words. We say in this           of sets of words, and what it means for an algorithm to be in-
case that the learners have learned an identity-based rule, and           variant under a symmetry. In our main result we show that
are able to generalize the rule to novel inputs.                          if an algorithm is invariant under some symmetry, and the
   We do not know if this exact experiment has ever been per-             training data is invariant under the same symmetry, then the
formed, but there have been analogous tests in the phonolog-              algorithm cannot learn a grammar that is not invariant under
ical domain (Berent, Marcus, Shimron, & Gafos, 2002; Gal-                 that symmetry. As an application, we demonstrate a symme-
lagher, 2013). In artificial language learning tasks, human               try that identity-based rules are not invariant under, and then
subjects are sensitive to identity relations between segments,            show that a wide class of algorithms are invariant under it.
and are able to generalize them to novel inputs. This kind                This means that such algorithms cannot learn identity-based
of effect is not specific to language though: consider a task             grammars with invariant training data, in contrast to human
where subjects are presented with pictures of pairs of socks,             performance on analogous tasks. We then demonstrate how
and are asked to say whether they form a matching pair.                   feed-forward neural networks suffer from these limitations,
   Surprisingly, given how obvious the above pattern is to hu-            independent of the number of hidden layers in the network.
man learners, many computer models of learning are not able
to learn identity-based rules like those implicit in the data                                 Formal Definitions
above, without being presented with nearly all possible in-               We consider a set W , which we call the set of words, con-
puts. These computational learners may give the same rating               taining all well-formed inputs. We stress that in the linguis-
                                                                      1289

tic case W consists of both words that are good (grammati-               We define a symmetry σ to be a bijective map from the set
cal) and words that are bad (ungrammatical). In what follows          of words W to itself; in other words, a map such that σ (w)
we will consider words to be strings of letters, but individual       is in W for all w in W , and for all v in W there is an u in W
words can be anything, such as strings of segments or feature         such that σ (u) = v. As an example of a symmetry, let σ̃ be
bundles.                                                              the map from W   e to itself given by
   To fix ideas, in what follows we will often consider a par-
ticular example of a set of words: we let W   e be the set of all                               σ̃ (XY) = YX,                        (2)
two letter words, where the letters are capitals taken from the
                                                                      for any letters X, Y. Thus the symmetry σ̃ reverses the order
English alphabet, such as AA or MG.
                                                                      of letters in two-letter words.
   We define the training data D to be a collection of word-
                                                                         We introduce symmetries in order to analyze algorithms:
rating pairs hw, ri where w is a word in W and r is a number
                                                                      we are not claiming that they have any psychological or lin-
interpreted as a rating of how “good” w is. For example,
                                                                      guistic reality. Indeed, as far as we know all maps that are nat-
using the word set W e , a dataset D might consist of
                                                                      urally occurring phonological processes are not symmetries.
  hCC, 1i, hAA, 1i, hEE, 1i, hGA, 0i, hEH, 0i, hRA, 0i. (1)           For one thing, most phonological maps satisfy σ (σ (x)) =
                                                                      σ (x) for all x (also known as idempotency (Magri, 2015)).
This dataset says that CC, AA, and EE have rating 1 (and thus         But this can only happen with a symmetry if σ (x) = x for all
are good words) and that GA, EH, and RA have rating 0 (and            x, meaning that σ does nothing.
thus are bad words). Alternatively, in a training task where             A word w is invariant under a symmetry σ if σ (w) = w. To
only good words are given to the learner, D might consist             apply a symmetry to a set of training data, we say that σ just
only of good words paired with the rating 1. But there are            acts on each word in every word-rating pair in the data set,
other possibilities: words could be paired with a rating given        but does not change the rating of that word. So if the word-
by their prevalence in a corpus, for example.                         rating pair hw, ri is in D, then the pair hσ (w), ri is in σ (D).
   To formally define a learning algorithm, consider what a           For example, if we applied σ̃ (as defined in (2)) to the dataset
learning algorithm such as the UCLA Phonotactic Learner               in (1) we would get the dataset
(Hayes & Wilson, 2008) does. First, a collection of data D is
input to the algorithm and used to choose a set of parameters            hCC, 1i, hAA, 1i, hEE, 1i, hAG, 0i, hHE, 0i, hAR, 0i.
p in a model. We can formalize this as p = A (D). Once
                                                                         We say that a dataset D is invariant under a symmetry σ
we have p, given any new input w the algorithm outputs a
                                                                      if σ (D) has precisely the same word-rating pairs as D. The
score, which we can formalize as f (p, w). Typically, the com-
                                                                      simplest way for data D to be invariant under a symmetry σ is
putation of p from D is computationally intensive whereas
                                                                      if each word in each word-rating pair in D is invariant under
once we have p, the score f (p, w) is cheap to evaluate. This
                                                                      σ . But there are other ways. For example, the symmetry σ̃
matches our experience of human behaviour where learning
                                                                      leaves the data
a language occurs over long periods of time, whereas judge-
ments of the well-formedness of novel words are readily pro-                             hBB, 1i, hGG, 2i, hEE, 0i
duced by adult speakers.
   Here we will abstract away issues of parameter setting and         invariant because the words BB, GG, EE are all invariant un-
computational effort and just view an algorithm as a map that         der σ̃ . On the other hand the data
takes a set of training data D and an input w and outputs a
rating. We consider the map L given by                                             hBG, 1i, hGB, 1i, hEA, 2i, hAE, 2i
                    L (D, w) = f (A (D), w).                          is also invariant under σ̃ , but in this case the individual words
                                                                      are not invariant, it is just that w and σ (w) always have the
Specifically, a learning algorithm L is a map that takes train-       same rating in this data set.
ing data D and word w and outputs a score L (D, w). The                  We say an algorithm L is invariant under σ if
interpretation is that this score is what you would get if you        L (σ (D), σ (w)) = L (D, w) for all D and w. In words, the
used the data D to train the algorithm and then used the re-          rating that the algorithm gives to w when trained on D is the
sulting computational model to evaluate the word w.                   same that the algorithm gives to σ (w) when trained on σ (D).
   We note that we interpret both the ratings coupled with               Our main result is a simple consequence of these defini-
words in the training data D and the scores output by the al-         tions.
gorithm L as measures of the goodness of a word. This is
                                                                      Theorem 1 If algorithm L and training data D are invariant
natural, since we expect the algorithm to give good scores to
                                                                      under symmetry σ then
words that have high ratings in the training data. However,
ratings and scores are distinct in general; for example, ratings                          L (D, w) = L (D, σ (w)),
in D could be how common a word is in a corpus and scores
from L could be intended to model how well-formed a word              for all w in W . In other words, the algorithm L gives the
is on a scale from 0 to 1.                                            same rating to w and σ (w) when trained on D.
                                                                  1290

Proof. We have                                                       rithm prefer AB to BA, since both the algorithm and the train-
                                                                     ing data are invariant under σ̃ , and BA = σ̃ (AB). Of course,
          L (D, w) = L (σ (D), σ (w)) = L (D, σ (w))                 this is not necessarily a defect of the algorithm L ; if some
                                                                     words with increasing or decreasing sonority were included
where the first equality follows from the invariance of L un-        in D, then D would not be invariant under σ̃ , and L could
der σ , and the second inequality follows from the invariance        learn the grammar.
of D under σ .                                                         In the next section we will give a less straightforward ex-
   Example: Consider a language containing 10 letters, each          ample, allowing us to formalize the idea of identity-based
letter having a sonority value between 1 and 5 according to          rules for learning algorithms.
the following table. (Sonority is an abstract phonological
variable, roughly corresponding to how close a segment is                                Identity-Based Rules
to a vowel.) Words in the language consist of only two let-          We now use the above result to show that certain algorithms
                                                                     cannot learn identity-based rules unless trained on words con-
         Table 1: Segments in a Hypothetical Language                taining virtually all letters in the alphabet. That is, the algo-
                                                                     rithm cannot extend the identity-based rules to words contain-
                       segments   sonority                           ing letters that it has not explicitly been trained on. This is in
                       A O        5                                  sharp contrast to human learners who are able to generalize
                       W Y        4                                  identify-based rules (in the phonological context, for exam-
                       M N        3                                  ple) to segments they have not encountered before(Berent et
                       V Z        2                                  al., 2002).
                       B D        1                                     We return to the example at the beginning of the paper: W    e
                                                                     is the set of all words consisting of two letters. We stipulate
                                                                     that grammatical words are those consisting of two identical
ters. Suppose that all words in the language have increasing         letters and all other words are ungrammatical. Suppose we
or constant sonority. So, BA, MO, ZW, BD could all be words          want the algorithm to learn this grammar, but train it on data
in the language, but AD, AN, and WV could not be. Consider           omitting any words containing the letters Y and Z. What al-
the letter reversing symmetry σ̃ given in (2). If you apply σ̃       gorithms will not be able to learn the correct grammar under
to an ungrammatical word (e.g. AB) you get a grammati-               these conditions?
cal word (BA). If you apply σ̃ to a grammatical word with               Define the symmetry σ of W by the following:
increasing sonority you get an ungrammatical word. Words
with two letters of the same sonority give you back another             σ (X1Y ) = X1 Z, σ (X1 Z) = X1Y, σ (X1 X2 ) = X1 X2 ,
word with letters of the same sonority.
   Now suppose you have a learning algorithm L that is in-           for all letters X1 , X2 , with X2 not equal to Y or Z. In other
variant under σ̃ . This means that if you take a data set D,         words, if the second segment is Y , σ changes it to Z, if the
train the algorithm on it, and then use the algorithm to evalu-      second segment is Z, σ changes it to Y , and if the second
ate word w, you will get the same result if you train the algo-      segment is neither, then the word is unchanged.
rithm on σ̃ (D) (in which all the words are reversed) and then          Now suppose our training data D contains no words with
use the algorithm to evaluate σ̃ (w), which is just the reversal     either segments Y or Z as the second segment. D may contain
of w.                                                                both grammatical words (e.g. CC) with rating 1 and ungram-
   Suppose we give the algorithm data D that is invariant un-        matical words (e.g. CE) with rating 0. Then D is invariant
der σ̃ . For simplicity we assume that D consists only of gram-      under σ . Theorem 1 shows that if the algorithm L is also
matical words each assigned the rating 1. In this case, the          invariant under σ then it will give the same rating to w and
only way D can be invariant under σ̃ is if all the words in D        σ (w) for any word W when trained on D. In that case we
have constant sonority, and for every such word XY in D, YX          would have that it gives the same rating to the words YY and
is also in D. Can the algorithm correctly learn the generaliza-      Y Z, showing that it cannot learn the identity based grammar.
tion that words in the language must have increasing or level           Below we provide an example of an algorithm invariant
sonority from this data set?                                         under this symmetry. But in general we informally argue that
   Theorem 1 shows that it cannot, as follows. According to          any algorithm that does not in some way explicitly check for
the theorem, L (D, w) = L (D, σ̃ (w)). All we need to do is          identity between the letters, or somehow enforce a similar
let w equal a word of increasing sonority, such as BA, to see        treatment of those two letters in processing, cannot correctly
that the algorithm with training data D gives the same score         learn that YY is a more well formed word than Y Z, if it is
to BA and AB. Since the first is grammatical and the second is       never given words with a second letter Y or Z as training data.
ungrammatical, the algorithm clearly has not learned the cor-
rect rule governing grammaticality in the language. This is                           Randomized Algorithms
pretty commonsensical: one way to think of it is that there is       Many algorithms for learning use randomness at some point
nothing in the algorithm or the training data to make the algo-      in their operation. It may either be in the computation that
                                                                 1291

takes the input data to the parameters p (for example, by             1, along with 48 randomly generated words with mismatched
which order the input words are used) or in the map from              segments taken from the list A, . . . , X, each paired with rating
the parameters and a new input word to a word score s. In             0.
the former case p = A (D) is a random function of D; in the              To assess the ability of the learner to generalize to novel
latter s = f (p, w) is a random function of p and w. In either        inputs, after training we tested it on the words
case, this leads to L (D, w) being random for any fixed D and
w.
   Under these conditions, it is unlikely that invariance of the                      YY, ZZ, XY, YZ, XZ, ZY,
form described above will hold. Instead we now define in-
variance of L under σ to be                                           where X ∈ {A, B, . . . , X} were randomly selected. For each
                                                                      learner, the experiment was independently repeated 40 times
                EL (σ (D), σ (w)) = EL (D, w),                        with different random seeds.
where E denotes expectation. (If X is a random variable, EX
is approximately what we would get if we took the average of
a large number of samples of X.)                                      Encodings. We distinguish two different representations
   We now get the same result as before. This is a strictly           for the segments A to Z, namely the localist and distributed
stronger result than Theorem 1, since a deterministic algo-           encodings. Both of these representations use a fixed length
rithm is just a special case of a randomized algorithm.               bit string. However, while localist codes (also known as 1-of-
Theorem 2 If random algorithm L and training data D are               k encoding) are constrained to include a single non-zero bit,
invariant under symmetry σ then                                       distributed codes can be any arbitrary combination of k bits,
                                                                      for some fixed k. Distributed encodings are a much more
                  EL (D, w) = EL (D, σ (w)),                          compact representation of data; indeed, for the same string-
                                                                      length k, we can represent an exponentially large number of
for all w in W . In other words, the algorithm L gives on             segments 2k . The experiment was run on both types of encod-
average the same rating to w and σ (w) when trained on D.             ing with k = 26. When distributed encoding was used, codes
Proof. We have                                                        for each letter were randomly generated each repetition, so
                                                                      the exact encoding of the segment X, for instance, is almost
       EL (D, w) = EL (σ (D), σ (w)) = EL (D, σ (w))                  certainly different between two repetitions of a given run.
where the first equality follows from the invariance of L un-         Neural Network Learners
der σ , and the second inequality follows from the invariance
of D under σ .                                                 
                                                                      We tested our theoretical findings on the most popular model
                         Experiments                                  in the machine learning literature today: the artificial neu-
                                                                      ral network. The words were fed into the neural network
We demonstrate the consequences of our theorems in a com-
                                                                      by simply concatenating the two 26-bit codes of their let-
putational experiment where we use a deep neural network
                                                                      ters. We experimented with many different architectures,
to learn the grammar described in our introduction. The net-
                                                                      ranging from one to three hidden layers, and from 256 to
works are trained using data in which two-letter words with
                                                                      1024 units per layer, with tanh nonlinearities for all hidden
two identical letters are good, and two-letter words with two
                                                                      units. We trained the models via backpropagation using an
different letters are bad. The network is then asked to as-
                                                                      iterative quasi-Newton gradient descent algorithm called the
sess novel words containing segments it has not seen in the
                                                                      limited memory Broyden-Fletcher-Goldfarb-Shanno method
training set. Randomness enters into the training of these net-
                                                                      (L-BFGS), with a maximum of 100 iterations. Both the neu-
works in various places and so Theorem 2 is the relevant re-
                                                                      ral network and its optimization are implemented in torch
sult in this case. Consequently, we do not compare individual
                                                                      (Collobert, Bengio, & Mariéthoz, 2002).
trainings of the network on the novel stimuli. For each novel
stimulus we train the network numerous times and take the
average score over all the trainings. It is these scores that are     Results
compared between stimuli.
                                                                      We present results for the case of each hidden layer having
Task and Dataset                                                      256 units, as the results are similar for more units per hidden
Before discussing the neural network learners that were               layer. In Figure 1, for the localist encoding, we plot the av-
tested, we describe the dataset and task that was required of         erage score output by the neural network for each of the test
them. As before, our set of words W consisted of all two let-         words above, for 1, 2, and 3 hidden layers. In addition, the
ter words with letters running from A to Z. The training set          averaged training scores are reported in the top two bars of
consisted of the 24 words AA, BB, . . ., XX paired with rating        each panel.
                                                                  1292

Figure 1: Scores for various words for the network with lo-     Figure 2: Scores for various words for the network with dis-
calist encoding for 1, 2, and 3 hidden layers.                  tributed encoding for 1, 2, and 3 hidden layers.
                                                            1293

   Looking at the top plot in the figure, showing the results                                 References
for one hidden layer, the words YY and ZZ get scores of              Berent, I. (2012). The phonological mind. Cambridge Uni-
around 0.3 in contrast to the score of near 1 given for the well-      versity Press.
formed input AA. The networks are unable to determine that           Berent, I., Marcus, G. F., Shimron, J., & Gafos, A. I. (2002).
YY and ZZ are grammatical. Likewise, the other test words              The scope of linguistic generalizations: evidence from He-
with differing segments and containing the segments Y or Z             brew word formation. Cognition, 83(2), 113 - 139.
have scores ranging from approximately 0.3 to 0.5. The net-          Colavin, R., Levy, R., & Rose, S. (2010). Modeling OCP-
works are not able to distinguish between grammatical and              place in Amharic with the Maximum Entropy phonotactic
ungrammatical words in this case.                                      learner. In Proceedings volume of the 46th meeting of the
   The ability of the networks to generalize to novel inputs is        Chicago Linguistics Society.
not improved by adding further hidden layers. The second             Collobert, R., Bengio, S., & Mariéthoz, J. (2002). Torch: a
and third plots in Figure 1, corresponding to two and three            modular machine learning software library. Technical Re-
hidden layers, show very similar results to the first. To within       port IDIAP-RR 02-46.
statistical accuracy, the scores for YY, ZZ, YZ, and ZY are all      Gallagher, G. (2013, 8). Learning the identity effect as an
the same. The networks are not able to discriminate between            artificial language: bias and generalisation. Phonology, 30,
grammatical and ungrammatical words when the words in-                 253–295. doi: 10.1017/S0952675713000134
cluded the novel segments Y and Z.                                   Hayes, B., & Wilson, C. (2008, 2016/01/09). A maximum en-
   This poor performance is perhaps not surprising for the lo-         tropy model of phonotactics and phonotactic learning. Lin-
calist encoding, as observed by Marcus (Marcus, 2003): in              guistic Inquiry, 39(3), 379–440.
the localist encoding, introducing new segments correspond           LeCun, Y., & Bengio, Y. (1995). Convolutional networks for
to activating new input units that were never active during            images, speech, and time series. The handbook of brain
training, and therefore whose weights never changed from               theory and neural networks, 3361(10).
their random initializations. However, in Figure 2 we show           Magri, G. (2015). Idempotency and chain shifts. In K.-
that the poor performance remains true in the case of dis-             m. Kim (Ed.), Proceedings of WCCFL 33: the 33rd annual
tributed representations. In the first plot, we show the results       West Coast Conference in Formal Linguistics. Cascadilla
for a single hidden layer. The networks give a rating higher           Proceedings Project.
than 0.5 for both YY and ZZ, which is higher than the score          Marcus, G. F. (2003). The algebraic mind: Integrating con-
given by the localist networks, but the same high rating is            nectionism and cognitive science. MIT press.
given to the words YZ and ZY. A similar pattern is repeated          Rumelhart, D. E., & McClelland, J. L. (1988). Parallel dis-
for the two and three-layer case. The networks are not able to         tributed processing (Vol. 1). IEEE.
discriminate between grammatical and ungrammatical words
containing novel segments, even when distributed representa-
tions are used.
                          Discussion
That connectionist networks are unable to generalize what are
sometimes called “algebraic” rules to novel inputs is not a
new observation (Marcus, 2003; Berent, 2012). Our contri-
bution has been to give a formalized description and proof
of this phenomenon. Furthermore, our results and computer
experiments reinforce that Deep Learning, in the form of the
ability to train connectionist networks with multiple hidden
layers, does not alone overcome these limitations.
                    Acknowledgments
The authors thank Nilima Nigam for comments on an earlier
draft of this manuscript. PT was supported by an NSERC
Discovery Grant, a Research Accelerator Supplement, and
held a Tier II Canada Research Chair. BS was supported by
an NSERC Discovery Grant.
                                                                 1294

