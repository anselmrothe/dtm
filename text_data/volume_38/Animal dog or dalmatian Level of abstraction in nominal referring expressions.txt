 Animal, dog, or dalmatian? Level of abstraction in nominal referring expressions
                         Caroline Graf, Judith Degen, Robert X.D. Hawkins, Noah D. Goodman
                                         cgraf@uos.de, {jdegen,rxdh,ngoodman}@stanford.edu
                                                 Department of Psychology, 450 Serra Mall
                                                          Stanford, CA 94305 USA
                              Abstract
   Nominal reference is very flexible—the same object may be
   called a dalmatian, a dog, or an animal when all are literally
   true. What accounts for the choices that speakers make in how
   they refer to objects? The addition of modifiers (e.g. big dog)
   has been extensively explored in the literature, but fewer stud-
   ies have explored the choice of noun, including its level of ab-
   straction. We collected freely produced referring expressions
   in a multi-player reference game experiment, where we ma-
   nipulated the object’s context. We find that utterance choice
   is affected by the contextual informativeness of a description,
   its length and frequency, and the typicality of the object for       Figure 1: Screenshots from speakers’ and listeners’ points
   that description. Finally, we show how these factors naturally
   enter into a formal model of production within the Rational          of view, showing role names and short task descriptions, the
   Speech-Acts framework, and that the resulting model predicts         chatbox used for communication and a display of three pic-
   our quantitative production data. Keywords: referential ex-          tures of objects. The referent was identified to the speaker by
   pressions, levels of reference, basic level, experimental prag-
   matics, computational pragmatics                                     a green box.
   Referring to objects is a core function of human language,
and a wealth of research has explored how speakers choose               to produce a cheap ambiguous utterance rather than a costly
referring expressions (Herrmann & Deutsch, 1976; Pech-                  (e.g. long or difficult-to-retrieve) unambiguous one. Finally,
mann, 1989; van Deemter, Gatt, van Gompel, & Krahmer,                   classic work on concepts suggests that typicality of a refer-
2012). However, most of this literature has focused on the ad-          ent within its category affects the choice of reference (Rosch,
dition of modifiers (as in the choice between “the dog”, “the           Mervis, Gray, Johnson, & Boyes-Braem, 1976). In particu-
brown dog”, and “the big brown dog”, e.g., Sedivy, 2003;                lar, speakers will generally choose to refer at the basic level
Koolen, Gatt, Goudbeek, & Krahmer, 2011). Here we in-                   (e.g. “dog”), but may become more specific for objects that
vestigate how speakers choose a simple nominal referring                are atypical for the basic level term.
expression—what governs the choice of calling a particular                 To evaluate the impact of these factors on nominal refer-
object “the dalmatian”, “the dog”, or “the animal” when all             ence we constructed a two-player online game (Fig. 1). Par-
are literally true? That is, what governs the choice of the tax-        ticipants saw a shared context of objects, one of which was
onomic level at which an object is referred to? Noun choice             indicated as the referent only to the speaker. The speaker was
can be seen as the most basic decision in forming a referring           asked to communicate this object to the listener, who then
expression. Like modification, these choices differ in their            chose among the objects. Critically, the speaker and listener
specificity; unlike modification, the number of words used              communicated by free use of a chat window, allowing us to
does not differ—in English, some noun must be chosen. In                gather relatively natural referring expressions. We manipu-
this paper we provide experimental evidence from a coordina-            lated the category of distractor objects and used items that
tion game regarding the flexible choice of nominal referring            varied in utterance complexity and object typicality. This al-
expressions and explain this data with a probabilistic model            lowed us to evaluate whether each factor influences the re-
of pragmatic production.                                                ferring expressions generated by participants. We expect that
   Previous evidence about the generation of referring expres-          speakers will (1) tend to avoid longer or less frequent terms,
sions suggests that choice of reference level will depend on            and (2) will pragmatically prefer more specific referring ex-
the interplay of several factors. Grice’s Maxim of Quan-                pressions when the target and distractor(s) belong to the same
tity (Grice, 1975) implies a pressure for speakers to be suf-           higher-level taxonomic category or when distractors are more
ficiently informative. For instance, a speaker who is trying            typical members of that category level.
to distinguish a dalmatian from a German Shepherd would                    A promising modeling approach for capturing the quanti-
be expected to avoid the insufficiently specific term “dog”             tative details of human language use is the Rational Speech-
(Brennan & Clark, 1996). On the other hand, recent work                 Acts (RSA) framework (Frank & Goodman, 2012; Good-
in experimental pragmatics has shown that the choice of re-             man & Stuhlmüller, 2013). The RSA framework has been
ferring expression depends on the cost of utterance alterna-            applied to many language interpretation tasks (e.g. Good-
tives (Rohde, Seyfarth, Clark, Jäger, & Kaufmann, 2012; De-            man & Stuhlmüller, 2013; Kao, Wu, Bergen, & Goodman,
gen, Franke, & Jäger, 2013); sometimes, speakers are willing           2014), but relatively rarely to production data (but see Franke,
                                                                    2261

2014; Orita, Vornov, Feldman, & Daumé III, 2015). We de-
scribe an RSA model of nominal reference that includes in-
formativeness, cost, and typicality effects. A speaker in RSA
is treated as an approximately optimal decision maker who
chooses which utterance to use to communicate to a listener.
The speaker has a utility which includes terms for the cost of
producing an utterance (in terms of length or frequency) and
the informativeness of the utterance for a listener. The lis-
tener is treated as a literal Bayesian interpreter who updates
her beliefs given the truth of the utterance. These truth val-
ues are usually treated as deterministic (an object either is a
                                                                       Figure 2: The four context conditions, exemplified by the dog
“dog” or it is not); here we relax this formulation in order to
                                                                       domain. The target is outlined in green; the types of distrac-
incorporate typicality effects. That is, we elicit typicality rat-
                                                                       tors differ with condition (see text).
ings in a separate experiment, and model the listener as updat-
ing her beliefs by weighting the possible referents according
to how typical each is for the description used. We evalu-             from filler trials were not reused on target trials. Trial order
ate the quantitative model predictions against our production          was randomized.
data. The model also allows us to evaluate the need for each
extra component—typicality, length, frequency—and deter-
                                                                       Procedure Pairs of participants were connected through a
mine whether the empirical bias toward reference at the basic
                                                                       real-time multi-player interface (Hawkins, 2015), with one
level (Rosch et al., 1976) can be accounted for without build-
                                                                       member of each pair assigned the speaker role and the other
ing it in as a separate factor.
                                                                       to the listener role. Participants kept their allotted roles for
        Experiment: nominal reference game                             the entire experiment. The setup for both the speaker and
                                                                       the listener is shown in Fig. 1. Each saw the same set of
Methods                                                                three images, but positions were randomized to rule out trivial
Participants and materials We recruited 56 self-reported               position-based references like “the middle one.” The target
native speakers of English over Mechanical Turk. Partici-              object was identified by a green square surrounding it for the
pants completed the experiment in pairs of two, yielding 28            speaker (but not listener). Players used a chatbox to send text
speaker-listener pairs.                                                messages to each other. The task was for the speaker to get
   Stimuli were selected from nine distinct domains, each cor-         the listener to select the target object.
responding to distinct basic level categories such as “dog.”
For each domain, we selected four subcategories to form our            Annotation To determine the level of reference for each
target set (e.g. “dalmatian”, “pug”, “German Shepherd” and             trial, we followed the following procedure. First, trials on
“husky”). Each domain also contained an additional item                which the listener selected the wrong referent were excluded,
which belonged to the same basic level category as the tar-            leading to the elimination of 1.2% of trials. Then, speak-
get (e.g. “greyhound”) and items which belonged to the same            ers’ and listeners’ messages were parsed automatically; the
supercategory but not the same basic level (e.g. “elephant” or         referential expression used by the speaker was extracted for
“squirrel”). The latter items were used as distractors.                each trial and checked for whether it contained the current
   Each trial consisted of a display of three images, one of           target’s correct sub, basic or super level term using a sim-
which was designated as the target object. Every pair of par-          ple grep search. In this way, 66.2% of trials were labelled
ticipants saw every target exactly once, for a total of 36 trials      as mentioning a pre-coded level of reference. In the next
per pair. These target items were randomly assigned distrac-           step, remaining utterances were checked manually to deter-
tor items which were selected from four different context con-         mine whether they contained a correct level of reference term
ditions, corresponding to different communicative pressures            which was not detected by the parsing algorithm due to typos
(see Fig. 2). We refer to these conditions with pairs of nu-           or grammatical modification of the expression. In this way,
merals specifying which levels of the taxonomy are present             meaning-equivalent alternatives such as “doggie” for “dog”,
in the distractors: (a) item12: one distractor of the same ba-         or contractions such as “gummi”,“gummies” and “bears” for
sic level and one distractor of the same superlevel (e.g. target:      “gummy bears” were counted as containing a level of ref-
“dalmatian”, distractor 1: “greyhound”, distractor 2: “squir-          erence term. This caught another 13.8% of trials. A total
rel”), (b) item22: two distractors of the same superlevel, (c)         of 20.0% of correct trials were excluded because the utter-
item23: one distractor of the same superlevel and one unre-            ance consisted only of an attribute of the superclass (“the
lated item and (d) item33: two unrelated items.                        living thing” for “animal”), of the basic level (“can fly” for
   Furthermore, the experiment contained 36 filler items, in           “bird”), of the subcategory (“barks” for “dog”) or of the par-
which participants were asked to produce referential expres-           ticular instance (“the thing facing left”) rather than a cate-
sions for objects which differed only in size and color. Images        gory noun. These kinds of attributes were also sometimes
                                                                   2262

mentioned in addition to the noun in the trials which were                                               sub     basic           super
included in the analysis—4.0% of sub level terms, 12.6% of                                       0.8
basic level terms, and 46.2% of super level terms contained an                                   0.6
                                                                                                                                         empirical
additional modifier. On 0.5% of trials two different levels of
                                                                         Utterance probability
                                                                                                 0.4
reference were mentioned; in this case the more specific level
                                                                                                 0.2
of reference was counted as being mentioned in this trial.
                                                                                                 0.0
                                                                                                 0.8
Typicality norms To examine the influence of typicality on
                                                                                                 0.6
speaker behavior, we obtained typicality estimates in a sepa-
rate norming study. 240 participants were recruited through                                      0.4                                     model
Mechanical Turk. On each trial, we presented participants                                        0.2
with an image from the main experiment and asked them                                            0.0
“How typical is this for X?”, where X was a category label                                                 m
                                                                                                       ite 2
                                                                                                           m
                                                                                                       ite 2 1
                                                                                                             2                 m
                                                                                                                            ite 21
                                                                                                           m 2                 m
at the sub-, basic-, or super- level. They then adjusted a slider
                                                                                                       ite 3
                                                                                                       ite m 33             ite 22
                                                                                                           m
                                                                                                       ite 2 1                 m
                                                                                                                            ite 32
                                                                                                   ite     m
                                                                                                       ite 2
                                                                                                           m
                                                                                                       ite 3
                                                                                                           m 2
                                                                                                             2
                                                                                                             33            ite m 33
bar ranging from not at all typical to very typical.                                                           Condition
   Due to the large number of possible combinations of ob-
jects, we only collected norms for certain combinations of           Figure 3: Empirical utterance probabilities (top row) and
objects and descriptions: for each target (e.g., dalmatian),         model posterior predictive MAP estimates (bottom row) by
we collected typicality at all three levels (“dalmatian,” “dog,”     condition, collapsed across targets and domains. Error bars
and “animal”). For each distractor of the same superclass as         indicate bootstrapped 95% confidence intervals.
the target (distsamesuper, e.g., a kitten), we collected typ-
icality at all three levels of the target. For each distractor
of a different superclass (distdiffsuper, e.g., a basketball) we     English corpus ranging from 1960 to 2008. Length was coded
only collected typicality at the super- level of the target (“an-    as the ratio of the sub to the basic level’s length.1 That is, a
imal”) and assumed lowest typicality at the other levels. This       higher frequency difference indicates a lower cost for the sub
resulted in the following distribution of 745 norms: target-         level term compared to the basic level, while a higher length
sub (36), target-basic (36), target-super (36), distdiffsuper-       ratio reflects a higher cost for the sub level term compared to
super (168), distsamesuper-sub (331), distsamesuper-basic            the basic level.2 Typicality was coded as the ratio of the tar-
(93), and distsamesuper-super (45).                                  get’s sub to basic level label typicality. That is, the higher the
   Each participant provided typicality ratings for 7 target, 10     ratio, the more typical the object was for the sub level label
distdiffsuper, and 28 distsamesuper cases (randomly sampled          compared to the basic level. For instance, the panda was rel-
from the total set of items). Each case received between 6           atively atypical for its basic level “bear” (mean rating 0.75)
and 27 ratings. Raw slider values ranged from 0 (not typical)        compared to the sub level term “panda bear” (mean rating
to 1 (very typical); average slider values were used as the          0.98), which resulted in a relatively high typicality ratio.
typicality values throughout our results.                               Condition was coded as a three-level factor: sub neces-
                                                                     sary, basic sufficient, and super sufficient, where item22 and
Results                                                              item23 were collapsed into basic sufficient. Condition was
Proportions of sub, basic, and super level utterance choices in      Helmert-coded: two contrasts over the three condition levels
the different context conditions are shown in the top row of         were included in the model, comparing each level against the
Fig. 3. The sub level term was preferred where it was nec-           mean of the remaining levels (in order: sub necessary, ba-
essary for unambiguous referent identification, i.e., when a         sic sufficient, super sufficient). This allowed us to determine
distractor of the same basic level category as the target was        whether the probability of type mention for neighboring con-
present in the scene (item12, e.g. target: dalmatian, distrac-       ditions were significantly different from each other, as sug-
tor: greyhound). Where it was not necessary (i.e., when there        gested by Fig. 3.3 The model included random by-speaker
was no other object of the same basic level category present,        and by-domain intercepts.
as in conditions item22, item23 and item33), there was a clear          A summary of results is shown in Table 1. The log odds
preference for the basic level term. The super level term was
strongly dispreferred overall, though it was used on some tri-           1 We used the mean empirical lengths in characters of the utter-
als, especially where informativeness constraints on utterance       ances participants produced. For example, the minivan, when re-
                                                                     ferred to at the subcategory level, was sometimes called “minivan”
choice were weakest (item33).                                        and sometimes “van” leading to a mean empirical length of 5.64.
   To test for the independent effects of informativeness,           This is the value that was used, rather than 7, the length of “mini-
length, frequency, and typicality on sub-level mention, we           van”.
                                                                         2 We replicate the well-documented negative correlation between
conducted a mixed effects logistic regression. Frequency was         length and log frequency (r = −.53 in our dataset).
coded as the difference between the sub and the basic level’s            3 Adding terms that code the ratio of the sub vs super level fre-
log frequency, as extracted from the Google Books Ngram              quency and length did not lead to an improvement of model fit.
                                                                  2263

                                                                                                                 sub             basic           super                 sub             basic           super
                                                                                                              necessary        sufficient       sufficient          necessary        sufficient       sufficient
            Table 1: Mixed effects model summary.
                                                                            Proportion of sub level mention
                                                                                                       0.8
                                                                                                                                                             0.8
                                   Coef β      SE(β)         p
                                                                                                       0.6                                                   0.6
   Intercept                        −0.30       0.35     >0.4
   Condition sub.vs.rest             2.46       0.24     <.0001                                        0.4                                                   0.4
   Condition basic.vs.super          0.52       0.23     <.05
   Length                           −0.52       0.14     <.001                                         0.2                                                   0.2
   Frequency                        −0.02       0.08     >0.78
                                                                                                       0.0                                                   0.0
   Typicality                        4.17       0.84     <.0001                                               short long     short long        short long           more less       more less       more less
   Length:Frequency                 −0.30       0.11     <.01                                                              Sub level length
                                                                                                                                                                   typical typical typical typical typical typical
                                                                                                                                                                               Sub level typicality
                                                                         Figure 4: Probability of using sub, basic and super level
of mentioning the sub level term was greater in the sub nec-             terms. Left: when the sub length is relatively short (.67,2] or
essary condition than in either of the other two conditions,             long [2,4.67) compared to the basic level term length. Right:
and greater in the basic sufficient condition than in the su-            when the target object was relatively more [1.06,1.91) or less
per sufficient condition, suggesting that the contextual infor-          (.88,1.06] typical for the sub compared to the basic level term.
mativeness of the sub level mention has a gradient effect on
utterance choice.4 There was also a main effect of typical-              and typicality. As in earlier Rational Speech-Acts (RSA)
ity, such that the sub level term was preferred for objects that         models (Frank & Goodman, 2012; Goodman & Stuhlmüller,
were more typical for the sub level compared to the basic                2013), the speaker seeks to be informative with respect to an
level description (Fig. 4). In addition, there was a main effect         internal model of a literal listener. This listener updates her
of length, such that as the length of the sub level term in-             beliefs to rule out possible worlds that are inconsistent with
creased compared to the basic level term (“chihuahua”/“dog”              the meaning of the speaker’s utterance. Rather than assuming
vs. “pug”/“dog”), the sub level term was dispreferred (“chi-             that words have deterministic truth conditions, as has usually
huahua” is dispreferred compared to “pug”, Fig. 4). Finally,             been done in the past, we account for typicality by allowing
while there was no main effect of frequency, we observed                 each label a graded meaning. For instance, the word “dog”
a significant length by frequency interaction, such that there           describes a dalmatian better than a grizzly bear, but it also
was a frequency effect for the relatively shorter but not the            describes a grizzly bear better than a tennis ball. The speaker
relatively longer sub level cases: for shorter sub level terms,          also seeks to be parsimonious: the speaker utility includes
relatively high-frequency sub level terms were more likely to            both informativeness and word cost; cost includes both length
be used than relatively low-frequency sub level terms.                   and frequency.
   Unsurprisingly, there was also significant by-participant                Formally, we start by specifying a literal listener L0 who
and by-domain variation in the log odds of sub level term                hears a word l at a particular level of reference in the context
mention. For instance, mentioning the subclass over the ba-              of some set of objects O and forms a distribution over the
sic level term was preferred more in some domains (e.g. in               referenced object, o ∈ O :
the “candy” domain) than in others. Likewise, some domains
had a greater preference for basic level terms (e.g. the “shirt”                                                                            PL0 (o|l) ∝ [[l]](o).
domain). Using the superclass term also ranged from hardly
                                                                         Here [[l]](o) is the lexical meaning of the word l when ap-
being observable (e.g. the “flower” domain) to being used
                                                                         plied to object o. We take this to be a real number indicating
more frequently (e.g. in the “bird” domain). Nevertheless,
                                                                         the degree of acceptability of object o for category l. We re-
mentioning the sub level term was always the most frequent
                                                                         late this to our empirically elicited typicality norms via an ex-
choice where a distractor of the same basic level was dis-
                                                                         ponential relationship: [[l]](o) = exp(typicality(o, l)).5 This
played. Furthermore, it was the case in all domains that the
                                                                         relationship is motivated by considering the effect of a small
sub level term was mentioned most frequently and the basic
                                                                         difference in typicality on choice probability: in our elicita-
level least frequently in just this condition, compared to the
                                                                         tion experiment a small difference in rating should mean the
other three conditions.
                                                                         same thing at the top and bottom of the scale (it is visually
               Modeling level of reference                               equivalent on the slider that participants used). In order for a
                                                                         small difference in typicality rating to have a constant effect
We formulated a probabilistic model of reference level selec-
                                                                         on relative choice probability (which is a ratio), the relation-
tion that integrates contextual informativeness, utterance cost,
                                                                         ship must be exponential.
   4 Importantly, model comparison between the reported model and           Next, we specify a speaker S1 who intends to refer to a
one that subsumes basic and super under the same factor level re-        particular object o ∈ O and chooses among possible nouns l ∈
vealed that the three-level condition variable is justified (χ2 (1) =
5.7, p < .05), suggesting that participants don’t simply revert to the      5 Cases where typicality was not elicited were assumed to have
basic level unless contextually forced not to.                           typicality 0.
                                                                     2264

                                                                                                                                                                 beta_f           beta_l          lambda
                                                                                                                                                      3000
                         1.00                                                     ●       ●        ●●● ●
                                                                                                       ●●●● ●
                                                                                                                                              count
                                                                                                                         condition                    2000
                                                                                                                     ●
                                                                                                             ●
                                                                                                         ● ●
                                                                                                                 ●        ●   item12                  1000
  Empirical proportion
                         0.75                     ●
                                                          ●                   ●                ●         ●
                                                                                                                 ●            item22                    0
                                                                              ●
                                                                      ●       ●       ●              ●
                                                                                                                              item23                         0   1        2   2        3   4 8   10   12   14
                                                                          ●                                  ●                                                                    value
                                                                                                                              item33
                         0.50          ●          ●   ●   ●                       ●       ●    ●             ●
                                       ●                              ●
                                                                                                                         refLevel         Figure 6: Posterior distribution over model parameters. Max-
                                           ●   ●                  ●       ●
                                                                  ●                       ●                               ●   basic       imum a posteriori (MAP) λ = 10.8, 95% highest density in-
                         0.25              ●   ●              ●                       ●
                                   ●
                                     ● ●                                                                                  ●   sub         terval (HDI) = [9.7, 12.8]; MAP βl = 2.5, HDI = [1.9, 3.1];
                                 ● ● ●
                                 ●                                                                                        ●   super       MAP β f = 1.3, HDI = [0.8, 1.8].
                         0.00    ●
                                 ●●
                                 ●
                                 ●●
                                 ●
                                 ●●●
                                   ●● ●●●
                                        ●●● ● ●       ●       ●
                                0.00           0.25                   0.50                    0.75               1.00                     items. On examination, candy items like “gummy bears” or
                                           Model predicted probability
                                                                                                                                          “jelly beans” were particularly problematic, being referred to
                                                                                                                                          primarily by their sub level term in all contexts.
Figure 5: Mean empirical production data for each level of                                                                                   Parameter posteriors are presented in Fig. 6. Informative-
reference against the MAP of the model posterior predictive                                                                               ness is weighted relatively strongly, while length is weighted
at the by-target level.                                                                                                                   somewhat more strongly than frequency. Note that the 95%
                                                                                                                                          highest density intervals (HDIs) for all three weight param-
                                                                                                                                          eters exclude zero, indicating that some contribution of each
L (o). We take L (o) to be the three labels for o at sub, basic,
                                                                                                                                          is useful in explaining the data. In order to ascertain whether
and super level. The speaker chooses among these nouns in a
                                                                                                                                          typicality was indeed contributing to the explanatory power of
way that is influenced by informativeness of the noun for the
                                                                                                                                          the model, we ran an additional Bayesian data analysis with
literal listener (ln PL0 (o|l)), the frequency (ĉ f ) and the length
                                                                                                                                          an added typicality weight parameter βt ∈ [0, 1]. This param-
(ĉl ), each weighted by a free parameter:
                                                                                                                                          eter interpolated between empirical typicality values (when
                                PS1 (l|o) ∝ exp(λ ln PL0 (o|l) + β f ĉ f + βl ĉl )                                                      βt =1) and deterministic (i.e. 0 or 1) a priori values based on
                                                                                                                                          the true taxonomy (when βt =0). We found a MAP estimate
Length cost ĉl was defined as the empirical mean number of                                                                               for βt of .94, HDI = [0.88, 1], strongly indicating that it is
characters used to refer at that level and frequency cost ĉ f was                                                                        useful to incorporate empirical typicality values. Finally, we
the log frequency in the Google Books corpus from 1960 to                                                                                 ran a model including a parameter weighting the product of
the present.                                                                                                                              frequency and cost, corresponding to the interaction term in
   We performed Bayesian data analysis to generate model                                                                                  our regression analysis. Its posterior distribution was strongly
predictions, conditioning on the observed production data                                                                                 peaked at 0, indicating that any contribution of the interaction
(coded into sub, basic, and super labels as described above)                                                                              is already captured by other aspects of the model.
and integrating over the three free parameters. We as-
sumed uniform priors for each parameter: λ ∼ Uni f (0, 20),                                                                                                      Discussion and conclusion
β f ∼ Uni f (0, 5), βl ∼ Uni f (0, 5). We implemented both the                                                                            The choice speakers make of how to refer to an object is in-
cognitive and data-analysis models in the probabilistic pro-                                                                              fluenced by a rich variety of factors. In this paper, we specif-
gramming language WebPPL (Goodman & Stuhlmüller, elec-                                                                                   ically investigated the choice of level of reference in nominal
tronic). Inference for the cognitive model was exact, while we                                                                            referring expressions. In an interactive reference game task in
used Markov Chain Monte Carlo (MCMC) to infer posteriors                                                                                  which speakers freely produced referring expressions, utter-
for the three free parameters.                                                                                                            ance choice was affected by utterance cost (in terms of length
   Point-wise maximum a posteriori (MAP) estimates of the                                                                                 and frequency), contextual informativeness (as manipulated
model’s posterior predictives at the target level (collapsing                                                                             via distractor objects), and object typicality. The interplay of
across distractors for each target, within each condition) are                                                                            these factors is naturally modeled within the RSA framework,
compared to empirical data in Fig. 5. On the by-target level                                                                              where speakers are treated as choosing utterances by soft-
the model achieves a correlation of r = .79. Looking at re-                                                                               maximizing utterance utility, which includes terms for infor-
sults on the by-domain level (collapsing across targets) and                                                                              mativeness and cost. In previous formulations of RSA mod-
on the by-condition level (further collapsing across domains,                                                                             els, informativeness was determined by a deterministic se-
as in Fig. 3) yields correlations of .88 and .96, respectively.                                                                           mantics; here we “softened” the semantics by allowing nouns
The model does a good job of capturing the quantitative pat-                                                                              to apply to objects to the extent that those objects were rated
terns in the data, especially considering the sparsity of our                                                                             as typical for the nouns. The resulting model provided a good
data at the by-target level. One clear flaw is that the model                                                                             fit to speakers’ empirical utterance choices, both qualitatively
predicts greater use of the super level label than people ex-                                                                             and quantitatively.
hibit. Further systematic deviation appears likely for specific                                                                               The model predicts a well-documented preference for
                                                                                                                                       2265

speakers to refer to objects at the basic level when not con-           B. Scassellati (Eds.), Proceedings of the 36th annual con-
strained by contextual considerations (Rosch et al., 1976). In          ference of the cognitive science society.
our model, this preference emerges naturally from cost con-           Gatt, A., Krahmer, E., van Deemter, K., & van Gompel, R. P.
siderations: basic-level labels tend to be shorter and more             (2014). Models and empirical data for the production of
frequent than sub and super level terms. However, speak-                referring expressions. Language, Cognition and Neuro-
ers did not always use the basic level term, even when un-              science, 29(8), 899–911.
constrained by context. In certain cases where object typ-            Goodman, N. D., & Stuhlmüller, A. (2013). Knowledge
icality was relatively high for the sub level term compared             and implicature: modeling language understanding as so-
to the basic level term, that term was preferred (as was the            cial cognition. Topics in Cognitive Science, 5(1), 173–84.
case for “panda bear”), suggesting an interesting interplay be-       Goodman, N. D., & Stuhlmüller, A. (electronic). The de-
tween typicality and level of description. While our results            sign and implementation of probabilistic programming lan-
show that a model can capture several basic-level phenomena             guages. Retrieved 2015/1/16, from http://dippl.org
through frequency, length, and typicality features, it leaves         Grice, H. P. (1975). Logic and Conversation. Syntax and
open the origin and causal role of these linguistic regularities.       Semantics, 3, 41–58.
Future research will be needed to determine how linguistic            Hawkins, R. X. D. (2015). Conducting real-time multi-
regularities are related to conceptual regularities and why.            player experiments on the web. Behavior Research Meth-
   An interesting analogy can be drawn from choosing a noun             ods, 47(4), 966-976.
to choosing a set of adjectives; that is, between selection of        Herrmann, T., & Deutsch, W. (1976). Psychologie der Ob-
a level of reference in simple nominal referring expressions            jektbenennung. Huber.
and selection of a set of features to include in modified re-         Kao, J., Wu, J., Bergen, L., & Goodman, N. D. (2014). Non-
ferring expressions. For the latter, a much discussed phe-              literal understanding of number words. Proceedings of the
nomenon is that of overinformative modifier use (Gatt, Krah-            National Academy of Sciences of the United States of Amer-
mer, van Deemter, & van Gompel, 2014)—for example, say-                 ica, 111(33), 12002–12007.
ing “big blue” when all objects in the context are blue. The          Koolen, R., Gatt, A., Goudbeek, M., & Krahmer, E. (2011).
preference for the basic level in the super sufficient condition        Factors causing overspecification in definite descriptions.
and the still substantial use of sub level terms in the basic           Journal of Pragmatics, 43(13), 3231–3250.
sufficient condition can also be considered overinformative.          Orita, N., Vornov, E., Feldman, N., & Daumé III, H. (2015).
However, we showed that a Rational Speech-Acts model us-                Why discourse affects speakers’ choice of referring expres-
ing non-deterministic semantics, derived from typicality esti-          sions. Proceedings of the 53rd Annual Meeting of the Asso-
mates, predicts that speakers should use these more specific            ciation for Computational Linguistics and the 7th Interna-
descriptions. The extent to which similar considerations may            tional Joint Conference on Natural Language Processing
apply to modified referring expressions should be explored.             (Volume 1: Long Papers), 1639–1649.
Future research should also examine the interaction of these          Pechmann, T. (1989). Incremental speech production and
choices: circumstances under which speakers choose a mod-               referential overspecification. Linguistics, 27(1), 89–110.
ifier and how nominal and modifier choice interact.                   Rohde, H., Seyfarth, S., Clark, B., Jäger, G., & Kaufmann,
                                                                        S. (2012). Communicating with Cost-based Implicature: a
                     Acknowledgments                                    Game-Theoretic Approach to Ambiguity. In Proceedings
This work was supported by ONR grant N00014-13-1-0788 and a
                                                                        of the 16th workshop on the semantics and pragmatics of
James S. McDonnell Foundation Scholar Award to NDG and an SNF           dialogue (pp. 107 – 116).
Early Postdoc. Mobility Award to JD. RXDH was supported by the        Rosch, E., Mervis, C. B., Gray, W. D., Johnson, D. M., &
Stanford Graduate Fellowship and the National Science Foundation        Boyes-Braem, P. (1976). Basic objects in natural cate-
Graduate Research Fellowship under Grant No. DGE-114747.
                                                                        gories. Cognitive psychology, 8(3), 382–439.
                          References                                  Sedivy, J. C. (2003, jan). Pragmatic versus form-based ac-
                                                                        counts of referential contrast: evidence for effects of in-
Brennan, S. E., & Clark, H. H. (1996, nov). Conceptual pacts            formativity expectations. Journal of psycholinguistic re-
   and lexical choice in conversation. Journal of experimental          search, 32(1), 3–23.
   psychology. Learning, memory, and cognition, 22(6), 1482           van Deemter, K., Gatt, A., van Gompel, R. P. G., & Krahmer,
   – 1493.                                                              E. (2012, apr). Toward a computational psycholinguistics
Degen, J., Franke, M., & Jäger, G. (2013). Cost-Based Prag-            of reference production. Topics in cognitive science, 4(2),
   matic Inference about Referential Expressions. In Proceed-           166–83.
   ings of the 35th annual conference of the cognitive science
   society.
Frank, M. C., & Goodman, N. D. (2012). Predicting prag-
   matic reasoning in language games. Science, 336, 998.
Franke, M. (2014). Typical use of quantifiers: A probabilis-
   tic speaker model. In P. Bello, M. Guarini, M. McShane, &
                                                                  2266

