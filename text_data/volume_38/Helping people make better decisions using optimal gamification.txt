                 Helping people make better decisions using optimal gamification
                                               Falk Lieder (falk.lieder@berkeley.edu)
                          Helen Wills Neuroscience Institute, University of California at Berkeley, CA, USA
                                        Thomas L. Griffiths (tom griffiths@berkeley.edu)
                               Department of Psychology, University of California at Berkeley, CA, USA
                               Abstract                                ple achieve their goals and improve themselves (McGonigal,
                                                                       2015; Kamb, 2016; Henry, 2014).
   Game elements like points and levels are a popular tool to
   nudge and engage students and customers. Yet, no theory can            While gamification can have positive effects on motivation,
   tell us which incentive structures work and how to design them.     engagement, behavior, and learning outcomes (Hamari et al.,
   Here we connect the practice of gamification to the theory of       2014), it can also have unintended negative consequences
   reward shaping in reinforcement learning. We leverage this
   connection to develop a method for designing effective incen-       (Callan, Bauer, & Landers, 2015; Devers & Gurung, 2015).
   tive structures and delineating when gamification will succeed      Unfortunately, it is currently impossible to predict whether
   from when it will fail. We evaluate our method in two behav-        gamification will succeed or fail (Hamari et al., 2014; Devers
   ioral experiments. The results of the first experiment demon-
   strate that incentive structures designed by our method help        & Gurung, 2015), and there is no principled way to deter-
   people make better, less short-sighted decisions and avoid the      mine exactly how many points should be awarded for a given
   pitfalls of less principled approaches. The results of the sec-     action. Here we address these problems by connecting the
   ond experiment illustrate that such incentive structures can be
   effectively implemented using game elements like points and         practice of gamification to the theory of pseudo-rewards in
   badges. These results suggest that our method provides a prin-      reinforcement learning. We leverage this connection to of-
   cipled way to leverage gamification to help people make better      fer a mathematical framework for gamification and a com-
   decisions.
                                                                       putational method for designing optimal incentive structures.
   Keywords: Gamification; Decision-Making; Bounded Ratio-             Our method offloads the computations of long-term planning
   nality; Reinforcement Learning; Decision-Support
                                                                       from people by building the optimal decision into the incen-
                                                                       tive structure so that foresight is no longer necessary. This
                           Introduction                                helps people make better decisions that are less short-sighted.
Most decisions have both immediate and delayed conse-                     The plan for this paper is as follows: We start by introduc-
quences. For instance, investing in a pension plan entails less        ing the theory of pseudo-rewards from reinforcement learn-
fun today than buying a wide-screen TV but a higher standard           ing. We then apply this theory to derive a method for design-
of living 25 years later. Unfortunately, the immediate out-            ing optimal incentive structures. Finally, we test the effective-
comes often dominate people’s considerations because future            ness of our method in two behavioral experiments. We close
outcomes are discounted disproportionately (Berns, Laibson,            with implications for decision support and gamification.
& Loewenstein, 2007). One of the reasons for this myopia is
that optimal long-term planning is often intractable because
the number of possible scenarios grows exponentially as you                              Formalizing Gamification
look ahead further. Consequently, people have to rely on falli-
ble heuristics to limit the length and number of scenarios they        Each sequential decision problem can be modeled as a
consider (Huys et al., 2015). While these heuristics can make          Markov Decision Process (MDP)
us short-sighted in some situations (Huys et al., 2012), they
work very well in others (Gigerenzer, 2008). Thus, perhaps,                                    M = (S , A , T, γ, r, P0 ) ,           (1)
it is not our heuristics that are broken but the decision envi-
ronments in which they fail (Gigerenzer, 2008; McGonigal,              where S is the set of states, A is the set of actions, T (s, a, s0 )
2011).                                                                 is the probability that the agent will transition from state s to
   The myopic nature of human decision-making (Berns et                state s0 if it takes action a, 0 ≤ γ ≤ 1 is the discount factor,
al., 2007) suggests that decision environments can be repaired         r(s, a, s0 ) is the reward generated by this transition, and P0 is
by aligning each action’s immediate rewards with the value of          the probability distribution of the initial state S0 (Sutton &
its long-term consequences. This could be achieved through             Barto, 1998). A policy π : S 7→ A specifies which action to
gamification (Deterding, Dixon, Khaled, & Nacke, 2011).                take in each of the states. The expected sum of discounted
Gamification is the use of game elements such as points, lev-          rewards that a policy π will generate in the MDP M starting
els, and badges in non-game contexts like education, the work          from a state s is known as its value function
place, health, and business. This approach has become ex-
tremely popular in the past five years. It is now widely used to                                  "                             #
                                                                                                    ∞
engage people and nudge their decisions (Hamari, Koivisto,                            VMπ (s) = E       t
& Sarsa, 2014), and it has also inspired tools that help peo-
                                                                                                    ∑ γ · r (St , π(St ), St+1 )  .   (2)
                                                                                                   t=0
                                                                   2075

The optimal policy π?M maximizes the expected sum of dis-                      1. Model the decision environment as a MDP.
counted rewards, that is
                                     "                              #          2. Define a potential function Φ that specifies the value of
                                       ∞                                           each state of the MDP.
             π?M = arg max E           ∑ γt · r (St , π(St ), St+1 )  ,  (3)
                             π
                                      t=0                                      3. Assign points according to Equation 6.
and its value function satisfies the Bellman equation                           This method may be useful to avoid some of the dark sides of
           VM? (st ) = max E [r (st , a, St+1 ) + γ ·VM? (St+1 )].       (4)    gamification (Callan et al., 2015; Devers & Gurung, 2015).
                            a                                                   To make this proposal more concrete and actionable, the next
We can therefore rewrite the optimal policy as                                  section presents a method for designing good potential func-
                                                                                tions.
         π?M (s) = arg max E [r(st , a, St+1 ) + γ ·VM? (St+1 )] ,       (5)
                              a
                                                                                      Designing Optimal Incentive Structures
which reveals that it is myopic with respect to the sum of the
                                                                                While the shaping theorem constrains pseudo-rewards to be
immediate reward and the discounted value of the next state.
                                                                                potential-based there are infinitely many potential functions
    Here, we leverage the MDP framework to model game el-
                                                                                that one could choose. Given that people’s cognitive limita-
ements like points and badges as pseudo-rewards f (s, a, s0 )
                                                                                tions prevent them from fully incorporating distant rewards
that are added to the reward function r(s, a, s0 ) of a de-
                                                                                (Huys et al., 2012; Berns et al., 2007), the modified reward
cision environment M to create a modified environment
                                                                                structure r0 (s, a, s0 ) should be such that the best action yields
M 0 = (S , A , T, γ, r0 , P0 ) with a more benign reward function
                                                                                the highest immediate reward, that is
r0 (s, a, s0 ) = r(s, a, s0 ) + f (s, a, s0 ). From this perspective, the
problem with misaligned incentives is that they change the                                           π?M (s) = arg max r0 (s, a, s0 ).              (9)
optimal policy π?M of the original decision problem M into a                                                            a
different policy π?M0 that is optimal for the gamified environ-                 Here, we show that this can be achieved with our method by
ment M 0 but not for the original environment M. To avoid this                  setting the potential function Φ to the optimal value function
problem we have to ensure that each optimal policy of M 0 is                    VM? of the decision environment M, that is
also an optimal policy of M.
    Fortunately, research in reinforcement learning has identi-                                    Φ? (s) = VM? (s) = max VMπ (s).                 (10)
                                                                                                                              π
fied the necessary and sufficient conditions pseudo-rewards
have to satisfy to achieve this: according to the shaping the-                  First, note that the resulting pseudo-rewards are
orem (Ng, Harada, & Russell, 1999) adding pseudo-rewards
retains the optimal policies of any original MDP if and only                                       f (s, a, s0 ) = γ ·VM? (s0 ) −VM? (s),          (11)
if the pseudo reward function f is potential-based, that is if
                                                                                which leads to the modified reward function
there exists a potential function Φ : S 7→ R such that
                        f (s, a, s0 ) = γ · Φ(s0 ) − Φ(s),               (6)              r0 (s, a, s0 ) = r(s, a, s0 ) + γ ·VM? (s0 ) −VM? (s).   (12)
for all states s, actions a, and successor states s0 .                          Hence, if the agent was myopic its policy would be
    Pseudo-rewards can be shifted and scaled without chang-
                                                                                     π(s) = arg max E r(s, a, s0 ) + γ · VM? (s0 ) − VM? (s)
                                                                                                                                                
ing the optimal policy, because linear transformations of                                            a
potential-based pseudo-rewards are also potential-based:                                                                                           (13)
                                                                                          = arg max E r(s, a, s0 ) + γ · VM? (s0 ) .
                                                                                                                                       
                                                                                                     a
            a · f (s, a, s0 ) + b = γ · Φ0 (s0 ) − Φ0 (s),               (7)
                                                                   b            According to Equation 5, this is the optimal policy π?M for
                                          0
                                   for Φ (s) = a · Φ(s) −              . (8)    the original decision environment M. Thus, people would
                                                                1−γ
                                                                                act optimally even if they were completely myopic. And
Shifting all pseudo-rewards f (s, a, s0 ) by the rewards r(s, a, s0 )           they should perform equally well, if they do optimal long-
also retains the optimal policy, because it is equivalent to mul-               term planning to fully exploit the gamified environment M 0 or
tiplying all rewards by 2 and positive linear transformations                   learn its optimal policy π?M0 through trial-and-error, because
of the rewards preserve the optimal policy (Ng et al., 1999).                   the shaping theorem (Eq. 6) guarantees that the gamified en-
    If gamification is to help people achieve their goals, then                 vironment M 0 has the same optimal policy, that is π?M0 = π?M .
the pseudo-rewards added in the form of points or badges                           This suggests that potential-based pseudo-rewards derived
must not divert people from the best course of action but make                  from VM? should allow even the most short-sighted agent who
its path easier to follow. Otherwise gamification would lead                    considers only the immediate reward to perform optimally.
people astray instead of guiding them to their goals. Hence,                    In this sense, the pseudo-rewards defined in Equation 11 can
the practical significance of the shaping theorem is that it                    be considered optimal. In addition, optimal pseudo-rewards
gives the architects of incentive structures a method to rule                   accelerate learning when the agent’s initial estimate of the
out incentivizing counter-productive behaviors:                                 value function is close to 0 (Ng et al., 1999).
                                                                            2076

   Computing the optimal pseudo-rewards requires perfect
knowledge of the decision environment and the decision-
maker’s preferences. This information may be unavailable
in practice. Yet, even when the optimal value function VM?
cannot be computed, it is often possible to approximate it. If
so, the approximate value function V̂M can be used to approx-
imate the optimal pseudo-rewards (Eq. 11) by
                   fˆ(s, a, s0 ) = γ · V̂M (s0 ) − V̂M (s).     (14)
For instance, you can estimate the value of a state s from its
approximate distance to the goal s? :
                                         distance(s, s? )
                                                           
                        ?
       V̂M (s) = V̂M (s ) · 1 −                               , (15)
                                     maxs distance(s, s? )               Figure 1: Interface of the control condition of Experiment 1.
                                                                         The map shows the unmodified rewards r.
where V̂M (s? ) is the estimated value of achieving the goal.
Based on previous simulations (Ng et al., 1999), we predict               Pseudo-Rewards       Smiths-    Jones- Williams- Browns-  Clarks- Bakers-
that approximate pseudo-rewards (Eq. 14) can have beneficial              None                140    30 −30 −70 −30 −70    −30 30  −30 −70 −30 −70
                                                                          Optimal               2 −76      2 −5 −12      2  −4 2     2     0 2 −42
effects similar to those of optimal pseudo-rewards but weaker.            Approximate           8 −102 −22 −4 −22 −4        36 38  −34 −16 24 −32
                                                                          Non-Potential-Based 119      9 −51 −41 −51 −41    −1 9   −51 41 −1       9
We tested these predictions in two behavioral experiments.
                                                                         Table 1: Rewards in Experiment 1. The first entry of each cell
          Experiment 1: Modifying Rewards
                                                                         is the (modified) reward of the counter-clockwise move and
Methods                                                                  the second one is the (modified) reward of the other move.
We recruited 250 adult participants on Amazon Mechanical
Turk. Participants were paid $0.50 for playing the game
shown in Figure 1. In this game, the player receives points              (Figure 1). In this condition finding the optimal path required
for routing an airplane along profitable routes between six              planning 4 steps ahead. In the three experimental condi-
cities. In each of the 24 trials the initial location of the air-        tions the rewards were modified by adding pseudo-rewards.
plane was chosen uniformly at random, and the task was to                To keep the average reward constant, the pseudo-rewards
earn as many points as possible. Participants were incen-                were mean-centered by subtracting their average; since mean-
tivized by a performance dependent bonus of up to $2. This               centering is a linear transformation this retained the guaran-
game is based on the planning task developed by Huys et al.              tees of the shaping theorem (see Eq. 7). Next, the mean-
(2012). Our version of this task is isomorphic to a MDP with             centered pseudo-rewards were added to the rewards of the
six states, two actions, deterministic transitions, and a dis-           control condition (see Figure 1) yielding the modified rewards
count factor of γ = 1 − 1/6. The locations correspond to the             shown in Table 1, and the flight map was updated accordingly.
states of the MDP, the two actions correspond to flying to the           In all other respects the interfaces of the experimental condi-
first or the second destination available from the current loca-         tions were exactly the same as the interface of the control
tion, the routes correspond to state-transitions, and the points         condition (see Figure 1). All participants were thus unaware
participants received for flying those routes are the rewards.           of the existence of pseudo-rewards. In the first experimental
The current state was indicated by the position of the aircraft          condition the pseudo-rewards were derived from the optimal
and was updated according to the flight chosen by the par-               value function according to the shaping theorem (Eq. 11). In
ticipant. The number of points collected in the current trial            this condition, looking only 1 step ahead was sufficient to find
was shown in the upper right corner of the screen. After each            the optimal path. The second experimental condition used
choice there was a 1 in 6 chance that the game would end                 the approximate potential-based pseudo-rewards defined in
and the experiment would advance to the next trial, and a 5              Equation 14 with the distance-based heuristic value function
in 6 chance that the participant could choose another flight.            defined in Equation 15 where s? was Smithsville, Φ(s? ) was
The participants were instructed to score as high as possible,           its highest immediate reward (i.e., +140), and distance(a, b)
and their financial bonus was proportional to the rank of their          was the minimum number of actions needed to get from state
score relative to the scores of all participants in the same con-        a to state b. The resulting pseudo-rewards simplified plan-
dition. The optimal policy in this MDP is to move counter-               ning but not as much as the optimal pseudo-rewards. Find-
clockwise around the circle in all states except Williamsville           ing the optimal path required planning 2-3 steps ahead and
and Brownsville (see Figure 1). Importantly, at Williamsville            the immediate losses were smaller. In the third experimental
the optimal policy incurs a large immediate loss, and no other           condition, the pseudo-rewards violated the shaping theorem:
policy achieves a positive reward rate.                                  the pseudo-reward was +50 for each transition that reduced
   Participants were randomly assigned to one of four con-               the distance to the most valuable state (i.e. Smithsville) but
ditions: In the control group, there were no pseudo-rewards              there was no penalty for moving away from it.
                                                                     2077

 Figure 2: Performance and reaction times in Experiment 1.
Results and Discussion
The average completion time of the experiment was 13:37             Figure 3: Choice frequencies in each state of Experiment 1
min. The median response time was 1.3 sec. per choice. We           by condition. Error bars enclose 95% confidence intervals.
excluded 3 participants who invested less than one third of the
median response time of their condition and 11 participants            Next, we inspected how the pseudo-rewards affected the
who scored lower than 95% of all participants in their con-         choices our participants made in each of the six states (see
dition. The boxplots in Figure 2 summarize the median per-          Figure 3). The optimal pseudo-rewards significantly changed
formance and reaction times of participants in the four condi-      the choice frequencies in each of the six states and suc-
tions. The median performer of the control group lost 18.75         cessfully nudged participants to follow the optimal cycle
points per trial. By contrast, the majority of the group with       Smithsville → Jonesville → Williamsville → Bakersville →
optimal pseudo-rewards achieved a net gain in the unmod-            Smithsville (see Figure 1). Their strongest effect was to elim-
ified MDP (median performance: +5.00 points/trial). The             inate the problem that most people would avoid the large loss
median performance in the group with approximate potential-         associated with the correct move from Williamsville to Bak-
based pseudo-rewards was −5.00 points per trial, and in the         ersville (χ2 (2) = 1393.8, p < 10−15 ). The optimal pseudo-
group with non-potential-based pseudo-rewards the median            rewards also increased the frequency of all other correct
performance was −21.25 points per trial. A Kruskal-Wallis           choices along the optimal cycle, that is the decisions to fly
ANOVA revealed that the type of pseudo-rewards added to             from Bakersville to Smithsville (χ2 (2) = 326.5, p < 10−15 ),
the reward function significantly affected people’s perfor-         from Smithsville to Jonesville (χ2 (2) = 7.9, p = 0.0191), and
mance in the original MDP (H(3) = 40.35, p < 10−8 ) as              from Jonesville to Williamsville (χ2 (2) = 299.8, p < 10−15 ).
well as their reaction times (H(3) = 29.96, p < 10−5 ). Given       In addition, the optimal pseudo-rewards increased the fre-
that the pseudo-reward type had a significant effect, we per-       quency of the correct move from Clarksville to Bakersville
formed pairwise Wilcoxon rank sum tests to compare the              (χ2 (2) = 92.0, p < 10−15 ). The only negative effect of the op-
medians of the four conditions. The non-potential-based             timal pseudo-rewards was to slightly increase the frequency
pseudo-rewards failed to improve people’s performance (Z =          of the suboptimal move from Brownsville to Clarksville
0.72, p = 0.47). By contrast, the approximate potential-            (χ2 (2) = 13.2, p = 0.0013). By contrast, the non-potential-
based pseudo-rewards succeeded to improve their perfor-             based pseudo-rewards misled our participants to follow the
mance (Z = 2.86, p = 0.0042) and led to better performance          unprofitable cycle Jonesville → Clarksville → Smithsville
than the heuristic pseudo-rewards that violated the shaping         → Jonesville by raising the frequency of the reckless moves
theorem (Z = 3.61, p = 0.0003). People performed even               from Jonesville to Clarksville (χ2 (2) = 1578.6, p < 10−15 )
better when gamification was based on the optimal pseudo-           and from Clarksville to Smithsville (χ2 (2) = 813.7, p <
rewards: adding optimal pseudo-rewards led to better deci-          10−15 ). The effect of the approximate pseudo-rewards was
sions than adding the approximate potential-based pseudo-           beneficial in Smithsville, Williamsville, and Bakersville, but
rewards (Z = 2.68, p = 0.0074), presenting the true reward          negative in Jonesville, Brownsville, and Clarksville (see Fig-
structure (Z = 4.76, p < 10−5 ), or adding the non-potential-       ure 3). This explains why only potential-based pseudo-
based pseudo-rewards (Z = 5.34, p < 10−7 ).                         rewards had a positive net-effect on performance (Figure 2).
   In addition, some pseudo-rewards accelerated the deci-              Finally, we investigated learning effects by comparing the
sion process (Figure 2): optimal pseudo-rewards decreased           average choice frequencies in the first five trials versus the last
the median response time from 1.72 to 1.14 sec/decision             five trials. While people’s decisions improved with learning
(Z = −4.19, p < 0.0001), and non-potential-based pseudo-            in the conditions with potential-based pseudo-rewards, learn-
rewards decreased it to 1.12 sec/decision (Z = −3.38, p =           ing had a negative effect when the pseudo-rewards violated
0.0007). But approximate potential-based pseudo-rewards             the shaping theorem: In the condition with non-potential-
had no significant effect on response time (1.65 sec/decision;      based pseudo-rewards learning reduced the frequency of the
Z = −0.28, p = 0.78).                                               correct choice in Jonesville (χ2 (2) = 9.22, p = 0.01). By con-
                                                                2078

trast, in the condition with optimal pseudo-rewards learning
improved people’s choices in Smithsville (χ2 (2) = 13.02, p =
0.0015), and in the condition with approximate potential-
based pseudo-rewards learning improved people’s choices in
Jonesville (χ2 (2) = 11.44, p = 0.0033). In the control con-
dition, learning made people more likely to take the correct
action in Williamsville (χ2 (2) = 24.16, p < 0.0001) and Bak-
ersville (χ2 (2) = 22.74, p < 0.0001) but less likely to take
the correct action in Clarksville (χ2 (2) = 8.80, p = 0.0123).
In summary, while learning with potential-based pseudo-
rewards guided people closer towards the optimal policy,
non-potential-based pseudo-rewards lured them away from it.
This is consistent with the shaping theorem’s assertion that
pseudo-rewards have to be potential-based to always retain
the optimal policy.
   In summary, we found that pseudo-rewards can help peo-
ple make better decisions–but only when they are designed
                                                                                Figure 4: Screenshot of Experiment 2.
well. The results support the proposed method for design-
ing incentive structures: Assigning pseudo-rewards accord-
ing to the shaping theorem avoided the negative effects of         the instructions stated that the stars were designed to help the
non-potential-based pseudo-rewards. Furthermore, using the         pilots make better decisions and explained their meaning. In
optimal value function as the shaping potential lead to the        addition, the instructions included the tip that the better flight
greatest improvement in decision-quality.                          is the one with the higher sum of stars plus dollars (condition
                                                                   3) or that it is the one awarded more stars (condition 4). The
      Experiment 2: Explicit Pseudo-Rewards                        stars had no monetary value, but they determined whether
To assess the potential of gamification for decision-support       and when the character played by the participant would be
in real life, Experiment 2 conveyed pseudo-rewards by points       promoted. The character could rise from Trainee to ATP se-
without monetary value.                                            nior captain via 15 intermediate levels. The number of points
                                                                   required to reach the next level increased according to the
Methods                                                            difficulty curve proposed by Bostan and Öğüt (2009). Partic-
We recruited 400 participants on Amazon Mechanical Turk.           ipants were told how many stars and dollars were required to
Participants were paid $2.50 for about 20 min of work plus         reach the next level (see Figure 4). The current score and the
a performance dependent bonus of up to $2 that averaged at         shoulder batch corresponding to the current level were shown
$1. We inspected the data from the 355 participants who had        at the top of the screen, and a feedback message appeared
not participated in earlier versions of the experiment and ex-     whenever the character was promoted. The player started the
cluded 19 participants who responded in less than one third        game with +$50 so that their balance would remain positive
of the median response time of all participants or performed       as they learned to play the game. In all conditions a quiz en-
worse than 95% of the participants in their condition.             sured the participants understood the task and its incentives
   The task from Experiment 1 was modified by adding stars         before they could start the game. This quiz comprised three
in two new experimental conditions (see Figure 4). To make         questions on how the participant’s financial bonus would be
it easy for participants to add rewards plus pseudo-rewards,       determined and three questions testing they understood the
the rewards were scaled down by a factor of 10 and the opti-       mechanics of the task.
mal pseudo-rewards were recomputed according to Equation
11. The pseudo-rewards awarded for each action were then           Results and Discussion
shifted by the expected return of the optimal policy ($0.90)       Overall, presenting pseudo-rewards in one of the three for-
so that they predict how much money the player will earn in        mats significantly improved people’s performance (Z = 3.43,
the long-run if they choose that action and act optimally af-      p = 0.0006). Most importantly, adding points (i.e., stars)
terwards. The experiment had four conditions: In the control       without monetary value can be just as effective as directly
condition no pseudo-rewards were presented. Three exper-           modifying the reward structure of the environment: Inte-
imental conditions presented the optimal pseudo-rewards in         grated pseudo-rewards significantly increased people’s per-
three different formats: condition 2 embedded the pseudo-          formance from −0.73 dollars/trial to +0.17 dollars/trial (Z =
rewards into the reward structure as in Experiment 1; con-         3.69, p = 0.0002). The resulting level of performance was
dition 3 presented them separately in the form of stars; and       not significantly different from the performance in the condi-
in condition 4 the number of stars communicated the sum            tion with embedded pseudo-rewards (+0.42 dollars/trial, Z =
of pseudo-reward plus immediate reward. All numbers were           0.52, p = 0.62). By contrast, presenting pseudo-rewards sep-
rounded to one significant digit. In the conditions with stars     arately failed to significantly increase people’s performance
                                                               2079

                                                                     proving inter-temporal choice. Our findings are consistent
                                                                     with the view that the limitations of human decision-making
                                                                     can be overcome by reshaping incentive structures that make
                                                                     us prone to fail into ones that our heuristics were designed for.
                                                                     Our method achieves this by solving people’s planning prob-
                                                                     lems for them and restructuring their incentives accordingly.
                                                                     The program providing the pseudo-rewards can be seen as a
                                                                     cognitive prosthesis because it compensates for people’s cog-
                                                                     nitive limitations without restricting their freedom. In con-
                                                                     clusion, optimal gamification may provide a principled way
                                                                     to help people achieve their goals and procrastinate less.
                                                                     Acknowledgments. This work was supported by grant number
                                                                     ONR MURI N00014-13-1-0341. We thank Rika Antonova, Ellie
                                                                     Kon, Paul Krueger, Mike Pacer, Daniel Reichman, Stuart Russell,
Figure 5: Choice Frequencies in Experiment 2: Effects of             Jordan Suchow, and the CoCoSci lab for feedback and discussions.
stars and badges on performance
                                                                                                 References
                                                                     Berns, G. S., Laibson, D., & Loewenstein, G. (2007). Intertempo-
(median performance: −0.5 dollars/trial; Z = 0.22, p = 0.83).         ral choice–toward an integrative framework. Trends in Cognitive
   Inspecting the choice frequencies (Figure 5) confirmed             Sciences, 11(11), 482–488.
that the three presentation formats had significantly different      Bostan, B., & Öğüt, S. (2009). Game challenges and difficulty lev-
effects: Embedded pseudo-rewards and integrated pseudo-               els: lessons learned from RPGs. In G. K. Yeo & Y. Cai (Eds.),
                                                                      Learn to Game, Game to Learn: Proceedings of the 40th Confer-
rewards were more beneficial than separately presented                ence of the International Simulation and Gaming Association.
pseudo-rewards in all 6 states (all p ≤ 0.0218). Embedded            Callan, R. C., Bauer, K. N., & Landers, R. N. (2015). How to avoid
pseudo-rewards were more beneficial than integrated pseudo-           the dark side of gamification: Ten business scenarios and their un-
                                                                      intended consequences. In T. Reiners & L. C. Wood (Eds.), Gami-
rewards in Jonesville and Clarksville (both p ≤ 0.0001), but          fication in education and business (pp. 553–568). Cham: Springer.
integrated pseudo-rewards were more beneficial than embed-           Deterding, S., Dixon, D., Khaled, R., & Nacke, L. (2011). From
ded pseudo-rewards in Brownsville (p < 10−9 ). Furthermore,           game design elements to gamefulness: defining gamification. In
                                                                      Proceedings of the 15th International Academic MindTrek Confer-
participants were significantly faster when pseudo-rewards            ence: Envisioning Future Media Environments (pp. 9–15).
were embedded in the decision environment than when they             Devers, C. J., & Gurung, R. A. (2015). Critical perspective on gam-
were presented separately (Z = −4.06, p < 0.0001) or in the           ification in education. In T. Reiners & L. C. Wood (Eds.), Gamifi-
                                                                      cation in education and business (pp. 417–430). Cham: Springer.
integrated format (Z = −2.78, p = 0.0053).                           Gigerenzer, G. (2008). Rationality for mortals: How people cope
   In conclusion, adding points that convey the sum of opti-          with uncertainty. Oxford: Oxford University Press.
mal pseudo-rewards plus immediate reward can be as effec-            Hamari, J., Koivisto, J., & Sarsa, H. (2014). Does gamification
                                                                      work?–A literature review of empirical studies on gamification. In
tive as changing the reward structure itself.                         Proceedings of the 47th Hawaii International Conference on Sys-
                                                                      tem Sciences (pp. 3025–3034).
                          Conclusion                                 Henry, A. (2014). The best tools to (productively) gamify every
                                                                      aspect of your life. Retrieved from http://lifehacker.com/
We have proposed a general method for improving incentive             the-best-tools-to-productively-gamify-every-aspect
structures based on the theory of MDPs and the shaping theo-          -of-1531404316
                                                                     Huys, Q. J. M., Eshel, N., O’Nions, E., Sheridan, L., Dayan, P.,
rem. Its basic idea is to offload the computation necessary for       & Roiser, J. P. (2012). Bonsai trees in your head: how the pavlo-
long-term planning into the reward structure of the environ-          vian system sculpts goal-directed choices by pruning decision trees.
ment such that people will act optimally even when they con-          PLOS Computational Biology, 8(3), e1002410.
                                                                     Huys, Q. J. M., Lally, N., Faulkner, P., Eshel, N., Seifritz, E., Ger-
sider only immediate rewards. The results of Experiment 1             shman, S. J., . . . Roiser, J. P. (2015). Interplay of approximate
provide a proof of principle that our approach can help people        planning strategies. Proceedings of the National Academy of Sci-
make better sequential decisions. Our findings suggest that           ences, 112(10), 3098–3103.
                                                                     Kamb, S. (2016). Level up your life: How to unlock adventure
the shaping theorem can be used to delineate when gamifica-           and happiness by becoming the hero of your own story. Emmaus:
tion will succeed from when it will fail and to design incentive      Rodale Books.
structures that avoid the perils of less-principled approaches       McGonigal, J. (2011). Reality is broken: Why games make us better
                                                                      and how they can change the world. New York: Penguin.
to gamification. Experiment 2 illustrated that the incentive         McGonigal, J. (2015). SuperBetter: A revolutionary approach to
structures designed with our method can be implemented                getting stronger, happier, braver and more resilient–powered by the
with game elements like points and badges. In both ex-                science of games. London, UK: Penguin Press.
                                                                     Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under
periments the pseudo-rewards helped people overcome their             reward transformations: Theory and application to reward shaping.
short-sighted tendency to avoid an aversive action with desir-        In I. Bratko & S. Dzeroski (Eds.), Proceedings of the 16th Annual
able long-term consequences in favor of immediate reward–a            International Conference on Machine Learning (pp. 278–287). San
                                                                      Francisco: Morgan Kaufmann.
cognitive limitation that can manifest in procrastination and        Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An
impulsivity. Therefore, our method might be useful for im-            introduction. Cambridge, MA, USA: MIT press.
                                                                 2080

