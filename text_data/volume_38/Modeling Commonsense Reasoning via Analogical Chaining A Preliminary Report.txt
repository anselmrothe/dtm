 Modeling Commonsense Reasoning via Analogical Chaining: A Preliminary Report
                                        Joseph A. Blass (joeblass@u.northwestern.edu)
                                       Kenneth D. Forbus (forbus@northwestern.edu)
                                             Northwestern University, 2133 Sheridan Road,
                                                           Evanston, IL 60208
                             Abstract                                   continuous systems (e.g. Forbus & Gentner, 1997; Forbus
                                                                        2001). Here we explore how analogy might be used for
   Understanding the nature of commonsense reasoning is one of
   the deepest questions of cognitive science. Prior work has           commonsense more broadly. We have argued that much of
   proposed analogy as a mechanism for commonsense reasoning,           human abduction and prediction might be explained by
   with prior simulations focusing on reasoning about continuous        analogy over experiences and generalizations constructed
   behavior of physical systems. This paper examines how                from experience (Forbus, 2015). This paper explores in more
   analogy might be used in commonsense more broadly. The two           detail how that might work. Specifically, we propose that
   contributions are (1) the idea of common sense units,                multiple analogical retrievals are used to quickly elaborate a
   intermediate-sized collections of facts extracted from
   experience (including cultural experience) which improves
                                                                        situation, providing a set of plausible explanations and
   analogical retrieval and simplifies inferencing, and (2)             predictions. We call this process analogical chaining (AC).
   analogical chaining, where multiple rounds of analogical             The units that are retrieved might be specific situations or
   retrieval and mapping are used to rapidly construct                  larger structures, such as traditional scripts (e.g. Schank &
   explanations and predictions. We illustrate these ideas via an       Abelson, 1977) and frames (e.g. Minsky, 1974), if they are
   implemented computational model, tested on examples from             good matches for the situation. However, we also propose
   an independently-developed test of commonsense reasoning.            that experience is factored into Common Sense Units, cases
   Keywords: Analogical Reasoning; Commonsense Reasoning;               in the case-based reasoning sense, that are typically larger
   Analogical Abduction                                                 than single facts and smaller than frames or scripts1. A CSU
                                                                        consists of several facts that connect, for example, an event
                         Introduction                                   of a particular type with a precursor or with a potential
Consider three situations. (1) A person throws a crumpled-up            outcome. Such cases can be useful for prediction when the
piece of paper the size of an egg at another person’s head. (2)         precursor matches the current situation, and for explanation
is like (1), but the item thrown is an actual egg. (3) is like (1),     when the outcome matches the current situation (Forbus,
but the item is a small, white stone of the same size. Despite          2015). Because they are smaller, they should be more easily
these situations’ similarities, you would likely interpret the          transferrable to a wider range of situations, because they
first as a playful action, the second as emotionally aggressive         contain less non-overlapping information.
but perhaps not too harmful, and the third as a serious act of             We begin by reviewing the structure-mapping models that
aggression. These conclusions come quickly and easily to us,            this model is built upon, and the Cyc-derived ontology used.
without conscious pondering. Such extremely rapid                       We describe our model, including our hypotheses about the
construction of explanations and predictions is a hallmark of           nature of CSUs and the computational issues raised by AC.
commonsense reasoning.                                                  We present an experiment where a pool of CSUs are used to
   Several models for commonsense reasoning have been                   answer questions from the Choice of Plausible Alternatives
proposed, ranging from logical reasoning using general, first-          (COPA, Roemmele et al., 2011) test of commonsense
principles axioms (e.g. Davis, 1990, Lenat, 1995) to                    reasoning. We close with related and future work.
numerical simulation (e.g. Battaglia et al., 2013). We take
analogical reasoning as a promising approach for explaining                                     Background
commonsense reasoning, for three reasons. First, analogical             Analogy is an important tool for reasoning and decision-
reasoning can work with partial knowledge: we may not have              making; we use past experiences to understand and make
a fully articulated general theory of how much harm being hit           decisions in new situations (Markman & Medin 2002). We
by something might have, but if we have examples, we can                use Gentner’s (1983) structure-mapping theory of analogical
still work with those. Second, analogical generalization                reasoning, which argues that analogy involves finding an
provides a potential mechanism for learning probabilistic               alignment between two structured descriptions. The
generalizations to represent experience. Third, analogy can             Structure-Mapping Engine (SME, Forbus et al. 2016) is a
allow a system to generate multiple inferences by importing             computational model of analogy and similarity based on
whole relational structures from a single case, rather than             structure mapping theory2. SME takes in two structured,
requiring separate rules for each inference.                            relational cases (a base and a target) and computes up to three
   Our prior work on exploring analogy in commonsense                   mappings between them. These include the correspondence
reasoning focused on reasoning about the behavior of                    between the two cases, candidate inferences suggested by the
   1                                                                       2
      We are inspired by the Goldilocks Effect argument for                  See (Gentner & Forbus, 2011) for a survey of other models.
analogical reasoning (Finlayson & Winston, 2005).
                                                                    556

mapping, and a similarity score that serves as a measure of         perform a positive deed for the first at some future time.
how good the mapping is. If a candidate inference involves          CSUs are intended to be smaller than situations, hence
an entity not present in the other case, that entity is             making them more compositional. We have not yet explored
hypothesized as a skolem entity.                                    learning CSUs, because we first want to establish that they
   Running SME across every case in memory would be                 can be useful. The current paper provides evidence for this.
prohibitively expensive, and implausible for human-scale
memories. MAC/FAC (Forbus et al. 1995) retrieves cases                             Analogical Chaining (AC)
that may be helpful for analogical reasoning from a case               Many prior computational models of analogical reasoning
library, without relying on any indexing scheme. It takes in a      have treated analogy as a one-shot process, where a single
probe case like those used by SME, as well as a case library        analog is retrieved and used, or perhaps replaced with another
of other such cases. MAC/FAC efficiently generates                  if the first is not satisfactory. We go beyond that here by
remindings, which are SME mappings, for the probe case              chaining analogies, i.e. using the elaboration of a situation via
with the most similar case retrieved from the case library.         analogical inferences to retrieve yet more analogs, similar to
MAC/FAC proceeds in two stages: first, it computes dot              how chaining in logical inference works. This is conceptually
products between content vectors of the probe and each case         similar to Derivational Analogy (Veloso & Carbonell, 1993;
in the case library, a coarse approximation for scalability. Up     differences discussed below).
to the three most similar cases are passed to the second stage,        AC proceeds as follows. The case library of CSUs is a
which uses SME to calculate similarity based on structured          stand-in for some of the commonsense knowledge a human
descriptions. Typically only one, but up to three if close,         gains over their lifetime. The current situation (the target) is
retrievals are output by MAC/FAC.                                   used as a probe for MAC/FAC over the case library. If no
   We use the Cyc ontology (Lenat, 1995) as a source of             mapping is produced, the program seeks another reminding,
representations. The subset of contents of ResearchCyc that         without cases that were rejected or previously used. If a
we use for our knowledge base contains over 110,000                 mapping is found, any candidate inferences are asserted into
concepts and over 32,000 relations, constrained by over 3.8         an inference context, along with statements indicating what
million facts. We extend this knowledge base to support             category any skolems belong to. Inferences are placed in a
qualitative reasoning, analogical reasoning and learning, and       separate context from the case because there is no guarantee
additional lexical and semantic information. The knowledge          that they are correct. Another retrieval is then performed,
is partitioned into over 58,000 microtheories, which can be         with the probe being the union of the target and the inference
linked via inheritance relationships to form logical                context. If no information was added to the case, the
environments to support and control reasoning. The                  previously retrieved analog is suppressed, to prevent looping.
MiddleEarthMt or other microtheories representing fictional         When information is added to the inference context,
worlds, for example, are rarely useful in commonsense               previously rejected CSUs are freed up to be retrieved against
reasoning (although the converse is not true). Microtheories        in case they might build off the inferences made. The process
simplify the consideration of alternatives during reasoning.        repeats until an answer has been found (for a question-
   Using ResearchCyc representations allows us to leverage          answering task) or there are no more inferences to carry over
the several person-centuries of work that has gone into its         into the target case (Figure 1).
development, but also reduces the risk of tailorability. By            There are several potential advantages to this model. Cases
using natural language inputs and someone else’s                    can be dynamically added to the case library, and can be used
representations, we reduce the chance that our results are an       immediately. AC enables both inference about what is
effect of having spoon-fed answers to our systems.                  present in the case (filling in implicit relational links) as well
                                                                    as abductive explanations for what caused an event or
                  Common Sense Units                                predictions about what might happen next.
People are spontaneously reminded of similar prior                     The strongest advantage of analogical reasoning is that,
situations. We further hypothesize that experience, both            unlike logical inference, it does not require a fully articulated
direct and culturally transmitted (e.g., reading, watching          logically quantified theory. The difficulty in creating such
videos) is carved up into smaller pieces as well, and               theories is well-known, and seems to stem from two reasons.
combined via analogical generalization to create probabilistic      First, people have difficulty articulating a complete, accurate
structures (via SAGE, McLure et al. 2015). These lack logical       account of their reasoning. Second, their models tend to be
variables but can behave like rules when applied by analogy,        full of gaps and unintended consequences. By contrast,
and serve as grist for analogical reasoning about novel             reasoning by analogy from experience does not require a
situations. Because they include fewer statements they are          complete axiomatic theory of, for example, causality or
less specific (in the model theory sense), and more likely to       human actions. It only requires examples with explanations
match to a wide range of cases.                                     specific to those cases. Analogical reasoning moreover is
   We think of CSUs as the set of facts surrounding a               guided by what has happened, rather than what might be
particular common plausible inference. For example, a CSU           logically possible. To be sure, analogy can go awry as well –
for reciprocity might encode simply that one agent performs         no powerful reasoning system with imperfect information
a positive deed for another, which causes the other agent to        and finite resources can always guarantee valid results. AC
                                                                557

  Figure 1: Analogical Chaining Workflow for answering COPA questions
should provide a compression of the inference space, both in              We created 32 CSUs designed to be relevant to the topics
terms of the number of inferences completed per step and                of the questions, plus distractors. These CSUs ranged in size
fewer inappropriate branches explored, compared to logical              from 2 to 8 facts. COPA questions are designed so that both
chaining.                                                               answers are actually plausible, but one answer is always more
                                                                        plausible than the other. Consequently, CSUs that would
                          Simulation                                    contribute to incorrect answers were encoded as part of the
                                                                        set, as well as several CSUs irrelevant to answering the
Method                                                                  specific questions chosen (for example, that a system with a
To explore the plausibility of these ideas, we implemented              faulty component may malfunction). Sample CSUs are
AC using the Companion cognitive architecture (Forbus et al.            shown in Figure 3. Representations for 19 CSUs were almost
2009). For testing, we focused on a small subset of COPA                 When a loved one is hurt, you call an ambulance.
training set questions3, and automatically encoded the                   (loves caller6829 person6293)
                                                                         (senderOfInfo call22246 caller6829)
questions and the majority of the CSUs via natural language              (communicationTarget call22246 ambulance22371)
understanding capabilities built into the architecture. These            (isa ambulance22371 Ambulance)
                                                                         (isa call22246 MakingAPhoneCall)
include six questions involving the causes and consequences              (causes-PropProp
of situations involving violent impacts, and a seventh                    (and (objectHarmed someHarm1523 person6293)
                                                                                 (loves caller6829 person6293))
question involving boiling water. These questions are shown               (and (isa call22246 MakingAPhoneCall)
in Figure 2. This paper uses question 461 for illustration.                      (senderOfInfo call22246 caller6829)
                                                                                 (communicationTarget call22246 ambulance22371)
  214: The vandals threw a rock at the window.                                   (isa ambulance22371 Ambulance)))
  What happened as a RESULT?                                                                           *********
        The window [cracked / fogged up].                                Mothers love their sons (similar CSUs cover mothers & daughters)
  294: The egg splattered. What was the CAUSE of                         (isa mother22349 HumanMother)
                                                                         (sons mother22349 son26849)
  this?                                                                  (loves mother22349 son26849)
        I [dropped / boiled] it.                                         (causes-PropProp
  347: The boy got a black eye. What was the CAUSE                        (and (isa mother22349 HumanMother)
  of this?                                                                       (sons mother22349 son26849))
        The bully [mocked /punched] the boy.                              (loves mother22349 son26849))
  370: The water in the teapot started to boil.                                                        *********
  What happened as a RESULT?                                             Falling out of bed hurts.
        The teapot [cooled / whistled].                                  (isa bed2498 Bed-PieceOfFurniture)
                                                                         (isa fall24789 FallingEvent)
  390: The truck crashed into the motorcycle on                          (isa impact1953 ViolentImpact)
  the bridge. What happened as a RESULT?                                 (objectHarmed impact1953 person22386)
    [The motorcyclist died / The bridge collapsed].                      (primaryObjectMoving fall24789 person22386)
  461: The mother called an ambulance. What was                          (causes-PropProp
  the CAUSE of this?                                                      (and (isa fall24789 FallingEvent)
    Her son [lost his cat / fell out of his bed].                               (from-UnderspecifiedLocation bed2498 person22386)
  496: My ears were ringing. What was the CAUSE of                               (isa bed2498 Bed-PieceOfFurniture)
                                                                                 (primaryObjectMoving fall24789 person22386))
  this?                                                                   (and (isa impact1953 ViolentImpact)
        I went to a [museum / concert].                                          (objectHarmed impact1953 person22386)))
                                                                           Figure 3: CSUs required to solve COPA question 461.
     Figure 2: The Selected COPA Questions and Answers
   3
     We use training set questions here because publishing test set
questions would violate the security of the test.
                                                                    558

 (communicationTarget call22246 ambulance22371)                         For each question, the Companion read the question and
 (isa ambulance22371 Ambulance)                                      answers into separate microtheories. The system read and
 (isa call22246 MakingAPhoneCall)
 (loves caller6829 person6293)                                       understood the questions without human intervention. The
 (senderOfInfo call22246 caller6829)                                 Companion automatically filtered out the phrase asking for
 (causes-Underspecified love9172 call22246)
                         *********                                   cause or effect, since we found that for most COPA questions
 (causes-PropProp                                                    only one answer is plausible regardless of which is asked for.
   (and (objectHarmed someHarm1523 person6293)
        (loves caller6829 person6293))
                                                                     The Companion then used AC to flesh out the question,
   (and (communicationTarget call22246 ambulance22371)               storing the inferences in a separate microtheory.
        (isa ambulance22371 Ambulance)
        (isa call22246 MakingAPhoneCall)
                                                                        After each extension, the Companion would check whether
        (senderOfInfo call22246 caller6829)))                        it had reasoned its way to one of the answers, using SME.
                                                                     The base normalized score (i.e. the similarity of the
     Figure 4: Top: the NLU-output CSU about calling an
                                                                     base/target divided by the similarity of the base with itself)
      ambulance when a loved one is hurt. Bottom: the
                                                                     measures how much of the base is covered by the match.
    causal fact after manual editing (other facts the same)
                                                                     Here, an answer is used as the base and the union of the
entirely automatically generated by our NLU system, with             question and inferences microtheories are used as the target.
only causal representations manually edited (described next).        If the base normalized score of the comparison is above
The remaining 13 CSUs also began as automatically                    0.999, all the facts in the answer have identical (but for entity
processed natural language, but required more significant            tokens) corresponding facts in the reasoning microtheory,
manual changes, due to various limitations of the NLU                and the model selects that answer as correct.
system, mostly involving words unknown to the system.
   Causal representations automatically generated from               Results
natural language were modified by hand. The NLU system               Of the seven questions selected, a Companion using AC was
generates structurally flat causal representations, which are        able to answer six correctly. Most inferences generated
difficult for SME to operate over. For example, saying that          through chaining either helped the system find the answer or
someone you loved being hurt leads to calling them an                were perfectly plausible (Figure 5), although in some cases it
ambulance results in an underspecified causal relation. That         considered at least one strange possibility before finding the
information was automatically extracted by the NLU system            right answer (Figure 6). Answering five of these six questions
but not connected to the causal fact; we edited those facts to       correctly involved chaining through the same CSU
connect those relevant automatically generated facts (Figure         expressing that a violent impact causes harm, demonstrating
4). Additionally, we added facts to several CSUs indicating          that AC can use the same CSU in different contexts.
that a particular event was an instance of a ViolentImpact, (a          Question 461 was the only question which was not
new concept for our system), and removed facts which made            answered correctly from its raw NLU output, which included
the CSU overly specific (i.e., information that would be worn        representations that prevented SME from detecting success.
away via analogical generalization).                                 Specifically, in the correct answer “her son fell out of his
   Since AC involves within-domain analogies, we use                 bed,” the multiple possessives “her” and “his” were
required partition constraints (Forbus et al., 2016) to restrict     interpreted as (possesses mother son) and (possesses his bed),
matching entities to be within the same categories. For              which are reasonable. However, the coreference system did
example, matches with the CSU in Figure 3 had the constraint         not resolve “his” to “son” (i.e., (possesses son bed)), and the
that ambulance and phone call could only be placed in                CSU did not contain the first “possesses” fact (another fact in
correspondence with an ambulance and a phone call,                   the CSU expressed the mother/son relationship), so the base
respectively.                                                        normalized score of the match was not quite high enough to
                                                                     detect success. However, there was still the information that
(causes-PropProp                                                     the son fell out of a bed, if not his bed. To verify this analysis,
  (and (isa rock2942-skolem StoneStuff)
       (isa throw2912-skolem ThrowingAnObject)                       we removed these extra “possesses” facts, and the system was
       (objectActedOn throw2912-skolem rock2942-skolem)              able to correctly find the answer.
       (target throw2912-skolem person6293-skolem))
  (and (isa someHarm1523-skolem ViolentImpact)
                                                                        Was analogical chaining necessary? Yes, since every
       (objectHarmed someHarm1523-skolem                             question required two or three analogies to reach the correct
                       person6293-skolem)))                          answer. For example, after amending question 461 as noted
                          *********
(causes-PropProp                                                      (causes-PropProp
  (and (from-UnderspecifiedLocation bed2498-skolem                         (and (isa someHarm1523-skolem Concert)
                                      person6293-skolem)                         (isa go-to35116-skolem AttendingSomething)
       (isa bed2498-skolem Bed-PieceOfFurniture)                            (toLocation go-to35116-skolem someHarm1523-skolem)
       (isa fall24789-skolem FallingEvent)                                   (performedBy go-to35116-skolem person5082-skolem)
       (primaryObjectMoving fall24789-skolem                                     (loudnessOfEvent someHarm1523-skolem Loud)))
                              person6293-skolem))                          (and (isa someHarm1523-skolem ViolentImpact)
  (and (isa someHarm1523-skolem ViolentImpact)                                   (objectActedOn someHarm1523-skolem
       (objectHarmed someHarm1523-skolem                                                           ear2942-skolem)
                       person6293-skolem)))                                      (isa ear2942-skolem Ear)))
       Figure 5: Plausible inferences for question 461: the               Figure 6: Implausible inference for question 461: the
 person was hit by a rock; the person fell out of bed (correct)                harm was a concert which hurt their ears
                                                                 559

above, the system was able to find the answer only after                Much AI research on commonsense reasoning has relied
retrieving and applying the three CSUs in Figure 2. It first         on formal logic and deductive inference (see Davis, 1990 and
postulated that a loved one had been hurt, then that it was her      Mueller, 2014). All such approaches rely on using large
son, and from a fall. We suspect that most COPA questions            numbers of logically quantified axioms. We have noted
should be answerable within three AC steps, but confirming           several problems with this approach, including the difficulty
this remains future work.                                            of constructing correct logically quantified axioms. Analogy
                                                                     only requires acquiring relevant cases and refining them via
                        Related Work                                 analogical generalization, rather than complete and correct
As of this writing, three other systems have been tested on          domain theories. Furthermore, reasoning using formal logic
COPA, all focused on text analysis. Gordon et al. (2011) used        must proceed serially: each inference rule asserts only its
Pointwise Mutual Information to evaluate how often words             consequences. AC also proceeds serially, but a highly
in the questions co-occurred with words in the answer. While         relevant case can lead to several inferences (not necessarily
their system performed significantly above chance (65.4%             derivable from the same logical rule) being asserted at once,
accuracy), it only slightly gained in accuracy as the training       potentially reducing the number of inference steps needed.
corpus dramatically increased, from 106 to 107 stories.                 In Explanation-Based Learning (EBL) (DeJong, 2006), a
Goodwin et al. (2012) achieved a similar performance with            human provides a formal domain theory and examples from
other textual analysis techniques (63.4% accuracy), but found        a domain to a system, which it uses to refine its own formal
that using multiple components in their analysis did not             theory of that domain. AC differs in that it only requires the
significantly improve accuracy over using only bigram co-            human to provide (in simplified natural language) cases that
occurrence. Luo et al. (2016) achieved higher accuracy               illustrate an underlying principle, rather than the logic of that
(70.2%) using a large corpus to automatically extract causal         underlying principle, which is simpler for non-experts. Also,
relationships between concepts, then using this extracted            the domain theories generated through EBL are still in logic
information to determine the ‘causal strength’ between a             and as such face the same drawbacks listed above.
question and each of its answers. While the extracted causal            AnalogySpace (Speer, Havasi & Lierberman 2008) used a
information appears more effective than the other two                large knowledge base of commonsense assertions in natural
techniques, it still requires that information to be represented     language to make predictions about concepts, which could
in the training corpus, which much of commonsense                    then be compounded with further predictions. However, they
knowledge is not. Together these findings suggest that there         define similarity as a linear operation over feature vectors,
are upper limits to text-based techniques, which argues for          using a reduced-dimensionality approximation of MAC’s
investigating approaches like ours that use conceptual               dot-product of content vectors to retrieve relevant concepts,
representations. Of course, all three of these techniques were       and do not include any measure of structural similarity.
able to attempt the entire COPA test. AC will require a large        Furthermore, this work was only used to predict features of
case library of CSUs before we test it on the full COPA test.        individual concepts, and it is unclear how it would scale up
   Derivational analogy, as implemented in the PRODIGY               to explain or predict larger cases.
architecture, similarly chains together previously known                Though AC generates possible explanations for situations,
cases to derive solutions to problems (Veloso & Carbonell,           it differs from using logic for abduction (e.g. Hobbs, 2006)
1993). It plans for a goal by analogy to plans that previously       since it does not require a logically quantified domain theory,
achieved a similar goal, with subgoals recursively planned for       and does prediction as well as explanation.
by analogy. Stored cases are indexed by and retrieved via               The importance of the Goldilocks Principle, using cases
their justifications, initials states, and goal states.              that are neither too small or too large in analogical matching,
Derivational analogy differs from AC as we have described            was highlighted by Finlayson and Winston (2005), which
it in three important ways. First, our cases are stored and          helped inspire our thinking about CSUs..
retrieved without requiring any information about what they
previously allowed the system to conclude. Although this                          Conclusions and Future Work
means that sometimes a highly dissimilar yet nonetheless                There is already evidence that analogy is widely used in
relevant case may not be retrieved by MAC/FAC, it also               human cognition (Gentner, 2003), so it would be surprising
circumvents issues with indexing and retrieval, and enables          if it were not used for commonsense reasoning. This paper
AC to use a relevant case even when it has not been useful in        has explored how that might work. We proposed Common
similar past situations. Second, derivational analogy was            Sense Units, intermediate-sized representations, closer to
specifically used to create plans to achieve goals, rather than      rules in size than raw experiences, but still without logical
to explain a state of affairs or predict future outcomes. It is      variables, as a means of encoding experience that supports
not clear whether derivational analogy could be used for tasks       flexible analogical processing. We proposed analogical
that cannot be easily framed in terms of planning or problem-        chaining, the repeated use of analogies to rapidly construct
solving (although answering COPA questions could be                  explanations and predications, as a means of performing
framed in such a way). Finally, PRODIGY made use of both             commonsense reasoning. While AC is serial at the level of
case-based and first-principles reasoning. AC does not use           applying an analog, it is parallel with respect to the
any first-principles reasoning at any stage.                         application of candidate inferences within a step, thereby
                                                                 560

being more efficient than traditional chaining with logical         Gentner, D. (1983). Structure‐Mapping: A Theoretical
axioms. The bundling of common patterns of facts via CSUs             Framework for Analogy. Cognitive Sci., 7(2), 155-170.
also provides more focus for each inference step. CSUs and          Gentner, D. (2003). Why we're so smart. In D. Gentner and
AC were used to answer COPA questions, demonstrating its              S. Goldin-Meadow (Eds.), Language in mind: Advances in
potential as a model of commonsense reasoning.                        the study of language and thought (pp.195-235).
   We plan to explore several directions of future work. First,       Cambridge, MA: MIT Press.
we plan to expand our NLU capabilities to support fully             Gentner, D., & Forbus, K. (2011). Computational models of
automatic construction of CSUs from natural language, rather          analogy. WIREs Cognitive Science, 2. 266-276.
than mixing automatic generation with some manual editing,          Gentner, D., Ratterman, M. J., & Forbus, K. (1993). The roles
both to reduce tailorability and to expand coverage, including        of similarity in transfer. Cog. Psych., 25, 524–575.
crowdsourcing CSUs (c.f. Li et al., 2013). Extracting CSUs          Gordon, A., Bejan, C., and Sagae, K. (2011) Commonsense
from larger stories via analogical generalization is, we think,       Causal Reasoning Using Millions of Personal Stories.
a promising approach. Second, we plan to expand the                   Twenty-Fifth Conference on Artificial Intelligence (AAAI-
reasoning techniques used for checking the validity of                11), August 7–11, 2011, San Francisco, CA
retrieved cases, skolem resolution, and determining when            Goodwin, T., Rink, B., Roberts, K., and Harabagiu, S. (2012)
sufficient reasoning has been done. Answers to multiple-              UTDHLT: COPACETIC System for Choosing Plausible
choice questions can also help guide chaining. Finally, we            Alternatives. Procs of the 6th Int’l Workshop on Semantic
plan to test the expanded model on the entire COPA and other          Eval. (SemEval 2012), June 7-8, 2012, Montreal, Canada.
commonsense reasoning tests, such as Winograd schemas4.             Hobbs, J. (2006) Abduction in Natural Language
                                                                      Understanding. In Horn & Ward (Eds.)., The Handbook of
                    Acknowledgements                                  Pragmatics Blackwell Publishing Ltd, Oxford, UK.
This research was supported by the Socio-Cognitive                  Lenat, D. (1995). CYC: A large-scale investment in
Architectures for Adaptable Autonomous Systems Program                knowledge infrastructure. Comm. of ACM, 38(11), 33-38.
of the Office of Naval Research, N00014-13-1-0470.                  Li, B., Lee-Urban, S., Johnston, G., & Riedl, M. (2013). Story
                                                                      Generation       with     Crowdsourced       Plot   Graphs.
                                                                      In Proceeding of the 27th AAAI Conference on Artificial
                         References
                                                                      Intelligence, Bellevue, Washington.
Battaglia, P., Hamrick, J., & Tenenbaum, J. (2013).                 Luo, Z., Sha, Y., Zhu, K., Hwang, S., & Wang, Z. (2016).
  Simulation as an engine of physical scene                           Commonsense Causal Reasoning between Short Texts.
  understanding. PNAS, 110(45), 18327-18332.                          15th Int’l Conference on Principles of Knowledge
Davis, E. (1990, 2014). Representations of commonsense                Representation and Reasoning, Cape Town, South Africa.
  knowledge. Morgan Kaufmann.                                       Markman, A. B., & Medin, D. L. (2002). Decision
DeJong, G. (2006). Toward robust real-world inference: A              making. Stevens' Handbook of Experimental Psychology.
  new perspective on explanation-based learning. In                 McLure, M. D., Friedman, S. E., & Forbus, K. D. (2015).
  Machine Learning: ECML 2006 (pp. 102-113). Springer.                Extending Analogical Generalization with Near-Misses.
Finlayson, M. A., & Winston, P. H. (2005) Intermediate                In Proceedings of the Twenty-Ninth AAAI Conference on
  Features and Informational-level Constraint on Analogical           Artificial Intelligence, Austin, TX (pp. 565-571).
  Retrieval. In Proceedings of the 27th Annual Meeting of           Minsky, M. (1974). A Framework for Representing
  the Cog. Sci. Society (CogSci 2005), Stresa, Italy. 666–671.        Knowledge. Reprinted in The Psychology of Computer
Forbus, K. 2001. Exploring analogy in the large. In Gentner,          Vision, P. Winston (Ed.), McGraw-Hill, 1975.
  Holyoak, and Kokinov (Eds) The Analogical Mind:                   Mueller, E. T. (2014). Commonsense Reasoning: An Event
  Perspectives from Cog. Sci. Cambridge, MA: MIT Press.               Calculus Based Approach. Morgan Kaufmann.
Forbus, K. (2015). Analogical Abduction and Prediction:             Roemmele, M., Bejan, C., & Gordon, A. (2011) Choice of
  Their Impact on Deception. AAAI Fall Symposium on                   Plausible Alternatives: An Evaluation of Commonsense
  Deceptive and Counter-Deceptive Machines.                           Causal Reasoning. AAAI Spr. Symp. on Log. Form. of
Forbus, K., Ferguson, R., Lovett, A., & Gentner, D. (2016).           Commonsense Reasoning, Stanford University
  Extending SME to handle large-scale cognitive modeling.           Schank, R.C. & Abelson, R. (1977). Scripts, Plans, Goals,
  Cognitive Science.                                                  and Understanding. Hillsdale , NJ: Earlbaum Assoc.
Forbus, K. & Gentner, D. 1997. Qualitative mental models:           Speer, R., Havasi, C., & Lierberman, H. (2008).
  Simulations or memories? QR 1997, Cortona, Italy.                   AnalogySpace: Reducing the Dimensionality of Common
Forbus, K., Gentner, D., & Law, K. (1995). MAC/FAC: A                 Sense Knowledge. Proceedings of the Twenty-Second
  model        of     similarity‐based     retrieval. Cognitive       AAAI Conference on Artificial Intelligence, Chicago, IL
  Science, 19(2), 141-205.                                          Veloso, M., & Carbonell, J. (1993). Derivational analogy in
Forbus, K., Klenk, M. and Hinrichs, T. (2009). Companion              PRODIGY: Automating case acquisition, storage, and
  Cognitive Systems: Design Goals and Lessons Learned So              utilization. In Case-Based Learning (55-84). Springer US.
  Far. IEEE Intelligent Systems, 24(4), 36-46.
  4 http://commonsensereasoning.org/winograd.html
                                                                561

