Causal Contrasts Promote Algebra Problem Solving
Jian-Ping Ye (jpy2110@tc.columbia.edu)
Department of Human Development, 525 W 120th Street
New York, NY 10027-6625 USA
Jessica M. Walker (jewalker@chapman.edu)
Department of Psychology, 1 University Drive
Orange, CA 92866-1005 USA
Patricia W. Cheng (cheng@lifesci.ucla.edu)
Department of Psychology, 405 Hilgard Avenue
Los Angeles, CA 90095-1563 USA
Abstract

emphasizing worked examples to support schema
acquisition (Carroll, 1994; Sweller & Cooper, 1985), selfgenerating explanations during learning to improve
understanding and knowledge integration (e.g., Chi et al.,
1989; Chi, 2000), labeling subgoals to illustrate the reason
why certain solution steps should be applied to promote
learning and transfer (e.g., Catrambone, 1998) and using
comparisons and contrasting examples in the instruction to
improve the learning of concepts and procedures (e.g.,
Hattikudur & Alibali, 2010; Rittle-Johnson & Star, 2007;
Richland & McDonough, 2010).
Fewer studies have examined the use of causal learning to
support mathematical learning and problem solving.
Whereas mathematics is traditionally taught using explicit
instruction to convey analytic knowledge, the causal
contrast approach is an instructional method that recruits an
implicit empirical-learning process to help students discover
the reasons underlying mathematical procedures (Walker et
al., 2014). Humans have a natural capacity for learning
cause-and-effect relations (Cheng, 1997; Gopnik et al.,
2004; Leslie & Keeble, 1987). The causal-contrast approach
hypothesizes that operation-outcome relations in
mathematics can be thought of as causal. The approach 1)
induces students to formulate the goal of an operation in a
mathematical procedure by allowing them to fail to solve a
challenging problem using their prior mathematical
knowledge and 2) promptly provides the relevant
conditional contingency information (Cheng & Holyoak,
1995) by adding a contrasting problem, one that controls for
confounding factors by being as similar as possible to the
challenging problem but with the features causing the
difficulty removed. “No confounding” is a pre-requisite for
causal learning. The goal is for students to readily discover
the goal of each step in the solution. It is not the use of
comparison per se that characterizes the causal-contrast
approach, but rather the targeting of critical concepts by
juxtaposing contrasting information designed to enable
discovering the causes of outcomes. The causal-contrast
approach is also distinct from subgoal-learning in that it
does not explicitly mention what the subgoals are. Instead,
it recruits the student’s natural causal reasoning to construct

The causal-contrast approach is a new teaching method that
recruits learners’ implicit causal discovery process to improve
math learning by juxtaposing contrasting information critical
to discovering the goal of each solution step. Students often
blindly memorize mathematical procedures and have
difficulty transferring their knowledge to novel problems. By
enabling learners to infer the goal of each step, the causalcontrast approach substantially improved high-school algebra
problem solving compared to a traditional instructional
control (Walker, Cheng & Stigler, 2014). The present study
developed Walker et al.’s instructional materials into a
computer-based teaching program and tested the new
approach on community-college students, a population for
whom the traditional approach is often ineffective. The study
added two new conditions: a baseline that received no
instruction and a condition using a teaching video from Khan
Academy, a well-regarded online educational website
representative of the traditional approach. A delayed post-test
indicated that the causal-contrast condition produced
dramatically greater success in solving transfer problems than
the other three conditions.
Keywords: causal contrasts; causal induction; implicit
learning; knowledge transfer; mathematics education

Introduction
Compared to previous years, U.S. students’ ranking in
science, technology, engineering, and math (STEM)
subjects has been improving gradually; however, students’
performance on the international mathematics assessments
continue to fall below the international average (PISA,
2003, 2006, 2009, 2012). A massive amount of research has
studied this issue, with the goal of improving students’
mathematical learning and understanding.
One of the most common yet ineffective strategies that
students use to learn mathematics is to blindly memorize the
solution steps without truly understanding the reason behind
each step (e.g., Stigler, Givvin, & Thompson, 2010).
Without connecting the procedures to goals or concepts,
repeatedly solving a large number of conventional problems
may not be helpful for fostering the flexible use of
mathematical knowledge (Cooper & Sweller, 1987;
Schwartz et al., 2011). Previous research have suggested
various ways to enhance mathematics education, such as
1

480

the goals and subgoals within a causal structure that
supports the application of solution steps.
Traditional instructional approaches teach students
analytically and explicitly the rules and steps for solving
specific types of problems. Figure 1 illustrates how this
approach teaches how to solve quadratic equations.

their failure: unlike the second and third equations, the top
equation has both an x and an x2 term, preventing one from
isolating x by simply rearranging the equation. The newly
formulated cause – namely, having both x and an x2 terms –
in turn becomes an “effect” for a subsequent operation,
namely, factoring, to remove. Without their initial attempt to
isolate x, learners would have no “effect” for which to
discover its cause. This effect – failure to isolate x by
rearranging the equation – is often not explicitly noted in
traditional instruction. Similarly, without comparing the top
equation with the next two equations, a learner who is asked
to explain why the top equation is difficult may mention the
effect alone, “I’m unable to rearrange the equation to isolate
x”, omitting to identify its “cause”: having both x and x2
terms in the same equation. Thus, each comparison is
designed to direct attention to the accurate formulation of an
essential causal relation in the structure of the solution.

Figure 1. A cumulative static screen shot of sequentially
presented and narrated traditional instructions.
For example, a student presented with the top equation in
the figure is shown how to re-arrange the equation into
standard form (second equation in the figure), factor the
expression on the left-hand side, then determine the possible
values of x, the desired unknown, using the zero-product
property (i.e., if a • b = 0, then a = 0 or b = 0). For a
substantial fraction of students, as evidenced by their failure
to flexibly generalize their learning to novel problems, this
approach does not lead to an understanding of the causal
structure of the solution, the reason behind each step in the
procedure and how the steps work together. For example,
what is the purpose of factoring the expression on the lefthand side of the equation? And what is the relationship
between factoring and the zero-product property?
Opportunities to compare worked examples (RittleJohnson & Star, 2007) to explain solutions (e.g., Chi et al.,
1989; Chi, 2000) or to isolate subgoals (e.g., Catrambone,
1998) may enable understanding of the causal structure of a
problem. However, even then a student may not identify all
the concepts in the causal structure, or have all the requisite
information to infer the causal relations when the concepts
are identified. While Sfard (2007) emphasizes the use of
teacher-student discourse (students compare their own
solution to teacher’s and explain the differences) to help
students identify and correct misconceptions, the students
may not discover the purposes of the concepts critical to
transfer their success to novel problems.
The causal-contrast approach more directly targets
specific links in the causal structure that make use of critical
mathematical concepts essential to the solution of relevant
types of problems. For example, students are first asked to
try solving a quadratic equation (see top equation in Figure
2). If they fail to solve it, they are presented with the next
two equations in the figure and asked to solve them. After
students solve these, they are asked why these problems are
easier for them to solve than the top equation. This
comparison enables students to readily discover a cause of

Figure 2. A screen shot of the causal-contrast intervention.
The process repeats as the student proceeds through the
solution. In Figure 3, the pair of equations enclosed by the
light gray rectangle illustrates the use of causal contrast for
inferring a perceptual feature that causes a difficulty.
Students are asked to solve the factored form of the
quadratic equation (see top equation inside rectangle). If
they fail to solve the problem, they are asked to solve a
variant of the problem that has the difficulty removed
(bottom equation inside rectangle), and to answer the
question, “What values of x would make x • y = 0 true,
regardless of the value of y?” In this simple form, most
students have no difficulty. They are then asked to compare
the two equations. The comparison may allow them to
discover why they initially failed, for example, that the
perceptual complexity of the product prevented them from
recognizing that the expression on the left-hand side is a
product, that inside each pair of parentheses is “just a
number”. At this point, the perceptual complexity can
switch causal roles, taking on the role of an effect that the
student can prevent or remove in the future, for example, by
pausing to take note of the perceptual complexity and to
register the perceptual cues indicating when the zeroproduct property applies.
In more general terms, the comparisons we construct
provide conditional contingency information (Cheng &
Holyoak, 1995) consisting of the state of an effect (e.g.,
succeeding to solve a problem or not) in the presence of a
candidate cause (a feature of the math problem, e.g., an
equation having both an x and an x2 term) and in its absence
2

481

(an equation not having both an x and an x2 term), with
alternative causes held constant.

Participants
Sixty-eight community-college students recruited in
Southern California and Downstate New York participated
in our study. Participants (N=68) were randomly assigned
into one of the three experimental conditions: CausalContrast (N=17), Traditional (N=18), Khan Academy
(N=18), or the Baseline (N=15) control condition.

Study Design
This study consisted of two sessions, which were scheduled
one to three weeks apart. In the first session, all participants
were given a pretest measure assessing their knowledge of
algebraic notation and of solving quadratic equations.
Participants in the three experimental conditions received a
lesson on solving quadratic equations on a computer
followed by practice with an identical set of problems with
identical feedback. For the baseline control group,
immediately after the pretest, without receiving any
instruction, participants were randomly assigned to receive
one of two highly similar problem sets (Set A and Set B),
which were the post-tests given to the experimental groups.
The two sets consisted of analogous problems, and both
assessed students’ algebra problem solving mostly on
quadratic equations. There was no time limit on the pretest
and post-test, and the intervention lasted 25 minutes on
average across the three experimental conditions. The
groups did not differ in intervention time: MCausal-Contrast =
24.4, sd = 8.76, MTraditional = 23.3, sd = 7.55, and MKhan Academy
= 29.5, sd = 12.3, F (2, 50) = 2.05, p = .14. After a one- to
three-week delay, participants in the experimental
conditions were given one or the other post-test set
alternately in the second session, and those in the Baseline
condition received their second post-test set.

Figure 3. A later causal-contrast screen shot.

Comparing Causal-Contrast and
Traditional Instruction
We conducted the present study on community-college
students using a pretest/ intervention/posttest design to test a
computer-based version of the Causal-Contrast instructional
method (CC) against two computer-based traditional
instructional methods and a baseline condition that received
no instruction. The computer programs both animated and
narrated the teaching materials for clearer and less attentiondemanding instructions, reducing learners’ cognitive load
(Mayer & Moreno, 2003). By equating the instructions and
feedback across conditions in the programs, we eliminated
potential bias due to the human experimenter interacting
with participants in Walker et al. (2014).
Although the causal approach was found to benefit both
university students (at UCLA) and community-college
students (Walker et al., 2014), the latter are less likely to
benefit from traditional mathematics education. Causal
induction is an implicit and evolutionarily old process
(Hollis, 1997) and should be more uniformly available
across the population than explicit reasoning processes.
One of the Traditional conditions (T) used materials
identical to the causal contrast condition except for the
instructional approach. The other traditional condition
(Khan Academy; K) used an algebra teaching video from
Khan Academy, a popular and well-regarded online
educational website representative of traditional teaching.
Khan Academy has become the largest school in the world;
over 10 million students have watched its teaching videos
online (Noer, 2012). Additionally, a Baseline condition (B)
that did not receive any instruction was used as a control.
All participants received information on the same
mathematical concepts, solved the exact same set of
problems with the same feedback.
The causal-contrast hypothesis predicts that students
receiving the causal-contrast instructional intervention
would have better performance in the post-test compared to
students in the two traditional intervention conditions and in
the baseline condition.

Instructional Materials
Causal-Contrast
Approach.
The
causal-contrast
instructional materials are as explained earlier. Participants
in the causal-contrast approach were given three challenging
problems that students tend to fail to solve. If the
participants failed to solve a problem, a feedback slide
informed them that they solved the problem incorrectly. A
branching function was used to identify the nature of the
failure (e.g., lack of understanding vs. careless mistakes) to
enable the appropriate feedback. Instead of showing the
correct solution, a contrasting problem was then presented.
If the student solved a challenging problem successfully, the
next challenging problem was presented.
Traditional Instructional Method. The instructional
materials used in this condition were designed to make use
of techniques that are representative of traditional
instruction and were based on a popular textbook (Sullivan
& Sullivan, 2007). Participants in this condition were shown
step-by-step procedures through worked examples, written
solutions of example problems that provide justifications for
each procedural step (see Figure 1). The examples were
followed by practice problems. Combining worked
3

482

Post-test

examples and problem solving has been proved to facilitate
learning (Sweller & Cooper, 1985). The instructions stated
the subgoals of the critical steps in the procedure; for
example, subjects were told that a quadratic equation is
rearranged to standard form (having a zero on one side and a
polynomial on the other) so that it could be factored and
solved using the zero-product property. Emphasizing
subgoals in problem solving has been shown to promote
learning and transfer (Eiriksdottir & Catrambone, 2011).

Explanation of Analyses. Before evaluating the effects of
instruction type on participants’ post-test performance, a
correlation analysis was conducted to test whether the posttest performance was correlated with the pretest score.
Pearson’s correlation confirmed that the pretest and post-test
scores were strongly correlated, r (68) = .711, p < .01,
suggesting that the participants’ post-test performance was
in part due to their prior mathematical knowledge as
indicated by their pretest scores.
Baseline Group’s Post-test Sets A and B. To examine
whether our two post-test sets in the Baseline group
produced different performance, a one-way repeated
measures ANCOVA with pretest as a covariate was
conducted on the post-test scores, separately for the
instructed problems and for the transfer problems. For
neither type of problems was there a significant difference
between the scores of Post-test Sets A and B, F(1,13) =
.090, p = .768 ηp2 = .007 and F (1,13) = 1.15, p = 0.303,
ηp2= .081 for the instructed and transfer problems
respectively. Because the two sets produced highly similar
performance, our analyses below collapsed across both sets.
Comparison across Conditions. Because the effect of
mathematical instructions is strongly influenced by learners’
prior knowledge (Clarke, Ayres & Sweller, 2005; RittleJohnson et al., 2009), to show a clearer picture of the effect
of the interventions, we separated the participants into three
groups based on their pretest scores: low-pretest (i.e., pretest
< or = 50%), medium-pretest (i.e., 50% < pretest < 90%),
and high-pretest (i.e., pretest > or = 90%). Tables 2, 3, and 4
show the respective means of pretest scores, instructed
problems scores, and transfer problems scores in each
subgroup for each intervention condition.

Khan Academy. The instructional material for this
condition was an online video from Khan Academy’s
website: https://www.youtube.com/watch?v=uktzcTg_N7U.
It makes use of animated digital technology designs. The
video covers the necessary techniques for solving a
quadratic equation, including factoring techniques and use
of the zero-product property.

Measures
Post-test. The post-test included two types of problems:
instructed and transfer. Instructed problems could be solved
using the same solution procedures as the study problems.
Transfer problems required generalization of concepts
learned in the intervention. These problems included
factorable quadratics in non-standard form. Table 1 lists the
transfer problems:
Table 1. Transfer Problems

Table 2. Descriptive Statistics for Pretest Performance

Results
Pretest
A one-way between-subjects ANOVA performed on
participants’ pretest performance shows that the four groups
did not differ in their pretest performance: MCausal-Contrast =
73.2 (sd = 19.4), MTraditional = 76.1 (sd = 16.0), MKhan Academy
= 68.6 (sd = 18.1), and MBaseline = 70.3 (sd = 25.0), F (3, 64)
= .497, p = .686).

As shown in Table 2, there were relatively few low-pretest
participants. Relative to other groups, these participants’
pre-test scores varied substantially across conditions,
making the assessment of the effectiveness of training in
this group problematic. In view of the nature our pretest
problems, these participants’ pretest performance suggests
that they lack sufficient arithmetic or algebra knowledge to
benefit from the interventions. In contrast, the high-pretest
participants can potentially still benefit because the transfer
problems are considerably harder than the pretest problems.
In the following analyses, to better assess the
instructional interventions in their effective range, we
excluded the low-pretest participants. We combined the rest

Delay Interval
A one-way between-subjects ANOVA performed on time
between instruction and post-test (in days) shows that the
four conditions did not differ in their average delay time:
MCausal-Contrast =14.5, sd = 5.37, MTraditional = 13.1, sd =
5.61, MKhan Academy = 12.9, sd = 4.83, and MBaseline =
13.9, sd = 5.85, F (3, 64) = .318, p = .812.

4

483

of the participants because of our small sample sizes. Figure
4 shows these participants’ post-test performance results.

group greatly outperformed the other three groups on
Transfer Problem 6, χ2(1, N=56) = 8.13, p=.004. This
superiority is notable in that the CC instructions could have
misled the participants into formulating a simplistic rule
regarding the joint presence of x and x2, one that ignores
whether they are terms or factors. They could have been
stymied, for example, because the factored equation still
contains both x and x2 terms, as they saw in the pair of
contrasting problems in Figure 2. Instead, the intervention
enhanced their correct flexible use of the relevant concepts
and procedures.
Performance on Transfer Problem 3 is also notable in that
this problem does not involve a quadratic equation. And yet
the CC group outperformed the other three groups, χ2(1,
N=56) = 9.71, p=.002. The CC group’s deeper
understanding of the causal structure of solving quadratic
equations might have allowed them to reason more flexibly
on another type of algebra problem. An intriguing
possibility, which of course requires further research, is that
the causal-contrast approach may awaken students’ natural
causal inference processes so that they create and test their
own causal contrasts as they encounter new mathematical
domains.
This possibility was collaborated by the CC group’s
superior performance on Transfer Problem 7, χ2(1, N=56) =
6.21, p=.013. This problem involved an x2 term with a
fractional coefficient, and there was no training on fractional
coefficients in any of the interventions.

Figure 4. Estimated marginal means for medium- or highpretest participants’ Post-test scores.
Instructed Problems. A one-way ANCOVA on the posttest scores using pretest score as a covariate shows no
significant difference in scores among the four conditions, F
(3, 51) = 2.37, p = .081, ηp2= .122, perhaps due to our small
sample size.
Table 3. Descriptive Statistics for Instructed Problems

Transfer Problems. A one-way ANCOVA with pretest
score as a covariate conducted on the transfer problems
shows a significant difference among the intervention
conditions, F (3, 51) = 8.22, p < .001, ηp2= .326. Follow-up
pairwise comparisons using the Bonferroni correction
indicate that the CC group outperformed the T group (p =
.002), the K group (p < .001), and the B group (p = .023).
The CC group transferred their knowledge about as well as
UCLA students given similar training in our previous
studies (Walker et al., 2014), with mean scores ranging from
80% to 82%. There was no statistically significant
difference between the other three groups, p > .50.
Table 4. Descriptive Statistics for Transfer Problems

Figure 5. Performance on transfer problems in the four
conditions.

Discussion
The causal contrast instructional approach invokes learners’
natural causal induction process to identify the cause-andeffect relationships in a mathematical procedure. This
approach decomposes the causal structure in a mathematical
problem and accordingly caters the learning materials to
allow learners to formulate the purposes of mathematical
operations. These purposes are often not explicitly
mentioned in traditional mathematical training.
Our current findings testing community-college students
replicated and extended the results of previous studies

Figure 5 shows performance on individual transfer problems
for the four intervention groups. The CC group consistently
outperformed the other three groups.
Comparing the frequency of participants who solved a
problem correctly versus incorrectly, we see that the CC
5

484

conducted on university students and community-college
students (Walker et al., 2014). These studies show large
improvements in solving algebra problems due to causalcontrast training. The improvements were especially
dramatic in the community-college students.
Our comparison conditions now included a baseline
condition and instructions from the Khan Academy as a
benchmark. We eliminated the potential for experimenterbias by incorporating all materials as animated computer
programs, with no experimenter-participant interaction
during the interventions.
Our results show that even when explicit analytic
instruction focuses on teaching the reasons for mathematical
procedures, students still often fail to learn in a way that
promotes generalization to novel problems after a one-tothree week delay. In contrast, by allowing students to use an
implicit, empirical learning process to discover the causal
structure of solutions, students are able to fill in the missing
links of their causal structure, need not rely on their rote
memory of procedures, and become able to flexibly use
their mathematical knowledge.

Eiriksdottir, E., & Catrambone, R. (2011). Procedural
instructions, principles, and examples how to structure
instructions for procedural tasks to enhance performance,
learning, and transfer. Human Factors: The Journal of the
Human Factors and Ergonomics Society, 53(6), 749-770.
Gopnik, A., Glymour, C. Sobel, D. Schulz, L. Kushnir, T.,
& Danks, D. (2004). A theory of causal learning in
children: Causal maps and Bayes nets. Psychological
Review, 111(1), 1-31.
Hattikudur, S., & Alibali, M. W. (2010). Learning about the
equal sign: Does comparing with inequality symbols
help? Journal of experimental child psychology, 107(1),
15-30.
Hollis, K. L. (1997). Contemporary research on pavlovian
conditioning: A "new" functional analysis. American
Psychologist, 52(9), 956-965.
Leslie A. M., & Keeble, S. (1987). Do six-month-old
infants perceive causality?'' Cognition, 25, 265-288.
Mayer, R. E., & Moreno, R. (2003). Nine ways to reduce
cognitive load in multimedia learning. Educational
psychologist, 38(1), 43-52.
Noer, M. (2012, November). One man, one computer, 10
million students: How Khan Academy is reinventing
education. Forbes. Retrieved from http://www.forbes.com.
PISA (2003, 2006, 2009, 2012). Program for International
Student Assessment. International Association for the
Evaluation of Educational Achievement.
Richland, L. E., & McDonough, I. M. (2010). Learning by
analogy: Discriminating between potential analogs.
Contemporary Educational Psychology, 35(1), 28-43.
Rittle-Johnson, B., & Star, J. R. (2007). Does comparing
solution methods facilitate conceptual and procedural
knowledge? An experimental study on learning to solve
equations. Journal of Educational Psychology, 99(3), 561.
Schwartz, D. L., Chase, C. C., Oppezzo, M. A., & Chin, D.
B. (2011). Practicing versus inventing with contrasting
cases: The effects of telling first on learning and transfer.
Journal of Educational Psychology, 103(4), 759.
Sfard, A. (2007). When the rules of discourse change, but
nobody tells you: Making sense of mathematics learning
from a commognitive standpoint. Journal for Learning
Sciences, 16(4), 567-615.
Stigler, J. W., Givvin, K. B., & Thompson, B. J. (2010).
What community college developmental mathematics
students understand about mathematics. MathAMATYC
Educator, 1(3), 4-16.
Sullivan, M.I., & Sullivan, M. (2007). College algebra:
Concepts through functions. New York: Pearson Education.
Sweller, J. & Cooper, G. (1985) The use of worked
examples as a substitute for problem solving in learning
Algebra. Cognition and Instruction, 2, 59-89.
Walker, J. M., Cheng, P.W., & Stigler, J.M. (2014).
Equations are effects: Using causal contrasts to support
algebra learning. In P. Bello, M. Guarini, M. McShane,
& B. Scasellati. (Eds.), Proceedings of the 36th Annual
Conference of the Cognitive Science Society. Austin, TX:
Cognitive Science Society.

References
Carroll, W. M. (1994). Using worked examples as an
instructional support in the algebra classroom. Journal of
Educational Psychology, 86(3), 360.
Caspi, S. & Sfard, A. (2012). Spontaneous meta-arithmetic
as a first step toward school algebra. International
Journal of Educational Research. 51-52, 45-65.
Catrambone, R. (1998). The subgoal learning model:
Creating better examples so that students can solve novel
problems. Journal of Experimental Psychology: General,
127(4), 355.
Cheng, P.W.(1997). From covariation to causation: A causal
power theory. Psychological Review, 104(2), 367-405.
Cheng, P.W., & Holyoak, K.J. (1995). Complex adaptive
systems as intuitive statisticians: causality, contingency,
and prediction. In Comparative Approaches to Cognitive
Science, Ed. HL Roitblat, J-A Meyer. Cambridge, MA:
MIT Press.
Chi, M.H. (2000). Self-explaining expository texts: The
dual processes of generating inferences and repairing
mental models. In Glaser, R. (Ed.), Advances in
Instructional Psychology. Mahwah, NJ: Lawrence
Erbaum Associates.
Chi, M. T. H., Bassok, M., Lewis, M. W., Reimann, P., &
Glaser, R. (1989). Self-explanations: how students study
and use examples in learning to solve problems. Cognitive
Science, 13(2), 145–182.
Clarke, T., Ayres, P., & Sweller, J. (2005). The impact of
sequencing and prior knowledge on learning mathematics
through
spreadsheet
applications.
Educational
Technology Research and Development, 53(3), 15-24.
Cooper G. & Sweller, J. (1987). Effects of schema
acquisition and rule automation on mathematical
problem-solving transfer. Journal of Educational
Psychology, 79(4), 347-62.
6

485

