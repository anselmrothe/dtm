             Know Your Enemy: Applying Cognitive Modeling in Security Domain
                                             Vladislav D. Veksler (vdv718@gmail.com)
                                               DCS Corp, U.S. Army Research Laboratory
                                                   Aberdeen Proving Ground, MD, USA
                                         Norbou Buchler (norbou.buchler.civ@mail.mil)
                                                      U.S. Army Research Laboratory
                                                   Aberdeen Proving Ground, MD, USA
                              Abstract                                   a more realistic view of human strategy selection based on a
                                                                         large body of empirical evidence, and argues for use of be-
   Game Theory -based decision aids have been successfully em-
   ployed in real-world policing, anti-terrorism, and wildlife con-      havioral/cognitive models to predict human behavior.
   servation efforts (Tambe, Jiang, An, & Jain, 2013). Cognitive            Cognitive modeling is a method for predicting behavior
   modeling, in concert with model tracing and dynamic parame-           based on known cognitive processes and biases. Computa-
   ter fitting techniques, may be used to improve the performance
   of such decision aids by predicting individual attacker behav-        tional cognitive models take the form of software that simu-
   ior in repeated security games. We present three simulations,         lates human decisions on a given task. Computational cogni-
   showing that (1) cognitive modeling can aid in greatly improv-        tive models have been employed to account for game play in
   ing decision-aid performance in the security domain; and (2)
   despite the fact that individual attackers will differ in initial     Prisoner’s dilemma (Lebiere, Wallach, & West, 2000), rock-
   preferences and in how they learn, model parameters can be            paper-scissors (West, Lebiere, & Bothell, 2006), and a col-
   adjusted dynamically to make useful predictions for each at-          laborative foraging Geo Game (Reitter & Lebiere, 2011).
   tacker.
                                                                            In this paper, computational cognitive models are em-
   Keywords: cognitive modeling; game theory; behavioral
   game theory; strategy selection; agent simulation; model trac-        ployed to predict human behavior in security games. The rest
   ing                                                                   of this paper describes a normative game theory approach to
                                                                         decision-making in the security domain, and suggests an al-
                           Introduction                                  ternative approach that employs cognitive modeling for se-
Game Theory (GT) focuses on mathematical models of ra-                   lecting the best strategy in response to individual attacker’s
tional decision-making. In recent years, GT-based decision-              evolving preferences. We present three simulations highlight-
aiding software has received significant attention for success           ing the advantages of using cognitive modeling over norma-
in real-world security domain problems, such as scheduling               tive game theory, and examine the use of model tracing and
patrols conducted by the US Coast Guard at multiple major                dynamic parameter fitting for predicting individual attacker’s
US ports (Shieh et al., 2012), scheduling police patrols at ma-          strategy selection.
jor airports such as LAX (Pita et al., 2008), allocating federal
air marshals on flights of US Air Carriers and several other                  Game Theory Approach to Decision Aids
applications (Tambe, 2011; Tambe et al., 2013). Success of               Tambe et al. (2013) describe several successful applications
GT approaches can be further improved by dropping the as-                of game theory decision aid software in real-world security
sumption that humans are optimally rational decision-makers,             games. Airport security, coast guard, and police officers em-
and by using cognitive modeling to predict adversary strategy            ploy this software to patrol for criminal activity. Animal
selection.                                                               preservation patrols are aided with this software in their ef-
   Humans are not perfectly rational, rather, we are boundedly           forts to control poaching. Tambe et al. (2013) focus on Stack-
rational (Simon, 1972). Our ability to make optimal deci-                elberg security games where the defender must perpetually
sions is limited by available information, available time, and           defend a set of targets with limited resources, and the attacker
a myriad of cognitive constraints. There is a body of litera-            can choose to attack a given target after observing defender
ture describing biases in human decisions (e.g., Kahneman,               actions. The general idea of picking an optimal mix of actions
2011), cultural preferences (e.g., Sample, 2015), and cogni-             (i.e. mixed strategy) for the defender so as to decrease the
tive process interactions (e.g., Anderson, 2007) that can aid in         chances of a successful attack applies across a much wider
predicting attacker decisions in real-world security problems.           context (e.g., sports, cyber security, anti-terrorism).
   Behavioral game theory is a modification of rational game                Game theory suggests that the defender’s mixed strategy
theory informed by, “experimental evidence and psychologi-               should be a distribution of actions that removes any incen-
cal intuition” (Camerer, 2003, p. 465). Ultimately, the goal             tive for the opponent to choose one action over another. For
of behavioral game theory is to predict behavior and inform              example, imagine a simple game where there are only two
decisions in real-world strategic situations (Gächter, 2008).            possible actions for the defender, D1 and D2, and two pos-
Whereas the success of normative game theory in security                 sible actions for the attacker, A1 and A2. Let us assume that
domain comes from providing efficient randomization of se-               attacker payoffs are probabilities of a successful attack, these
curity plans and processes, behavioral game theory provides              probabilities/payoffs being as listed in Table 1 (when the de-
                                                                     2405

fender chooses D1, the probabilities of attacker success for             Cognitive models are typically held to account for behav-
actions A1 and A2 are .2 and .6, respectively; when the de-           ior in the aggregate (average group behavior) rather than for
fender chooses D2 these probabilities are .5 and .3, respec-          individual differences. However, even in the absence of pre-
tively). In this scenario, if the defender always chose D1, a         cise individual predicitons, general behavioral tendencies can
rational attacker would always play A2, winning 60% of the            be helpful in predicting likely behavior. This is not differ-
time. If the defender always chose D2, the attacker would             ent from predicting large-scale events in physical sciences:
always play A1, winning 50% of the time. If the defender              fundamental principles of physics may not help us to predict
played randomly, the attacker would have the incentive to al-         exactly where a tsunami will hit, but it is useful to know that
ways play A2, winning 45% of the time.1 A GT-computed                 some locations are more probable than others.
mixed strategy in this game would be for the defender to play            Additionally, in repeated security games2 cognitive models
D1 one third of the time, and D2 two thirds of the time. This         can be dynamically updated to provide better predictions of
would leave the attacker with no preference for either option:        individual attacker’s cognition and behavior by use of model
playing either A1 or A2 would only lead to a successful attack        tracing and dynamic parameter fitting. The model trac-
40% of the time.                                                      ing technique comprises force-feeding a participant’s expe-
                                                                      riences to the cognitive model. That is, if the participant and
Table 1
                                                                      the model were to choose different strategies, model actions
Sample payoffs for the attacker in a security game, where
                                                                      would be overwritten with participant actions in the model’s
there are only two possible actions for the defender, D1 and
                                                                      memory. This method was employed in computerized in-
D2, and two possible actions for the attacker, A1 and A2.
                                                                      structional aids, “cognitive tutors”, for students learning high
                                A1 A2
                                                                      school math in Pittsburgh (Anderson, Corbett, Koedinger,
                         D1 .2         .6
                                                                      & Pelletier, 1995).Dynamic parameter fitting is used to ad-
                         D2 .5         .3
                                                                      just model parameters based on known data points, so as to
                                                                      make better individual predictions for future behavior. This
   In real world security games there are many more ac-               method was employed to predict performance of individual
tions, and payoffs must take into account much more than              F-16 pilot teams (Jastrzembski, Gluck, & Rodgers, 2009) and
success/failure. For example, Kar, Fang, Fave, Sintov, and            is employed in software that predicts optimal training sched-
Tambe (2015) describe a scenario where the attacker, an ani-          ules based on individual performance histories (Jastrzembski,
mal poacher, is drawn not only by the success of a poaching           Rodgers, Gluck, & Krusmark, 2014). The following simula-
effort, but also by animal density and travel time. That is,          tions examine the use of these techniques for defender agent
when humans play the security game as an attacker (i.e. an-           software in the security domain.
imal poacher) they are more likely to choose an action that
leads to less travel time and higher animal density, even when                       Simulation 1: Model Tracing
the risk of failure (capture) is high.                                A cognitive model can predict general tendencies, but it is
   Although animal density and travel distance are character-         unlikely that a model will predict all decisions of a given in-
istics of the task environment, these factors are also latent in-     dividual, even on fairly simple tasks. Model predictions for
dicators of cognitive biases. Direct consideration of such cog-       each of the attacker’s decisions contain an element of uncer-
nitive biases and limitations in formal attacker models should        tainty, X. This simulation explores CM-based agent perfor-
improve the performance of decision aids in the security do-          mance for varying sizes of X, and compares CM performance
main.                                                                 to a normative game theory approach.
                                                                         For this simulation we employ a sample repeated security
             Cognitive Modeling Approach                              game where the attacker and defender have four strategies
There are many computational models that provide robust               each, and attacker payoffs are probabilities of attacker suc-
predictions of human behavior. For example, models of re-             cess as represented in Table 2. Given this payoffs matrix, if
inforcement learning provide robust accounts of human trial-          the defender played a fixed strategy (e.g., always play D2),
and-error behavior and its neural correlates (e.g., Anderson,         the attacker could find a corresponding strategy that would
2007; Fu & Anderson, 2006; Holroyd & Coles, 2002; Nason               win 90% of the time (e.g., if defender always plays D2, then
& Laird, 2005); models of declarative memory provide robust           attacker should always play A3). If the defender was equally
predictions of fact recall latency and probability (e.g., An-         likely to choose any action, the attacker could optimize by
derson, 2007; Anderson & Reder, 1999; Mackintosh, 1983;               always playing A1, winning 52.5% of the games. A defender
Shanks, 1994); and skill acquisition models provide robust            agent based on GT would employ a mixed strategy, choosing
predictions of how people achieve experise (e.g., Chase &             actions D1, D2, D3, and D4 with probabilities .275, .240,
Simon, 1973; Gobet, 1998; Gobet et al., 2001). Furthermore,
                                                                          1
multiple individual process models can be integrated together               Playing A1 against a random opponent would have a 35%
                                                                      chance of winning, and playing any mix of A1 and A2 would have a
to generate more complete and general predictions of behav-           chance of winning that is between 35% and 45%.
ior across many contexts (Anderson, 2007; Choi & Ohlsson,                 2
                                                                            Repeated security games differ from "one shot" security games
2011; Gray, 2007; Veksler, Myers, & Gluck, 2014).                     in that the attacker attempts multiple attacks in sequence.
                                                                  2406

Table 2                                                            the attacker has no prior preferences for any actions. Holding
Payoffs for the attacker in a security game used for Simula-       these variables constant allows us to answer what the effect
tions 1-3, where there are four possible actions for the de-       of X will be – the lack of predictability in human decision-
fender {D1,D2,D3,D4}, and four possible actions for the at-        making.
tacker {A1,A2,A3,A4}.                                                 For general population X is often estimated as gaussian
                                Attacker                           noise with a mean of 0 and a standard deviation, σ, that varies
                          A1 A2 A3 A4                              between 5% and 25% of maximum reward values. We ran the
                    D1 .15 .45 .50 .90                             simulation for σ values of .05, .15, and .25, averaging over
               Defender
                    D2 .55 .10 .90 .45                             1024 simulation runs per parameter setting. The results of
                    D3 .50 .90 .15 .45                             this simulation, displaying the number of prevented attacks
                    D4 .90 .50 .50 .10                             over the course of 200 consecutive security games, are shown
                                                                   in Figure 1.
.275, and .210, respectively. This mixed strategy would leave
the attacker without a preference, where any given action
would have a 50% probability of success.
   However, humans are not perfectly rational, and human at-
tacker action preferences will change based on their experi-
ence. For example, if the attacker chooses A1 and happens to
lose, they will be less likely to choose A1 in future attacks,
regardless of whether A1 is ultimately a good choice. Con-
versely, if the attacker chooses A1 and happens to win, they
will be more likely to choose A1 in future attacks, regardless     Figure 1. Simulation 1: Predicted advantages of using cogni-
of whether A1 is ultimately a poor choice.                         tive models in security games, given various levels of decision
   More formally, after performing some action, A, the ex-         predictability. σ refers to uncertainty in the human attacker
pected utility of this action, U A , is incremented by the fol-    decisions, rather than in CM defender.
lowing term:
                       ∆U A = α(R − U A ),                  (1)       For all σ values in this simulation GT prevented 50.0%
where α is the learning rate, and R is the value of the feed-      of the attacks (100.0 attacks prevented in 200 consecutive
back (e.g. success/failure, reward/punishment). This type of       attempts). Depending on the predictability factor, CM pre-
learning (error-driven reinforcement learning) is a very robust    vented between 121.5 and 153.6 attempts on average (22%-
principle of biological brains (e.g., Anderson, 2007; Bayer &      54% more than GT). In addition to GT and CM, Figure 1 in-
Glimcher, 2005; Fu & Anderson, 2006; Kable & Glimcher,             cludes predicted baseline performances against random and
2009).3 The action chosen at each decision point is one with       fixed-strategy defenders. Predictably, a fixed-strategy de-
the highest value of the term U + X, where X represents ex-        fender does worst, losing in 75-90% of the games after about
ploratory tendencies plus all the unknown factors driving hu-      10 initial games. Less obvious may be the fact that random-
man decision-making at the given moment.                           strategy agent performs almost as well as GT, winning on av-
                                                                   erage 49.6% of the games, compared to 47.5% that we may
   From the perspective of predicting attacker actions,
                                                                   have predicted against a perfectly rational agent.
decision-making can be thought of as a stochastic process,
                                                                      The conclusion to draw from these simulation results is that
where the action with the highest U is the most likely to be se-
                                                                   humans are not normative decision makers, nor are we com-
lected. A defender agent based on CM would employ model
                                                                   pletely unpredictable; thus an approach that considers human
tracing to keep track of U values for each attacker action, and
                                                                   cognition can perform better than normative GT. We can em-
then employ this knowledge to outguess the attacker. That
                                                                   ploy model tracing to improve defender performance in the
is, once each game plays out, and actual attacker action in
                                                                   security domain despite the fact that many attacker actions are
that game, A, and outcome of the game, R, are both known,
                                                                   not perfectly predictable. Finally, the less uncertainty there is
the model of the attacker can be updated in accordance with
                                                                   in attacker behavior, the more attacks can be prevented. Thus,
Equation 1. For each consecutive game, CM would predict
                                                                   as we begin to account for a greater proportion of attacker
that the most likely attacker action is one with highest U, and
                                                                   cognition we can reduce the size of the X factor, and further
makes a corresponding best choice (e.g., if the attacker in this
                                                                   improve defender performance.
simulation is likely to choose A2, defender will choose D3).
   Let us assume, for now, that we know attacker learning              3
                                                                         To be clear, the mechanism being described here, Reinforce-
rate (in this simulation we assume α = .2, as is the default in    ment Learning, is only one of many cognitive processes that guides
the ACT-R cognitive architecture; Anderson, 2007), and their       attacker behavior. In this paper we only focus on Reinforcement
                                                                   Learning in repeated security games as a clear and tractable exam-
perceived rewards for success and failure (in this simulation      ple of how CM can aid in building better decision aids for real-world
we assume R = 1 for each win, and R = −1 for each loss), and       security domain problems.
                                                               2407

          Simulation 2: Preferred Strategies
Simulation 1 highlights the advantages of cognitive model
predictions despite the uncertainty factor, X, in human
decision-making. To isolate the effect of X we made a few
assumptions, one of those being that the attacker has no ini-
tial action preferences. However, the four attacker strategies
described in Simulation 1, Table 2 are not some arbitrarily
named buttons A1-A4, but rather meaningful action-paths to
the person(s) performing the attacks.
   Let us assume, for example, that in the context of a cyber-
attack, the hacker has two decisions: (1) whether to scan for
vulnerabilities at a faster or a slower rate, and (2) whether
to focus the attack on the main data server or on multiple
perimeter machines. The hacker in this case has some risk
aversion and believes that faster scans and attacking the main
server present higher risks, resulting in initial perceived utili-
ties for the safer-perimeter, safer-main, faster-perimeter, and
faster-main options of +.10, +.05, 0.00, and -.10, respec-
tively. We will refer to these initial utilities as preference
set A, when they correspond to actions A1, A2, A3, and A4,
respectively; and preference set B when they correspond to
actions A4, A3, A2, and A1, respectively.
   The question is, given that the attacker has some prior pref-
erences, does it hurt the defender to assume that the attacker is
a “blank slate”? Simulation revealed that CM would prevent
about the same number of attacks against attackers with pref-
erence sets A or B as it would against blank slate attackers,
see top of Figure 2. At worst, against an attacker with pref-          Figure 2. Simulation 2: Predicted CM performance when at-
erence set B, σ = .05, CM prevents 2.3 attacks less in 200             tacker has biases prior to the first attack. Each trend-line rep-
attempts than against a blank-slate, σ = .05 attacker. The             resents CM performance against a different attacker type (at-
difference for each other attacker type is less than one attack        tackers differ by uncertainty factors and initial preferences).
in 200 attempts. GT agent performance remains at 50% re-               Preference sets A, B, C, and D are represented in figure leg-
gardless of attacker preferences.                                      ends as (A), (B), (C), and (D) respectively. Each data point
   The reason why initial preference sets A and B do not               represents an average over 1024 simulation runs.
greatly disturb the model tracing approach has to do with the
nature of error-driven learning (Rescorla & Wagner, 1972).
This long-established principle of human learning suggests             This would throw off model predictions. A CM defender
that the more surprising (i.e. unexpected) a given outcome             would still perform better than GT against attackers with such
is, the greater the change in the human (or simulated) brain.          preferences, but much worse than it would against attackers
Thus, if human subjective utility for some action is 0.5 and           with no initial preferences, see middle graph in Figure 2.
the model assumes that utility to be 0.0, and the actual out-
come value after that action was performed was 1.0, the                   To account for potential extreme negative preferences, we
change in action-utility in the model would be half of that            can add Initial Utility Decay (IUD), dynamically adjusting
in the human. After just a few experiences the model and               initial action-utilities for any actions that are not being at-
human action-utilities would begin to converge, and model              tempted by the attacker. The assumption is that if the attacker
predictions would become more accurate.                                is not choosing a given option, then that option must have a
                                                                       lower subjective utility for them. For current simulation pur-
   A problem may occur in the instances where initial hu-
                                                                       poses we will employ linear decay, decrementing U A by 0.05
man preferences are strong enough that some actions are
                                                                       at each decision point for every action A that the attacker does
never even attempted. For example, let us examine prefer-
                                                                       not exercise.
ence sets C and D that are five times as strong as preference
sets A and B, with initial perceived utilities for the safer-             The results of employing a CM defender agent with initial
perimeter, safer-main, faster-perimeter, and faster-main op-           utility decay are shown at bottom of Figure 2. As the figure
tions of +.50, +.25, 0.00, and -.50, respectively. Given these         suggests, initial utility decay greatly aids in accounting for
preference sets, an attacker (especially one with a low X fac-         strong initial preferences, resulting in almost no difference
tor) will be very unlikely to choose the faster-main option.           (less than 1 attack in 200 attempts) between defending against
                                                                   2408

blank-slate attackers and those with preference sets C and D.         23 attacks in 200 attempts; though assuming a higher attacker
This improvement comes at a slight cost against a blank-slate         learning rate does not hurt CM defender performance as much
attacker. CM performance with IUD is slightly worse than              as assuming a lower learning rate.
that without IUD against a blank-slate attacker (red lines at            The “auto” learning rate (right-most column) represents a
bottom and top of Figure 2, respectively), preventing 1.3, 0.4,       CM defender that dynamically adjusts the learning rate pa-
and 0.4 attacks less in 200 attempts against attackers with σ         rameter prior each decision. That is, the “auto” CM agent
values of .05, .15, and .25, respectively.                            adjusts its assumption about the attacker learning rate de-
   IUD is just one potential method for dynamic parameter             pending on which assumed learning rate would result in a
adjustment in cognitive modeling. There are undoubtedly               greater number of correct predictions for all known attacker
better alternatives to overcoming the problem of initial pref-        decisions. As implemented in this simulation, the “auto”
erences other than linear IUD. The focus here is not in find-         agent contrasts prediction history of CM agents with assumed
ing the optimal method, but rather in highlighting the bene-          learning rates of .1, .2, .3, and .4, and mimics the next deci-
fits of using cognitive modeling in the security domain. This         sion of the agent with the best prediction rate.
simulation suggests that even when individual attackers have             Dynamically fitting the learning rate parameter in this way
unpredictable initial preferences, CM preferences can be ad-          does not guarantee that the attacker learning rate will be cor-
justed dynamically without incurring a significant loss in per-       rectly inferred at any given decision point, mostly due to the
formance.                                                             uncertainty in attacker decisions. Thus, the “auto” CM per-
                                                                      formance cannot be as good as that of an omniscient agent.
              Simulation 3: Learning Rate                             However, the “auto” learning rate produces near-optimal per-
Simulations 1 and 2 explore how CM-based defender agent               formance, see Figure 3.
performance in repeated security games is affected by deci-
sion unpredictability and initial preferences of human attack-
ers. One other variable from Equation 1 that we have yet to
discuss is the learning rate, α. This simulation focuses on CM
performance given different attacker learning rates.
Table 3
Number of attacks prevented in 200 attempts by CM defender
(IUD=.05). Each data point represents an average over 1024
simulations. α and σ refer to learning rate and uncertainty
in the human attacker decisions. GT-based defender perfor-
mance for all attackers is 100.0.
                                   CM learning rate                   Figure 3. Simulation 3: Best, worst, and auto performance
                        0.1      0.2      0.3      0.4      auto      from Table 3. α and σ refer to learning rate and uncertainty
                                                                      in the human attacker decisions, rather than in CM defender.
  α=0.1, σ=0.05        139.0    140.8    138.1   136.0     140.4
  α=0.2, σ=0.05        139.4    152.5    152.3   151.6     151.9         To be clear, the “auto” learning rate method employed
  α=0.3, σ=0.05        133.5    152.8    155.0   154.8     154.5      here is not the only method for adjusting model parame-
  α=0.4, σ=0.05        131.6    150.1    154.4   154.7     153.9      ters. The focus here, however, is not on finding the optimal
  α=0.1, σ=0.15        117.3    117.6    117.5   116.4     117.1      method for dynamic parameter fitting, but rather on highlight-
  α=0.2, σ=0.15        130.9    133.0    133.0   132.2     132.3      ing the availability of techniques in cognitive modeling, such
  α=0.3, σ=0.15        136.0    140.8    141.3   141.6     140.8      as model tracing and dynamic parameter fitting, for provid-
  α=0.4, σ=0.15        136.1    142.6    144.4   144.4     143.9      ing individual/team -tailored decision predictions that can be
  α=0.1, σ=0.25        110.6    110.8    110.5   109.8     110.4      of great use in the security domain and beyond.
  α=0.2, σ=0.25        120.5    121.6    121.5   121.4     121.3
  α=0.3, σ=0.25        127.4    129.9    130.5   130.3     129.6                        Summary & Discussion
  α=0.4, σ=0.25        131.3    134.6    135.9   136.0     135.1      In recent years, game theory -based decision-aid software has
                                                                      received significant attention for success in real-world secu-
   Table 3 displays CM (IUD=.05) performance with differ-             rity domain problems, such as scheduling patrols conducted
ent assumed learning rates, playing 200 consecutive security          by the US Coast Guard and police, allocating federal air mar-
games against attackers with different actual learning rates.4        shals on flights, and major anti-poaching efforts. Behavioral
In general, the higher the attacker learning rate (i.e., the more     game theory points out that normative approaches provide un-
of a factor their experiences are in their decision-making),          realistic predictions of human choice, and suggests the use of
the easier it is to predict attacker decisions and prevent future         4
                                                                            Results in Table 3 and Figure 3 are based on games against a
attacks. Depending on the attacker, the range of differences          blank-slate agent, but all effects hold against agents with initial pref-
between the best and worst CM performance was as high as              erences.
                                                                  2409

behavioral/cognitive models. In this paper we focus on the                         human learning. Trends in Cognitive Sciences, 5(6), 236–
use of cognitive modeling to improve on the success of nor-                        243.
                                                                           Gray, W. D. (Ed.). (2007). Integrated models of cognitive systems.
mative game-theory approaches in the security domain.                              New York: Oxford University Press.
   We present three simulations that highlight the potential               Holroyd, C. B., & Coles, M. G. H. (2002). The neural ba-
advantages of employing cognitive models for predicting at-                        sis. of human error processing: Reinforcement learning,
                                                                                   dopamine, and the error-related negativity. Psychological Re-
tacker decisions. The simulations suggest that (1) cognitive                       view, 109(4), 679–709.
modeling provides performance advantages over normative                    Jastrzembski, T. S., Gluck, K. A., & Rodgers, S. (2009). Improving
game theory approaches, and (2) model parameters can be                            military readiness: A state-of-the-art cognitive tool to predict
                                                                                   performance and optimize training effectiveness. In The in-
adjusted dynamically to make useful predictions about each                         terservice/industry training, simulation, and education con-
individual human attacker despite the fact that each individual                    ference (i/itsec).
attacker may have different preferences and learning abilities.            Jastrzembski, T. S., Rodgers, S. M., Gluck, K. A., & Krusmark,
                                                                                   M. A. (2014). Predictive performance optimizer. Google
   The presented simulation results provide encouraging evi-                       Patents.
dence of potential usefulness of cognitive models in the con-              Kable, J. W., & Glimcher, P. W. (2009). The Neurobiology of Deci-
text of real-world security problems. Despite the fact that                        sion: Consensus and Controversy. Neuron, 63(6), 733–745.
                                                                                   doi: 10.1016/j.neuron.2009.09.003
simulations in this paper are based on robust behavioral phe-              Kahneman, D. (2011). Thinking, fast and slow. Macmillan.
nomena, the presented results should be taken as theoretical               Kar, D., Fang, F., Fave, F. D., Sintov, N., & Tambe, M. (2015).
in nature, requiring further empirical validation. In future                       A Game of Thrones: When Human Behavior Models Com-
                                                                                   pete in Repeated Stackelberg Security Games. In 2015 inter-
work we plan to gather human data and validate current sim-                        national conference on autonomous agents and multiagent
ulation results.                                                                   systems (pp. 1381–1390). International Foundation for Au-
   In the current paper we only focus on a single cognitive                        tonomous Agents and Multiagent Systems.
                                                                           Lebiere, C., Wallach, D., & West, R. L. (2000). A memory-based
process, as an example of how cognitive modeling may be
                                                                                   account of the prisonerâĂŹs dilemma and other 2x2 games.
employed in this domain. There is a wide array of established                      In Proceedings of international conference on cognitive mod-
cognitive models beyond what we could explore in this paper.                       eling (pp. 185–193).
Integration of more models in CM-based decision aids would                 Mackintosh, N. J. (1983). Conditioning and associative learn-
                                                                                   ing. Oxford, New York: Clarendon Press, Oxford University
help in reducing the uncertainty factor, further improving the                     Press.
rate of prevented attacks.                                                 Nason, S., & Laird, J. E. (2005). Soar-RL: Integrating reinforcement
                                                                                   learning with Soar. Cognitive Systems Research, 6, 51–59.
                                                                           Pita, J., Jain, M., Ordónez, F., Portway, C., Tambe, M., Western,
                     Acknowledgements                                              C., & Kraus, S. (2008). ARMOR Security for Los Ange-
This work was funded under Cooperative Agreement Number                            les International Airport. In Twenty-third aaai conference on
                                                                                   artificial intelligence (pp. 1884–1885).
W911NF-09-2-0053.                                                          Reitter, D., & Lebiere, C. (2011). Towards cognitive models of
                                                                                   communication and group intelligence. In Proceedings of the
                           References                                              33rd annual meeting of the cognitive science society, boston
Anderson, J. R. (2007). How can the human mind occur in the                        (pp. 734–739).
       physical universe? Oxford ; New York: Oxford University             Rescorla, R. A., & Wagner, A. R. (1972). A theory of Pavlovian
       Press.                                                                      conditioning: Variations in the effectiveness of reinforcement
Anderson, J. R., Corbett, A. T., Koedinger, K. R., & Pelletier,                    and nonreinforcement. In P. W. F. Black AH (Ed.), Classi-
       R. (1995). Cognitive Tutors: Lessons Learned. The                           cal conditioning ii: Current research and theory (pp. 64–99).
       Journal of the Learning Sciences, 4(2), 167–207. doi:                       New York: Appleton Century Crofts.
       10.1207/s15327809jls0402_2                                          Sample, C. (2015). Cyber + Culture Early Warning Study (Tech.
Anderson, J. R., & Reder, L. M. (1999). The fan effect: New re-                    Rep.). CERT. doi: CMU/SEI-2015-SR-025
       sults and new theories. Journal of Experimental Psychology-         Shanks, D. (1994). Human associative learning. In N. J. Mackin-
       General, 128(2), 186–197.                                                   tosh (Ed.), Animal learning and cognition. San Diego, CA:
Bayer, H. M., & Glimcher, P. W. (2005). Midbrain dopamine                          Academic Press.
       neurons encode a quantitative reward prediction error signal.       Shieh, E., An, B., Yang, R., Tambe, M., Baldwin, C., DiRenzo, J.,
       Neuron, 47(1), 129–141.                                                     . . . Meyer, G. (2012). Protect: A deployed game theoretic
Camerer, C. (2003). Behavioral Game Theory: Experiments in                         system to protect the ports of the united states. In 11th in-
       Strategic Interaction.                                                      ternational conference on autonomous agents and multiagent
Chase, W. G., & Simon, H. A. (1973). Perception in chess. Cogni-                   systems (pp. 13–20).
       tive psychology, 4(1), 55–81.                                       Simon, H. A. (1972). Theories of bounded rationality. In
Choi, D., & Ohlsson, S. (2011). Effects of multiple learning mecha-                C. B. McGuire & R. Radner (Eds.), Decision and organiza-
       nisms in a cognitive architecture. In L. Carlson, C. Hoelscher,             tion (pp. 161–176). msterdam: Elsevier.
       & T. F. Shipley (Eds.), Proceedings of the thirty-third annual      Tambe, M. (2011). Security and Game Theory (Vol. 9781107096).
       meeting of the cognitive science society (pp. 3003–3008).                   doi: 10.1017/CBO9780511973031
       Boston, MA: Cognitive Science Society.                              Tambe, M., Jiang, A. X., An, B., & Jain, M. (2013). Computational
Fu, W. T., & Anderson, J. R. (2006). From recurrent choice to                      game theory for security: Progress and challenges. In Aaai
       skilled learning: A reinforcement learning model. Journal of                spring symposium on applied computational game theory.
       Experimental Psychology: General, 135(2), 184–206.                  Veksler, V. D., Myers, C. W., & Gluck, K. A. (2014). SAwSu: An
Gächter, S. (2008). Behavioral Game Theory. In Blackwell hand-                     Integrated Model of Associative and Reinforcement Learn-
       book of judgment and decision making (pp. 485–503).                         ing. Cognitive Science, 38(3), 580–598.
Gobet, F. (1998). Expert memory: a comparison of four theories.            West, R. L., Lebiere, C., & Bothell, D. J. (2006). Cognitive archi-
       Cognition, 66(2), 115–152.                                                  tectures, game playing, and human evolution. Cognition and
Gobet, F., Lane, P. C. R., Croker, S., Cheng, P. C. H., Jones, G.,                 multi-agent interaction: From cognitive modeling to social
       Oliver, I., & Pine, J. M. (2001). Chunking mechanisms in                    simulation, 103–123.
                                                                       2410

