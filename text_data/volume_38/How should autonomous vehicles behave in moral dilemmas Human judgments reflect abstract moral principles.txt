             How should autonomous vehicles behave in moral dilemmas? Human
                                    judgments reflect abstract moral principles
                                             Derek Powell1 (derekpowell@ucla.edu)
                                            Patricia Cheng1 (cheng@lifesci.ucla.edu)
                                  1
                                    Department of Psychology, University of California, Los Angeles,
                                                      Los Angeles, CA 90095 USA
                           Michael R. Waldmann (michael.waldmann@bio.uni-goettingen.de)
                                          Department of Psychology, University of Göttingen
                                               Gosslerstr. 14, 37073 Göttingen, Germany
                              Abstract                                 will have to decide how to mitigate the harm caused by
   Self-driving autonomous vehicles (AVs) have the potential to        these situations in morally appropriate ways. Though they
   make the world a safer and cleaner place. A challenge               are by no means easily solved, navigation, steering, and
   confronting the development of AVs is how these vehicles            object detection and avoidance are relatively well-defined
   should behave in traffic situations where harm is unavoidable.      problems. That is, it is clear what constitutes success: for
   It is important that AVs behave in ethically appropriate ways       example, arriving at the correct destination, staying on the
   to mitigate harm. Ideally, they should obey a system of             correct side of the road, and braking before hitting a
   principles that both concur with human moral judgments and          pedestrian, respectively. In contrast, moral judgments are
   are ethically defensible. Here we compare people’s moral            sometimes ill-defined: individuals may differ in their moral
   judgments of AV programming with their judgments about              judgments, and there is considerable disagreement within
   the behavior of human drivers, with the goal of beginning to        the field of ethics over how judgments should be made.
   identify such principles. As many debates within ethics                Traffic conditions may pose difficult choices similar to
   remain unresolved, empirical investigations like ours may           the classic Trolley dilemma. In the Trolley or Switch
   guide the development of ethical AVs (Bonnefon et al., 2015).       dilemma, a runaway Trolley threatens to kill five men
   In addition, people’s judgments about the behavior of AVs           working on a track unless it is redirected toward a side track
   may serve as a window into the abstract principles people           with only one person working. The situation poses a
   apply in their moral reasoning.                                     dilemma: maximizing utility (saving the five) requires
                                                                       violating a moral rule (harming another person). Faced with
Keywords: Ethics; Moral Judgment; Robotics                             the Switch dilemma, people generally make utilitarian
                                                                       judgments. The majority of people (85%; Hauser et al.,
                          Introduction                                 2007) judge that it is morally acceptable to flip the switch,
A number of auto manufacturers and tech companies are                  redirecting the train away from the five and toward the one.
working to develop self-driving autonomous vehicles (AV).                 Vehicles in traffic might face dilemmas like the Switch
These developments hold great promise: creating safer roads            dilemma when mechanical failures or road conditions
(Gao et al., 2014), alleviating traffic congestion (Van Arem           prevent them from stopping or when other drivers act
et al., 2006), reducing pollution (Spieser et al., 2014), and of       unpredictably. Should AVs be programmed to sacrifice lives
course removing the tedious burden of driving through                  if this produces better consequences overall? Bonnefon et al.
traffic. To be effective, autonomous vehicles must overcome            (2015) presented participants with a switch-like scenario
a number of challenges: they must navigate to their                    where an unavoidable deadly collision was about to occur
destinations, steer properly to remain on roadways, and,               with a group of 10 pedestrians unless an AV turned toward a
perhaps most challengingly, identify objects and predict               single pedestrian. Participants approved of the AV turning
movements of pedestrians and other vehicles to avoid                   to kill the one at rates very similar to those for human
collisions. These challenges are rapidly being overcome,               drivers. Participants were also generally willing to allow
and experimental AVs like the Google Car have                          AVs to sacrifice the lives of their passenger by swerving to
successfully driven for thousands of miles on public                   collide with a wall rather than a group of pedestrians. These
roadways (Waldrop, 2015).                                              researchers conclude that people are willing to accept AVs
   Inevitably, AVs will face decisions with moral                      programmed for utilitarian sacrificial behaviors in at least
consequences—situations where an impending collision                   some circumstances.
may injure or even kill human drivers or pedestrians. AVs                 However, there are some moral situations where people
                                                                       refuse to sacrifice even when it would create the best
                                                                   307

outcomes. For instance, although the vast majority of people        both the killing of the one on the side track and the saving of
approve of sacrificing one to save five in the classic Trolley      the five on the main track, highlighting both of these
dilemma, in the very similar Footbridge or Push dilemma a           outcomes. In contrast, pushing the man off the bridge in
majority refuse to intervene. In the Push dilemma, a                order to stop the trolley represents a causal chain structure,
runaway trolley threatens to kill five workmen on a track. A        and focuses subjects initially on the fate of the one victim.
large man is standing on a footbridge over the track, and the          This account leaves open the question of why causal
only way to save the five workmen is to push this man off           models should be morally relevant. One possible answer,
the footbridge so that his body will stop the trolley.              suggested by Kamm (2015), is built on the assumption that
Approximately 88% of people refuse to push the man                  victims are endowed with rights (e.g., the right not to be
(Hauser et al., 2007). Whereas people are sometimes willing         harmed) and causal models highlight inter-victim relations.
to trade-off between moral rules and better outcomes, at            Victims stand in a substitutive relation if their roles in a
other times they are unwilling to do so—even for                    situation could be arbitrarily swapped as, for instance, in the
comparable outcomes. Will people approve of sacrificial             common-cause scenario. Shepard (2008) calls a similar
behaviors from AVs in dilemmas that more closely                    concept the symmetry principle of invariance under
resemble the Push dilemma, in contrast to their judgments           permutation of individuals. Shepard regards this principle as
about AVs facing switch-like traffic problems (Bonnefon et          a necessary overarching constraint for moral acts: An act is
al., 2015)?                                                         morally acceptable to such an extent as it would remain
   As it is impossible to anticipate all possible traffic           acceptable if individuals in a situation were permutated into
scenarios in which an AV might find itself, AVs will need           different roles. In the trolley dilemma, sacrificing one to
to respond to novel traffic situations according to abstract        save five satisfies this criterion.
principles. Human moral judgments do not appear to                     In contrast, in a causal chain scenario such as the
perfectly adhere to any simple normative proposal, and              footbridge, victims are in what Kamm (2015) terms a
researchers have offered divergent accounts of the principles       subordinative relation. Cause and effect, unlike effects of a
underlying their judgments. For instance, some have argued          common cause, are asymmetrical and not arbitrarily
that differences in Push and Switch judgments owe to                substitutable. Here Kamm argues that it violates our
affective reactions to the use of personal force (e.g. Greene       understanding of human rights to harm a person as a means
et al., 2001) whereas others have argued that these                 of a later occurring greater good. Thus, the substitutability
judgments are the result of sophisticated deontological rules       of agents’ roles in the common-cause scenario, but not the
like the “doctrine of double effect” (DDE), which prohibits         causal chain scenario, could provide a justification for the
intentionally harmful actions, but may permit actions that          seemingly opposite moral intuitions in a Switch and a Push
produce a greater good for which harm is a foreseen but             dilemma.
unintended consequence (Mikhail, 2011; Hauser et al.,                  The importance of causal structure (or likewise, of
2007). It is unclear how these proposals would apply to             symmetry or substitutability) suggests that people’s moral
programmed AVs—it doesn’t seem as if such programming               judgments may be influenced by the level of abstraction or
involves “personal force” and the role of intentions is rather      concreteness at which they consider a situation. The Switch
murky in this context. Can AVs even be said to have                 case represents an extreme—a rare case of perfect
intentions in the sense of other moral agents?                      symmetry. In contrast, for situations with more complex
   Nevertheless, the development of moral AVs will require          causal structures—essentially any case that is not a simple
us to craft a system of principles that both satisfies human        artificial dilemma—there are many avenues by which
moral judgments in most cases and that is ethically                 varying degrees of asymmetry might be produced. If the
defensible—even if imperfect.                                       intervention in the Switch scenario is considered concretely,
   Waldmann and colleagues (Waldmann & Dieterich, 2007;             the perfect symmetry of the scenario licenses the sacrifice of
Wiegmann & Waldmann, 2014) have proposed that the                   the one to save the five. However, if we consider the
causal structure of a moral situation is an important               intervention in the abstract, other scenarios would be
determinant of people’s moral judgments. Their theory is            possible, and few of these will have perfectly symmetrical
related to the DDE, which claims that intentionality is             causal structures. On this account, encouraging abstract
inferred on the basis of the causal structure underlying the        consideration of the dilemma may reduce approval for
moral dilemma. However, rather than the intention of the            sacrificial actions. Making moral judgments about the
agent, in Waldmann et al’s theory it is the causal structure        programming of autonomous vehicles should encourage this
itself that drives moral intuitions. According to their theory,     sort of abstraction by introducing the possibility of other
the locus of interventions in a causal system influences the        causal scenarios in which this programming will be applied.
attentional focus to different aspects of the moral dilemma,        Thus, in considering moral judgments at a greater level of
which in turn affects moral intuitions. In the switch               abstraction, we might expect people to less strongly endorse
dilemma, flipping the switch acts as a common cause of
                                                                308

sacrificial actions by AVs than in situations in which a           Participants. Participants were 413 workers (237 female,
human drives the vehicle.                                          median age = 32 years old) recruited from Amazon’s
                                                                   Mechanical Turk (mTurk) work distribution website.
                The present experiments                            Workers were paid $0.25 to participate in the study.
   To investigate how evaluating moral dilemmas at
differing levels of abstraction would affect moral                 Materials. The Switch dilemma described a situation where
judgments, our experiments examined people’s moral                 the brakes of a truck have failed and the truck is headed
judgments of specific moral instances involving human              toward a red car with three passengers. The driver (human
drivers as compared with judgments about how AVs should            condition) or the computer system directing the truck (AV
be programmed to behave in these and other similar                 conditions) must choose whether to continue on their
situations. In addition, the wording of the instructions were      present course, killing the three passengers in the red car, or
manipulated to encourage concrete or abstract consideration        to turn into a yellow car waiting at an intersection on a side
of the cases. We predicted that people would less strongly         street with only one passenger. Unfortunately, the truck
approve of sacrificial actions by programmed AVs than by           cannot safely turn off the road onto a sidewalk as they are
human drivers, and when they are encouraged to think               all full of pedestrians who will also be killed.
abstractly, in common-cause scenarios due to the possibility          In the Push dilemmas, a runaway truck is again headed
of the application of this judgment in other causal scenarios.     toward a red car with three passengers. However, in this
However, no differences in judgments were predicted                dilemma the vehicle of interest is a blue car waiting at an
between judgments of AVs and human drivers in causal               intersection behind a yellow car with one passenger. The
chain scenarios because these scenarios are already strongly       driver (human condition) or the computer directing the blue
aversive due to the asymmetrical inter-victim relation             car (AV conditions) must decide whether to push the
implied by the chain structure.                                    waiting yellow car with one passenger from the side street
   A secondary goal of the study was to examine people’s           into the path of the runaway truck, saving the three
moral judgments about AV programming in moral                      passengers in the red car but killing the one in the pushed
dilemmas with varied causal structures. Bonnefon et al.            car. Each dilemma was accompanied by a diagram that
(2015) found that people are at least sometimes willing to         depicted the situation, an example of which is shown in
allow AVs to sacrifice human lives to save the lives of            Figure 1.
others. However, their study only examined trolley-like               The level of abstraction was manipulated between the
cases with a common-cause structure, leaving open                  concrete AV and abstract AV conditions by including
questions about the generality of their findings.                  additional wording in the abstract AV condition
                                                                   highlighting the generality of the principles employed by the
                        Experiment 1                               AV:
Experiment 1 examined participants’ moral judgments for               “… In fact, there are thousands of possible dangerous
dilemmas where a vehicle, either driven by a human or a            traffic situations whose precise characteristics one cannot
driverless AV steered by a computer program, faced an              possibly anticipate. It is therefore crucial that the driverless
unavoidable and deadly collision. We compared a car                car be equipped with a computer program that implements
analog of the Switch and Push dilemma. In addition to              general rules designed to be applicable across a large variety
manipulating the type of vehicle and dilemma, we also
manipulated the abstractness of the problem description and
the phrasing of the judgment probe for the AV condition,
resulting in a concrete AV condition and an abstract AV
condition. The main goal of the experiment was to study
how abstractness of the steering mechanism (a driver facing
a specific situation vs. a general program for the AV) would
affect intuitions about Switch and Push cases.
Design. The experiment followed a 2 x 3 factorial between-           Figure 1: Diagram of traffic situation that was shown to
subjects design (dilemma x abstraction): Participants were           participants for the Push dilemmas. The blue car is either
assigned either the Switch or Push dilemma, and to one of 3          an autonomous vehicle or is driven by a human driver,
levels of abstraction: either the human driver condition, the        and must choose whether to push the yellow car into the
concrete AV condition, or the abstract AV condition.                 path of the truck.
                                                               309

of possible scenarios. The program then directs how the car                             some brief questions about how they made their judgments
should best behave in these critical situations.”                                       and whether they had ever seen the dilemmas before, two
   After reading the dilemmas participants were asked to                                simple comprehension check questions (e.g., “two plus two
make a moral judgment. In the human conditions                                          is equal to what?”) and whether they had paid attention and
participants were asked “Would it be appropriate for the                                taken their participation seriously.
[truck to turn onto the side street / blue car to push the
yellow car into the crossing]?” for Switch and Push                                     Results. Of the 413 participants recruited, 99 failed at least
dilemmas, respectively. In the AV conditions participants                               one comprehension check or indicated they had not paid
were asked, “Would it be appropriate to design the steering                             attention. These participants were excluded, leaving 314
function so that the [driverless truck turns onto the side                              participants in the following analyses.
street / blue car pushes the yellow car into the crossing]?”                               Participants’ moral judgments are shown in Figure 2.
   The level of abstraction between the concrete AV and                                 These judgments were examined with a 2 x 3 (dilemma x
abstract AV conditions was also manipulated by additional                               abstraction) between-subjects ANOVA. Participants
language introducing the questions and problems. Before                                 approved of acting in the Switch cases much more strongly
the “would it be appropriate …” question was asked, the                                 than in the Push cases for all conditions, as indicated by a
abstract AV condition added instructions to “Imagine a                                  significant main effect of dilemma, F(1, 308) = 231.5, p <
world in which the steering function of a driverless car                                .001.
would decide that the blue car should push the yellow car                                  In support of our causal model explanation of the Switch
into the crossing.”                                                                     and Push dilemmas, the main effect of abstraction was
Procedure. Participants were recruited from mTurk and                                   significant (F(2, 308) = 5.359, p = .005) and there was a
redirected to a Qualtrics survey website where study                                    significant interaction, F(2, 308) = 4.898, p = .008. The
procedures were administered. Participants gave their                                   interaction appears to be driven by a difference between
consent to participate and answered some brief demographic                              conditions for the Switch dilemma, and an absence of
questions before they were randomly assigned to a                                       differences for the Push dilemma. Whereas significant
condition. Participants were then given some context about                              differences were observed for the Switch dilemma between
the topic of the study. Those assigned to the AV conditions                             the abstract AV condition and the human condition (t(103)
read a brief explanation about the development of driverless                            = 3.886, p < .001), as well as between the abstract AV
cars, wherein it was explained that the cars would                                      condition and the concrete AV condition (t(104) = 2.355, p
sometimes have to make decisions where an accident was                                  = .02), no significant differences were observed among the
unavoidable. Participants in the human driver conditions                                Push scenarios (all ps > .05). The contrast between the
read a similarly worded introduction, but with the                                      concrete AV and human condition was also non-significant,
discussion of driverless cars omitted. Participants then                                t(101) = 1.39, p = .168.
considered their assigned dilemma and made a moral
judgment about whether it was appropriate for the driver to                                                    Experiment 2
take action in the dilemma. These judgments were made on                                To further test the effect of abstractness, we added an
a Likert scale ranging from 1 (completely appropriate) to 6                             abstract human driver condition in Experiment 2. In other
(completely inappropriate). Finally, participants answered                              words, we added a manipulation of abstraction by wording.
                                                                                        Two additional goals are to test the replicability of the
                                    6                            Switch dilemma         results of the previous study and to evaluate a floor effect as
    Mean Appropriateness Rating
                                  5.5                                                   an alternative explanation of the lack of influence of driver
                                    5                            Push dilemma           (human vs AV) for the Push dilemma.
                                  4.5
                                    4                                                   Participants. Participants were 610 workers (351 female,
                                  3.5
                                                                                        median age = 32 years old) recruited from Amazon’s
                                                                                        Mechanical Turk (mTurk) work distribution website.
                                    3
                                                                                        Workers were paid $0.25 to participate in the study.
                                  2.5
                                    2                                                   Design, Materials, and Procedure. The materials and
                                  1.5                                                   procedure of this experiment were nearly identical to those
                                    1                                                   of Experiment 1 save for the addition of an abstract human
                                         Human     AV concrete    AV abstract           driver condition for Switch and Push dilemmas. This
                                        concrete                                        resulted in a 2 x 2 x 2 factorial design (dilemma x driver x
                                                                                        wording).
 Figure 2: Participants’ moral judgments in Experiment 1.
                                                                                  310

   In the abstract human driver conditions, participants were               Also as predicted, moral approval generally appears to be
introduced to the dilemmas in the context of developing a                lower for abstract wordings than for concrete wordings,
training manual for delivery and ridesharing companies. The              although this effect was not significant, F(1, 519) = 2.889, p
introduction was similar to the abstract AV conditions,                  = .09. Although this manipulation was meant to introduce a
stressing that the rules must be made to apply across many               greater level of abstraction in the same way as the AV
different scenarios. For the Switch and Push variants of                 conditions, the manipulation was unfortunately not fully
these conditions, the moral judgment question read                       equated. For example, the manual explicitly reminded
“Imagine a world where general guidelines in the                         subjects of 6the specifics of the dilemmas (Switch, Push),
transportation company manual prescribe that the driver of               whereas the5.5instructions for abstract AV condition did not.
                                                                               Mean Appropriateness Rating
the [truck should turn onto the side street and hit the yellow           This may have5 made it more difficult for participants in the
car / blue car should push the yellow car into the                       abstract human condition to invoke abstract principles.
                                                                                    4.5
crossing]. Would it be appropriate to write the general
guidelines so that a company driver [would turn the
                                                                                      4
                                                                         Table 1. Mean moral appropriateness judgments by
truck onto the side street / in the blue car would push the                         3.5
                                                                         dilemma (switch, push), driver (human, AV) and wording
yellow car into the crossing]?”                                                       3
                                                                         (concrete, abstract)  from participants in Experiment 2.
   To examine the possibility of floor effects for the Push                         2.5                       Human            AV
items, participants who were assigned to the Push dilemma                             2      Concrete       4.25 (1.49)    3.75 (1.55)
conditions were also assigned to make a judgment about a                     Switch1.5
                                                                                              Abstract      3.83 (1.65)    3.54 (1.48)
Transplant dilemma. This dilemma asks participants to                                 1      Concrete       1.54 (1.18)    1.45 (1.04)
judge whether it is acceptable for a doctor to kill a patient in              Push            Human       AV1.29
                                                                                                              concrete
                                                                                              Abstract           (0.85) AV1.55
                                                                                                                             abstract
                                                                                                                                (1.14)
order to use his organs to save several other patients.                                      concrete
                                                                         Note: Means with standard deviations in parentheses.
Results. Of the 610 participants originally recruited, 83
                                                                           A further investigation of participants’ Switch dilemma
were excluded for failing at least one comprehension check
                                                                         judgments in a 2 x 2 ANOVA (driver x wording) reveals a
or for indicating that they had not paid attention, leaving
                                                                         significant effect of driver (F(1, 260) = 4.348, p = .038).
527 participants in the final analysis.
                                                                         These findings qualitatively replicate the differences
   Participants’ moral judgments in Experiment 2 are shown
                                                                         between conditions in Experiment 1, although only the
in Table 1. These judgments were analyzed using a 2 x 2 x 2
                                                                         contrast between the abstract AV and concrete human
(dilemma x driver x wording) between-subjects ANOVA.
                                                                         condition was significant, t(122) = 11.36, p < .001. All other
As in Experiment 1, approval was much lower for Push
                                                                         effects were non-significant (all Ps > .06).
dilemmas than Switch dilemmas, indicated by a significant
                                                                           The absence of differences among the Push scenarios
effect of dilemma, F(1, 519) = 422.8, p < .001. Replicating
                                                                         seems unlikely to be a simple floor effect, as indicated by
the results of Experiment 1, moral approval was lower in
                                                                         the still lower approval for the Transplant dilemma (Mean =
AV conditions than human conditions for the Switch
                                                                         1.29, SD = .819), t(262) = 2.771, p = .006.
dilemma but not the Push dilemma, as indicated by the two-
way interaction between dilemma (switch or push) and
driver (human or AV), F(1, 519) = 4.351, p = .037.
                                                                         Meta-Analysis of Experiments 1 and 2
                                                                         With the exception of abstract human driver conditions, the
                                    6                                    materials and procedures of Experiments 1 and 2 are
    Mean Appropriateness Rating
                                  5.5                                    identical, allowing data from these experiments to be pooled
                                    5                                    for their shared conditions for increased statistical power.
                                  4.5                                       Of particular interest are comparisons between the human,
                                    4                                    concrete AV and abstract AV conditions for the Switch
                                  3.5                                    dilemma, as qualitatively similar yet somewhat different
                                    3                                    patterns of results were observed in Experiments 1 and 2.
                                  2.5                                    Pooling these data reveals significant contrasts for all
                                    2                                    pairwise comparisons: lower approval was observed for the
                                                                         abstract AV condition as compared with the concrete AV
                                  1.5
                                                                         (t(240) = 2.125, p = .035) and human conditions, t(238) =
                                    1                                    4.609, p < .001. Lower approval was also observed for the
                 Human       AV concrete AV abstract                     concrete AV condition as compared with the human
                concrete                                                 condition, t(228) = 2.329, p = .021.
 Figure 3: Participants moral judgments for switch
 dilemmas averaged from Experiments 1 and 2.
                                                                   311

                          Discussion                                                        References
Across two experiments we found that people were                   Bonnefon, J.-F., Shariff, A., & Rahwan, I. (2015).
generally willing to allow AVs to act to sacrifice one life in       Autonomous vehicles need experimental ethics: Are we
order to save three others in a common-cause scenario like           ready for utilitarian cars? arXiv:1510.03346 [cs.CY]
the Switch but not in a causal chain scenario like the Push.       Gao, P., Hensley, R., & Zielke, A. (2014). A road map to
However, as compared with judgments of human drivers,                the future for the auto industry. McKinsey Quarterly.
people were less willing to see AVs sacrifice in a common-         Greene, J. D., Sommerville, R. B., Nystrom, L. E., Darley,
cause dilemma. There was no analogous difference for a               J. M., & Cohen, J. D. (2001). An fMRI investigation of
causal-chain dilemma. These findings are consistent with             emotional engagement in moral judgment. Science,
the role of causal structure in human moral judgments and            293(5537), 2105–8.
with the constraint of symmetry based on Kamm’s (2015)             Hauser, M., Cushman, F., Young, L., Jin, R. K.-X., &
analysis of inter-victim relationships (see also Waldmann,           Mikhail, J. (2007). A dissociation between moral
Wiegman, & Nagel, in press).                                         judgments and justifications. Mind & Language, 22, 1–
                                                                     21.
   Our main goal was to provide a theoretical account that         Kamm, F. M. (2015). The trolley problem mysteries. New
both concurs with human moral judgments and is ethically             York: Oxford University Press.
defensible. We tested and confirmed the predicted                  Shepard, R. (2008). The step to rationality: The efficacy of
interaction between the abstractness of construals of moral          thought experiments in science, ethics, and free will.
dilemmas and the causal structure of moral dilemmas. We              Cognitive Science, 32, 3-35.
acknowledge, however, that the experiments were not                Spieser, K. et al (2014). Toward a systematic approach to
designed to test our account of the interaction against              the design and evaluation of automated mobility-on-
alternatives. It may be possible, for example, to derive             demand systems: A case study in Singapore. In G. Meyer
predictions from two-system theories, although we do not             and S. Beiker (Eds.), Road Vehicle Automation, 229–245.
see how they can make plausible predictions about                    Springer.
abstractness. If anything, this account should seem to             Van Arem, B., Van Driel, C. J., & Visser, R. (2006). The
predict more utilitarian reasoning in abstract cases, which          impact of cooperative adaptive cruise control on traffic-
should not trigger emotional involvement, contrary to what           flow characteristics. IEEE Transactions on Intelligent
we found.                                                            Transportation Systems, 7, 429– 436.
   Another factor that is often neglected in research on moral     Waldmann, M. R., & Dieterich, J. H. (2007). Throwing a
dilemmas concerns legal considerations. Trolley dilemmas             bomb on a person versus throwing a person on a bomb:
describe rare situations which do not routinely happen.              intervention myopia in moral intuitions. Psychological
Therefore, there are no clear legal regulations about such           Science, 18, 247–53.
                                                                   Waldmann, M. R., Wiegmann, A., & Nagel, J. (in press).
accidents. By contrast, accidents involving cars are frequent.
                                                                     Causal models mediate moral inferences. In J.-F.
Traffic is strictly regulated by laws. In fact, articles about
                                                                     Bonnefon & B. Trémolière (Eds), Moral inferences.
AVs typically focus on the possibility of accidents and on
                                                                     Hove: Psychology Press.
issues of liability. Thus, it seems plausible to assume that a     Waldrop, M. M. (2015). Autonomous vehicles: No drivers
programmer of an AV steering mechanism will try to                   required. Nature, 518, 20–23.
prevent situations in which the AV either kills its owner or       Wiegmann, A., & Waldmann, M. R. (2014). Transfer effects
innocent bystanders. However, legal considerations seem              between moral dilemmas: A causal model theory.
less able to explain the lower moral approval we observed in         Cognition, 131(1), 28–43.
the abstract AV condition compared to the concrete AV
condition, both in Experiment 1 and in our meta-analysis.
Both conditions involved the program in AVs operating
across all traffic situations, and the company developing the
program would be equally liable.
   The development of ethical autonomous machines is an
important and exciting application of the budding field of
experimental ethics, to which we hope significant further
inquiry will be devoted. Keeping pace with the rapid
development of AI research and engineering certainly
demands it.
                                                               312

