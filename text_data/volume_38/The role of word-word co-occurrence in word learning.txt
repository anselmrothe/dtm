                  The role of word-word co-occurrence in word meaning learning
                                        Abdellah Fourtassi (a.fourtassi@ueuromed.org)
                                                The Euro-Mediterranean University of Fes
                                             Department of Psychology, Stanford University
                                      Emmanuel Dupoux (emmanuel.dupoux@gmail.com)
                           Ecole Normale Supérieure / PSL Research University / EHESS / CNRS, France
                              Abstract                                   a given word. New evidence either corroborate this hypoth-
                                                                         esis or contradict it (Medina, Snedeker, Trueswell, & Gleit-
   A growing body of research on early word learning suggests            man, 2011; Trueswell, Medina, Hafri, & Gleitman, 2013).
   that learners gather word-object co-occurrence statistics across
   learning situations. Here we test a new mechanism whereby             Yurovsky and Frank (2015) proposed a synthesis of both ac-
   learners are also sensitive to word-word co-occurrence statis-        counts, whereby the learner’s choice to adopt one of the two
   tics. Indeed, we find that participants can infer the likely ref-     learning strategies depends on the complexity of the learning
   erent of a novel word based on its co-occurrence with other
   words, in a way that mimics a machine learning algorithm              situation.
   dubbed ‘zero-shot learning’. We suggest that the interaction             This being said, XSL is unlikely to be the unique mecha-
   between referential and distributional regularities can bring ro-     nism of word learning at work. First, real learning situations
   bustness to the process of word acquisition.
                                                                         are much more ambiguous than typical simulated situations
   Keywords: word learning; semantics; cross-situational learn-          used in laboratory experiments. When subjects are tested in a
   ing; distributional semantic models; zero-shot learning.
                                                                         more realistic learning context, the load on memory increases
                                                                         and, therefore, the ability to make use of the available vi-
                          Introduction                                   sual information diminishes (Medina et al., 2011; Yurovsky
How do children learn the meanings of words in their na-                 & Frank, 2015). Second, XSL assumes a perfect covariance
tive language? This question has intrigued a lot of schol-               between words and their referents. This assumption does not
ars studying human language acquisition. Quine (1960) fa-                take into account the fact that words –in real situations– are
mously noted the difficulty of this process. In fact, every nam-         sometimes uttered in the absence of their referents (e.g. when
ing situation is ambiguous. For example, if I utter the word             talking about past events, “remember that cat?”). In this ex-
gavagai and point to a rabbit, you may possibly infer that I             periment, we propose a statistical learning mechanism that
mean the rabbit, the rabbit’s ear, or its tail or color,...etc. A        purports to complements XSL, through relying on cues from
popular proposal in the language acquisition literature sug-             the concomitant linguistic information, and more precisely on
gests that, even if one naming situation is ambiguous, be-               word co-occurrence.
ing exposed to many situations allows the learner to nar-
                                                                                Form word co-occurrence to semantic
row down, over time, the set of possible word-object map-
pings (e.g., Pinker, 1989). This proposed learning mecha-                                           similarity
nism has come to be called Cross-Situational Learning (here-             Typical XSL settings assume that words occur in isolation.
after, XSL). Laboratory experiments have shown that humans               In real learning contexts, however, words are embedded in
are cognitively equipped to learn in this way. For example,              natural speech, and have consistent distributional properties.
L. Smith and Yu (2008) presented adults with trials that simu-           In particular, semantically similar words tend to co-occur
lated real world uncertainty: each trial was composed of a set           more often than semantically unrelated words. For exam-
of words and a set of objects, in such a way that no single trial        ple, the word “ball” and “play” tend to co-occur more often
had enough information about the precise mappings. How-                  than “ball” and “eat”. This fact is documented in linguis-
ever, after being exposed to many of such trials, participants           tics under the name of the ‘distributional hypothesis’ (here-
were eventually able to name the objects with a better-than-             after, DH) (Harris, 1954), and has been popularized by Firth’s
chance performance. Many experiments replicated this effect              famous quote “You shall know a word by the company it
with adults, children and infants (Yu & Smith, 2007; Suanda,             keeps” (Firth, 1957). The distributional hypothesis is also the
Mugwanya, & Namy, 2014; Vlach & Johnson, 2013). Sub-                     basis for distributional semantics, the sub-field of computa-
sequent research tried to characterize the algorithmic under-            tional linguistics that aims at characterizing words’ similarity,
pinnings of XSL. Some experiments suggested that learn-                  based on their distributional properties in large text corpora.
ers accumulate in a parallel fashion all statistical regularities        Tools from the field of distributional semantics such as La-
about word-object co-occurrences, and they use them to grad-             tent Semantic Analysis (Landauer & Dumais, 1997), Topic
ually reduce ambiguity across learning situations (McMurray,             Models (Blei, Ng, & Jordan, 2003), or more recently Neural
Horst, & Samuelson, 2012; Vouloumanos, 2008; Yurovsky,                   Networks (Mikolov, Karafiát, Burget, Cernocký, & Khudan-
Yu, & Smith, 2013). Other experiments suggested that learn-              pur, 2010) have proved to be effective in modeling human
ers maintain, instead, a single hypothesis about the referent of         word similarity judgement (Landauer, McNamara, Dennis,
                                                                     662

 & Kintsch, 2007; Griffiths, Steyvers, & Tenenbaum, 2007;
 Fourtassi & Dupoux, 2013; Parviz, Johnson, Johnson, &
 Brock, 2011).
                       Zero-shot learning
 Models that learn through DH typically require a large cor-
 pus, especially if nothing is known about the language. Here,
 we explore the case where some words are already known and
 only one word is learned through DH. This corresponds to the
 so-called ‘zero-shot learning’ situation.
    An interesting example of this situation has been given
 by Socher, Ganjoo, Manning, and Ng (2013). They built a
 model that can map a label to a picture even when the la-             Figure 1: Referential familiarization. Participants are pre-
 bel has not been used in training! More precisely, using the          sented with multiple series of word-objects pairings. The ob-
 CIFAR-10 dataset, the model was first trained to map 8 out            jects belong to the category of animals or the category of ve-
 of the 10 labels (“automobile”, “airplane”, “ship”, “horse”,          hicles.
 “bird”, “dog”, “deer”, “frog”) in the dataset, to their visual
 instances. The remaining labels (“cat” and “truck”) were
 omitted and reserved for the zero-shot analysis. Second, they         in an artificial language and their referents. In the distribu-
 used a distributional semantic model (based on Neural Net-            tional familiarization, participants hear ‘sentences’ made of
 works) to obtain vector representations for the entire set of         words from this artificial language without visual referents;
 labels (i.e., including “cat” and “truck”) based on their co-         some of these words were familiar (introduced in the referen-
 occurrence statistics in a large text corpus (Wikipedia text).        tial familiarization), and others were novel words. Crucially,
 When tested on it ability to classify a new picture (a cat or         the novel items co-occur consistently with words of the same
 a truck) under either the label of “truck” or “cat”, the model        semantic category. Finally, the semantic generalization phase
 performed with a high accuracy, using only the patterns of            tests whether subjects can rely on distributional information
 co-occurrence among labels, and the semantic similarity be-           alone to infer the semantic category of the novel words, with-
 tween the new and old pictures. For example, when presented           out any prior informative referential situation. Below is a de-
 with the picture of a cat, the model has to classify it as “cat”      tailed description of each step of the experimental procedure.
 or “truck”. The models makes the link between the picture of          Step 1: referential familiarization In this phase of the ex-
 the cat and that of a similar picture (e.g. dog), and chooses the     periment (Figure 1), participants are taught the pairing of 4
 label that is more related to the label of this similar picture,      words in an artificial language1 with 4 objects. The objects
 i.e., “cat”. In fact, “cat” co-occurs more often with “dog” than      belong to either the category of vehicles (car, motorcycle) or
 with, say, “airplane”. Therefore the label “cat” is favored over      the category of animals (deer, swan). Participants see a pic-
 the alternative label (i.e., “truck”).                                ture of the referent on the screen and hear its label simulta-
    The conditions of zero-shot learning are often met in the          neously. There are 3 trials, each consists of a randomized
 context of word acquisition. For instance, this corresponds to        presentation of the series of 4 pairings.
 the (rather ubiquitous) situation where an unknown word is            Step 2: learning consolidation The purpose of this phase
 heard in the absence of its visual referent. Therefore, we sug-       is to consolidate and strengthen the participants’ knowledge
 gest that human learners can go about it in a way that mimics         about the 4 word-object pairings (Figure 2). Participants
 the mechanism of zero-shot learning. In the following, we             are tested using a Two Alternative Forced Choice paradigm
 test this hypothesis with adults, following closely the spirit of     (2AFC). They are presented with a series of trials where they
 the model developed by Socher et al. (2013).                          hear a label (pibu, nulo, romu or komi) and are shown two
                                                                       objects; one of which is the correct referent, and the other
                              Method                                   belongs to the other semantic category. Crucially, after they
 The experiment consists of 4 steps:                                   have made a choice, they get a feedback on their answers
                                                                       (“correct”/“wrong”). Participants are presented with 16 ques-
1. Referential familiarization                                         tions of this sort, which correspond to the combinatorial pos-
2. Learning consolidation                                              sibilities of forming pairs of items from one semantic cate-
                                                                       gory with items from the other category (4 cases), in conjunc-
3. Distributional familiarization                                      tion with the order of the visual presentation of the referents
                                                                       (4 × 2 cases) and the item being labeled (4 × 2 × 2 = 16 cases
4. Semantic generalization                                             in total).
    The referential familiarization and consolidation consists
 in explicitly teaching subjects the association between words             1 The audio stimuli were graciously provided by Naomi Feldman.
                                                                   663

                                                                   Figure 3: Distributional familiarization. Sequences of words
                                                                   are presented with no visual referents. Two new words
Figure 2: Learning consolidation. Two-Alternative Forced           (“guta” and “lita”) are introduced and co-occur consistently
Choice paradigm (2AFC), with feedback.                             with the words corresponding to one of the two semantic cat-
                                                                   egories (“romu” and “komi” for the category of animals, and
                                                                   “nulo” and “pibu” for the category of vehicles)
Step 3: distributional familiarization Distributional fa-
miliarization follows the referential training and consolida-
tion. Participants listen to ‘sentences’ made of words from        tions about familiar items so as to avoid any form of cross-
this artificial language without any visual referent. As ex-       situational learning during the test phase.
plained in Figure 3, each sentence consists of 3 words. Two        Procedure As shown in Figure 4, participants are first
of which are familar words from one semantic category, i.e.,       trained on the pairing between 4 artificial words and their ref-
either romu and komi (animals) or pibu and nulo (vehicles).        erents (part 1 and part 2). Then they are exposed to 2 blocks
The third word is a new artificial word that consistently co-      of distributional familiarization (part 3), and they are tested
occur with them. The new words are guta and lita. The way          3 times (part 4): before any exposure to distributional infor-
guta/lita are distributed with either (romu, komi) or (pibu,       mation (baseline) and after the first and the second block of
nulo) was counterbalanced across participants so as to avoid       distributional exposure (respectively session 1 and 2).
different sorts of linguistic and perceptual biases that may
arise from the way the stimulus is organized. There is a 750
ms pause between words, and 2500 ms pause between sen-
tences. There are 16 sentences in total, 8 for each semantic
context; (romu, komi) and (pibu, nulo). Words within sen-
tences are randomized and the semantic context is alternated
during the exposure.
Step 4: testing semantic generalization Participants are
presented again with a two alternative forced choice. As ex-
plained previously in the learning consolidation phase, they
hear a label and they are asked to choose between two ob-
jects, but here participants do not get feedback on their an-
swers. We are particularly interested in how participants re-
spond in the situation where they hear the novel item (guta
or lita) and are presented with two new objects that represent
                                                                   Figure 4: Order of exposure in the experiment. Participants
a new animal (squirrel) and a new vehicle (trolley). Partici-
                                                                   are trained referentially once (part 1 and part 2), distribution-
pants have never been shown the referential mapping of the
                                                                   ally twice (part 3). They are tested in three sessions (part 4):
new words, so their answer would reveal whether distribu-
                                                                   before and after each block of distributional learning
tional learning alone had helped them infer semantic knowl-
edge about the word (i.e., the semantic category of the ref-
erent). This test phase is composed of 4 questions about the       Population and rejection criterion 50 Participants were
novel labels/objects, varying the visual order of the objects      recruited online through Amazon Mechanical Turk. We in-
(1 × 2) and the object being named (1 × 2 × 2 = 4 cases in         cluded in the analysis participants whose total score on the
total), in addition to 4 selected questions about the familiar     familiar word-object questions during the testing phases (i.e.,
words/objects used in the referential training. We eliminated      part 4) were above chance level. This is a way to select only
any overlap between questions about novel items and ques-          subjects who paid attention during the training parts. 2 par-
                                                               664

                                                                      and objects (as suggested in XSL), but also to co-occurrence
                                                                      statistics between words themselves (as suggested in the DH).
                                                                      More importantly, we showed that these two sensitivities in-
                                                                      teract in a way that mimics a machine learning mechanism
                                                                      called zero-shot learning. In fact, participants in our exper-
                                                                      iment were able to guess the semantic category of a novel
                                                                      word whose visual referent was never presented through the
                                                                      semantic properties of the words with which it co-occurred
                                                                      consistently. Participants knew beforehand that they would
                                                                      be introduced to an artificial language and that they would
                                                                      have to learn the meaning of words in this language, but they
                                                                      were not explicitly instructed about the fact that words that
Figure 5: proportion of correct answers for familiar and novel        co-occur in the same sentences are supposed to have simi-
test items, before any distributional exposure (baseline) and         lar meanings. Participants have spontaneously turned to co-
after the first and second block of exposure (session 1 and 2)        occurrence in order to cue semantic similarity, and infer the
                                                                      category of the ambiguous words.
ticipants were excluded based on this criterion.                         Although we used an artificial language whose ‘sentences’
                                                                      fall short, on many aspects, of real speech, this work pro-
                   Results and Analysis                               vides evidence for the cognitive plausibility of this learning
Figure 5 shows the proportion of correct answers on both fa-          mechanism, much in the spirit of the statistical learning liter-
miliar and novel items, as a function of the testing session.         ature (e.g., L. Smith & Yu, 2008; Saffran, Aslin, & Newport,
In the familiar condition, answers were almost perfect in the         1996). If it scales up to real languages, this word-word co-
three sessions (before exposure, after one block, and after           occurrence mechanism would prove crucial in complement-
two blocks of exposure to part 3). This shows that partic-            ing word-object co-occurrence mechanisms. In fact, most
ipants have reliably learned the association between words            word-object co-occurrence learning strategies (e.g. XSL) as-
and their referents during the training phase, and that this          sume that words covary perfectly with their referents. This
learning was not affected by subsequent exposure to distribu-         assumption is not always correct. For example, when talk-
tional information. In the novel condition, and before distri-        ing about a past event, the conversation may not match the
butional training (i.e., baseline), subjects were at chance level     immediate visual context. In contrast, words used in a given
(M = 50.5% of correct answers). A one sample t-test compar-           conversation, be it about present, past or future events, nor-
ing the mean against chance (i.e, 50%) gives a t(47) = 0.083          mally co-occur in a coherent fashion. The learner can rely on
with p-value = 0.93. The absence of learning is a predictable         this intrinsic property of speech to bring about robustness to
result since participants had no prior cue about the relevant         the learning process. For example, suppose the learner, while
object mapping. However, after one and two blocks of dis-             at home, hears a discussion about the last visit to the “zoo”.
tributional training, subjects were significantly above chance        XSL learning, if operating alone, would be confusing. In con-
level. A one sample t-test gives, respectively, for session 1         trast, if XSL operates in concert with DH, the learner would
an average of correct answers M = 72.4%, with t(47) = 3.94            tend, if in doubt, to link a new word (e.g., “zoo”) not to some
(p < 0.001), and for session 2, an average of M = 68.2%,              surrounding object, but to other co-occurring words, which
with t(47) = 2.85 (p = 0.006). In order to compare the                are likely to be zoo-related words (such as “animals”, “bird”
behaviour of the participants before and after distributional         and “monkey”). Further work is needed to characterize the
training, we performed a paired t-test. For baseline vs. ses-         precise conditions under which learners would rather switch
sion 1, there was a significant change, the difference mean is        to the word-word co-occurrence cue to infer meaning.
equal to M = 0.218, with t(47) = 2.99 (p < 0.01). Similarly,             Moreover, the proposed mechanism can help learners de-
for baseline vs. session 2, the difference mean is M = 0.177,         velop an early semantic representation for words with a rather
with t(47) = 2.24 (p = 0.029). However, between session 1             abstract meaning. Abstract words (like “eat” and “good”)
and session 2, the difference mean M = 0.041 was not sig-             are learned later in development than words with salient con-
nificant, t(47) = 0.662, p = 0.51. This shows that most of            crete referents (such as “ball” and “shoe”) (e.g., Bergelson
the learning occurred during the first block of distributional        & Swingley, 2013). They are presumably harder to learn be-
exposure. Additional training did not significantly improve           cause there is no obvious or/and lasting correspondence be-
learning (if anything, it seems to slightly decrease the aver-        tween the word and the physical environment. Bruni, Tran,
age of correct responses).                                            and Baroni (2014) developed a model which extends purely
                                                                      word-word co-occurrence learning strategies (such as LSA
                          Discussion                                  model) to also encompass co-occurrence with the visual con-
The results show that, when learning the meaning of words,            text. They assessed the contribution of textual and visual in-
people are sensitive, not only to the co-occurrence of words          formation in approximating the meaning of abstract vs. con-
                                                                  665

crete words. They found that visual information was mostly                               Acknowledgments
beneficial in the concrete domain, while it maintained an al-       This work was supported by the European Research Coun-
most neutral impact on the abstract domain where most learn-        cil (ERC-2011-AdG-295810 BOOTPHON), the Agence Na-
ing was based on word-word co-occurrence. Future work will          tionale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-
investigate the extent to which this finding squares with psy-      10-IDEX-0001-02 PSL*), the École des Neurosciences de
chological behaviour. For instance, an interesting question         Paris Ile-de-France, the Fondation de France, the Region Ile
would be to test whether human learners switch from word-           de France (DIM Cerveau et Pensée), and an AWS in Educa-
object cue to word-word cue when the potential abstractness         tion Research Grant award.
of the target word increases.
                                                                                              References
   Finally, during the write-up of this paper, it came to our
knowledge that Ouyang, Boroditsky, and Frank (in press)             Bergelson, E., & Swingley, D. (2013). The acquisition of
conducted an experiment that shared many similarities with             abstract words by young infants. Cognition, 127.
ours. However, it also presented interesting differences both       Blei, D., Ng, A., & Jordan, M. (2003). Latent dirichlet al-
in terms of the experimental setup and the results. Ouyand             location. The Journal of Machine Learning Research, 3,
at al. exposed adult participants to auditory sentences from a         993–1022.
MNPQ language. It is an artificial language where sentences         Bruni, E., Tran, N., & Baroni, M. (2014). Multimodal Dis-
take the form of “M and N” or “P and Q”. Ms and Ps are                 tributional Semantics. Journal of Artificial Intelligence Re-
used as context words, whereas Ns and Qs are target words.             search, 49.
We believe there are two crucial differences between the two        Firth, J. R. (1957). A synopsis of linguistic theory 19301955.
experiments. First, the context words (M and P) were com-              In Studies in linguistic analysis. Oxford, Blackwell.
posed of a mix of various proportions of real English words or      Fourtassi, A., & Dupoux, E. (2013). A corpus-based eval-
non-words. In our experiment, they were all non-words. Sec-            uation method for distributional semantic models. In Pro-
ond and more important, Ouyang et al. (in press) followed              ceedings of ACL.
the spirit of MNPQ’s paradigm in keeping constant the or-           Griffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007).
der of the words in the sentences, that is, M and P always             Topics in semantic representation. Psychological Review,
occurring first in the sentence, and N and Q always occur-             114(2), 211-244.
ring last. Our experiment was more faithful to the hypothesis       Harris, Z. (1954). Distributional structure. Word, 10(23),
of bag-of-words, which is crucial in distributional semantic           146-162.
models: order within a particular semantic context (e.g., a         Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s
sentence) is irrelevant. It was therefore randomized across            problem: The Latent Semantic Analysis theory of acquisi-
trials. Interestingly, although none of the context words we           tion, induction and representation of knowledge. Psycho-
used were known words, we obtained a high learning rate. In            logical Review, 104(2), 211-240.
contrast, Ouyang et al. (in press) obtained successful learning     Landauer, T. K., McNamara, D. S., Dennis, S., & Kintsch, W.
only when most of the context words were familiar English              (2007). Handbook of latent semantic analysis. Mahwah,
words. A plausible explanation for this difference is that, in         NJ: Erlbaum.
the case of MNPQ language, participants have two possible           McMurray, B., Horst, J. S., & Samuelson, L. K. (2012). Word
learning dimensions: learning the positional patterns (what            learning emerges from the interaction of online referent se-
word comes first, and what words comes last) and learning              lection and slow associative learning. Psychological Re-
the co-occurrence patterns (what couple of words co-occurred           view, 119.
with each other). In fact, it has been shown that when both         Medina, T., Snedeker, J., Trueswell, J., & Gleitman, L.
positional and co-occurrence cues are available, participants          (2011). How words can and cannot be learned by obser-
tend focus on the first ones (K. Smith, 1966). By using fa-            vation. Proceedings of the National Academy of Sciences,
miliar words, Ouyang et al. (in press) showed that partici-            108(22), 9014.
pants were more likely to learn co-occurrence patterns, prob-       Mikolov, T., Karafiát, M., Burget, L., Cernocký, J., & Khu-
ably through alleviating part of the memory constraint. In our         danpur, S. (2010). Recurrent neural network based lan-
case, the positional patterns were random, which left partic-          guage model. In Proceedings of INTERSPEECH.
ipants with only one learning dimension (i.e., co-occurrence        Ouyang, L., Boroditsky, L., & Frank, M. C. (in press). Se-
pattern).                                                              mantic coherence facilitates distributional learning of word
                                                                       meaning. Cognitive Science.
   To conclude, this experiment provided a cognitive proof          Parviz, M., Johnson, M., Johnson, B., & Brock, J. (2011). Us-
of principle to the zero-shot learning mechanism, according            ing language models and latent semantic analysis to char-
to which (early) semantic knowledge can be learned through             acterise the n400m neural response. In Proceedings of the
sensitivity to word co-occurrence in speech. Future work will          Australasian Language Technology Association Workshop.
focus on exploring properties of this new learning and how it       Pinker, S. (1989). Learnability and cognition: The acquisi-
interacts with cross-situational learning.                             tion of argument structure. Cambridge, MA: MIT press.
                                                                666

Quine, W. (1960). Word and object. The MIT Press.
Saffran, J. R., Aslin, R., & Newport, E. (1996). Statisti-
  cal learning by 8-month-old infants. Science, 274(5294),
  1926.
Smith, K. (1966). Grammatical intrusions in the recall of
  structured letter pairs: mediated transfer or position learn-
  ing? Journal of Experimental Psychology, 72, 580–588.
Smith, L., & Yu, C. (2008). Infants rapidly learn word-
  referent mappings via cross-situational statistics. Cogni-
  tion, 106(3), 1558–1568.
Socher, R., Ganjoo, M., Manning, C. D., & Ng, A. Y. (2013).
  Zero-Shot Learning Through Cross-Modal Transfer. In
  Proceedings of Conference on Neural Information Process-
  ing Systems (NIPS).
Suanda, S. H., Mugwanya, N., & Namy, L. L. (2014).
  Cross-situational statistical word learning in young chil-
  dren. Journal of Experimental Child Psychology, 126.
Trueswell, J. C., Medina, T. N., Hafri, A., & Gleitman, L. R.
  (2013). Propose but verify: Fast mapping meets cross-
  situational learning. Cognitive Psychology, 66.
Vlach, H. A., & Johnson, S. P. (2013). Memory constraints
  on infants’ cross-situational statistical learning. Cognition,
  127.
Vouloumanos, A. (2008). Fine-grained sensitivity to statisti-
  cal information in adult word learning. Cognition, 107(2),
  729–742.
Yu, C., & Smith, L. (2007). Rapid word learning under un-
  certainty via cross-situational statistics. Psychological Sci-
  ence, 18(5), 414–420.
Yurovsky, D., & Frank, M. C. (2015). An Integrative Account
  of Constraints on Cross-Situational Learning. Cognition,
  145.
Yurovsky, D., Yu, C., & Smith, L. B. (2013). Competitive
  processes in cross-situational word learning. Cognitive Sci-
  ence, 37.
                                                                 667

